<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996152">
Modularity and Information Content
Classes in Principle-based Parsing
</title>
<author confidence="0.986649">
Paola Merlo*
</author>
<bodyText confidence="0.967977875">
Universite de Geneve
In recent years models of parsing that are isomorphic to a principle-based theory of grammar (most
notably Government and Binding (GB) Theory) have been proposed (Berwick et al. 1991). These
models are natural and direct implementations of the grammar, but they are not efficient, because
GB is not a computationally modular theory. This paper investigates one problem related to the
tension between building linguistically based parsers and building efficient ones. In particular, the
issue of what is a linguistically motivated way of deriving a parser from principle-based theories
of grammar is explored. It is argued that an efficient and faithful parser can be built by taking
advantage of the way in which principles are stated. To support this claim, two features of an
implemented parser are discussed. First, configurations and lexical information are precompiled
separately into two tables (an 5 table and a table of lexical co-occurrence) which gives rise to
more compact data structures. Secondly, precomputation of syntactic features (0-roles, case, etc.)
results in efficient computation of chains, because it reduces several problems of chain formation
to a local computation, thus avoiding extensive search of the tree for an antecedent or extensive
backtracking. It is also shown that this method of building long-distance dependencies can be
computed incrementally.
</bodyText>
<sectionHeader confidence="0.992127" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999775111111111">
In the development of parsers for syntactic analysis, it is standard practice to posit
two working levels: the grammar, on the one hand, and the algorithms, which produce
the analysis of the sentence by using the grammar as the source of syntactic knowl-
edge, on the other hand. Usually the grammar is derived directly from the work of
theoretical linguists. The interest in building a parser that is grounded in a linguistic
theory as closely as possible rests on two sets of reasons: first, theories are developed
to account for empirical facts about language in a concise way—they seek general,
abstract, language-independent explanations for linguistic phenomena; second, cur-
rent linguistic theories are supposed to be models of humans&apos; knowledge of language.
Parsers that can use grammars directly are more likely to have wide coverage, and to
be valid for many languages; they also constitute the most economical model of the
human ability to put knowledge of language to use. Therefore, postulating a direct
correspondence between the parser and theories of grammar is, methodologically, the
strongest position, and is usually assumed as a starting point of investigation. How-
ever, experiments with parsers that are tightly related to linguistic principles have
often been a disappointment, largely because these parsers are inefficient.
Inefficiency is a problem that cannot simply be cast aside. Computationally, it
renders the use of linguistic theories impractical, and, empirically, it clashes with the
</bodyText>
<footnote confidence="0.631374">
* Departement de Linguistique Generale, Universite de Geneve, 2 rue de Candolle, 1204 Geneve,
Switzerland
</footnote>
<note confidence="0.8699765">
C) 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.99983">
observation that humans make use of their knowledge of language very effectively.
In this paper, I investigate the computational problem related to the tension between
building linguistically based parsers and building efficient ones, which, I argue, derives
from the particular forms linguistic theories have taken recently. In particular, I explore
the issue of what is a good parsing technique to apply to principle-based theories of
grammar. I take Government-Binding (GB) theory (Chomsky 1986a,b; Rizzi 1990) to
be a suitable illustration of such theories, and also to show in all clarity the problems
that might arise. I differ from other investigations on the import of principle-based
parsing in not drawing on cognitive issues or psycholinguistic results to justify my
assumptions. Indeed, part of the spirit of this work is to explore how far one can go in
advocating principle-based parsing, in the absence of motivations given by cognitive
modelling.
</bodyText>
<subsectionHeader confidence="0.979596">
1.1 The Problem
</subsectionHeader>
<bodyText confidence="0.9834413">
When generative grammatical theory in the &apos;70s talked about &amp;quot;dative shift,&amp;quot; &amp;quot;topi-
calization,&amp;quot; &amp;quot;passive,&amp;quot; it meant that each of these constructions was captured in the
grammar by a specific rule. Consequently, rules were not only construction-specific,
but also language-specific (French, Italian and Spanish, for instance, have no &amp;quot;dative
shift&amp;quot;). The conceptual development of the &apos;80s, in many frameworks, consists in hav-
ing identified the unifying principles of many of these construction-specific rules. For
example, according to GB theory, the same set of principles are at work in the &amp;quot;raising&amp;quot;
construction, (la) and in passive, (lb). The principles are X theory, the Theta Criterion,
and the Case Filter. In both cases, the relation between the underlying position and
the surface string is expressed by chains. Chains consist of the word that undergoes
</bodyText>
<listItem confidence="0.69178275">
movement and all the positions this word occupies in the course of a derivation. In
(1) the chains are (John, t) and (The children, t).
(1) a. John seems [0 t to like Bill
b. The children are loved t by John.
</listItem>
<bodyText confidence="0.999881789473685">
The advantage of this treatment is that common properties of language, here certain
classes of verbs, are expressed by common principles.
This search for generality is not unique to GB theory. Feature-structure formalisms
also use rule schemata to capture similarities among grammar rules. Moreover, reen-
trancy as a notational device to express common features seeks the same type of
representational economy that is expressed by the use of &amp;quot;traces&amp;quot; in GB theory
It is desirable for a syntactic analyser to make use of linguistic theories to obtain,
at least in principle, the same empirical coverage as the theory, and to capture the
same generalizations. Moreover, a parser that makes direct use of a linguistic theory is
more explanatory. A guiding belief for the development of the generative framework
is that a theory that can derive its descriptions from the interaction of a small set of
general principles is more explanatory than a theory in which descriptive adequacy is
obtained by the interaction of a greater number of more particular, specific principles
(Chomsky 1965). This is because the former theory is smaller. Thus, each principle can
generate a set the encoding of which would require a much larger number of bits than
the bits needed to encode the principle itself. The classic example is the use of natural
classes of distinctive features in phonology, in order to compact several rules into one.
A modular theory that encodes universal principles has obtained a greater degree of
succinctness than a nonmodular theory, and is considered more explanatory. Since it
</bodyText>
<page confidence="0.992823">
516
</page>
<note confidence="0.864907">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.9978690625">
is desirable for the parser to maintain the level of explanatory power of the theory, it
must maintain its modularity
It has also been argued (Berwick 1991) that the current shift from rules to a modu-
lar system of principles has computational advantages. Principle-based grammars en-
gender compactness: Given a set of principles, Pi, P2,. , Pn, the principles are stored
separately and their interaction is computed on-line; the multiplicative interaction of
the principles, Pi X P2 X • • • X P, does not need to be stored. Hence, the size of the
grammar is the sum of the sizes of its components: IG I = Pi + P2 ± • • • + P,. Con-
sequently, a parser based on such a grammar is compact, and, theoretically, easier
to debug, maintain and update.&apos; In practice, however, designing and implementing
faithful and efficient parsers is not a simple matter.
Defining &amp;quot;faithfulness&amp;quot; to a linguistic theory is not a trivial task, as a direct relation
between the grammar and the parser is not the only option (see Bresnan 1978; Berwick
and Weinberg 1984; van de Koot 1990, and references therein). In general, it is not
necessary for a parser to implement the principles of the grammar directly. Rather, a
covering grammar could be used, more suited to the purpose of parsing. However, it
is important that such covering be done in such a way that accidental properties of
a particular grammar, which would not hold under counterfactual changes, are not
used. Otherwise, the covering grammar would not be sufficiently general.
A faithful implementation is particularly difficult in the GB framework, as GB
principles are informally expressed as English statements, and can take a variety of
forms. For example, X theory (a condition on graphs), the Case Filter (an output filter
on strings), and the 0 criterion (a bijection relation on predicates and arguments) all
fall under the label of principles. Attempts have been made to formalize GB principles
to a set of axioms (Stabler 1992).
One possible, extreme interpretation of the direct use of principles is an approach
where no grammar compilation is allowed (Abney 1989; Frank 1992; Crocker 1992).2
This approach is appealing because it reflects, intuitively, the idea of using the grammar
as a set of axioms and reduces parsing to a deduction process. This is very much in
the spirit of the current shift in linguistic theories from construction-dependent rules
to general principles, and it separates quite clearly the grammar from the parsing
algorithm.
However, it is not obvious that this approach is efficient. Partial evaluation and
variable substitution can increase performance, but, as usual, a space/time trade-off
will ensue. Excess of partial evaluation off-line increases the size of the grammar,
which might, in turn, slow down the parse. Experimentation with different kinds
of algorithms suggests that some amount of compilation of the principles might be
necessary to alleviate the problem of inefficiency, but that too much compilation slows
down the parser again.
1 Berwick (1982, 403ff.) shows that the size of a cascade of distinct principles (viewed as machines) is the
size of its subparts, while if these same principles are collapsed, the size of the entire system grows
multiplicatively. Modularity corresponds to maximal succinctness when all independent principles are
stated separately. Independent principles are, intuitively, principles that can be computed
independently of each other, and therefore whose interactions are all possible. Barton et al. (1987) and
Berwick (1990) attempt to formalize the concept of independence as separability, assuming that the
topology of a principle-based theory like GB can be mapped onto a planar graph. In fact, if
independent modules are separable modules, there is little reason to think that GB is modular, as it
corresponds to a highly connected graph.
</bodyText>
<footnote confidence="0.577599333333333">
2 By compilation, here and below, I mean off-line computation of some general property of the grammar,
for example the off-line computation of the interaction of principles, using partial evaluation or
variable substitution.
</footnote>
<page confidence="0.97333">
517
</page>
<note confidence="0.7175675">
Computational Linguistics Volume 21, Number 4
1.2 On-line Computation is Inefficient
</note>
<bodyText confidence="0.999971466666667">
Several researchers note that principle-based parsers allowing no grammar precom-
pilation are inefficient. Firstly, Johnson (1989), Stabler (1990), and van de Koot (1991)
note that the computation of a multi-level theory without any precompilation might
not even terminate. Secondly, experimental results show that an entirely deductive
approach is inefficient. Kashket (1991) discusses a principle-based parser, where no
grammar precompilation is performed, and which parses English and Warlpiri us-
ing a parameterized theory of grammar. The parsing algorithm is a generate-and-test,
backtracking regime. Kashket (1991) reports, for instance, that a 5-word sentence in
Warlpiri (which can have 5! analyses, given the free word order of the language) can
take up to 40 minutes to parse. He concludes that, although no mathematical analysis
for the algorithm is available, the complexity appears to increase exponentially with
the input size.
Fong (1991, 123) discusses a parsing algorithm. He shows that an initial version of
the parser, where the phrase structure rules were expressed as a DCG and interpreted
on-line, spent 80% of the total parsing time building structure. In a later version,
where rules were compiled into an LR(1) table, structure-building constituted 20% of
the total parsing time. This same parser includes a module for the computation of
long distance dependencies, which works by generate-and-test. Fong finds that this
parsing approach is also inefficient.
Dorr (1987) notices similar effects in a parser that uses an algorithm more parallel
in spirit (Earley 1970). Dorr notes that a limited amount of precompilation of the prin-
ciples speeds up the parse, otherwise too many incorrect alternatives are carried along
before being eliminated. For example, in her design, X theory and the other principles
are coroutined. She finds that precompiling the principles that license empty categories
with the phrase structure rules reduces considerably the number of structures that are
submitted to the filtering action of the other principles, and thus speeds up the parse.
In all these cases, the source of inefficiency stems from the principle-based design.
Because each principle is formulated to be as general as possible, the &amp;quot;logical&amp;quot; abstrac-
tion of each principle from the others causes a lot of overgeneration of structure and,
consequently, a very large search space.
</bodyText>
<subsectionHeader confidence="0.9966">
1.3 Too Much Precompilation is Inefficient
</subsectionHeader>
<bodyText confidence="0.999864888888889">
Simple precompilation is not a solution to the inefficiency of principle-based parsing,
however. Experimentation with different amounts of precompilation shows that off-
line precompilation speeds up parsing only up to a certain point, and that too much
precompilation slows down the parser again.
The logic of why this happens is clear. The complexity of a parsing algorithm is
a composite function of the length of the input and the size of the grammar. For the
kind of input lengths that are relevant for natural language, the size of the grammar
easily becomes the predominant factor. If principles are precompiled in the form of
grammar rules, the size of the grammar increases.
As Tomita (1986) points out, input length does not cause a noticeable increase in
running time up to 35 to 40 input tokens. For sentences of this length, grammar size be-
comes a relevant factor for grammars that contain more than approximately 220 rules,
in his algorithm (an LR parser with parallel stacks). Both Dorr (1987) and Tomita (1986)
show experimental results confirming that there is a critical point beyond which the
parser is slowed down by the increasing size of the grammar. In the Generalized Phrase
Structure Grammar (GPSG) formalism (Gazdar et al. 1985), similar experiments have
been performed, which confirm this result. Parsers for GPSG are particularly interest-
ing, because they use a formalism that expresses many grammatical generalizations in
</bodyText>
<page confidence="0.991344">
518
</page>
<note confidence="0.392946">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.999732">
a uniform format. Therefore, GPSG is, in principle, more amenable to being processed
by known parsing techniques. Thompson (1982) finds that expanding metarules, rather
than computing them on-line, is advantageous, but that instantiating the variables in
the expanded rules is not. Phillips and Thompson (1985) also remark that compiling
out a grammar of twenty-nine phrase-structure rules and four metarules is equivalent
to &amp;quot;several tens of millions of context-free rules.&amp;quot; Phillips (1992) proposes a modifica-
tion to GPSG that makes it easier to parse, by using propagation rules, but still notes
that variables should not be expanded.
In conclusion, the lesson from experimentation is that parsing done totally on-line
is inefficient, but that compilation is not always a solution. A parser that uses linguistic
principles directly must fulfill apparently contradictory demands: for the parser to be
linguistically valid it must use the grammar directly, while a limited amount of off-line
precompilation might make the parser more efficient.&apos; In the next section, I propose
and discuss a solution to this problem that builds on other approaches and relates the
parser to the grammar in a principled way.
</bodyText>
<sectionHeader confidence="0.989795" genericHeader="method">
2. The Proposal
</sectionHeader>
<bodyText confidence="0.93687915">
Two avenues have generally been pursued to build efficient GB parsers. In one case, a
&amp;quot;covering grammar&amp;quot; is compiled, which overgenerates and is then filtered by con-
straints. The compilation is done in such a way that the overgeneration is well-
behaved. For instance, the correct distribution of empty categories is calculated off-line
(Dorr 1993). In the other case, all the principles are applied on line, but they apply
only to a portion of the tree, and are therefore restricted to a local computation (Frank
1992).4 My proposal combines these two approaches: it adopts the idea of compiling
the grammar, at least partially, off-line but it attempts to find a principled way of
doing so. In this, I differ from Dorr, where the amount of compilation is heuristic
and based on practical experimentation. The approach shares Frank&apos;s intuition that
linguistic principles have a form, which can be exploited in structuring the parser.
This proposal is based on two observations. First, each principle of linguistic theory
has a canonical form, and second, primitives of linguistic theories can be partitioned
into classes, based on their content.
As an illustration of the first observation, we can look at the principle that regulates
the distribution of the empty categories in the phrase marker, the Empty Category
Principle (ECP), as stated below (adapted from Rizzi 1990, 25).
(2) The Empty Category Principle
An empty category x is licensed if the 3 following conditions are
satisfied:
</bodyText>
<listItem confidence="0.853408333333333">
1. x is in the domain of a head H
3 For CF parsers, just how much compilation speeds up the parser is defined precisely by the analysis of
the algorithm. No such precise analysis is available for principle-based algorithms.
</listItem>
<tableCaption confidence="0.445548777777778">
4 Frank (1992) presents a parsing model that is claimed not to allow any compilation of the linguistic
theory, and to operate in linear time. Two objections can be raised to these claims: first, the use of TAG
elementary trees to restrict the working space of the parser amounts to a precompilation of
phrase-structure and locality constraints, so that locality is not computed in the course of the parse, but
basically done as template matching. Second, in the measure of complexity, Frank does not count the
cost of choosing which elementary tree to unadjoin or unsubstitute, or the cost of backtracking if the
wrong decision is made. There are indeed cases where, in order to perform the correct operation, more
than one elementary tree must be spanned. It is not clear that linear time complexity can actually be
claimed if all factors are taken into account. For a more detailed discussion, see Merlo 1992, to appear.
</tableCaption>
<page confidence="0.991881">
519
</page>
<bodyText confidence="0.105257">
Computational Linguistics Volume 21, Number 4
</bodyText>
<listItem confidence="0.992699">
2. the category of H E {A, Agr, N, P. T, V}
3. there is no barrier or head H&apos; that intervenes between H and x
</listItem>
<bodyText confidence="0.999912935483871">
It can be observed that this principle has an internal structure and can be decom-
posed into separate pieces of information: (2.1) imposes a condition on configurations,
namely, a condition on the shape of the tree; (2.2) imposes a condition on the labelling
of the nodes in the tree; and (2.3) imposes a locality condition, as it defines the subtree
available for the computation of the principle. These three conditions are independent.
For instance, the configuration does not depend on the categorial labelling of the head
node. The precompilation of these conditions would require computing all the possible
combinations, without any reduction of the space of analysis.
The second observation is based on a detailed inspection of the form of the prin-
ciples of the grammar. What is presented in (2) as an illustrative example is, in fact, a
consistent form of organization of the principles. If one looks at several of the princi-
ples of the grammar that are involved in building structure and annotating the phrase
marker, one finds the same internal organization.
Theta-assignment occurs in the configuration of sisterhood, it requires a 0-assigning
head, and it must occur between a node and its most local assigner. Assignment of
Case occurs in a given configuration (according to Chomsky (1988, 1992) it is always
a specifier-head configuration), given a certain lexical property of the head a-ND, and
locally, within the same maximal projection). The same restriction occurs again for
what is called the wh-criterion (Rizzi 1991), which regulates wh-movement, where the
head must have a +wh feature and occur within a specifier-head configuration. Cat-
egorial selection and functional selection also occur under the same restrictions, in
the complement configuration (i.e., between a head and a maximal projection). The
licensing of subjects in the phrase marker, done by predication, must occur in the
specifier-head configuration. The licensing of the empty category pro also requires the
inflectional head of the sentence to bear the feature Strong Agr, and it occurs in the
specifier-head configuration. The assignment of the feature [± barrier] depends on
L-marking, which in turn requires that the head is lexical, and that marking occurs in
the complement configuration.
Thus, each different &amp;quot;factor&amp;quot; that composes a principle can be considered a sepa-
rate primitive, and such primitives can be grouped into classes defined according to
their content. Linguistic information can be classified into five different classes:
</bodyText>
<listItem confidence="0.9985856">
(3) a. Configurations: sisterhood, c-command, m-command, + maximal
b. Lexical features: ±N, ±V, ±Funct, ±c-selected
c. Syntactic features: +Case, ±9, ±1,, +barrier, +Strong Agr
d. Locality information: minimality, antecedent government
e. Referential information: +anaphor, +pronominal, indices
</listItem>
<bodyText confidence="0.976724">
This qualitative classification forms a partitioning into natural classes based on
information content. I call these IC Classes.&apos;
</bodyText>
<footnote confidence="0.7031765">
5 Differently from Crocker (1992, to appear) and Frazier (1985), this partitioning does not rely on the
particular representation used. The spirit of the hypothesis is that linguistic theory is formed by
heterogeneous types of information, and that the representation used to describe them is a derived
concept. Frazier (1990) proposes an evolutionary partitioning of the parser based on tasks. This
</footnote>
<page confidence="0.989645">
520
</page>
<note confidence="0.392985">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.997464">
It can then be hypothesized that the amount of compilation (or, conversely, the
modularity of the parser) is captured by the notion of IC classes as follows:
</bodyText>
<subsectionHeader confidence="0.749055">
IC Modularity Hypothesis (ICMH)
</subsectionHeader>
<bodyText confidence="0.984689111111111">
Precompilation within IC Classes improves efficiency.
Precompilation across IC Classes does not.
In other words, a parser that takes advantage of the structure of linguistic principles
will maintain a modular design based on the five classes in (3).
Although the ICMH is not so stringent as to make predictions that converge on
a single parsing architecture, it does provide some predictive power about the orga-
nization of the parser. First, structural information is encoded separately from lexical
information. Standard context-free rules, specified with category, such as VP —AT NP,
are not compatible with the ICMH, nor are proposals in the spirit of licensing gram-
mars (Abney 1989, Frank 1992), where information is encoded in each lexical item.
Second, the ICMH predicts that long-distance dependencies, represented as chains,
are computed in steps. Empty categories are licensed in two computational steps:
structural licensing by an appropriate head, and feature instantiation. With respect to
feature instantiation in particular, it is predicted that precompiling syntactic features
speeds up the parsing process. This is different from functional approaches such as
Fong (1991), and Fong and Berwick (1992), in which there is no precompilation.6
These predictions seem to be supported (and, consequently, so is the ICMH) by
two main results, which are illustrated below:
</bodyText>
<listItem confidence="0.953528666666667">
1. separating X from lexical information yields more compact data
structures; I propose a parser that uses two compiled tables: one that
encodes structural information, and the other that encodes lexical
information.
2. using syntactic features to compute empty categories reduces the search
space, complex chains can be computed efficiently.
</listItem>
<bodyText confidence="0.700725928571429">
These claims are supported in the next section, where I discuss the properties of an
implemented parser, which computes simple, complex, and multiple chain formation,
as exemplified in Figure 1. This subset of constructions has been chosen because it con-
stitutes the crucial test set for principle-based parsers: it involves complex interactions
of principles over large portions of the tree/
perspective is not in opposition to the current proposal, as the specialization of the parser in different
tasks is likely to be an adaptive reaction to the different types of inputs.
At first sight it might appear that the notion of types proposed by Fong (1991) is similar to IC
classes. In fact, the similarity is superficial. Clearly, both notions constitute an attempt to partition the
set of principles into smaller subsets. However, Fong&apos;s types are a mechanism to interleave constraints
and phrase structure rules automatically. They are a method to schedule the on-line computation of
principles that are the direct translation of the theory, and not a way of defining the design of the
parser. In Fong&apos;s view, all computations are done on-line and the parser reflects the theory as directly
as possible.
</bodyText>
<footnote confidence="0.994828">
6 It is difficult to separate precisely &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot;
those features that are used to determine the well-formedness of syntactic trees. In the spirit of more
recent developments in syntactic theory, I consider syntactic those features that are involved in some
particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles,
case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and
Laenzlinger (1993), also wh, neg, adverb.
7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for
</footnote>
<page confidence="0.989443">
521
</page>
<figure confidence="0.788183">
Computational Linguistics Volume 21, Number 4
</figure>
<figureCaption confidence="0.983356">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.90183">
Types of Sentences.
</subsectionHeader>
<bodyText confidence="0.999988571428572">
In the rest of the paper, I first discuss the advantages of storing X- information
separately from lexical information (section 3). I then turn to the computation of long-
distance dependencies. I illustrate two algorithms to compute chains: I show that a
particular use of syntactic feature information speeds up the parse, and I discuss the
plausibility of using algorithms that require strict left-to-right annotation of the nodes
(section 4). In fact, the algorithm I propose appears to be interestingly correlated to a
gap in the typology of natural languages.
</bodyText>
<sectionHeader confidence="0.658847" genericHeader="method">
3. The Computation of Phrase Structure
</sectionHeader>
<bodyText confidence="0.999907961538462">
In order to explore the validity of the proposed hypothesis about the modularity of
the parser, an analyzer for English was developed. Each of the data structures is the
direct implementation of linguistic objects with different information contents. The
input to the algorithm is an unannotated sentence. The output consists of a tree and a
list of two chains: the list of A chains and the list of A chains, that is, chains formed
by wh-movement and NP movement, respectively. The main parsing algorithm is a
modified LR parsing algorithm augmented by multi-action entries and constraints on
reduction.&apos;
The structure-building component of the parser is driven by an LR(k) parser
(Knuth 1965) which consults two tables. One table encodes X information (following
Kornai and Pullum 1990). The other table encodes lexical information. Lexical infor-
mation is consulted only if it is needed to disambiguate a state containing multiple
actions in the LR parser. An overview of this design is shown in Figure 2.
instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992). In formalism
other than GB theory, gaps are encoded directly into the rules. Both GPSG and HPSG use slash features
to percolate features to gaps. The use of slash features probably simplifies the computation. There has
been a debate on the explanatory adequacy of grammars that employ slash features (see van de Koot
1990, and Stabler 1994). For my purposes, note that, if anything, I am dealing with the worst case for
the parser.
8 The ICMH is not sufficient to predict a specific parsing architecture, but rather it loosely dictates the
organization of the parser. The choice of an LR parser then is the result of the ICMH (with which the
parser&apos;s organization must be compatible) and additional independent factors. First, LR parsers have
the valid prefix property, namely they recognize that a string is not in the language as soon as possible
(other parsing methods have this property as well, for instance Schabes 1991). A parser with this
property is incremental, in the sense that it does not perform unnecessary work, and it fails as soon as
an error occurs. Second, the stack of an LR parser encodes the notion of c-command implicitly. This is
</bodyText>
<figure confidence="0.975167365853658">
TYPE EXAMPLE
1 Simple Transitive
2 Simple Intransitive
3 Simple Passive
4 Simple Raising
5 Embedded Transitive
6 Embedded Intransitive
7 Embedded Raising
8 Simple Question
9 Embedded Question
10 Embedded Question and Raising
11 Embedded Wh-Question
john loves mary
john runs
mary was loved
mary seems to like john
john thinks that mary loves bill
john thinks that mary runs
mary thinks that john seems to like bill
who does john love ?
who do you think that john likes ?
who did you think that john seemed to like?
* who did you wonder why mary liked?
522
Modularity and Information Content Classes
Paola Merlo
Input Chains
Stack
LR Parsing
Program
constr
Co-occurrence Table
LR Table
11111RWAIIM Erd11111111M
11111111116Mil IIIIMMIEUIN
iimmum
mmusim !Mall
moiNNEEE-a MITIMAME 111.
MIME= MEM=
mialmommummornim
MMENIMMIIIIIIIIIMMIN
</figure>
<figureCaption confidence="0.926457">
Figure 2
</figureCaption>
<construct confidence="0.354637333333333">
Organization of the Parser: The data structures (tables, stack and chains) are represented as
rectangles. Operations on feature annotation are performed by constraints, represented as
ovals.
X&amp;quot; —&gt; Y&amp;quot; X&apos; specification
X&apos; —4 X Y&amp;quot; complementation
X&apos; —&gt; Y&amp;quot; X&apos; modification
—x, Y&amp;quot;
X&apos; —&gt; X unary head
X&amp;quot; —&gt; X&apos; unary Xmax
X —4 empty empty heads
X&amp;quot; —&gt; empty empty Xmaxn
Figure 3
</construct>
<subsectionHeader confidence="0.80728">
Category-Neutral Grammar.
</subsectionHeader>
<bodyText confidence="0.9848613">
The context-free grammar compiled in the LR table is shown in Figure 3. The
crucial feature of this grammar is that nonterminals specify only the X projection
level, and not the category. Because the LR table is underspecified with respect to the
categorial labels of the input, many instances of LR conflicts arise, which can be teased
apart by looking at the co-occurrence restrictions on categories. This information would
be stored in the rules themselves in ordinary context-free rules. However, ordinary
context-free rules do not encode many other types of lexical information also used in
parsing. Thus, they lose generality, without exploiting all the available information.
As an illustration, consider the following set of context-free rules.
crucial for fast computation of chains. Third, LR parsers are fast.
</bodyText>
<page confidence="0.984537">
523
</page>
<note confidence="0.274958">
Computational Linguistics Volume 21, Number 4
</note>
<listItem confidence="0.9886104">
(4) 1. C&apos;
2. I&apos;
3. V&apos;
4. V&apos;
5. V&apos;
Co IP
—+ I0 VP
- Vo NP
- Vo e
Vo
</listItem>
<bodyText confidence="0.998996045454546">
Rules 1-4 have the same )( structure, but they differ in the labels of the nodes. In
rules 1 and 2 the heads, Co and Io respectively, are followed by IP and VP obligatorily.
Rules 4 and 5 cover the same string. Clearly, by writing 1-4 as different rules, the fact
that they are instances of the same structure is not captured. Similarly, the obligatori-
ness of IP and VP as complements of Co and Io is lost. Finally, the choice of rule 4 or
rule 5 depends on the actual verb in the string. If the verb is intransitive, rule 4 cannot
apply.
In the parser, structural information is separate from information about co-occur-
rence (rules 1-4), functional selection (rules 1, 2) and subcategorization (rules 4, 5).
This information is stored in a table, called a co-occurrence table. The table stores
information about obligatory complementation, such as the fact that Io must be fol-
lowed by a VP. It also stores compatible continuations based on subcategorization.
For instance, consider the case in which the current token is an intransitive verb. The
LR table contains two actions that match the input: one action generates a projection
of the input node (V&apos;), without branching, while the other action creates an empty
object NP. By consulting the subcategorization information, the parser can eliminate
the second option as incorrect.
Using an LR table together with a co-occurrence table is equivalent in coverage to a
fully instantiated LR table, but it is more advantageous in other respects. Conceptually,
the latter organization encodes X theory directly, and it maintains a general design,
which makes it applicable to several languages. Practically, there is reason to think
that it is more efficient.
</bodyText>
<subsectionHeader confidence="0.999977">
3.1 Testing the ICMH for phrase structure
</subsectionHeader>
<bodyText confidence="0.999823076923077">
The prediction made by the ICMH is that compiling together 3---( theory and categorial
information will increase the size of the grammar without reducing the nondetermin-
ism contained in the grammar, because category/subcategory information belongs to
a different IC Class than structural (i.e., X) information.
Method and Materials. The size of the grammar is measured as the number of rules or
number of states in the LR table. The amount of nondeterminism is measured as the
average number of conflicts (the ratio between the number of actions and the number
of entries in a table.)9
Three grammars were constructed, constituting (pairwise) as close an approxima-
tion as possible to minimal pairs (with respect to IC Classes). They are shown in the
Appendix. Grammar 1 differs minimally from Grammar 2, because each head is in-
stantiated by category The symbol YP stands for any maximal projection admitted
by linguistic theory. Grammar 3 differs minimally from Grammar 2, because it also
</bodyText>
<footnote confidence="0.890425666666667">
9 The average number of conflicts in the table gives a rough measure of the amount of nondeterminism
the parser has to face at each step. However, it is only an approximate measure for at least two
reasons: taking the mean of the conflicts abstracts away from the size of the grammar, which might be
a factor, as the search in the table becomes more burdensome for larger tables (but, if anything, it plays
against small grammars/tables); moreover, it does not take into account the fact that some states might
be visited more often than others.
</footnote>
<page confidence="0.99255">
524
</page>
<note confidence="0.602501">
Paola Merlo Modularity and Information Content Classes
</note>
<tableCaption confidence="0.998128">
Table 1
</tableCaption>
<table confidence="0.944649666666667">
Comparison of the 3 grammars (compiled into LR tables)
NB OF NB OF NB OF AVERAGE
ENTRIES ACTIONS RULES CONFLICTS
GRAMMAR 1 63 123 16 1.95
GRAMMAR 2 793 1319 51 1.78
GRAMMAR 3 251 962 41 3.83
</table>
<tableCaption confidence="0.995292">
Table 2
</tableCaption>
<table confidence="0.854086">
Number of actions in the 3 LR tables
NUMBER OF ACTIONS
ENTRIES 1 2 3 4 5 6 7 8 9 10 11 12 13 14
GRAMMAR 1 38 6 8 8 1 2
GRAMMAR 2 465 68 168 6 42
GRAMMAR 3 144 43 3 8 5 4 30 14
</table>
<bodyText confidence="0.999482103448276">
includes some subcategorization information (such as transitive, intransitive, raising),
and some co-occurrence restrictions and functional selection. Moreover, empty cate-
gories are &amp;quot;moved up,&amp;quot; so that they are encountered as high in the tree as possible.
These three grammars are then compiled by the same program (BisoN) into three
(LA)LR tables. The results are shown in Table 1, which compares some of the indices
of the nondeterminism in a given grammar to its size, and Table 2, which shows the
distribution of actions in each of the grammars.
Discussion. Consider Grammar 1 and Grammar 2 in Table 1. Grammar 2 has a slightly
smaller average of conflicts, while it has three times the number of rules and twelve
times the number of entries, compared to Grammar 1. The fact that Grammar 2 is
larger than Grammar 1, with only a slightly smaller average of conflicts, confirms the
prediction made by the ICMH that compiling X theory with categorical information
will increase the size of the grammar without decreasing nondeterminism. Since the
number of rules is expanded, but no &amp;quot;filtering&amp;quot; constraint is incorporated in Grammar
2 with respect to Grammar 1, this result might not seem surprising.
However, the ICMH is also confirmed by the other pairwise comparisons and by
the global results. Grammar 3 has a higher number of average conflicts than Grammar
2, but it is smaller, both by rules and LR entries, so it is more compact. Notice that
adding information (subcategory, selection, etc.) has a filtering effect, and the resulting
grammar is smaller. However, adding information does not reduce nondeterminism.
Compared to Grammar 1, Grammar 3 does not show any improvement on either
dimension: Grammar 3 is both larger (four times as many LR entries) and more non-
deterministic than Grammar 1. Globally, one can observe that an increase in grammar
size, either as a number of rules or number of LR entries, does not correspond to a
parallel decrease in nondeterminism.
As Table 2 shows, the distribution of the conflicts in Grammar 3 presents some
gaps. This occurs because certain groups of actions go together. Two main patterns of
conflict are observed: In those states that have the highest number of conflicts, all rules
that cover the empty string can apply; in those states that have an intermediate number
</bodyText>
<page confidence="0.994746">
525
</page>
<note confidence="0.433123">
Computational Linguistics Volume 21, Number 4
</note>
<tableCaption confidence="0.987887">
Table 3
</tableCaption>
<table confidence="0.957948">
Comparison of the 3 grammars (compiled into LL tables)
NB OF NB OF NB OF AVERAGE
ENTRIES ACTIONS RULES CONFLICTS
GRAMMAR 1 19 62 16 3.26
GRAMMAR 1&apos; 19 46 13 2.42
GRAMMAR 2 112 255 51 2.28
GRAMMAR 3 144 368 41 2.62
</table>
<bodyText confidence="0.9866045">
of conflicts, only some rules can apply, namely, those that have a certain T( projection
level, and that cover the empty string (e.g., all XP&apos;s, independent of category, that
cover the empty string). This observation confirms that categorial information does
not reduce nondeterminism. On the contrary, adding categorical information multi-
plies nondeterminism by adding structural configurations. Even introducing &amp;quot;filter-
ing&amp;quot; lexical information (co-occurrence restrictions and functional complementation)
does not appear to help. In fact, ambiguities caused by empty categories occur accord-
ing to structural partitions. The qualitative observation supports the numerical results:
Introducing categorial information is not advantageous, because it increases the size
of the grammar without decreasing significantly the average number of conflicts.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Extending the test to other compilation techniques
</subsectionHeader>
<bodyText confidence="0.99869112">
The effects discussed above could be an artifact of the compilation technique. In order
to check that this is not the case, the same three grammars (reported in the appendix)
were compiled into LL and Left Corner (LC) tables.
LL compilation: Discussion. The LL compilation method yields results similar to those
of the LR compilation, although less clear cut. This confirms the intuition that the
results reflect some structural property of the grammar, and are not an artifact of the
LR compilation.
The results of the compilation of the same grammars into LL tables are shown in
Table 3. Grammar 1&apos; is a modified version of Grammar 1, without adjunction rules.
These figures show that there is no relation between the increased specialization of
the grammar and the decrease of nondeterminism. Note that the LL compilation does
not maintain the paired rankings of actions and rules. So, for the LL table, the co-
occurrence of lexical categories does not play a filtering role.
Globally, there appears to be an inverse relation between the size of the grammar,
measured by the number of rules, and the average number of conflicts: the larger
the grammar the smaller the number of conflicts. This might make one think that
there is some sort of relation between grammar size and nondeterminism after all.
However, this is not true if we use the number of entries as the relevant measure of
size. Moreover, if one looks at Grammar 1&apos;, which is smaller than Grammar 1, one can
see that the average number of conflicts decreases quite a bit. This confirms a weaker
hypothesis, which is nonetheless related to the initial one, namely that nondeterminism
does not vary in an inverse function to &amp;quot;content of information.&amp;quot;
Some qualitative observations might help clarify the sources of ambiguity in the
tables. In all three grammars, the same ambiguities are repeated, for each terminal
item. In other words, all columns of the LL table are identical (with the exception
</bodyText>
<page confidence="0.992373">
526
</page>
<note confidence="0.605653">
Paola Merlo Modularity and Information Content Classes
</note>
<tableCaption confidence="0.997418">
Table 4
</tableCaption>
<table confidence="0.981907833333333">
Comparison of the 3 grammars (compiled into LC tables)
NB OF NB OF NB OF AVERAGE
ENTRIES ACTIONS RULES CONFLICTS
GRAMMAR 1 49 136 16 2.77
GRAMMAR 2 1456 4030 51 2.76
GRAMMAR 3 398 610 41 1.53
</table>
<tableCaption confidence="0.994179">
Table 5
</tableCaption>
<table confidence="0.944932833333333">
Number of actions in the 3 compiled LC tables
NUMBER OF ACTIONS
ENTRIES 1 2 3 4 5 6 7 18 19
GRAMMAR 1 4 18 15 9 3
GRAMMAR 2 602 702 48 96 8
GRAMMAR 3 282 92 4 4 4 12
</table>
<bodyText confidence="0.998776083333333">
of cell [X0, wp] in Grammar 1.) This suggests that lexical tokens do not provide any
selective information. Moreover, as we saw in the LR tables, projections to the same
level have the same pattern of conflicts. (In Grammar 2, the number of conflicts is
multiplied by the number of categories.)10
LC-compilation: Discussion. The same three grammars were compiled in left corner (LC)
tables. The result of the compilation are shown in Table 4, and the distribution of the
conflicts is shown in Table 5. As can be seen from Table 4, Grammar 2 is three times
larger than Grammar 1 and is compiled in a table that has twenty-nine times as many
entries, but the average number of conflicts is not significantly smaller.
The interpretation of the LC table derived from Grammar 3 poses a problem for
the ICMH. Grammar 3 is larger than Grammar 1, as it contains category and some
co-occurrence information, but its average of conflicts is smaller. In this case, it seems
that adding information reduces nondeterminism. On the other hand, compared to
Grammar 2, both the table and the average number of conflicts are smaller. I take this to
mean that the ICMH is confirmed only by a global assessment of the relation between
the content of information and the average conflicts, but not by pairwise comparisons
of the grammars. Notice however, that the difference in the two pairwise comparisons
confirms that simple categorial information does not perform a filtering action on the
structure, while lexical co-occurrence does. This is precisely what I propose to compile
in the lexical co-occurrence table.
The qualitative inspection of the tables confirms the clustering of conflicts sug-
gested by Table 5. Grammar 1 and Grammar 2 show the same patterns of conflicts as
the LR and LL tables: conflicting actions cluster with the bar level of the category. So,
for example, in Grammar 2, one finds that when the left corner is a maximal projection
</bodyText>
<footnote confidence="0.73292625">
10 In all cases, this is caused by the X form of the grammar. Namely, the loci of recursion and gapping are
at both sides of the head, and anything can occur there. Eliminating this property would be incorrect,
as it would amount to eliminating one of the crucial principles of GB, namely move-a, which says that
any maximal projection or head can be gapped.
</footnote>
<page confidence="0.958384">
527
</page>
<note confidence="0.643227">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999853909090909">
the action is unique, while when the corner is a bar level projection there are multiple
actions and they are the same, independently of the input token. In Grammar 3, the
same patterns of actions are repeated for each left corner, independently of the goal
or of the input token.
The qualitative inspection of the compiled tables is coherent across compilation
methods and appears, in general, to support the ICMH, as the interaction of structural
and lexical information is the cause of repeated patterns of conflicts. Quantitatively, the
results, which are very suggestive in the LR compilation, are less clear in the other two
methods. However, in no case do they clearly disconfirm the hypothesis. I conclude
that categorial information should be factored out of the compiled table and separate
data structures should be used.&apos;
</bodyText>
<sectionHeader confidence="0.979182" genericHeader="method">
4. The Computation of Chains
</sectionHeader>
<bodyText confidence="0.998040666666667">
As a result of using a category-neutral context-free backbone to parse, most of the
feature annotation is performed by conditions on rule reduction associated with each
context-free rule, which are shown in Figure 4.
The most interesting issues arise in the treatment of filler-gap dependencies, which
are represented as chains. Informally, a chain is a syntactic object that defines an
equivalence class of positions for the purpose of feature assignment and interpretation.
</bodyText>
<listItem confidence="0.97569475">
(5) a. Mary, was loved ti
b. Whoi did John love t1?
c. Maryi seemed t; to have been loved ti .
d. Who, did John think t; that Mary loved ti ?
</listItem>
<bodyText confidence="0.997239961538461">
The sentence in (5a), for example, contains the chain (Mary,, t,), which encodes the fact
that Mary is the object of love, represented by the empty category t.
In this parser, empty categories are postulated by the LR parser, when building
structure, and their licensing is immediately checked by the appropriate condition on
rule reductions, shown in Figure 4.
Many principles regulate the distribution of chains. For the purpose of the follow-
ing discussion, it is only necessary to recall that a chain can only contain one thematic
position and one position that receives case. Moreover, chains divide into two types:
11 It should be noted that, although phrase-structure rules are reduced to the bare bones, they cannot be
eliminated altogether. Parsers that project phrase structure and attachments entirely from the lexicon
have been presented by Abney (1989) and Frank (1992), using licensing grammars (LS). They suffer
from serious shortcomings when faced with ambiguous input, as they do not have enough global
knowledge of the possible structures in the language to recover from erroneous parses. Abney
alleviates this problem by attaching LR states to the constructed nodes, thus losing much of the initial
motivation of the licensing approach. Frank&apos;s parser is augmented by a parse stack to parse head-final
languages. Frank does not discuss this issue in detail, but it seems that a &amp;quot;shift&amp;quot; operation must be
added to the operations of the parser. As there could always be a licensing head in the right context,
which would license a left-branching structure, the &amp;quot;shift&amp;quot; operation is always correct. But then, the
parser might reach the end of the input (or at least the end of the relevant elementary tree, i.e., the
main predicate-argument structure) before realizing either that it pursued an incorrect analysis, in the
case of ambiguous input, or that the input is ill-formed. Thus, this augmented parser could not
recognize errors as soon as they are encountered. Finally, note that all the augmentation necessary to
make the LS grammar work make it equivalent to a phrase-structure grammar, possibly with the
disadvantage of being procedurally instead of declaratively encoded. On the other hand, a precompiled
table which keeps track of all the alternative configurations guarantees that incorrect parses are
detected as soon as possible, and, if alternative parses exist, they will be found.
</bodyText>
<page confidence="0.980677">
528
</page>
<table confidence="0.988535869565217">
Paola Merlo Modularity and Information Content Classes
CONSTRAINT FUNCTION
0-criterion checks if all chains in the chain list have
received a 0-role
Case filter checks if all chains in the chain list have
Case
node labelling determines what kind of chain link the cur-
rent node is: head, intermediate, foot
chain selection selects chain to unify with current node
chain unification unifies node with selected chain
head feature percolation consults cooccurrence table and
determines cooccurrence restrictions
among heads
0-marked marks node with available 0-role
case marked marks node with available Case
c-select categorial selection
is-a barrier checks if maximal projection is a barrier
license empty head checks features of closest lexical head
licensing head finds a lexical head to license a maximal
projection
locality checks that the maximal projections be-
tween antecedent and empty category are
not barriers
</table>
<figureCaption confidence="0.7097885">
Figure 4
The Constraints.
</figureCaption>
<bodyText confidence="0.999178333333333">
wh-chains, also called A-chains, and NP-movement chains, also called A-chains; the
empty categories that occur in these chains have different properties. More than one
chain can occur in a sentence. Multiple chains occurring in the same sentence can
either be disjoint or intersected.&apos; Disjoint chains are nested, as in (6a). If chains in-
tersect, they share the same index and they have exactly one element in common, as
in (6b).
</bodyText>
<listItem confidence="0.9577775">
(6) a. Who1 did Mary i seem ti to like ti?
b. Whoi did you think ti seemed ti to like Mary?
</listItem>
<subsectionHeader confidence="0.984984">
4.1 The Algorithms
</subsectionHeader>
<bodyText confidence="0.99988">
When building chains, several problems must be solved. First of all, the parser must
decide whether to start a new chain or not. It must also decide whether to start a
chain headed by an element in an argument position (A-chain), such as the head of a
passive chain, or a chain headed by an element in a non-argument position (A-chain),
</bodyText>
<footnote confidence="0.954133">
12 Actually, chains can also compose. If chains compose they do not have intersecting elements, but they
create a new link. This type of chain is exemplified in (i). We will only discuss chains of the types of (6).
(i) Who, did you meet ti 0, without greeting ti?
</footnote>
<page confidence="0.982731">
529
</page>
<note confidence="0.654992">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999801875">
such as the head of a wh-chain. Second, on positing an empty element, the parser must
decide to which chain it belongs.&apos;
The two decisions can be seen as instances of the same problem, which consists
in identifying the type of link in the chain that a given input node can form (whether
head, intermediate or foot, abbreviated as H,I,F in what follows.) One can describe this
sequence of decisions as two problems that must be solved in order to form chains: the
Node Labelling Problem (NLAB), and the Chain Selection Problem (CSEL), formulated
below.
</bodyText>
<subsectionHeader confidence="0.976567">
The Node Labelling Problem (NLAB).
</subsectionHeader>
<bodyText confidence="0.9384555">
Given a node N to be inserted in a chain, determine its label L, where
L E {AH, AH, AT, AT, AF, AFT
This problem defines a relation R: N x L, where N belongs to the set of nodes, and
L belongs to the set of labels for the elements of chains. The labels of possible chain
links reflect the theoretical distinctions between A-movement and A-movement, and
the fact that links of a chain can be either the first element of the chain, the head (H),
or an intermediate element (I) in the case of chains formed by several links, or the last
element, the foot (F).14
</bodyText>
<figure confidence="0.861028538461538">
Algorithm 1
Input: Node, Local Configuration
Output: List of Labels
If Node is not empty then
If Node is [+wh] then Label &lt;— AH
else Label 4-- AH
else
If Node has 0-role then
If Node has Case then Label &lt;— AF
else Label &lt;— AF
else
If Local Configuration = Spec of C then Label &lt;— AT
else Label &lt;— AT
</figure>
<bodyText confidence="0.879561866666667">
There are six possible outputs for this algorithm. The first case arises when the
node N is a lexical wh-word, which starts a wh-chain. The second possibility is if the
head is lexical, but not a question word. In this case, an argument chain (A-chain)
is started, as in passives. The last four cases deal with empty categories. The feature
13 Strictly speaking, it must also provide a rescuing procedure. This can be done by checking whether all
the chains satisfy the well-formedness. conditions. If not all the chains satisfy the well-formedness
constraints, the parser can attempt to intersect or compose two or more chains in order to satisfy the
well-formedness conditions. These two problems are not treated here. For an illustration, under the
name of Chain Intersection Problem and Chain Composition Problem, see Merlo 1992.
14 I present here a simplified version of the algorithm, to avoid technical linguistic details, which are not
relevant for the following discussion. However, one should also output a label A0p, which designates
the empty operator that binds, for instance, the empty variable in a parasitic gap construction and
other cases of non-overt movement, such as relative clauses. In the man OP I saw an empty operator is
postulated by analogy to the man whom/that I saw. A0p is licensed by the same conditions that license
an intermediate A trace.
</bodyText>
<page confidence="0.986561">
530
</page>
<note confidence="0.659438">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.999921333333333">
annotation of the category is inspected: case distinguishes the foot of an A-chain from
the foot of an A-chain, while intermediate traces are characterized by a lack of 0-role
and by their configurations (i.e., intermediate A empty categories occur in A positions
(spec of I), while intermediate A empty categories occur in A positions (spec of C)).
Once the potential chain links have been labelled, a second algorithm looks for a
chain that can &amp;quot;accept&amp;quot; a node with that label.
</bodyText>
<subsectionHeader confidence="0.98512">
The Chain Selection Problem (CSEL)
</subsectionHeader>
<bodyText confidence="0.9998185">
Given a node N of label L, and an ordered list of chains C, return the chain Ci, possibly
none, to which N has unified.
</bodyText>
<figure confidence="0.900051416666667">
Algorithm 2
Input: Node, Label(s), Ordered List of Chains
Output: Chain or empty set
If Label E {AH, AH} then
start new chain
else
If Label E {AF, AF, AI} then
choose (nearest) unsaturated chain
else
If Label = AT then
choose nearest unsaturated chain,
unless it is the immediately preceding element in the stack.
</figure>
<bodyText confidence="0.999933476190476">
The list of chains given as input is ordered by the structure-building algorithm:
when new chains are started, they are added at the end of the list. The first clause&apos; of
Algorithm 2 starts a new chain whenever a lexical element is seen. No other type of
chain link can start a chain. The second clause selects a chain when the foot is seen. By
choosing the nearest chain (i.e., the last one in the list), only nested dependencies are
built. The third clause assigns Al in a condition that is more complex than the others,
to deal with subject-oriented parasitic-gaps.&amp;quot;
In Figure 5, I show schematically how these algorithms build chains. A pseudo-
Prolog notation is used, which is similar to the output of the parser, where chains are
represented as lists enclosed in square brackets. I show the I/O of each algorithm,
given the sentence Who did you think that John seemed to like?, where a multiple A-chain
and an A-chain must be recovered. NLAB takes an input word and outputs a label,
while CSEL takes a triple (Node, Label, Chains) as input, and returns a new chain list.
Note that, in Algorithms 1 and 2, features such as Case and 0-role must be available
as input for the correct labelling and chain assignment of the empty category. This is
a crucial feature of the algorithms for chain formation proposed here.
In GB theory, empty categories can be freely coindexed with an antecedent, from
which they inherit their features. Features that are incompatible with a given context
are automatically excluded, since the sentence will be ungrammatical (Brody 1984).
This theory is called functional determination of empty categories. In GB parsing, there
have been two approaches to the implementation of chains: one that mirrors directly
</bodyText>
<footnote confidence="0.9172355">
15 This restriction handles sentences such as A man ] that 1- whenever I meet 1 looks old.] This construction,
although marginal, like all parasitic gaps, is accepted by many speakers. Parasitic gap constructions
have many interesting properties that must be dealt with for the algorithms that treat chains to be fully
general.
</footnote>
<page confidence="0.990145">
531
</page>
<table confidence="0.997823928571429">
Computational Linguistics Volume 21, Number 4
Who did you think el that John seemed e2 to like e3?
NLAB who -A- Head
CSEL who A Head [(who)]
NLAB you A Head
CSEL you A Head lf(who)1[(you)]1
NLAB el A Intermediate
CSEL el A I [Rwho)11(you)]] R(who,e1)][(you)]]
NL AB John A Head
CSEL John A Head [Rwho,e1)1[(you)11 [[(who,e1)][(you)(John)I1
NLAB e2 A Foot
CSEL e2 A Foot ][(who,e1)][(you)(John)]] [[(who,e1)][(you)(John,e2)]]
NLAB e3 -A Foot
CSEL e3 A Foot [[(who,e1)][(you)(John,e2)1I [1(who,e1,e3))1E(you)(John,e2)11
</table>
<figureCaption confidence="0.894332">
Figure 5
</figureCaption>
<bodyText confidence="0.918282194444444">
Chain building example.
the generate-and-test nondeterminism of the theory (Fong 1991; Kashket 1991), and
another that takes advantage of structural constraints to limit the space of hypotheses,
and therefore is called structural determination of empty categories (Correa 1988, 1991;
Crocker 1991). Of these two positions, the ICMH predicts that the latter will be more
efficient: features belonging to the same class must be compiled.
The algorithms proposed here amount to a &amp;quot;look-up&amp;quot; of all the relevant features
that divide all empty categories into classes, as opposed to a functional algorithm,
where the empty categories are all the same, and their different syntactic function
is determined in a second stage of parsing. An algorithm of this latter type cannot
make use of the intrinsic properties of empty categories to direct the search for the
antecedent and the construction of chains. Fong (1991) shows experimentally that
an algorithm that computes categories functionally slows down the parser by orders
of magnitude. Fong and Berwick (1992) also report that functional determination of
empty categories causes Japanese to be parsed more slowly than English, as too many
categories are posited. I adopt a more indirect implementation of the theory, along the
lines proposed by Correa (1991). In particular, I use features that define the typology
of empty categories immediately, in the course of the parse.&apos;
This position is more indirectly related to linguistic theory, thus it is a weaker
theoretical position, but it is preferable because it is more efficient, with full generality.
I address these two issues in the next two sections: first, I show that disregarding
16 It should be noted that the algorithms differ from Correa&apos;s in some respects, which give them wider
empirical coverage. Correa&apos;s algorithm for chain evaluation is based on an attribution rule. To restrict
attribute percolation, Correa imposes a restriction, such that a node can only participate in one A-chain
and one A-chain at a time. (The theoretical motivation for this limit is that an attribute grammar with
unrestricted percolation of attributes corresponds to a type 0 grammar, thus it is too powerful to
describe natural languages correctly.) Thus, some locality restrictions such as wh-islands and the
Complex NP Constraint are modelled. These locality restrictions, though, depend on the language. This
attribution rule does not work for less restrictive languages, such as the British variant of English
(Grimshaw 1986) and Italian (Rizzi 1982), which allow the types of extractions that Correa&apos;s limit is
designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps
in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to
capture restrictions on movement. Reasons of space prevent me from discussing the issues related to
locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and
Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic
theory, based essentially on a parametrization of locality restrictions, see Merlo 1992, to appear.
</bodyText>
<page confidence="0.988201">
532
</page>
<note confidence="0.661995">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.999620333333333">
features such as case and thematic roles when building chains leads to an exponential
growth of the space of hypotheses; second, I argue that using these features does not
restrict the validity of the algorithm to specific constructions or languages.
</bodyText>
<subsectionHeader confidence="0.997242">
4.2 Restricting the Search Space
</subsectionHeader>
<bodyText confidence="0.999958391304348">
As the previous section on phrase structure has shown, computing features is not
always profitable, as some features reduce the search space while others do not. To
see that checking features does indeed pay off, the cost of checking these features must
be compared to the benefit of reducing the search space.
This analysis mostly concerns the first algorithm, NLAB, which is constituted of
a series of binary choices. More precisely, recall that the relevant information is: a)
whether a node is lexical or not; b) whether it has a 0-role or not; c) whether it has
Case or not; d) whether it is a sister of C (hence, in an A-position) or not (if not, it
counts as an A-position). For the chain selection algorithm (CSEL) there are four main
constraints: first, A-nodes can only be inserted in A-chains and A-nodes can only be
inserted in A-chains. Second, empty nodes never start a new chain. Third, the closest
head is always chosen as a potential chain to which to unify. Finally, only unsaturated
chains are chosen.
Consider what would result if NLAB did not check for all of these factors. If (b)
were not checked, NLAB&apos; would not distinguish between feet and intermediate traces,
even in the same type of chain, thus it would output four sets of labels: AH, AH, { AF,
AI}, {AF, Al}. If (c) were not checked, NLAB&amp;quot; would not distinguish between A-feet
and A-feet, thus it would output AH, AH, AI, Al, {AF, AFL If (d) were not checked,
NLAB&amp;quot; would output AH, AH, { AI, AI}, AF, AE If (b), (c) and (d) together were not
checked, NLAB&amp;quot; would output AH, AH, { AI, Al, AF, AF}.
In accounting for the growth rate in the space of hypothesis of these modified
algorithms, two factors must be taken into consideration. One factor is the number of
active chain types, namely, whether a sentence presents only A-chains, only A-chains,
or both. This factor encodes the second and third restriction of the CSEL algorithm,
with the consequence that not all combinations are attempted. The second factor ac-
counts for the growth rate proper, which is reducible to counting the set of k-strings
over an n-sized alphabet, hence 11k. Here, k is the number of relevant links in the sen-
tence (for instance, feet in NLAB&amp;quot;), and n is given by the size of the set of features
collapsed by lifting some of these checks, hence, 2, 2, 2 and 4, respectively.
The hypothesis space in the three algorithms grows in slightly different ways.
In NLAB&apos;, where there is no restriction on the number of active chains, the growth
rate is 11k. For NLAB&amp;quot; and NLAB&amp;quot;, the formula is NA&amp;quot;, where NA is the number of
active chains. Practically, this amounts to 2&amp;quot; at most, as the number of active chains
is not more than 2, because of the restriction requiring that the nearest unsaturated
chain be selected. For NLAB&amp;quot;, the restriction for active chains no longer holds. In
this algorithm, no features are checked, so it is impossible to establish if a chain is
saturated or not until structure building ends. Thus, the growth factor is a function
of the number of heads seen up to a certain point in the parse, the number of empty
categories, and their respective order in the input. Notice that the different size of the
collapsed feature set, which is larger for NLAB&apos;&amp;quot;, is implicitly taken into account by
k, as the number of relevant links varies with the size of the collapsed feature sets. For
the same sentence, there are more relevant links if the collapsed feature set is larger.
Now, in all cases, growth is exponential in the number of relevant links, while
the possible gain obtained by not checking features can be at most logarithmic in the
number of potential empty categories. Since the number of potential empty categories
is at most 2f, for f binary features, this gain is expressed as f. Hence, suppressing
</bodyText>
<page confidence="0.997418">
533
</page>
<note confidence="0.567478">
Computational Linguistics Volume 21, Number 4
</note>
<tableCaption confidence="0.88735">
Table 6
</tableCaption>
<figure confidence="0.662004083333333">
Growth of Hypothesis Space: S = sentence; TL = Total number of links; RL= Relevant Links;
AC = Number of Active Chains; G = Growth rate
S TL NLAB&apos; NLAB&amp;quot; NLAB&amp;quot; NLAB&amp;quot;
RL G RL AC G RL AC G RL G
3 2 1 2 1 1 1 - - 1 1 1
4 3 1 2 1 1 1 - - 1 1 1
5 3 - - 0 1 1 - 1 - -
6 2 - - 0 1 1 - 1 - -
7 4 1 2 1 1 1 - - 1 1 2
8 3 - - 1 2 2 - - 1 1 2
9 5 - 1 2 2 1 2 2 2 6
10 6 1 2 2 2 4 1 2 2 3 18
</figure>
<bodyText confidence="0.98366459375">
feature checks becomes beneficial only if kf &gt; ilk. Now notice that 2 &lt; n &lt; 21. For
n = 2 and f = 3, the inequality is satisfied for k &lt; 4. This means that for algorithms
NLAB&amp;quot; and NLAB&amp;quot;, all sentences with more than three relevant links are computed
faster if features are checked. For n = 4, i.e. algorithm NLAB&apos;&amp;quot;&apos;, the inequality is never
satisfied!&apos;
The results of some calculations are reported in Table 6. The numbers in the &amp;quot;sen-
tence&amp;quot; column refer to the type of construction, as exemplified in Figure 1 (sentence
types 1 and 2 are not considered because they contain only trivial chains). If one con-
siders a sentence such as Who did you say that John thought that Mary seemed to like?, with
four gaps and four heads, there are 96 hypotheses about chain formation to explore
using NLAB&amp;quot;. Clearly, checking features and using them for building chains, and
keeping the hypothesis search space small, is beneficial in most cases.
Extensibility. These algorithms deal in detail with the somewhat neglected problem
of what to do when more than one chain has to be constructed. They do not discuss
specifically the issues of adjunction or rightward movement. However, they could be
extended.
In the unextended algorithm, the postulation and structural licensing of empty
categories is always performed by the same mechanism. According to the ECP (as
formulated in Rizzi 1990, 25; Cinque 1990; Chomsky 1986b, among others), for an
empty category to be licensed, two conditions must be satisfied: the empty category
must be within the maximal projection of a lexical head to be licensed structurally,
and it must be identified by an antecedent. The structural licenser and the antecedent
need not be the same element. In fact, they hardly ever are. Whether movement is to
the left or to the right does not affect structural licensing (which is here performed by
the conditions that apply to the reduction of an c-rule).
Rightward movement requires an extension of the algorithm to incorporate the
empty category in a chain. An empty category that is the foot of rightward move-
ment must be licensed structurally, before its antecedent is seen. When the NP that
is the antecedent (head of chain) is found, it starts a new chain, according to CSEL.
Therefore, an extension is needed to check if there are any empty categories wait-
17 Note that here I am assuming that checking a feature and checking a chain have the same
computational cost, which is an approximation, as a chain cannot be checked with a single operation.
</bodyText>
<page confidence="0.993676">
534
</page>
<note confidence="0.717013">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.99924825">
ing to be identified. This requires computing c-command, to check that the NP can
be the antecedent of the empty category, if one is found. Explicit computation of c-
command is not needed for leftward movement, since it is a property of the stack
of an LR parser that it encodes c-command. Only the fact that a constituent contains
an &amp;quot;orphan&amp;quot; empty category must be recorded, perhaps by composite categories. If,
on the stack, the antecedent immediately follows the element that contains the empty
category, c-command obtains (as a consequence of binary branching), and the empty
category can be unified to the antecedent.&apos;
</bodyText>
<subsectionHeader confidence="0.738194">
4.3 Incrementality
</subsectionHeader>
<bodyText confidence="0.999933857142857">
For the parser presented here to be able to perform structural interpretation of empty
categories, features must be assigned from left to right while scanning the input.
In particular, case features must be assigned immediately to an empty element of
the chain, as case is crucially used by NLAB to determine if the empty category
belongs to an A-chain or an A-chain. In general, in order to perform feature assignment
incrementally, an LR parser augmented by constraints must be able to assign features
at any point in a rule. Compare, for instance, (7a) and (7b).19
</bodyText>
<listItem confidence="0.8584805">
(7) a. IP NP I VP { assign Case if I + fin }
b. IP —&gt; NP I { assign Case if I ± fin } VP
</listItem>
<bodyText confidence="0.9943045625">
In a parser that uses rule (7a), case assignment to the NP in subject position is
performed only after the VP is seen, even if no information about the VP is needed
to perform the case-assigning action. This means that if IP is the topmost node, Case
assignment to the subject will occur only when the entire tree for the sentence is
built. A parser that uses rule (7b), on the other hand, would assign Case as soon
as the necessary information, namely, the value of the Inflection node, is available.
Formally, this is the problem of transforming an L-attributed grammar into an S-
attributed grammar.&apos; An L-attributed grammar GL can be evaluated by an LR parser
if GL is transformed into a grammar Gs such that the actions that perform attribution
in GL always occur at the end of a production in Gs.
Although it is literally true that LR parsers can evaluate actions only on reductions
(i.e., they only operate on S-attributed grammars), there are techniques to transform an
18 Alternatively, one could adopt the (linguistically radical) position that rightward movement does not
exist, as proposed by Kayne (1994). Although this generalization seems to be true for head movement,
Kayne&apos;s proposal is more controversial for maximal projections. Discussion of these issues falls
completely outside of the topic of the present paper.
19 LR parsers have been criticized as possible models of linguistic performance because, supposedly, they
cannot perform any parsing action until the end of the sentence is seen, if the structure is
right-branching (see Abney 1989; Steedman 1989). Stabler (1991) argues that this criticism adopts an
unnecessarily naive view of the interleaving of structure building and interpretation in an LR parser.
Shieber and Johnson (1993) show that by using the (set of) left contexts encoded in the LR table,
interpretation can be done incrementally. I show that a similar kind of argument can be built with
respect to case feature assignment: case features can be assigned to the links of a chain from left to
right while scanning the input. Even if, strictly speaking, mother nodes are built only after their
children are built.
20 A grammar G with productions of the sort shown in (7), i.e., a grammar that performs attribute
assignment upon reduction of a rule, is called an attribute grammar (Irons 1961; Knuth 1968; Correa
1988, 1991.) An attribute grammar is S-attributed if all the attribution rules have the form
A.a --* B.b C.c A.a f(B.b, C.c) }, where the attribute of the parent node is a function of the attributes
of the offspring. An attribute grammar is L-attributed if all the attribution rules have the form either
A.a g.b { 0.1) f(A.a)} or the form A.a 0.1) -y.c ,y.c f(B.b) 1, where the attribute of a node is a
function of the attributes of a preceding node in the rule, or of the parent node.
</bodyText>
<page confidence="0.994818">
535
</page>
<note confidence="0.749743">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.9997838">
L-attributed rule into an S-attributed rule (Aho, Sethi and Ullman 1977, 282ff discuss
the marker nonterminals technique). Such a transformation is possible if the attributes
of the tokens on the left of the current token are at a fixed position in the stack.
We can use this S-attribution transformation for Case assignment to the subject
(nominative Case or structural case). In English, structural case is assigned to the
subject position, if the subject is a sibling of (the projection of) a finite inflectional
node. This position can occur both in main and embedded clauses. English is head-
initial, and the Specifier precedes the head. These properties interact, so that when
the subject NP is reduced, INFL is always the next token in the parsing configuration.
Thus, rule (8) can be used.21
</bodyText>
<listItem confidence="0.854117">
(8) IP NP {Case assign, if I +fin} I&apos;
</listItem>
<bodyText confidence="0.999844214285714">
This rule assigns case correctly only if the attribution is not a function of the
subconstituents of I. This is precisely what distinguishes case assigned to the subject
(structural case assignment) from other types of case assignments (e.g., case assigned
to the object by either a verb or a preposition): it is assigned independently of the
properties of the main verb.
The S-attribution transformation is not restricted to languages with the properties
of English; it can also be extended to head-final languages. In verb-final languages
(German, for example) the subject of the sentence in embedded clauses is not string
adjacent to the head of the sentence, as it is in English. However, structural case can
be assigned from left to right, since the complementizer, which necessarily marks the
left edge of an IP, is obligatory, and the finite complementizer is always different from
the infinitival complementizer.
S-attribution could not be performed, however, in parsing a language with all the
characteristics given in (9).
</bodyText>
<listItem confidence="0.9741775">
(9) a. no overt case marking
b. no distinct finite complementizer
c. verb final
d. right branching in the projections other than the verb
</listItem>
<bodyText confidence="0.816804631578947">
21 At first sight, this might appear as a wild overidealization. In fact, there are both theoretical and
empirical reasons to think that this is the right way to idealize the data. A corpus analysis on 111
occurrences of the verb announce in the Penn Treebank shows that the subject is followed by an
aspectual adverb 11 times, twice by incidental phrases, and 4 times by an apposition. In all other cases
the subject and the verb are indeed adjacent. I do not consider appositions and incidentals as
challenging for the general claim: incidentals are clearly outside of an X structure assigned to the
sentence; while appositions are &amp;quot;internal&amp;quot; to the NP, thus when the verb is reached, the phrase sitting
on the stack is indeed the NP subject, which can therefore receive Case. The treatment of aspectual
adverbs is more complex. There are at least two possible tacks. First, one can notice that adverbs,
although they are analysed as maximal projections because they can be modified, never take a
complement, thus they are usually limited to a very short sequence of words, and they do not have a
recursive structure. A minimum amount of lookahead, even limited to these particular instances of
aspectual adverbs, would solve the problem. Clearly, this is an inelegant solution. A more principled
treatment comes from recent developments in the theory, that have changed somewhat the
representation used for adverbs. Laenzlinger (1993) suggests that all maximal projections have two
specifiers, one A and one A, the higher of the two is the A-position, which can be occupied by adverbs,
if they are licensed by the appropriate head (the Adv-Criterion). For these adverbs, the appropriate
head is Aspo which we find only with finite verbs. The parser could compile this information and
assign case directly, without even waiting to see the (lexical) verb.
</bodyText>
<page confidence="0.994323">
536
</page>
<note confidence="0.716442">
Paola Merlo Modularity and Information Content Classes
</note>
<bodyText confidence="0.999826263157895">
Because of property (9a), case could not be inferred from explicit information contained
in the input (unlike Japanese or German); because of property (9b) the subject position
of an embedded clause would not be unmistakably signalled (unlike German but like
Japanese); because of property (9c), the inflectional head would occur after the NP that
needs to be assigned case; finally, because of property (9d), an LR parser could give
worst-case results (which is not the case for verb-final, but left-branching, languages,
like Japanese): it could require the entire sentence to be stacked before starting to
assemble it.
Although a problem in principle, this limitation disappears in practice. Inspec-
tion of some of the sources on language typology shows that such languages are
very difficult to find (Steele 1978; Shopen 1985; Comrie 1981). According to Downing
(1978), verb-final languages usually have prenominal relative clauses, which is a sign
that they are left branching. Only two verb-final languages have postnominal relative
clauses, Persian and Turkish. In Persian, the clause boundary is overtly marked by
the suffix -i on the antecedent. Moreover, both languages have overt case marking
of the subject. Although this is by no means definitive evidence, it suggests that the
algorithm for chain formation and feature assignment that I have presented is not
obviously inadequate, and that it is applicable to a variety of languages with different
properties.
</bodyText>
<sectionHeader confidence="0.989398" genericHeader="method">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999936428571429">
The parser described in this paper has been implemented for English. It parses a
homogeneous, though small, set of sentences. As a matter of fact, one of the interesting
features of this implementation is that it offers a unified treatment of all of the chain
types presented above.
The parser has clear limitations due to the fact that it was developed mainly for
exploratory purposes. For instance, it deals only with very simple nominal phrases
and it does not treat adjunction. In other respects, however, this design lends itself
readily to extensions: The structure building and chain formation routines do not rely
on characteristics that are found only in English or in a head initial language, as was
discussed in the previous section.
In the course of pondering the relation between the grammar and the parser,
and mostly how the conceptual modularity of current linguistic theories can be imple-
mented, one learns that, in fact, the notion of modular theory is both true and false,
at least in its present incarnation. All linguists strive to develop theories that rest on
general, abstract principles, which interact in complex ways, so that many empirical
facts &amp;quot;fall out&amp;quot; from a few principles. Such a theory is clearly not modular, although
highly general and abstract. On the other hand, linguistic concepts operate on different
primitives: intuitively, X-theory, and principles of argument structure or coreference
are different objects. Future research must lead in a direction that enables us to de-
fine more precisely this basic intuition. Modularity, if it exists, is to be found in the
linguistic content, and not in the organization of the theory.
</bodyText>
<sectionHeader confidence="0.836912" genericHeader="method">
Acknowledgments References
</sectionHeader>
<footnote confidence="0.415923166666667">
I would like to thank those who have Abney, Steven (1989). &amp;quot;A Computational
helped me in this work: Michael Brent, Model of Human Parsing.&amp;quot; Journal of
Bonnie Dorr, Uli Frauenfelder, Paul Gorrell, Psycholinguistic Research, 18, 129-144.
Luigi Rizzi, Graham Russell, Eric Wehrli, Aho, Alfred V.; Sethi, Ravi; and Ullman,
Amy Weinberg, and two anonymous Jeffrey D. (1977). Compilers: Principles,
reviewers. All remaining errors are my own. Techniques and Tools. Addison-Wesley
</footnote>
<page confidence="0.916455">
537
</page>
<note confidence="0.705452">
Computational Linguistics Volume 21, Number 4
</note>
<reference confidence="0.996228918032787">
Publishing Company, Reading, MA.
Barton, Edward; Berwick, Robert; and
Ristad, Eric (1987). Computational
Complexity and Natural Language. MIT
Press, Cambridge, MA.
Berwick, Robert (1982). &amp;quot;Locality Principles
and the Acquisition of Syntactic
Knowledge.&amp;quot; Doctoral dissertation, MIT,
Cambridge, MA.
Berwick, Robert (1990). &amp;quot;Ross was Right:
Constraints on Variables.&amp;quot; Manuscript,
MIT.
Berwick, Robert (1991). &amp;quot;Principle-Based
Parsing.&amp;quot; In Foundational Issues in Natural
Language Processing, edited by Peter Sells,
Stuart M. Shieber, and Thomas Wasow.
MIT Press, 115-226.
Berwick, Robert; Abney, Steven; and Tenny,
Carol (1991). Principle-Based Parsing.
Kluwer Academic Publishers, Dordrecht.
Berwick, Robert, and Weinberg, Amy (1984).
The Grammatical Basis of Linguistic
Performance. MIT Press, Cambridge, MA.
Berwick, Robert, and Weinberg, Amy (1985).
&amp;quot;Deterministic Parsing and Linguistic
Explanation.&amp;quot; Language and Cognitive
Processes, 1(2), 109-134.
Bresnan, Joan (1978). &amp;quot;A Realistic
Transformational Grammar.&amp;quot; In Linguistic
Theory and Psychological Reality, edited by
Morris Halle, Joan Bresnan, and George
Miller. MIT Press, Cambridge, MA.
Brody, Michael (1984). &amp;quot;On Contextual
Definitions and the Role of Chains.&amp;quot;
Linguistic Inquiry, 15(3), 355-380.
Chomsky, Noam (1965). Aspects of the Theory
of Syntax. MIT Press, Cambridge, MA.
Chomsky, Noam (1986a). Knowledge of
Language: Its Nature, Origin and Use.
Praeger, New York, New York.
Chomsky, Noam (1986b). Barriers. MIT
Press, Cambridge, MA.
Chomsky, Noam (1988). &amp;quot;Some Notes on
Economy of Derivation and
Representation.&amp;quot; MIT Working Papers in
Linguistics 10, MIT, 43-74.
Chomsky, Noam (1992). &amp;quot;A Minimalist
Program for Linguistic Theory&amp;quot;
Occasional MIT Working Papers in
Linguistics 1. (also appeared in The View
from Building 20, edited by Ken Hale and
Jay Keyser. MIT Press, Cambridge, MA,
1993,1-52.)
Cinque, Guglielmo (1990). Types of A
Dependencies. MIT Press, Cambridge, MA.
Comrie, Bernard (1981). Language Universals
and Linguistic Typology. Basil Blackwell,
Oxford.
Correa, Nelson (1988). Syntactic Analysis of
English with Respect to Government-Binding
Theory. Doctoral dissertation, Syracuse
University, Syracuse, NY.
Correa, Nelson (1991). &amp;quot;Empty Categories,
Chain Binding and Parsing.&amp;quot; In
Principle-Based Parsing, edited by Robert
Berwick, Steven Abney, and Carol Tenny.
Kluwer Academic Publishers, Dordrecht,
83-122.
Crocker, Matthew (1991). &amp;quot;A Principle-based
System for Syntactic Analysis.&amp;quot; Canadian
Journal of Linguistics, 36(1), 1-26.
Crocker, Matthew (1992). &amp;quot;A Logical Model
of Competence and Performance in the
Human Sentence Processor.&amp;quot; Doctoral
dissertation, University of Edinburgh,
Edinburgh.
Crocker, Matthew (to appear). Computational
Psycholinguistics: An Interdisciplinary
Perspective. Kluwer Academic Publishers,
Dordrecht.
Dorr, Bonnie (1987). &amp;quot;UNITRAN: a
Principle-based Approach to Machine
Translation.&amp;quot; Al Lab Memo 100, MIT,
Cambridge, MA.
Dorr, Bonnie (1993). Machine Translation: A
View from the Lexicon. MIT Press,
Cambridge, MA.
Downing, Bruce T. (1978). &amp;quot;Some Universals
of Relative Clause Structure.&amp;quot; In
Universals of Human Language, edited by
Joseph H. Greenberg. Stanford University
Press, 375-418.
Earley, Jay (1970). &amp;quot;An Efficient
Context-Free Parsing Algorithm.&amp;quot;
Communications of the Association for
Computing Machinery, 14,453-460.
Fong, Sandiway (1990). &amp;quot;Free Indexation:
Combinatorial Analysis and a
Compositional Algorithm.&amp;quot; In Proceedings,
28th Meeting of the ACL, Pittsburgh, PA,
105-110.
Fong, Sandiway (1991). &amp;quot;Computational
Properties of Principle-based Grammatical
Theories.&amp;quot; Doctoral dissertation, MIT,
Cambridge, MA.
Fong, Sandiway, and Berwick, Robert
(1992). &amp;quot;Isolating Cross-linguistic Parsing
Complexity with a Principle-and
Parameters Parser: a Case Study of
Japanese and English.&amp;quot; In Proceedings,
COLING 92, Nantes, France, 631-637.
Frank, Robert (1992). &amp;quot;Syntactic Locality and
Tree Adjoining Grammar: Grammatical,
Acquisition and Processing Perspectives.&amp;quot;
Doctoral dissertation, University of
. Pennsylvania, Philadelphia, PA.
Frazier, Lyn (1985). &amp;quot;Modularity and the
Representational Hypothesis.&amp;quot; In
Proceedings of NELS /6,131-146.
Frazier, Lyn (1990). &amp;quot;Exploring the
Architecture of the Language Processing
System.&amp;quot; In Cognitive Models of Speech
</reference>
<page confidence="0.995377">
538
</page>
<note confidence="0.828992">
Paola Merlo Modularity and Information Content Classes
</note>
<reference confidence="0.99404495">
Processing, edited by Gerry Altmann, MIT
Press, 409-433.
Gazdar, Gerald; Klein, Ewan; Pullum,
Geoffrey; and Sag, Ivan (1985). Generalized
Phrase Structure Grammar. Blackwell,
Oxford.
Grimshaw, Jane (1986). &amp;quot;Subjacency and the
S/S&apos; Parameter.&amp;quot; Linguistic Inquiry. 17(2),
364-369.
Haegemann, Liliane, and Zanuttini,
Raffaella (1991). &amp;quot;Negative Heads and the
Neg Criterion.&amp;quot; The Linguistic Review, 8
(2/4), 233-251.
Irons, Edgar (1961). &amp;quot;A Syntax Directed
Compiler for ALGOL 60.&amp;quot; Communications
of the Association for Computing Machinery,
4(1) : 51-55.
Johnson, Mark (1989). &amp;quot;Parsing as
Deduction: The Use of Knowledge of
Language.&amp;quot; Journal of Psycholinguistic
Research, 18(1), 233-251.
Kashket, Michael (1991). &amp;quot;A Parameterized
Parser for English and Warlpiri.&amp;quot; Doctoral
dissertation, MIT, Cambridge, MA.
Kayne, Richard (1994). &amp;quot;The Antisymmetry
of Syntax.&amp;quot; Linguistic Inquiry Monograph
25, MIT Press, Cambridge, MA.
Knuth, Donald E. (1965). &amp;quot;On the
Translation of Languages from Left to
Right.&amp;quot; Information and Control, 8: 607-639.
Knuth, Donald E. (1968). &amp;quot;Semantics of
Context-free Languages.&amp;quot; Mathematical
Systems Theory, 2: 127-145.
van de Koot, Hans (1990). Essay on the
Grammar-Parser Relation. Foris, Dordrecht.
van de Koot, Hans (1991). &amp;quot;Parsing with
Principles: on Constraining Derivations.&amp;quot;
UCL Working Papers in Linguistics,
University College, London, 369-396.
Kornai, Andrais, and Pullum, Geoffrey
(1990). &amp;quot;The X-bar Theory of Phrase
Structure.&amp;quot; Language, 66,24-50.
Laenzlinger, Christopher (1993). &amp;quot;Principles
for a Formal Account of Adverb Syntax.&amp;quot;
Geneva Generative Papers, 1(2),
University of Geneva, 47-75.
Marcus, Mitchell (1980). A Theory of Syntactic
Recognition for Natural Language. MIT
Press, Cambridge MA.
Merlo, Paola (1992). &amp;quot;On Modularity and
Compilation in a Government and
Binding Parser.&amp;quot; Doctoral dissertation,
University of Maryland, College Park,
MD.
Merlo, Paola (to appear). Parsing with
Principles and Classes of Information. Kluwer
Academic Publishers, Dordrecht.
Phillips, John (1992). &amp;quot;A Computational
Representation for Generalized Phrase
Structure Grammars.&amp;quot; Linguistics and
Philosophy, 15(3): 255-287.
Phillips, John, and Thompson, Henry (1985).
&amp;quot;GPSGP: A Parser for Generalized Phrase
Structure Grammars.&amp;quot; Linguistics, 23(2),
245-262.
Rizzi, Luigi (1982). Issues in Italian Syntax.
Foris, Dordrecht.
Rizzi, Luigi (1990). Relativized Minimality.
MIT Press, Cambridge, MA.
Rizzi, Luigi (1991). &amp;quot;Residual Verb Second
and the Wh-Criterion,&amp;quot; Technical Reports
in Formal and Computational Linguistics,
2, University of Geneva, Geneva.
Schabes, Yves (1991). &amp;quot;Polynomial Time and
Space Shift-reduce Parsing of Arbitrary
Context-free Grammars.&amp;quot; In Proceedings,
29th Meeting of the ACL, Berkeley, CA,
106-113.
Shieber, Stuart M., and Johnson, Mark
(1993). &amp;quot;Variations on Incremental
Interpretations.&amp;quot; Journal of Psycholinguistic
Research, 22(2), 287-319.
Shopen, Timothy (1985). Language Typology
and Syntactic Description. Cambridge
University Press, Cambridge, England.
Sportiche, Dominique (1992). &amp;quot;Clitic
Constructions.&amp;quot; Manuscript, UCLA.
Stabler, Edward (1990). &amp;quot;Relaxation
Techniques for Principle-based Parsing.&amp;quot;
Manuscript, UCLA.
Stabler, Edward (1991). &amp;quot;Avoid the
Pedestrian&apos;s Paradox.&amp;quot; In Principle-Based
Parsing, edited by Robert Berwick, Steven
Abney, and Carol Tenny. Kluwer
Academic Publishers, Dordrecht, 199-238.
Stabler, Edward (1992). The Logical Approach
to Syntax. MIT Press, Cambridge, MA.
Stabler, Edward (1994). &amp;quot;The Finite
Connectivity of Linguistic Structure.&amp;quot; In
Perspectives on Sentence Processing, edited
by Charles Clifton, Lyn Frazier, and Keith
Rayner. Lawrence Erlbaum, Hillsdale, NJ,
303-336.
Steedman, Mark (1989). &amp;quot;Grammar,
Interpretation and Processing from the
Lexicon.&amp;quot; In Lexical Representation and
Processes, edited by William
Marslen-Wilson. MIT Press, Cambridge,
MA, 463-504.
Steele, Susan (1978). &amp;quot;Word Order
Variation.&amp;quot; In Universals of Human
Language, edited by Joseph H. Greenberg.
Stanford University Press, 585-623.
Thompson, Henry (1982). &amp;quot;Handling
Metarules in a Parser for GPSG.&amp;quot;
Research Paper 175, Department of
Artificial Intelligence, University of
Edinburgh, Edinburgh, UK.
Tomita, Masaru (1986). Efficient Parsing for
Natural Language. Kluwer, Hingham, MA.
</reference>
<page confidence="0.996348">
539
</page>
<note confidence="0.274771">
Computational Linguistics Volume 21, Number 4
</note>
<sectionHeader confidence="0.649601" genericHeader="method">
Appendix A
A.1 Grammar 1
</sectionHeader>
<equation confidence="0.9995205">
s : x2; /* 1 */
x2 : y2 xi; /* 2 */
x2 : y2 x2; /* 3 */
x2 : xl y2; /* 4 */
x2 : x2 y2; /* 5 */
xl : x0 y2; /* 6 */
xl : y2 x0; /* 7 */
xl :xi y2; /* 8 */
xl : y2 xl; /* 9 */
y2 :x2 /* 10 */
I w2 /* 11 */
I /*empty*/ ; /* 12 */
x0 : w0 /* 13 */
I /* empty *1; /* 14*/
x2 : xl; /* 15*/
xl :x0; /* 16 */
A.2 Grammar 2
s : c2; /* 1 */
c2 : y2 cl; /* 2 */
cl : CO y2; /* 3 */
ci : y2 cl; /* 4 */
cl : cl y2; /* 6 */
c2 : y2 c2; /* 6 */
i2 : y2 il; /* 7 */
il : i0 y2; /* 8 */
il : y2 il; /* 9 */
il : il y2; /* 10 */
i2 : y2 12; /* 11 */
v2 : y2 vi; /* 12*/
vi : v0 y2; /* 13 */
vi : y2 v1; /* 14 */
vi : vi y2; /* 15*/
v2 : y2 v2; /* 16*/
a2 : y2 al; /* 17*/
al : a0 y2; /* 18*/
al : y2 al; /* 19 */
al : al y2; /* 20 */
a2 : y2 a2; /* 21 */
p2 : y2 pi; /* 22*/
pi : p0 y2; /* 23 */
pi : y2 p1; /* 24 */
131 : pi y2; /* 25 */
p2 : y2 p2; /* 26 */
d2 : y2 dl; /* 27 */
dl : dO y2; /* 28*/
di :y2 dl; /* 29 */
di : dl y2; /* 30 */
d2 : y2 d2; /* 31*/
c2 : cl; /* 32 */
cl : c0; /* 33 */
i2 : il; /* 34 */
il : i0; /* 35 */
v2 : vi; /* 36 */
vi : v0; /* 37 */
a2 : al; /* 38 */
al : a0; /* 39 */
</equation>
<page confidence="0.851286">
540
</page>
<bodyText confidence="0.441161">
Paola Merlo Modularity and Information Content Classes
</bodyText>
<equation confidence="0.998458423076923">
p2 : pi; /* 40 */
pi : p0; /* 41 */
d2 : dl; /* 42 */
di : d0; /* 43 */
y2 : /*empty*/1n21c21121v21a21p2; /* 44-51 */
AO Grammar 3
: 12 1 c2 ;
c2 : y2 cil c11 /* empty */ ;
ci : c0 i2 1 c0;
c0 : c 1 /*empty*/ ;
12 : y2 il 1 11 1 /* empty */;
: 10 v2 1 10;
10 : i 1 /*empty*/ ;
v2 : y2 v1 1 /*empty */1v1 ;
vi : vOint p2 ;
vOint : vint1 /* empty *1;
vi : vOt n2;
vOt : vt 1 /*empty*/ ;
vi : vOrais 12;
vOrais : vrais 1 /*empty*/ ;
v1 : vOint c2;
y2 : c2 1 i2 1 n2 1 v2 1 p2
/* empty */ ;
n2 : n 1 /* empty */;
p2 : p0 y2 1 /* empty */;
p0 : p 1/*empty*/;
</equation>
<page confidence="0.989122">
541
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852806">
<title confidence="0.9977935">Modularity and Information Content Classes in Principle-based Parsing</title>
<author confidence="0.996274">Paola Merlo</author>
<affiliation confidence="0.892104">Universite de Geneve</affiliation>
<abstract confidence="0.9975426">In recent years models of parsing that are isomorphic to a principle-based theory of grammar (most notably Government and Binding (GB) Theory) have been proposed (Berwick et al. 1991). These models are natural and direct implementations of the grammar, but they are not efficient, because GB is not a computationally modular theory. This paper investigates one problem related to the tension between building linguistically based parsers and building efficient ones. In particular, the issue of what is a linguistically motivated way of deriving a parser from principle-based theories of grammar is explored. It is argued that an efficient and faithful parser can be built by taking advantage of the way in which principles are stated. To support this claim, two features of an implemented parser are discussed. First, configurations and lexical information are precompiled separately into two tables (an 5 table and a table of lexical co-occurrence) which gives rise to more compact data structures. Secondly, precomputation of syntactic features (0-roles, case, etc.) results in efficient computation of chains, because it reduces several problems of chain formation to a local computation, thus avoiding extensive search of the tree for an antecedent or extensive backtracking. It is also shown that this method of building long-distance dependencies can be computed incrementally.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<publisher>Publishing Company,</publisher>
<location>Reading, MA.</location>
<marker></marker>
<rawString>Publishing Company, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Barton</author>
<author>Robert Berwick</author>
<author>Ristad</author>
</authors>
<title>Eric</title>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10453" citStr="Barton et al. (1987)" startWordPosition="1639" endWordPosition="1642">be necessary to alleviate the problem of inefficiency, but that too much compilation slows down the parser again. 1 Berwick (1982, 403ff.) shows that the size of a cascade of distinct principles (viewed as machines) is the size of its subparts, while if these same principles are collapsed, the size of the entire system grows multiplicatively. Modularity corresponds to maximal succinctness when all independent principles are stated separately. Independent principles are, intuitively, principles that can be computed independently of each other, and therefore whose interactions are all possible. Barton et al. (1987) and Berwick (1990) attempt to formalize the concept of independence as separability, assuming that the topology of a principle-based theory like GB can be mapped onto a planar graph. In fact, if independent modules are separable modules, there is little reason to think that GB is modular, as it corresponds to a highly connected graph. 2 By compilation, here and below, I mean off-line computation of some general property of the grammar, for example the off-line computation of the interaction of principles, using partial evaluation or variable substitution. 517 Computational Linguistics Volume </context>
</contexts>
<marker>Barton, Berwick, Ristad, 1987</marker>
<rawString>Barton, Edward; Berwick, Robert; and Ristad, Eric (1987). Computational Complexity and Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
</authors>
<title>Locality Principles and the Acquisition of Syntactic Knowledge.&amp;quot; Doctoral dissertation, MIT,</title>
<date>1982</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="9962" citStr="Berwick (1982" startWordPosition="1570" endWordPosition="1571">and it separates quite clearly the grammar from the parsing algorithm. However, it is not obvious that this approach is efficient. Partial evaluation and variable substitution can increase performance, but, as usual, a space/time trade-off will ensue. Excess of partial evaluation off-line increases the size of the grammar, which might, in turn, slow down the parse. Experimentation with different kinds of algorithms suggests that some amount of compilation of the principles might be necessary to alleviate the problem of inefficiency, but that too much compilation slows down the parser again. 1 Berwick (1982, 403ff.) shows that the size of a cascade of distinct principles (viewed as machines) is the size of its subparts, while if these same principles are collapsed, the size of the entire system grows multiplicatively. Modularity corresponds to maximal succinctness when all independent principles are stated separately. Independent principles are, intuitively, principles that can be computed independently of each other, and therefore whose interactions are all possible. Barton et al. (1987) and Berwick (1990) attempt to formalize the concept of independence as separability, assuming that the topol</context>
</contexts>
<marker>Berwick, 1982</marker>
<rawString>Berwick, Robert (1982). &amp;quot;Locality Principles and the Acquisition of Syntactic Knowledge.&amp;quot; Doctoral dissertation, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
</authors>
<title>Ross was Right: Constraints on Variables.&amp;quot;</title>
<date>1990</date>
<tech>Manuscript, MIT.</tech>
<contexts>
<context position="10472" citStr="Berwick (1990)" startWordPosition="1644" endWordPosition="1645"> the problem of inefficiency, but that too much compilation slows down the parser again. 1 Berwick (1982, 403ff.) shows that the size of a cascade of distinct principles (viewed as machines) is the size of its subparts, while if these same principles are collapsed, the size of the entire system grows multiplicatively. Modularity corresponds to maximal succinctness when all independent principles are stated separately. Independent principles are, intuitively, principles that can be computed independently of each other, and therefore whose interactions are all possible. Barton et al. (1987) and Berwick (1990) attempt to formalize the concept of independence as separability, assuming that the topology of a principle-based theory like GB can be mapped onto a planar graph. In fact, if independent modules are separable modules, there is little reason to think that GB is modular, as it corresponds to a highly connected graph. 2 By compilation, here and below, I mean off-line computation of some general property of the grammar, for example the off-line computation of the interaction of principles, using partial evaluation or variable substitution. 517 Computational Linguistics Volume 21, Number 4 1.2 On</context>
</contexts>
<marker>Berwick, 1990</marker>
<rawString>Berwick, Robert (1990). &amp;quot;Ross was Right: Constraints on Variables.&amp;quot; Manuscript, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
</authors>
<title>Principle-Based Parsing.&amp;quot; In Foundational Issues in Natural Language Processing, edited by Peter Sells,</title>
<date>1991</date>
<pages>115--226</pages>
<publisher>MIT Press,</publisher>
<location>Stuart</location>
<contexts>
<context position="7076" citStr="Berwick 1991" startWordPosition="1098" endWordPosition="1099">h would require a much larger number of bits than the bits needed to encode the principle itself. The classic example is the use of natural classes of distinctive features in phonology, in order to compact several rules into one. A modular theory that encodes universal principles has obtained a greater degree of succinctness than a nonmodular theory, and is considered more explanatory. Since it 516 Paola Merlo Modularity and Information Content Classes is desirable for the parser to maintain the level of explanatory power of the theory, it must maintain its modularity It has also been argued (Berwick 1991) that the current shift from rules to a modular system of principles has computational advantages. Principle-based grammars engender compactness: Given a set of principles, Pi, P2,. , Pn, the principles are stored separately and their interaction is computed on-line; the multiplicative interaction of the principles, Pi X P2 X • • • X P, does not need to be stored. Hence, the size of the grammar is the sum of the sizes of its components: IG I = Pi + P2 ± • • • + P,. Consequently, a parser based on such a grammar is compact, and, theoretically, easier to debug, maintain and update.&apos; In practice,</context>
</contexts>
<marker>Berwick, 1991</marker>
<rawString>Berwick, Robert (1991). &amp;quot;Principle-Based Parsing.&amp;quot; In Foundational Issues in Natural Language Processing, edited by Peter Sells, Stuart M. Shieber, and Thomas Wasow. MIT Press, 115-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
<author>Steven Abney</author>
<author>Carol Tenny</author>
</authors>
<title>Principle-Based Parsing.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<marker>Berwick, Abney, Tenny, 1991</marker>
<rawString>Berwick, Robert; Abney, Steven; and Tenny, Carol (1991). Principle-Based Parsing. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
<author>Amy Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance.</title>
<date>1984</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7961" citStr="Berwick and Weinberg 1984" startWordPosition="1252" endWordPosition="1255">n-line; the multiplicative interaction of the principles, Pi X P2 X • • • X P, does not need to be stored. Hence, the size of the grammar is the sum of the sizes of its components: IG I = Pi + P2 ± • • • + P,. Consequently, a parser based on such a grammar is compact, and, theoretically, easier to debug, maintain and update.&apos; In practice, however, designing and implementing faithful and efficient parsers is not a simple matter. Defining &amp;quot;faithfulness&amp;quot; to a linguistic theory is not a trivial task, as a direct relation between the grammar and the parser is not the only option (see Bresnan 1978; Berwick and Weinberg 1984; van de Koot 1990, and references therein). In general, it is not necessary for a parser to implement the principles of the grammar directly. Rather, a covering grammar could be used, more suited to the purpose of parsing. However, it is important that such covering be done in such a way that accidental properties of a particular grammar, which would not hold under counterfactual changes, are not used. Otherwise, the covering grammar would not be sufficiently general. A faithful implementation is particularly difficult in the GB framework, as GB principles are informally expressed as English </context>
<context position="58839" citStr="Berwick and Weinberg 1984" startWordPosition="9530" endWordPosition="9533">ttribution rule does not work for less restrictive languages, such as the British variant of English (Grimshaw 1986) and Italian (Rizzi 1982), which allow the types of extractions that Correa&apos;s limit is designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to capture restrictions on movement. Reasons of space prevent me from discussing the issues related to locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic theory, based essentially on a parametrization of locality restrictions, see Merlo 1992, to appear. 532 Paola Merlo Modularity and Information Content Classes features such as case and thematic roles when building chains leads to an exponential growth of the space of hypotheses; second, I argue that using these features does not restrict the validity of the algorithm to specific constructions or languages. 4.2 Restricting the Search Space As the previous section on phrase structure has shown, computing f</context>
</contexts>
<marker>Berwick, Weinberg, 1984</marker>
<rawString>Berwick, Robert, and Weinberg, Amy (1984). The Grammatical Basis of Linguistic Performance. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
<author>Amy Weinberg</author>
</authors>
<title>Deterministic Parsing and Linguistic Explanation.&amp;quot; Language and Cognitive Processes,</title>
<date>1985</date>
<volume>1</volume>
<issue>2</issue>
<pages>109--134</pages>
<marker>Berwick, Weinberg, 1985</marker>
<rawString>Berwick, Robert, and Weinberg, Amy (1985). &amp;quot;Deterministic Parsing and Linguistic Explanation.&amp;quot; Language and Cognitive Processes, 1(2), 109-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>A Realistic Transformational Grammar.&amp;quot; In Linguistic Theory and Psychological Reality, edited by</title>
<date>1978</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7934" citStr="Bresnan 1978" startWordPosition="1250" endWordPosition="1251"> is computed on-line; the multiplicative interaction of the principles, Pi X P2 X • • • X P, does not need to be stored. Hence, the size of the grammar is the sum of the sizes of its components: IG I = Pi + P2 ± • • • + P,. Consequently, a parser based on such a grammar is compact, and, theoretically, easier to debug, maintain and update.&apos; In practice, however, designing and implementing faithful and efficient parsers is not a simple matter. Defining &amp;quot;faithfulness&amp;quot; to a linguistic theory is not a trivial task, as a direct relation between the grammar and the parser is not the only option (see Bresnan 1978; Berwick and Weinberg 1984; van de Koot 1990, and references therein). In general, it is not necessary for a parser to implement the principles of the grammar directly. Rather, a covering grammar could be used, more suited to the purpose of parsing. However, it is important that such covering be done in such a way that accidental properties of a particular grammar, which would not hold under counterfactual changes, are not used. Otherwise, the covering grammar would not be sufficiently general. A faithful implementation is particularly difficult in the GB framework, as GB principles are infor</context>
</contexts>
<marker>Bresnan, 1978</marker>
<rawString>Bresnan, Joan (1978). &amp;quot;A Realistic Transformational Grammar.&amp;quot; In Linguistic Theory and Psychological Reality, edited by Morris Halle, Joan Bresnan, and George Miller. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brody</author>
</authors>
<title>On Contextual Definitions and the Role of Chains.&amp;quot;</title>
<date>1984</date>
<journal>Linguistic Inquiry,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>355--380</pages>
<contexts>
<context position="54774" citStr="Brody 1984" startWordPosition="8897" endWordPosition="8898">input word and outputs a label, while CSEL takes a triple (Node, Label, Chains) as input, and returns a new chain list. Note that, in Algorithms 1 and 2, features such as Case and 0-role must be available as input for the correct labelling and chain assignment of the empty category. This is a crucial feature of the algorithms for chain formation proposed here. In GB theory, empty categories can be freely coindexed with an antecedent, from which they inherit their features. Features that are incompatible with a given context are automatically excluded, since the sentence will be ungrammatical (Brody 1984). This theory is called functional determination of empty categories. In GB parsing, there have been two approaches to the implementation of chains: one that mirrors directly 15 This restriction handles sentences such as A man ] that 1- whenever I meet 1 looks old.] This construction, although marginal, like all parasitic gaps, is accepted by many speakers. Parasitic gap constructions have many interesting properties that must be dealt with for the algorithms that treat chains to be fully general. 531 Computational Linguistics Volume 21, Number 4 Who did you think el that John seemed e2 to lik</context>
</contexts>
<marker>Brody, 1984</marker>
<rawString>Brody, Michael (1984). &amp;quot;On Contextual Definitions and the Role of Chains.&amp;quot; Linguistic Inquiry, 15(3), 355-380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6355" citStr="Chomsky 1965" startWordPosition="980" endWordPosition="981">sirable for a syntactic analyser to make use of linguistic theories to obtain, at least in principle, the same empirical coverage as the theory, and to capture the same generalizations. Moreover, a parser that makes direct use of a linguistic theory is more explanatory. A guiding belief for the development of the generative framework is that a theory that can derive its descriptions from the interaction of a small set of general principles is more explanatory than a theory in which descriptive adequacy is obtained by the interaction of a greater number of more particular, specific principles (Chomsky 1965). This is because the former theory is smaller. Thus, each principle can generate a set the encoding of which would require a much larger number of bits than the bits needed to encode the principle itself. The classic example is the use of natural classes of distinctive features in phonology, in order to compact several rules into one. A modular theory that encodes universal principles has obtained a greater degree of succinctness than a nonmodular theory, and is considered more explanatory. Since it 516 Paola Merlo Modularity and Information Content Classes is desirable for the parser to main</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, Noam (1965). Aspects of the Theory of Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Knowledge of Language: Its Nature, Origin and Use.</title>
<date>1986</date>
<publisher>Praeger,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="3718" citStr="Chomsky 1986" startWordPosition="556" endWordPosition="557">eneve, Switzerland C) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 observation that humans make use of their knowledge of language very effectively. In this paper, I investigate the computational problem related to the tension between building linguistically based parsers and building efficient ones, which, I argue, derives from the particular forms linguistic theories have taken recently. In particular, I explore the issue of what is a good parsing technique to apply to principle-based theories of grammar. I take Government-Binding (GB) theory (Chomsky 1986a,b; Rizzi 1990) to be a suitable illustration of such theories, and also to show in all clarity the problems that might arise. I differ from other investigations on the import of principle-based parsing in not drawing on cognitive issues or psycholinguistic results to justify my assumptions. Indeed, part of the spirit of this work is to explore how far one can go in advocating principle-based parsing, in the absence of motivations given by cognitive modelling. 1.1 The Problem When generative grammatical theory in the &apos;70s talked about &amp;quot;dative shift,&amp;quot; &amp;quot;topicalization,&amp;quot; &amp;quot;passive,&amp;quot; it meant that</context>
<context position="65005" citStr="Chomsky 1986" startWordPosition="10665" endWordPosition="10666"> Clearly, checking features and using them for building chains, and keeping the hypothesis search space small, is beneficial in most cases. Extensibility. These algorithms deal in detail with the somewhat neglected problem of what to do when more than one chain has to be constructed. They do not discuss specifically the issues of adjunction or rightward movement. However, they could be extended. In the unextended algorithm, the postulation and structural licensing of empty categories is always performed by the same mechanism. According to the ECP (as formulated in Rizzi 1990, 25; Cinque 1990; Chomsky 1986b, among others), for an empty category to be licensed, two conditions must be satisfied: the empty category must be within the maximal projection of a lexical head to be licensed structurally, and it must be identified by an antecedent. The structural licenser and the antecedent need not be the same element. In fact, they hardly ever are. Whether movement is to the left or to the right does not affect structural licensing (which is here performed by the conditions that apply to the reduction of an c-rule). Rightward movement requires an extension of the algorithm to incorporate the empty cate</context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>Chomsky, Noam (1986a). Knowledge of Language: Its Nature, Origin and Use. Praeger, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1986</date>
<publisher>Barriers. MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3718" citStr="Chomsky 1986" startWordPosition="556" endWordPosition="557">eneve, Switzerland C) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 observation that humans make use of their knowledge of language very effectively. In this paper, I investigate the computational problem related to the tension between building linguistically based parsers and building efficient ones, which, I argue, derives from the particular forms linguistic theories have taken recently. In particular, I explore the issue of what is a good parsing technique to apply to principle-based theories of grammar. I take Government-Binding (GB) theory (Chomsky 1986a,b; Rizzi 1990) to be a suitable illustration of such theories, and also to show in all clarity the problems that might arise. I differ from other investigations on the import of principle-based parsing in not drawing on cognitive issues or psycholinguistic results to justify my assumptions. Indeed, part of the spirit of this work is to explore how far one can go in advocating principle-based parsing, in the absence of motivations given by cognitive modelling. 1.1 The Problem When generative grammatical theory in the &apos;70s talked about &amp;quot;dative shift,&amp;quot; &amp;quot;topicalization,&amp;quot; &amp;quot;passive,&amp;quot; it meant that</context>
<context position="65005" citStr="Chomsky 1986" startWordPosition="10665" endWordPosition="10666"> Clearly, checking features and using them for building chains, and keeping the hypothesis search space small, is beneficial in most cases. Extensibility. These algorithms deal in detail with the somewhat neglected problem of what to do when more than one chain has to be constructed. They do not discuss specifically the issues of adjunction or rightward movement. However, they could be extended. In the unextended algorithm, the postulation and structural licensing of empty categories is always performed by the same mechanism. According to the ECP (as formulated in Rizzi 1990, 25; Cinque 1990; Chomsky 1986b, among others), for an empty category to be licensed, two conditions must be satisfied: the empty category must be within the maximal projection of a lexical head to be licensed structurally, and it must be identified by an antecedent. The structural licenser and the antecedent need not be the same element. In fact, they hardly ever are. Whether movement is to the left or to the right does not affect structural licensing (which is here performed by the conditions that apply to the reduction of an c-rule). Rightward movement requires an extension of the algorithm to incorporate the empty cate</context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>Chomsky, Noam (1986b). Barriers. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Some Notes on Economy of Derivation and Representation.&amp;quot;</title>
<date>1988</date>
<journal>MIT Working Papers in Linguistics</journal>
<volume>10</volume>
<pages>43--74</pages>
<contexts>
<context position="20374" citStr="Chomsky (1988" startWordPosition="3220" endWordPosition="3221">servation is based on a detailed inspection of the form of the principles of the grammar. What is presented in (2) as an illustrative example is, in fact, a consistent form of organization of the principles. If one looks at several of the principles of the grammar that are involved in building structure and annotating the phrase marker, one finds the same internal organization. Theta-assignment occurs in the configuration of sisterhood, it requires a 0-assigning head, and it must occur between a node and its most local assigner. Assignment of Case occurs in a given configuration (according to Chomsky (1988, 1992) it is always a specifier-head configuration), given a certain lexical property of the head a-ND, and locally, within the same maximal projection). The same restriction occurs again for what is called the wh-criterion (Rizzi 1991), which regulates wh-movement, where the head must have a +wh feature and occur within a specifier-head configuration. Categorial selection and functional selection also occur under the same restrictions, in the complement configuration (i.e., between a head and a maximal projection). The licensing of subjects in the phrase marker, done by predication, must occ</context>
</contexts>
<marker>Chomsky, 1988</marker>
<rawString>Chomsky, Noam (1988). &amp;quot;Some Notes on Economy of Derivation and Representation.&amp;quot; MIT Working Papers in Linguistics 10, MIT, 43-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>A Minimalist Program for Linguistic Theory&amp;quot; Occasional MIT Working Papers in Linguistics 1. (also appeared in The View from Building 20, edited by Ken Hale and</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Chomsky, 1992</marker>
<rawString>Chomsky, Noam (1992). &amp;quot;A Minimalist Program for Linguistic Theory&amp;quot; Occasional MIT Working Papers in Linguistics 1. (also appeared in The View from Building 20, edited by Ken Hale and Jay Keyser. MIT Press, Cambridge, MA, 1993,1-52.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guglielmo Cinque</author>
</authors>
<title>Types of A Dependencies.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="64991" citStr="Cinque 1990" startWordPosition="10663" endWordPosition="10664"> using NLAB&amp;quot;. Clearly, checking features and using them for building chains, and keeping the hypothesis search space small, is beneficial in most cases. Extensibility. These algorithms deal in detail with the somewhat neglected problem of what to do when more than one chain has to be constructed. They do not discuss specifically the issues of adjunction or rightward movement. However, they could be extended. In the unextended algorithm, the postulation and structural licensing of empty categories is always performed by the same mechanism. According to the ECP (as formulated in Rizzi 1990, 25; Cinque 1990; Chomsky 1986b, among others), for an empty category to be licensed, two conditions must be satisfied: the empty category must be within the maximal projection of a lexical head to be licensed structurally, and it must be identified by an antecedent. The structural licenser and the antecedent need not be the same element. In fact, they hardly ever are. Whether movement is to the left or to the right does not affect structural licensing (which is here performed by the conditions that apply to the reduction of an c-rule). Rightward movement requires an extension of the algorithm to incorporate </context>
</contexts>
<marker>Cinque, 1990</marker>
<rawString>Cinque, Guglielmo (1990). Types of A Dependencies. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Comrie</author>
</authors>
<title>Language Universals and Linguistic Typology.</title>
<date>1981</date>
<publisher>Basil Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="75132" citStr="Comrie 1981" startWordPosition="12355" endWordPosition="12356">lled (unlike German but like Japanese); because of property (9c), the inflectional head would occur after the NP that needs to be assigned case; finally, because of property (9d), an LR parser could give worst-case results (which is not the case for verb-final, but left-branching, languages, like Japanese): it could require the entire sentence to be stacked before starting to assemble it. Although a problem in principle, this limitation disappears in practice. Inspection of some of the sources on language typology shows that such languages are very difficult to find (Steele 1978; Shopen 1985; Comrie 1981). According to Downing (1978), verb-final languages usually have prenominal relative clauses, which is a sign that they are left branching. Only two verb-final languages have postnominal relative clauses, Persian and Turkish. In Persian, the clause boundary is overtly marked by the suffix -i on the antecedent. Moreover, both languages have overt case marking of the subject. Although this is by no means definitive evidence, it suggests that the algorithm for chain formation and feature assignment that I have presented is not obviously inadequate, and that it is applicable to a variety of langua</context>
</contexts>
<marker>Comrie, 1981</marker>
<rawString>Comrie, Bernard (1981). Language Universals and Linguistic Typology. Basil Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Correa</author>
</authors>
<title>Syntactic Analysis of English with Respect to Government-Binding Theory. Doctoral dissertation,</title>
<date>1988</date>
<institution>Syracuse University,</institution>
<location>Syracuse, NY.</location>
<contexts>
<context position="56084" citStr="Correa 1988" startWordPosition="9094" endWordPosition="9095">AB el A Intermediate CSEL el A I [Rwho)11(you)]] R(who,e1)][(you)]] NL AB John A Head CSEL John A Head [Rwho,e1)1[(you)11 [[(who,e1)][(you)(John)I1 NLAB e2 A Foot CSEL e2 A Foot ][(who,e1)][(you)(John)]] [[(who,e1)][(you)(John,e2)]] NLAB e3 -A Foot CSEL e3 A Foot [[(who,e1)][(you)(John,e2)1I [1(who,e1,e3))1E(you)(John,e2)11 Figure 5 Chain building example. the generate-and-test nondeterminism of the theory (Fong 1991; Kashket 1991), and another that takes advantage of structural constraints to limit the space of hypotheses, and therefore is called structural determination of empty categories (Correa 1988, 1991; Crocker 1991). Of these two positions, the ICMH predicts that the latter will be more efficient: features belonging to the same class must be compiled. The algorithms proposed here amount to a &amp;quot;look-up&amp;quot; of all the relevant features that divide all empty categories into classes, as opposed to a functional algorithm, where the empty categories are all the same, and their different syntactic function is determined in a second stage of parsing. An algorithm of this latter type cannot make use of the intrinsic properties of empty categories to direct the search for the antecedent and the co</context>
<context position="69834" citStr="Correa 1988" startWordPosition="11484" endWordPosition="11485"> Johnson (1993) show that by using the (set of) left contexts encoded in the LR table, interpretation can be done incrementally. I show that a similar kind of argument can be built with respect to case feature assignment: case features can be assigned to the links of a chain from left to right while scanning the input. Even if, strictly speaking, mother nodes are built only after their children are built. 20 A grammar G with productions of the sort shown in (7), i.e., a grammar that performs attribute assignment upon reduction of a rule, is called an attribute grammar (Irons 1961; Knuth 1968; Correa 1988, 1991.) An attribute grammar is S-attributed if all the attribution rules have the form A.a --* B.b C.c A.a f(B.b, C.c) }, where the attribute of the parent node is a function of the attributes of the offspring. An attribute grammar is L-attributed if all the attribution rules have the form either A.a g.b { 0.1) f(A.a)} or the form A.a 0.1) -y.c ,y.c f(B.b) 1, where the attribute of a node is a function of the attributes of a preceding node in the rule, or of the parent node. 535 Computational Linguistics Volume 21, Number 4 L-attributed rule into an S-attributed rule (Aho, Sethi and Ullman 1</context>
</contexts>
<marker>Correa, 1988</marker>
<rawString>Correa, Nelson (1988). Syntactic Analysis of English with Respect to Government-Binding Theory. Doctoral dissertation, Syracuse University, Syracuse, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Correa</author>
</authors>
<title>Empty Categories, Chain Binding and Parsing.&amp;quot; In Principle-Based Parsing, edited by</title>
<date>1991</date>
<pages>83--122</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="57113" citStr="Correa (1991)" startWordPosition="9257" endWordPosition="9258">rmined in a second stage of parsing. An algorithm of this latter type cannot make use of the intrinsic properties of empty categories to direct the search for the antecedent and the construction of chains. Fong (1991) shows experimentally that an algorithm that computes categories functionally slows down the parser by orders of magnitude. Fong and Berwick (1992) also report that functional determination of empty categories causes Japanese to be parsed more slowly than English, as too many categories are posited. I adopt a more indirect implementation of the theory, along the lines proposed by Correa (1991). In particular, I use features that define the typology of empty categories immediately, in the course of the parse.&apos; This position is more indirectly related to linguistic theory, thus it is a weaker theoretical position, but it is preferable because it is more efficient, with full generality. I address these two issues in the next two sections: first, I show that disregarding 16 It should be noted that the algorithms differ from Correa&apos;s in some respects, which give them wider empirical coverage. Correa&apos;s algorithm for chain evaluation is based on an attribution rule. To restrict attribute </context>
</contexts>
<marker>Correa, 1991</marker>
<rawString>Correa, Nelson (1991). &amp;quot;Empty Categories, Chain Binding and Parsing.&amp;quot; In Principle-Based Parsing, edited by Robert Berwick, Steven Abney, and Carol Tenny. Kluwer Academic Publishers, Dordrecht, 83-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crocker</author>
</authors>
<title>A Principle-based System for Syntactic Analysis.&amp;quot;</title>
<date>1991</date>
<journal>Canadian Journal of Linguistics,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>1--26</pages>
<contexts>
<context position="56105" citStr="Crocker 1991" startWordPosition="9097" endWordPosition="9098">e CSEL el A I [Rwho)11(you)]] R(who,e1)][(you)]] NL AB John A Head CSEL John A Head [Rwho,e1)1[(you)11 [[(who,e1)][(you)(John)I1 NLAB e2 A Foot CSEL e2 A Foot ][(who,e1)][(you)(John)]] [[(who,e1)][(you)(John,e2)]] NLAB e3 -A Foot CSEL e3 A Foot [[(who,e1)][(you)(John,e2)1I [1(who,e1,e3))1E(you)(John,e2)11 Figure 5 Chain building example. the generate-and-test nondeterminism of the theory (Fong 1991; Kashket 1991), and another that takes advantage of structural constraints to limit the space of hypotheses, and therefore is called structural determination of empty categories (Correa 1988, 1991; Crocker 1991). Of these two positions, the ICMH predicts that the latter will be more efficient: features belonging to the same class must be compiled. The algorithms proposed here amount to a &amp;quot;look-up&amp;quot; of all the relevant features that divide all empty categories into classes, as opposed to a functional algorithm, where the empty categories are all the same, and their different syntactic function is determined in a second stage of parsing. An algorithm of this latter type cannot make use of the intrinsic properties of empty categories to direct the search for the antecedent and the construction of chains.</context>
</contexts>
<marker>Crocker, 1991</marker>
<rawString>Crocker, Matthew (1991). &amp;quot;A Principle-based System for Syntactic Analysis.&amp;quot; Canadian Journal of Linguistics, 36(1), 1-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crocker</author>
</authors>
<title>A Logical Model of Competence and Performance in the Human Sentence Processor.&amp;quot; Doctoral dissertation,</title>
<date>1992</date>
<institution>University of Edinburgh,</institution>
<location>Edinburgh.</location>
<contexts>
<context position="9059" citStr="Crocker 1992" startWordPosition="1431" endWordPosition="1432">implementation is particularly difficult in the GB framework, as GB principles are informally expressed as English statements, and can take a variety of forms. For example, X theory (a condition on graphs), the Case Filter (an output filter on strings), and the 0 criterion (a bijection relation on predicates and arguments) all fall under the label of principles. Attempts have been made to formalize GB principles to a set of axioms (Stabler 1992). One possible, extreme interpretation of the direct use of principles is an approach where no grammar compilation is allowed (Abney 1989; Frank 1992; Crocker 1992).2 This approach is appealing because it reflects, intuitively, the idea of using the grammar as a set of axioms and reduces parsing to a deduction process. This is very much in the spirit of the current shift in linguistic theories from construction-dependent rules to general principles, and it separates quite clearly the grammar from the parsing algorithm. However, it is not obvious that this approach is efficient. Partial evaluation and variable substitution can increase performance, but, as usual, a space/time trade-off will ensue. Excess of partial evaluation off-line increases the size o</context>
<context position="22066" citStr="Crocker (1992" startWordPosition="3465" endWordPosition="3466">, and such primitives can be grouped into classes defined according to their content. Linguistic information can be classified into five different classes: (3) a. Configurations: sisterhood, c-command, m-command, + maximal b. Lexical features: ±N, ±V, ±Funct, ±c-selected c. Syntactic features: +Case, ±9, ±1,, +barrier, +Strong Agr d. Locality information: minimality, antecedent government e. Referential information: +anaphor, +pronominal, indices This qualitative classification forms a partitioning into natural classes based on information content. I call these IC Classes.&apos; 5 Differently from Crocker (1992, to appear) and Frazier (1985), this partitioning does not rely on the particular representation used. The spirit of the hypothesis is that linguistic theory is formed by heterogeneous types of information, and that the representation used to describe them is a derived concept. Frazier (1990) proposes an evolutionary partitioning of the parser based on tasks. This 520 Paola Merlo Modularity and Information Content Classes It can then be hypothesized that the amount of compilation (or, conversely, the modularity of the parser) is captured by the notion of IC classes as follows: IC Modularity H</context>
</contexts>
<marker>Crocker, 1992</marker>
<rawString>Crocker, Matthew (1992). &amp;quot;A Logical Model of Competence and Performance in the Human Sentence Processor.&amp;quot; Doctoral dissertation, University of Edinburgh, Edinburgh.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Matthew Crocker</author>
</authors>
<title>(to appear). Computational Psycholinguistics: An Interdisciplinary Perspective.</title>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<marker>Crocker, </marker>
<rawString>Crocker, Matthew (to appear). Computational Psycholinguistics: An Interdisciplinary Perspective. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>UNITRAN: a Principle-based Approach to Machine Translation.&amp;quot; Al Lab Memo 100, MIT,</title>
<date>1987</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="12598" citStr="Dorr (1987)" startWordPosition="1963" endWordPosition="1964">ty appears to increase exponentially with the input size. Fong (1991, 123) discusses a parsing algorithm. He shows that an initial version of the parser, where the phrase structure rules were expressed as a DCG and interpreted on-line, spent 80% of the total parsing time building structure. In a later version, where rules were compiled into an LR(1) table, structure-building constituted 20% of the total parsing time. This same parser includes a module for the computation of long distance dependencies, which works by generate-and-test. Fong finds that this parsing approach is also inefficient. Dorr (1987) notices similar effects in a parser that uses an algorithm more parallel in spirit (Earley 1970). Dorr notes that a limited amount of precompilation of the principles speeds up the parse, otherwise too many incorrect alternatives are carried along before being eliminated. For example, in her design, X theory and the other principles are coroutined. She finds that precompiling the principles that license empty categories with the phrase structure rules reduces considerably the number of structures that are submitted to the filtering action of the other principles, and thus speeds up the parse.</context>
<context position="14551" citStr="Dorr (1987)" startWordPosition="2278" endWordPosition="2279">on of the length of the input and the size of the grammar. For the kind of input lengths that are relevant for natural language, the size of the grammar easily becomes the predominant factor. If principles are precompiled in the form of grammar rules, the size of the grammar increases. As Tomita (1986) points out, input length does not cause a noticeable increase in running time up to 35 to 40 input tokens. For sentences of this length, grammar size becomes a relevant factor for grammars that contain more than approximately 220 rules, in his algorithm (an LR parser with parallel stacks). Both Dorr (1987) and Tomita (1986) show experimental results confirming that there is a critical point beyond which the parser is slowed down by the increasing size of the grammar. In the Generalized Phrase Structure Grammar (GPSG) formalism (Gazdar et al. 1985), similar experiments have been performed, which confirm this result. Parsers for GPSG are particularly interesting, because they use a formalism that expresses many grammatical generalizations in 518 Paola Merlo Modularity and Information Content Classes a uniform format. Therefore, GPSG is, in principle, more amenable to being processed by known pars</context>
</contexts>
<marker>Dorr, 1987</marker>
<rawString>Dorr, Bonnie (1987). &amp;quot;UNITRAN: a Principle-based Approach to Machine Translation.&amp;quot; Al Lab Memo 100, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>Machine Translation: A View from the Lexicon.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="16623" citStr="Dorr 1993" startWordPosition="2595" endWordPosition="2596">ile a limited amount of off-line precompilation might make the parser more efficient.&apos; In the next section, I propose and discuss a solution to this problem that builds on other approaches and relates the parser to the grammar in a principled way. 2. The Proposal Two avenues have generally been pursued to build efficient GB parsers. In one case, a &amp;quot;covering grammar&amp;quot; is compiled, which overgenerates and is then filtered by constraints. The compilation is done in such a way that the overgeneration is wellbehaved. For instance, the correct distribution of empty categories is calculated off-line (Dorr 1993). In the other case, all the principles are applied on line, but they apply only to a portion of the tree, and are therefore restricted to a local computation (Frank 1992).4 My proposal combines these two approaches: it adopts the idea of compiling the grammar, at least partially, off-line but it attempts to find a principled way of doing so. In this, I differ from Dorr, where the amount of compilation is heuristic and based on practical experimentation. The approach shares Frank&apos;s intuition that linguistic principles have a form, which can be exploited in structuring the parser. This proposal</context>
</contexts>
<marker>Dorr, 1993</marker>
<rawString>Dorr, Bonnie (1993). Machine Translation: A View from the Lexicon. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce T Downing</author>
</authors>
<title>Some Universals of Relative Clause Structure.&amp;quot; In Universals of Human Language,</title>
<date>1978</date>
<pages>375--418</pages>
<publisher>Stanford University Press,</publisher>
<note>edited by</note>
<contexts>
<context position="75161" citStr="Downing (1978)" startWordPosition="12359" endWordPosition="12360">e Japanese); because of property (9c), the inflectional head would occur after the NP that needs to be assigned case; finally, because of property (9d), an LR parser could give worst-case results (which is not the case for verb-final, but left-branching, languages, like Japanese): it could require the entire sentence to be stacked before starting to assemble it. Although a problem in principle, this limitation disappears in practice. Inspection of some of the sources on language typology shows that such languages are very difficult to find (Steele 1978; Shopen 1985; Comrie 1981). According to Downing (1978), verb-final languages usually have prenominal relative clauses, which is a sign that they are left branching. Only two verb-final languages have postnominal relative clauses, Persian and Turkish. In Persian, the clause boundary is overtly marked by the suffix -i on the antecedent. Moreover, both languages have overt case marking of the subject. Although this is by no means definitive evidence, it suggests that the algorithm for chain formation and feature assignment that I have presented is not obviously inadequate, and that it is applicable to a variety of languages with different properties</context>
</contexts>
<marker>Downing, 1978</marker>
<rawString>Downing, Bruce T. (1978). &amp;quot;Some Universals of Relative Clause Structure.&amp;quot; In Universals of Human Language, edited by Joseph H. Greenberg. Stanford University Press, 375-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.&amp;quot;</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>14--453</pages>
<contexts>
<context position="12695" citStr="Earley 1970" startWordPosition="1979" endWordPosition="1980">lgorithm. He shows that an initial version of the parser, where the phrase structure rules were expressed as a DCG and interpreted on-line, spent 80% of the total parsing time building structure. In a later version, where rules were compiled into an LR(1) table, structure-building constituted 20% of the total parsing time. This same parser includes a module for the computation of long distance dependencies, which works by generate-and-test. Fong finds that this parsing approach is also inefficient. Dorr (1987) notices similar effects in a parser that uses an algorithm more parallel in spirit (Earley 1970). Dorr notes that a limited amount of precompilation of the principles speeds up the parse, otherwise too many incorrect alternatives are carried along before being eliminated. For example, in her design, X theory and the other principles are coroutined. She finds that precompiling the principles that license empty categories with the phrase structure rules reduces considerably the number of structures that are submitted to the filtering action of the other principles, and thus speeds up the parse. In all these cases, the source of inefficiency stems from the principle-based design. Because ea</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay (1970). &amp;quot;An Efficient Context-Free Parsing Algorithm.&amp;quot; Communications of the Association for Computing Machinery, 14,453-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandiway Fong</author>
</authors>
<title>Free Indexation: Combinatorial Analysis and a Compositional Algorithm.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Meeting of the ACL,</booktitle>
<pages>105--110</pages>
<location>Pittsburgh, PA,</location>
<marker>Fong, 1990</marker>
<rawString>Fong, Sandiway (1990). &amp;quot;Free Indexation: Combinatorial Analysis and a Compositional Algorithm.&amp;quot; In Proceedings, 28th Meeting of the ACL, Pittsburgh, PA, 105-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandiway Fong</author>
</authors>
<title>Computational Properties of Principle-based Grammatical Theories.&amp;quot; Doctoral dissertation, MIT,</title>
<date>1991</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="12055" citStr="Fong (1991" startWordPosition="1879" endWordPosition="1880">oach is inefficient. Kashket (1991) discusses a principle-based parser, where no grammar precompilation is performed, and which parses English and Warlpiri using a parameterized theory of grammar. The parsing algorithm is a generate-and-test, backtracking regime. Kashket (1991) reports, for instance, that a 5-word sentence in Warlpiri (which can have 5! analyses, given the free word order of the language) can take up to 40 minutes to parse. He concludes that, although no mathematical analysis for the algorithm is available, the complexity appears to increase exponentially with the input size. Fong (1991, 123) discusses a parsing algorithm. He shows that an initial version of the parser, where the phrase structure rules were expressed as a DCG and interpreted on-line, spent 80% of the total parsing time building structure. In a later version, where rules were compiled into an LR(1) table, structure-building constituted 20% of the total parsing time. This same parser includes a module for the computation of long distance dependencies, which works by generate-and-test. Fong finds that this parsing approach is also inefficient. Dorr (1987) notices similar effects in a parser that uses an algorit</context>
<context position="23870" citStr="Fong (1991)" startWordPosition="3737" endWordPosition="3738">P —AT NP, are not compatible with the ICMH, nor are proposals in the spirit of licensing grammars (Abney 1989, Frank 1992), where information is encoded in each lexical item. Second, the ICMH predicts that long-distance dependencies, represented as chains, are computed in steps. Empty categories are licensed in two computational steps: structural licensing by an appropriate head, and feature instantiation. With respect to feature instantiation in particular, it is predicted that precompiling syntactic features speeds up the parsing process. This is different from functional approaches such as Fong (1991), and Fong and Berwick (1992), in which there is no precompilation.6 These predictions seem to be supported (and, consequently, so is the ICMH) by two main results, which are illustrated below: 1. separating X from lexical information yields more compact data structures; I propose a parser that uses two compiled tables: one that encodes structural information, and the other that encodes lexical information. 2. using syntactic features to compute empty categories reduces the search space, complex chains can be computed efficiently. These claims are supported in the next section, where I discuss</context>
<context position="28018" citStr="Fong 1991" startWordPosition="4388" endWordPosition="4389">ement, respectively. The main parsing algorithm is a modified LR parsing algorithm augmented by multi-action entries and constraints on reduction.&apos; The structure-building component of the parser is driven by an LR(k) parser (Knuth 1965) which consults two tables. One table encodes X information (following Kornai and Pullum 1990). The other table encodes lexical information. Lexical information is consulted only if it is needed to disambiguate a state containing multiple actions in the LR parser. An overview of this design is shown in Figure 2. instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992). In formalism other than GB theory, gaps are encoded directly into the rules. Both GPSG and HPSG use slash features to percolate features to gaps. The use of slash features probably simplifies the computation. There has been a debate on the explanatory adequacy of grammars that employ slash features (see van de Koot 1990, and Stabler 1994). For my purposes, note that, if anything, I am dealing with the worst case for the parser. 8 The ICMH is not sufficient to predict a specific parsing architecture, but rather it loosely dictates the organization of the parser. The ch</context>
<context position="55893" citStr="Fong 1991" startWordPosition="9067" endWordPosition="9068">putational Linguistics Volume 21, Number 4 Who did you think el that John seemed e2 to like e3? NLAB who -A- Head CSEL who A Head [(who)] NLAB you A Head CSEL you A Head lf(who)1[(you)]1 NLAB el A Intermediate CSEL el A I [Rwho)11(you)]] R(who,e1)][(you)]] NL AB John A Head CSEL John A Head [Rwho,e1)1[(you)11 [[(who,e1)][(you)(John)I1 NLAB e2 A Foot CSEL e2 A Foot ][(who,e1)][(you)(John)]] [[(who,e1)][(you)(John,e2)]] NLAB e3 -A Foot CSEL e3 A Foot [[(who,e1)][(you)(John,e2)1I [1(who,e1,e3))1E(you)(John,e2)11 Figure 5 Chain building example. the generate-and-test nondeterminism of the theory (Fong 1991; Kashket 1991), and another that takes advantage of structural constraints to limit the space of hypotheses, and therefore is called structural determination of empty categories (Correa 1988, 1991; Crocker 1991). Of these two positions, the ICMH predicts that the latter will be more efficient: features belonging to the same class must be compiled. The algorithms proposed here amount to a &amp;quot;look-up&amp;quot; of all the relevant features that divide all empty categories into classes, as opposed to a functional algorithm, where the empty categories are all the same, and their different syntactic function </context>
</contexts>
<marker>Fong, 1991</marker>
<rawString>Fong, Sandiway (1991). &amp;quot;Computational Properties of Principle-based Grammatical Theories.&amp;quot; Doctoral dissertation, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandiway Fong</author>
<author>Robert Berwick</author>
</authors>
<title>Isolating Cross-linguistic Parsing Complexity with a Principle-and Parameters Parser: a Case Study of Japanese and English.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, COLING 92,</booktitle>
<pages>631--637</pages>
<location>Nantes, France,</location>
<contexts>
<context position="23899" citStr="Fong and Berwick (1992)" startWordPosition="3740" endWordPosition="3743"> compatible with the ICMH, nor are proposals in the spirit of licensing grammars (Abney 1989, Frank 1992), where information is encoded in each lexical item. Second, the ICMH predicts that long-distance dependencies, represented as chains, are computed in steps. Empty categories are licensed in two computational steps: structural licensing by an appropriate head, and feature instantiation. With respect to feature instantiation in particular, it is predicted that precompiling syntactic features speeds up the parsing process. This is different from functional approaches such as Fong (1991), and Fong and Berwick (1992), in which there is no precompilation.6 These predictions seem to be supported (and, consequently, so is the ICMH) by two main results, which are illustrated below: 1. separating X from lexical information yields more compact data structures; I propose a parser that uses two compiled tables: one that encodes structural information, and the other that encodes lexical information. 2. using syntactic features to compute empty categories reduces the search space, complex chains can be computed efficiently. These claims are supported in the next section, where I discuss the properties of an impleme</context>
<context position="28042" citStr="Fong and Berwick 1992" startWordPosition="4390" endWordPosition="4393">ectively. The main parsing algorithm is a modified LR parsing algorithm augmented by multi-action entries and constraints on reduction.&apos; The structure-building component of the parser is driven by an LR(k) parser (Knuth 1965) which consults two tables. One table encodes X information (following Kornai and Pullum 1990). The other table encodes lexical information. Lexical information is consulted only if it is needed to disambiguate a state containing multiple actions in the LR parser. An overview of this design is shown in Figure 2. instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992). In formalism other than GB theory, gaps are encoded directly into the rules. Both GPSG and HPSG use slash features to percolate features to gaps. The use of slash features probably simplifies the computation. There has been a debate on the explanatory adequacy of grammars that employ slash features (see van de Koot 1990, and Stabler 1994). For my purposes, note that, if anything, I am dealing with the worst case for the parser. 8 The ICMH is not sufficient to predict a specific parsing architecture, but rather it loosely dictates the organization of the parser. The choice of an LR parser the</context>
<context position="56864" citStr="Fong and Berwick (1992)" startWordPosition="9216" endWordPosition="9219">d. The algorithms proposed here amount to a &amp;quot;look-up&amp;quot; of all the relevant features that divide all empty categories into classes, as opposed to a functional algorithm, where the empty categories are all the same, and their different syntactic function is determined in a second stage of parsing. An algorithm of this latter type cannot make use of the intrinsic properties of empty categories to direct the search for the antecedent and the construction of chains. Fong (1991) shows experimentally that an algorithm that computes categories functionally slows down the parser by orders of magnitude. Fong and Berwick (1992) also report that functional determination of empty categories causes Japanese to be parsed more slowly than English, as too many categories are posited. I adopt a more indirect implementation of the theory, along the lines proposed by Correa (1991). In particular, I use features that define the typology of empty categories immediately, in the course of the parse.&apos; This position is more indirectly related to linguistic theory, thus it is a weaker theoretical position, but it is preferable because it is more efficient, with full generality. I address these two issues in the next two sections: f</context>
</contexts>
<marker>Fong, Berwick, 1992</marker>
<rawString>Fong, Sandiway, and Berwick, Robert (1992). &amp;quot;Isolating Cross-linguistic Parsing Complexity with a Principle-and Parameters Parser: a Case Study of Japanese and English.&amp;quot; In Proceedings, COLING 92, Nantes, France, 631-637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frank</author>
</authors>
<title>Syntactic Locality and Tree Adjoining Grammar: Grammatical, Acquisition and Processing Perspectives.&amp;quot; Doctoral dissertation,</title>
<date>1992</date>
<institution>University of .</institution>
<location>Pennsylvania, Philadelphia, PA.</location>
<contexts>
<context position="9044" citStr="Frank 1992" startWordPosition="1429" endWordPosition="1430"> A faithful implementation is particularly difficult in the GB framework, as GB principles are informally expressed as English statements, and can take a variety of forms. For example, X theory (a condition on graphs), the Case Filter (an output filter on strings), and the 0 criterion (a bijection relation on predicates and arguments) all fall under the label of principles. Attempts have been made to formalize GB principles to a set of axioms (Stabler 1992). One possible, extreme interpretation of the direct use of principles is an approach where no grammar compilation is allowed (Abney 1989; Frank 1992; Crocker 1992).2 This approach is appealing because it reflects, intuitively, the idea of using the grammar as a set of axioms and reduces parsing to a deduction process. This is very much in the spirit of the current shift in linguistic theories from construction-dependent rules to general principles, and it separates quite clearly the grammar from the parsing algorithm. However, it is not obvious that this approach is efficient. Partial evaluation and variable substitution can increase performance, but, as usual, a space/time trade-off will ensue. Excess of partial evaluation off-line incre</context>
<context position="16794" citStr="Frank 1992" startWordPosition="2626" endWordPosition="2627">n other approaches and relates the parser to the grammar in a principled way. 2. The Proposal Two avenues have generally been pursued to build efficient GB parsers. In one case, a &amp;quot;covering grammar&amp;quot; is compiled, which overgenerates and is then filtered by constraints. The compilation is done in such a way that the overgeneration is wellbehaved. For instance, the correct distribution of empty categories is calculated off-line (Dorr 1993). In the other case, all the principles are applied on line, but they apply only to a portion of the tree, and are therefore restricted to a local computation (Frank 1992).4 My proposal combines these two approaches: it adopts the idea of compiling the grammar, at least partially, off-line but it attempts to find a principled way of doing so. In this, I differ from Dorr, where the amount of compilation is heuristic and based on practical experimentation. The approach shares Frank&apos;s intuition that linguistic principles have a form, which can be exploited in structuring the parser. This proposal is based on two observations. First, each principle of linguistic theory has a canonical form, and second, primitives of linguistic theories can be partitioned into class</context>
<context position="23381" citStr="Frank 1992" startWordPosition="3669" endWordPosition="3670">does not. In other words, a parser that takes advantage of the structure of linguistic principles will maintain a modular design based on the five classes in (3). Although the ICMH is not so stringent as to make predictions that converge on a single parsing architecture, it does provide some predictive power about the organization of the parser. First, structural information is encoded separately from lexical information. Standard context-free rules, specified with category, such as VP —AT NP, are not compatible with the ICMH, nor are proposals in the spirit of licensing grammars (Abney 1989, Frank 1992), where information is encoded in each lexical item. Second, the ICMH predicts that long-distance dependencies, represented as chains, are computed in steps. Empty categories are licensed in two computational steps: structural licensing by an appropriate head, and feature instantiation. With respect to feature instantiation in particular, it is predicted that precompiling syntactic features speeds up the parsing process. This is different from functional approaches such as Fong (1991), and Fong and Berwick (1992), in which there is no precompilation.6 These predictions seem to be supported (an</context>
<context position="26250" citStr="Frank 1992" startWordPosition="4106" endWordPosition="4107">ate precisely &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot; those features that are used to determine the well-formedness of syntactic trees. In the spirit of more recent developments in syntactic theory, I consider syntactic those features that are involved in some particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles, case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and Laenzlinger (1993), also wh, neg, adverb. 7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for 521 Computational Linguistics Volume 21, Number 4 Figure 1 Types of Sentences. In the rest of the paper, I first discuss the advantages of storing X- information separately from lexical information (section 3). I then turn to the computation of longdistance dependencies. I illustrate two algorithms to compute chains: I show that a particular use of syntactic feature information speeds up the parse, and I discuss the plausibility of using algorithms that require strict left-to-right annotation of the nodes (section 4). In fact, the algorithm I propose appears to be interesti</context>
<context position="45707" citStr="Frank (1992)" startWordPosition="7360" endWordPosition="7361">g is immediately checked by the appropriate condition on rule reductions, shown in Figure 4. Many principles regulate the distribution of chains. For the purpose of the following discussion, it is only necessary to recall that a chain can only contain one thematic position and one position that receives case. Moreover, chains divide into two types: 11 It should be noted that, although phrase-structure rules are reduced to the bare bones, they cannot be eliminated altogether. Parsers that project phrase structure and attachments entirely from the lexicon have been presented by Abney (1989) and Frank (1992), using licensing grammars (LS). They suffer from serious shortcomings when faced with ambiguous input, as they do not have enough global knowledge of the possible structures in the language to recover from erroneous parses. Abney alleviates this problem by attaching LR states to the constructed nodes, thus losing much of the initial motivation of the licensing approach. Frank&apos;s parser is augmented by a parse stack to parse head-final languages. Frank does not discuss this issue in detail, but it seems that a &amp;quot;shift&amp;quot; operation must be added to the operations of the parser. As there could alway</context>
<context position="58856" citStr="Frank 1992" startWordPosition="9534" endWordPosition="9535">r less restrictive languages, such as the British variant of English (Grimshaw 1986) and Italian (Rizzi 1982), which allow the types of extractions that Correa&apos;s limit is designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to capture restrictions on movement. Reasons of space prevent me from discussing the issues related to locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic theory, based essentially on a parametrization of locality restrictions, see Merlo 1992, to appear. 532 Paola Merlo Modularity and Information Content Classes features such as case and thematic roles when building chains leads to an exponential growth of the space of hypotheses; second, I argue that using these features does not restrict the validity of the algorithm to specific constructions or languages. 4.2 Restricting the Search Space As the previous section on phrase structure has shown, computing features is not al</context>
</contexts>
<marker>Frank, 1992</marker>
<rawString>Frank, Robert (1992). &amp;quot;Syntactic Locality and Tree Adjoining Grammar: Grammatical, Acquisition and Processing Perspectives.&amp;quot; Doctoral dissertation, University of . Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>Modularity and the Representational Hypothesis.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings of NELS</booktitle>
<pages>6--131</pages>
<contexts>
<context position="22097" citStr="Frazier (1985)" startWordPosition="3470" endWordPosition="3471">rouped into classes defined according to their content. Linguistic information can be classified into five different classes: (3) a. Configurations: sisterhood, c-command, m-command, + maximal b. Lexical features: ±N, ±V, ±Funct, ±c-selected c. Syntactic features: +Case, ±9, ±1,, +barrier, +Strong Agr d. Locality information: minimality, antecedent government e. Referential information: +anaphor, +pronominal, indices This qualitative classification forms a partitioning into natural classes based on information content. I call these IC Classes.&apos; 5 Differently from Crocker (1992, to appear) and Frazier (1985), this partitioning does not rely on the particular representation used. The spirit of the hypothesis is that linguistic theory is formed by heterogeneous types of information, and that the representation used to describe them is a derived concept. Frazier (1990) proposes an evolutionary partitioning of the parser based on tasks. This 520 Paola Merlo Modularity and Information Content Classes It can then be hypothesized that the amount of compilation (or, conversely, the modularity of the parser) is captured by the notion of IC classes as follows: IC Modularity Hypothesis (ICMH) Precompilation</context>
</contexts>
<marker>Frazier, 1985</marker>
<rawString>Frazier, Lyn (1985). &amp;quot;Modularity and the Representational Hypothesis.&amp;quot; In Proceedings of NELS /6,131-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>Exploring the Architecture of the Language Processing System.&amp;quot; In Cognitive Models of Speech Processing, edited by Gerry Altmann,</title>
<date>1990</date>
<pages>409--433</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="22360" citStr="Frazier (1990)" startWordPosition="3510" endWordPosition="3511">: +Case, ±9, ±1,, +barrier, +Strong Agr d. Locality information: minimality, antecedent government e. Referential information: +anaphor, +pronominal, indices This qualitative classification forms a partitioning into natural classes based on information content. I call these IC Classes.&apos; 5 Differently from Crocker (1992, to appear) and Frazier (1985), this partitioning does not rely on the particular representation used. The spirit of the hypothesis is that linguistic theory is formed by heterogeneous types of information, and that the representation used to describe them is a derived concept. Frazier (1990) proposes an evolutionary partitioning of the parser based on tasks. This 520 Paola Merlo Modularity and Information Content Classes It can then be hypothesized that the amount of compilation (or, conversely, the modularity of the parser) is captured by the notion of IC classes as follows: IC Modularity Hypothesis (ICMH) Precompilation within IC Classes improves efficiency. Precompilation across IC Classes does not. In other words, a parser that takes advantage of the structure of linguistic principles will maintain a modular design based on the five classes in (3). Although the ICMH is not so</context>
</contexts>
<marker>Frazier, 1990</marker>
<rawString>Frazier, Lyn (1990). &amp;quot;Exploring the Architecture of the Language Processing System.&amp;quot; In Cognitive Models of Speech Processing, edited by Gerry Altmann, MIT Press, 409-433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="14797" citStr="Gazdar et al. 1985" startWordPosition="2315" endWordPosition="2318">rammar rules, the size of the grammar increases. As Tomita (1986) points out, input length does not cause a noticeable increase in running time up to 35 to 40 input tokens. For sentences of this length, grammar size becomes a relevant factor for grammars that contain more than approximately 220 rules, in his algorithm (an LR parser with parallel stacks). Both Dorr (1987) and Tomita (1986) show experimental results confirming that there is a critical point beyond which the parser is slowed down by the increasing size of the grammar. In the Generalized Phrase Structure Grammar (GPSG) formalism (Gazdar et al. 1985), similar experiments have been performed, which confirm this result. Parsers for GPSG are particularly interesting, because they use a formalism that expresses many grammatical generalizations in 518 Paola Merlo Modularity and Information Content Classes a uniform format. Therefore, GPSG is, in principle, more amenable to being processed by known parsing techniques. Thompson (1982) finds that expanding metarules, rather than computing them on-line, is advantageous, but that instantiating the variables in the expanded rules is not. Phillips and Thompson (1985) also remark that compiling out a </context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan (1985). Generalized Phrase Structure Grammar. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Subjacency and the S/S&apos; Parameter.&amp;quot; Linguistic Inquiry.</title>
<date>1986</date>
<volume>17</volume>
<issue>2</issue>
<pages>364--369</pages>
<contexts>
<context position="58330" citStr="Grimshaw 1986" startWordPosition="9447" endWordPosition="9448">colation, Correa imposes a restriction, such that a node can only participate in one A-chain and one A-chain at a time. (The theoretical motivation for this limit is that an attribute grammar with unrestricted percolation of attributes corresponds to a type 0 grammar, thus it is too powerful to describe natural languages correctly.) Thus, some locality restrictions such as wh-islands and the Complex NP Constraint are modelled. These locality restrictions, though, depend on the language. This attribution rule does not work for less restrictive languages, such as the British variant of English (Grimshaw 1986) and Italian (Rizzi 1982), which allow the types of extractions that Correa&apos;s limit is designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to capture restrictions on movement. Reasons of space prevent me from discussing the issues related to locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic </context>
</contexts>
<marker>Grimshaw, 1986</marker>
<rawString>Grimshaw, Jane (1986). &amp;quot;Subjacency and the S/S&apos; Parameter.&amp;quot; Linguistic Inquiry. 17(2), 364-369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liliane Haegemann</author>
<author>Raffaella Zanuttini</author>
</authors>
<title>Negative Heads and the Neg Criterion.&amp;quot; The Linguistic Review,</title>
<date>1991</date>
<volume>8</volume>
<issue>2</issue>
<pages>233--251</pages>
<contexts>
<context position="26124" citStr="Haegemann and Zanuttini (1991)" startWordPosition="4082" endWordPosition="4085">e parser. In Fong&apos;s view, all computations are done on-line and the parser reflects the theory as directly as possible. 6 It is difficult to separate precisely &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot; those features that are used to determine the well-formedness of syntactic trees. In the spirit of more recent developments in syntactic theory, I consider syntactic those features that are involved in some particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles, case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and Laenzlinger (1993), also wh, neg, adverb. 7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for 521 Computational Linguistics Volume 21, Number 4 Figure 1 Types of Sentences. In the rest of the paper, I first discuss the advantages of storing X- information separately from lexical information (section 3). I then turn to the computation of longdistance dependencies. I illustrate two algorithms to compute chains: I show that a particular use of syntactic feature information speeds up the parse, and I discuss the plausibility of using algorithms t</context>
</contexts>
<marker>Haegemann, Zanuttini, 1991</marker>
<rawString>Haegemann, Liliane, and Zanuttini, Raffaella (1991). &amp;quot;Negative Heads and the Neg Criterion.&amp;quot; The Linguistic Review, 8 (2/4), 233-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Irons</author>
</authors>
<title>A Syntax Directed Compiler for ALGOL 60.&amp;quot;</title>
<date>1961</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>51--55</pages>
<contexts>
<context position="69809" citStr="Irons 1961" startWordPosition="11480" endWordPosition="11481">n LR parser. Shieber and Johnson (1993) show that by using the (set of) left contexts encoded in the LR table, interpretation can be done incrementally. I show that a similar kind of argument can be built with respect to case feature assignment: case features can be assigned to the links of a chain from left to right while scanning the input. Even if, strictly speaking, mother nodes are built only after their children are built. 20 A grammar G with productions of the sort shown in (7), i.e., a grammar that performs attribute assignment upon reduction of a rule, is called an attribute grammar (Irons 1961; Knuth 1968; Correa 1988, 1991.) An attribute grammar is S-attributed if all the attribution rules have the form A.a --* B.b C.c A.a f(B.b, C.c) }, where the attribute of the parent node is a function of the attributes of the offspring. An attribute grammar is L-attributed if all the attribution rules have the form either A.a g.b { 0.1) f(A.a)} or the form A.a 0.1) -y.c ,y.c f(B.b) 1, where the attribute of a node is a function of the attributes of a preceding node in the rule, or of the parent node. 535 Computational Linguistics Volume 21, Number 4 L-attributed rule into an S-attributed rule</context>
</contexts>
<marker>Irons, 1961</marker>
<rawString>Irons, Edgar (1961). &amp;quot;A Syntax Directed Compiler for ALGOL 60.&amp;quot; Communications of the Association for Computing Machinery, 4(1) : 51-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing as Deduction: The Use of Knowledge of Language.&amp;quot;</title>
<date>1989</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>233--251</pages>
<contexts>
<context position="11234" citStr="Johnson (1989)" startWordPosition="1757" endWordPosition="1758"> a planar graph. In fact, if independent modules are separable modules, there is little reason to think that GB is modular, as it corresponds to a highly connected graph. 2 By compilation, here and below, I mean off-line computation of some general property of the grammar, for example the off-line computation of the interaction of principles, using partial evaluation or variable substitution. 517 Computational Linguistics Volume 21, Number 4 1.2 On-line Computation is Inefficient Several researchers note that principle-based parsers allowing no grammar precompilation are inefficient. Firstly, Johnson (1989), Stabler (1990), and van de Koot (1991) note that the computation of a multi-level theory without any precompilation might not even terminate. Secondly, experimental results show that an entirely deductive approach is inefficient. Kashket (1991) discusses a principle-based parser, where no grammar precompilation is performed, and which parses English and Warlpiri using a parameterized theory of grammar. The parsing algorithm is a generate-and-test, backtracking regime. Kashket (1991) reports, for instance, that a 5-word sentence in Warlpiri (which can have 5! analyses, given the free word ord</context>
<context position="26264" citStr="Johnson 1989" startWordPosition="4108" endWordPosition="4109">y &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot; those features that are used to determine the well-formedness of syntactic trees. In the spirit of more recent developments in syntactic theory, I consider syntactic those features that are involved in some particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles, case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and Laenzlinger (1993), also wh, neg, adverb. 7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for 521 Computational Linguistics Volume 21, Number 4 Figure 1 Types of Sentences. In the rest of the paper, I first discuss the advantages of storing X- information separately from lexical information (section 3). I then turn to the computation of longdistance dependencies. I illustrate two algorithms to compute chains: I show that a particular use of syntactic feature information speeds up the parse, and I discuss the plausibility of using algorithms that require strict left-to-right annotation of the nodes (section 4). In fact, the algorithm I propose appears to be interestingly correlate</context>
</contexts>
<marker>Johnson, 1989</marker>
<rawString>Johnson, Mark (1989). &amp;quot;Parsing as Deduction: The Use of Knowledge of Language.&amp;quot; Journal of Psycholinguistic Research, 18(1), 233-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kashket</author>
</authors>
<title>A Parameterized Parser for English and Warlpiri.&amp;quot; Doctoral dissertation, MIT,</title>
<date>1991</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="11480" citStr="Kashket (1991)" startWordPosition="1792" endWordPosition="1793">al property of the grammar, for example the off-line computation of the interaction of principles, using partial evaluation or variable substitution. 517 Computational Linguistics Volume 21, Number 4 1.2 On-line Computation is Inefficient Several researchers note that principle-based parsers allowing no grammar precompilation are inefficient. Firstly, Johnson (1989), Stabler (1990), and van de Koot (1991) note that the computation of a multi-level theory without any precompilation might not even terminate. Secondly, experimental results show that an entirely deductive approach is inefficient. Kashket (1991) discusses a principle-based parser, where no grammar precompilation is performed, and which parses English and Warlpiri using a parameterized theory of grammar. The parsing algorithm is a generate-and-test, backtracking regime. Kashket (1991) reports, for instance, that a 5-word sentence in Warlpiri (which can have 5! analyses, given the free word order of the language) can take up to 40 minutes to parse. He concludes that, although no mathematical analysis for the algorithm is available, the complexity appears to increase exponentially with the input size. Fong (1991, 123) discusses a parsin</context>
<context position="55908" citStr="Kashket 1991" startWordPosition="9069" endWordPosition="9070">Linguistics Volume 21, Number 4 Who did you think el that John seemed e2 to like e3? NLAB who -A- Head CSEL who A Head [(who)] NLAB you A Head CSEL you A Head lf(who)1[(you)]1 NLAB el A Intermediate CSEL el A I [Rwho)11(you)]] R(who,e1)][(you)]] NL AB John A Head CSEL John A Head [Rwho,e1)1[(you)11 [[(who,e1)][(you)(John)I1 NLAB e2 A Foot CSEL e2 A Foot ][(who,e1)][(you)(John)]] [[(who,e1)][(you)(John,e2)]] NLAB e3 -A Foot CSEL e3 A Foot [[(who,e1)][(you)(John,e2)1I [1(who,e1,e3))1E(you)(John,e2)11 Figure 5 Chain building example. the generate-and-test nondeterminism of the theory (Fong 1991; Kashket 1991), and another that takes advantage of structural constraints to limit the space of hypotheses, and therefore is called structural determination of empty categories (Correa 1988, 1991; Crocker 1991). Of these two positions, the ICMH predicts that the latter will be more efficient: features belonging to the same class must be compiled. The algorithms proposed here amount to a &amp;quot;look-up&amp;quot; of all the relevant features that divide all empty categories into classes, as opposed to a functional algorithm, where the empty categories are all the same, and their different syntactic function is determined i</context>
</contexts>
<marker>Kashket, 1991</marker>
<rawString>Kashket, Michael (1991). &amp;quot;A Parameterized Parser for English and Warlpiri.&amp;quot; Doctoral dissertation, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kayne</author>
</authors>
<title>The Antisymmetry of Syntax.&amp;quot;</title>
<date>1994</date>
<journal>Linguistic Inquiry Monograph</journal>
<volume>25</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="68591" citStr="Kayne (1994)" startWordPosition="11283" endWordPosition="11284">ormally, this is the problem of transforming an L-attributed grammar into an Sattributed grammar.&apos; An L-attributed grammar GL can be evaluated by an LR parser if GL is transformed into a grammar Gs such that the actions that perform attribution in GL always occur at the end of a production in Gs. Although it is literally true that LR parsers can evaluate actions only on reductions (i.e., they only operate on S-attributed grammars), there are techniques to transform an 18 Alternatively, one could adopt the (linguistically radical) position that rightward movement does not exist, as proposed by Kayne (1994). Although this generalization seems to be true for head movement, Kayne&apos;s proposal is more controversial for maximal projections. Discussion of these issues falls completely outside of the topic of the present paper. 19 LR parsers have been criticized as possible models of linguistic performance because, supposedly, they cannot perform any parsing action until the end of the sentence is seen, if the structure is right-branching (see Abney 1989; Steedman 1989). Stabler (1991) argues that this criticism adopts an unnecessarily naive view of the interleaving of structure building and interpretat</context>
</contexts>
<marker>Kayne, 1994</marker>
<rawString>Kayne, Richard (1994). &amp;quot;The Antisymmetry of Syntax.&amp;quot; Linguistic Inquiry Monograph 25, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>On the Translation of Languages from Left to Right.&amp;quot;</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<pages>607--639</pages>
<contexts>
<context position="27645" citStr="Knuth 1965" startWordPosition="4329" endWordPosition="4330">f the parser, an analyzer for English was developed. Each of the data structures is the direct implementation of linguistic objects with different information contents. The input to the algorithm is an unannotated sentence. The output consists of a tree and a list of two chains: the list of A chains and the list of A chains, that is, chains formed by wh-movement and NP movement, respectively. The main parsing algorithm is a modified LR parsing algorithm augmented by multi-action entries and constraints on reduction.&apos; The structure-building component of the parser is driven by an LR(k) parser (Knuth 1965) which consults two tables. One table encodes X information (following Kornai and Pullum 1990). The other table encodes lexical information. Lexical information is consulted only if it is needed to disambiguate a state containing multiple actions in the LR parser. An overview of this design is shown in Figure 2. instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992). In formalism other than GB theory, gaps are encoded directly into the rules. Both GPSG and HPSG use slash features to percolate features to gaps. The use of slash features probably simplifies the compu</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>Knuth, Donald E. (1965). &amp;quot;On the Translation of Languages from Left to Right.&amp;quot; Information and Control, 8: 607-639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>Semantics of Context-free Languages.&amp;quot;</title>
<date>1968</date>
<journal>Mathematical Systems Theory,</journal>
<volume>2</volume>
<pages>127--145</pages>
<contexts>
<context position="69821" citStr="Knuth 1968" startWordPosition="11482" endWordPosition="11483"> Shieber and Johnson (1993) show that by using the (set of) left contexts encoded in the LR table, interpretation can be done incrementally. I show that a similar kind of argument can be built with respect to case feature assignment: case features can be assigned to the links of a chain from left to right while scanning the input. Even if, strictly speaking, mother nodes are built only after their children are built. 20 A grammar G with productions of the sort shown in (7), i.e., a grammar that performs attribute assignment upon reduction of a rule, is called an attribute grammar (Irons 1961; Knuth 1968; Correa 1988, 1991.) An attribute grammar is S-attributed if all the attribution rules have the form A.a --* B.b C.c A.a f(B.b, C.c) }, where the attribute of the parent node is a function of the attributes of the offspring. An attribute grammar is L-attributed if all the attribution rules have the form either A.a g.b { 0.1) f(A.a)} or the form A.a 0.1) -y.c ,y.c f(B.b) 1, where the attribute of a node is a function of the attributes of a preceding node in the rule, or of the parent node. 535 Computational Linguistics Volume 21, Number 4 L-attributed rule into an S-attributed rule (Aho, Sethi</context>
</contexts>
<marker>Knuth, 1968</marker>
<rawString>Knuth, Donald E. (1968). &amp;quot;Semantics of Context-free Languages.&amp;quot; Mathematical Systems Theory, 2: 127-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van de Koot</author>
</authors>
<title>Essay on the Grammar-Parser Relation. Foris,</title>
<date>1990</date>
<location>Dordrecht.</location>
<marker>van de Koot, 1990</marker>
<rawString>van de Koot, Hans (1990). Essay on the Grammar-Parser Relation. Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van de Koot</author>
</authors>
<title>Parsing with Principles: on Constraining Derivations.&amp;quot; UCL Working Papers in Linguistics,</title>
<date>1991</date>
<pages>369--396</pages>
<location>University College, London,</location>
<marker>van de Koot, 1991</marker>
<rawString>van de Koot, Hans (1991). &amp;quot;Parsing with Principles: on Constraining Derivations.&amp;quot; UCL Working Papers in Linguistics, University College, London, 369-396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrais Kornai</author>
<author>Geoffrey Pullum</author>
</authors>
<title>The X-bar Theory of Phrase Structure.&amp;quot;</title>
<date>1990</date>
<journal>Language,</journal>
<pages>66--24</pages>
<contexts>
<context position="27739" citStr="Kornai and Pullum 1990" startWordPosition="4341" endWordPosition="4344">s the direct implementation of linguistic objects with different information contents. The input to the algorithm is an unannotated sentence. The output consists of a tree and a list of two chains: the list of A chains and the list of A chains, that is, chains formed by wh-movement and NP movement, respectively. The main parsing algorithm is a modified LR parsing algorithm augmented by multi-action entries and constraints on reduction.&apos; The structure-building component of the parser is driven by an LR(k) parser (Knuth 1965) which consults two tables. One table encodes X information (following Kornai and Pullum 1990). The other table encodes lexical information. Lexical information is consulted only if it is needed to disambiguate a state containing multiple actions in the LR parser. An overview of this design is shown in Figure 2. instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992). In formalism other than GB theory, gaps are encoded directly into the rules. Both GPSG and HPSG use slash features to percolate features to gaps. The use of slash features probably simplifies the computation. There has been a debate on the explanatory adequacy of grammars that employ slash feat</context>
</contexts>
<marker>Kornai, Pullum, 1990</marker>
<rawString>Kornai, Andrais, and Pullum, Geoffrey (1990). &amp;quot;The X-bar Theory of Phrase Structure.&amp;quot; Language, 66,24-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Laenzlinger</author>
</authors>
<title>Principles for a Formal Account of Adverb Syntax.&amp;quot;</title>
<date>1993</date>
<journal>Geneva Generative Papers,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>47--75</pages>
<institution>University of Geneva,</institution>
<contexts>
<context position="26148" citStr="Laenzlinger (1993)" startWordPosition="4087" endWordPosition="4088">ations are done on-line and the parser reflects the theory as directly as possible. 6 It is difficult to separate precisely &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot; those features that are used to determine the well-formedness of syntactic trees. In the spirit of more recent developments in syntactic theory, I consider syntactic those features that are involved in some particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles, case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and Laenzlinger (1993), also wh, neg, adverb. 7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for 521 Computational Linguistics Volume 21, Number 4 Figure 1 Types of Sentences. In the rest of the paper, I first discuss the advantages of storing X- information separately from lexical information (section 3). I then turn to the computation of longdistance dependencies. I illustrate two algorithms to compute chains: I show that a particular use of syntactic feature information speeds up the parse, and I discuss the plausibility of using algorithms that require strict left-</context>
<context position="73811" citStr="Laenzlinger (1993)" startWordPosition="12145" endWordPosition="12146">re complex. There are at least two possible tacks. First, one can notice that adverbs, although they are analysed as maximal projections because they can be modified, never take a complement, thus they are usually limited to a very short sequence of words, and they do not have a recursive structure. A minimum amount of lookahead, even limited to these particular instances of aspectual adverbs, would solve the problem. Clearly, this is an inelegant solution. A more principled treatment comes from recent developments in the theory, that have changed somewhat the representation used for adverbs. Laenzlinger (1993) suggests that all maximal projections have two specifiers, one A and one A, the higher of the two is the A-position, which can be occupied by adverbs, if they are licensed by the appropriate head (the Adv-Criterion). For these adverbs, the appropriate head is Aspo which we find only with finite verbs. The parser could compile this information and assign case directly, without even waiting to see the (lexical) verb. 536 Paola Merlo Modularity and Information Content Classes Because of property (9a), case could not be inferred from explicit information contained in the input (unlike Japanese or</context>
</contexts>
<marker>Laenzlinger, 1993</marker>
<rawString>Laenzlinger, Christopher (1993). &amp;quot;Principles for a Formal Account of Adverb Syntax.&amp;quot; Geneva Generative Papers, 1(2), University of Geneva, 47-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="58812" citStr="Marcus 1980" startWordPosition="9528" endWordPosition="9529">guage. This attribution rule does not work for less restrictive languages, such as the British variant of English (Grimshaw 1986) and Italian (Rizzi 1982), which allow the types of extractions that Correa&apos;s limit is designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to capture restrictions on movement. Reasons of space prevent me from discussing the issues related to locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic theory, based essentially on a parametrization of locality restrictions, see Merlo 1992, to appear. 532 Paola Merlo Modularity and Information Content Classes features such as case and thematic roles when building chains leads to an exponential growth of the space of hypotheses; second, I argue that using these features does not restrict the validity of the algorithm to specific constructions or languages. 4.2 Restricting the Search Space As the previous section on phrase struc</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, Mitchell (1980). A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
</authors>
<title>On Modularity and Compilation in a Government and Binding Parser.&amp;quot; Doctoral dissertation,</title>
<date>1992</date>
<institution>University of Maryland, College Park, MD.</institution>
<contexts>
<context position="18901" citStr="Merlo 1992" startWordPosition="2974" endWordPosition="2975">f phrase-structure and locality constraints, so that locality is not computed in the course of the parse, but basically done as template matching. Second, in the measure of complexity, Frank does not count the cost of choosing which elementary tree to unadjoin or unsubstitute, or the cost of backtracking if the wrong decision is made. There are indeed cases where, in order to perform the correct operation, more than one elementary tree must be spanned. It is not clear that linear time complexity can actually be claimed if all factors are taken into account. For a more detailed discussion, see Merlo 1992, to appear. 519 Computational Linguistics Volume 21, Number 4 2. the category of H E {A, Agr, N, P. T, V} 3. there is no barrier or head H&apos; that intervenes between H and x It can be observed that this principle has an internal structure and can be decomposed into separate pieces of information: (2.1) imposes a condition on configurations, namely, a condition on the shape of the tree; (2.2) imposes a condition on the labelling of the nodes in the tree; and (2.3) imposes a locality condition, as it defines the subtree available for the computation of the principle. These three conditions are in</context>
<context position="51657" citStr="Merlo 1992" startWordPosition="8364" endWordPosition="8365">e, an argument chain (A-chain) is started, as in passives. The last four cases deal with empty categories. The feature 13 Strictly speaking, it must also provide a rescuing procedure. This can be done by checking whether all the chains satisfy the well-formedness. conditions. If not all the chains satisfy the well-formedness constraints, the parser can attempt to intersect or compose two or more chains in order to satisfy the well-formedness conditions. These two problems are not treated here. For an illustration, under the name of Chain Intersection Problem and Chain Composition Problem, see Merlo 1992. 14 I present here a simplified version of the algorithm, to avoid technical linguistic details, which are not relevant for the following discussion. However, one should also output a label A0p, which designates the empty operator that binds, for instance, the empty variable in a parasitic gap construction and other cases of non-overt movement, such as relative clauses. In the man OP I saw an empty operator is postulated by analogy to the man whom/that I saw. A0p is licensed by the same conditions that license an intermediate A trace. 530 Paola Merlo Modularity and Information Content Classes</context>
<context position="59017" citStr="Merlo 1992" startWordPosition="9556" endWordPosition="9557"> limit is designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to capture restrictions on movement. Reasons of space prevent me from discussing the issues related to locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic theory, based essentially on a parametrization of locality restrictions, see Merlo 1992, to appear. 532 Paola Merlo Modularity and Information Content Classes features such as case and thematic roles when building chains leads to an exponential growth of the space of hypotheses; second, I argue that using these features does not restrict the validity of the algorithm to specific constructions or languages. 4.2 Restricting the Search Space As the previous section on phrase structure has shown, computing features is not always profitable, as some features reduce the search space while others do not. To see that checking features does indeed pay off, the cost of checking these feat</context>
</contexts>
<marker>Merlo, 1992</marker>
<rawString>Merlo, Paola (1992). &amp;quot;On Modularity and Compilation in a Government and Binding Parser.&amp;quot; Doctoral dissertation, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paola Merlo</author>
</authors>
<title>(to appear). Parsing with Principles and Classes of Information.</title>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<marker>Merlo, </marker>
<rawString>Merlo, Paola (to appear). Parsing with Principles and Classes of Information. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Phillips</author>
</authors>
<title>A Computational Representation for Generalized Phrase Structure Grammars.&amp;quot;</title>
<date>1992</date>
<journal>Linguistics and Philosophy,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>255--287</pages>
<contexts>
<context position="15544" citStr="Phillips (1992)" startWordPosition="2423" endWordPosition="2424">e a formalism that expresses many grammatical generalizations in 518 Paola Merlo Modularity and Information Content Classes a uniform format. Therefore, GPSG is, in principle, more amenable to being processed by known parsing techniques. Thompson (1982) finds that expanding metarules, rather than computing them on-line, is advantageous, but that instantiating the variables in the expanded rules is not. Phillips and Thompson (1985) also remark that compiling out a grammar of twenty-nine phrase-structure rules and four metarules is equivalent to &amp;quot;several tens of millions of context-free rules.&amp;quot; Phillips (1992) proposes a modification to GPSG that makes it easier to parse, by using propagation rules, but still notes that variables should not be expanded. In conclusion, the lesson from experimentation is that parsing done totally on-line is inefficient, but that compilation is not always a solution. A parser that uses linguistic principles directly must fulfill apparently contradictory demands: for the parser to be linguistically valid it must use the grammar directly, while a limited amount of off-line precompilation might make the parser more efficient.&apos; In the next section, I propose and discuss a</context>
</contexts>
<marker>Phillips, 1992</marker>
<rawString>Phillips, John (1992). &amp;quot;A Computational Representation for Generalized Phrase Structure Grammars.&amp;quot; Linguistics and Philosophy, 15(3): 255-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Phillips</author>
<author>Henry Thompson</author>
</authors>
<title>GPSGP: A Parser for Generalized Phrase Structure Grammars.&amp;quot;</title>
<date>1985</date>
<journal>Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>245--262</pages>
<contexts>
<context position="15363" citStr="Phillips and Thompson (1985)" startWordPosition="2395" endWordPosition="2398">Phrase Structure Grammar (GPSG) formalism (Gazdar et al. 1985), similar experiments have been performed, which confirm this result. Parsers for GPSG are particularly interesting, because they use a formalism that expresses many grammatical generalizations in 518 Paola Merlo Modularity and Information Content Classes a uniform format. Therefore, GPSG is, in principle, more amenable to being processed by known parsing techniques. Thompson (1982) finds that expanding metarules, rather than computing them on-line, is advantageous, but that instantiating the variables in the expanded rules is not. Phillips and Thompson (1985) also remark that compiling out a grammar of twenty-nine phrase-structure rules and four metarules is equivalent to &amp;quot;several tens of millions of context-free rules.&amp;quot; Phillips (1992) proposes a modification to GPSG that makes it easier to parse, by using propagation rules, but still notes that variables should not be expanded. In conclusion, the lesson from experimentation is that parsing done totally on-line is inefficient, but that compilation is not always a solution. A parser that uses linguistic principles directly must fulfill apparently contradictory demands: for the parser to be linguis</context>
</contexts>
<marker>Phillips, Thompson, 1985</marker>
<rawString>Phillips, John, and Thompson, Henry (1985). &amp;quot;GPSGP: A Parser for Generalized Phrase Structure Grammars.&amp;quot; Linguistics, 23(2), 245-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Rizzi</author>
</authors>
<date>1982</date>
<booktitle>Issues in Italian Syntax. Foris,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="58355" citStr="Rizzi 1982" startWordPosition="9451" endWordPosition="9452">estriction, such that a node can only participate in one A-chain and one A-chain at a time. (The theoretical motivation for this limit is that an attribute grammar with unrestricted percolation of attributes corresponds to a type 0 grammar, thus it is too powerful to describe natural languages correctly.) Thus, some locality restrictions such as wh-islands and the Complex NP Constraint are modelled. These locality restrictions, though, depend on the language. This attribution rule does not work for less restrictive languages, such as the British variant of English (Grimshaw 1986) and Italian (Rizzi 1982), which allow the types of extractions that Correa&apos;s limit is designed to exclude. Finally, as Correa himself notes, this attribution would not work for parasitic gaps in English. Thus, imposing a one-slot limit to the propagation of A chains is not the right way to capture restrictions on movement. Reasons of space prevent me from discussing the issues related to locality here, which have been the topic of debate for a rather long time (see Marcus 1980; Berwick and Weinberg 1984,1985; Frank 1992 and references therein). For a proposal which follows current linguistic theory, based essentially</context>
</contexts>
<marker>Rizzi, 1982</marker>
<rawString>Rizzi, Luigi (1982). Issues in Italian Syntax. Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Rizzi</author>
</authors>
<title>Relativized Minimality.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3734" citStr="Rizzi 1990" startWordPosition="558" endWordPosition="559">d C) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 observation that humans make use of their knowledge of language very effectively. In this paper, I investigate the computational problem related to the tension between building linguistically based parsers and building efficient ones, which, I argue, derives from the particular forms linguistic theories have taken recently. In particular, I explore the issue of what is a good parsing technique to apply to principle-based theories of grammar. I take Government-Binding (GB) theory (Chomsky 1986a,b; Rizzi 1990) to be a suitable illustration of such theories, and also to show in all clarity the problems that might arise. I differ from other investigations on the import of principle-based parsing in not drawing on cognitive issues or psycholinguistic results to justify my assumptions. Indeed, part of the spirit of this work is to explore how far one can go in advocating principle-based parsing, in the absence of motivations given by cognitive modelling. 1.1 The Problem When generative grammatical theory in the &apos;70s talked about &amp;quot;dative shift,&amp;quot; &amp;quot;topicalization,&amp;quot; &amp;quot;passive,&amp;quot; it meant that each of these c</context>
<context position="17650" citStr="Rizzi 1990" startWordPosition="2761" endWordPosition="2762">c and based on practical experimentation. The approach shares Frank&apos;s intuition that linguistic principles have a form, which can be exploited in structuring the parser. This proposal is based on two observations. First, each principle of linguistic theory has a canonical form, and second, primitives of linguistic theories can be partitioned into classes, based on their content. As an illustration of the first observation, we can look at the principle that regulates the distribution of the empty categories in the phrase marker, the Empty Category Principle (ECP), as stated below (adapted from Rizzi 1990, 25). (2) The Empty Category Principle An empty category x is licensed if the 3 following conditions are satisfied: 1. x is in the domain of a head H 3 For CF parsers, just how much compilation speeds up the parser is defined precisely by the analysis of the algorithm. No such precise analysis is available for principle-based algorithms. 4 Frank (1992) presents a parsing model that is claimed not to allow any compilation of the linguistic theory, and to operate in linear time. Two objections can be raised to these claims: first, the use of TAG elementary trees to restrict the working space of</context>
<context position="64974" citStr="Rizzi 1990" startWordPosition="10660" endWordPosition="10661">ation to explore using NLAB&amp;quot;. Clearly, checking features and using them for building chains, and keeping the hypothesis search space small, is beneficial in most cases. Extensibility. These algorithms deal in detail with the somewhat neglected problem of what to do when more than one chain has to be constructed. They do not discuss specifically the issues of adjunction or rightward movement. However, they could be extended. In the unextended algorithm, the postulation and structural licensing of empty categories is always performed by the same mechanism. According to the ECP (as formulated in Rizzi 1990, 25; Cinque 1990; Chomsky 1986b, among others), for an empty category to be licensed, two conditions must be satisfied: the empty category must be within the maximal projection of a lexical head to be licensed structurally, and it must be identified by an antecedent. The structural licenser and the antecedent need not be the same element. In fact, they hardly ever are. Whether movement is to the left or to the right does not affect structural licensing (which is here performed by the conditions that apply to the reduction of an c-rule). Rightward movement requires an extension of the algorith</context>
</contexts>
<marker>Rizzi, 1990</marker>
<rawString>Rizzi, Luigi (1990). Relativized Minimality. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Rizzi</author>
</authors>
<title>Residual Verb Second and the Wh-Criterion,&amp;quot; Technical Reports</title>
<date>1991</date>
<booktitle>in Formal and Computational Linguistics,</booktitle>
<volume>2</volume>
<institution>University of Geneva, Geneva.</institution>
<contexts>
<context position="20611" citStr="Rizzi 1991" startWordPosition="3256" endWordPosition="3257">principles of the grammar that are involved in building structure and annotating the phrase marker, one finds the same internal organization. Theta-assignment occurs in the configuration of sisterhood, it requires a 0-assigning head, and it must occur between a node and its most local assigner. Assignment of Case occurs in a given configuration (according to Chomsky (1988, 1992) it is always a specifier-head configuration), given a certain lexical property of the head a-ND, and locally, within the same maximal projection). The same restriction occurs again for what is called the wh-criterion (Rizzi 1991), which regulates wh-movement, where the head must have a +wh feature and occur within a specifier-head configuration. Categorial selection and functional selection also occur under the same restrictions, in the complement configuration (i.e., between a head and a maximal projection). The licensing of subjects in the phrase marker, done by predication, must occur in the specifier-head configuration. The licensing of the empty category pro also requires the inflectional head of the sentence to bear the feature Strong Agr, and it occurs in the specifier-head configuration. The assignment of the </context>
<context position="26092" citStr="Rizzi (1991)" startWordPosition="4080" endWordPosition="4081">e design of the parser. In Fong&apos;s view, all computations are done on-line and the parser reflects the theory as directly as possible. 6 It is difficult to separate precisely &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot; those features that are used to determine the well-formedness of syntactic trees. In the spirit of more recent developments in syntactic theory, I consider syntactic those features that are involved in some particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles, case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and Laenzlinger (1993), also wh, neg, adverb. 7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for 521 Computational Linguistics Volume 21, Number 4 Figure 1 Types of Sentences. In the rest of the paper, I first discuss the advantages of storing X- information separately from lexical information (section 3). I then turn to the computation of longdistance dependencies. I illustrate two algorithms to compute chains: I show that a particular use of syntactic feature information speeds up the parse, and I discuss the pl</context>
</contexts>
<marker>Rizzi, 1991</marker>
<rawString>Rizzi, Luigi (1991). &amp;quot;Residual Verb Second and the Wh-Criterion,&amp;quot; Technical Reports in Formal and Computational Linguistics, 2, University of Geneva, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Polynomial Time and Space Shift-reduce Parsing of Arbitrary Context-free Grammars.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 29th Meeting of the ACL,</booktitle>
<pages>106--113</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="28970" citStr="Schabes 1991" startWordPosition="4548" endWordPosition="4549">oot 1990, and Stabler 1994). For my purposes, note that, if anything, I am dealing with the worst case for the parser. 8 The ICMH is not sufficient to predict a specific parsing architecture, but rather it loosely dictates the organization of the parser. The choice of an LR parser then is the result of the ICMH (with which the parser&apos;s organization must be compatible) and additional independent factors. First, LR parsers have the valid prefix property, namely they recognize that a string is not in the language as soon as possible (other parsing methods have this property as well, for instance Schabes 1991). A parser with this property is incremental, in the sense that it does not perform unnecessary work, and it fails as soon as an error occurs. Second, the stack of an LR parser encodes the notion of c-command implicitly. This is TYPE EXAMPLE 1 Simple Transitive 2 Simple Intransitive 3 Simple Passive 4 Simple Raising 5 Embedded Transitive 6 Embedded Intransitive 7 Embedded Raising 8 Simple Question 9 Embedded Question 10 Embedded Question and Raising 11 Embedded Wh-Question john loves mary john runs mary was loved mary seems to like john john thinks that mary loves bill john thinks that mary ru</context>
</contexts>
<marker>Schabes, 1991</marker>
<rawString>Schabes, Yves (1991). &amp;quot;Polynomial Time and Space Shift-reduce Parsing of Arbitrary Context-free Grammars.&amp;quot; In Proceedings, 29th Meeting of the ACL, Berkeley, CA, 106-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Mark Johnson</author>
</authors>
<title>Variations on Incremental Interpretations.&amp;quot;</title>
<date>1993</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>287--319</pages>
<contexts>
<context position="69238" citStr="Shieber and Johnson (1993)" startWordPosition="11378" endWordPosition="11381">alization seems to be true for head movement, Kayne&apos;s proposal is more controversial for maximal projections. Discussion of these issues falls completely outside of the topic of the present paper. 19 LR parsers have been criticized as possible models of linguistic performance because, supposedly, they cannot perform any parsing action until the end of the sentence is seen, if the structure is right-branching (see Abney 1989; Steedman 1989). Stabler (1991) argues that this criticism adopts an unnecessarily naive view of the interleaving of structure building and interpretation in an LR parser. Shieber and Johnson (1993) show that by using the (set of) left contexts encoded in the LR table, interpretation can be done incrementally. I show that a similar kind of argument can be built with respect to case feature assignment: case features can be assigned to the links of a chain from left to right while scanning the input. Even if, strictly speaking, mother nodes are built only after their children are built. 20 A grammar G with productions of the sort shown in (7), i.e., a grammar that performs attribute assignment upon reduction of a rule, is called an attribute grammar (Irons 1961; Knuth 1968; Correa 1988, 19</context>
</contexts>
<marker>Shieber, Johnson, 1993</marker>
<rawString>Shieber, Stuart M., and Johnson, Mark (1993). &amp;quot;Variations on Incremental Interpretations.&amp;quot; Journal of Psycholinguistic Research, 22(2), 287-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Shopen</author>
</authors>
<title>Language Typology and Syntactic Description.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="75118" citStr="Shopen 1985" startWordPosition="12353" endWordPosition="12354">takably signalled (unlike German but like Japanese); because of property (9c), the inflectional head would occur after the NP that needs to be assigned case; finally, because of property (9d), an LR parser could give worst-case results (which is not the case for verb-final, but left-branching, languages, like Japanese): it could require the entire sentence to be stacked before starting to assemble it. Although a problem in principle, this limitation disappears in practice. Inspection of some of the sources on language typology shows that such languages are very difficult to find (Steele 1978; Shopen 1985; Comrie 1981). According to Downing (1978), verb-final languages usually have prenominal relative clauses, which is a sign that they are left branching. Only two verb-final languages have postnominal relative clauses, Persian and Turkish. In Persian, the clause boundary is overtly marked by the suffix -i on the antecedent. Moreover, both languages have overt case marking of the subject. Although this is by no means definitive evidence, it suggests that the algorithm for chain formation and feature assignment that I have presented is not obviously inadequate, and that it is applicable to a var</context>
</contexts>
<marker>Shopen, 1985</marker>
<rawString>Shopen, Timothy (1985). Language Typology and Syntactic Description. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Sportiche</author>
</authors>
<title>Clitic Constructions.&amp;quot;</title>
<date>1992</date>
<tech>Manuscript, UCLA.</tech>
<contexts>
<context position="26006" citStr="Sportiche 1992" startWordPosition="4068" endWordPosition="4069">f principles that are the direct translation of the theory, and not a way of defining the design of the parser. In Fong&apos;s view, all computations are done on-line and the parser reflects the theory as directly as possible. 6 It is difficult to separate precisely &amp;quot;lexical&amp;quot; from &amp;quot;syntactic&amp;quot; features. One can consider &amp;quot;syntactic&amp;quot; those features that are used to determine the well-formedness of syntactic trees. In the spirit of more recent developments in syntactic theory, I consider syntactic those features that are involved in some particular incarnation of the &amp;quot;Generalized Licensing Condition&amp;quot; (Sportiche 1992.) These include 0-roles, case, (possibly all 0 features), and, following Rizzi (1991), Haegemann and Zanuttini (1991), and Laenzlinger (1993), also wh, neg, adverb. 7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for 521 Computational Linguistics Volume 21, Number 4 Figure 1 Types of Sentences. In the rest of the paper, I first discuss the advantages of storing X- information separately from lexical information (section 3). I then turn to the computation of longdistance dependencies. I illustrate two algorithms to compute chains: I show that a par</context>
</contexts>
<marker>Sportiche, 1992</marker>
<rawString>Sportiche, Dominique (1992). &amp;quot;Clitic Constructions.&amp;quot; Manuscript, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
</authors>
<title>Relaxation Techniques for Principle-based Parsing.&amp;quot;</title>
<date>1990</date>
<tech>Manuscript, UCLA.</tech>
<contexts>
<context position="11250" citStr="Stabler (1990)" startWordPosition="1759" endWordPosition="1760"> In fact, if independent modules are separable modules, there is little reason to think that GB is modular, as it corresponds to a highly connected graph. 2 By compilation, here and below, I mean off-line computation of some general property of the grammar, for example the off-line computation of the interaction of principles, using partial evaluation or variable substitution. 517 Computational Linguistics Volume 21, Number 4 1.2 On-line Computation is Inefficient Several researchers note that principle-based parsers allowing no grammar precompilation are inefficient. Firstly, Johnson (1989), Stabler (1990), and van de Koot (1991) note that the computation of a multi-level theory without any precompilation might not even terminate. Secondly, experimental results show that an entirely deductive approach is inefficient. Kashket (1991) discusses a principle-based parser, where no grammar precompilation is performed, and which parses English and Warlpiri using a parameterized theory of grammar. The parsing algorithm is a generate-and-test, backtracking regime. Kashket (1991) reports, for instance, that a 5-word sentence in Warlpiri (which can have 5! analyses, given the free word order of the langua</context>
</contexts>
<marker>Stabler, 1990</marker>
<rawString>Stabler, Edward (1990). &amp;quot;Relaxation Techniques for Principle-based Parsing.&amp;quot; Manuscript, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
</authors>
<title>Avoid the Pedestrian&apos;s Paradox.&amp;quot; In Principle-Based Parsing, edited by</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="69071" citStr="Stabler (1991)" startWordPosition="11355" endWordPosition="11356">ternatively, one could adopt the (linguistically radical) position that rightward movement does not exist, as proposed by Kayne (1994). Although this generalization seems to be true for head movement, Kayne&apos;s proposal is more controversial for maximal projections. Discussion of these issues falls completely outside of the topic of the present paper. 19 LR parsers have been criticized as possible models of linguistic performance because, supposedly, they cannot perform any parsing action until the end of the sentence is seen, if the structure is right-branching (see Abney 1989; Steedman 1989). Stabler (1991) argues that this criticism adopts an unnecessarily naive view of the interleaving of structure building and interpretation in an LR parser. Shieber and Johnson (1993) show that by using the (set of) left contexts encoded in the LR table, interpretation can be done incrementally. I show that a similar kind of argument can be built with respect to case feature assignment: case features can be assigned to the links of a chain from left to right while scanning the input. Even if, strictly speaking, mother nodes are built only after their children are built. 20 A grammar G with productions of the </context>
</contexts>
<marker>Stabler, 1991</marker>
<rawString>Stabler, Edward (1991). &amp;quot;Avoid the Pedestrian&apos;s Paradox.&amp;quot; In Principle-Based Parsing, edited by Robert Berwick, Steven Abney, and Carol Tenny. Kluwer Academic Publishers, Dordrecht, 199-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
</authors>
<title>The Logical Approach to Syntax.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8895" citStr="Stabler 1992" startWordPosition="1406" endWordPosition="1407"> particular grammar, which would not hold under counterfactual changes, are not used. Otherwise, the covering grammar would not be sufficiently general. A faithful implementation is particularly difficult in the GB framework, as GB principles are informally expressed as English statements, and can take a variety of forms. For example, X theory (a condition on graphs), the Case Filter (an output filter on strings), and the 0 criterion (a bijection relation on predicates and arguments) all fall under the label of principles. Attempts have been made to formalize GB principles to a set of axioms (Stabler 1992). One possible, extreme interpretation of the direct use of principles is an approach where no grammar compilation is allowed (Abney 1989; Frank 1992; Crocker 1992).2 This approach is appealing because it reflects, intuitively, the idea of using the grammar as a set of axioms and reduces parsing to a deduction process. This is very much in the spirit of the current shift in linguistic theories from construction-dependent rules to general principles, and it separates quite clearly the grammar from the parsing algorithm. However, it is not obvious that this approach is efficient. Partial evaluat</context>
</contexts>
<marker>Stabler, 1992</marker>
<rawString>Stabler, Edward (1992). The Logical Approach to Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
</authors>
<title>The Finite Connectivity of Linguistic Structure.&amp;quot; In Perspectives on Sentence Processing, edited by Charles</title>
<date>1994</date>
<pages>303--336</pages>
<location>Clifton, Lyn</location>
<contexts>
<context position="28384" citStr="Stabler 1994" startWordPosition="4449" endWordPosition="4450">l information. Lexical information is consulted only if it is needed to disambiguate a state containing multiple actions in the LR parser. An overview of this design is shown in Figure 2. instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992). In formalism other than GB theory, gaps are encoded directly into the rules. Both GPSG and HPSG use slash features to percolate features to gaps. The use of slash features probably simplifies the computation. There has been a debate on the explanatory adequacy of grammars that employ slash features (see van de Koot 1990, and Stabler 1994). For my purposes, note that, if anything, I am dealing with the worst case for the parser. 8 The ICMH is not sufficient to predict a specific parsing architecture, but rather it loosely dictates the organization of the parser. The choice of an LR parser then is the result of the ICMH (with which the parser&apos;s organization must be compatible) and additional independent factors. First, LR parsers have the valid prefix property, namely they recognize that a string is not in the language as soon as possible (other parsing methods have this property as well, for instance Schabes 1991). A parser wit</context>
</contexts>
<marker>Stabler, 1994</marker>
<rawString>Stabler, Edward (1994). &amp;quot;The Finite Connectivity of Linguistic Structure.&amp;quot; In Perspectives on Sentence Processing, edited by Charles Clifton, Lyn Frazier, and Keith Rayner. Lawrence Erlbaum, Hillsdale, NJ, 303-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Grammar, Interpretation and Processing from the Lexicon.&amp;quot; In Lexical Representation and Processes, edited by William Marslen-Wilson.</title>
<date>1989</date>
<pages>463--504</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="69055" citStr="Steedman 1989" startWordPosition="11353" endWordPosition="11354">ansform an 18 Alternatively, one could adopt the (linguistically radical) position that rightward movement does not exist, as proposed by Kayne (1994). Although this generalization seems to be true for head movement, Kayne&apos;s proposal is more controversial for maximal projections. Discussion of these issues falls completely outside of the topic of the present paper. 19 LR parsers have been criticized as possible models of linguistic performance because, supposedly, they cannot perform any parsing action until the end of the sentence is seen, if the structure is right-branching (see Abney 1989; Steedman 1989). Stabler (1991) argues that this criticism adopts an unnecessarily naive view of the interleaving of structure building and interpretation in an LR parser. Shieber and Johnson (1993) show that by using the (set of) left contexts encoded in the LR table, interpretation can be done incrementally. I show that a similar kind of argument can be built with respect to case feature assignment: case features can be assigned to the links of a chain from left to right while scanning the input. Even if, strictly speaking, mother nodes are built only after their children are built. 20 A grammar G with pro</context>
</contexts>
<marker>Steedman, 1989</marker>
<rawString>Steedman, Mark (1989). &amp;quot;Grammar, Interpretation and Processing from the Lexicon.&amp;quot; In Lexical Representation and Processes, edited by William Marslen-Wilson. MIT Press, Cambridge, MA, 463-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Steele</author>
</authors>
<title>Word Order Variation.&amp;quot; In Universals of Human Language,</title>
<date>1978</date>
<pages>585--623</pages>
<publisher>Stanford University Press,</publisher>
<note>edited by</note>
<contexts>
<context position="75105" citStr="Steele 1978" startWordPosition="12351" endWordPosition="12352"> not be unmistakably signalled (unlike German but like Japanese); because of property (9c), the inflectional head would occur after the NP that needs to be assigned case; finally, because of property (9d), an LR parser could give worst-case results (which is not the case for verb-final, but left-branching, languages, like Japanese): it could require the entire sentence to be stacked before starting to assemble it. Although a problem in principle, this limitation disappears in practice. Inspection of some of the sources on language typology shows that such languages are very difficult to find (Steele 1978; Shopen 1985; Comrie 1981). According to Downing (1978), verb-final languages usually have prenominal relative clauses, which is a sign that they are left branching. Only two verb-final languages have postnominal relative clauses, Persian and Turkish. In Persian, the clause boundary is overtly marked by the suffix -i on the antecedent. Moreover, both languages have overt case marking of the subject. Although this is by no means definitive evidence, it suggests that the algorithm for chain formation and feature assignment that I have presented is not obviously inadequate, and that it is applic</context>
</contexts>
<marker>Steele, 1978</marker>
<rawString>Steele, Susan (1978). &amp;quot;Word Order Variation.&amp;quot; In Universals of Human Language, edited by Joseph H. Greenberg. Stanford University Press, 585-623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Thompson</author>
</authors>
<title>Handling Metarules in a Parser for GPSG.&amp;quot;</title>
<date>1982</date>
<tech>Research Paper 175,</tech>
<institution>Department of Artificial Intelligence, University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="15182" citStr="Thompson (1982)" startWordPosition="2371" endWordPosition="2372">6) show experimental results confirming that there is a critical point beyond which the parser is slowed down by the increasing size of the grammar. In the Generalized Phrase Structure Grammar (GPSG) formalism (Gazdar et al. 1985), similar experiments have been performed, which confirm this result. Parsers for GPSG are particularly interesting, because they use a formalism that expresses many grammatical generalizations in 518 Paola Merlo Modularity and Information Content Classes a uniform format. Therefore, GPSG is, in principle, more amenable to being processed by known parsing techniques. Thompson (1982) finds that expanding metarules, rather than computing them on-line, is advantageous, but that instantiating the variables in the expanded rules is not. Phillips and Thompson (1985) also remark that compiling out a grammar of twenty-nine phrase-structure rules and four metarules is equivalent to &amp;quot;several tens of millions of context-free rules.&amp;quot; Phillips (1992) proposes a modification to GPSG that makes it easier to parse, by using propagation rules, but still notes that variables should not be expanded. In conclusion, the lesson from experimentation is that parsing done totally on-line is inef</context>
</contexts>
<marker>Thompson, 1982</marker>
<rawString>Thompson, Henry (1982). &amp;quot;Handling Metarules in a Parser for GPSG.&amp;quot; Research Paper 175, Department of Artificial Intelligence, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwer,</publisher>
<location>Hingham, MA.</location>
<contexts>
<context position="14243" citStr="Tomita (1986)" startWordPosition="2225" endWordPosition="2226">g, however. Experimentation with different amounts of precompilation shows that offline precompilation speeds up parsing only up to a certain point, and that too much precompilation slows down the parser again. The logic of why this happens is clear. The complexity of a parsing algorithm is a composite function of the length of the input and the size of the grammar. For the kind of input lengths that are relevant for natural language, the size of the grammar easily becomes the predominant factor. If principles are precompiled in the form of grammar rules, the size of the grammar increases. As Tomita (1986) points out, input length does not cause a noticeable increase in running time up to 35 to 40 input tokens. For sentences of this length, grammar size becomes a relevant factor for grammars that contain more than approximately 220 rules, in his algorithm (an LR parser with parallel stacks). Both Dorr (1987) and Tomita (1986) show experimental results confirming that there is a critical point beyond which the parser is slowed down by the increasing size of the grammar. In the Generalized Phrase Structure Grammar (GPSG) formalism (Gazdar et al. 1985), similar experiments have been performed, whi</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Tomita, Masaru (1986). Efficient Parsing for Natural Language. Kluwer, Hingham, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>