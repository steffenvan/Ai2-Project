<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008958">
<title confidence="0.994226">
The PEACE SLDS understanding evaluation paradigm of the French
MEDIA campaign
</title>
<author confidence="0.956814">
Laurence Devillers, H´el`ene Maynard, Patrick Paroubek, Sophie Rosset
</author>
<affiliation confidence="0.848238">
LIMSI-CNRS
</affiliation>
<address confidence="0.745072">
Bt 508 University of Paris XI - BP 133 F-91403 ORSAY Cedex, France
</address>
<email confidence="0.999322">
{devil,hbm,pap,rossetg@limsi.fr
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993527027027">
This paper presents a paradigm for
evaluating the context-sensitive under-
standing capability of any spoken lan-
guage dialog system: PEACE (French
acronym for Paradigme d’Evaluation
Automatique de la Compr´ehension hors
et En-contexte). This paradigm will be
the basis of the French Technolangue
MEDIA project, in which dialog sys-
tems from various academic and indus-
trial sites will be tested in an evaluation
campaign coordinated by ELRA/ELDA
(over the next two years). Despite pre-
vious efforts such as EAGLES, DISC,
AUPELF ARCB2 or the ongoing Ameri-
can DARPA COMMUNICATOR project,
the spoken dialog community still lacks
common reference tasks and widely
agreed upon methods for comparing
and diagnosing systems and techniques.
Automatic solutions are nowadays be-
ing sought both to make possible the
comparison of different approaches by
means of reliable indicators with generic
evaluation methodologies and also to re-
duce system development costs. How-
ever achieving independence from both
the dialog system and the task per-
formed seems to be more and more a
utopia. Most of the evaluations have
up to now either tackled the system as
a whole, or based the measurements
on dialog-context-free information. The
PEACE proposal aims at bypassing some
of these shortcomings by extracting,
from real dialog corpora, test sets that
synthesize contextual information.
</bodyText>
<sectionHeader confidence="0.966796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999739166666667">
Generally speaking common reference
tasks (Whittaker et al., 2002) and methods
to compare and diagnose spoken language dialog
systems (SLDS) and spoken dialog techniques
are lacking despite previous efforts futher dis-
cussed in the next section such as EAGLES,
DISC, AUPELF ARCB2 or the ongoing American
project DARPA COMMUNICATOR. Without
an objective assessment of dialog systems, it is
difficult to reuse previous work and to advance
theories. The assessment of a dialog system is
complex in part to the high integration factor
and tight coupling between the various modules
present in any SLDS, for which unfortunately
today, no common accepted reference architecture
exists. Nevertheless, a major problem remains the
dynamic nature of dialog. Consequently to these
shortcomings, researchers are often unable to
provide principled design and system capabilities
for technology transfer. In other research areas,
such as speech recognition and information re-
trieval, common reference tasks have been highly
effective in sharing research costs and efforts. A
similar development is highly needed in the dialog
community.
In this contribution which addresses only a part
of the SLDS evaluation problem, a paradigm for
evaluating the context-sensitive understanding ca-
pability of any spoken language dialog system is
proposed. PEACE (Devillers et al., 2002a) de-
scribed in section 3, is based on test sets extracted
from real corpora, and has three main aspects: it
is generic, contextual and it offers diagnostic ca-
pabilities. Here genericity is envisaged in a con-
text of information dialogs access. The diagnos-
tic aspect is important in order to determine the
different qualities of the systems under test. The
contextual aspect of evaluation is a crucial point
since dialog is dynamic by nature. We propose
to simulate/synthesize the contextual information.
The PEACE paradigm will be tested in the French
Technolangue MEDIA project and will serve as
basis in the comparison and diagnostic evaluation
of systems presented by various academic and in-
dustrial sites (section 4). ELRA/ELDA is the co-
ordinator of the larger scope evaluation campaign
EVALDA, which includes the MEDIA campaign
that began in January 2003.
</bodyText>
<sectionHeader confidence="0.871777" genericHeader="method">
2 Overview of SLDS evaluation
</sectionHeader>
<bodyText confidence="0.999974173611111">
Without an attempt to be exhaustive, we overview
some recent efforts for evaluation of SLDS.
The objective of the European DISC project was
to write the best-practice guidelines for SLDS de-
velopment and evaluation of its time. DISC has
collected a systematic list of bottom-up evalua-
tion criteria, each corresponding to a partially or-
dered list of properties likely to be encountered
in any SLDS. This properties are positioned on a
grid defining an SLDS abstract architecture and re-
late to various phases of the generic DISC SLDS
development life-cycle (Dybkjær and al., 1998).
They are complemented by a standard evaluation
pattern made of 10 generic questions (e.g. “Which
symptoms need to be observed?” ) which has been
instantiated for all the evaluation criteria. If the
DISC results are quite extensive and presented in
an homogeneous way, they do not provide a di-
rect answer to the question of SLDS evaluation.
Its contribution lies more at the specification level.
Although the approach and the goals of the Euro-
pean EAGLES project were different, one could
forward the same remark about the results of the
speech evaluation work group (D. Gibbon, 1997).
In (Fraser, 1998), one find a set of evaluation cri-
teria for voice oriented products and services, or-
ganized in four broad categories.: 1) voice com-
mand, 2) document generation, 3) phone services
4) other.
To the best of our knowledge, the MADCOW
(Multi Site Data COllection Working group) co-
ordination group set up in the USA by ARPA in
the context of the ATIS (Air Travel Information
Services) task to collect corpora, was the first to
propose a common infrastructure for SLDS auto-
matic evaluation (MADCOW, 1992), which also
addressed the problem of language understand-
ing evaluation, based on system answer compar-
ison. Unfortunately no direct diagnostic informa-
tion can be produced, since understanding is ap-
preciated by gauging the distance from the answer
to a pair of minimal and a maximal reference an-
swers. In ATIS, the protocol was only been ap-
plied to context free sentences. Up to now it has
been one of the most used by the community since
it is relatively objective and generic because it re-
lies on counts of explicit information and allows
for a certain variation in the answers. On the other
hand, the method displays a bias toward silence
and does not give the means to appreciate error
severity.
In ARISE (Automatic Railway Information Sys-
tems for Europe) (Lamel, 1998), a corpus of
roughly 10,000 calls has been used in conjunc-
tion with user debriefing questionnaire analysis to
diagnose different versions of a phone informa-
tion server. The hand-tagging objective measures
of the corpus include understanding error counts
(glass box methodology). Although it provides
fine grained diagnostic information, this procedure
cannot be easily generalized since it requires hand-
annotated corpus and access to the internal repre-
sentation of the system.
Two metrics have been developped at MIT
(Glass et al., 2000): the Query Density (QD)
and the Concept Efficiency (CE), which measure
respectively over the course of a dialogue: the
mean number of new concepts introduced per user
query, and the number of turns necessary for each
concept to be understood by the system. Con-
cepts are generated automatically for each utter-
ance with a parsable orthographic transcription as
a series of keyword-value pairs. The higher the
QD, the more effectively a user is able to commu-
nicate information to the system. The CE is an in-
dicator of recognition or understanding errors; the
higher it is, the fewer times a user needs to repeat
himself. These metrics were evaluated on single
systems (JUPITER and and MERCURY); to com-
pare different systems of the same type, one would
need a common ontology. In (Glass et al., 2000),
the authors believe that CE should be related to
user frustation, but to show it they would need to
use the PARADISE framework.
PARADISE (Walker et al., 1998) can be seen
as a sort of meta-paradigm which correlates ob-
jective and subjective measurements. Its ground-
ing hypothesis states that the goal of any SLDS is
to achieve user-satisfaction, which in turn can be
predicted through task success and various interac-
tion costs. With the help of the kappa coefficient
(Carletta, 1996) proposes to represent the dialog
success independently from the task intrinsic com-
plexity, thus opening the way to task generic com-
parative evaluation. PARADISE has been tested
in the COMMUNICATOR project (Walker et al.,
2001) with 9 systems working on the same task
over different databases. With four basic measures
(e.g. task completion) the protocol has been able
to predict 37% of user satisfaction variation, and
42% with the help of a few extra measurements on
dialog acts and subtasks. One critic, one can make
about PARADISE concern its cost (real user tests
are costly) and the use of subjective assessment.
The adaption of the DQR text understanding
evaluation methodology (Sabatier et al., 2000) to
speech resulted in a generic and qualitative proce-
dure. Each element of its test set holds three parts,
the Declaration to define the context, a Question
which bears on point present in the context and the
Response. The test set is organized through seven
levels of test, from basic explicit understanding
to semantic interpretation and reply pertinence as-
sessment. This protocol is task and system generic
but test set construction is not straightforward and
the bias introduced by the wording of the question
is difficult to assess.
Recently the GDR-13 work group of CNRS
on spoken dialog understanding, has proposed an
evaluation methodology for literal understanding.
According to (Antoine and al., 2002), DEFI tries
to remedy two important weaknesses of the MAD-
COW methodology, namely the lack of genericity
and the lack of diagnostic information, by craft-
ing system specific test sets from a primary set of
enunciations representative of the task (provided
by the developers). Secondary enunciations are
then derived from the primary ones in order to ex-
hibit particular language phenomena. Afterwards,
the systems are evaluated by their developers us-
ing specific test set and their own metrics. The
various results can be mapped over a generic ab-
stract architecture for comparison (although this
mapping is still unspecified at the time of writ-
ing). DEFI has already been used in one evalua-
tion campaign, with 5 systems presented by 4 lab-
oratories. (Antoine and al., 2002) has reported the
following weaknesses of the protocol: how to con-
trol the bias introduced by the derivation of enun-
ciations, how to guaranty that derived enunciation
will remain in the task scope (this prevented some
system from being evaluated over the complete
test set) and finally how to restrict and organize
the language phenomena used in the test set.
</bodyText>
<sectionHeader confidence="0.996796" genericHeader="method">
3 The PEACE paradigm
</sectionHeader>
<bodyText confidence="0.999953666666667">
We first describe the paradigm and relate prelim-
inary experiments with PEACE. This paradigm
which is as basement for the MEDIA project will
be refined by all the partners and use for an evalua-
tion campaign between seven systems of industrial
and academic sites.
</bodyText>
<subsectionHeader confidence="0.996333">
3.1 Description
</subsectionHeader>
<bodyText confidence="0.998782875">
The PEACE paradigm relies on the idea that for
database querying tasks, it is possible to define a
common semantic representation, onto which all
the systems are able to convert their own repre-
sentation (Moore, 1994). The paradigm based on
data extracted from real corpus, includes both lit-
eral and contextual understanding test sets. More
precisely, it provides:
</bodyText>
<listItem confidence="0.9981172">
• the definition of a semantic representation
(see 3.1.1),
• the definition of a model for dialogic contexts
(see 3.1.2),
• the definition and typology of linguistic phe-
nomena and dialogic functions used to selec-
tively diagnoze the system language capabil-
ities (anaphora resolution, constraints relax-
ation, etc.) (see 3.1.3),
• a data structuring method. The format of the
annotated data will be adapted to language
resource standard annotations implemented
(see 3.1.4),
• and evaluation metrics with the correspond-
ing evaluation tool (see 3.1.5).
</listItem>
<subsubsectionHeader confidence="0.516272">
3.1.1 Generic semantic representation
</subsubsectionHeader>
<bodyText confidence="0.997430681818182">
The difficulty of choosing a semantic represen-
tation lies in finding a complete and simple repre-
sentation of a user utterance meaning in a unified
format. A frame Attribute Value Representation
(AVR) has been chosen, allowing a fast and re-
liable annotation. The values are either numeric
units, proper names, or semantic classes, that
group together lexical units which are synonyms
for the task. The order of the (attribute, value)
pairs in the semantic representation matches their
respective position in the utterance. A modal in-
formation (positive (+) and negative(-)) is also as-
signed to each (attribute, value) pair. The semantic
representation of an utterance consists then in a list
of triplets of the form (mode, attribute, normalized
value). An example is given in figure 1. In order
to take into account for long-time dependencies or
to allow multiple referenced objects, the semantic
representation may be enriched by adding a refer-
ence value to each triplet for the representation of
links between 2 attributes of the utterance.
Attributes can grouped into different classes:
</bodyText>
<listItem confidence="0.991802545454545">
• the database attributes (the most frequent)
correspond to the attributes of the database
tables (e.g. category for an hotel);
• the modifier attributes are associated to
the database concepts. Their values are
used to modify the database concept in-
terpretation values (e.g. the attribute
category-modifier with possible val-
ues: &gt;; &lt;, =, Max, Min);
• the discursive attributes are introduced to
handle various aspects of dialogic interaction
</listItem>
<table confidence="0.98601925">
User c’est pas Paris c’est Passy
Query it is not Paris it is Passy
(LU) AVR (-, place, Paris)
(+, place, Passy)
</table>
<figureCaption confidence="0.998166">
Figure 1: Example of a semantic representation of an ut-
terance with positive and negative information for the ARISE
task. Place is an database attribute,Paris and Passy are
values and +/- modal markers.
</figureCaption>
<bodyText confidence="0.954401666666667">
(e.g. command with values cancelation, cor-
rection, error specification..., or response
with values yes or no);
• the argument attribute which represents the
topic at the focus of the utterance.
When dealing with information retrieval appli-
cations, defining the database and modifier at-
tributes and the appropriate values can be done
in a rather straightforward way. Most of those
attributes are derived directly from the informa-
tion stored in the database. Furthermore, most of
the discursive attributes are domain-independent.
Some database attributes remain unchanged across
many tasks, such as those dealing with dates or
prices.
This semantic representation has been used at
LIMSI for PARIS-SITI TASK (touristic informa-
tion) and ARISE TASK (traintable information)
both with triplet representation. More recently in
the context of the AMITIES project, quadruplets
were used.
</bodyText>
<subsectionHeader confidence="0.659181">
3.1.2 Contextual understanding modeling
</subsectionHeader>
<bodyText confidence="0.999195594594595">
Contextual understanding evaluation provides
information about the capability of the system
to take into account the dialog history in order
to properly interpret the user query. Contextual
understanding evaluation is rarely performed be-
cause of the dynamic nature of the dialog make
the dialog context depend on the system’s dialog
strategy.
Nevertheless PEACE proposes a system-
independent way to evaluate local contextual
interpretation. Given U1...Ut the user inter-
actions, and S1...St the answers of the agent
or system, the context a time t is a function
f(U1, S1, U2, S2, ...Ut, St). In the PEACE
paradigm, a paraphrase of the context is derived
from the semantic representation (Bonneau-
Maynard et al., 2000).
The dialog contexts are extracted from real di-
alogs in three steps. First, the internal semantic
frames representing the dialog contexts are auto-
matically extracted from the log files of the ses-
sion recordings. Secondly, the semantic frames
are converted into AVR format and then hand-
corrected to faithfully represent the dialog history.
The last step consists in the writing of a sentence
for each context (the context paraphrase), which
results in the same AVR representation as the one
of the dialog context.
Two possibilities may be investigated for build-
ing the paraphrase from the internal semantic rep-
resentation of the dialog context. A rule-based or
template-based natural language generation mod-
ule can be used to automatically produce the para-
phrase. The paraphrase can also be obtained
by concatenating the sentences preceding the ex-
tracted dialog state. In both cases, a manual veri-
fication is needed.
</bodyText>
<subsectionHeader confidence="0.5939705">
3.1.3 A typology of linguistic phenomena and
dialogic functions
</subsectionHeader>
<bodyText confidence="0.999942555555556">
For dialog system evaluation, it is essential to
build test sets randomly extracted from real cor-
pus. For dialog system diagnosis, it is also crucial
to build test sets labeled with the linguistic phe-
nomena and dialogic functions. Thus, the capabil-
ities of system’s contextual understanding can be
assessed for the main linguistic and dialogic dif-
ficulties such as, for instance, anaphora or ellipsis
resolution.
</bodyText>
<subsectionHeader confidence="0.862963">
3.1.4 A data structuring method
</subsectionHeader>
<bodyText confidence="0.999703583333333">
Two types of units, one for literal understanding
(LU), the other for contextual understanding (CU)
are defined. The format of the annotated data will
be adapted to language resource standard annota-
tions implemented in XML, e.g. (Geoffrois et al.,
2000), (Ide and Romary, 2002).
Each unit is extracted from a real dialog cor-
pus. LU units are composed of the user query,
the corresponding audio signal, an automatic tran-
scription obtained with a recognition system, and
finally the literal semantic representation of the ut-
terance (see Figure 1). CU units are composed of
</bodyText>
<table confidence="0.9782181875">
Context je voudrais un hˆotel 4
paraphrase ´etoiles dans le neuvi`eme
I would like a 4 category
hotel in the ninth
(LU) AVR (+, argument, hotel)
(+, district, 9)
(+, category, 4)
User la mˆeme cat´egorie dans
query un autre arrondissement
the same category in
another district
(LU) AVR (+, other, district)
(+, same, category)
(CU) AVR (+, argument, hotel)
(-, district, 9)
(+, category, 4)
</table>
<figureCaption confidence="0.9994122">
Figure 2: Example of a contextual understanding unit com-
posed of a context paraphrase, a user query and the resulting
AVR. AVR of context paraphrase and user query are given in
TYPEWRITING MODE. Ellipsis (“in the ninth”) and anaphora
(“same category”, “another district”) maybe observed.
</figureCaption>
<bodyText confidence="0.9997648">
the dialog context (given by the paraphrase), the
user query and the resulting AVR of the user query
in the given context (see Figure 2). Those units are
also labeled with linguistic and dialogic phenom-
ena.
</bodyText>
<subsectionHeader confidence="0.879737">
3.1.5 Evaluation metrics and scoring tool
</subsectionHeader>
<bodyText confidence="0.999985961538462">
Common evaluation metrics are essential for
analyzing the system capabilities. The scoring tool
for AVR comparison is able to compare between
two AVR frame representation sets. For evalu-
ation, system outputs translated in AVR format
composed one set, the other one contains the AVR
references which are manually annotated. Both
frame sets have the form of a list of AVRs (fixed
length records). Each record is composed of three
or four fields (mode, attribute, value, reference).
The comparison consists in applying a set of pre-
defined operators each assigned with a cost value.
The comparison process looks for operator lists
to be applied to the test frame in order to obtain
the reference frame that minimizes the final cost
value. For a global evaluation, the classical opera-
tors from speech evaluation (DELetion, INSertion
and SUBstitution) may be used (as used for first
two values of Accuracy percentage in Table 1).
With our scoring tool the definition of new opera-
tors is quite easy. It is then also possible to distin-
guish between different types of errors by defining
specific operators (as used to estimate Topic iden-
tification in Table 1), or by using different cost val-
ues (for example a substitution is often considered
more costly for dialog management).
</bodyText>
<subsectionHeader confidence="0.996384">
3.2 Example use of PEACE
</subsectionHeader>
<bodyText confidence="0.999996">
In order to validate the evaluation paradigm, a
set of approximatively 1,700 literal units and a
set of 100 contextual units has been used for
the PARIS-SITI task (Bonneau-Maynard and Dev-
illers, 2000). Results for both literal and contex-
tual understanding test sets are given in Table 1. In
order to observe the ability of the systems to deal
with recognition errors, each literal understand-
ing unit also contains the ASR transcription of the
original user utterance. The various measures of
understanding accuracy are computed as the ratio
between the sum of the number of deleted, inserted
and substituted attributes, and the total number of
AVR attributes in the test set. The possibility of
an automatic evaluation of the LU accuracy and
the ability of the scoring tool to point out the er-
rors allowed us to easily improve the literal un-
derstanding accuracy from 89.0% to 93.5%. Due
to a 26.5% ASR error rate, the LU accuracy goes
down from 93.5% to 72% after ASR transcription.
The contextual understanding accuracy on the 100
test units is 82.6% on exact transcription. For
instance, anaphoric references are relatively well
solved, with 80.4% accuracy on the 50 units con-
taining at least one anaphoric reference. For each
example, the anaphoric referenced object is gen-
erally correctly identified and remaining errors are
often due to a bad history constraint management.
</bodyText>
<subsectionHeader confidence="0.998088">
3.3 Discussing the PEACE paradigm
</subsectionHeader>
<bodyText confidence="0.999883444444444">
The PEACE paradigm enables automatic evalua-
tion of literal and contextual dialog understand-
ing. The evaluation paradigm makes the distinc-
tion between different types of errors, allowing a
qualitative and diagnostic analysis of the perfor-
mances of a speech understanding module. Very
few evaluation paradigms propose automatic di-
agnosis of contextual interpretation (Glass et al.,
2000). The proposed methodology is based on
</bodyText>
<table confidence="0.9997874">
#Units #Attr. %Acc. Prec.
LU exact 1 681 3 991 93.5% 0.7
LU ASR. 1 681 3 991 72.0% 1.4
Topic id. 680 833 94.3% 1.6
Modifier id. 323 445 95.7% 1.9
CU exact 100 430 86.8% 3.2
Anaphoric 50 245 84.4% 4.5
resolution
Ellipsis 25 106 85.3% 6.7
resolution
</table>
<tableCaption confidence="0.67327375">
Table 1: Literal understanding (LU) accuracy on both exact
and ASR transcription, and contextual understanding (CU)
accuracy. Second column indicates the number of units in-
cluded in the test set (i.e # of user utterances), third col-
</tableCaption>
<bodyText confidence="0.995329309090909">
umn gives the total number of attributes in the correct AVR
test sets. Details, using specific operators, are given for
argument (topic) and modifier identification for LU on
exact transcription, and for anaphoric reference and ellipsis
resolution for CU. Last column gives the 95% precision of
the accuracy estimation (Montaci´e and Chollet, 1997)
.
semi-automatically built reference test sets, and
therefore is much more time effective than manual
evaluation. Furthermore, it provides reproducible
tests.
Although the semantic representation is task de-
pendent, the example described above shows the
feasibility of the paradigm for any dialog system
interfacing to a database. Robustness to many
linguistic phenomena such as repetitions, hesita-
tions or auto-corrections may be evaluated with
this method. XML coding will facilitate the gener-
icity and the reusability of the test sets, by al-
lowing the selection of the dialogic contexts to be
studied.
The representation of the dialog context with a
single paraphrase, derived from a “flat” structured
AVR, may have some limitations in case of long-
time dialog dependencies. It does not allow for
memorizing all the steps of the dialog. For ex-
ample, if the speaker says first “I would like a
2 star hotel”, then “no I prefer 3 stars” and fi-
nally says “give me again my first choice”, the
CU unit cannot take into account this succession
of queries. However, this kind of interaction is
rarely observed in dialogue corpora: the user usu-
ally repeats the constraint value (“give me again
a 2 star hotel”). To represent more precisely the
dialog state, the representation of the dialog con-
text should incorporate some meta-information in-
spired for example from the DAMSL annotation
standard 1 (Devillers et al., 2002b).
Another point is the representativity of the test
sets. This may be considered as a limitation as
far as PEACE paradigm is built on the idea that
the test units are extracted from real dialogs. Ob-
viously, the larger the test sets are, the better. A
diagnostic evaluation may need a very large test
corpora to validate system performance against the
wide range of phenomena present in spontaneous
dialog.
The ability to automatically diagnose the per-
formances of contextual understanding modules
on local difficulties such as ellipsis, negations,
anaphoric reference or constraint relaxation is one
of the major advantages of the PEACE paradigm,
which has not been investigated by other method-
ologies. This is why it has been chosen for the
MEDIA project described in the next section.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="method">
4 The MEDIA project
</sectionHeader>
<bodyText confidence="0.999926956521739">
The MEDIA project proposes a paradigm based
on a reference task and on test sets extracted from
real corpora for evaluating literal and contextual
understanding in dialog systems. The PEACE
paradigm will serve as basis for the MEDIA
project. The consortium is composed of IRIT,
LIA, LIMSI, LORIA, VALORIA for the French
academic sites and France Telecom R&amp;D and
TELIP for the industrial sites. The scientific com-
mittee contains representatives of AT&amp;T (USA),
Tilburg University (Netherlands), IBM, IMAG,
LIUM and VECSYS (France).
The project has four main parts. First, the selec-
tion of reference task such as for example a task
of web-based travel agency. The reference task
has to correspond to a real-life application allow-
ing real user tests. Secondly, multi-level represen-
tation such as the semantic representation, the ty-
pology of linguistic phenomena and dialogic func-
tions, the dialog context model... will be com-
monly refined and adapted to the reference task.
The third part deals with the recording and la-
beling of a dialog corpus which will be used for
</bodyText>
<footnote confidence="0.86672">
1http://www.cs.rochester.edu/research/trains/annotation
</footnote>
<bodyText confidence="0.999672181818182">
both system adaptation and test set selection. The
last part is the organisation of the evaluation cam-
paigns by ELRA/ELDA for the participating sites.
ELRA/ELDA is the coordinator of a larger
scope project: EVALDA which includes among
others, the MEDIA project. ELDA with VEC-
SYS will provide transcribed and annotated cor-
pora and evaluation tools according to consortium
specifications. The recording of 1200 French di-
alogs (240 speakers, 5 dialogs each, 15k user
queries) is planned. Three sets of LU and CU
units will be built from this corpus. A large size
adaptation set will be used by the participants to
adapt their system to the task and the semantic
representation. The development set (around 1K
LU (resp. CU) units) will be used to validate the
evaluation protocole. The size of the test set is
planned to be around 3K LU (resp. CU) units. Var-
ious approaches are currently used at the partici-
pating sites; stochastic or syntactic and semantic
rule-based modeling. The project started in Jan-
uary 2003 and will last two years.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988">
Assessing the dialog system understanding capa-
bilities requires to evaluate the transition between
successive states of the dialog. At least, we must
be able to test a sequence of two states at any
point in the dialog. The dynamic and interac-
tive nature of the dialog makes construction and
reuse of test sets difficult. Furthermore, to eval-
uate one particular dialog transition, the system
has to be put in a particular state corresponding to
the original dialog context. The variable describ-
ing the dialog state can be composed of complex
information such as the current semantic frame
(list of triplets (mode,attribute,value) or quadru-
ples (mode, attribute, value, reference)), the dialog
history semantic frame and potentially other infor-
mation like recognition scores, dialog acts, etc.
The PEACE paradigm allows the evaluation of
two successive simplified dialog states. It has been
successfully tested with test samples focusing on
linguistic difficulties of literal and contextual un-
derstanding. For these tests, the dialog state is
the dialog history semantic frame. The contextual
understanding modeling in PEACE is system inde-
pendent since the context is given by a paraphrase
of queries. PEACE allows a diagnostic evaluation
of specific semantic attributes and particular lin-
guistic phenomena.
In our opinion, it is crucial for the dialog com-
munity to agree on a common reference task and
reference test sets in order to be able to compare
and diagnose dialog systems. Both evaluation with
real users and artificial simulation of successive
dialog states using test sets extracted from real cor-
pora have to be carried out in parallel. The use of
test sets reduces the global cost of dialog system
evaluation, moreover such tests are reproducible.
The PEACE protocol will be used as basis for
the French Technolangue MEDIA project in a two
year evaluation campaign where dialog systems
from both academia and industry will be evalu-
ated. In other domains, it could be related with
(Hirschman, 2000) propositions for Question An-
swering evaluation.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999749957142857">
J.Y. Antoine and al. 2002. Predictive and objective evalua-
tion of speech understanding: the ”challenge” evaluation
campaign of the i3 speech workgroup of th french cnrs. In
LREC2002, Spain, May. ELRA.
H. Bonneau-Maynard and L. Devillers. 2000. A framework
for evaluating contextual understanding. In ICSLP.
H. Bonneau-Maynard, L. Devillers, and S. Rosset. 2000.
Predictive performance of dialog systems. In LREC2000,
volume 1, pages 177–181, Athens, Greece, May. ELRA.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistics. Computational Linguistics,
2(22):249–254.
R. Winski D. Gibbon, R. Moore. 1997. Handbook of Stan-
dards and Ressources for Spoken Language Ressources.
Mouton de Gruyter, New York.
L. Devillers, H. Maynard, and P. Paroubek. 2002a.
M´ethodologies d’´evaluation des syst`emes de dia-
logue parl´e : r´eflexions et exp´eriences autour de la
compr´ehension. In Traitement Automatique des Langues,
volume 43, pages 155–184.
L. Devillers, S. Rosset, H. Bonneau-Maynard, and L. Lamel.
2002b. Annotations for dynamic diagnosis of the dialog
state. In LREC2002, Spain, May. ELRA.
L. Dybkjær and al. 1998. The disc approach to spo-
ken language systems development and evaluation. In
LREC1998), volume 1, pages 185–189, Spain, May.
ELRA.
N. Fraser. 1998. Spoken Language System Assessment, vol-
ume 3. Mouton de Gruyter, New York.
E. Geoffrois, C. Barras, S. Bird, and Z. Wu. 2000. Tran-
scribing with annotation graphs. In LREC2000, volume 2,
pages 1517–1521, Greece, May. ELRA.
J. Glass, J. Polifroni, S. Seneff, and V. Zue. 2000. Data
collection and performance evaluation ofspoken dialogue
systems: the MIT experience.
Lynette Hirschman. 2000. Reading comprehension and
question answering new evaluation paradigms for human
language technology. In LREC2000 Workshop ”Using
Evaluation within HLT Programs: Results and Trends”,
pages 54–59, Greece, May. ELRA.
N. Ide and L. Romary. 2002. Towards multimodal content
representation. In LREC 2002.
L. Lamel. 1998. Spoken language dialog system develop-
ment and evaluation at limsi. In Actes de l’International
Symposium on Spoken Dialogue, Sydney, Australia,
November.
MADCOW. 1992. Multi-site data collection for a spoken
language corpus. In DARPA Speech and Natural Lan-
guage Workshop.
C. Montaci´e and G. Chollet. 1997. Syst`emes de r´ef´erence
pour l’´evaluation d’applications et la caract´erisation de
bases de donn´ees en reconnaissance de la parole. In
16`eme JEP.
R.C. Moore. 1994. Semantic evaluation for spoken-language
systems. In DARPA Speech and Natural Language Work-
shop.
P. Sabatier, Ph. Blache, J. Guizol, F. L´evy, A. Nazarenko,
and S. N’Guema. 2000. ´evaluer des syst`emes de
compr´ehension de textes. In Ressources et Evaluation en
Ing´enierie Linguistique, pages 265–275. Chibout K. et al.
(Eds) Duculot.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Eval-
uating spoken dialogue agents with paradise: 2 cases stud-
ies. Computer Speech and Language, 3(12):317–347.
M. Walker, R. Passonneau, and J.E. Boland. 2001. Quantita-
tive and qualitative evaluation of darpa communicatorspo-
ken dialog systems. In Actes du 39me ACL, pages 515–
522, Toulouse, France, July. ACL.
S. Whittaker, L. Terveen, and B. Nardi. 2002. Reference task
agenda for HCI. In ISLE workshop 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.136270">
<title confidence="0.750967">The PEACE SLDS understanding evaluation paradigm of the French MEDIA campaign</title>
<author confidence="0.51433">Laurence Devillers</author>
<author confidence="0.51433">H´el`ene Maynard</author>
<author confidence="0.51433">Patrick Paroubek</author>
<author confidence="0.51433">Sophie Bt University of Paris</author>
<abstract confidence="0.999328631578947">This paper presents a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system: PEACE (French for d’Evaluation Automatique de la Compr´ehension hors This paradigm will be the basis of the French Technolangue MEDIA project, in which dialog systems from various academic and industrial sites will be tested in an evaluation campaign coordinated by ELRA/ELDA (over the next two years). Despite preefforts such as or the ongoing Ameri- DARPA the spoken dialog community still lacks common reference tasks and widely agreed upon methods for comparing and diagnosing systems and techniques. Automatic solutions are nowadays being sought both to make possible the comparison of different approaches by means of reliable indicators with generic evaluation methodologies and also to reduce system development costs. However achieving independence from both the dialog system and the task performed seems to be more and more a utopia. Most of the evaluations have up to now either tackled the system as a whole, or based the measurements on dialog-context-free information. The aims at bypassing some of these shortcomings by extracting, from real dialog corpora, test sets that synthesize contextual information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Y Antoine</author>
<author>al</author>
</authors>
<title>Predictive and objective evaluation of speech understanding: the ”challenge” evaluation campaign of the i3 speech workgroup of th french cnrs.</title>
<date>2002</date>
<booktitle>In LREC2002,</booktitle>
<publisher>ELRA.</publisher>
<location>Spain,</location>
<marker>Antoine, al, 2002</marker>
<rawString>J.Y. Antoine and al. 2002. Predictive and objective evaluation of speech understanding: the ”challenge” evaluation campaign of the i3 speech workgroup of th french cnrs. In LREC2002, Spain, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bonneau-Maynard</author>
<author>L Devillers</author>
</authors>
<title>A framework for evaluating contextual understanding.</title>
<date>2000</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="19860" citStr="Bonneau-Maynard and Devillers, 2000" startWordPosition="3163" endWordPosition="3167">s used for first two values of Accuracy percentage in Table 1). With our scoring tool the definition of new operators is quite easy. It is then also possible to distinguish between different types of errors by defining specific operators (as used to estimate Topic identification in Table 1), or by using different cost values (for example a substitution is often considered more costly for dialog management). 3.2 Example use of PEACE In order to validate the evaluation paradigm, a set of approximatively 1,700 literal units and a set of 100 contextual units has been used for the PARIS-SITI task (Bonneau-Maynard and Devillers, 2000). Results for both literal and contextual understanding test sets are given in Table 1. In order to observe the ability of the systems to deal with recognition errors, each literal understanding unit also contains the ASR transcription of the original user utterance. The various measures of understanding accuracy are computed as the ratio between the sum of the number of deleted, inserted and substituted attributes, and the total number of AVR attributes in the test set. The possibility of an automatic evaluation of the LU accuracy and the ability of the scoring tool to point out the errors al</context>
</contexts>
<marker>Bonneau-Maynard, Devillers, 2000</marker>
<rawString>H. Bonneau-Maynard and L. Devillers. 2000. A framework for evaluating contextual understanding. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bonneau-Maynard</author>
<author>L Devillers</author>
<author>S Rosset</author>
</authors>
<title>Predictive performance of dialog systems.</title>
<date>2000</date>
<booktitle>In LREC2000,</booktitle>
<volume>1</volume>
<pages>177--181</pages>
<publisher>ELRA.</publisher>
<location>Athens, Greece,</location>
<marker>Bonneau-Maynard, Devillers, Rosset, 2000</marker>
<rawString>H. Bonneau-Maynard, L. Devillers, and S. Rosset. 2000. Predictive performance of dialog systems. In LREC2000, volume 1, pages 177–181, Athens, Greece, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistics.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>2</volume>
<issue>22</issue>
<contexts>
<context position="8160" citStr="Carletta, 1996" startWordPosition="1297" endWordPosition="1298">JUPITER and and MERCURY); to compare different systems of the same type, one would need a common ontology. In (Glass et al., 2000), the authors believe that CE should be related to user frustation, but to show it they would need to use the PARADISE framework. PARADISE (Walker et al., 1998) can be seen as a sort of meta-paradigm which correlates objective and subjective measurements. Its grounding hypothesis states that the goal of any SLDS is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs. With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation. PARADISE has been tested in the COMMUNICATOR project (Walker et al., 2001) with 9 systems working on the same task over different databases. With four basic measures (e.g. task completion) the protocol has been able to predict 37% of user satisfaction variation, and 42% with the help of a few extra measurements on dialog acts and subtasks. One critic, one can make about PARADISE concern its cost (real user tests are costly) and the use of subj</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: the kappa statistics. Computational Linguistics, 2(22):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Winski D Gibbon</author>
<author>R Moore</author>
</authors>
<title>Handbook of Standards and Ressources for Spoken Language Ressources. Mouton de Gruyter,</title>
<date>1997</date>
<location>New York.</location>
<marker>Gibbon, Moore, 1997</marker>
<rawString>R. Winski D. Gibbon, R. Moore. 1997. Handbook of Standards and Ressources for Spoken Language Ressources. Mouton de Gruyter, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Devillers</author>
<author>H Maynard</author>
<author>P Paroubek</author>
</authors>
<title>M´ethodologies d’´evaluation des syst`emes de dialogue parl´e : r´eflexions et exp´eriences autour de la compr´ehension.</title>
<date>2002</date>
<booktitle>In Traitement Automatique des Langues,</booktitle>
<volume>43</volume>
<pages>155--184</pages>
<contexts>
<context position="2999" citStr="Devillers et al., 2002" startWordPosition="443" endWordPosition="446">re of dialog. Consequently to these shortcomings, researchers are often unable to provide principled design and system capabilities for technology transfer. In other research areas, such as speech recognition and information retrieval, common reference tasks have been highly effective in sharing research costs and efforts. A similar development is highly needed in the dialog community. In this contribution which addresses only a part of the SLDS evaluation problem, a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system is proposed. PEACE (Devillers et al., 2002a) described in section 3, is based on test sets extracted from real corpora, and has three main aspects: it is generic, contextual and it offers diagnostic capabilities. Here genericity is envisaged in a context of information dialogs access. The diagnostic aspect is important in order to determine the different qualities of the systems under test. The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature. We propose to simulate/synthesize the contextual information. The PEACE paradigm will be tested in the French Technolangue MEDIA project and will serve as bas</context>
<context position="23739" citStr="Devillers et al., 2002" startWordPosition="3795" endWordPosition="3798">ow for memorizing all the steps of the dialog. For example, if the speaker says first “I would like a 2 star hotel”, then “no I prefer 3 stars” and finally says “give me again my first choice”, the CU unit cannot take into account this succession of queries. However, this kind of interaction is rarely observed in dialogue corpora: the user usually repeats the constraint value (“give me again a 2 star hotel”). To represent more precisely the dialog state, the representation of the dialog context should incorporate some meta-information inspired for example from the DAMSL annotation standard 1 (Devillers et al., 2002b). Another point is the representativity of the test sets. This may be considered as a limitation as far as PEACE paradigm is built on the idea that the test units are extracted from real dialogs. Obviously, the larger the test sets are, the better. A diagnostic evaluation may need a very large test corpora to validate system performance against the wide range of phenomena present in spontaneous dialog. The ability to automatically diagnose the performances of contextual understanding modules on local difficulties such as ellipsis, negations, anaphoric reference or constraint relaxation is on</context>
</contexts>
<marker>Devillers, Maynard, Paroubek, 2002</marker>
<rawString>L. Devillers, H. Maynard, and P. Paroubek. 2002a. M´ethodologies d’´evaluation des syst`emes de dialogue parl´e : r´eflexions et exp´eriences autour de la compr´ehension. In Traitement Automatique des Langues, volume 43, pages 155–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Devillers</author>
<author>S Rosset</author>
<author>H Bonneau-Maynard</author>
<author>L Lamel</author>
</authors>
<title>Annotations for dynamic diagnosis of the dialog state.</title>
<date>2002</date>
<booktitle>In LREC2002,</booktitle>
<publisher>ELRA.</publisher>
<location>Spain,</location>
<contexts>
<context position="2999" citStr="Devillers et al., 2002" startWordPosition="443" endWordPosition="446">re of dialog. Consequently to these shortcomings, researchers are often unable to provide principled design and system capabilities for technology transfer. In other research areas, such as speech recognition and information retrieval, common reference tasks have been highly effective in sharing research costs and efforts. A similar development is highly needed in the dialog community. In this contribution which addresses only a part of the SLDS evaluation problem, a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system is proposed. PEACE (Devillers et al., 2002a) described in section 3, is based on test sets extracted from real corpora, and has three main aspects: it is generic, contextual and it offers diagnostic capabilities. Here genericity is envisaged in a context of information dialogs access. The diagnostic aspect is important in order to determine the different qualities of the systems under test. The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature. We propose to simulate/synthesize the contextual information. The PEACE paradigm will be tested in the French Technolangue MEDIA project and will serve as bas</context>
<context position="23739" citStr="Devillers et al., 2002" startWordPosition="3795" endWordPosition="3798">ow for memorizing all the steps of the dialog. For example, if the speaker says first “I would like a 2 star hotel”, then “no I prefer 3 stars” and finally says “give me again my first choice”, the CU unit cannot take into account this succession of queries. However, this kind of interaction is rarely observed in dialogue corpora: the user usually repeats the constraint value (“give me again a 2 star hotel”). To represent more precisely the dialog state, the representation of the dialog context should incorporate some meta-information inspired for example from the DAMSL annotation standard 1 (Devillers et al., 2002b). Another point is the representativity of the test sets. This may be considered as a limitation as far as PEACE paradigm is built on the idea that the test units are extracted from real dialogs. Obviously, the larger the test sets are, the better. A diagnostic evaluation may need a very large test corpora to validate system performance against the wide range of phenomena present in spontaneous dialog. The ability to automatically diagnose the performances of contextual understanding modules on local difficulties such as ellipsis, negations, anaphoric reference or constraint relaxation is on</context>
</contexts>
<marker>Devillers, Rosset, Bonneau-Maynard, Lamel, 2002</marker>
<rawString>L. Devillers, S. Rosset, H. Bonneau-Maynard, and L. Lamel. 2002b. Annotations for dynamic diagnosis of the dialog state. In LREC2002, Spain, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dybkjær</author>
<author>al</author>
</authors>
<title>The disc approach to spoken language systems development and evaluation.</title>
<date>1998</date>
<booktitle>In LREC1998),</booktitle>
<volume>1</volume>
<pages>185--189</pages>
<publisher>ELRA.</publisher>
<marker>Dybkjær, al, 1998</marker>
<rawString>L. Dybkjær and al. 1998. The disc approach to spoken language systems development and evaluation. In LREC1998), volume 1, pages 185–189, Spain, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fraser</author>
</authors>
<title>Spoken Language System Assessment, volume 3. Mouton de Gruyter,</title>
<date>1998</date>
<location>New York.</location>
<contexts>
<context position="5071" citStr="Fraser, 1998" startWordPosition="782" endWordPosition="783">l., 1998). They are complemented by a standard evaluation pattern made of 10 generic questions (e.g. “Which symptoms need to be observed?” ) which has been instantiated for all the evaluation criteria. If the DISC results are quite extensive and presented in an homogeneous way, they do not provide a direct answer to the question of SLDS evaluation. Its contribution lies more at the specification level. Although the approach and the goals of the European EAGLES project were different, one could forward the same remark about the results of the speech evaluation work group (D. Gibbon, 1997). In (Fraser, 1998), one find a set of evaluation criteria for voice oriented products and services, organized in four broad categories.: 1) voice command, 2) document generation, 3) phone services 4) other. To the best of our knowledge, the MADCOW (Multi Site Data COllection Working group) coordination group set up in the USA by ARPA in the context of the ATIS (Air Travel Information Services) task to collect corpora, was the first to propose a common infrastructure for SLDS automatic evaluation (MADCOW, 1992), which also addressed the problem of language understanding evaluation, based on system answer compari</context>
</contexts>
<marker>Fraser, 1998</marker>
<rawString>N. Fraser. 1998. Spoken Language System Assessment, volume 3. Mouton de Gruyter, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Geoffrois</author>
<author>C Barras</author>
<author>S Bird</author>
<author>Z Wu</author>
</authors>
<title>Transcribing with annotation graphs.</title>
<date>2000</date>
<booktitle>In LREC2000,</booktitle>
<volume>2</volume>
<pages>1517--1521</pages>
<publisher>ELRA.</publisher>
<contexts>
<context position="17118" citStr="Geoffrois et al., 2000" startWordPosition="2711" endWordPosition="2714">xtracted from real corpus. For dialog system diagnosis, it is also crucial to build test sets labeled with the linguistic phenomena and dialogic functions. Thus, the capabilities of system’s contextual understanding can be assessed for the main linguistic and dialogic difficulties such as, for instance, anaphora or ellipsis resolution. 3.1.4 A data structuring method Two types of units, one for literal understanding (LU), the other for contextual understanding (CU) are defined. The format of the annotated data will be adapted to language resource standard annotations implemented in XML, e.g. (Geoffrois et al., 2000), (Ide and Romary, 2002). Each unit is extracted from a real dialog corpus. LU units are composed of the user query, the corresponding audio signal, an automatic transcription obtained with a recognition system, and finally the literal semantic representation of the utterance (see Figure 1). CU units are composed of Context je voudrais un hˆotel 4 paraphrase ´etoiles dans le neuvi`eme I would like a 4 category hotel in the ninth (LU) AVR (+, argument, hotel) (+, district, 9) (+, category, 4) User la mˆeme cat´egorie dans query un autre arrondissement the same category in another district (LU) </context>
</contexts>
<marker>Geoffrois, Barras, Bird, Wu, 2000</marker>
<rawString>E. Geoffrois, C. Barras, S. Bird, and Z. Wu. 2000. Transcribing with annotation graphs. In LREC2000, volume 2, pages 1517–1521, Greece, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>J Polifroni</author>
<author>S Seneff</author>
<author>V Zue</author>
</authors>
<title>Data collection and performance evaluation ofspoken dialogue systems: the MIT experience.</title>
<date>2000</date>
<contexts>
<context position="6878" citStr="Glass et al., 2000" startWordPosition="1078" endWordPosition="1081">verity. In ARISE (Automatic Railway Information Systems for Europe) (Lamel, 1998), a corpus of roughly 10,000 calls has been used in conjunction with user debriefing questionnaire analysis to diagnose different versions of a phone information server. The hand-tagging objective measures of the corpus include understanding error counts (glass box methodology). Although it provides fine grained diagnostic information, this procedure cannot be easily generalized since it requires handannotated corpus and access to the internal representation of the system. Two metrics have been developped at MIT (Glass et al., 2000): the Query Density (QD) and the Concept Efficiency (CE), which measure respectively over the course of a dialogue: the mean number of new concepts introduced per user query, and the number of turns necessary for each concept to be understood by the system. Concepts are generated automatically for each utterance with a parsable orthographic transcription as a series of keyword-value pairs. The higher the QD, the more effectively a user is able to communicate information to the system. The CE is an indicator of recognition or understanding errors; the higher it is, the fewer times a user needs </context>
<context position="21458" citStr="Glass et al., 2000" startWordPosition="3419" endWordPosition="3422">aining at least one anaphoric reference. For each example, the anaphoric referenced object is generally correctly identified and remaining errors are often due to a bad history constraint management. 3.3 Discussing the PEACE paradigm The PEACE paradigm enables automatic evaluation of literal and contextual dialog understanding. The evaluation paradigm makes the distinction between different types of errors, allowing a qualitative and diagnostic analysis of the performances of a speech understanding module. Very few evaluation paradigms propose automatic diagnosis of contextual interpretation (Glass et al., 2000). The proposed methodology is based on #Units #Attr. %Acc. Prec. LU exact 1 681 3 991 93.5% 0.7 LU ASR. 1 681 3 991 72.0% 1.4 Topic id. 680 833 94.3% 1.6 Modifier id. 323 445 95.7% 1.9 CU exact 100 430 86.8% 3.2 Anaphoric 50 245 84.4% 4.5 resolution Ellipsis 25 106 85.3% 6.7 resolution Table 1: Literal understanding (LU) accuracy on both exact and ASR transcription, and contextual understanding (CU) accuracy. Second column indicates the number of units included in the test set (i.e # of user utterances), third column gives the total number of attributes in the correct AVR test sets. Details, u</context>
</contexts>
<marker>Glass, Polifroni, Seneff, Zue, 2000</marker>
<rawString>J. Glass, J. Polifroni, S. Seneff, and V. Zue. 2000. Data collection and performance evaluation ofspoken dialogue systems: the MIT experience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>Reading comprehension and question answering new evaluation paradigms for human language technology.</title>
<date>2000</date>
<booktitle>In LREC2000 Workshop ”Using Evaluation within HLT Programs: Results and Trends”,</booktitle>
<pages>54--59</pages>
<publisher>ELRA.</publisher>
<marker>Hirschman, 2000</marker>
<rawString>Lynette Hirschman. 2000. Reading comprehension and question answering new evaluation paradigms for human language technology. In LREC2000 Workshop ”Using Evaluation within HLT Programs: Results and Trends”, pages 54–59, Greece, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>L Romary</author>
</authors>
<title>Towards multimodal content representation.</title>
<date>2002</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="17142" citStr="Ide and Romary, 2002" startWordPosition="2715" endWordPosition="2718"> For dialog system diagnosis, it is also crucial to build test sets labeled with the linguistic phenomena and dialogic functions. Thus, the capabilities of system’s contextual understanding can be assessed for the main linguistic and dialogic difficulties such as, for instance, anaphora or ellipsis resolution. 3.1.4 A data structuring method Two types of units, one for literal understanding (LU), the other for contextual understanding (CU) are defined. The format of the annotated data will be adapted to language resource standard annotations implemented in XML, e.g. (Geoffrois et al., 2000), (Ide and Romary, 2002). Each unit is extracted from a real dialog corpus. LU units are composed of the user query, the corresponding audio signal, an automatic transcription obtained with a recognition system, and finally the literal semantic representation of the utterance (see Figure 1). CU units are composed of Context je voudrais un hˆotel 4 paraphrase ´etoiles dans le neuvi`eme I would like a 4 category hotel in the ninth (LU) AVR (+, argument, hotel) (+, district, 9) (+, category, 4) User la mˆeme cat´egorie dans query un autre arrondissement the same category in another district (LU) AVR (+, other, district)</context>
</contexts>
<marker>Ide, Romary, 2002</marker>
<rawString>N. Ide and L. Romary. 2002. Towards multimodal content representation. In LREC 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
</authors>
<title>Spoken language dialog system development and evaluation at limsi.</title>
<date>1998</date>
<booktitle>In Actes de l’International Symposium on Spoken Dialogue,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="6340" citStr="Lamel, 1998" startWordPosition="999" endWordPosition="1000">e produced, since understanding is appreciated by gauging the distance from the answer to a pair of minimal and a maximal reference answers. In ATIS, the protocol was only been applied to context free sentences. Up to now it has been one of the most used by the community since it is relatively objective and generic because it relies on counts of explicit information and allows for a certain variation in the answers. On the other hand, the method displays a bias toward silence and does not give the means to appreciate error severity. In ARISE (Automatic Railway Information Systems for Europe) (Lamel, 1998), a corpus of roughly 10,000 calls has been used in conjunction with user debriefing questionnaire analysis to diagnose different versions of a phone information server. The hand-tagging objective measures of the corpus include understanding error counts (glass box methodology). Although it provides fine grained diagnostic information, this procedure cannot be easily generalized since it requires handannotated corpus and access to the internal representation of the system. Two metrics have been developped at MIT (Glass et al., 2000): the Query Density (QD) and the Concept Efficiency (CE), whic</context>
</contexts>
<marker>Lamel, 1998</marker>
<rawString>L. Lamel. 1998. Spoken language dialog system development and evaluation at limsi. In Actes de l’International Symposium on Spoken Dialogue, Sydney, Australia, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MADCOW</author>
</authors>
<title>Multi-site data collection for a spoken language corpus.</title>
<date>1992</date>
<booktitle>In DARPA Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="5568" citStr="MADCOW, 1992" startWordPosition="867" endWordPosition="868">d forward the same remark about the results of the speech evaluation work group (D. Gibbon, 1997). In (Fraser, 1998), one find a set of evaluation criteria for voice oriented products and services, organized in four broad categories.: 1) voice command, 2) document generation, 3) phone services 4) other. To the best of our knowledge, the MADCOW (Multi Site Data COllection Working group) coordination group set up in the USA by ARPA in the context of the ATIS (Air Travel Information Services) task to collect corpora, was the first to propose a common infrastructure for SLDS automatic evaluation (MADCOW, 1992), which also addressed the problem of language understanding evaluation, based on system answer comparison. Unfortunately no direct diagnostic information can be produced, since understanding is appreciated by gauging the distance from the answer to a pair of minimal and a maximal reference answers. In ATIS, the protocol was only been applied to context free sentences. Up to now it has been one of the most used by the community since it is relatively objective and generic because it relies on counts of explicit information and allows for a certain variation in the answers. On the other hand, t</context>
</contexts>
<marker>MADCOW, 1992</marker>
<rawString>MADCOW. 1992. Multi-site data collection for a spoken language corpus. In DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Montaci´e</author>
<author>G Chollet</author>
</authors>
<title>Syst`emes de r´ef´erence pour l’´evaluation d’applications et la caract´erisation de bases de donn´ees en reconnaissance de la parole.</title>
<date>1997</date>
<booktitle>In 16`eme JEP.</booktitle>
<marker>Montaci´e, Chollet, 1997</marker>
<rawString>C. Montaci´e and G. Chollet. 1997. Syst`emes de r´ef´erence pour l’´evaluation d’applications et la caract´erisation de bases de donn´ees en reconnaissance de la parole. In 16`eme JEP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>Semantic evaluation for spoken-language systems.</title>
<date>1994</date>
<booktitle>In DARPA Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="11225" citStr="Moore, 1994" startWordPosition="1796" endWordPosition="1797"> test set) and finally how to restrict and organize the language phenomena used in the test set. 3 The PEACE paradigm We first describe the paradigm and relate preliminary experiments with PEACE. This paradigm which is as basement for the MEDIA project will be refined by all the partners and use for an evaluation campaign between seven systems of industrial and academic sites. 3.1 Description The PEACE paradigm relies on the idea that for database querying tasks, it is possible to define a common semantic representation, onto which all the systems are able to convert their own representation (Moore, 1994). The paradigm based on data extracted from real corpus, includes both literal and contextual understanding test sets. More precisely, it provides: • the definition of a semantic representation (see 3.1.1), • the definition of a model for dialogic contexts (see 3.1.2), • the definition and typology of linguistic phenomena and dialogic functions used to selectively diagnoze the system language capabilities (anaphora resolution, constraints relaxation, etc.) (see 3.1.3), • a data structuring method. The format of the annotated data will be adapted to language resource standard annotations implem</context>
</contexts>
<marker>Moore, 1994</marker>
<rawString>R.C. Moore. 1994. Semantic evaluation for spoken-language systems. In DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Guizol Blache</author>
<author>F L´evy</author>
<author>A Nazarenko</author>
<author>S N’Guema</author>
</authors>
<title>evaluer des syst`emes de compr´ehension de textes.</title>
<date>2000</date>
<booktitle>In Ressources et Evaluation en Ing´enierie Linguistique,</booktitle>
<tech>(Eds) Duculot.</tech>
<pages>265--275</pages>
<note>Chibout</note>
<marker>Blache, L´evy, Nazarenko, N’Guema, 2000</marker>
<rawString>P. Sabatier, Ph. Blache, J. Guizol, F. L´evy, A. Nazarenko, and S. N’Guema. 2000. ´evaluer des syst`emes de compr´ehension de textes. In Ressources et Evaluation en Ing´enierie Linguistique, pages 265–275. Chibout K. et al. (Eds) Duculot.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>D Litman</author>
<author>C Kamm</author>
<author>A Abella</author>
</authors>
<title>Evaluating spoken dialogue agents with paradise: 2 cases studies.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>3</volume>
<issue>12</issue>
<contexts>
<context position="7835" citStr="Walker et al., 1998" startWordPosition="1242" endWordPosition="1245">aphic transcription as a series of keyword-value pairs. The higher the QD, the more effectively a user is able to communicate information to the system. The CE is an indicator of recognition or understanding errors; the higher it is, the fewer times a user needs to repeat himself. These metrics were evaluated on single systems (JUPITER and and MERCURY); to compare different systems of the same type, one would need a common ontology. In (Glass et al., 2000), the authors believe that CE should be related to user frustation, but to show it they would need to use the PARADISE framework. PARADISE (Walker et al., 1998) can be seen as a sort of meta-paradigm which correlates objective and subjective measurements. Its grounding hypothesis states that the goal of any SLDS is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs. With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation. PARADISE has been tested in the COMMUNICATOR project (Walker et al., 2001) with 9 systems working on the same task over di</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1998</marker>
<rawString>M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Evaluating spoken dialogue agents with paradise: 2 cases studies. Computer Speech and Language, 3(12):317–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>R Passonneau</author>
<author>J E Boland</author>
</authors>
<title>Quantitative and qualitative evaluation of darpa communicatorspoken dialog systems.</title>
<date>2001</date>
<booktitle>In Actes du 39me ACL,</booktitle>
<pages>515--522</pages>
<publisher>ACL.</publisher>
<location>Toulouse, France,</location>
<contexts>
<context position="8387" citStr="Walker et al., 2001" startWordPosition="1330" endWordPosition="1333">d need to use the PARADISE framework. PARADISE (Walker et al., 1998) can be seen as a sort of meta-paradigm which correlates objective and subjective measurements. Its grounding hypothesis states that the goal of any SLDS is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs. With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation. PARADISE has been tested in the COMMUNICATOR project (Walker et al., 2001) with 9 systems working on the same task over different databases. With four basic measures (e.g. task completion) the protocol has been able to predict 37% of user satisfaction variation, and 42% with the help of a few extra measurements on dialog acts and subtasks. One critic, one can make about PARADISE concern its cost (real user tests are costly) and the use of subjective assessment. The adaption of the DQR text understanding evaluation methodology (Sabatier et al., 2000) to speech resulted in a generic and qualitative procedure. Each element of its test set holds three parts, the Declara</context>
</contexts>
<marker>Walker, Passonneau, Boland, 2001</marker>
<rawString>M. Walker, R. Passonneau, and J.E. Boland. 2001. Quantitative and qualitative evaluation of darpa communicatorspoken dialog systems. In Actes du 39me ACL, pages 515– 522, Toulouse, France, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whittaker</author>
<author>L Terveen</author>
<author>B Nardi</author>
</authors>
<title>Reference task agenda for HCI.</title>
<date>2002</date>
<booktitle>In ISLE workshop</booktitle>
<contexts>
<context position="1710" citStr="Whittaker et al., 2002" startWordPosition="251" endWordPosition="254">ent approaches by means of reliable indicators with generic evaluation methodologies and also to reduce system development costs. However achieving independence from both the dialog system and the task performed seems to be more and more a utopia. Most of the evaluations have up to now either tackled the system as a whole, or based the measurements on dialog-context-free information. The PEACE proposal aims at bypassing some of these shortcomings by extracting, from real dialog corpora, test sets that synthesize contextual information. 1 Introduction Generally speaking common reference tasks (Whittaker et al., 2002) and methods to compare and diagnose spoken language dialog systems (SLDS) and spoken dialog techniques are lacking despite previous efforts futher discussed in the next section such as EAGLES, DISC, AUPELF ARCB2 or the ongoing American project DARPA COMMUNICATOR. Without an objective assessment of dialog systems, it is difficult to reuse previous work and to advance theories. The assessment of a dialog system is complex in part to the high integration factor and tight coupling between the various modules present in any SLDS, for which unfortunately today, no common accepted reference architec</context>
</contexts>
<marker>Whittaker, Terveen, Nardi, 2002</marker>
<rawString>S. Whittaker, L. Terveen, and B. Nardi. 2002. Reference task agenda for HCI. In ISLE workshop 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>