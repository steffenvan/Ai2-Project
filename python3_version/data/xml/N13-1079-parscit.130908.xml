<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001277">
<title confidence="0.7115155">
Unsupervised Domain Tuning to Improve Word Sense Disambiguation
Judita Preiss and Mark Stevenson
</title>
<author confidence="0.456729">
j.preiss@sheffield.ac.uk and m.stevenson@dcs.shef.ac.uk
</author>
<affiliation confidence="0.992853">
Department of Computer Science, University of Sheffield
</affiliation>
<address confidence="0.882153">
211 Portobello, Sheffield, S1 4DP, UK
</address>
<sectionHeader confidence="0.96916" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999716785714286">
The topic of a document can prove to be use-
ful information for Word Sense Disambigua-
tion (WSD) since certain meanings tend to be
associated with particular topics. This paper
presents an LDA-based approach for WSD,
which is trained using any available WSD sys-
tem to establish a sense per (Latent Dirich-
let allocation based) topic. The technique is
tested using three unsupervised and one su-
pervised WSD algorithms within the SPORT
and FINANCE domains giving a performance
increase each time, suggesting that the tech-
nique may be useful to improve the perfor-
mance of any available WSD system.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943807692308">
Assigning each word its most frequent sense (MFS)
is commonly used as a baseline in Word Sense Dis-
ambiguation (WSD). This baseline can be difficult to
beat, particularly for unsupervised systems which do
not have access to the annotated training data used to
determine the MFS. However, it has also been shown
that unsupervised methods can be used to identify
the most likely sense for each ambiguous word type
and this approach can be effective for disambigua-
tion (McCarthy et al., 2004).
Knowledge of the domain of a document has been
shown to be useful information for WSD. For ex-
ample, Khapra et al. (2010) improve the perfor-
mance of a graph-based WSD system using a small
number of hand-tagged examples, but further ex-
amples would be required for each new domain.
Agirre et al. (2009) automatically construct a the-
saurus from texts in a domain which they use for
WSD. Unfortunately, performance drops when the
thesaurus is combined with information from local
context. Stevenson et al. (2011) showed that per-
formance of an unsupervised WSD algorithm can
be improved by supplementing the context with do-
main information. Cai et al. (2007) use LDA to
create an additional feature for a supervised WSD
algorithm, by inferring topics for labeled training
data. Boyd-Graber et al. (2007) integrate a topic
model with WordNet and use it to carry out dis-
ambiguation and learn topics simultaneously. Li et
al. (2010) use sense paraphrases to estimate prob-
abilities of senses and carry out WSD. Koeling et
al. (2005) showed that automatically acquiring the
predominant sense of a word from a corpus from
the same domain increases performance (over using
a predominant sense acquired from a balanced cor-
pus), but their work requires a separate thesaurus to
be built for each domain under investigation. Nav-
igli et al. (2011) extracted relevant terms from texts
in a domain and used them to initialize a random
walk over the WordNet graph.
Our approaches rely on a one sense per topic
hypothesis (Gale et al., 1992), making use of top-
ics induced using LDA – we present three novel
techniques for exploiting domain information that
are employable with any WSD algorithm (unsuper-
vised or supervised). Using any WSD algorithm, we
create a sense per topic distribution for each LDA
topic, and the classification of a new document into a
topic determines the sense distribution of the words
within. Once a sense per topic distribution is ob-
tained, no further WSD annotation of new texts is
required. Instead of fixing domains, our technique
</bodyText>
<page confidence="0.962643">
680
</page>
<subsectionHeader confidence="0.295189">
Proceedings of NAACL-HLT 2013, pages 680–684,
</subsectionHeader>
<bodyText confidence="0.972672538461538">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
allows these to be dynamically created (using LDA)
and we using four existing publicly available WSD
algorithms (three unsupervised and one supervised)
to show that our technique increases their perfor-
mance with no changes to the original algorithm.
Section 2 briefly introduces LDA, while Section 3
describes our three techniques for adding domain
information to a WSD algorithm. The WSD algo-
rithms employed in the evaluation of our techniques
are described in Section 4 with experiments and re-
sults in Section 5. Section 6 draws our conclusions
and presents avenues for future work.
</bodyText>
<sectionHeader confidence="0.971451" genericHeader="method">
2 Latent Dirichlet allocation
</sectionHeader>
<bodyText confidence="0.999935615384616">
LDA (Blei et al., 2003) is a widely used topic model,
which views the underlying document distribution
as having a Dirichlet prior. We employ a pub-
licly available implementation of LDA1 which has
two main execution methods: parameter estimation
(model building) and inference for new data (classi-
fication of a new document). Both invocation meth-
ods produce 0 distributions (the topic-document dis-
tributions, i.e., p(ti|d) for ti topics and d document),
and 0 distributions (word-topic distributions, i.e.,
p(wj|ti) for words wj). The parameter estimation
phase also creates a list of n words most likely to be
associated with each topic.
</bodyText>
<sectionHeader confidence="0.944776" genericHeader="method">
3 Using LDA for WSD
</sectionHeader>
<bodyText confidence="0.9999823125">
The underlying idea of our approach lies in deriv-
ing a document invariant sense distribution for each
topic, p(w, s|t). Once this word sense distribution
is obtained, the underlying WSD algorithm is never
needed again. We make the assumption that while
the WSD algorithm may not be able to select the
correct sense within an individual text due to insuf-
ficient domain information, the topic specific sense
will be selected with a greater frequency over all
documents pertaining to a topic, and thus the prob-
ability distributions over senses generated in this
fashion should be more accurate.
Only the distribution p(w, s|t) is dependent on an
underlying WSD algorithm – once this distribution
is obtained, it can be combined with the LDA de-
rived 0 distribution, p(t|dnew), to compute the de-
</bodyText>
<footnote confidence="0.823267">
1http://jgibblda.sourceforge.net/.
</footnote>
<bodyText confidence="0.9293455">
sired word sense distribution within the new docu-
ment dnew:
</bodyText>
<equation confidence="0.9938515">
�p(w, s|dnew) _ p(w, s|t)p(t|dnew)
t
</equation>
<bodyText confidence="0.999975142857143">
Sections 3.1, 3.2 and 3.3 describe three different
methods for deriving p(w, s|t), and we investigate
the performance changes with different WSD algo-
rithms: two versions of Personalized PageRank, de-
scribed in Section 4.1, a similarity based WSD sys-
tem outlined in Section 4.2, and a supervised graph
based algorithm (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.99891">
3.1 Sense-based topic model (SBTM)
</subsectionHeader>
<bodyText confidence="0.999986352941176">
In its usual form, the 0 distribution generated
by LDA merely provides a word-topic distribution
(p(w|t)). However, we modify the approach to di-
rectly output p(w, s|t), but we remain able to clas-
sify (non WSD annotated) new text. The topic
model is built from documents annotated with word
senses using the chosen WSD algorithm.2 The topic
model created from this data is based on word-sense
combinations and thus 0 represents p(w, s|t).
To classify new (non sense disambiguated) doc-
uments, the model is transformed to a word (rather
than word-sense) based for: i.e., the p(w, s|t) prob-
abilities are summed over all senses of w to give re-
sulting probabilities for the wordform. A new docu-
ment, dnew, classified using this system gives rise to
a number of distributions, including the probability
of a topic given a document distribution (p(t|dnew)).
</bodyText>
<subsectionHeader confidence="0.998648">
3.2 Linear equations (LinEq)
</subsectionHeader>
<bodyText confidence="0.999922">
If the topic model is created directly from word-
forms, we can use the known probabilities p(s|w, d)
(obtained from the WSD algorithm), and p(t|d)
(from the LDA classifier) to yield an overdetermined
system of linear equations of the form
</bodyText>
<equation confidence="0.9960275">
p(s|w, d) _ � p(s|w, t)p(t|d)
t
</equation>
<bodyText confidence="0.904140625">
We use an existing implementation of linear least
squares to find a solution (i.e. p(s|w, t) for each t)
2It is not crucial to word sense disambiguate all words in the
text – a word can be passed to LDA in either its word-sense, dis-
ambiguated, form or in its raw form. While we do not attempt
this in our work, it would be possible to build a model specifi-
cally for noun senses of a word, by including noun senses of the
word and leaving the raw form for any non-noun occurrences.
</bodyText>
<page confidence="0.99465">
681
</page>
<bodyText confidence="0.995145333333333">
by minimizing the sum of squared differences be-
tween the data values and their corresponding mod-
eled values, i.e., minimizing:
</bodyText>
<equation confidence="0.9280195">
2
� p(sjw, d) − � p(sjw,t)p(tjd)
</equation>
<bodyText confidence="0.638494">
d t
</bodyText>
<subsectionHeader confidence="0.953993">
3.3 Topic words (TopicWord)
</subsectionHeader>
<bodyText confidence="0.9999462">
The techniques presented in Sections 3.1 and 3.2
both require the WSD algorithm to annotate a rea-
sonably high proportion of the data used to build the
topic model. For systems which do not rely on word
order, an alternative based on the most likely words
per topic is possible: the LDA algorithm generates
0, a word-topic distribution. It is therefore possible
to extract the most likely words per topic.
To acquire a sense-topic distribution for a topic t,
each target word w is included in a bag of words
which includes the most likely words for t and
the unsupervised WSD algorithm is executed (w is
added to the list if t does not already contain it).
This technique is not applicable to non bag-of-words
WSD algorithms, as structure is absent.
</bodyText>
<sectionHeader confidence="0.989137" genericHeader="method">
4 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.9998956">
Only the topic model documents need to be auto-
matically annotated with the chosen WSD system,
after this, the WSD system is never applied again
(an LDA classification determines the sense distri-
bution) – this is particularly useful for supervised
system which frequently have a long execution time.
We explore three different types of WSD system:
two versions of a knowledge base based system
(Section 4.1), an unsupervised system (Section 4.2)
and a supervised system (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.975673">
4.1 Personalized PageRank (ppr and w2w)
</subsectionHeader>
<bodyText confidence="0.999649875">
We use the freely available3 Personalized PageRank
algorithm (Agirre and Soroa, 2009) with WordNet
3.0. In Section 5 we present results from two options
of the Personalized PageRank algorithm: ppr, which
performs one PageRank calculation for a whole con-
tent, and w2w, which performs one PageRank cal-
culation for every word in the context to be disam-
biguated.
</bodyText>
<footnote confidence="0.967273">
3Available from http://ixa2.si.ehu.es/ukb/
</footnote>
<subsectionHeader confidence="0.929508">
4.2 WordNet similarity (sim)
</subsectionHeader>
<bodyText confidence="0.999699833333333">
We also evaluated another unsupervised approach,
the Perl package WordNet::SenseRelate::AllWords
(Pedersen and Kolhatkar, 2009), which finds senses
of each word in a text based on senses of the sur-
rounding words. The algorithm is invoked with Lesk
similarity (Banerjee and Pedersen, 2002).
</bodyText>
<subsectionHeader confidence="0.978429">
4.3 Vector space model (vsm)
</subsectionHeader>
<bodyText confidence="0.9997806">
An existing vector space model (VSM) based state-
of-the-art supervised WSD system with features de-
rived from the text surrounding the ambiguous word
(Stevenson et al., 2008) is trained on Semcor (Miller
et al., 1993).4
</bodyText>
<sectionHeader confidence="0.999582" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.844021">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999971066666667">
The approach is evaluated using a domain-specific
WSD corpus (Koeling et al., 2005) which includes
articles from the FINANCE and SPORTS domains
taken from the Reuters corpus (Rose et al., 2002).
This corpus contains 100 manually annotated in-
stances (from each domain) for 41 words.5
The word-sense LDA topic models are created
from 80,128 documents randomly selected from the
Reuters corpus (this corresponds to a tenth of the en-
tire Reuters corpus). LDA can abstract a model from
a relatively small corpus and a tenth of the Reuters
corpus is much more manageable in terms of mem-
ory and time requirements, particularly given the
need to word sense disambiguate (some part of) each
document in this dataset.6
</bodyText>
<footnote confidence="0.9633854">
4A version of Semcor automatically transformed to
WordNet 3.0 available from http://www.cse.unt.edu/
˜rada/downloads.html#semcor was used in this work.
5Unfortunately, the entire domain-specific sense disam-
biguated corpus could not be used in the evaluation of
our system, as the released corpus does not link each
annotated sentence to its source document, and it is
not always possible to recover these; approximately 87%
of the data could be used. This dataset is available
at http://staffwww.dcs.shef.ac.uk/people/J.
Preiss/downloads/source_texts.tgz
6In this work, all 80,128 documents were word sense disam-
biguated. However, it would be possible to restrict this set to a
smaller number, as long as a reliable distribution of word senses
per topic could be obtained.
</footnote>
<page confidence="0.992266">
682
</page>
<table confidence="0.998653">
ppr w2w sim vsm
Baseline 36 41 23 27
SBTM model 39 43 30 31
LinEq 41 44 – 33
TopicWord 38 41 – –
</table>
<tableCaption confidence="0.99984">
Table 1: Summary of results based on 150 topics
</tableCaption>
<subsectionHeader confidence="0.801407">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999879022727273">
Table 1 presents the performance results for the four
WSD algorithms based on 150 topics. A range of
topic values was explored, and 150 topics yielded
highest performance, though the variance between
the performance based on different topics (ranging
from 50 to 250) was very small (0.4% difference to
the average performance with 250 topics, and 3%
with 50). The performance shown indicates the pre-
cision (number correct / number attempted). Recall
is 100% in all cases.
The similarity algorithm (sim) fails on certain
documents and therefore the linear equations tech-
nique could not be applied. The topic word tech-
nique (TopicWord) could not be evaluated using the
similarity algorithm, due to the high sensitivity to
word order within the test paragraph. In addition,
the topic words technique is not applicable to su-
pervised systems, due to its reliance on structured
sentences. The best results with this technique were
obtained with including all likely words with proba-
bilities exceeding 0.001 and smoothing of 0.1 of the
topic document distribution.
Using a Wilcoxon signed-rank test, the results
were found to be significantly better over the orig-
inal algorithms in every case (apart from Topic-
Words). Both the WordNet similarity (sim) and
the VSM approach (vsm) have a lower performance
than the two PPR based WSD algorithms (ppt and
w2w). For example, sim assigns the same (usually
incorrect) sense to all occurrences of the word tie,
while both PPR based algorithms detect an obvious
domain change. The vsm approach suffers from a
lack of training data (only a small number of exam-
ples of each word appear in Semcor), while sim does
not get enough information from the context.
As an interesting aside, the topic models based on
word-sense combinations, as opposed to wordforms
only, are more informative with less overlap. Exam-
ining the word stake annotated with the w2w WSD
algorithm: only topic 1 contains stake among the top
12 terms associated with a topic in the word-sense
model, while 10 topics are found in the wordform
topic model. Table 2 shows the top 12 terms associ-
ated with topics containing the word stake.
</bodyText>
<table confidence="0.5998668">
Topic Word-based model
39 say, will, company, share, deal, euro-
pean, buy, agreement, stake, new, hun-
gary, oil
63 say, share, united, market, offer, stock,
union, percent, stake, will, point, new
90 say, will, fund, price, london, sell,
stake, indonesia, court, investment,
share, buy
91 say, market, bond, russia, press, party,
stake, russian, country, indonesia, new,
election
97 say, million, bank, uk, percent, share,
stake, world, will, year, central, british
113 say, will, percent, week, billion, last,
italy, plan, stake, year, budget, czech
134 say, china, percent, hong, kong, offi-
cial, stake, billion, report, buy, group,
year
142 say, percent, market, first, bank, rate,
year, dealer, million, money, close,
stake
145 say, will, new, brazil, dollar, group,
percent, stake, year, one, make, do
147 say, yen, forecast, million, parent, mar-
ket, share, will, profit, percent, stake,
group
Sense-based model
1 stake*13286801-n, share*13285176-
n, sell*02242464-v, buy*02207206-v,
have*02204692-v, group*00031264-
n, company*08058098-n,
percent*13817526-n, hold*02203362-
v, deal*01110274-n, shareholder,
interest*13286801-n
</table>
<tableCaption confidence="0.9934165">
Table 2: The presence of stake within the word- and
sense-based topic models
</tableCaption>
<page confidence="0.998986">
683
</page>
<sectionHeader confidence="0.998796" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999947444444444">
We present three unsupervised techniques based on
acquiring LDA topics which can be used to improve
the performance of a number of WSD algorithms.
All approaches make use of topic information ob-
tained using LDA and do not require any modifi-
cation of the underlying WSD system. While the
technique is dependent on the accuracy of the WSD
algorithm, it consistently outperforms the baselines
for all four different algorithms.
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9990255">
This research was supported by a Google Research
Award. Our thanks also go to the two anonymous
reviewers whose comments have made this paper
much clearer.
</bodyText>
<sectionHeader confidence="0.985785" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.721551578947368">
Agirre, E., de Lacalle, O. L., and Soroa, A. (2009).
Knowledge-based WSD on specific domains: per-
forming better than generic supervised WSD. In Pro-
ceedings of the 21st International Joint Conference on
Artificial Intelligence, pages 1501–1506.
Agirre, E. and Soroa, A. (2009). Personalizing pager-
ank for word sense disambiguation. In Proceedings of
EACL.
Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In Proceedings of the Third International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 135–145.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent
Dirichlet allocation. Journal of Machine Learning Re-
search, 3:993–1022.
Boyd-Graber, J., Blei, D., and Zhu, X. (2007). A topic
model for word sense disambiguation. In Proceedings
of the EMNLP-CoNLL, pages 1024–1033.
</bodyText>
<reference confidence="0.99930510909091">
Cai, J. F., Lee, W. S., and Teh, Y. W. (2007). Nus-ml:
Improving word sense disambiguation using topic fea-
tures. In Proceedings of SEMEVAL.
Gale, W. A., Church, K. W., and Yarowsky, D. (1992).
One sense per discourse. In Proceedings of the
4th DARPA Speech and Natural Language Workshop,
pages 233–237.
Khapra, M., Kulkarni, A., Sohoney, S., and Bhat-
tacharyya, P. (2010). All words domain adapted WSD:
Finding a middle ground between supervision and
unsupervision. In Proceedings of ACL 2010, pages
1532–1541, Uppsala, Sweden.
Koeling, R., Mccarthy, D., and Carroll, J. (2005). Do-
main specific sense distributions and predominant
sense acquisition. In Proceedings of Joint HLT-
EMNLP05, pages 419–426.
Li, L., Roth, B., and Sporleder, C. (2010). Topic models
for word sense disambiguation and token-based idiom
detection. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1138–1147.
McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.
(2004). Finding predominant senses in untagged text.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 280–
287.
Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technology,
pages 303–308.
Navigli, R., Faralli, S., Soroa, A., de Lacalle, O. L., and
Agirre, E. (2011). Two birds with one stone: learn-
ing semantic models for text categorization and word
sense disambiguation. In CIKM, pages 2317–2320.
Pedersen, T. and Kolhatkar, V. (2009). Word-
net::senserelate::allwords - a broad coverageword
sense tagger that maximizes semantic relatedness
(demonstration system). In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
Conference, pages 17–20.
Rose, T. G., Stevenson, M., and Whitehead, M. (2002).
The Reuters corpus volume 1 - from yesterday’s news
to tomorrow’s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, pages 827–832.
Stevenson, M., Agirre, E., and Soroa, A. (2012). Exploit-
ing domain information for word sense disambigua-
tion of medical documents. Journal of the American
Medical Informatics Association, 19(2):235–240.
Stevenson, M., Guo, Y., Gaizauskas, R., and Martinez,
D. (2008). Knowledge sources for word sense disam-
biguation of biomedical text. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing at ACL, pages 80–87.
</reference>
<page confidence="0.998587">
684
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798032">
<title confidence="0.998559">Unsupervised Domain Tuning to Improve Word Sense Disambiguation</title>
<author confidence="0.81483">Preiss</author>
<affiliation confidence="0.998862">Department of Computer Science, University of</affiliation>
<address confidence="0.996704">211 Portobello, Sheffield, S1 4DP, UK</address>
<abstract confidence="0.9988386">The topic of a document can prove to be useful information for Word Sense Disambiguation (WSD) since certain meanings tend to be associated with particular topics. This paper presents an LDA-based approach for WSD, which is trained using any available WSD system to establish a sense per (Latent Dirichlet allocation based) topic. The technique is tested using three unsupervised and one su- WSD algorithms within the giving a performance increase each time, suggesting that the technique may be useful to improve the performance of any available WSD system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Cai</author>
<author>W S Lee</author>
<author>Y W Teh</author>
</authors>
<title>Nus-ml: Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of SEMEVAL.</booktitle>
<contexts>
<context position="2005" citStr="Cai et al. (2007)" startWordPosition="319" endWordPosition="322">nt has been shown to be useful information for WSD. For example, Khapra et al. (2010) improve the performance of a graph-based WSD system using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate the</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Cai, J. F., Lee, W. S., and Teh, Y. W. (2007). Nus-ml: Improving word sense disambiguation using topic features. In Proceedings of SEMEVAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th DARPA Speech and Natural Language Workshop,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="2872" citStr="Gale et al., 1992" startWordPosition="465" endWordPosition="468">usly. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to initialize a random walk over the WordNet graph. Our approaches rely on a one sense per topic hypothesis (Gale et al., 1992), making use of topics induced using LDA – we present three novel techniques for exploiting domain information that are employable with any WSD algorithm (unsupervised or supervised). Using any WSD algorithm, we create a sense per topic distribution for each LDA topic, and the classification of a new document into a topic determines the sense distribution of the words within. Once a sense per topic distribution is obtained, no further WSD annotation of new texts is required. Instead of fixing domains, our technique 680 Proceedings of NAACL-HLT 2013, pages 680–684, Atlanta, Georgia, 9–14 June 2</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W. A., Church, K. W., and Yarowsky, D. (1992). One sense per discourse. In Proceedings of the 4th DARPA Speech and Natural Language Workshop, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Khapra</author>
<author>A Kulkarni</author>
<author>S Sohoney</author>
<author>P Bhattacharyya</author>
</authors>
<title>All words domain adapted WSD: Finding a middle ground between supervision and unsupervision.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>1532--1541</pages>
<location>Uppsala,</location>
<contexts>
<context position="1473" citStr="Khapra et al. (2010)" startWordPosition="232" endWordPosition="235">roduction Assigning each word its most frequent sense (MFS) is commonly used as a baseline in Word Sense Disambiguation (WSD). This baseline can be difficult to beat, particularly for unsupervised systems which do not have access to the annotated training data used to determine the MFS. However, it has also been shown that unsupervised methods can be used to identify the most likely sense for each ambiguous word type and this approach can be effective for disambiguation (McCarthy et al., 2004). Knowledge of the domain of a document has been shown to be useful information for WSD. For example, Khapra et al. (2010) improve the performance of a graph-based WSD system using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algori</context>
</contexts>
<marker>Khapra, Kulkarni, Sohoney, Bhattacharyya, 2010</marker>
<rawString>Khapra, M., Kulkarni, A., Sohoney, S., and Bhattacharyya, P. (2010). All words domain adapted WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of ACL 2010, pages 1532–1541, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Koeling</author>
<author>D Mccarthy</author>
<author>J Carroll</author>
</authors>
<title>Domain specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of Joint HLTEMNLP05,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="2375" citStr="Koeling et al. (2005)" startWordPosition="381" endWordPosition="384">formance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to initialize a random walk over the WordNet graph. Our approaches rely on a one sense per topic hypothesis (Gale et al., 1992), making use of topics induced using LDA – we present three novel techniques for exploiting domain info</context>
<context position="10288" citStr="Koeling et al., 2005" startWordPosition="1677" endWordPosition="1680">her unsupervised approach, the Perl package WordNet::SenseRelate::AllWords (Pedersen and Kolhatkar, 2009), which finds senses of each word in a text based on senses of the surrounding words. The algorithm is invoked with Lesk similarity (Banerjee and Pedersen, 2002). 4.3 Vector space model (vsm) An existing vector space model (VSM) based stateof-the-art supervised WSD system with features derived from the text surrounding the ambiguous word (Stevenson et al., 2008) is trained on Semcor (Miller et al., 1993).4 5 Experiments 5.1 Data The approach is evaluated using a domain-specific WSD corpus (Koeling et al., 2005) which includes articles from the FINANCE and SPORTS domains taken from the Reuters corpus (Rose et al., 2002). This corpus contains 100 manually annotated instances (from each domain) for 41 words.5 The word-sense LDA topic models are created from 80,128 documents randomly selected from the Reuters corpus (this corresponds to a tenth of the entire Reuters corpus). LDA can abstract a model from a relatively small corpus and a tenth of the Reuters corpus is much more manageable in terms of memory and time requirements, particularly given the need to word sense disambiguate (some part of) each d</context>
</contexts>
<marker>Koeling, Mccarthy, Carroll, 2005</marker>
<rawString>Koeling, R., Mccarthy, D., and Carroll, J. (2005). Domain specific sense distributions and predominant sense acquisition. In Proceedings of Joint HLTEMNLP05, pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>B Roth</author>
<author>C Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1138--1147</pages>
<contexts>
<context position="2276" citStr="Li et al. (2010)" startWordPosition="364" endWordPosition="367">ically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to initialize a random walk over the WordNet graph. Our approaches rely on a one sense per topic hypothesis (Gale et al., 1992), ma</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Li, L., Roth, B., and Sporleder, C. (2010). Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<contexts>
<context position="1351" citStr="McCarthy et al., 2004" startWordPosition="209" endWordPosition="212">ncrease each time, suggesting that the technique may be useful to improve the performance of any available WSD system. 1 Introduction Assigning each word its most frequent sense (MFS) is commonly used as a baseline in Word Sense Disambiguation (WSD). This baseline can be difficult to beat, particularly for unsupervised systems which do not have access to the annotated training data used to determine the MFS. However, it has also been shown that unsupervised methods can be used to identify the most likely sense for each ambiguous word type and this approach can be effective for disambiguation (McCarthy et al., 2004). Knowledge of the domain of a document has been shown to be useful information for WSD. For example, Khapra et al. (2010) improve the performance of a graph-based WSD system using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing </context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, D., Koeling, R., Weeds, J., and Carroll, J. (2004). Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 280– 287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="10179" citStr="Miller et al., 1993" startWordPosition="1660" endWordPosition="1663">isambiguated. 3Available from http://ixa2.si.ehu.es/ukb/ 4.2 WordNet similarity (sim) We also evaluated another unsupervised approach, the Perl package WordNet::SenseRelate::AllWords (Pedersen and Kolhatkar, 2009), which finds senses of each word in a text based on senses of the surrounding words. The algorithm is invoked with Lesk similarity (Banerjee and Pedersen, 2002). 4.3 Vector space model (vsm) An existing vector space model (VSM) based stateof-the-art supervised WSD system with features derived from the text surrounding the ambiguous word (Stevenson et al., 2008) is trained on Semcor (Miller et al., 1993).4 5 Experiments 5.1 Data The approach is evaluated using a domain-specific WSD corpus (Koeling et al., 2005) which includes articles from the FINANCE and SPORTS domains taken from the Reuters corpus (Rose et al., 2002). This corpus contains 100 manually annotated instances (from each domain) for 41 words.5 The word-sense LDA topic models are created from 80,128 documents randomly selected from the Reuters corpus (this corresponds to a tenth of the entire Reuters corpus). LDA can abstract a model from a relatively small corpus and a tenth of the Reuters corpus is much more manageable in terms </context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T. (1993). A semantic concordance. In Proceedings of the ARPA Workshop on Human Language Technology, pages 303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>S Faralli</author>
<author>A Soroa</author>
<author>O L de Lacalle</author>
<author>E Agirre</author>
</authors>
<title>Two birds with one stone: learning semantic models for text categorization and word sense disambiguation.</title>
<date>2011</date>
<booktitle>In CIKM,</booktitle>
<pages>2317--2320</pages>
<marker>Navigli, Faralli, Soroa, de Lacalle, Agirre, 2011</marker>
<rawString>Navigli, R., Faralli, S., Soroa, A., de Lacalle, O. L., and Agirre, E. (2011). Two birds with one stone: learning semantic models for text categorization and word sense disambiguation. In CIKM, pages 2317–2320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>V Kolhatkar</author>
</authors>
<title>Wordnet::senserelate::allwords - a broad coverageword sense tagger that maximizes semantic relatedness (demonstration system).</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies Conference,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="9772" citStr="Pedersen and Kolhatkar, 2009" startWordPosition="1592" endWordPosition="1595">pervised system (Section 4.3). 4.1 Personalized PageRank (ppr and w2w) We use the freely available3 Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet 3.0. In Section 5 we present results from two options of the Personalized PageRank algorithm: ppr, which performs one PageRank calculation for a whole content, and w2w, which performs one PageRank calculation for every word in the context to be disambiguated. 3Available from http://ixa2.si.ehu.es/ukb/ 4.2 WordNet similarity (sim) We also evaluated another unsupervised approach, the Perl package WordNet::SenseRelate::AllWords (Pedersen and Kolhatkar, 2009), which finds senses of each word in a text based on senses of the surrounding words. The algorithm is invoked with Lesk similarity (Banerjee and Pedersen, 2002). 4.3 Vector space model (vsm) An existing vector space model (VSM) based stateof-the-art supervised WSD system with features derived from the text surrounding the ambiguous word (Stevenson et al., 2008) is trained on Semcor (Miller et al., 1993).4 5 Experiments 5.1 Data The approach is evaluated using a domain-specific WSD corpus (Koeling et al., 2005) which includes articles from the FINANCE and SPORTS domains taken from the Reuters </context>
</contexts>
<marker>Pedersen, Kolhatkar, 2009</marker>
<rawString>Pedersen, T. and Kolhatkar, V. (2009). Wordnet::senserelate::allwords - a broad coverageword sense tagger that maximizes semantic relatedness (demonstration system). In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies Conference, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Rose</author>
<author>M Stevenson</author>
<author>M Whitehead</author>
</authors>
<title>The Reuters corpus volume 1 - from yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation,</booktitle>
<pages>827--832</pages>
<contexts>
<context position="10398" citStr="Rose et al., 2002" startWordPosition="1695" endWordPosition="1698">finds senses of each word in a text based on senses of the surrounding words. The algorithm is invoked with Lesk similarity (Banerjee and Pedersen, 2002). 4.3 Vector space model (vsm) An existing vector space model (VSM) based stateof-the-art supervised WSD system with features derived from the text surrounding the ambiguous word (Stevenson et al., 2008) is trained on Semcor (Miller et al., 1993).4 5 Experiments 5.1 Data The approach is evaluated using a domain-specific WSD corpus (Koeling et al., 2005) which includes articles from the FINANCE and SPORTS domains taken from the Reuters corpus (Rose et al., 2002). This corpus contains 100 manually annotated instances (from each domain) for 41 words.5 The word-sense LDA topic models are created from 80,128 documents randomly selected from the Reuters corpus (this corresponds to a tenth of the entire Reuters corpus). LDA can abstract a model from a relatively small corpus and a tenth of the Reuters corpus is much more manageable in terms of memory and time requirements, particularly given the need to word sense disambiguate (some part of) each document in this dataset.6 4A version of Semcor automatically transformed to WordNet 3.0 available from http://</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Rose, T. G., Stevenson, M., and Whitehead, M. (2002). The Reuters corpus volume 1 - from yesterday’s news to tomorrow’s language resources. In Proceedings of the Third International Conference on Language Resources and Evaluation, pages 827–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Exploiting domain information for word sense disambiguation of medical documents.</title>
<date>2012</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Stevenson, Agirre, Soroa, 2012</marker>
<rawString>Stevenson, M., Agirre, E., and Soroa, A. (2012). Exploiting domain information for word sense disambiguation of medical documents. Journal of the American Medical Informatics Association, 19(2):235–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Guo</author>
<author>R Gaizauskas</author>
<author>D Martinez</author>
</authors>
<title>Knowledge sources for word sense disambiguation of biomedical text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing at ACL,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="10136" citStr="Stevenson et al., 2008" startWordPosition="1652" endWordPosition="1655">culation for every word in the context to be disambiguated. 3Available from http://ixa2.si.ehu.es/ukb/ 4.2 WordNet similarity (sim) We also evaluated another unsupervised approach, the Perl package WordNet::SenseRelate::AllWords (Pedersen and Kolhatkar, 2009), which finds senses of each word in a text based on senses of the surrounding words. The algorithm is invoked with Lesk similarity (Banerjee and Pedersen, 2002). 4.3 Vector space model (vsm) An existing vector space model (VSM) based stateof-the-art supervised WSD system with features derived from the text surrounding the ambiguous word (Stevenson et al., 2008) is trained on Semcor (Miller et al., 1993).4 5 Experiments 5.1 Data The approach is evaluated using a domain-specific WSD corpus (Koeling et al., 2005) which includes articles from the FINANCE and SPORTS domains taken from the Reuters corpus (Rose et al., 2002). This corpus contains 100 manually annotated instances (from each domain) for 41 words.5 The word-sense LDA topic models are created from 80,128 documents randomly selected from the Reuters corpus (this corresponds to a tenth of the entire Reuters corpus). LDA can abstract a model from a relatively small corpus and a tenth of the Reute</context>
</contexts>
<marker>Stevenson, Guo, Gaizauskas, Martinez, 2008</marker>
<rawString>Stevenson, M., Guo, Y., Gaizauskas, R., and Martinez, D. (2008). Knowledge sources for word sense disambiguation of biomedical text. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing at ACL, pages 80–87.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>