<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003241">
<title confidence="0.9987965">
Detecting Semantic Relations between Named Entities in Text
Using Contextual Features
</title>
<author confidence="0.952573">
Toru Hirano, Yoshihiro Matsuo, Genichiro Kikui
</author>
<affiliation confidence="0.679595">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.827451">
1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, 239-0847, Japan
</address>
<email confidence="0.998752">
{hirano.tohru, matsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939428571429">
This paper proposes a supervised learn-
ing method for detecting a semantic rela-
tion between a given pair of named enti-
ties, which may be located in different sen-
tences. The method employs newly intro-
duced contextual features based on center-
ing theory as well as conventional syntac-
tic and word-based features. These features
are organized as a tree structure and are
fed into a boosting-based classification al-
gorithm. Experimental results show the pro-
posed method outperformed prior methods,
and increased precision and recall by 4.4%
and 6.7%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903634146341">
Statistical and machine learning NLP techniques are
now so advanced that named entity (NE) taggers are
in practical use. Researchers are now focusing on
extracting semantic relations between NEs, such as
“George Bush (person)” is “president (relation)” of
“the United States (location)”, because they provide
important information used in information retrieval,
question answering, and summarization.
We represent a semantic relation between two
NEs with a tuple [NE1, NE2, Relation Label]. Our
final goal is to extract tuples from a text. For exam-
ple, the tuple [George Bush (person), the U.S. (loca-
tion), president (Relation Label)] would be extracted
from the sentence “George Bush is the president of
the U.S.”. There are two tasks in extracting tuples
from text. One is detecting whether or not a given
pair of NEs are semantically related (relation detec-
tion), and the other is determining the relation label
(relation characterization).
In this paper, we address the task of relation de-
tection. So far, various supervised learning ap-
proaches have been explored in this field (Culotta
and Sorensen, 2004; Zelenko et al., 2003). They
use two kinds of features: syntactic ones and word-
based ones, for example, the path of the given pair of
NEs in the parse tree and the word n-gram between
NEs (Kambhatla, 2004).
These methods have two problems which we con-
sider in this paper. One is that they target only intra-
sentential relation detection in which NE pairs are
located in the same sentence, in spite of the fact that
about 35% of NE pairs with semantic relations are
inter-sentential (See Section 3.1). The other is that
the methods can not detect semantic relations cor-
rectly when NE pairs located in a parallel sentence
arise from a predication ellipsis. In the following
Japanese example1, the syntactic feature, which is
the path of two NEs in the dependency structure,
of the pair with a semantic relation (“Ken11” and
“Tokyo12”) is the same as the feature of the pair with
no semantic relation (“Ken11” and “New York14”).
</bodyText>
<equation confidence="0.94752875">
(S-1) Ken11-wa Tokyo12-de, Tom13-wa
New York14-de umareta15.
(Ken11 was born15 in Tokyo12, Tom13 in
New York14.)
</equation>
<bodyText confidence="0.999869166666667">
To solve the above problems, we propose a super-
vised learning method using contextual features.
The rest of this paper is organized as follows. Sec-
tion 2 describes the proposed method. We report the
results of our experiments in Section 3 and conclude
the paper in Section 4.
</bodyText>
<sectionHeader confidence="0.989772" genericHeader="method">
2 Relation Detection
</sectionHeader>
<bodyText confidence="0.998880428571429">
The proposed method employs contextual features
based on centering theory (Grosz et al., 1983) as
well as conventional syntactic and word-based fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classification
algorithm. The method consists of three parts: pre-
processing (POS tagging, NE tagging, and parsing),
</bodyText>
<footnote confidence="0.983471">
1The numbers show correspondences of words between
Japanese and English.
</footnote>
<page confidence="0.952922">
157
</page>
<bodyText confidence="0.905097571428571">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157–160,
Prague, June 2007. c�2007 Association for Computational Linguistics
feature extraction (contextual, syntactic, and word-
based features), and classification.
In this section, we describe the underlying idea of
contextual features and how contextual features are
used for detecting semantic relations.
</bodyText>
<subsectionHeader confidence="0.994023">
2.1 Contextual Features
</subsectionHeader>
<bodyText confidence="0.944727142857143">
When a pair of NEs with a semantic relation appears
in different sentences, the antecedent NE must be
contextually easily referred to in the sentence with
the following NE. In the following Japanese exam-
ple, the pair “Ken22” and “amerika32 (the U.S.)”
have a semantic relation “wataru33 (go)”, because
“Ken22” is contextually referred to in the sentence
with “amerika32” (In fact, the zero pronoun Oi
refers to “Ken22”). Meanwhile, the pair “Naomi25”
and “amerika32” has no semantic relation, because
the sentence with “amerika32” does not refer to
“Naomi25”.
(S-2) asu21, Ken22-wa Osaka23-o otozure24
Naomi25-to au26.
</bodyText>
<equation confidence="0.950867166666666">
(Ken22 is going to visit24 Osaka23 to see26
Naomi25, tomorrow21.)
(S-3) sonogo31, (Oi-ga) amerika32-ni watari33
Tom34-to ryoko35 suru.
(Then31, (hei) will go33 to the U.S.32 to travel35
with Tom34.)
</equation>
<bodyText confidence="0.999976">
Furthermore, when a pair of NEs with a seman-
tic relation appears in a parallel sentence arise from
predication ellipsis, the antecedent NE is contextu-
ally easily referred to in the phrase with the follow-
ing NE. In the example of “(S-1)”, the pair “Ken11”
and “Tokyo12” have a semantic relation “umareta15
(was born)”. Meanwhile, the pair “Ken11” and
“New York14” has no semantic relation.
Therefore, using whether the antecedent NE is re-
ferred to in the context with the following NE as fea-
tures of a given pair of NEs would improve relation
detection performance. In this paper, we use cen-
tering theory (Kameyama, 1986) to determine how
easily a noun phrase can be referred to in the follow-
ing context.
</bodyText>
<subsectionHeader confidence="0.999862">
2.2 Centering Theory
</subsectionHeader>
<bodyText confidence="0.999956857142857">
Centering theory is an empirical sorting rule used to
identify the antecedents of (zero) pronouns. When
there is a (zero) pronoun in the text, noun phrases
that are in the previous context of the pronoun are
sorted in order of likelihood of being the antecedent.
The sorting algorithm has two steps. First, from the
beginning of the text until the pronoun appears, noun
</bodyText>
<table confidence="0.8217915">
Priority
wa Ken
ga
ni
o Osaka
others asu, Naomi
</table>
<figureCaption confidence="0.965163">
Figure 1: Information Stacked According to Center-
ing Theory
</figureCaption>
<bodyText confidence="0.985993142857143">
phrases are stacked depending on case markers such
as particles. In the above example, noun phrases,
“asu21”, “Ken22”, “Osaka23” and “Naomi25”, which
are in the previous context of the zero pronoun Oi,
are stacked and then the information shown in Fig-
ure 1 is acquired. Second, the stacked information is
sorted by the following rules.
</bodyText>
<listItem confidence="0.98073275">
1. The priority of case markers is as follows: “wa
&gt; ga &gt; ni &gt; o &gt; others”
2. The priority of stack structure is as follows:
last-in first-out, in the same case marker
</listItem>
<bodyText confidence="0.9996574">
For example, Figure 1 is sorted by the above rules
and then the order, 1: “Ken22”, 2: “Osaka23”, 3:
“Naomi25”, 4: “asu21”, is assigned. In this way, us-
ing centering theory would show that the antecedent
of the zero pronoun Oi is “Ken22”.
</bodyText>
<subsectionHeader confidence="0.998031">
2.3 Applying Centering Theory
</subsectionHeader>
<bodyText confidence="0.999962625">
When detecting a semantic relation between a given
pair of NEs, we use centering theory to determine
how easily the antecedent NE can be referred to in
the context with the following NE. Note that we do
not explicitly execute anaphora resolutions here.
Applied centering theory to relation detection is
as follows. First, from the beginning of the text until
the following NE appears, noun phrases are stacked
depending on case markers, and the stacked infor-
mation is sorted by the above rules (Section 2.2).
Then, if the top noun phrase in the sorted order is
identical to the antecedent NE, the antecedent NE is
”positive” when being referred to in the context with
the following NE.
When the pair of NEs, “Ken22” and “amerika32”,
is given in the above example, the noun phrases,
“asu21”, “Ken22”, “Osaka23” and “Naomi25”, which
are in the previous context of the following NE
“amerika32”, are stacked (Figure 1). Then they are
sorted by the above sorting rules and the order, 1:
“Ken22”, 2: “Osaka23”, 3: “Naomi25”, 4: “asu21”,
is acquired. Here, because the top noun phrase in
the sorted order is identical to the antecedent NE,
the antecedent NE “Ken22” is ”positive” when be-
</bodyText>
<page confidence="0.996791">
158
</page>
<figureCaption confidence="0.976866">
Figure 2: Centering Structure
</figureCaption>
<table confidence="0.470398777777778">
wa: Ken
amerika
others: Naomi
others: asst
o: Osaka
Type % of pairs with semantic relations
(A) Intra-sentential 31.4% (3333 / 10626)
(B) Inter-sentential 0.8% (1777 / 225516)
(A)+(B) Total 2.2% (5110 / 236142)
</table>
<tableCaption confidence="0.9618135">
Table 1: Percent of pairs with semantic relations in
annotated text
</tableCaption>
<bodyText confidence="0.9373872">
ing referred to in the context with the following NE
“amerika32”. Whether or not the antecedent NE is
referred to in the context with the following NE is
used as a feature. We call this feature Centering Top
(CT).
</bodyText>
<subsectionHeader confidence="0.999001">
2.4 Using Stack Structure
</subsectionHeader>
<bodyText confidence="0.999976666666667">
The sorting algorithm using centering theory tends
to rank highly thoes words that easily become sub-
jects. However, for relation detection, it is necessary
to consider both NEs that easily become subjects,
such as person and organization, and NEs that do not
easily become subjects, such as location and time.
We use the stack described in Section 2.3 as a
structural feature for relation detection. We call this
feature Centering Structure (CS). For example, the
stacked information shown in Figure 1 is assumed
to be structure information, as shown in Figure 2.
The method of converting from a stack (Figure 1)
into a structure (Figure 2) is described as follows.
First, the following NE, “amerika32”, becomes the
root node because Figure 1 is stacked information
until the following NE appears. Then, the stacked
information is converted to Figure 2 depending on
the case markers. We use the path of the given pair
of NEs in the structure as a feature. For example,
“amerika32 → wa:Ken22”2 is used as the feature of
the given pair “Ken22” and “amerika32”.
</bodyText>
<subsectionHeader confidence="0.985673">
2.5 Classification Algorithm
</subsectionHeader>
<bodyText confidence="0.991039875">
There are several structure-based learning algo-
rithms proposed so far (Collins and Duffy, 2001;
Suzuki et al., 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto’s
boosting-based algorithm using sub trees as features,
which is implemented as the BACT system.
In relation detection, given a set of training exam-
ples each of which represents contextual, syntactic,
and word-based features of a pair of NEs as a tree
labeled as either having semantic relations or not,
the BACT system learns that a set of rules are ef-
fective in classifying. Then, given a test instance,
which represents contextual, syntactic, and word-
2“A → B” means A has a dependency relation to B.
based features of a pair of NEs as a tree, the BACT
system classifies using a set of learned rules.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999885333333333">
We experimented with texts from Japanese newspa-
pers and weblogs to test the proposed method. The
following four models were compared:
</bodyText>
<listItem confidence="0.9978738">
1. WD : Pairs of NEs within n words are detected
as pairs with semantic relation.
2. STR : Supervised learning method using syn-
tactic3 and word-based features, the path of the
pairs of NEs in the parse tree and the word n-
gram between pairs of NEs (Kambhatla, 2004)
3. STR-CT : STR with the centering top feature
explained in Section 2.3.
4. STR-CS : STR with the centering structure fea-
ture explained in Section 2.4.
</listItem>
<subsectionHeader confidence="0.9993">
3.1 Setting
</subsectionHeader>
<bodyText confidence="0.9998262">
We used 1451 texts from Japanese newspapers and
weblogs, whose semantic relations between person
and location had been annotated by humans for the
experiments4. There were 5110 pairs with seman-
tic relations out of 236,142 pairs in the annotated
text. We conducted ten-fold cross-validation over
236,142 pairs of NEs so that sets of pairs from a
single text were not divided into the training and test
sets.
We also divided pairs of NEs into two types: (A)
intra-sentential and (B) inter-sentential. The reason
for dividing them is so that syntactic structure fea-
tures would be effective in type (A) and contextual
features would be effective in type (B). Another rea-
son is that the percentage of pairs with semantic rela-
tions out of the total pairs in the annotated text differ
significantly between types, as shown in Table 1.
In the experiments, all features were automati-
cally acquired using a Japanese morphological and
dependency structure analyzer.
</bodyText>
<footnote confidence="0.9999905">
3There is no syntactic feature in inter-sentential.
4We are planning to evaluate the other pairs of NEs.
</footnote>
<page confidence="0.991298">
159
</page>
<table confidence="0.956968857142857">
(A)+(B) Total (A) Intra-sentential (B) Inter-sentential
Precision Recall Precision Recall Precsion Recall
WD10 43.0(2501/5819) 48.9(2501/5110) 48.1(2441/5075) 73.2(2441/3333) 8.0(60/744) 3.4(60/1777)
STR 69.3(2562/3696) 50.1(2562/5110) 75.6(2374/3141) 71.2(2374/3333) 33.9(188/555) 10.6(188/1777)
STR-CT 71.4(2764/3870) 54.1(2764/5110) 78.4(2519/3212) 75.6(2519/3333) 37.2(245/658) 13.8(245/1777)
STR-CS 73.7(2902/3935) 56.8(2902/5110) 80.1(2554/3187) 76.6(2554/3333) 46.5(348/748) 27.6(348/1777)
WD10: NE pairs that appear within 10 words are detected.
</table>
<tableCaption confidence="0.98411">
Table 2: Results for Relation Detection
</tableCaption>
<figureCaption confidence="0.99582">
Figure 3: Recall-precision Curves: (A)+(B) total
</figureCaption>
<subsectionHeader confidence="0.654008">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999983066666667">
To improve relation detection performance, we in-
vestigated the effect of the proposed method using
contextual features. Table 2 shows results for Type
(A), Type (B), and (A)+(B). We also plotted recall-
precision curves5, altering threshold parameters, as
shown in Figure 3.
The comparison between STR and STR-CT and
between STR and STR-CS in Figure 3 indicates that
the proposed method effectively contributed to rela-
tion detection. In addition, the results for Type (A):
intra-sentential, and (B): inter-sentential, in Table
2 indicate that the proposed method contributed to
both Type (A), improving precision by about 4.5%
and recall by about 5.4% and Type (B), improving
precision by about 12.6% and recall by about 17.0%.
</bodyText>
<subsectionHeader confidence="0.964936">
3.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999148666666667">
Over 70% of the errors are covered by two major
problems left in relation detection.
Parallel sentence: The proposed method solves
problems, which result from when a parallel
sentence arises from predication ellipsis. How-
ever, there are several types of parallel sentence
that differ from the one we explained. (For ex-
ample, Ken and Tom was born in Osaka and
New York, respectively.)
</bodyText>
<footnote confidence="0.941971333333333">
5Precision = # of correctly detected pairs / # of detected pairs
Recall = # of correctly detected pairs / # of pairs with semantic
relations
</footnote>
<bodyText confidence="0.9701354">
Definite anaphora: Definite noun phrase, such as
“Shusho (the Prime Minister)” and “Shacho
(the President)”, can be anaphors. We should
consider them in centering theory, but it is dif-
ficult to find them in Japanese .
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999813">
In this paper, we propose a supervised learning
method using words, syntactic structures, and con-
textual features based on centering theory, to im-
prove both inter-sentential and inter-sentential rela-
tion detection. The experiments demonstrated that
the proposed method increased precision by 4.4%,
up to 73.7%, and increased recall by 6.7%, up to
56.8%, and thus contributed to relation detection.
In future work, we plan to solve the problems re-
lating to parallel sentence and definite anaphora, and
address the task of relation characterization.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990962962963">
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proceedings of the Neural Information
Processing Systems, pages 625–632.
A. Culotta and J. Sorensen. 2004. Dependency Tree Kernels
for Relation Extraction. Annual Meeting of Association of
Computational Linguistics, pages 423–429.
B. J. Grosz, A. K. Joshi, and S. Weistein. 1983. Providing a
unified account of definite nounphrases in discourse. Annual
Meeting ofAssociation of Computational Linguistics, pages
44–50.
N. Kambhatla. 2004. Combining Lexical, Syntactic, and Se-
mantic Features with Maximum Entropy Models for Infor-
mation Extraction. Annual Meeting of Association of Com-
putational Linguistics, pages 178–181.
M. Kameyama. 1986. A property-sharing constraint in center-
ing. Annual Meeting of Association of Computational Lin-
guistics, pages 200–206.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for
classification of semi-structured text. In Proceedings of the
2004 EMNLP, pages 301–308.
J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003. Hier-
archical directed acyclic graph kernel : Methods for struc-
tured natural language data. Annual Meeting ofAssociation
of Computational Linguistics, pages 32–39.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel Meth-
ods for Relation Extraction. Journal of Machine Learning
Research, pages 3:1083–1106.
</reference>
<page confidence="0.997485">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839895">
<title confidence="0.999765">Detecting Semantic Relations between Named Entities in Text Using Contextual Features</title>
<author confidence="0.99802">Toru Hirano</author>
<author confidence="0.99802">Yoshihiro Matsuo</author>
<author confidence="0.99802">Genichiro Kikui</author>
<affiliation confidence="0.99975">NTT Cyber Space Laboratories, NTT Corporation</affiliation>
<address confidence="0.973707">1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, 239-0847, Japan</address>
<email confidence="0.981792">matsuo.yoshihiro,</email>
<abstract confidence="0.990544866666667">This paper proposes a supervised learning method for detecting a semantic relation between a given pair of named entities, which may be located in different sentences. The method employs newly introduced contextual features based on centering theory as well as conventional syntactic and word-based features. These features are organized as a tree structure and are fed into a boosting-based classification algorithm. Experimental results show the proposed method outperformed prior methods, and increased precision and recall by 4.4% and 6.7%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>Proceedings of the Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="9844" citStr="Collins and Duffy, 2001" startWordPosition="1592" endWordPosition="1595">e 2. The method of converting from a stack (Figure 1) into a structure (Figure 2) is described as follows. First, the following NE, “amerika32”, becomes the root node because Figure 1 is stacked information until the following NE appears. Then, the stacked information is converted to Figure 2 depending on the case markers. We use the path of the given pair of NEs in the structure as a feature. For example, “amerika32 → wa:Ken22”2 is used as the feature of the given pair “Ken22” and “amerika32”. 2.5 Classification Algorithm There are several structure-based learning algorithms proposed so far (Collins and Duffy, 2001; Suzuki et al., 2003; Kudo and Matsumoto, 2004). The experiments tested Kudo and Matsumoto’s boosting-based algorithm using sub trees as features, which is implemented as the BACT system. In relation detection, given a set of training examples each of which represents contextual, syntactic, and word-based features of a pair of NEs as a tree labeled as either having semantic relations or not, the BACT system learns that a set of rules are effective in classifying. Then, given a test instance, which represents contextual, syntactic, and word2“A → B” means A has a dependency relation to B. based</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. Proceedings of the Neural Information Processing Systems, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<journal>Annual Meeting of Association of Computational Linguistics,</journal>
<pages>423--429</pages>
<contexts>
<context position="1978" citStr="Culotta and Sorensen, 2004" startWordPosition="292" endWordPosition="295">Label]. Our final goal is to extract tuples from a text. For example, the tuple [George Bush (person), the U.S. (location), president (Relation Label)] would be extracted from the sentence “George Bush is the president of the U.S.”. There are two tasks in extracting tuples from text. One is detecting whether or not a given pair of NEs are semantically related (relation detection), and the other is determining the relation label (relation characterization). In this paper, we address the task of relation detection. So far, various supervised learning approaches have been explored in this field (Culotta and Sorensen, 2004; Zelenko et al., 2003). They use two kinds of features: syntactic ones and wordbased ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). These methods have two problems which we consider in this paper. One is that they target only intrasentential relation detection in which NE pairs are located in the same sentence, in spite of the fact that about 35% of NE pairs with semantic relations are inter-sentential (See Section 3.1). The other is that the methods can not detect semantic relations correctly when NE pairs located in </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>A. Culotta and J. Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. Annual Meeting of Association of Computational Linguistics, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weistein</author>
</authors>
<title>Providing a unified account of definite nounphrases in discourse. Annual Meeting ofAssociation of Computational Linguistics,</title>
<date>1983</date>
<pages>44--50</pages>
<contexts>
<context position="3408" citStr="Grosz et al., 1983" startWordPosition="534" endWordPosition="537">Ken11” and “Tokyo12”) is the same as the feature of the pair with no semantic relation (“Ken11” and “New York14”). (S-1) Ken11-wa Tokyo12-de, Tom13-wa New York14-de umareta15. (Ken11 was born15 in Tokyo12, Tom13 in New York14.) To solve the above problems, we propose a supervised learning method using contextual features. The rest of this paper is organized as follows. Section 2 describes the proposed method. We report the results of our experiments in Section 3 and conclude the paper in Section 4. 2 Relation Detection The proposed method employs contextual features based on centering theory (Grosz et al., 1983) as well as conventional syntactic and word-based features. These features are organized as a tree structure and are fed into a boosting-based classification algorithm. The method consists of three parts: preprocessing (POS tagging, NE tagging, and parsing), 1The numbers show correspondences of words between Japanese and English. 157 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157–160, Prague, June 2007. c�2007 Association for Computational Linguistics feature extraction (contextual, syntactic, and wordbased features), and classification. In this section, we describe the underl</context>
</contexts>
<marker>Grosz, Joshi, Weistein, 1983</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weistein. 1983. Providing a unified account of definite nounphrases in discourse. Annual Meeting ofAssociation of Computational Linguistics, pages 44–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kambhatla</author>
</authors>
<title>Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction. Annual Meeting of Association of Computational Linguistics,</title>
<date>2004</date>
<pages>178--181</pages>
<contexts>
<context position="2184" citStr="Kambhatla, 2004" startWordPosition="332" endWordPosition="333">nt of the U.S.”. There are two tasks in extracting tuples from text. One is detecting whether or not a given pair of NEs are semantically related (relation detection), and the other is determining the relation label (relation characterization). In this paper, we address the task of relation detection. So far, various supervised learning approaches have been explored in this field (Culotta and Sorensen, 2004; Zelenko et al., 2003). They use two kinds of features: syntactic ones and wordbased ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). These methods have two problems which we consider in this paper. One is that they target only intrasentential relation detection in which NE pairs are located in the same sentence, in spite of the fact that about 35% of NE pairs with semantic relations are inter-sentential (See Section 3.1). The other is that the methods can not detect semantic relations correctly when NE pairs located in a parallel sentence arise from a predication ellipsis. In the following Japanese example1, the syntactic feature, which is the path of two NEs in the dependency structure, of the pair with a semantic relati</context>
<context position="10951" citStr="Kambhatla, 2004" startWordPosition="1786" endWordPosition="1787">tance, which represents contextual, syntactic, and word2“A → B” means A has a dependency relation to B. based features of a pair of NEs as a tree, the BACT system classifies using a set of learned rules. 3 Experiments We experimented with texts from Japanese newspapers and weblogs to test the proposed method. The following four models were compared: 1. WD : Pairs of NEs within n words are detected as pairs with semantic relation. 2. STR : Supervised learning method using syntactic3 and word-based features, the path of the pairs of NEs in the parse tree and the word ngram between pairs of NEs (Kambhatla, 2004) 3. STR-CT : STR with the centering top feature explained in Section 2.3. 4. STR-CS : STR with the centering structure feature explained in Section 2.4. 3.1 Setting We used 1451 texts from Japanese newspapers and weblogs, whose semantic relations between person and location had been annotated by humans for the experiments4. There were 5110 pairs with semantic relations out of 236,142 pairs in the annotated text. We conducted ten-fold cross-validation over 236,142 pairs of NEs so that sets of pairs from a single text were not divided into the training and test sets. We also divided pairs of NEs</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>N. Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction. Annual Meeting of Association of Computational Linguistics, pages 178–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>A property-sharing constraint in centering.</title>
<date>1986</date>
<journal>Annual Meeting of Association of Computational Linguistics,</journal>
<pages>200--206</pages>
<contexts>
<context position="5574" citStr="Kameyama, 1986" startWordPosition="870" endWordPosition="871">ermore, when a pair of NEs with a semantic relation appears in a parallel sentence arise from predication ellipsis, the antecedent NE is contextually easily referred to in the phrase with the following NE. In the example of “(S-1)”, the pair “Ken11” and “Tokyo12” have a semantic relation “umareta15 (was born)”. Meanwhile, the pair “Ken11” and “New York14” has no semantic relation. Therefore, using whether the antecedent NE is referred to in the context with the following NE as features of a given pair of NEs would improve relation detection performance. In this paper, we use centering theory (Kameyama, 1986) to determine how easily a noun phrase can be referred to in the following context. 2.2 Centering Theory Centering theory is an empirical sorting rule used to identify the antecedents of (zero) pronouns. When there is a (zero) pronoun in the text, noun phrases that are in the previous context of the pronoun are sorted in order of likelihood of being the antecedent. The sorting algorithm has two steps. First, from the beginning of the text until the pronoun appears, noun Priority wa Ken ga ni o Osaka others asu, Naomi Figure 1: Information Stacked According to Centering Theory phrases are stack</context>
</contexts>
<marker>Kameyama, 1986</marker>
<rawString>M. Kameyama. 1986. A property-sharing constraint in centering. Annual Meeting of Association of Computational Linguistics, pages 200–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 EMNLP,</booktitle>
<pages>301--308</pages>
<contexts>
<context position="9892" citStr="Kudo and Matsumoto, 2004" startWordPosition="1600" endWordPosition="1603">gure 1) into a structure (Figure 2) is described as follows. First, the following NE, “amerika32”, becomes the root node because Figure 1 is stacked information until the following NE appears. Then, the stacked information is converted to Figure 2 depending on the case markers. We use the path of the given pair of NEs in the structure as a feature. For example, “amerika32 → wa:Ken22”2 is used as the feature of the given pair “Ken22” and “amerika32”. 2.5 Classification Algorithm There are several structure-based learning algorithms proposed so far (Collins and Duffy, 2001; Suzuki et al., 2003; Kudo and Matsumoto, 2004). The experiments tested Kudo and Matsumoto’s boosting-based algorithm using sub trees as features, which is implemented as the BACT system. In relation detection, given a set of training examples each of which represents contextual, syntactic, and word-based features of a pair of NEs as a tree labeled as either having semantic relations or not, the BACT system learns that a set of rules are effective in classifying. Then, given a test instance, which represents contextual, syntactic, and word2“A → B” means A has a dependency relation to B. based features of a pair of NEs as a tree, the BACT s</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proceedings of the 2004 EMNLP, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>T Hirao</author>
<author>Y Sasaki</author>
<author>E Maeda</author>
</authors>
<title>Hierarchical directed acyclic graph kernel : Methods for structured natural language data. Annual Meeting ofAssociation of Computational Linguistics,</title>
<date>2003</date>
<pages>32--39</pages>
<contexts>
<context position="9865" citStr="Suzuki et al., 2003" startWordPosition="1596" endWordPosition="1599">ting from a stack (Figure 1) into a structure (Figure 2) is described as follows. First, the following NE, “amerika32”, becomes the root node because Figure 1 is stacked information until the following NE appears. Then, the stacked information is converted to Figure 2 depending on the case markers. We use the path of the given pair of NEs in the structure as a feature. For example, “amerika32 → wa:Ken22”2 is used as the feature of the given pair “Ken22” and “amerika32”. 2.5 Classification Algorithm There are several structure-based learning algorithms proposed so far (Collins and Duffy, 2001; Suzuki et al., 2003; Kudo and Matsumoto, 2004). The experiments tested Kudo and Matsumoto’s boosting-based algorithm using sub trees as features, which is implemented as the BACT system. In relation detection, given a set of training examples each of which represents contextual, syntactic, and word-based features of a pair of NEs as a tree labeled as either having semantic relations or not, the BACT system learns that a set of rules are effective in classifying. Then, given a test instance, which represents contextual, syntactic, and word2“A → B” means A has a dependency relation to B. based features of a pair o</context>
</contexts>
<marker>Suzuki, Hirao, Sasaki, Maeda, 2003</marker>
<rawString>J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003. Hierarchical directed acyclic graph kernel : Methods for structured natural language data. Annual Meeting ofAssociation of Computational Linguistics, pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="2001" citStr="Zelenko et al., 2003" startWordPosition="296" endWordPosition="299"> extract tuples from a text. For example, the tuple [George Bush (person), the U.S. (location), president (Relation Label)] would be extracted from the sentence “George Bush is the president of the U.S.”. There are two tasks in extracting tuples from text. One is detecting whether or not a given pair of NEs are semantically related (relation detection), and the other is determining the relation label (relation characterization). In this paper, we address the task of relation detection. So far, various supervised learning approaches have been explored in this field (Culotta and Sorensen, 2004; Zelenko et al., 2003). They use two kinds of features: syntactic ones and wordbased ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). These methods have two problems which we consider in this paper. One is that they target only intrasentential relation detection in which NE pairs are located in the same sentence, in spite of the fact that about 35% of NE pairs with semantic relations are inter-sentential (See Section 3.1). The other is that the methods can not detect semantic relations correctly when NE pairs located in a parallel sentence ari</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, pages 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>