<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003309">
<title confidence="0.985859">
Global Models of Document Structure Using Latent Permutations
</title>
<author confidence="0.997994">
Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger
</author>
<affiliation confidence="0.999076">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.987299">
{harr, branavan, regina, karger}@csail.mit.edu
</email>
<sectionHeader confidence="0.998514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999396">
We present a novel Bayesian topic model for
learning discourse-level document structure.
Our model leverages insights from discourse
theory to constrain latent topic assignments in
a way that reflects the underlying organiza-
tion of document topics. We propose a global
model in which both topic selection and order-
ing are biased to be similar across a collection
of related documents. We show that this space
of orderings can be elegantly represented us-
ing a distribution over permutations called the
generalized Mallows model. Our structure-
aware approach substantially outperforms al-
ternative approaches for cross-document com-
parison and single-document segmentation.1
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890642857143">
In this paper, we introduce a novel latent topic model
for the unsupervised learning of document structure.
Traditional topic models assume that topics are ran-
domly spread throughout a document, or that the
succession of topics in a document is Markovian.
In contrast, our approach takes advantage of two
important discourse-level properties of text in de-
termining topic assignments: first, that each docu-
ment follows a progression of nonrecurring coher-
ent topics (Halliday and Hasan, 1976); and sec-
ond, that documents from the same domain tend
to present similar topics, in similar orders (Wray,
2002). We show that a topic model incorporat-
ing these long-range dependencies outperforms al-
</bodyText>
<footnote confidence="0.9196555">
1Code, data, and annotations used in this work are available
at http://groups.csail.mit.edu/rbg/code/mallows/
</footnote>
<bodyText confidence="0.997598911764706">
ternative approaches for segmentation and cross-
document comparison.
For example, consider a collection of encyclope-
dia articles about cities. The first constraint captures
the notion that a single topic, such as Architecture,
is expressed in a contiguous block within the docu-
ment, rather than spread over disconnected sections.
The second constraint reflects our intuition that all
of these related articles will generally mention some
major topics associated with cities, such as History
and Culture, and will often exhibit similar topic or-
derings, such as placing History before Culture.
We present a Bayesian latent topic model over re-
lated documents that encodes these discourse con-
straints by positing a single distribution over a doc-
ument’s entire topic structure. This global view on
ordering is able to elegantly encode discourse-level
properties that would be difficult to represent using
local dependencies, such as those induced by hid-
den Markov models. Our model enforces that the
same topic does not appear in disconnected portions
of the topic sequence. Furthermore, our approach
biases toward selecting sequences with similar topic
ordering, by modeling a distribution over the space
of topic permutations.
Learning this ordering distribution is a key tech-
nical challenge in our proposed approach. For this
purpose, we employ the generalized Mallows model,
a permutation distribution that concentrates proba-
bility mass on a small set of similar permutations.
It directly captures the intuition of the second con-
straint, and uses a small parameter set to control how
likely individual topics are to be reordered.
We evaluate our model on two challenging
</bodyText>
<page confidence="0.980266">
371
</page>
<note confidence="0.8904875">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371–379,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.996343615384615">
document-level tasks. In the alignment task, we aim Modeling Ordering Constraints Sentence order-
to discover paragraphs across different documents ing has been extensively studied in the context of
that share the same topic. We also consider the seg- probabilistic text modeling for summarization and
mentation task, where the goal is to partition each generation (Barzilay et al., 2002; Lapata, 2003;
document into a sequence of topically coherent seg- Karamanis et al., 2004). The emphasis of that body
ments. We find that our structure modeling approach of work is on learning ordering constraints from
substantially outperforms state-of-the-art baselines data, with the goal of reordering new text from the
for both tasks. Furthermore, we demonstrate the im- same domain. Our emphasis, however, is on ap-
portance of explicitly modeling a distribution over plications where ordering is already observed, and
topic permutations; our model yields significantly how that ordering can improve text analysis. From
better results than variants that either use a fixed or- the methodological side, that body of prior work is
dering, or are order-agnostic. largely driven by local pairwise constraints, while
we aim to encode global constraints.
</bodyText>
<sectionHeader confidence="0.9997335" genericHeader="introduction">
2 Related Work
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999664109090909">
Topic and Content Models Our work is grounded
in topic modeling approaches, which posit that la-
tent state variables control the generation of words.
In earlier topic modeling work such as latent Dirich-
let allocation (LDA) (Blei et al., 2003; Griffiths and
Steyvers, 2004), documents are treated as bags of
words, where each word receives a separate topic
assignment; the topic assignments are auxiliary vari-
ables to the main task of language modeling.
More recent work has attempted to adapt the con-
cepts of topic modeling to more sophisticated repre-
sentations than a bag of words; they use these rep-
resentations to impose stronger constraints on topic
assignments (Griffiths et al., 2005; Wallach, 2006;
Purver et al., 2006; Gruber et al., 2007). These
approaches, however, generally model Markovian
topic or state transitions, which only capture lo-
cal dependencies between adjacent words or blocks
within a document. For instance, content mod-
els (Barzilay and Lee, 2004; Elsner et al., 2007)
are implemented as HMMs, where the states cor-
respond to topics of domain-specific information,
and transitions reflect pairwise ordering prefer-
ences. Even approaches that break text into con-
tiguous chunks (Titov and McDonald, 2008) as-
sign topics based on local context. While these
locally constrained models can implicitly reflect
some discourse-level constraints, they cannot cap-
ture long-range dependencies without an explosion
of the parameter space. In contrast, our model cap-
tures the entire sequence of topics using a compact
representation. As a result, we can explicitly and
tractably model global discourse-level constraints.
Our document structure learning problem can be for-
malized as follows. We are given a corpus of D
related documents. Each document expresses some
subset of a common set of K topics. We assign a
single topic to each paragraph,2 incorporating the
notion that paragraphs are internally topically con-
sistent (Halliday and Hasan, 1976). To capture the
discourse constraint on topic progression described
in Section 1, we require that topic assignments be
contiguous within each document.3 Furthermore,
we assume that the underlying topic sequences ex-
hibit similarity across documents. Our goal is to re-
cover a topic assignment for each paragraph in the
corpus, subject to these constraints.
Our formulation shares some similarity with the
standard LDA setup, in that a common set of topics
is assigned across a collection of documents. How-
ever, in LDA each word’s topic assignment is con-
ditionally independent, following the bag of words
view of documents. In contrast, our constraints on
how topics are assigned let us connect word distri-
butional patterns to document-level topic structure.
</bodyText>
<sectionHeader confidence="0.996061" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.99995225">
We propose a generative Bayesian model that ex-
plains how a corpus of D documents, given as se-
quences of paragraphs, can be produced from a set
of hidden topic variables. Topic assignments to each
</bodyText>
<footnote confidence="0.9862935">
2Note that our analysis applies equally to other levels of tex-
tual granularity, such as sentences.
3That is, if paragraphs i and j are assigned the same topic,
every paragraph between them must have that topic.
</footnote>
<page confidence="0.998254">
372
</page>
<bodyText confidence="0.999992357142857">
paragraph, ranging from 1 to K, are the model’s
final output, implicitly grouping topically similar
paragraphs. At a high level, the process first selects
the bag of topics to be expressed in the document,
and how they are ordered; these topics then deter-
mine the selection of words for each paragraph.
For each document d with Nd paragraphs, we sep-
arately generate a bag of topics td and a topic order-
ing πd. The unordered bag of topics, which contains
Nd elements, expresses how many paragraphs of the
document are assigned to each of the K topics. Note
that some topics may not appear at all. Variable td
is constructed by taking Nd samples from a distri-
bution over topics τ, a multinomial representing the
probability of each topic being expressed. Sharing
τ between documents captures the intuition that cer-
tain topics are more likely across the entire corpus.
The topic ordering variable πd is a permutation
over the numbers 1 through K that defines the order
in which topics appear in the document. We draw πd
from the generalized Mallows model, a distribution
over permutations that we explain in Section 4.1. As
we will see, this particular distribution biases the
permutation selection to be close to a single cen-
troid, reflecting the discourse constraint of prefer-
ring similar topic structures across documents.
Together, a document’s bag of topics td and or-
dering πd determine the topic assignment zd,p for
each of its paragraphs. For example, in a corpus
with K = 4, a seven-paragraph document d with
td = 11,1,1,1, 2, 4, 4} and πd = (2 4 3 1) would
induce the topic sequence zd = (2 4 4 1 1 1 1). The
induced topic sequence zd can never assign the same
topic to two unconnected portions of a document,
thus satisfying the constraint of topic contiguity.
As with LDA, we assume that each topic k is as-
sociated with a language model θk. The words of a
paragraph assigned to topic k are then drawn from
that topic’s language model θk.
Before turning to a more formal discussion of the
generative process, we first provide background on
the permutation model for topic ordering.
</bodyText>
<subsectionHeader confidence="0.998758">
4.1 The Generalized Mallows Model
</subsectionHeader>
<bodyText confidence="0.999930083333333">
A central challenge of the approach we take is mod-
eling the distribution over possible topic permuta-
tions. For this purpose we use the generalized Mal-
lows model (GMM) (Fligner and Verducci, 1986;
Lebanon and Lafferty, 2002; Meil˘a et al., 2007),
which exhibits two appealing properties in the con-
text of this task. First, the model concentrates proba-
bility mass on some “canonical” ordering and small
perturbations of that ordering. This characteris-
tic matches our constraint that documents from the
same domain exhibit structural similarity. Second,
its parameter set scales linearly with the permuta-
tion length, making it sufficiently constrained and
tractable for inference. In general, this distribution
could potentially be applied to other NLP applica-
tions where ordering is important.
Permutation Representation Typically, permuta-
tions are represented directly as an ordered sequence
of elements. The GMM utilizes an alternative rep-
resentation defined as a vector (v1, ... , vK−1) of in-
version counts with respect to the identity permuta-
tion (1, ... , K). Term vj counts the number of times
a value greater than j appears before j in the permu-
tation.4 For instance, given the standard-form per-
mutation (3 1 5 2 4), v2 = 2 because 3 and 5 appear
before 2; the entire inversion count vector would be
(1 2 0 1). Every vector of inversion counts uniquely
identifies a single permutation.
The Distribution The GMM assigns proba-
bility mass according to the distance of a
given permutation from the identity permutation
11, ... , K}, based on K − 1 real-valued parameters
(ρ1,... ρK−1).5 Using the inversion count represen-
tation of a permutation, the GMM’s probability mass
function is expressed as an independent product of
probabilities for each vj:
</bodyText>
<equation confidence="0.860484571428571">
e
GMM(v  |ρ) =
n−1H e−ρjvj
j=1 ψj(ρj), (1)
where ψj(ρj) is a normalization factor with value:
/, 1 − e−(K−j+1)ρj
4&apos;j (ρj) =
</equation>
<footnote confidence="0.99146">
4The sum of a vector of inversion counts is simply that per-
mutation’s Kendall’s τ distance to the identity permutation.
5In our work we take the identity permutation to be the fixed
</footnote>
<bodyText confidence="0.427339">
centroid, which is a parameter in the full GMM. As we explain
later, our model is not hampered by this apparent restriction.
</bodyText>
<equation confidence="0.9865655">
− Ej ρjvj
ψ(ρ)
.
1 − e−ρj
</equation>
<page confidence="0.989969">
373
</page>
<bodyText confidence="0.999974555555556">
Due to the exponential form of the distribution, re-
quiring that ρj &gt; 0 constrains the GMM to assign
highest probability mass to each vj being zero, cor-
responding to the identity permutation. A higher
value for ρj assigns more probability mass to vj be-
ing close to zero, biasing j to have fewer inversions.
The GMM elegantly captures our earlier require-
ment for a probability distribution that concentrates
mass around a global ordering, and uses few param-
eters to do so. Because the topic numbers in our
task are completely symmetric and not linked to any
extrinsic observations, fixing the identity permuta-
tion to be that global ordering does not sacrifice any
representational power. Another major benefit of
the GMM is its membership in the exponential fam-
ily of distributions; this means that it is particularly
amenable to a Bayesian representation, as it admits
a natural conjugate prior:
</bodyText>
<equation confidence="0.992427">
GMM0(ρj  |vj,0, ν0) a e(−Pjvj,0−lo9 ψj(Pj))ν0. (2)
</equation>
<bodyText confidence="0.9998775">
Intuitively, this prior states that over ν0 prior trials,
the total number of inversions was ν0vj,0. This dis-
tribution can be easily updated with the observed vj
to derive a posterior distribution.6
</bodyText>
<subsectionHeader confidence="0.992365">
4.2 Formal Generative Process
</subsectionHeader>
<bodyText confidence="0.999694181818182">
We now fully specify the details of our model. We
observe a corpus of D documents, each an ordered
sequence of paragraphs, and a specification of a
number of topics K. Each paragraph is represented
as a bag of words. The model induces a set of hid-
den variables that probabilistically explain how the
words of the corpus were produced. Our final de-
sired output is the distributions over the paragraphs’
hidden topic assignment variables. In the following,
variables subscripted with 0 are fixed prior hyperpa-
rameters.
</bodyText>
<listItem confidence="0.9934155">
1. For each topic k, draw a language model Bk —
Dirichlet(B0). As with LDA, these are topic-
specific word distributions.
2. Draw a topic distribution T — Dirichlet(T0),
which expresses how likely each topic is to ap-
pear regardless of position.
</listItem>
<footnote confidence="0.7417455">
6Because each vj has a different range, it is inconvenient
to set the prior hyperparameters vj,0 directly. In our work, we
instead fix the mode of the prior distribution to a value p0, which
works out to setting v j,o = 1 — K−j+1
</footnote>
<page confidence="0.491922">
7 exp(ρ0)−1 —p((K−j+1)ρ0)−1.
</page>
<listItem confidence="0.969493473684211">
3. Draw the topic ordering distribution parame-
ters ρj — GMM0(ρ0, ν0) for j = 1 to K — 1.
These parameters control how rapidly probabil-
ity mass decays for having more inversions for
each topic. A separate ρj for every topic allows
us to learn that some topics are more likely to
be reordered than others.
4. For each document d with Nd paragraphs:
(a) Draw a bag of topics td by sampling Nd
times from Multinomial(T).
(b) Draw a topic ordering 7rd by sampling a
vector of inversion counts vd — GMM(ρ).
(c) Compute the vector of topic assignments
zd for document d’s paragraphs, by sorting
td according to 7rd.7
(d) For each paragraph p in document d:
i. Sample each word wd,p,j according to
the language model of p: wd,p,j —
Multinomial(Bzd,p).
</listItem>
<sectionHeader confidence="0.999327" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.999676157894737">
The variables that we aim to infer are the topic as-
signments z of each paragraph, which are deter-
mined by the bag of topics t and ordering 7r for each
document. Thus, our goal is to estimate the marginal
distributions of t and 7r given the document text.
We accomplish this inference task through Gibbs
sampling (Bishop, 2006). A Gibbs sampler builds
a Markov chain over the hidden variable state space
whose stationary distribution is the actual posterior
of the joint distribution. Each new sample is drawn
from the distribution of a single variable conditioned
on previous samples of the other variables. We can
“collapse” the sampler by integrating over some of
the hidden variables in the model, in effect reducing
the state space of the Markov chain. Collapsed sam-
pling has been previously demonstrated to be effec-
tive for LDA and its variants (Griffiths and Steyvers,
2004; Porteous et al., 2008; Titov and McDonald,
2008). Our sampler integrates over all but three sets
</bodyText>
<footnote confidence="0.8344255">
7Multiple permutations can contribute to the probability of a
single document’s topic assignments zd, if there are topics that
do not appear in td. As a result, our current formulation is bi-
ased toward assignments with fewer topics per document. In
practice, we do not find this to negatively impact model perfor-
mance.
</footnote>
<page confidence="0.988863">
374
</page>
<bodyText confidence="0.995612222222222">
Equation 3 and 4 will be used again to compute the
conditional distributions of the hidden variables.
We now turn to a discussion of how each individ-
ual random variable is resampled.
Bag of Topics First we consider how to resample
td,i, the ith topic draw for document d conditioned
on all other parameters being fixed (note this is not
the topic of the ith paragraph, as we reorder topics
using πd):
</bodyText>
<equation confidence="0.999824333333333">
P(td,i = t  |...)
∝ P(td,i = t  |t−(d,i),τ0)P(wd  |td, πd, w−d, z−d, θ0)
N(t−(d i), t) + τ0
</equation>
<bodyText confidence="0.9998628125">
of hidden variables: bags of topics t, orderings π,
and permutation inversion parameters ρ. After a
burn-in period, we treat the last samples of t and
π as a draw from the true posterior.
Document Probability As a preliminary step,
consider how to calculate the probability of a single
document’s words wd given the document’s para-
graph topic assignments zd, and other documents
and their topic assignments. Note that this proba-
bility is decomposable into a product of probabil-
ities over individual paragraphs, where paragraphs
with different topics have conditionally independent
word probabilities. Let w−d and z−d indicate the
words and topic assignments to documents other
than d, and W be the vocabulary size. The proba-
bility of the words in d is then:
</bodyText>
<equation confidence="0.999483714285714">
P(wd  |z, w−d, θ0)
K IP(wd  |zd, θk)P(θk  |z, w−d, θ0)dθk
�
k=1
K
= H DCM({wd,i : zd,i = k}
k=1
</equation>
<bodyText confidence="0.9770980625">
 |{w−d,i : z−d,i = k}, θ0), (3)
where DCM(·) refers to the Dirichlet compound
multinomial distribution, the result of integrat-
ing over multinomial parameters with a Dirichlet
prior (Bernardo and Smith, 2000). For a Dirichlet
prior with parameters α = (α1, ... , αW), the DCM
assigns the following probability to a series of ob-
servations x = {x1, ... , xn}:
where N(x, i) refers to the number of times word
i appears in x. Here, Γ(·) is the Gamma function,
a generalization of the factorial for real numbers.
Some algebra shows that the DCM’s posterior prob-
ability density function conditioned on a series of
observations y = {y1, ... , yn} can be computed by
updating each αi with counts of how often word i
appears in y:
</bodyText>
<equation confidence="0.9976692">
DCM(x  |y, α)
= DCM(x  |α1 + N(y,1), ... , αW + N(y, W)).
(4)
∝ , P(wd  |z, w−d, θ0),
|t−(d,i) |+ Kτ0
</equation>
<bodyText confidence="0.999982">
where td is updated to reflect td,i = t, and zd is de-
terministically computed by mapping td and πd to
actual paragraph topic assignments. The first step
reflects an application of Bayes rule to factor out the
term for wd. In the second step, the first term arises
out of the DCM, by updating the parameters τ0 with
observations t−(d,i) as in equation 4 and dropping
constants. The document probability term is com-
puted using equation 3. The new td,i is selected
by sampling from this probability computed over all
possible topic assignments.
Ordering The parameterization of a permutation
π as a series of inversion values vj reveals a natural
way to decompose the search space for Gibbs sam-
pling. For a single ordering, each vj can be sampled
independently, according to:
</bodyText>
<equation confidence="0.899335333333333">
P(vj = v  |...)
∝ P(vj = v  |ρj)P(wd  |td, πd, w−d, z−d, θ0)
= GMMj(v  |ρj)P(wd  |zd, w−d, z−d, θ0),
</equation>
<bodyText confidence="0.998671">
where πd is updated to reflect vj = v, and zd is com-
puted according to td and πd. The first term refers
to the jth multiplicand of equation 1; the second is
computed using equation 3. Term vj is sampled ac-
cording to the resulting probabilities.
GMM Parameters For each j = 1 to K − 1, we
resample ρj from its posterior distribution:
</bodyText>
<equation confidence="0.996757875">
P(ρj  |...)
= GMM0(ρj
E�
i vj,i + vj,0ν0N + ν0 ,
N + ν0
DCM(x  |α) = Γ(Ej αj) W Γ(N(x, i) + αi)
H i=1 Γ(|x |+ Ej αj) ,
j Γ(αj)
</equation>
<page confidence="0.993949">
375
</page>
<bodyText confidence="0.9999348">
where GMMO is evaluated according to equation 2.
The normalization constant of this distribution is un-
known, meaning that we cannot directly compute
and invert the cumulative distribution function to
sample from this distribution. However, the distri-
bution itself is univariate and unimodal, so we can
expect that an MCMC technique such as slice sam-
pling (Neal, 2003) should perform well. In practice,
the MATLAB black-box slice sampler provides a ro-
bust draw from this distribution.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.995045333333333">
Data Sets We evaluate our model on two data sets
drawn from the English Wikipedia. The first set
is 100 articles about large cities, with topics such
as History, Culture, and Demographics. The sec-
ond is 118 articles about chemical elements in the
periodic table, including topics such as Biological
Role, Occurrence, and Isotopes. Within each cor-
pus, articles often exhibit similar section orderings,
but many have idiosyncratic inversions. This struc-
tural variability arises out of the collaborative nature
of Wikipedia, which allows articles to evolve inde-
pendently. Corpus statistics are summarized below.
</bodyText>
<table confidence="0.587165">
Corpus Docs Paragraphs Vocab Words
Cities 100 6,670 41,978 492,402
Elements 118 2,810 18,008 191,762
</table>
<bodyText confidence="0.99981953125">
In each data set, the articles’ noisy section head-
ings induce a reference structure to compare against.
This reference structure assumes that two para-
graphs are aligned if and only if their section head-
ings are identical, and that section boundaries pro-
vide the correct segmentation of each document.
These headings are only used for evaluation, and are
not provided to any of the systems.
Using the section headings to build the reference
structure can be problematic, as the same topic may
be referred to using different titles across different
documents, and sections may be divided at differing
levels of granularity. Thus, for the Cities data set, we
manually annotated each article’s paragraphs with a
consistent set of section headings, providing us an
additional reference structure to evaluate against. In
this clean section headings set, we found approxi-
mately 18 topics that were expressed in more than
one document.
Tasks and Metrics We study performance on the
tasks of alignment and segmentation. In the former
task, we measure whether paragraphs identified to
be the same topic by our model have the same sec-
tion headings, and vice versa. First, we identify the
“closest” topic to each section heading, by finding
the topic that is most commonly assigned to para-
graphs under that section heading. We compute the
proportion of paragraphs where the model’s topic as-
signment matches the section heading’s topic, giv-
ing us a recall score. High recall indicates that
paragraphs of the same section headings are always
being assigned to the same topic. Conversely, we
can find the closest section heading to each topic,
by finding the section heading that is most com-
mon for the paragraphs assigned to a single topic.
We then compute the proportion of paragraphs from
that topic whose section heading is the same as the
reference heading for that topic, yielding a preci-
sion score. High precision means that paragraphs
assigned to a single topic usually correspond to the
same section heading. The harmonic mean of recall
and precision is the summary F-score.
Statistical significance in this setup is measured
with approximate randomization (Noreen, 1989), a
nonparametric test that can be directly applied to
nonlinear metrics such as F-score. This test has been
used in prior evaluations for information extraction
and machine translation (Chinchor, 1995; Riezler
and Maxwell, 2005).
For the second task, we take the boundaries at
which topics change within a document to be a
segmentation of that document. We evaluate us-
ing the standard penalty metrics Pk and WindowD-
iff (Beeferman et al., 1999; Pevzner and Hearst,
2002). Both pass a sliding window over the doc-
uments and compute the probability of the words
at the ends of the windows being improperly seg-
mented with respect to each other. WindowDiff re-
quires that the number of segmentation boundaries
between the endpoints be correct as well.8
Our model takes a parameter K which controls
the upper bound on the number of latent topics. Note
that our algorithm can select fewer than K topics for
each document, so K does not determine the number
</bodyText>
<footnote confidence="0.969403666666667">
8Statistical significance testing is not standardized and usu-
ally not reported for the segmentation task, so we omit these
tests in our results.
</footnote>
<page confidence="0.998138">
376
</page>
<bodyText confidence="0.999680630434783">
GMM parameters p to always be zero. Both variants
still enforce topic contiguity, and allow segments
across documents to be aligned by topic assignment.
of segments in each document. We report results
using both K = 10 and 20 (recall that the cleanly
annotated Cities data set had 18 topics).
Baselines and Model Variants We consider base-
lines from the literature that perform either align-
ment or segmentation. For the first task, we
compare against the hidden topic Markov model
(HTMM) (Gruber et al., 2007), which represents
topic transitions between adjacent paragraphs in a
Markovian fashion, similar to the approach taken in
content modeling work. Note that HTMM can only
capture local constraints, so it would allow topics to
recur noncontiguously throughout a document.
We also compare against the structure-agnostic
approach of clustering the paragraphs using the
CLUTO toolkit,9 which uses repeated bisection to
maximize a cosine similarity-based objective.
For the segmentation task, we compare to
BayesSeg (Eisenstein and Barzilay, 2008),10
a Bayesian topic-based segmentation model
that outperforms previous segmentation ap-
proaches (Utiyama and Isahara, 2001; Galley et al.,
2003; Purver et al., 2006; Malioutov and Barzilay,
2006). BayesSeg enforces the topic contiguity
constraint that motivated our model. We provide
this baseline with the benefit of knowing the correct
number of segments for each document, which is
not provided to our system. Note that BayesSeg
processes each document individually, so it cannot
capture structural relatedness across documents.
To investigate the importance of our ordering
model, we consider two variants of our model that
alternately relax and tighten ordering constraints. In
the constrained model, we require all documents to
follow the same canonical ordering of topics. This
is equivalent to forcing the topic permutation distri-
bution to give all its probability to one ordering, and
can be implemented by fixing all inversion counts v
to zero during inference. At the other extreme, we
consider the uniform model, which assumes a uni-
form distribution over all topic permutations instead
of biasing toward a small related set. In our im-
plementation, this can be simulated by forcing the
</bodyText>
<footnote confidence="0.99137875">
9http://glaros.dtc.umn.edu/gkhome/views/cluto/
10We do not evaluate on the corpora used in their work, since
our model relies on content similarity across documents in the
corpus.
</footnote>
<bodyText confidence="0.999729357142857">
Evaluation Procedures For each evaluation of
our model and its variants, we run the Gibbs sampler
from five random seed states, and take the 10,000th
iteration of each chain as a sample. Results shown
are the average over these five samples. All Dirich-
let prior hyperparameters are set to 0.1, encouraging
sparse distributions. For the GMM, we set the prior
decay parameter po to 1, and the sample size prior
vo to be 0.1 times the number of documents.
For the baselines, we use implementations pub-
licly released by their authors. We set HTMM’s pri-
ors according to values recommended in the authors’
original work. For BayesSeg, we use its built-in hy-
perparameter re-estimation mechanism.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999920555555555">
Alignment Table 1 presents the results of the
alignment evaluation. In every case, the best per-
formance is achieved using our full model, by a sta-
tistically significant and usually substantial margin.
In both domains, the baseline clustering method
performs competitively, indicating that word cues
alone are a good indicator of topic. While the sim-
pler variations of our model achieve reasonable per-
formance, adding the richer GMM distribution con-
sistently yields superior results.
Across each of our evaluations, HTMM greatly
underperforms the other approaches. Manual ex-
amination of the actual topic assignments reveals
that HTMM often selects the same topic for discon-
nected paragraphs of the same document, violating
the topic contiguity constraint, and demonstrating
the importance of modeling global constraints for
document structure tasks.
We also compare performance measured on the
manually annotated section headings against the ac-
tual noisy headings. The ranking of methods by per-
formance remains mostly unchanged between these
two evaluations, indicating that the noisy headings
are sufficient for gaining insight into the compara-
tive performance of the different approaches.
Segmentation Table 2 presents the segmentation
experiment results. On both data sets, our model
</bodyText>
<page confidence="0.992426">
377
</page>
<table confidence="0.998762916666667">
Cities: clean headings Cities: noisy headings Elements: noisy headings
Recall Prec F-score Recall Prec F-score Recall Prec F-score
K = 10 Clustering 0.578 0.439 * 0.499 0.611 0.331 * 0.429 0.524 0.361 * 0.428
HTMM 0.446 0.232 * 0.305 0.480 0.183 * 0.265 0.430 0.190 * 0.264
Constrained 0.579 0.471 * 0.520 0.667 0.382 * 0.485 0.603 0.408 * 0.487
Uniform 0.520 0.440 * 0.477 0.599 0.343 * 0.436 0.591 0.403 * 0.479
Our model 0.639 0.509 0.566 0.705 0.399 0.510 0.685 0.460 0.551
K = 20 Clustering 0.486 0.541 * 0.512 0.527 0.414 * 0.464 0.477 0.402 * 0.436
HTMM 0.260 0.217 * 0.237 0.304 0.187 * 0.232 0.248 0.243 * 0.246
Constrained 0.458 0.519 * 0.486 0.553 0.415 * 0.474 0.510 0.421 * 0.461
Uniform 0.499 0.551 * 0.524 0.571 0.423 * 0.486 0.550 0.479 o 0.512
Our model 0.578 0.636 0.606 0.648 0.489 0.557 0.569 0.498 0.531
</table>
<tableCaption confidence="0.992001333333333">
Table 1: Comparison of the alignments produced by our model and a series of baselines and model variations, for both
10 and 20 topics, evaluated against clean and noisy sets of section headings. Higher scores are better. Within the same
K, the methods which our model significantly outperforms are indicated with * for p &lt; 0.001 and o for p &lt; 0.01.
</tableCaption>
<table confidence="0.999278111111111">
Cities: clean headings Cities: noisy headings Elements: noisy headings
Pk WD # Segs Pk WD # Segs Pk WD # Segs
BayesSeg 0.321 0.376 † 12.3 0.317 0.376 † 13.2 0.279 0.316 † 7.7
K = 10 Constrained 0.260 0.281 7.7 0.267 0.288 7.7 0.227 0.244 5.4
Uniform 0.268 0.300 8.8 0.273 0.304 8.8 0.226 0.250 6.6
Our model 0.253 0.283 9.0 0.257 0.286 9.0 0.201 0.226 6.7
K = 20 Constrained 0.274 0.314 10.9 0.274 0.313 10.9 0.231 0.257 6.6
Uniform 0.234 0.294 14.0 0.234 0.290 14.0 0.209 0.248 8.7
Our model 0.221 0.278 14.2 0.222 0.278 14.2 0.203 0.243 8.6
</table>
<tableCaption confidence="0.998897">
Table 2: Comparison of the segmentations produced by our model and a series of baselines and model variations, for
</tableCaption>
<bodyText confidence="0.950574066666667">
both 10 and 20 topics, evaluated against clean and noisy sets of section headings. Lower scores are better. †BayesSeg
is given the true number of segments, so its segments count reflects the reference structure’s segmentation.
outperforms the BayesSeg baseline by a substantial
margin regardless of K. This result provides strong
evidence that learning connected topic models over
related documents leads to improved segmentation
performance. In effect, our model can take advan-
tage of shared structure across related documents.
In all but one case, the best performance is ob-
tained by the full version of our model. This result
indicates that enforcing discourse-motivated struc-
tural constraints allows for better segmentation in-
duction. Encoding global discourse-level constraints
leads to better language models, resulting in more
accurate predictions of segment boundaries.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999948727272727">
In this paper, we have shown how an unsupervised
topic-based approach can capture document struc-
ture. Our resulting model constrains topic assign-
ments in a way that requires global modeling of en-
tire topic sequences. We showed that the generalized
Mallows model is a theoretically and empirically ap-
pealing way of capturing the ordering component
of this topic sequence. Our results demonstrate the
importance of augmenting statistical models of text
analysis with structural constraints motivated by dis-
course theory.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999991083333333">
The authors acknowledge the funding support of
NSF CAREER grant IIS-0448168, the NSF Grad-
uate Fellowship, the Office of Naval Research,
Quanta, Nokia, and the Microsoft Faculty Fellow-
ship. We thank the members of the NLP group at
MIT and numerous others who offered suggestions
and comments on this work. We are especially grate-
ful to Marina Meil˘a for introducing us to the Mal-
lows model. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
</bodyText>
<page confidence="0.998252">
378
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999742684782609">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL/HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal ofAr-
tificial Intelligence Research, 17:35–55.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34:177–210.
Jos´e M. Bernardo and Adrian F.M. Smith. 2000.
Bayesian Theory. Wiley Series in Probability and
Statistics.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. Journal of Machine Learn-
ing Research, 3:993–1022.
Nancy Chinchor. 1995. Statistical significance of MUC-
6 results. In Proceedings of the 6th Conference on
Message Understanding.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
M.A. Fligner and J.S. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical Soci-
ety, Series B, 48(3):359–369.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse segmen-
tation of multi-party conversation. In Proceedings of
ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in NIPS.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Proceedings of AIS-
TATS.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence for text structuring using
a reliably annotated corpus. In Proceedings of ACL.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL.
Guy Lebanon and John Lafferty. 2002. Cranking: com-
bining rankings using conditional probability models
on permutations. In Proceedings of ICML.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL.
Marina Meil˘a, Kapil Phadnis, Arthur Patterson, and Jeff
Bilmes. 2007. Consensus ranking under the exponen-
tial model. In Proceedings of UAI.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705–767.
Eric W. Noreen. 1989. Computer Intensive Methods for
Testing Hypotheses. An Introduction. Wiley.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19–36.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent dirichlet allo-
cation. In Proceedings of SIGKDD.
Matthew Purver, Konrad K¨ording, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of ACL/COLING.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL.
Hanna M. Wallach. 2006. Topic modeling: beyond bag
of words. In Proceedings of ICML.
Alison Wray. 2002. Formulaic Language and the Lexi-
con. Cambridge University Press, Cambridge.
</reference>
<page confidence="0.999188">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324588">
<title confidence="0.99993">Global Models of Document Structure Using Latent Permutations</title>
<author confidence="0.993221">Harr Chen</author>
<author confidence="0.993221">S R K Branavan</author>
<author confidence="0.993221">Regina Barzilay</author>
<author confidence="0.993221">R David</author>
<affiliation confidence="0.978526">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<email confidence="0.447326">branavan,regina,</email>
<abstract confidence="0.9852993125">We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organizaof document topics. We propose a model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be elegantly represented using a distribution over permutations called the Mallows Our structureaware approach substantially outperforms alternative approaches for cross-document comand single-document</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL/HLT.</booktitle>
<contexts>
<context position="5869" citStr="Barzilay and Lee, 2004" startWordPosition="879" endWordPosition="882">topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicit</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>17--35</pages>
<contexts>
<context position="4007" citStr="Barzilay et al., 2002" startWordPosition="589" endWordPosition="592">e evaluate our model on two challenging 371 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371–379, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics document-level tasks. In the alignment task, we aim Modeling Ordering Constraints Sentence orderto discover paragraphs across different documents ing has been extensively studied in the context of that share the same topic. We also consider the seg- probabilistic text modeling for summarization and mentation task, where the goal is to partition each generation (Barzilay et al., 2002; Lapata, 2003; document into a sequence of topically coherent seg- Karamanis et al., 2004). The emphasis of that body ments. We find that our structure modeling approach of work is on learning ordering constraints from substantially outperforms state-of-the-art baselines data, with the goal of reordering new text from the for both tasks. Furthermore, we demonstrate the im- same domain. Our emphasis, however, is on apportance of explicitly modeling a distribution over plications where ordering is already observed, and topic permutations; our model yields significantly how that ordering can imp</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal ofArtificial Intelligence Research, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John D Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--177</pages>
<contexts>
<context position="23933" citStr="Beeferman et al., 1999" startWordPosition="3945" endWordPosition="3948">section heading. The harmonic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper bound on the number of latent topics. Note that our algorithm can select fewer than K topics for each document, so K does not determine the number 8Statistical significance testing is not standardized and usually not reported for the segment</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John D. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34:177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e M Bernardo</author>
<author>Adrian F M Smith</author>
</authors>
<title>Bayesian Theory.</title>
<date>2000</date>
<booktitle>Series in Probability and Statistics.</booktitle>
<publisher>Wiley</publisher>
<contexts>
<context position="18165" citStr="Bernardo and Smith, 2000" startWordPosition="2958" endWordPosition="2961">ty is decomposable into a product of probabilities over individual paragraphs, where paragraphs with different topics have conditionally independent word probabilities. Let w−d and z−d indicate the words and topic assignments to documents other than d, and W be the vocabulary size. The probability of the words in d is then: P(wd |z, w−d, θ0) K IP(wd |zd, θk)P(θk |z, w−d, θ0)dθk � k=1 K = H DCM({wd,i : zd,i = k} k=1 |{w−d,i : z−d,i = k}, θ0), (3) where DCM(·) refers to the Dirichlet compound multinomial distribution, the result of integrating over multinomial parameters with a Dirichlet prior (Bernardo and Smith, 2000). For a Dirichlet prior with parameters α = (α1, ... , αW), the DCM assigns the following probability to a series of observations x = {x1, ... , xn}: where N(x, i) refers to the number of times word i appears in x. Here, Γ(·) is the Gamma function, a generalization of the factorial for real numbers. Some algebra shows that the DCM’s posterior probability density function conditioned on a series of observations y = {y1, ... , yn} can be computed by updating each αi with counts of how often word i appears in y: DCM(x |y, α) = DCM(x |α1 + N(y,1), ... , αW + N(y, W)). (4) ∝ , P(wd |z, w−d, θ0), |t</context>
</contexts>
<marker>Bernardo, Smith, 2000</marker>
<rawString>Jos´e M. Bernardo and Adrian F.M. Smith. 2000. Bayesian Theory. Wiley Series in Probability and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="15643" citStr="Bishop, 2006" startWordPosition="2531" endWordPosition="2532">f inversion counts vd — GMM(ρ). (c) Compute the vector of topic assignments zd for document d’s paragraphs, by sorting td according to 7rd.7 (d) For each paragraph p in document d: i. Sample each word wd,p,j according to the language model of p: wd,p,j — Multinomial(Bzd,p). 5 Inference The variables that we aim to infer are the topic assignments z of each paragraph, which are determined by the bag of topics t and ordering 7r for each document. Thus, our goal is to estimate the marginal distributions of t and 7r given the document text. We accomplish this inference task through Gibbs sampling (Bishop, 2006). A Gibbs sampler builds a Markov chain over the hidden variable state space whose stationary distribution is the actual posterior of the joint distribution. Each new sample is drawn from the distribution of a single variable conditioned on previous samples of the other variables. We can “collapse” the sampler by integrating over some of the hidden variables in the model, in effect reducing the state space of the Markov chain. Collapsed sampling has been previously demonstrated to be effective for LDA and its variants (Griffiths and Steyvers, 2004; Porteous et al., 2008; Titov and McDonald, 20</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5138" citStr="Blei et al., 2003" startWordPosition="764" endWordPosition="767">rved, and topic permutations; our model yields significantly how that ordering can improve text analysis. From better results than variants that either use a fixed or- the methodological side, that body of prior work is dering, or are order-agnostic. largely driven by local pairwise constraints, while we aim to encode global constraints. 2 Related Work 3 Problem Formulation Topic and Content Models Our work is grounded in topic modeling approaches, which posit that latent state variables control the generation of words. In earlier topic modeling work such as latent Dirichlet allocation (LDA) (Blei et al., 2003; Griffiths and Steyvers, 2004), documents are treated as bags of words, where each word receives a separate topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Statistical significance of MUC6 results.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Conference on Message Understanding.</booktitle>
<contexts>
<context position="23691" citStr="Chinchor, 1995" startWordPosition="3905" endWordPosition="3906">rtion of paragraphs from that topic whose section heading is the same as the reference heading for that topic, yielding a precision score. High precision means that paragraphs assigned to a single topic usually correspond to the same section heading. The harmonic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper</context>
</contexts>
<marker>Chinchor, 1995</marker>
<rawString>Nancy Chinchor. 1995. Statistical significance of MUC6 results. In Proceedings of the 6th Conference on Message Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="25636" citStr="Eisenstein and Barzilay, 2008" startWordPosition="4218" endWordPosition="4221"> first task, we compare against the hidden topic Markov model (HTMM) (Gruber et al., 2007), which represents topic transitions between adjacent paragraphs in a Markovian fashion, similar to the approach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperforms previous segmentation approaches (Utiyama and Isahara, 2001; Galley et al., 2003; Purver et al., 2006; Malioutov and Barzilay, 2006). BayesSeg enforces the topic contiguity constraint that motivated our model. We provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. Note that BayesSeg processes each document individually, so it cannot capture structural relatedness across documents. To investigate the importance of our ordering model, we consider two</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>Eugene Charniak</author>
</authors>
<title>A unified local and global model for discourse coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT.</booktitle>
<contexts>
<context position="5891" citStr="Elsner et al., 2007" startWordPosition="883" endWordPosition="886">pic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicitly and tractably model</context>
</contexts>
<marker>Elsner, Austerweil, Charniak, 2007</marker>
<rawString>Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse coherence. In Proceedings of NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Fligner</author>
<author>J S Verducci</author>
</authors>
<title>Distance based ranking models.</title>
<date>1986</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="10350" citStr="Fligner and Verducci, 1986" startWordPosition="1626" endWordPosition="1629"> portions of a document, thus satisfying the constraint of topic contiguity. As with LDA, we assume that each topic k is associated with a language model θk. The words of a paragraph assigned to topic k are then drawn from that topic’s language model θk. Before turning to a more formal discussion of the generative process, we first provide background on the permutation model for topic ordering. 4.1 The Generalized Mallows Model A central challenge of the approach we take is modeling the distribution over possible topic permutations. For this purpose we use the generalized Mallows model (GMM) (Fligner and Verducci, 1986; Lebanon and Lafferty, 2002; Meil˘a et al., 2007), which exhibits two appealing properties in the context of this task. First, the model concentrates probability mass on some “canonical” ordering and small perturbations of that ordering. This characteristic matches our constraint that documents from the same domain exhibit structural similarity. Second, its parameter set scales linearly with the permutation length, making it sufficiently constrained and tractable for inference. In general, this distribution could potentially be applied to other NLP applications where ordering is important. Pe</context>
</contexts>
<marker>Fligner, Verducci, 1986</marker>
<rawString>M.A. Fligner and J.S. Verducci. 1986. Distance based ranking models. Journal of the Royal Statistical Society, Series B, 48(3):359–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen R. McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="5169" citStr="Griffiths and Steyvers, 2004" startWordPosition="768" endWordPosition="771">mutations; our model yields significantly how that ordering can improve text analysis. From better results than variants that either use a fixed or- the methodological side, that body of prior work is dering, or are order-agnostic. largely driven by local pairwise constraints, while we aim to encode global constraints. 2 Related Work 3 Problem Formulation Topic and Content Models Our work is grounded in topic modeling approaches, which posit that latent state variables control the generation of words. In earlier topic modeling work such as latent Dirichlet allocation (LDA) (Blei et al., 2003; Griffiths and Steyvers, 2004), documents are treated as bags of words, where each word receives a separate topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies betw</context>
<context position="16196" citStr="Griffiths and Steyvers, 2004" startWordPosition="2619" endWordPosition="2622">xt. We accomplish this inference task through Gibbs sampling (Bishop, 2006). A Gibbs sampler builds a Markov chain over the hidden variable state space whose stationary distribution is the actual posterior of the joint distribution. Each new sample is drawn from the distribution of a single variable conditioned on previous samples of the other variables. We can “collapse” the sampler by integrating over some of the hidden variables in the model, in effect reducing the state space of the Markov chain. Collapsed sampling has been previously demonstrated to be effective for LDA and its variants (Griffiths and Steyvers, 2004; Porteous et al., 2008; Titov and McDonald, 2008). Our sampler integrates over all but three sets 7Multiple permutations can contribute to the probability of a single document’s topic assignments zd, if there are topics that do not appear in td. As a result, our current formulation is biased toward assignments with fewer topics per document. In practice, we do not find this to negatively impact model performance. 374 Equation 3 and 4 will be used again to compute the conditional distributions of the hidden variables. We now turn to a discussion of how each individual random variable is resamp</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="5586" citStr="Griffiths et al., 2005" startWordPosition="836" endWordPosition="839">proaches, which posit that latent state variables control the generation of words. In earlier topic modeling work such as latent Dirichlet allocation (LDA) (Blei et al., 2003; Griffiths and Steyvers, 2004), documents are treated as bags of words, where each word receives a separate topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally cons</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2007</date>
<booktitle>In Proceedings of AISTATS.</booktitle>
<contexts>
<context position="5644" citStr="Gruber et al., 2007" startWordPosition="846" endWordPosition="849">e generation of words. In earlier topic modeling work such as latent Dirichlet allocation (LDA) (Blei et al., 2003; Griffiths and Steyvers, 2004), documents are treated as bags of words, where each word receives a separate topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level</context>
<context position="25096" citStr="Gruber et al., 2007" startWordPosition="4142" endWordPosition="4145">not standardized and usually not reported for the segmentation task, so we omit these tests in our results. 376 GMM parameters p to always be zero. Both variants still enforce topic contiguity, and allow segments across documents to be aligned by topic assignment. of segments in each document. We report results using both K = 10 and 20 (recall that the cleanly annotated Cities data set had 18 topics). Baselines and Model Variants We consider baselines from the literature that perform either alignment or segmentation. For the first task, we compare against the hidden topic Markov model (HTMM) (Gruber et al., 2007), which represents topic transitions between adjacent paragraphs in a Markovian fashion, similar to the approach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperfor</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007. Hidden topic markov models. In Proceedings of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="1448" citStr="Halliday and Hasan, 1976" startWordPosition="204" endWordPosition="207">roach substantially outperforms alternative approaches for cross-document comparison and single-document segmentation.1 1 Introduction In this paper, we introduce a novel latent topic model for the unsupervised learning of document structure. Traditional topic models assume that topics are randomly spread throughout a document, or that the succession of topics in a document is Markovian. In contrast, our approach takes advantage of two important discourse-level properties of text in determining topic assignments: first, that each document follows a progression of nonrecurring coherent topics (Halliday and Hasan, 1976); and second, that documents from the same domain tend to present similar topics, in similar orders (Wray, 2002). We show that a topic model incorporating these long-range dependencies outperforms al1Code, data, and annotations used in this work are available at http://groups.csail.mit.edu/rbg/code/mallows/ ternative approaches for segmentation and crossdocument comparison. For example, consider a collection of encyclopedia articles about cities. The first constraint captures the notion that a single topic, such as Architecture, is expressed in a contiguous block within the document, rather th</context>
<context position="6857" citStr="Halliday and Hasan, 1976" startWordPosition="1029" endWordPosition="1032">course-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicitly and tractably model global discourse-level constraints. Our document structure learning problem can be formalized as follows. We are given a corpus of D related documents. Each document expresses some subset of a common set of K topics. We assign a single topic to each paragraph,2 incorporating the notion that paragraphs are internally topically consistent (Halliday and Hasan, 1976). To capture the discourse constraint on topic progression described in Section 1, we require that topic assignments be contiguous within each document.3 Furthermore, we assume that the underlying topic sequences exhibit similarity across documents. Our goal is to recover a topic assignment for each paragraph in the corpus, subject to these constraints. Our formulation shares some similarity with the standard LDA setup, in that a common set of topics is assigned across a collection of documents. However, in LDA each word’s topic assignment is conditionally independent, following the bag of wor</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Massimo Poesio</author>
<author>Chris Mellish</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centeringbased metrics of coherence for text structuring using a reliably annotated corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4098" citStr="Karamanis et al., 2004" startWordPosition="603" endWordPosition="606">Conference of the North American Chapter of the ACL, pages 371–379, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics document-level tasks. In the alignment task, we aim Modeling Ordering Constraints Sentence orderto discover paragraphs across different documents ing has been extensively studied in the context of that share the same topic. We also consider the seg- probabilistic text modeling for summarization and mentation task, where the goal is to partition each generation (Barzilay et al., 2002; Lapata, 2003; document into a sequence of topically coherent seg- Karamanis et al., 2004). The emphasis of that body ments. We find that our structure modeling approach of work is on learning ordering constraints from substantially outperforms state-of-the-art baselines data, with the goal of reordering new text from the for both tasks. Furthermore, we demonstrate the im- same domain. Our emphasis, however, is on apportance of explicitly modeling a distribution over plications where ordering is already observed, and topic permutations; our model yields significantly how that ordering can improve text analysis. From better results than variants that either use a fixed or- the metho</context>
</contexts>
<marker>Karamanis, Poesio, Mellish, Oberlander, 2004</marker>
<rawString>Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centeringbased metrics of coherence for text structuring using a reliably annotated corpus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4021" citStr="Lapata, 2003" startWordPosition="593" endWordPosition="594"> two challenging 371 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371–379, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics document-level tasks. In the alignment task, we aim Modeling Ordering Constraints Sentence orderto discover paragraphs across different documents ing has been extensively studied in the context of that share the same topic. We also consider the seg- probabilistic text modeling for summarization and mentation task, where the goal is to partition each generation (Barzilay et al., 2002; Lapata, 2003; document into a sequence of topically coherent seg- Karamanis et al., 2004). The emphasis of that body ments. We find that our structure modeling approach of work is on learning ordering constraints from substantially outperforms state-of-the-art baselines data, with the goal of reordering new text from the for both tasks. Furthermore, we demonstrate the im- same domain. Our emphasis, however, is on apportance of explicitly modeling a distribution over plications where ordering is already observed, and topic permutations; our model yields significantly how that ordering can improve text anal</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Lebanon</author>
<author>John Lafferty</author>
</authors>
<title>Cranking: combining rankings using conditional probability models on permutations.</title>
<date>2002</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10378" citStr="Lebanon and Lafferty, 2002" startWordPosition="1630" endWordPosition="1633">s satisfying the constraint of topic contiguity. As with LDA, we assume that each topic k is associated with a language model θk. The words of a paragraph assigned to topic k are then drawn from that topic’s language model θk. Before turning to a more formal discussion of the generative process, we first provide background on the permutation model for topic ordering. 4.1 The Generalized Mallows Model A central challenge of the approach we take is modeling the distribution over possible topic permutations. For this purpose we use the generalized Mallows model (GMM) (Fligner and Verducci, 1986; Lebanon and Lafferty, 2002; Meil˘a et al., 2007), which exhibits two appealing properties in the context of this task. First, the model concentrates probability mass on some “canonical” ordering and small perturbations of that ordering. This characteristic matches our constraint that documents from the same domain exhibit structural similarity. Second, its parameter set scales linearly with the permutation length, making it sufficiently constrained and tractable for inference. In general, this distribution could potentially be applied to other NLP applications where ordering is important. Permutation Representation Typ</context>
</contexts>
<marker>Lebanon, Lafferty, 2002</marker>
<rawString>Guy Lebanon and John Lafferty. 2002. Cranking: combining rankings using conditional probability models on permutations. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="25831" citStr="Malioutov and Barzilay, 2006" startWordPosition="4245" endWordPosition="4248">proach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperforms previous segmentation approaches (Utiyama and Isahara, 2001; Galley et al., 2003; Purver et al., 2006; Malioutov and Barzilay, 2006). BayesSeg enforces the topic contiguity constraint that motivated our model. We provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. Note that BayesSeg processes each document individually, so it cannot capture structural relatedness across documents. To investigate the importance of our ordering model, we consider two variants of our model that alternately relax and tighten ordering constraints. In the constrained model, we require all documents to follow the same canonical ordering of topics. This is equival</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
<author>Kapil Phadnis</author>
<author>Arthur Patterson</author>
<author>Jeff Bilmes</author>
</authors>
<title>Consensus ranking under the exponential model.</title>
<date>2007</date>
<booktitle>In Proceedings of UAI.</booktitle>
<marker>Meil˘a, Phadnis, Patterson, Bilmes, 2007</marker>
<rawString>Marina Meil˘a, Kapil Phadnis, Arthur Patterson, and Jeff Bilmes. 2007. Consensus ranking under the exponential model. In Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="20477" citStr="Neal, 2003" startWordPosition="3394" endWordPosition="3395">e resulting probabilities. GMM Parameters For each j = 1 to K − 1, we resample ρj from its posterior distribution: P(ρj |...) = GMM0(ρj E� i vj,i + vj,0ν0N + ν0 , N + ν0 DCM(x |α) = Γ(Ej αj) W Γ(N(x, i) + αi) H i=1 Γ(|x |+ Ej αj) , j Γ(αj) 375 where GMMO is evaluated according to equation 2. The normalization constant of this distribution is unknown, meaning that we cannot directly compute and invert the cumulative distribution function to sample from this distribution. However, the distribution itself is univariate and unimodal, so we can expect that an MCMC technique such as slice sampling (Neal, 2003) should perform well. In practice, the MATLAB black-box slice sampler provides a robust draw from this distribution. 6 Experimental Setup Data Sets We evaluate our model on two data sets drawn from the English Wikipedia. The first set is 100 articles about large cities, with topics such as History, Culture, and Demographics. The second is 118 articles about chemical elements in the periodic table, including topics such as Biological Role, Occurrence, and Isotopes. Within each corpus, articles often exhibit similar section orderings, but many have idiosyncratic inversions. This structural varia</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="23490" citStr="Noreen, 1989" startWordPosition="3875" endWordPosition="3876">me topic. Conversely, we can find the closest section heading to each topic, by finding the section heading that is most common for the paragraphs assigned to a single topic. We then compute the proportion of paragraphs from that topic whose section heading is the same as the reference heading for that topic, yielding a precision score. High precision means that paragraphs assigned to a single topic usually correspond to the same section heading. The harmonic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improper</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--19</pages>
<contexts>
<context position="23960" citStr="Pevzner and Hearst, 2002" startWordPosition="3949" endWordPosition="3952">monic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper bound on the number of latent topics. Note that our algorithm can select fewer than K topics for each document, so K does not determine the number 8Statistical significance testing is not standardized and usually not reported for the segmentation task, so we omit thes</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28:19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Porteous</author>
<author>David Newman</author>
<author>Alexander Ihler</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Fast collapsed gibbs sampling for latent dirichlet allocation.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGKDD.</booktitle>
<contexts>
<context position="16219" citStr="Porteous et al., 2008" startWordPosition="2623" endWordPosition="2626">ce task through Gibbs sampling (Bishop, 2006). A Gibbs sampler builds a Markov chain over the hidden variable state space whose stationary distribution is the actual posterior of the joint distribution. Each new sample is drawn from the distribution of a single variable conditioned on previous samples of the other variables. We can “collapse” the sampler by integrating over some of the hidden variables in the model, in effect reducing the state space of the Markov chain. Collapsed sampling has been previously demonstrated to be effective for LDA and its variants (Griffiths and Steyvers, 2004; Porteous et al., 2008; Titov and McDonald, 2008). Our sampler integrates over all but three sets 7Multiple permutations can contribute to the probability of a single document’s topic assignments zd, if there are topics that do not appear in td. As a result, our current formulation is biased toward assignments with fewer topics per document. In practice, we do not find this to negatively impact model performance. 374 Equation 3 and 4 will be used again to compute the conditional distributions of the hidden variables. We now turn to a discussion of how each individual random variable is resampled. Bag of Topics Firs</context>
</contexts>
<marker>Porteous, Newman, Ihler, Asuncion, Smyth, Welling, 2008</marker>
<rawString>Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2008. Fast collapsed gibbs sampling for latent dirichlet allocation. In Proceedings of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Konrad K¨ording</author>
<author>Thomas L Griffiths</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING.</booktitle>
<marker>Purver, K¨ording, Griffiths, Tenenbaum, 2006</marker>
<rawString>Matthew Purver, Konrad K¨ording, Thomas L. Griffiths, and Joshua B. Tenenbaum. 2006. Unsupervised topic modelling for multi-party spoken discourse. In Proceedings of ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="23719" citStr="Riezler and Maxwell, 2005" startWordPosition="3907" endWordPosition="3910">phs from that topic whose section heading is the same as the reference heading for that topic, yielding a precision score. High precision means that paragraphs assigned to a single topic usually correspond to the same section heading. The harmonic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper bound on the number of late</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="6123" citStr="Titov and McDonald, 2008" startWordPosition="917" endWordPosition="920">resentations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicitly and tractably model global discourse-level constraints. Our document structure learning problem can be formalized as follows. We are given a corpus of D related documents. Each document expresses some subset of a common set of K topics. We assign a si</context>
<context position="16246" citStr="Titov and McDonald, 2008" startWordPosition="2627" endWordPosition="2630">ampling (Bishop, 2006). A Gibbs sampler builds a Markov chain over the hidden variable state space whose stationary distribution is the actual posterior of the joint distribution. Each new sample is drawn from the distribution of a single variable conditioned on previous samples of the other variables. We can “collapse” the sampler by integrating over some of the hidden variables in the model, in effect reducing the state space of the Markov chain. Collapsed sampling has been previously demonstrated to be effective for LDA and its variants (Griffiths and Steyvers, 2004; Porteous et al., 2008; Titov and McDonald, 2008). Our sampler integrates over all but three sets 7Multiple permutations can contribute to the probability of a single document’s topic assignments zd, if there are topics that do not appear in td. As a result, our current formulation is biased toward assignments with fewer topics per document. In practice, we do not find this to negatively impact model performance. 374 Equation 3 and 4 will be used again to compute the conditional distributions of the hidden variables. We now turn to a discussion of how each individual random variable is resampled. Bag of Topics First we consider how to resamp</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="25758" citStr="Utiyama and Isahara, 2001" startWordPosition="4233" endWordPosition="4236">between adjacent paragraphs in a Markovian fashion, similar to the approach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperforms previous segmentation approaches (Utiyama and Isahara, 2001; Galley et al., 2003; Purver et al., 2006; Malioutov and Barzilay, 2006). BayesSeg enforces the topic contiguity constraint that motivated our model. We provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. Note that BayesSeg processes each document individually, so it cannot capture structural relatedness across documents. To investigate the importance of our ordering model, we consider two variants of our model that alternately relax and tighten ordering constraints. In the constrained model, we require all d</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: beyond bag of words.</title>
<date>2006</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="5601" citStr="Wallach, 2006" startWordPosition="840" endWordPosition="841">at latent state variables control the generation of words. In earlier topic modeling work such as latent Dirichlet allocation (LDA) (Blei et al., 2003; Griffiths and Steyvers, 2004), documents are treated as bags of words, where each word receives a separate topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models </context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: beyond bag of words. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Wray</author>
</authors>
<title>Formulaic Language and the Lexicon.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1560" citStr="Wray, 2002" startWordPosition="225" endWordPosition="226">oduction In this paper, we introduce a novel latent topic model for the unsupervised learning of document structure. Traditional topic models assume that topics are randomly spread throughout a document, or that the succession of topics in a document is Markovian. In contrast, our approach takes advantage of two important discourse-level properties of text in determining topic assignments: first, that each document follows a progression of nonrecurring coherent topics (Halliday and Hasan, 1976); and second, that documents from the same domain tend to present similar topics, in similar orders (Wray, 2002). We show that a topic model incorporating these long-range dependencies outperforms al1Code, data, and annotations used in this work are available at http://groups.csail.mit.edu/rbg/code/mallows/ ternative approaches for segmentation and crossdocument comparison. For example, consider a collection of encyclopedia articles about cities. The first constraint captures the notion that a single topic, such as Architecture, is expressed in a contiguous block within the document, rather than spread over disconnected sections. The second constraint reflects our intuition that all of these related art</context>
</contexts>
<marker>Wray, 2002</marker>
<rawString>Alison Wray. 2002. Formulaic Language and the Lexicon. Cambridge University Press, Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>