<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040510">
<title confidence="0.9974265">
Do Automatic Annotation Techniques Have Any Impact on Supervised
Complex Question Answering?
</title>
<author confidence="0.985075">
Yllias Chali Sadid A. Hasan Shafiq R. Joty
</author>
<affiliation confidence="0.8990115">
University of Lethbridge University of Lethbridge University of British Columbia
Lethbridge, AB, Canada Lethbridge, AB, Canada Vancouver, BC, Canada
</affiliation>
<email confidence="0.994445">
chali@cs.uleth.ca hasan@cs.uleth.ca rjoty@cs.ubc.ca
</email>
<sectionHeader confidence="0.993773" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980814814815">
In this paper, we analyze the impact of
different automatic annotation methods on
the performance of supervised approaches
to the complex question answering prob-
lem (defined in the DUC-2007 main task).
Huge amount of annotated or labeled
data is a prerequisite for supervised train-
ing. The task of labeling can be ac-
complished either by humans or by com-
puter programs. When humans are em-
ployed, the whole process becomes time
consuming and expensive. So, in order
to produce a large set of labeled data we
prefer the automatic annotation strategy.
We apply five different automatic anno-
tation techniques to produce labeled data
using ROUGE similarity measure, Ba-
sic Element (BE) overlap, syntactic sim-
ilarity measure, semantic similarity mea-
sure, and Extended String Subsequence
Kernel (ESSK). The representative super-
vised methods we use are Support Vec-
tor Machines (SVM), Conditional Ran-
dom Fields (CRF), Hidden Markov Mod-
els (HMM), and Maximum Entropy (Max-
Ent). Evaluation results are presented to
show the impact.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940333333333">
In this paper, we consider the complex question
answering problem defined in the DUC-2007 main
task1. We focus on an extractive approach of sum-
marization to answer complex questions where a
subset of the sentences in the original documents
are chosen. For supervised learning methods,
huge amount of annotated or labeled data sets are
obviously required as a precondition. The deci-
sion as to whether a sentence is important enough
</bodyText>
<footnote confidence="0.930328">
1http://www-nlpir.nist.gov/projects/duc/duc2007/
</footnote>
<bodyText confidence="0.999484125">
to be annotated can be taken either by humans or
by computer programs. When humans are em-
ployed in the process, producing such a large la-
beled corpora becomes time consuming and ex-
pensive. There comes the necessity of using au-
tomatic methods to align sentences with the in-
tention to build extracts from abstracts. In this
paper, we use ROUGE similarity measure, Basic
Element (BE) overlap, syntactic similarity mea-
sure, semantic similarity measure, and Extended
String Subsequence Kernel (ESSK) to automati-
cally label the corpora of sentences (DUC-2006
data) into extract summary or non-summary cat-
egories in correspondence with the document ab-
stracts. We feed these 5 types of labeled data into
the learners of each of the supervised approaches:
SVM, CRF, HMM, and MaxEnt. Then we exten-
sively investigate the performance of the classi-
fiers to label unseen sentences (from 25 topics of
DUC-2007 data set) as summary or non-summary
sentence. The experimental results clearly show
the impact of different automatic annotation meth-
ods on the performance of the candidate super-
vised techniques.
</bodyText>
<sectionHeader confidence="0.981236" genericHeader="method">
2 Automatic Annotation Schemes
</sectionHeader>
<subsectionHeader confidence="0.751571">
Using ROUGE Similarity Measures ROUGE
</subsectionHeader>
<bodyText confidence="0.998860571428571">
(Recall-Oriented Understudy for Gisting Evalua-
tion) is an automatic tool to determine the qual-
ity of a summary using a collection of measures
ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W
and ROUGE-S which count the number of over-
lapping units such as n-gram, word-sequences,
and word-pairs between the extract and the ab-
stract summaries (Lin, 2004). We assume each
individual document sentence as the extract sum-
mary and calculate its ROUGE similarity scores
with the corresponding abstract summaries. Thus
an average ROUGE score is assigned to each sen-
tence in the document. We choose the top N sen-
tences based on ROUGE scores to have the label
</bodyText>
<page confidence="0.990069">
329
</page>
<note confidence="0.9394715">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 329–332,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.975034515151515">
+1 (summary sentences) and the rest to have the
label −1 (non-summary sentences).
Basic Element (BE) Overlap Measure We ex-
tract BEs, the “head-modifier-relation” triples for
the sentences in the document collection using BE
package 1.0 distributed by ISI 2. The ranked list
of BEs sorted according to their Likelihood Ra-
tio (LR) scores contains important BEs at the top
which may or may not be relevant to the abstract
summary sentences. We filter those BEs by check-
ing possible matches with an abstract sentence
word or a related word. For each abstract sen-
tence, we assign a score to every document sen-
tence as the sum of its filtered BE scores divided
by the number of BEs in the sentence. Thus, ev-
ery abstract sentence contributes to the BE score
of each document sentence and we select the top
N sentences based on average BE scores to have
the label +1 and the rest to have the label −1.
Syntactic Similarity Measure In order to cal-
culate the syntactic similarity between the abstract
sentence and the document sentence, we first parse
the corresponding sentences into syntactic trees
using Charniak parser 3 (Charniak, 1999) and then
we calculate the similarity between the two trees
using the tree kernel (Collins and Duffy, 2001).
We convert each parenthesis representation gener-
ated by Charniak parser to its corresponding tree
and give the trees as input to the tree kernel func-
tions for measuring the syntactic similarity. The
tree kernel of two syntactic trees T1 and T2 is ac-
tually the inner product of the two m-dimensional
vectors, v(T1) and v(T2):
</bodyText>
<equation confidence="0.995278">
TK(T1,T2) = v(T1).v(T2)
</equation>
<bodyText confidence="0.999869153846154">
The TK (tree kernel) function gives the simi-
larity score between the abstract sentence and the
document sentence based on the syntactic struc-
ture. Each abstract sentence contributes a score to
the document sentences and the top N sentences
are selected to be annotated as +1 and the rest as
−1 based on the average of similarity scores.
Semantic Similarity Measure Shallow seman-
tic representations, bearing a more compact infor-
mation, can prevent the sparseness of deep struc-
tural approaches and the weakness of BOW mod-
els (Moschitti et al., 2007). To experiment with
semantic structures, we parse the corresponding
</bodyText>
<footnote confidence="0.999739">
2BE website:http://www.isi.edu/ cyl/BE
3available at ftp://ftp.cs.brown.edu/pub/nlparser/
</footnote>
<bodyText confidence="0.999529333333333">
sentences semantically using a Semantic Role La-
beling (SRL) system like ASSERT4. ASSERT is
an automatic statistical semantic role tagger, that
can annotate naturally occuring text with semantic
arguments. We represent the annotated sentences
using tree structures called semantic trees (ST).
Thus, by calculating the similarity between STs,
each document sentence gets a semantic similarity
score corresponding to each abstract sentence and
then the top N sentences are selected to be labeled
as +1 and the rest as −1 on the basis of average
similarity scores.
</bodyText>
<table confidence="0.5203294">
Extended String Subsequence Kernel (ESSK)
Formally, ESSK is defined as follows (Hirao et al.,
2004): Kessk(T, U) = d E E Km(ti, uj)
E tiET ujEU
m=1
</table>
<equation confidence="0.953174333333333">
� val(ti, uj) if m = 1
Km(ti, uj) = 0
Km−1(ti, uj) · val(ti, uj)
</equation>
<bodyText confidence="0.9983652">
Here, K�m(ti, uj) is defined below. ti and uj
are the nodes of T and U, respectively. Each node
includes a word and its disambiguated sense. The
function val(t, u) returns the number of attributes
common to the given nodes t and u.
</bodyText>
<equation confidence="0.995094">
� _ 0 / / if j = 1
Km (ti, uj) AK0m(ti, uj−1) + K00m (ti, uj−1)
</equation>
<bodyText confidence="0.891715333333333">
Here A is the decay parameter for the number
of skipped words. We choose A = 0.5 for this
research. K00m(ti, uj) is defined as:
</bodyText>
<equation confidence="0.997260333333333">
� _ 0 / / if i = 1
Km (ti, uj) —
λK00m(ti−1, uj) + Km(ti−1, uj)
</equation>
<bodyText confidence="0.99246">
Finally, the similarity measure is defined after
normalization as below:
</bodyText>
<equation confidence="0.99799325">
Kessk(T, U)
simessk(T, U) =
-VI
Kessk(T, T )Kessk(U, U)
</equation>
<bodyText confidence="0.997116">
Indeed, this is the similarity score we assign to
each document sentence for each abstract sentence
and in the end, top N sentences are selected to
be annotated as +1 and the rest as −1 based on
average similarity scores.
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9105892">
Task Description The problem definition at
DUC-2007 was: “Given a complex question (topic
description) and a collection of relevant docu-
ments, the task is to synthesize a fluent, well-
organized 250-word summary of the documents
</bodyText>
<footnote confidence="0.99918">
4available at http://cemantix.org/assert
</footnote>
<page confidence="0.998177">
330
</page>
<bodyText confidence="0.999797375">
that answers the question(s) in the topic”. We con-
sider this task and use the five automatic annota-
tion methods to label each sentence of the 50 doc-
ument sets of DUC-2006 to produce five differ-
ent versions of training data for feeding the SVM,
HMM, CRF and MaxEnt learners. We choose the
top 30% sentences (based on the scores assigned
by an annotation scheme) of a document set to
have the label +1 and the rest to have −1. Unla-
beled sentences of 25 document sets of DUC-2007
data are used for the testing purpose.
Feature Space We represent each of the
document-sentences as a vector of feature-values.
We extract several query-related features and
some other important features from each sen-
tence. We use the features: n-gram overlap,
Longest Common Subsequence (LCS), Weighted
LCS (WLCS), skip-bigram, exact word overlap,
synonym overlap, hypernym/hyponym overlap,
gloss overlap, Basic Element (BE) overlap, syn-
tactic tree similarity measure, position of sen-
tences, length of sentences, Named Entity (NE),
cue word match, and title match (Edmundson,
1969).
Supervised Systems For SVM we use second
order polynomial kernel for the ROUGE and
ESSK labeled training. For the BE, syntactic, and
semantic labeled training third order polynomial
kernel is used. The use of kernel is based on the
accuracy we achieved during training. We apply
3-fold cross validation with randomized local-grid
search for estimating the value of the trade-off pa-
rameter C. We try the value of C in 2i following
heuristics, where i E 1−5, −4,··· , 4, 51 and set
C as the best performed value 0.125 for second
order polynomial kernel and default value is used
for third order kernel. We use SV Mlight 5 pack-
age for training and testing in this research. In case
of HMM, we apply the Maximum Likelihood Esti-
mation (MLE) technique by frequency counts with
add-one smoothing to estimate the three HMM
parameters: initial state probabilities, transition
probabilities and emission probabilities. We use
Dr. Dekang Lin’s HMM package6 to generate
the most probable label sequence given the model
parameters and the observation sequence (unla-
beled DUC-2007 test data). We use MALLET-0.4
NLP toolkit7 to implement the CRF. We formu-
</bodyText>
<footnote confidence="0.999846333333333">
5http://svmlight.joachims.org/
6http://www.cs.ualberta.ca/ ˜lindek/hmm.htm
7http://mallet.cs.umass.edu/
</footnote>
<bodyText confidence="0.989304571428571">
late our problem in terms of MALLET’s Simple-
Tagger class which is a command line interface to
the MALLET CRF class. We modify the Simple-
Tagger class in order to include the provision for
producing corresponding posterior probabilities of
the predicted labels which are used later for rank-
ing sentences. We build the MaxEnt system using
Dr. Dekang Lin’s MaxEnt package8. To define the
exponential prior of the A values in MaxEnt mod-
els, an extra parameter α is used in the package
during training. We keep the value of α as default.
Sentence Selection The proportion of important
sentences in the training data will differ from the
one in the test data. A simple strategy is to rank
the sentences in a document, then select the top N
sentences. In SVM systems, we use the normal-
ized distance from the hyperplane to each sample
to rank the sentences. Then, we choose N sen-
tences until the summary length (250 words for
DUC-2007) is reached. For HMM systems, we
use Maximal Marginal Relevance (MMR) based
method to rank the sentences (Carbonell et al.,
1997). In CRF systems, we generate posterior
probabilities corresponding to each predicted label
in the label sequence to measure the confidence of
each sentence for summary inclusion. Similarly
for MaxEnt, the corresponding probability values
of the predicted labels are used to rank the sen-
tences.
Evaluation Results The multiple “reference
summaries” given by DUC-2007 are used in the
evaluation of our summary content. We evalu-
ate the system generated summaries using the au-
tomatic evaluation toolkit ROUGE (Lin, 2004).
We report the three widely adopted important
ROUGE metrics in the results: ROUGE-1 (uni-
gram), ROUGE-2 (bigram) and ROUGE-SU (skip
bi-gram). Figure 1 shows the ROUGE F-measures
for SVM, HMM, CRF and MaxEnt systems. The
X-axis containing ROUGE, BE, Synt (Syntactic),
Sem (Semantic), and ESSK stands for the annota-
tion scheme used. The Y-axis shows the ROUGE-
1 scores at the top, ROUGE-2 scores at the bottom
and ROUGE-SU scores in the middle. The super-
vised systems are distinguished by the line style
used in the figure.
From the figure, we can see that the ESSK la-
beled SVM system is having the poorest ROUGE -
1 score whereas the Sem labeled system performs
</bodyText>
<footnote confidence="0.978281">
8http://www.cs.ualberta.ca/˜lindek/downloads.htm
</footnote>
<page confidence="0.994512">
331
</page>
<figureCaption confidence="0.999669">
Figure 1: ROUGE F-scores for different supervised systems
</figureCaption>
<bodyText confidence="0.999633392857143">
best. The other annotation methods’ impact is al-
most similar here in terms of ROUGE-1. Ana-
lyzing ROUGE-2 scores, we find that the BE per-
forms the best for SVM, on the other hand, Sem
achieves top ROUGE-SU score. As for the two
measures Sem annotation is performing the best,
we can typically conclude that Sem annotation is
the most suitable method for the SVM system.
ESSK works as the best for HMM and Sem la-
beling performs the worst for all ROUGE scores.
Synt and BE labeled HMMs perform almost simi-
lar whereas ROUGE labeled system is pretty close
to that of ESSK. Again, we see that the CRF per-
forms best with the ESSK annotated data in terms
of ROUGE -1 and ROUGE-SU scores and Sem
has the highest ROUGE-2 score. But BE and Synt
labeling work bad for CRF whereas the ROUGE
labeling performs decently. So, we can typically
conclude that ESSK annotation is the best method
for the CRF system. Analyzing further, we find
that ESSK works best for MaxEnt and BE label-
ing is the worst for all ROUGE scores. We can
also see that ROUGE, Synt and Sem labeled Max-
Ent systems perform almost similar. So, from this
discussion we can come to a conclusion that SVM
system performs best if the training data uses se-
mantic annotation scheme and ESSK works best
for HMM, CRF and MaxEnt systems.
</bodyText>
<sectionHeader confidence="0.997415" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999976666666667">
In the work reported in this paper, we have per-
formed an extensive experimental evaluation to
show the impact of five automatic annotation
methods on the performance of different super-
vised machine learning techniques in confronting
the complex question answering problem. Experi-
mental results show that Sem annotation is the best
for SVM whereas ESSK works well for HMM,
CRF and MaxEnt systems. In the near future,
we plan to work on finding more sophisticated ap-
proaches to effective automatic labeling so that we
can experiment with different supervised methods.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99972334375">
Jaime Carbonell, Yibing Geng, and Jade Goldstein.
1997. Automated query-relevant summarization and
diversity-based reranking. In IJCAI-97 Workshop on
AI in Digital Libraries, pages 12–19, Japan.
Eugene Charniak. 1999. A Maximum-Entropy-
Inspired Parser. In Technical Report CS-99-12,
Brown University, Computer Science Department.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
Neural Information Processing Systems, pages 625–
632, Vancouver, Canada.
Harold P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM, 16(2):264–
285.
Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and Eisaku
Maeda. 2004. Dependency-based sentence align-
ment for multiple document summarization. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 446–452.
Chin-Yew Lin. 2004. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Proceed-
ings of Workshop on Text Summarization Branches
Out, Post-Conference Workshop of Association for
Computational Linguistics, pages 74–81, Barcelona,
Spain.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
Syntactic and Shallow Semantic Kernels for Ques-
tion/Answer Classificaion. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 776–783, Prague, Czech
Republic. ACL.
</reference>
<page confidence="0.998248">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771526">
<title confidence="0.997513">Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?</title>
<author confidence="0.999269">Yllias Chali Sadid A Hasan Shafiq R Joty</author>
<affiliation confidence="0.999874">University of Lethbridge University of Lethbridge University of British Columbia</affiliation>
<address confidence="0.996702">Lethbridge, AB, Canada Lethbridge, AB, Canada Vancouver, BC, Canada</address>
<email confidence="0.965102">chali@cs.uleth.cahasan@cs.uleth.carjoty@cs.ubc.ca</email>
<abstract confidence="0.968046392857143">In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task). Huge amount of annotated or labeled data is a prerequisite for supervised training. The task of labeling can be accomplished either by humans or by computer programs. When humans are employed, the whole process becomes time consuming and expensive. So, in order to produce a large set of labeled data we prefer the automatic annotation strategy. We apply five different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK). The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (Max- Ent). Evaluation results are presented to show the impact.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Yibing Geng</author>
<author>Jade Goldstein</author>
</authors>
<title>Automated query-relevant summarization and diversity-based reranking.</title>
<date>1997</date>
<booktitle>In IJCAI-97 Workshop on AI in Digital Libraries,</booktitle>
<pages>12--19</pages>
<contexts>
<context position="11346" citStr="Carbonell et al., 1997" startWordPosition="1842" endWordPosition="1845">ls, an extra parameter α is used in the package during training. We keep the value of α as default. Sentence Selection The proportion of important sentences in the training data will differ from the one in the test data. A simple strategy is to rank the sentences in a document, then select the top N sentences. In SVM systems, we use the normalized distance from the hyperplane to each sample to rank the sentences. Then, we choose N sentences until the summary length (250 words for DUC-2007) is reached. For HMM systems, we use Maximal Marginal Relevance (MMR) based method to rank the sentences (Carbonell et al., 1997). In CRF systems, we generate posterior probabilities corresponding to each predicted label in the label sequence to measure the confidence of each sentence for summary inclusion. Similarly for MaxEnt, the corresponding probability values of the predicted labels are used to rank the sentences. Evaluation Results The multiple “reference summaries” given by DUC-2007 are used in the evaluation of our summary content. We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE (Lin, 2004). We report the three widely adopted important ROUGE metrics in the results: ROUGE-</context>
</contexts>
<marker>Carbonell, Geng, Goldstein, 1997</marker>
<rawString>Jaime Carbonell, Yibing Geng, and Jade Goldstein. 1997. Automated query-relevant summarization and diversity-based reranking. In IJCAI-97 Workshop on AI in Digital Libraries, pages 12–19, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-EntropyInspired Parser. In</title>
<date>1999</date>
<tech>Technical Report CS-99-12,</tech>
<institution>Brown University, Computer Science Department.</institution>
<contexts>
<context position="4923" citStr="Charniak, 1999" startWordPosition="778" endWordPosition="779">d or a related word. For each abstract sentence, we assign a score to every document sentence as the sum of its filtered BE scores divided by the number of BEs in the sentence. Thus, every abstract sentence contributes to the BE score of each document sentence and we select the top N sentences based on average BE scores to have the label +1 and the rest to have the label −1. Syntactic Similarity Measure In order to calculate the syntactic similarity between the abstract sentence and the document sentence, we first parse the corresponding sentences into syntactic trees using Charniak parser 3 (Charniak, 1999) and then we calculate the similarity between the two trees using the tree kernel (Collins and Duffy, 2001). We convert each parenthesis representation generated by Charniak parser to its corresponding tree and give the trees as input to the tree kernel functions for measuring the syntactic similarity. The tree kernel of two syntactic trees T1 and T2 is actually the inner product of the two m-dimensional vectors, v(T1) and v(T2): TK(T1,T2) = v(T1).v(T2) The TK (tree kernel) function gives the similarity score between the abstract sentence and the document sentence based on the syntactic struct</context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A Maximum-EntropyInspired Parser. In Technical Report CS-99-12, Brown University, Computer Science Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5030" citStr="Collins and Duffy, 2001" startWordPosition="794" endWordPosition="797">he sum of its filtered BE scores divided by the number of BEs in the sentence. Thus, every abstract sentence contributes to the BE score of each document sentence and we select the top N sentences based on average BE scores to have the label +1 and the rest to have the label −1. Syntactic Similarity Measure In order to calculate the syntactic similarity between the abstract sentence and the document sentence, we first parse the corresponding sentences into syntactic trees using Charniak parser 3 (Charniak, 1999) and then we calculate the similarity between the two trees using the tree kernel (Collins and Duffy, 2001). We convert each parenthesis representation generated by Charniak parser to its corresponding tree and give the trees as input to the tree kernel functions for measuring the syntactic similarity. The tree kernel of two syntactic trees T1 and T2 is actually the inner product of the two m-dimensional vectors, v(T1) and v(T2): TK(T1,T2) = v(T1).v(T2) The TK (tree kernel) function gives the similarity score between the abstract sentence and the document sentence based on the syntactic structure. Each abstract sentence contributes a score to the document sentences and the top N sentences are selec</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of Neural Information Processing Systems, pages 625– 632, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>285</pages>
<contexts>
<context position="9045" citStr="Edmundson, 1969" startWordPosition="1467" endWordPosition="1468">s of 25 document sets of DUC-2007 data are used for the testing purpose. Feature Space We represent each of the document-sentences as a vector of feature-values. We extract several query-related features and some other important features from each sentence. We use the features: n-gram overlap, Longest Common Subsequence (LCS), Weighted LCS (WLCS), skip-bigram, exact word overlap, synonym overlap, hypernym/hyponym overlap, gloss overlap, Basic Element (BE) overlap, syntactic tree similarity measure, position of sentences, length of sentences, Named Entity (NE), cue word match, and title match (Edmundson, 1969). Supervised Systems For SVM we use second order polynomial kernel for the ROUGE and ESSK labeled training. For the BE, syntactic, and semantic labeled training third order polynomial kernel is used. The use of kernel is based on the accuracy we achieved during training. We apply 3-fold cross validation with randomized local-grid search for estimating the value of the trade-off parameter C. We try the value of C in 2i following heuristics, where i E 1−5, −4,··· , 4, 51 and set C as the best performed value 0.125 for second order polynomial kernel and default value is used for third order kerne</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Harold P. Edmundson. 1969. New methods in automatic extracting. Journal of the ACM, 16(2):264– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Dependency-based sentence alignment for multiple document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>446--452</pages>
<contexts>
<context position="6747" citStr="Hirao et al., 2004" startWordPosition="1059" endWordPosition="1062">ole Labeling (SRL) system like ASSERT4. ASSERT is an automatic statistical semantic role tagger, that can annotate naturally occuring text with semantic arguments. We represent the annotated sentences using tree structures called semantic trees (ST). Thus, by calculating the similarity between STs, each document sentence gets a semantic similarity score corresponding to each abstract sentence and then the top N sentences are selected to be labeled as +1 and the rest as −1 on the basis of average similarity scores. Extended String Subsequence Kernel (ESSK) Formally, ESSK is defined as follows (Hirao et al., 2004): Kessk(T, U) = d E E Km(ti, uj) E tiET ujEU m=1 � val(ti, uj) if m = 1 Km(ti, uj) = 0 Km−1(ti, uj) · val(ti, uj) Here, K�m(ti, uj) is defined below. ti and uj are the nodes of T and U, respectively. Each node includes a word and its disambiguated sense. The function val(t, u) returns the number of attributes common to the given nodes t and u. � _ 0 / / if j = 1 Km (ti, uj) AK0m(ti, uj−1) + K00m (ti, uj−1) Here A is the decay parameter for the number of skipped words. We choose A = 0.5 for this research. K00m(ti, uj) is defined as: � _ 0 / / if i = 1 Km (ti, uj) — λK00m(ti−1, uj) + Km(ti−1, uj</context>
</contexts>
<marker>Hirao, Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Dependency-based sentence alignment for multiple document summarization. In Proceedings of the 20th International Conference on Computational Linguistics, pages 446–452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of Association for Computational Linguistics,</booktitle>
<pages>74--81</pages>
<location>Barcelona,</location>
<contexts>
<context position="3360" citStr="Lin, 2004" startWordPosition="513" endWordPosition="514">007 data set) as summary or non-summary sentence. The experimental results clearly show the impact of different automatic annotation methods on the performance of the candidate supervised techniques. 2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram, word-sequences, and word-pairs between the extract and the abstract summaries (Lin, 2004). We assume each individual document sentence as the extract summary and calculate its ROUGE similarity scores with the corresponding abstract summaries. Thus an average ROUGE score is assigned to each sentence in the document. We choose the top N sentences based on ROUGE scores to have the label 329 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 329–332, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP +1 (summary sentences) and the rest to have the label −1 (non-summary sentences). Basic Element (BE) Overlap Measure We extract BEs, the “head-modifier-relation” triple</context>
<context position="11863" citStr="Lin, 2004" startWordPosition="1922" endWordPosition="1923"> Maximal Marginal Relevance (MMR) based method to rank the sentences (Carbonell et al., 1997). In CRF systems, we generate posterior probabilities corresponding to each predicted label in the label sequence to measure the confidence of each sentence for summary inclusion. Similarly for MaxEnt, the corresponding probability values of the predicted labels are used to rank the sentences. Evaluation Results The multiple “reference summaries” given by DUC-2007 are used in the evaluation of our summary content. We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE (Lin, 2004). We report the three widely adopted important ROUGE metrics in the results: ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-SU (skip bi-gram). Figure 1 shows the ROUGE F-measures for SVM, HMM, CRF and MaxEnt systems. The X-axis containing ROUGE, BE, Synt (Syntactic), Sem (Semantic), and ESSK stands for the annotation scheme used. The Y-axis shows the ROUGE1 scores at the top, ROUGE-2 scores at the bottom and ROUGE-SU scores in the middle. The supervised systems are distinguished by the line style used in the figure. From the figure, we can see that the ESSK labeled SVM system is having the poor</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of Association for Computational Linguistics, pages 74–81, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classificaion.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>776--783</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5928" citStr="Moschitti et al., 2007" startWordPosition="943" endWordPosition="946"> of the two m-dimensional vectors, v(T1) and v(T2): TK(T1,T2) = v(T1).v(T2) The TK (tree kernel) function gives the similarity score between the abstract sentence and the document sentence based on the syntactic structure. Each abstract sentence contributes a score to the document sentences and the top N sentences are selected to be annotated as +1 and the rest as −1 based on the average of similarity scores. Semantic Similarity Measure Shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). To experiment with semantic structures, we parse the corresponding 2BE website:http://www.isi.edu/ cyl/BE 3available at ftp://ftp.cs.brown.edu/pub/nlparser/ sentences semantically using a Semantic Role Labeling (SRL) system like ASSERT4. ASSERT is an automatic statistical semantic role tagger, that can annotate naturally occuring text with semantic arguments. We represent the annotated sentences using tree structures called semantic trees (ST). Thus, by calculating the similarity between STs, each document sentence gets a semantic similarity score corresponding to each abstract sentence and </context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classificaion. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783, Prague, Czech Republic. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>