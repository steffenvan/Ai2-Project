<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003562">
<title confidence="0.80292">
An Examination of Regret in Bullying Tweets
Amy Bellmore
</title>
<author confidence="0.997895">
Jun-Ming Xu, Benjamin Burchfiel, Xiaojin Zhu
</author>
<affiliation confidence="0.998634">
Department of Computer Sciences
University of Wisconsin-Madison
</affiliation>
<address confidence="0.643252">
Madison, WI 53706, USA
</address>
<email confidence="0.997347">
{xujm,burchfie,jerryzhu}@cs.wisc.edu
</email>
<affiliation confidence="0.994577">
Department of Educational Psychology
University of Wisconsin-Madison
</affiliation>
<address confidence="0.557875">
Madison, WI 53706, USA
</address>
<email confidence="0.995367">
abellmore@wisc.edu
</email>
<sectionHeader confidence="0.996599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998957272727273">
Social media users who post bullying related
tweets may later experience regret, potentially
causing them to delete their posts. In this pa-
per, we construct a corpus of bullying tweets
and periodically check the existence of each
tweet in order to infer if and when it becomes
deleted. We then conduct exploratory analy-
sis in order to isolate factors associated with
deleted posts. Finally, we propose the con-
struction of a regrettable posts predictor to
warn users if a tweet might cause regret.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996818367347">
A large body of literature suggests that participants
in bullying events, including victims, bullies, and
witnesses, are likely to report psychological adjust-
ment problems (Jimerson, Swearer, and Espelage,
2010). One potential source of therapy for these is-
sues can be self-disclosure of the experience to an
adult or friend (Mishna and Alaggia, 2005); exist-
ing research suggests that victims who seek advice
and help from others report less maladjustment than
victims who do not (Shelley and Craig, 2010).
Disclosure of bullying experiences through so-
cial media may be a particularly effective mecha-
nism for participants seeking support because so-
cial media has the potential to reach large audi-
ences and because participants may feel less inhi-
bition when sharing private information in an on-
line setting (Walther, 1996). Furthermore, there is
evidence that online communication stimulates self-
disclosure, which leads to higher quality social rela-
tionships and increased well-being (Valkenburg and
Peter, 2009).
Online disclosure may also present risks for
those involved in bullying however, such as re-
victimization, embarrassment, and social ostraciza-
tion. Evidence exists that some individuals may re-
act to these risks retroactively, by deleting their so-
cial media posts (Child et al., 2011; Christofides,
Muise, and Desmarais, 2009). Several relevant mo-
tives have been found to be associated with delet-
ing posted information, including conflict manage-
ment, safety, fear of retribution, impression manage-
ment, and emotional regulation (Child, Haridakis,
and Petronio, 2012).
Our previous work (Xu et al., 2012) demonstrates
that social media can be a valuable data source when
studying bullying, and proposes a text categorization
method to recognize social media posts describing
bullying episodes, bullying traces. To better under-
stand, and possibly prevent, user regret after posting
bullying related tweets, we collect bullying traces
using the same method and perform regular status
checks to determine if and when tweets become in-
accessible. While a tweet becoming inaccessible
does not guarantee it has been deleted, we attempt to
leverage http response codes to rule out other com-
mon causes of inaccessibility. Speculating that re-
gret may be a major cause of deletion, we first con-
duct exploratory analysis on this corpus and then re-
port the results of an off-the-shelf regret predictor.
</bodyText>
<sectionHeader confidence="0.974958" genericHeader="method">
2 Data Collection
</sectionHeader>
<bodyText confidence="0.999965">
We adopt the procedure used in (Xu et al., 2012) to
obtain bullying traces; each identified trace contains
</bodyText>
<page confidence="0.980942">
697
</page>
<subsectionHeader confidence="0.293687">
Proceedings of NAACL-HLT 2013, pages 697–702,
</subsectionHeader>
<bodyText confidence="0.9909383">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
at least one bullying related keyword and passes a
bullying-or-not text classifier.
Our data was collected in realtime using the
Twitter streaming API; once a tweet is collected,
we query its url (https://twitter.com/
USERID/status/TWEETID) at regular intervals
and infer its status from the resulting http response
code. We interpret an HTTP 200 response as an indi-
cation a tweet still exists and an HTTP 404 response,
which indicates the tweet is unavailable, as indicat-
ing deletion. A user changing their privacy settings
can also result in an HTTP 403 response; we do not
consider this to be a deletion. Other response codes,
which appear quite rarely, are treated as anomalies
and ignored. All non HTTP 200 responses are re-
tried twice to ensure they are not transient oddities.
To determine when a tweet is deleted, we at-
tempted to access each tweet at time points Ti =
5 x 4i minutes for i = 0,1... 7 after the creation
time. These roughly correspond to periods of 5 min-
utes, 20 minutes, 1.5 hours, 6 hours, 1 day, 4 days,
2 weeks, and 2 months. While we assume that user
deletion is the main cause of a tweet becoming un-
available, other causes are possible such as the cen-
sorship of illegal contents by Twitter (Twitter, 2012).
Our sample data was collected from July 31
through October 31, 2012 and contains 522,984 bul-
lying traces. Because of intermittent network and
computer issues, several multiple day data gaps ex-
ist in the data. To combat this, we filter our data to
include only tweets of unambiguous status. If any
check within the 20480 minutes (about two weeks)
interval returns an HTTP 404 code, the tweet is
no longer accessible and we consider it deleted. If
the 20480 minute or 81920 minute check returns an
HTTP 200 response, that tweet is still accessible and
we consider it surviving. The union of the surviving
and deleted groups formed our cleaned dataset, con-
taining 311,237 tweets in total.
</bodyText>
<sectionHeader confidence="0.99436" genericHeader="method">
3 Exploratory Data Analysis
</sectionHeader>
<bodyText confidence="0.999948">
A user’s decision to delete a bullying trace may be
the result of many factors which we would like to
isolate and understand. In this section we will ex-
amine several such possible factors.
</bodyText>
<subsectionHeader confidence="0.999761">
3.1 Word Usage
</subsectionHeader>
<bodyText confidence="0.99999">
Our dataset contains 331,070 distinct words and we
are interested in isolating those with a significantly
higher presence among either deleted or surviving
tweets. We define the odds ratio of a word w
</bodyText>
<equation confidence="0.999333">
P(w  |deleted)
r(w) = P(w  |surviving),
</equation>
<bodyText confidence="0.99998895">
where P(w  |deleted) is the probability of word w
occurring in a deleted tweet, and P(w  |surviving) is
the probability of w appearing in a surviving tweet.
In order to ensure stability in the probability estima-
tion, we only considered words appearing at least 50
times in either the surviving or deleted corpora.
Following (Bamman, OConnor, and Smith,
2012), we qualitatively analyzed words with ex-
treme values of r(w), and found some interesting
trends. There was a significant tendency for “jok-
ing” words to occur with r(w) &lt; 0.5; examples in-
clude “xd,” “haha,” and “hahaha.” Joking words oc-
cur less frequently in deleted tweets than surviving
ones. On the other end of the spectrum, there were
no joking words with r(w) &gt; 2. What we found
instead were words such as “rip,” “fat,” “kill,” and
“suicide.” While it is relatively clear that joking is
less likely to occur in deleted tweets, there was less
of a trend among words appearing more frequently
in deleted tweets.
</bodyText>
<subsectionHeader confidence="0.999979">
3.2 Surviving Time
</subsectionHeader>
<bodyText confidence="0.999458">
Let N be the total number of tweets in our cor-
pus, and D(Ti) be the number of tweets that were
first detected as deleted at minute Ti after creation.
Note that D(Ti) is not cumulative over time: it in-
cludes only deletions that occurred in the time inter-
val (Ti_1, Ti]. Then we may define the deletion rate
at time Ti as
</bodyText>
<equation confidence="0.994085333333333">
D(Ti)
RT (Ti) =
N(Ti − Ti_1).
</equation>
<bodyText confidence="0.999485571428571">
In other words, RT(t) is the fraction of tweets that
are deleted during the one minute period (t, t + 1).
We plot RT vs. t using logarithmic scales on both
axes in Figure 1 and the result is a quite strong linear
trend. Fitting the plot with a linear regression, we
derive an inverse relationship between RT and t of
the form
</bodyText>
<equation confidence="0.330603">
RT(t) a 1/t.
</equation>
<page confidence="0.926607">
698
</page>
<figure confidence="0.997047571428571">
1E−3
1E−4
1E−5
1E−6
1E−7
5 minutes 1.5 hours 1 day 2 weeks
t
</figure>
<figureCaption confidence="0.999993">
Figure 1: Deletion rate decays over time.
</figureCaption>
<bodyText confidence="0.999905181818182">
This result makes sense; the social effects of a par-
ticular bullying tweet may decay over time, making
regret less of a factor. Furthermore, the author may
assume an older tweet has already been seen, render-
ing deletion ineffective. Additionally, because the
drop off in deletion rate is so extreme, we are able to
safely exclude deletions occurring after two weeks
from our filtered dataset without introducing a sig-
nificant amount of noise. Finally, Et_�0 RT(t) gives
the overall fraction of deletion, which in our case is
around 4%.
</bodyText>
<subsectionHeader confidence="0.999265">
3.3 Location and Hour of Creations
</subsectionHeader>
<bodyText confidence="0.999968523809524">
Some bullying traces contain location meta-data in
the form of GPS coordinates or a user-created profile
string. We employed a reverse geocoding database
(http://www.datasciencetoolkit.org)
and a rule-based string matching method to map
these tweets to their origins (at the state level; only
for tweets within the USA). This also allowed us to
convert creation timestamps from UTC to local time
by mapping user location to timezone. Because
many users don’t share their location, we were only
able to successfully map 85,465 bullying traces to a
US state s, and local hour of day h. Among these
traces, 3,484 were deleted which translates to an
overall deletion rate of about 4%.
Let N(s, h) be the count of bullying traces cre-
ated in state s and hour h. Aggregating these counts
temporally yields NS(s) = Eh N(s, h), while ag-
gregating spatially produces NH(h) = Es N(s, h).
Similarly, we can define D(s, h), DS(s) and DH(h)
as the corresponding counts of deleted traces. We
can now compute the deletion rate
</bodyText>
<equation confidence="0.993009">
DH(h) DS(s)
RH(h) = NH(h), and RS(s) = NS(s).
</equation>
<bodyText confidence="0.968923565217391">
The top row of Figure 2 shows NH(h), DH(h),
and RH(h). We find that NH(h) and DH(h) peak
in the evening, indicating social media users are gen-
erally more active at that time. The peak of RH(h)
appears at late night and, while there are multiple
potential causes for this, we hypothesize that users
may fail to fully evaluate the consequences of their
posts when tired. The bottom row of Figure 2 shows
NS(s), DS(S), and RS(s). The plot of NS(s)
shows that bullying traces are more likely to origi-
nate in California, Texas or New York which is the
result of a population effect. Importantly however,
the deletion rate RS(s) is not affected by population
bias and we see, as expected, that spatial differences
in RS(s) are small. We performed χ2-test to see if
a state’s deletion rate is significantly different from
the national average. We chose the significance level
at 0.05 and used Bonferroni correction for multiple
testing. Only four states have significantly differ-
ent deletion rates from the average: Arizona (6.3%,
p = 5.9×10−5), California (5.2%, p = 2.7×10−7),
Maryland (1.9%, p = 2.3 × 10−5), and Oklahoma
(7.1%, p = 3.5 × 10−5).
</bodyText>
<subsectionHeader confidence="0.978916">
3.4 Author’s Role
</subsectionHeader>
<bodyText confidence="0.999955666666667">
Participants in a bullying episode assume well-
defined roles which dramatically affect the view-
point of the author describing the event. We trained
a text classifier to determine author role (Xu et al.,
2012), and used it to label each bullying trace in the
cleaned corpus by author role: Accuser, Bully, Re-
porter, Victim or Other.
Table 1 shows that compared to tweets produced
by bullies, victims create more bullying traces, pos-
sibly due to an increased need for social support on
the part of the victim. More importantly, P(deleted |
victim) is higher than P(deleted  |bully), a statis-
tically significant difference in a two-proportion z-
test. Possibly, victims are more sensitive to their au-
dience’s reaction than bullies.
</bodyText>
<subsectionHeader confidence="0.952989">
3.5 Teasing
</subsectionHeader>
<bodyText confidence="0.960969666666667">
Many bullying traces are written jokingly. We built a
text classifier to identify teasing bullying traces (Xu
et al., 2012) and applied it to the cleaned corpus.
Table 2 shows that P(deletion  |Teasing) is much
lower than P(deletion  |Not Teasing) and the differ-
ence is statistically significant in a two-proportion z-
</bodyText>
<equation confidence="0.68166075">
RT (t)
699
NH(h) DH(h) RH(h)
NS(s) DS(s) RS(s)
</equation>
<figureCaption confidence="0.997452">
Figure 2: Counts and deletion rates of geo-tagged bullying traces.
</figureCaption>
<table confidence="0.999705166666667">
Deleted Total P(deleted I Role)
Accuser 2541 50088 5.07%
Bully 1792 30123 5.95%
Reporter 11370 147164 7.73%
Victim 6497 83412 7.79%
Other 41 450 9.11%
</table>
<tableCaption confidence="0.996469">
Table 1: Counts and deletion rate for different roles.
</tableCaption>
<table confidence="0.99983">
Deleted Total P(deleted I Teasing?)
Yes 858 22876 3.75%
Not 21383 288361 7.42%
</table>
<tableCaption confidence="0.998826">
Table 2: Counts and deletion rate for teasing or not.
</tableCaption>
<bodyText confidence="0.999693">
test. It seems plausible that authors are less likely to
regret teasing posts because they are less controver-
sial and have less potential to generate negative au-
dience reactions. This also corroborates our findings
in word usage that joking words are less frequent in
deleted tweets.
</bodyText>
<sectionHeader confidence="0.888062" genericHeader="method">
4 Predicting Regrettable Tweets
</sectionHeader>
<bodyText confidence="0.9999217">
Once a bullying tweet is published and seen by oth-
ers, the ensuing effects are often impossible to undo.
Since ill-thought-out posts may cause unexpectedly
negative consequences to an author’s reputation, re-
lationship, and career (Wang et al., 2011), it would
be helpful if a system could warn users before a po-
tentially regrettable tweet is posted. One straightfor-
ward approach is to formulate the task as a binary
text categorization problem.
We use the cleaned dataset, in which each tweet
is known to be surviving or deleted after 20480 min-
utes (about two weeks). Since this dataset contains
22,241 deleted tweets, we randomly sub-sampled
the surviving tweets down to 22,241 to force our
deleted and surviving datasets to be of equal size.
Consequentially, the baseline accuracy of the clas-
sifier is 0.5. While this does make the problem ar-
tificially easier, our initial goal was to test for the
presence of a signal in the data.
We then followed the preprocessing procedure
in (Xu et al., 2012), performing case-folding,
anonymization, and tokenization, treating URLs,
emoticons and hashtags specially. We also chose
the unigrams+bigrams feature representation, only
keeping tokens appearing at least 15 times in the cor-
pus.
We chose to employ a linear SVM implemented
in LIBLINEAR (Fan et al., 2008) due to its effi-
ciency on this large sparse text categorization task
and a 10-fold cross validation was conducted to eval-
</bodyText>
<page confidence="0.98587">
700
</page>
<bodyText confidence="0.999936">
uate its performance. Within the first fold, we use
an inner 5-fold cross validation on the training por-
tion to tune the regularization parameter on the grid
{2−10, 2−9, ... ,1}; the selected parameter is then
fixed for all the remaining folds.
The resulting cross validation accuracy was 0.607
with a standard deviation of 0.012. While it is statis-
tically significantly better than the random-guessing
baseline accuracy of 0.5 with a p-value of 5.15 x
10−10, this accuracy is nevertheless too low to be
useful in a practical system. One possibility is that
the tweet text contains very limited information for
predicting inaccessibility; a user’s decision to delete
a tweet potentially depends on many other factors,
such as the conversation context and the characteris-
tics of the author and audience.
In the spirit of exploring additional informative
features for deletion prediction, we also used the
teasing and author role classifiers in (Xu et al.,
2012), and appended the predicted teasing, and au-
thor role labels to our feature vector. This aug-
mented feature representation achieved a cross val-
idation accuracy of 0.606, with standard deviation
0.007; not statistically significantly different from
the text-only feature representation. While it seems
that a signal does exist, leveraging it usefully in real
world scenarios may prove challenging due to the
highly-skewed nature of the data.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999980272727273">
There have been several recent works examin-
ing causes of deletion in social media. Wang
et al. (2011) qualitatively investigated regret associ-
ated with users’ posts on social networking sites and
identified several possible causes of regret. Bamman
et al. (2012) focused on censorship-related deletion
of social media posts, identifying a set of sensitive
terms related to message deletion through a statisti-
cal analysis and spatial variation of deletion rate.
Assuming that deletion in social media is indica-
tive of regret, we studied regret in a bullying con-
text by analyzing deletion trends in bullying re-
lated tweets. Through our analysis, we were able
to isolate several factors related to deletion, includ-
ing word usage, surviving time, and author role. We
used these factors to build a regret predictor which
achieved statistically significant results on this very
noisy data. In the future, we plan to explore more
factors to better understand deletion behavior and re-
gret, including users’ recent posts, historical behav-
ior, and other statistics related to their specific social
network.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99670175">
We thank Kwang-Sung Jun, Angie Calvin, and
Charles Dyer for helpful discussions. This work
is supported by National Science Foundation grants
IIS-1216758 and IIS-1148012.
</bodyText>
<sectionHeader confidence="0.998304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995692">
Bamman, David, Brendan OConnor, and Noah Smith.
2012. Censorship and deletion practices in chinese so-
cial media. First Monday, 17(3-5).
Child, Jeffrey T., Paul M. Haridakis, and Sandra Petro-
nio. 2012. Blogging privacy rule orientations, privacy
management, and content deletion practices: The vari-
ability of online privacy management activity at differ-
ent stages of social media use. Computers in Human
Behavior, 28(5):1859 – 1872.
Child, Jeffrey T, Sandra Petronio, Esther A Agyeman-
Budu, and David A Westermann. 2011. Blog scrub-
bing: Exploring triggers that change privacy rules.
Computers in Human Behavior, 27(5):2017–2027.
Christofides, Emily, Amy Muise, and Serge Desmarais.
2009. Information disclosure and control on facebook:
are they two sides of the same coin or two different
processes? CyberPsychology &amp; Behavior, 12(3):341–
345.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Jimerson, Shane R., Susan M. Swearer, and Dorothy L.
Espelage. 2010. Handbook of Bullying in Schools: An
international perspective. Routledge/Taylor &amp; Francis
Group, New York, NY.
Mishna, Faye and Ramona Alaggia. 2005. Weighing the
risks: A child’s decision to disclose peer victimization.
Children &amp; Schools, 27(4):217–226.
Shelley, Danielle and Wendy M Craig. 2010. Attri-
butions and coping styles in reducing victimization.
Canadian Journal of School Psychology, 25(1):84–
100.
Twitter. 2012. The twitter rules. http:
//support.twitter.com/articles/
18311-the-twitter-rules.
</reference>
<page confidence="0.973618">
701
</page>
<reference confidence="0.999322095238095">
Valkenburg, Patti M and Jochen Peter. 2009. Social
consequences of the internet for adolescents a decade
of research. Current Directions in Psychological Sci-
ence, 18(1):1–5.
Walther, Joseph B. 1996. Computer-mediated commu-
nication impersonal, interpersonal, and hyperpersonal
interaction. Communication research, 23(1):3–43.
Wang, Yang, Gregory Norcie, Saranga Komanduri,
Alessandro Acquisti, Pedro Giovanni Leon, and Lor-
rie Faith Cranor. 2011. “I regretted the minute I
pressed share”: a qualitative study of regrets on face-
book. In Proceedings of the Seventh Symposium on
Usable Privacy and Security, SOUPS ’11, pages 10:1–
10:16. ACM.
Xu, Jun-Ming, Kwang-Sung Jun, Xiaojin Zhu, and Amy
Bellmore. 2012. Learning from bullying traces in so-
cial media. In Proceedings of the 2012 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 656–666, Montr´eal, Canada, June. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.997789">
702
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964039">
<title confidence="0.99995">An Examination of Regret in Bullying Tweets</title>
<author confidence="0.9964425">Amy Bellmore Jun-Ming Xu</author>
<author confidence="0.9964425">Benjamin Burchfiel</author>
<author confidence="0.9964425">Xiaojin Zhu</author>
<affiliation confidence="0.999887">Department of Computer University of</affiliation>
<address confidence="0.996768">Madison, WI 53706,</address>
<affiliation confidence="0.9998615">Department of Educational University of</affiliation>
<address confidence="0.997478">Madison, WI 53706,</address>
<email confidence="0.999686">abellmore@wisc.edu</email>
<abstract confidence="0.998067416666667">Social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. In this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Brendan OConnor</author>
<author>Noah Smith</author>
</authors>
<title>Censorship and deletion practices in chinese social media.</title>
<date>2012</date>
<journal>First Monday,</journal>
<pages>17--3</pages>
<contexts>
<context position="6263" citStr="Bamman, OConnor, and Smith, 2012" startWordPosition="997" endWordPosition="1001">such possible factors. 3.1 Word Usage Our dataset contains 331,070 distinct words and we are interested in isolating those with a significantly higher presence among either deleted or surviving tweets. We define the odds ratio of a word w P(w |deleted) r(w) = P(w |surviving), where P(w |deleted) is the probability of word w occurring in a deleted tweet, and P(w |surviving) is the probability of w appearing in a surviving tweet. In order to ensure stability in the probability estimation, we only considered words appearing at least 50 times in either the surviving or deleted corpora. Following (Bamman, OConnor, and Smith, 2012), we qualitatively analyzed words with extreme values of r(w), and found some interesting trends. There was a significant tendency for “joking” words to occur with r(w) &lt; 0.5; examples include “xd,” “haha,” and “hahaha.” Joking words occur less frequently in deleted tweets than surviving ones. On the other end of the spectrum, there were no joking words with r(w) &gt; 2. What we found instead were words such as “rip,” “fat,” “kill,” and “suicide.” While it is relatively clear that joking is less likely to occur in deleted tweets, there was less of a trend among words appearing more frequently in</context>
<context position="15408" citStr="Bamman et al. (2012)" startWordPosition="2536" endWordPosition="2539">ted feature representation achieved a cross validation accuracy of 0.606, with standard deviation 0.007; not statistically significantly different from the text-only feature representation. While it seems that a signal does exist, leveraging it usefully in real world scenarios may prove challenging due to the highly-skewed nature of the data. 5 Discussion There have been several recent works examining causes of deletion in social media. Wang et al. (2011) qualitatively investigated regret associated with users’ posts on social networking sites and identified several possible causes of regret. Bamman et al. (2012) focused on censorship-related deletion of social media posts, identifying a set of sensitive terms related to message deletion through a statistical analysis and spatial variation of deletion rate. Assuming that deletion in social media is indicative of regret, we studied regret in a bullying context by analyzing deletion trends in bullying related tweets. Through our analysis, we were able to isolate several factors related to deletion, including word usage, surviving time, and author role. We used these factors to build a regret predictor which achieved statistically significant results on </context>
</contexts>
<marker>Bamman, OConnor, Smith, 2012</marker>
<rawString>Bamman, David, Brendan OConnor, and Noah Smith. 2012. Censorship and deletion practices in chinese social media. First Monday, 17(3-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey T Child</author>
<author>Paul M Haridakis</author>
<author>Sandra Petronio</author>
</authors>
<title>Blogging privacy rule orientations, privacy management, and content deletion practices: The variability of online privacy management activity at different stages of social media use.</title>
<date>2012</date>
<journal>Computers in Human Behavior,</journal>
<volume>28</volume>
<issue>5</issue>
<pages>1872</pages>
<contexts>
<context position="2434" citStr="Child, Haridakis, and Petronio, 2012" startWordPosition="356" endWordPosition="360">ial relationships and increased well-being (Valkenburg and Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as revictimization, embarrassment, and social ostracization. Evidence exists that some individuals may react to these risks retroactively, by deleting their social media posts (Child et al., 2011; Christofides, Muise, and Desmarais, 2009). Several relevant motives have been found to be associated with deleting posted information, including conflict management, safety, fear of retribution, impression management, and emotional regulation (Child, Haridakis, and Petronio, 2012). Our previous work (Xu et al., 2012) demonstrates that social media can be a valuable data source when studying bullying, and proposes a text categorization method to recognize social media posts describing bullying episodes, bullying traces. To better understand, and possibly prevent, user regret after posting bullying related tweets, we collect bullying traces using the same method and perform regular status checks to determine if and when tweets become inaccessible. While a tweet becoming inaccessible does not guarantee it has been deleted, we attempt to leverage http response codes to ru</context>
</contexts>
<marker>Child, Haridakis, Petronio, 2012</marker>
<rawString>Child, Jeffrey T., Paul M. Haridakis, and Sandra Petronio. 2012. Blogging privacy rule orientations, privacy management, and content deletion practices: The variability of online privacy management activity at different stages of social media use. Computers in Human Behavior, 28(5):1859 – 1872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey T Child</author>
<author>Sandra Petronio</author>
<author>Esther A AgyemanBudu</author>
<author>David A Westermann</author>
</authors>
<title>Blog scrubbing: Exploring triggers that change privacy rules.</title>
<date>2011</date>
<journal>Computers in Human Behavior,</journal>
<volume>27</volume>
<issue>5</issue>
<contexts>
<context position="2152" citStr="Child et al., 2011" startWordPosition="318" endWordPosition="321"> to reach large audiences and because participants may feel less inhibition when sharing private information in an online setting (Walther, 1996). Furthermore, there is evidence that online communication stimulates selfdisclosure, which leads to higher quality social relationships and increased well-being (Valkenburg and Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as revictimization, embarrassment, and social ostracization. Evidence exists that some individuals may react to these risks retroactively, by deleting their social media posts (Child et al., 2011; Christofides, Muise, and Desmarais, 2009). Several relevant motives have been found to be associated with deleting posted information, including conflict management, safety, fear of retribution, impression management, and emotional regulation (Child, Haridakis, and Petronio, 2012). Our previous work (Xu et al., 2012) demonstrates that social media can be a valuable data source when studying bullying, and proposes a text categorization method to recognize social media posts describing bullying episodes, bullying traces. To better understand, and possibly prevent, user regret after posting bul</context>
</contexts>
<marker>Child, Petronio, AgyemanBudu, Westermann, 2011</marker>
<rawString>Child, Jeffrey T, Sandra Petronio, Esther A AgyemanBudu, and David A Westermann. 2011. Blog scrubbing: Exploring triggers that change privacy rules. Computers in Human Behavior, 27(5):2017–2027.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Christofides</author>
<author>Amy Muise</author>
<author>Serge Desmarais</author>
</authors>
<title>Information disclosure and control on facebook: are they two sides of the same coin or two different processes?</title>
<date>2009</date>
<journal>CyberPsychology &amp; Behavior,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>345</pages>
<contexts>
<context position="2194" citStr="Christofides, Muise, and Desmarais, 2009" startWordPosition="322" endWordPosition="326">ences and because participants may feel less inhibition when sharing private information in an online setting (Walther, 1996). Furthermore, there is evidence that online communication stimulates selfdisclosure, which leads to higher quality social relationships and increased well-being (Valkenburg and Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as revictimization, embarrassment, and social ostracization. Evidence exists that some individuals may react to these risks retroactively, by deleting their social media posts (Child et al., 2011; Christofides, Muise, and Desmarais, 2009). Several relevant motives have been found to be associated with deleting posted information, including conflict management, safety, fear of retribution, impression management, and emotional regulation (Child, Haridakis, and Petronio, 2012). Our previous work (Xu et al., 2012) demonstrates that social media can be a valuable data source when studying bullying, and proposes a text categorization method to recognize social media posts describing bullying episodes, bullying traces. To better understand, and possibly prevent, user regret after posting bullying related tweets, we collect bullying </context>
</contexts>
<marker>Christofides, Muise, Desmarais, 2009</marker>
<rawString>Christofides, Emily, Amy Muise, and Serge Desmarais. 2009. Information disclosure and control on facebook: are they two sides of the same coin or two different processes? CyberPsychology &amp; Behavior, 12(3):341– 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="13608" citStr="Fan et al., 2008" startWordPosition="2250" endWordPosition="2253">our deleted and surviving datasets to be of equal size. Consequentially, the baseline accuracy of the classifier is 0.5. While this does make the problem artificially easier, our initial goal was to test for the presence of a signal in the data. We then followed the preprocessing procedure in (Xu et al., 2012), performing case-folding, anonymization, and tokenization, treating URLs, emoticons and hashtags specially. We also chose the unigrams+bigrams feature representation, only keeping tokens appearing at least 15 times in the corpus. We chose to employ a linear SVM implemented in LIBLINEAR (Fan et al., 2008) due to its efficiency on this large sparse text categorization task and a 10-fold cross validation was conducted to eval700 uate its performance. Within the first fold, we use an inner 5-fold cross validation on the training portion to tune the regularization parameter on the grid {2−10, 2−9, ... ,1}; the selected parameter is then fixed for all the remaining folds. The resulting cross validation accuracy was 0.607 with a standard deviation of 0.012. While it is statistically significantly better than the random-guessing baseline accuracy of 0.5 with a p-value of 5.15 x 10−10, this accuracy i</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane R Jimerson</author>
<author>Susan M Swearer</author>
<author>Dorothy L Espelage</author>
</authors>
<date>2010</date>
<booktitle>Handbook of Bullying in Schools: An international perspective. Routledge/Taylor &amp;</booktitle>
<publisher>Francis Group,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="1068" citStr="Jimerson, Swearer, and Espelage, 2010" startWordPosition="149" endWordPosition="153">ntially causing them to delete their posts. In this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret. 1 Introduction A large body of literature suggests that participants in bullying events, including victims, bullies, and witnesses, are likely to report psychological adjustment problems (Jimerson, Swearer, and Espelage, 2010). One potential source of therapy for these issues can be self-disclosure of the experience to an adult or friend (Mishna and Alaggia, 2005); existing research suggests that victims who seek advice and help from others report less maladjustment than victims who do not (Shelley and Craig, 2010). Disclosure of bullying experiences through social media may be a particularly effective mechanism for participants seeking support because social media has the potential to reach large audiences and because participants may feel less inhibition when sharing private information in an online setting (Wal</context>
</contexts>
<marker>Jimerson, Swearer, Espelage, 2010</marker>
<rawString>Jimerson, Shane R., Susan M. Swearer, and Dorothy L. Espelage. 2010. Handbook of Bullying in Schools: An international perspective. Routledge/Taylor &amp; Francis Group, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faye Mishna</author>
<author>Ramona Alaggia</author>
</authors>
<title>Weighing the risks: A child’s decision to disclose peer victimization.</title>
<date>2005</date>
<journal>Children &amp; Schools,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1209" citStr="Mishna and Alaggia, 2005" startWordPosition="174" endWordPosition="177"> order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret. 1 Introduction A large body of literature suggests that participants in bullying events, including victims, bullies, and witnesses, are likely to report psychological adjustment problems (Jimerson, Swearer, and Espelage, 2010). One potential source of therapy for these issues can be self-disclosure of the experience to an adult or friend (Mishna and Alaggia, 2005); existing research suggests that victims who seek advice and help from others report less maladjustment than victims who do not (Shelley and Craig, 2010). Disclosure of bullying experiences through social media may be a particularly effective mechanism for participants seeking support because social media has the potential to reach large audiences and because participants may feel less inhibition when sharing private information in an online setting (Walther, 1996). Furthermore, there is evidence that online communication stimulates selfdisclosure, which leads to higher quality social relatio</context>
</contexts>
<marker>Mishna, Alaggia, 2005</marker>
<rawString>Mishna, Faye and Ramona Alaggia. 2005. Weighing the risks: A child’s decision to disclose peer victimization. Children &amp; Schools, 27(4):217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danielle Shelley</author>
<author>Wendy M Craig</author>
</authors>
<title>Attributions and coping styles in reducing victimization.</title>
<date>2010</date>
<journal>Canadian Journal of School Psychology,</journal>
<volume>25</volume>
<issue>1</issue>
<pages>100</pages>
<contexts>
<context position="1363" citStr="Shelley and Craig, 2010" startWordPosition="199" endWordPosition="202"> propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret. 1 Introduction A large body of literature suggests that participants in bullying events, including victims, bullies, and witnesses, are likely to report psychological adjustment problems (Jimerson, Swearer, and Espelage, 2010). One potential source of therapy for these issues can be self-disclosure of the experience to an adult or friend (Mishna and Alaggia, 2005); existing research suggests that victims who seek advice and help from others report less maladjustment than victims who do not (Shelley and Craig, 2010). Disclosure of bullying experiences through social media may be a particularly effective mechanism for participants seeking support because social media has the potential to reach large audiences and because participants may feel less inhibition when sharing private information in an online setting (Walther, 1996). Furthermore, there is evidence that online communication stimulates selfdisclosure, which leads to higher quality social relationships and increased well-being (Valkenburg and Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as rev</context>
</contexts>
<marker>Shelley, Craig, 2010</marker>
<rawString>Shelley, Danielle and Wendy M Craig. 2010. Attributions and coping styles in reducing victimization. Canadian Journal of School Psychology, 25(1):84– 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Twitter</author>
</authors>
<title>The twitter rules. http: //support.twitter.com/articles/ 18311-the-twitter-rules.</title>
<date>2012</date>
<contexts>
<context position="4749" citStr="Twitter, 2012" startWordPosition="744" endWordPosition="745">nse codes, which appear quite rarely, are treated as anomalies and ignored. All non HTTP 200 responses are retried twice to ensure they are not transient oddities. To determine when a tweet is deleted, we attempted to access each tweet at time points Ti = 5 x 4i minutes for i = 0,1... 7 after the creation time. These roughly correspond to periods of 5 minutes, 20 minutes, 1.5 hours, 6 hours, 1 day, 4 days, 2 weeks, and 2 months. While we assume that user deletion is the main cause of a tweet becoming unavailable, other causes are possible such as the censorship of illegal contents by Twitter (Twitter, 2012). Our sample data was collected from July 31 through October 31, 2012 and contains 522,984 bullying traces. Because of intermittent network and computer issues, several multiple day data gaps exist in the data. To combat this, we filter our data to include only tweets of unambiguous status. If any check within the 20480 minutes (about two weeks) interval returns an HTTP 404 code, the tweet is no longer accessible and we consider it deleted. If the 20480 minute or 81920 minute check returns an HTTP 200 response, that tweet is still accessible and we consider it surviving. The union of the survi</context>
</contexts>
<marker>Twitter, 2012</marker>
<rawString>Twitter. 2012. The twitter rules. http: //support.twitter.com/articles/ 18311-the-twitter-rules.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patti M Valkenburg</author>
<author>Jochen Peter</author>
</authors>
<title>Social consequences of the internet for adolescents a decade of research.</title>
<date>2009</date>
<journal>Current Directions in Psychological Science,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="1869" citStr="Valkenburg and Peter, 2009" startWordPosition="274" endWordPosition="277"> victims who seek advice and help from others report less maladjustment than victims who do not (Shelley and Craig, 2010). Disclosure of bullying experiences through social media may be a particularly effective mechanism for participants seeking support because social media has the potential to reach large audiences and because participants may feel less inhibition when sharing private information in an online setting (Walther, 1996). Furthermore, there is evidence that online communication stimulates selfdisclosure, which leads to higher quality social relationships and increased well-being (Valkenburg and Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as revictimization, embarrassment, and social ostracization. Evidence exists that some individuals may react to these risks retroactively, by deleting their social media posts (Child et al., 2011; Christofides, Muise, and Desmarais, 2009). Several relevant motives have been found to be associated with deleting posted information, including conflict management, safety, fear of retribution, impression management, and emotional regulation (Child, Haridakis, and Petronio, 2012). Our previous work (Xu et al., 20</context>
</contexts>
<marker>Valkenburg, Peter, 2009</marker>
<rawString>Valkenburg, Patti M and Jochen Peter. 2009. Social consequences of the internet for adolescents a decade of research. Current Directions in Psychological Science, 18(1):1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph B Walther</author>
</authors>
<title>Computer-mediated communication impersonal, interpersonal, and hyperpersonal interaction.</title>
<date>1996</date>
<journal>Communication research,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1679" citStr="Walther, 1996" startWordPosition="251" endWordPosition="252">010). One potential source of therapy for these issues can be self-disclosure of the experience to an adult or friend (Mishna and Alaggia, 2005); existing research suggests that victims who seek advice and help from others report less maladjustment than victims who do not (Shelley and Craig, 2010). Disclosure of bullying experiences through social media may be a particularly effective mechanism for participants seeking support because social media has the potential to reach large audiences and because participants may feel less inhibition when sharing private information in an online setting (Walther, 1996). Furthermore, there is evidence that online communication stimulates selfdisclosure, which leads to higher quality social relationships and increased well-being (Valkenburg and Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as revictimization, embarrassment, and social ostracization. Evidence exists that some individuals may react to these risks retroactively, by deleting their social media posts (Child et al., 2011; Christofides, Muise, and Desmarais, 2009). Several relevant motives have been found to be associated with deleting posted inf</context>
</contexts>
<marker>Walther, 1996</marker>
<rawString>Walther, Joseph B. 1996. Computer-mediated communication impersonal, interpersonal, and hyperpersonal interaction. Communication research, 23(1):3–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Wang</author>
<author>Gregory Norcie</author>
<author>Saranga Komanduri</author>
<author>Alessandro Acquisti</author>
<author>Pedro Giovanni Leon</author>
<author>Lorrie Faith Cranor</author>
</authors>
<title>I regretted the minute I pressed share”: a qualitative study of regrets on facebook.</title>
<date>2011</date>
<booktitle>In Proceedings of the Seventh Symposium on Usable Privacy and Security, SOUPS ’11,</booktitle>
<pages>10--1</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12552" citStr="Wang et al., 2011" startWordPosition="2077" endWordPosition="2080">able 2: Counts and deletion rate for teasing or not. test. It seems plausible that authors are less likely to regret teasing posts because they are less controversial and have less potential to generate negative audience reactions. This also corroborates our findings in word usage that joking words are less frequent in deleted tweets. 4 Predicting Regrettable Tweets Once a bullying tweet is published and seen by others, the ensuing effects are often impossible to undo. Since ill-thought-out posts may cause unexpectedly negative consequences to an author’s reputation, relationship, and career (Wang et al., 2011), it would be helpful if a system could warn users before a potentially regrettable tweet is posted. One straightforward approach is to formulate the task as a binary text categorization problem. We use the cleaned dataset, in which each tweet is known to be surviving or deleted after 20480 minutes (about two weeks). Since this dataset contains 22,241 deleted tweets, we randomly sub-sampled the surviving tweets down to 22,241 to force our deleted and surviving datasets to be of equal size. Consequentially, the baseline accuracy of the classifier is 0.5. While this does make the problem artific</context>
<context position="15247" citStr="Wang et al. (2011)" startWordPosition="2513" endWordPosition="2516">sed the teasing and author role classifiers in (Xu et al., 2012), and appended the predicted teasing, and author role labels to our feature vector. This augmented feature representation achieved a cross validation accuracy of 0.606, with standard deviation 0.007; not statistically significantly different from the text-only feature representation. While it seems that a signal does exist, leveraging it usefully in real world scenarios may prove challenging due to the highly-skewed nature of the data. 5 Discussion There have been several recent works examining causes of deletion in social media. Wang et al. (2011) qualitatively investigated regret associated with users’ posts on social networking sites and identified several possible causes of regret. Bamman et al. (2012) focused on censorship-related deletion of social media posts, identifying a set of sensitive terms related to message deletion through a statistical analysis and spatial variation of deletion rate. Assuming that deletion in social media is indicative of regret, we studied regret in a bullying context by analyzing deletion trends in bullying related tweets. Through our analysis, we were able to isolate several factors related to deleti</context>
</contexts>
<marker>Wang, Norcie, Komanduri, Acquisti, Leon, Cranor, 2011</marker>
<rawString>Wang, Yang, Gregory Norcie, Saranga Komanduri, Alessandro Acquisti, Pedro Giovanni Leon, and Lorrie Faith Cranor. 2011. “I regretted the minute I pressed share”: a qualitative study of regrets on facebook. In Proceedings of the Seventh Symposium on Usable Privacy and Security, SOUPS ’11, pages 10:1– 10:16. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ming Xu</author>
<author>Kwang-Sung Jun</author>
<author>Xiaojin Zhu</author>
<author>Amy Bellmore</author>
</authors>
<title>Learning from bullying traces in social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>656--666</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2472" citStr="Xu et al., 2012" startWordPosition="364" endWordPosition="367"> Peter, 2009). Online disclosure may also present risks for those involved in bullying however, such as revictimization, embarrassment, and social ostracization. Evidence exists that some individuals may react to these risks retroactively, by deleting their social media posts (Child et al., 2011; Christofides, Muise, and Desmarais, 2009). Several relevant motives have been found to be associated with deleting posted information, including conflict management, safety, fear of retribution, impression management, and emotional regulation (Child, Haridakis, and Petronio, 2012). Our previous work (Xu et al., 2012) demonstrates that social media can be a valuable data source when studying bullying, and proposes a text categorization method to recognize social media posts describing bullying episodes, bullying traces. To better understand, and possibly prevent, user regret after posting bullying related tweets, we collect bullying traces using the same method and perform regular status checks to determine if and when tweets become inaccessible. While a tweet becoming inaccessible does not guarantee it has been deleted, we attempt to leverage http response codes to rule out other common causes of inaccess</context>
<context position="10689" citStr="Xu et al., 2012" startWordPosition="1774" endWordPosition="1777"> see if a state’s deletion rate is significantly different from the national average. We chose the significance level at 0.05 and used Bonferroni correction for multiple testing. Only four states have significantly different deletion rates from the average: Arizona (6.3%, p = 5.9×10−5), California (5.2%, p = 2.7×10−7), Maryland (1.9%, p = 2.3 × 10−5), and Oklahoma (7.1%, p = 3.5 × 10−5). 3.4 Author’s Role Participants in a bullying episode assume welldefined roles which dramatically affect the viewpoint of the author describing the event. We trained a text classifier to determine author role (Xu et al., 2012), and used it to label each bullying trace in the cleaned corpus by author role: Accuser, Bully, Reporter, Victim or Other. Table 1 shows that compared to tweets produced by bullies, victims create more bullying traces, possibly due to an increased need for social support on the part of the victim. More importantly, P(deleted | victim) is higher than P(deleted |bully), a statistically significant difference in a two-proportion ztest. Possibly, victims are more sensitive to their audience’s reaction than bullies. 3.5 Teasing Many bullying traces are written jokingly. We built a text classifier </context>
<context position="13302" citStr="Xu et al., 2012" startWordPosition="2206" endWordPosition="2209">o formulate the task as a binary text categorization problem. We use the cleaned dataset, in which each tweet is known to be surviving or deleted after 20480 minutes (about two weeks). Since this dataset contains 22,241 deleted tweets, we randomly sub-sampled the surviving tweets down to 22,241 to force our deleted and surviving datasets to be of equal size. Consequentially, the baseline accuracy of the classifier is 0.5. While this does make the problem artificially easier, our initial goal was to test for the presence of a signal in the data. We then followed the preprocessing procedure in (Xu et al., 2012), performing case-folding, anonymization, and tokenization, treating URLs, emoticons and hashtags specially. We also chose the unigrams+bigrams feature representation, only keeping tokens appearing at least 15 times in the corpus. We chose to employ a linear SVM implemented in LIBLINEAR (Fan et al., 2008) due to its efficiency on this large sparse text categorization task and a 10-fold cross validation was conducted to eval700 uate its performance. Within the first fold, we use an inner 5-fold cross validation on the training portion to tune the regularization parameter on the grid {2−10, 2−9,</context>
<context position="14693" citStr="Xu et al., 2012" startWordPosition="2427" endWordPosition="2430">tatistically significantly better than the random-guessing baseline accuracy of 0.5 with a p-value of 5.15 x 10−10, this accuracy is nevertheless too low to be useful in a practical system. One possibility is that the tweet text contains very limited information for predicting inaccessibility; a user’s decision to delete a tweet potentially depends on many other factors, such as the conversation context and the characteristics of the author and audience. In the spirit of exploring additional informative features for deletion prediction, we also used the teasing and author role classifiers in (Xu et al., 2012), and appended the predicted teasing, and author role labels to our feature vector. This augmented feature representation achieved a cross validation accuracy of 0.606, with standard deviation 0.007; not statistically significantly different from the text-only feature representation. While it seems that a signal does exist, leveraging it usefully in real world scenarios may prove challenging due to the highly-skewed nature of the data. 5 Discussion There have been several recent works examining causes of deletion in social media. Wang et al. (2011) qualitatively investigated regret associated </context>
</contexts>
<marker>Xu, Jun, Zhu, Bellmore, 2012</marker>
<rawString>Xu, Jun-Ming, Kwang-Sung Jun, Xiaojin Zhu, and Amy Bellmore. 2012. Learning from bullying traces in social media. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 656–666, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>