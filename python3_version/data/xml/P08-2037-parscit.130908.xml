<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001843">
<title confidence="0.97234">
Event Matching Using the Transitive Closure of Dependency Relations
</title>
<note confidence="0.5418145">
Daniel M. Bikel and Vittorio Castelli
IBM T. J. Watson Research Center
</note>
<address confidence="0.8081945">
1101 Kitchawan Road
Yorktown Heights, NY 10598
</address>
<email confidence="0.992038">
{dbikel,vittorio}@us.ibm.com
</email>
<sectionHeader confidence="0.993697" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996546">
This paper describes a novel event-matching
strategy using features obtained from the tran-
sitive closure of dependency relations. The
method yields a model capable of matching
events with an F-measure of 66.5%.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995825">
Question answering systems are evolving from their
roots as factoid or definitional answering systems
to systems capable of answering much more open-
ended questions. For example, it is one thing to ask
for the birthplace of a person, but it is quite another
to ask for all locations visited by a person over a
specific period of time.
Queries may contain several types of arguments:
person, organization, country, location, etc. By far,
however, the most challenging of the argument types
are the event or topic arguments, where the argument
text can be a noun phrase, a participial verb phrase
or an entire indicative clause. For example, the fol-
lowing are all possible event arguments:
</bodyText>
<listItem confidence="0.9996595">
• the U.S. invasion of Iraq
• Red Cross admitting Israeli and Palestinian
groups
• GM offers buyouts to union employees
</listItem>
<bodyText confidence="0.9992814">
In this paper, we describe a method to match
an event query argument to the sentences that
mention that event. That is, we seek to model
p(s contains e  |s, e), where e is a textual description
of an event (such as an event argument for a GALE
distillation query) and where s is an arbitrary sen-
tence. In the first example above, “the U.S. inva-
sion of Iraq”, such a model should produce a very
high score for that event description and the sentence
“The U.S. invaded Iraq in 2003.”
</bodyText>
<sectionHeader confidence="0.996757" genericHeader="method">
2 Low-level features
</sectionHeader>
<bodyText confidence="0.999899375">
As the foregoing implies, we are interested in train-
ing a binary classifier, and so we represent each
training and test instance in a feature space. Con-
ceptually, our features are of three different varieties.
This section describes the first two kinds, which we
call “low-level” features, in that they attempt to cap-
ture how much of the basic information of an event
e is present in a sentence s.
</bodyText>
<subsectionHeader confidence="0.987941">
2.1 Lexical features
</subsectionHeader>
<bodyText confidence="0.981486153846154">
We employ several types of simple lexical-matching
features. These are similar to the “bag-of-
words” features common to many IR and question-
answering systems. Specifically, we compute the
value overlap( s, e = ws·we,
&apos;) Iwe|1 where we (resp: ws) is
the {0,1}-valued word-feature vector for the event
(resp: sentence). This value is simply the fraction
of distinct words in e that are present in s. We then
quantize this fraction into the bins [0, 0], (0, 0.33],
(0.33, 0.66], (0.66, 0.99], (0.99, 1], to produce one
of five, binary-valued features to indicate whether
none, few, some, many or all of the words match.1
</bodyText>
<subsectionHeader confidence="0.999996">
2.2 Argument analysis and submodels
</subsectionHeader>
<bodyText confidence="0.999963625">
Since an event or topic most often involves entities
of various kinds, we need a method to recognize
those entity mentions. For example, in the event
“Abdul Halim Khaddam resigns as Vice President
of Syria”, we have a PERSoN mention, an occuPA-
TioN mention and a GPE (geopolitical entity) mention.
We use an information extraction toolkit (Florian
et al., 2004) to analyze each event argument. The
toolkit performs the following steps: tokenization,
part-of-speech tagging, parsing, mention detection,
within-document coreference resolution and cross-
document coreference resolution. We also apply the
toolkit to our entire search corpus.
After determining the entities in an event descrip-
tion, we rely on lower-level binary classifiers, each
of which has been trained to match a specific type
</bodyText>
<footnote confidence="0.995316333333333">
1Other binnings did not significantly alter the performance
of the models we trained, and so we used the above binning
strategy for all experiments reported in this paper.
</footnote>
<page confidence="0.960843">
145
</page>
<reference confidence="0.218254">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 145–148,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999909">
of entity. For example, we use a PERSON-matching
model to determine if, say, “Abdul Halim Khad-
dam” from an event description is mentioned in a
sentence.2 We build binary-valued feature functions
from the output of our four lower-level classifiers.
</bodyText>
<sectionHeader confidence="0.930318" genericHeader="method">
3 Dependency relation features
</sectionHeader>
<bodyText confidence="0.999974454545454">
Employing syntactic or dependency relations to aid
question answering systems is by no means new (At-
tardi et al., 2001; Cui et al., 2005; Shen and Klakow,
2006). These approaches all involved various de-
grees of loose matching of the relations in a query
relative to sentences. More recently, Wang et al.
(2007) explored the use a formalism called quasi-
synchronous grammar (Smith and Eisner, 2006) in
order to find a more explicit model for matching the
set of dependencies, and yet still allow for looseness
in the matching.
</bodyText>
<subsectionHeader confidence="0.999284">
3.1 The dependency relation
</subsectionHeader>
<bodyText confidence="0.995735583333333">
In contrast to previous work using relations, we do
not seek to model explicitly a process that trans-
forms one dependency tree to another, nor do we
seek to come up with ad hoc correlation measures
or path similarity measures. Rather, we propose to
use features based on the transitive closure of the
dependency relation of the event and that of the de-
pendency relation of the sentence. Our aim was to
achieve a balance between the specificity of depen-
dency paths and the generality of dependency pairs.
In its most basic form, a dependency tree for
a sentence w = (to1, tow, ... , tok) is a rooted tree
</bodyText>
<equation confidence="0.9185775">
T = (V, E, r), where V = {1, ... , k}, E =
{ }
</equation>
<bodyText confidence="0.940916714285714">
(i, j) : toi is the child of toj and r E {1, ... , k} :
tor is the root word. Each element toi of our word
sequence, rather than being a simple lexical item
drawn from a finite vocabulary, will be a complex
structure. With each word wi we associate a part-
of-speech tag ti, a morph (or stem) mi (which is wi
itself if wi has no variant), a set of nonterminal labels
Ni, a set of synonyms S i for that word and a canon-
ical mention cm(i). Formally, we let each sequence
element be a sextuple toi = (wi, ti, mi, Ni, Si, cm(i)).
2This is not as trivial as it might sound: the model must deal
with name variants (parts of names, alternate spellings, nick-
names) and with metonymic uses of titles (“Mr. President” re-
ferring to Bill Clinton or George W. Bush).
</bodyText>
<figure confidence="0.703774">
S(ate)
NP(Cathy)
Cathy
</figure>
<figureCaption confidence="0.999092">
Figure 1: Simple lexicalized tree.
</figureCaption>
<bodyText confidence="0.993640111111111">
We derive dependency trees from head-
lexicalized syntactic parse trees. The set of
nonterminal labels associated with each word is the
set of labels of the nodes for which that word was
the head. For example, in the lexicalized tree in
Figure 1, the head word “ate” would be associated
with both the nonterminals S and VP. Also, if a
head word is part of an entity mention, then the
“canonical” version of that mention is associated
with the word, where canonical essentially means
the best version of that mention in its coreference
chain (produced by our information extraction
toolkit), denoted cm(i). In Figure 1, the first word
w1 = Cathy would probably be recognized as a
PERSON mention, and if the coreference resolver
found it to be coreferent with a mention earlier
in the same document, say, Cathy Smith, then
cm(1) = Cathy Smith.
</bodyText>
<subsectionHeader confidence="0.999291">
3.2 Matching on the transitive closure
</subsectionHeader>
<bodyText confidence="0.999872916666667">
Since E represents the child-of dependency relation,
let us now consider the transitive closure, E&apos;, which
is then the descendant-of relation.3 Our features are
computed by examining the overlap between E&apos;e and
E&apos;s, the descendant-of relation of the event descrip-
tion e and the sentence s, respectively. We use the
following, two-tiered strategy.
Let de, ds be elements of E&apos;e and E&apos;s, with dx.d de-
noting the index of the word that is the descendant
in dx and dx.a denoting the ancestor. We define the
following matching function to match the pair of de-
scendants (or ancestors):
</bodyText>
<equation confidence="0.9988135">
matchd(de, ds) = (1)
(mde.d = mds.d) v (cm(de.d) = cm(ds.d))
</equation>
<bodyText confidence="0.9996924">
where matcha is defined analogously for ancestors.
That is, matchd(de, ds) returns true if the morph of
the descendant of de is the same as the morph of
the descendant of ds, or if both descendants have
canonical mentions with an exact string match; the
</bodyText>
<footnote confidence="0.924776">
3We remove all edges (i, j) from E&apos; where either wi or wj is
a stop word.
</footnote>
<equation confidence="0.6288585">
VP(ate)
ate
</equation>
<page confidence="0.994759">
146
</page>
<bodyText confidence="0.999723047619048">
function returns false otherwise, and matcha is de-
fined analogously for the pair of ancestors. Thus,
the pair of functions matchd, matcha are “morph or
mention” matchers. We can now define our main
matching function in terms of matchd and matcha:
match(de, ds) = matchd(de, ds) ∧ matcha(de, ds).
(2)
Informally, match(de, ds) returns true if the pair
of descendants have a “morph-or-mention” match
and if the pair of ancestors have a “morph-or-
mention” match. When match(de, ds) = true, we
use “morph-or-mention” matching features.
If match(de, ds) = false we then attempt to per-
form matching based on synonyms of the words in-
volved in the two dependencies (the “second tier” of
our two-tiered strategy). Recall that Sde.d is the set
of synonyms for the word at index de.d. Since we
do not perform word sense disambiguation, Sde.d is
the union of all possible synsets for wde.d. We then
define the following function for determining if two
dependency pairs match at the synonym level:
</bodyText>
<equation confidence="0.984559">
synmatch(de, ds) = (3)
(Sde.d n Sds.d # 0) ∧ (Sde.a n Sds.a # 0) .
</equation>
<bodyText confidence="0.9997755">
This function returns true iff the pair of descen-
dants share at least one synonym and the pair of an-
cestors share at least one synonym. If there is a syn-
onym match, we use synonym-matching features.
</bodyText>
<subsectionHeader confidence="0.995271">
3.3 Dependency matching features
</subsectionHeader>
<bodyText confidence="0.989259555555556">
The same sorts of features are produced whether
there is a “morph-or-mention” match or a synonym
match; however, we still distinguish the two types
of features, so that the model may learn different
weights according to what type of matching hap-
pened. The two matching situations each produce
four types of features. Figure 2 shows these four
types of features using the event of “Abdul Halim
Khaddam resigns as Vice President of Syria” and the
sentence “The resignation of Khaddam was abrupt”
as an example. In particular, the “depth” features at-
tempt to capture the “importance” the dependency
match, as measured by the depth of the ancestor in
the event dependency tree.
We have one additional type of feature: we com-
pute the following kernel function on the two sets of
dependencies E&apos;e and E&apos;s and create features based on
quantizing the value:
</bodyText>
<equation confidence="0.999320333333333">
K(E&apos;e, E&apos;s) = (4)
(A(de) - A(ds))-1,
(de,ds)EE&apos;eXE&apos;s : match(de,ds)
</equation>
<bodyText confidence="0.677494">
A((i, j)) being the path distance in τ from node i to j.
</bodyText>
<sectionHeader confidence="0.902108" genericHeader="method">
4 Data and experiments
</sectionHeader>
<bodyText confidence="0.999935666666667">
We created 159 queries to test this model frame-
work. We adapted a publicly-available search en-
gine (citation omitted) to retrieve documents au-
tomatically from the GALE corpus likely to be
relevant to the event queries, and then used a
set of simple heuristics—a subset of the low-
level features described in §2—to retrieve sen-
tences that were more likely than not to be rel-
evant. We then had our most experienced an-
notator annotate sentences with five possible tags:
relevant, irrelevant, relevant-in-context,
irrelevant-in-context and garbage (to deal
with sentences that were unintelligible “word
salad”).4 Crucially, the annotation guidelines for
this task were that an event had to be explicitly men-
tioned in a sentence in order for that sentence to be
tagged relevant.
We separated the data roughly into an 80/10/10
split for training, devtest and test. We then trained
our event-matching model solely on the examples
marked relevant or irrelevant, of which there
were 3546 instances. For all the experiments re-
ported, we tested on our development test set, which
comprised 465 instances that had been marked
relevant or irrelevant.
We trained the kernel version of an averaged per-
ceptron model (Freund and Schapire, 1999), using a
polynomial kernel with degree 4 and additive term 1.
As a baseline, we trained and tested a model using
only the lexical-matching features. We then trained
and tested models using only the low-level features
and all features. Figure 3 shows the performance
statistics of all three models, and Figure 4 shows the
ROC curves of these models. Clearly, the depen-
dency features help; at our normal operating point of
0, F-measure rises from 62.2 to 66.5. Looking solely
</bodyText>
<footnote confidence="0.960843">
4The *-in-context tags were to be able to re-use the data
for an upstream system capable of handling the GALE distilla-
tion query type “list facts about [event]”.
</footnote>
<page confidence="0.983427">
147
</page>
<table confidence="0.939719833333333">
Feature type Example Comment
Morph bigram x-resign-Khaddam Sparse, but helpful.
Tag bigram x-VBZ-NNP
Nonterminal x-VP-NP All pairs from Ni x Nj for (i, j) E E&apos; e.
Depth x-eventArgHeadDepth=m Depth is 0 because “resigns” is root of event.
Figure 2: Types of dependency features. Example features are for e = ”Abdul Halim Khaddam resigns as Vice
President of Syria” and s = ”The resignation of Khaddam was abrupt.” In example features, x E fm, s}, depending on
whether the dependency match was due to “morph-or-mention” matching or synonym matching.
Model R P F
lex 36.6 76.3 49.5
low-level 63.9 60.5 62.2
all 69.1 64.1 66.5
</table>
<figureCaption confidence="0.991167">
Figure 3: Performance of models.
</figureCaption>
<figure confidence="0.937709">
0 0.2 0.4 0.6 0.8 1
False positive rate
</figure>
<figureCaption confidence="0.9999225">
Figure 4: ROC curves of model with only low-level fea-
tures vs. model with all features.
</figureCaption>
<bodyText confidence="0.97315">
at pairs of predictions, McNemar’s test reveals dif-
ferences (p &lt;&lt; 0.05) between the predictions of the
baseline model and the other two models, but not
between those of the low-level model and the model
trained with all features.
</bodyText>
<sectionHeader confidence="0.999759" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999984">
There have been several efforts to incorporate de-
pendency information into a question-answering
system. These have attempted to define either ad
hoc similarity measures or a tree transformation pro-
cess, whose parameters must be learned. By using
the transitive closure of the dependency relation, we
believe that—especially in the face of a small data
set—we have struck a balance between the represen-
tative power of dependencies and the need to remain
agnostic with respect to similarity measures or for-
malisms; we merely let the features speak for them-
selves and have the training procedure of a robust
classifier learn the appropriate weights.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999012">
This work supported by DARPA grant HR0011-06-
02-0001. Special thanks to Radu Florian and Jeffrey
Sorensen for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926142857143">
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, Alessandro Tommasi, Ellen M.
Voorhees, and D. K. Harman. 2001. Selectively using
relations to improve precision in question answering.
In TREC-10, Gaithersburg, Maryland.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In SIGIR 2005,
Salvador, Brazil, August.
Radu Florian, Hani Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicholas Nicolov, and Salim Roukos. 2004. A statis-
tical model for multilingual entity detection and track-
ing. In HLT-NAACL 2004, pages 1–8.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277–296.
Dan Shen and Dietrich Klakow. 2006. Exploring corre-
lation of dependency relation paths for answer extrac-
tion. In COLING-ACL 2006, Sydney, Australia.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL Workshop
on Statistical Machine Translation, pages 23–30.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP-CoNLL
2007, pages 22–32.
</reference>
<figure confidence="0.969741">
True positive rate
0.8
0.6
0.4
0.2
0
1
all features
low-level features
lexical features
</figure>
<page confidence="0.93853">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819178">
<title confidence="0.999746">Event Matching Using the Transitive Closure of Dependency Relations</title>
<author confidence="0.999997">Daniel M Bikel</author>
<author confidence="0.999997">Vittorio Castelli</author>
<affiliation confidence="0.999965">IBM T. J. Watson Research Center</affiliation>
<address confidence="0.9907505">1101 Kitchawan Road Yorktown Heights, NY 10598</address>
<email confidence="0.999589">dbikel@us.ibm.com</email>
<email confidence="0.999589">vittorio@us.ibm.com</email>
<abstract confidence="0.972251666666667">This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations. The method yields a model capable of matching events with an F-measure of 66.5%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>145--148</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 145–148,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Antonio Cisternino</author>
<author>Francesco Formica</author>
<author>Maria Simi</author>
<author>Alessandro Tommasi</author>
<author>Ellen M Voorhees</author>
<author>D K Harman</author>
</authors>
<title>Selectively using relations to improve precision in question answering.</title>
<date>2001</date>
<booktitle>In TREC-10,</booktitle>
<location>Gaithersburg, Maryland.</location>
<marker>Attardi, Cisternino, Formica, Simi, Tommasi, Voorhees, Harman, 2001</marker>
<rawString>Giuseppe Attardi, Antonio Cisternino, Francesco Formica, Maria Simi, Alessandro Tommasi, Ellen M. Voorhees, and D. K. Harman. 2001. Selectively using relations to improve precision in question answering. In TREC-10, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>TatSeng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In SIGIR 2005,</booktitle>
<location>Salvador, Brazil,</location>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and TatSeng Chua. 2005. Question answering passage retrieval using dependency relations. In SIGIR 2005, Salvador, Brazil, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hani Hassan</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaoqiang Luo</author>
<author>Nicholas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking. In HLT-NAACL</title>
<date>2004</date>
<pages>1--8</pages>
<contexts>
<context position="3191" citStr="Florian et al., 2004" startWordPosition="531" endWordPosition="534">n s. We then quantize this fraction into the bins [0, 0], (0, 0.33], (0.33, 0.66], (0.66, 0.99], (0.99, 1], to produce one of five, binary-valued features to indicate whether none, few, some, many or all of the words match.1 2.2 Argument analysis and submodels Since an event or topic most often involves entities of various kinds, we need a method to recognize those entity mentions. For example, in the event “Abdul Halim Khaddam resigns as Vice President of Syria”, we have a PERSoN mention, an occuPATioN mention and a GPE (geopolitical entity) mention. We use an information extraction toolkit (Florian et al., 2004) to analyze each event argument. The toolkit performs the following steps: tokenization, part-of-speech tagging, parsing, mention detection, within-document coreference resolution and crossdocument coreference resolution. We also apply the toolkit to our entire search corpus. After determining the entities in an event description, we rely on lower-level binary classifiers, each of which has been trained to match a specific type 1Other binnings did not significantly alter the performance of the models we trained, and so we used the above binning strategy for all experiments reported in this pap</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hani Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicholas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In HLT-NAACL 2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Dietrich Klakow</author>
</authors>
<title>Exploring correlation of dependency relation paths for answer extraction.</title>
<date>2006</date>
<booktitle>In COLING-ACL 2006,</booktitle>
<location>Sydney, Australia.</location>
<marker>Shen, Klakow, 2006</marker>
<rawString>Dan Shen and Dietrich Klakow. 2006. Exploring correlation of dependency relation paths for answer extraction. In COLING-ACL 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies. In HLT-NAACL Workshop on Statistical Machine Translation, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasisynchronous grammar for QA. In EMNLP-CoNLL</title>
<date>2007</date>
<pages>22--32</pages>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? a quasisynchronous grammar for QA. In EMNLP-CoNLL 2007, pages 22–32.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>