<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<note confidence="0.824663">
Proceedings of EACL &apos;99
</note>
<title confidence="0.947798">
An Efficient Method for Determining Bilingual Word Classes
</title>
<author confidence="0.962042">
Franz Josef Och
</author>
<affiliation confidence="0.9282085">
Lehrstuhl für Informatik VI
RWTH Aachen - University of Technology
</affiliation>
<address confidence="0.740207333333333">
Ahornstralk 55
52056 Aachen
GERMANY
</address>
<email confidence="0.557585">
ochainformatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.99182" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999660333333333">
In statistical natural language process-
ing we always face the problem of sparse
data. One way to reduce this problem is
to group words into equivalence classes
which is a standard method in statistical
language modeling. In this paper we de-
scribe a method to determine bilingual
word classes suitable for statistical ma-
chine translation. We develop an opti-
mization criterion based on a maximum-
likelihood approach and describe a clus-
tering algorithm. We will show that the
usage of the bilingual word classes we get
can improve statistical machine transla-
tion.
</bodyText>
<sectionHeader confidence="0.998747" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999326321428571">
Word classes are often used in language modelling
to solve the problem of sparse data. Various clus-
tering techniques have been proposed (Brown et
al., 1992; Jardino and Adda, 1993; Martin et al.,
1998) which perform automatic word clustering
optimizing a maximum-likelihood criterion with
iterative clustering algorithms.
In the field of statistical machine translation
we also face the problem of sparse data. Our
aim is to use word classes in statistical machine
translation to allow for more robust statistical
translation models. A naive approach for doing
this would be the use of mono-lingually optimized
word classes in source and target language. Un-
fortunately we can not expect these independently
optimized classes to be correspondent. There-
fore mono-lingually optimized word classes do not
seem to be useful for machine translation (see also
(Fung and Wu, 1995)). We define bilingual word
clustering as the process of forming correspond-
ing word classes suitable for machine translation
purposes for a pair of languages using a parallel
training corpus.
The described method to determine bilingual
word classes is an extension and improvement
of the method mentioned in (Och and Weber,
1998). Our approach is simpler and computation-
ally more efficient than (Wang et al., 1996).
</bodyText>
<sectionHeader confidence="0.990979" genericHeader="method">
2 Monolingual Word Clustering
</sectionHeader>
<bodyText confidence="0.998274357142857">
The task of a statistical language model is to es-
timate the probability Pr(wiev) of a sequence of
words wiv = wi &apos;WN. A simple approximation
of Pr(41) is to model it as a product of bigram
probabilities: Pr (wPI) = HiN_, p(wi *2_1). If we
want to estimate the bigram probabilities p(wlw&apos;)
using a realistic natural language corpus we are
faced with the problem that most of the bigrams
are rarely seen. One possibility to solve this prob-
lem is to partition the set of all words into equiv-
alence classes. The function C maps words w to
their classes C(w). Rewriting the corpus probabil-
ity using classes we arrive at the following proba-
bility model p(wiv IC):
</bodyText>
<equation confidence="0.997539666666667">
p(WIC) := Hp(c(wor(wi_i))-p(wircwo)
i=1
(1)
</equation>
<bodyText confidence="0.9895946">
In this model we have two types of probabili-
ties: the transition probability p(C1C1) for class
C given its predecessor class C&apos; and the member-
ship probability p(wIC) for word w given class C.
To determine the optimal classes C for a given
number of classes M we perform a maximum-
likelihood approach:
= arg mrc p(wiv IC) (2)
We estimate the probabilities of Eq. (1) by
relative frequencies: p(CIC&amp;quot;) := n(C1C1&apos;)In(C&apos;),
p(wIC) = n(w)In(C). The function n(-) provides
the frequency of a uni- or bigram in the training
corpus. If we insert this into Eq. (2) and apply
the negative logarithm and change the summa-
tion order we arrive at the following optimization
</bodyText>
<page confidence="0.977544">
71
</page>
<bodyText confidence="0.4662815">
Proceedings of EACL &apos;99
criterion LP,. (Kneser and Ney, 1991):
</bodyText>
<equation confidence="0.94092225">
LPi(C,n) = — E h(n(CICT
+2 E h(n(C)) (3)
c
= arg min LPI(C, n). (4)
</equation>
<bodyText confidence="0.997993727272727">
The function h(n) is a shortcut for n • log(n).
It is necessary to fix the number of classes in
C in advance as the optimum is reached if every
word is a class of its own. Because of this it is
necessary to perform an additional optimization
process which determines the number of classes.
The use of leaving-one-out in a modified optimiza-
tion criterion as in (Kneser and Ney, 1993) could
in principle solve this problem.
An efficient optimization algorithm for LPI is
described in section 4.
</bodyText>
<sectionHeader confidence="0.979689" genericHeader="method">
3 Bilingual Word Clustering
</sectionHeader>
<bodyText confidence="0.999946666666667">
In bilingual word clustering we are interested in
classes F and E which form partitions of the vo-
cabulary of two languages. To perform bilingual
word clustering we use a maximum-likelihood ap-
proach as in the monolingual case. We maximize
the joint probability of a bilingual training corpus
</bodyText>
<equation confidence="0.9881402">
(ef
(61, .fr) = arg max p(ef , .F) (5)
E,Y
arg max p(el lE) • p(felef; E, .F)(6)
e,Y
</equation>
<bodyText confidence="0.9676337">
To perform the maximization of Eq. (6) we have to
model the monolingual a priori probability p(ef1E)
and the translation probability p(f lel; e, F). For
the first we use the class-based bigram probability
from Eq. (1).
To model p(fillef; E, .7) we assume the exis-
tence of an alignment af. We assume that ev-
ery word fj is produced by the word ea, at posi-
tion a3 in the training corpus with the probability
P(filea,):
</bodyText>
<equation confidence="0.9890335">
P(fli rip(f,lea,) (7)
j.1
</equation>
<bodyText confidence="0.9995285">
The word alignment ail is trained automatically
using statistical translation models as described in
(Brown et al., 1993; Vogel et al., 1996). The idea
is to introduce the unknown alignment a as hid-
den variable into a statistical model of the trans-
lation probability p(glef). By applying the EM-
algorithm we obtain the model parameters. The
alignment cif that we use is the Viterbi-Alignment
of an HMM alignment model similar to (Vogel et
al., 1996).
By rewriting the translation probability using
word classes, we obtain (corresponding to Eq. (1)):
</bodyText>
<equation confidence="0.611608">
PC f E,7) =11P(T(fi)le(ea;)).P(.61.T(.6))
</equation>
<bodyText confidence="0.89026025">
(8)
The variables F and E denote special classes in
and E. We use relative frequencies to estimate
p(FIE) and p(fIF):
</bodyText>
<equation confidence="0.966585">
nt(FIE)/ (E nt(FIE))
nt(f)/ (E nt(fIF))
nt (f)/ (E nt (F1E))
</equation>
<bodyText confidence="0.9992555">
The function nt(FIE) counts how often the words
in class F are aligned to words in class E. If we
insert these relative frequencies into Eq. (8) and
apply the same transformations as in the monolin-
gual case we obtain a similar optimization crite-
rion for the translation probability part of Eq. (6).
Thus the full optimization criterion for bilingual
word classes is:
</bodyText>
<equation confidence="0.99927625">
_ E h(n(E1.E1)) — E h(nt (F1E))
+2 E h(n(E))
E NE nt(FIE)) + E NE nt(FIE))
F E E F
</equation>
<bodyText confidence="0.997469666666667">
The two count functions n(E1E) and nt(FIE) can
be combined into one count function ng (X1Y)
n(X1Y) + nt (X1Y) as for all words f and all words
e and e&apos; holds n(f le) = 0 and nt(ele1) = 0. Using
the function n9 we arrive at the following opti-
mization criterion:
</bodyText>
<equation confidence="0.820777333333333">
LP2((E,F), ng) = — E h(n9(XIX&apos;))+
E h(n,,, (x)) E h(ng,2(X)) (9)
= argrnin LP2((E, .F), ng) (10)
</equation>
<bodyText confidence="0.992826">
Here we defined ng,i (X) = Ex, ng (XIX&apos;) and
n9,2(X) = Ex, n9(X11X). The variable X runs
over the classes in £ and F. In the optimiza-
tion process it cannot be allowed that words of
</bodyText>
<page confidence="0.995795">
72
</page>
<table confidence="0.940377571428571">
Proceedings of EACL &apos;99
INPUT: Parallel corpus (ef , f?) and number of classes in E and T.
Determine the word alignment a?.
Get some initial classes 6 and T.
UNTIL convergence criterion is met:
FOR EACH word e:
FOR EACH class E:
Determine the change of LP((e,T),n9) if e is moved to E.
Move e to the class with the largest improvement.
FOR EACH word f:
FOR EACH class F:
Determine the change of LP((E,T),n9) if f is moved to F.
Move f to the class with the largest improvement.
OUTPUT: Classes E and T.
</table>
<figureCaption confidence="0.998415">
Figure 1: Word Clustering Algorithm.
</figureCaption>
<bodyText confidence="0.999445222222222">
different languages occur in one class. It can be
seen that Eq. (3) is a special case of Eq. (9) with
ng,1 = n9,2.
Another possibility to perform bilingual word
clustering is to apply a two-step approach. In a
first step we determine classes S optimizing only
the monolingual part of Eq. (6) and secondly we
determine classes F optimizing the bilingual part
(without changing 6):
</bodyText>
<equation confidence="0.6429525">
= arg mein LP2(E, n) (11)
argnin LP2((i&apos;,.7.),nt). (12)
</equation>
<bodyText confidence="0.9997606">
By using these two optimization processes we en-
force that the classes E are mono-lingually &apos;good&apos;
classes and that the classes .7- correspond to 6.
Interestingly enough this results in a higher trans-
lation quality (see section 5).
</bodyText>
<sectionHeader confidence="0.996325" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.999818444444444">
An efficient optimization algorithm for LPI is the
exchange algorithm (Martin et al., 1998). For
the optimization of LP2 we can use the same al-
gorithm with small modifications. Our starting
point is a random partition of the training corpus
vocabulary. This initial partition is improved it-
eratively by moving a single word from one class
to another. The algorithm to determine bilingual
classes is depicted in Figure 1.
If only one word w is moved between the parti-
tions C and C&apos; the change LP(C,n9)— LP(C&apos;,n9)
can be computed efficiently looking only at classes
C for which ng (w, C) &gt; 0 or ng(C,w) &gt;0. We de-
fine Mc, to be the average number of seen predeces-
sor and successor word classes. With the notation
I for the number of iterations needed for conver-
gence, B for the number of word bigrams, M for
the number of classes and V for the vocabulary
</bodyText>
<figure confidence="0.983299222222222">
• •
•
•
the
co A0 m $4
g
a
a
0
</figure>
<figureCaption confidence="0.999964">
Figure 2: Examples of alignment templates.
</figureCaption>
<bodyText confidence="0.982815769230769">
size the computational complexity of this algo-
rithm is roughly I (B • log2 (B IV) +V M • Mo).
A detailed analysis of the complexity can be found
in (Martin et al., 1998).
The algorithm described above provides only a
local optimum. The quality of the resulting local
optima can be improved if we accept a short-term
degradation of the optimization criterion during
the optimization process. We do this in our imple-
mentation by applying the optimization method
threshold accepting (Dueck and Scheuer, 1990)
which is an efficient simplification of simulated an-
nealing.
</bodyText>
<figure confidence="0.988239909090909">
onwards
fifty-eight
six
from
Hanover
from
hourly
goes
train
AU
4.2
</figure>
<page confidence="0.980407">
73
</page>
<tableCaption confidence="0.9256645">
Proceedings of EACL &apos;99
Table 1: The EuTRANs-I corpus.
</tableCaption>
<table confidence="0.999842285714286">
Spanish English
Train: Sentences 10 000
Words 97 131 99 292
Vocabulary Size 686 513
Test: Sentences 2 996
Words 35 023 35 590
Bigr. Perplexity — 5.2
</table>
<tableCaption confidence="0.984692">
Table 2: The EuTRANs-II corpus.
</tableCaption>
<table confidence="0.999633857142857">
German English
Train: Sentences 16 226
Words 266 080 299 945
Vocabulary Size 39 511 25 751
Test: Sentences 187
Words 2 556 2 853
Bigr. Perplexity — 157
</table>
<sectionHeader confidence="0.999591" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.959376">
The statistical machine-translation method de-
scribed in (Och and Weber, 1998) makes use of
bilingual word classes. The key element of this
approach are the alignment templates (originally
referred to as translation rules) which are pairs of
phrases together with an alignment between the
words of the phrases. Examples of alignment tem-
plates are shown in Figure 2. The advantage of the
alignment template approach against word-based
statistical translation models is that word context
and local re-orderings are explicitly taken into ac-
count.
The alignment templates are automatically
trained using a parallel training corpus. The
translation of a sentence is done by a search pro-
cess which determines the set of alignment tem-
plates which optimally cover the source sentence.
The bilingual word classes are used to general-
ize the applicability of the alignment templates in
search. If there exists a class which contains all
cities in source and target language it is possible
that an alignment template containing a special
city can be generalized to all cities. More details
are given in (Och and Weber, 1998; Och and Ney,
1999).
We demonstrate results of our bilingual clus-
tering method for two different bilingual corpora
(see Tables 1 and 2). The EuTRANs-I corpus is
a subtask of the &amp;quot;Traveller Task&amp;quot; (Vidal, 1997)
which is an artificially generated Spanish-English
corpus. The domain of the corpus is a human-
to-human communication situation at a reception
Table 3: Example of bilingual word classes (corpus
EuTRANs-I, method BIL-2).
El: how it pardon what when where which•
who why
E2: my our
E3: today tomorrow
E4: ask call make
E5: carrying changing giving looking
moving putting sending showing waking
E6: full half quarter
Si: c&apos;omo cu&apos;al cu&apos;ando cu&apos;anta d&apos;onde
dice dicho hace qu&apos;e qui&apos;en tiene
</bodyText>
<listItem confidence="0.844142">
S2: ll&apos;eveme mi mis nuestra nuestras
nuestro nuestros s&apos;ubanme
S3: hoy ma nana mismo
S4: hacerme ll&apos;ameme ll&apos;amenos llama
llamar llamarme llamarnos llame p&apos;idame
p&apos;idanos pedir pedirme pedirnos
pida pide
S5: cambiarme cambiarnos despertarme
despertarnos llevar llevarme llevarnos
subirme subirnos usted ustedes
S6: completa cuarto media menos
</listItem>
<bodyText confidence="0.999169034482759">
desk of a hotel. The EuTRANs-II corpus is a natu-
ral German-English corpus consisting of different
text types belonging to the domain of tourism:
bilingual Web pages of hotels, bilingual touristic
brochures and business correspondence. The tar-
get language of our experiments is English.
We compare the three described methods to
generate bilingual word classes. The classes
MONO are determined by monolingually opti-
mizing source and target language classes with
Eq. (4). The classes BIL are determined by bilin-
gually optimizing classes with Eq. (10). The
classes BIL-2 are determined by first optimiz-
ing mono-lingually classes for the target language
(English) and afterwards optimizing classes for
the source language (Eq. (11) and Eq. (12)).
For EuTRANs-I we used 60 classes and for
EuTRANs-II we used 500 classes. We chose the
number of classes in such a way that the final per-
formance of the translation system was optimal.
The CPU time for optimization of bilingual word
classes on an Alpha workstation was under 20 sec-
onds for EuTRANs-I and less than two hours for
EuTRANs-II.
Table 3 provides examples of bilingual word
classes for the EuTRANs-I corpus. It can be seen
that the resulting classes often contain words that
are similar in their syntactic and semantic func-
tions. The grouping of words with a different
</bodyText>
<page confidence="0.997795">
74
</page>
<table confidence="0.462505">
Proceedings of EACL &apos;99
</table>
<tableCaption confidence="0.978466">
Table 4: Perplexity (PP) of different classes.
</tableCaption>
<table confidence="0.927305333333333">
Corpus MONO BIL BIL-2
EuTRANs-I 2.13 1.78 1.80
EuTRANs-II 13.2 9.3 9.8
</table>
<tableCaption confidence="0.987418">
Table 5: Average &amp;mirror of different classes.
</tableCaption>
<table confidence="0.996498333333333">
Corpus MONO BIL BIL-2
EuTRANs-I 3.5 2.6 2.6
EuTRANs-II 2.2 1.8 2.0
</table>
<bodyText confidence="0.968696">
meaning like today and tomorrow does not im-
ply that these words should be translated by the
same Spanish word, but it does imply that the
translations of these words are likely to be in the
same Spanish word class.
To measure the quality of our bilingual word
classes we applied two different evaluation mea-
sures:
</bodyText>
<listItem confidence="0.9917935">
1. Average e-mirror size (Wang et al., 1996):
The e-mirror of a class E is the set of classes
which have a translation probability greater
than c. We use E = 0.05.
2. The perplexity of the class transition proba-
bility on a bilingual test corpus:
</listItem>
<bodyText confidence="0.837116875">
exp (J-1 E maxi log (p (C ( f j) (ei))))
3=1
Both measures determine the extent to which the
translation probability is spread out. A small
value means that the translation probability is
very focused and that the knowledge of the source
language class provides much information about
the target language class.
</bodyText>
<tableCaption confidence="0.910748888888889">
Table 4 shows the perplexity of the obtained
translation lexicon without word classes, with
monolingual and with bilingual word classes. As
expected the bilingually optimized classes (BIL,
BIL-2) achieve a significantly lower perplexity and
a lower average &amp;mirror than the mono-lingually
optimized classes (MONO).
The tables 6 and 7 show the translation qual-
ity of the statistical machine translation system
described in (Och and Weber, 1998) using no
classes (WORD) at all, mono-lingually, and bi-
lingually optimized word classes. The trans-
lation system was trained using the bilingual
training corpus without any further knowledge
sources. Our evaluation criterion is the word er-
ror rate (WER) - the minimum number of in-
Table 6: Word error rate (WER) and average
alignment template length (AATL) on EUTRANSI.
</tableCaption>
<table confidence="0.988733">
-
Method WER [70] AATL
WORD 6.31 2.85
MONO 5.64 5.03
BIL 5.38 4.40
BIL-2 4.76 5.19
</table>
<tableCaption confidence="0.971774">
Table 7: Word error rate (WER) and average
alignment template length (AATL) on EUTRANS-
</tableCaption>
<table confidence="0.999329">
Method WER [70] AATL
WORD 64.3 1.36
MONO 63.5 1.74
BIL 63.2 1.53
BIL-2 62.5 1.54
</table>
<bodyText confidence="0.959624958333333">
sertions/deletions/substitutions relative to a ref-
erence translation.
As expected the translation quality improves
using classes. For the small EuTRANs-I task the
word error rates reduce significantly. The word er-
ror rates for the EuTRANs-II task are much larger
because the task has a very large vocabulary and is
more complex. The bilingual classes show better
results than the monolingual classes MONO. One
explanation for the improvement in translation
quality is that the bilingually optimized classes
result in an increased average size of used align-
ment templates. For example the average length
of alignment templates with the EuTRANs-I cor-
pus using WORD is 2.85 and using BIL-2 it is
5.19. The longer the average alignment template
length, the more context is used in the translation
and therefore the translation quality is higher.
An explanation for the superiority of BIL-2
over BIL is that by first optimizing the English
classes mono-lingually, it is much more probable
that longer sequences of classes occur more often
thereby increasing the average alignment template
size.
</bodyText>
<sectionHeader confidence="0.989172" genericHeader="conclusions">
6 Summary and future works
</sectionHeader>
<bodyText confidence="0.999807">
By applying a maximum-likelihood approach to
the joint probability of a parallel corpus we ob-
tained an optimization criterion for bilingual word
classes which is very similar to the one used in
monolingual maximum-likelihood word clustering.
For optimization we used the exchange algorithm.
The obtained word classes give a low translation
lexicon perplexity and improve the quality of sta-
</bodyText>
<page confidence="0.995945">
75
</page>
<bodyText confidence="0.956320125">
Proceedings of EACL &apos;99
tistical machine translation.
We expect improvements in translation quality
by allowing that words occur in more than one
class and by performing a hierarchical clustering.
Acknowledgements This work has been par-
tially supported by the European Community un-
der the ESPRIT project number 30268 (EuTrans).
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996840096774194">
P. F. Brown, V. J. Della Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992. Class-based
n-gram models of natural language. Computa-
tional Linguistics, 18(4):467-479.
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, and Robert L. Mercer.
1993. The mathematics of statistical machine
translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
G. Dueck and T. Scheuer. 1990. Threshold ac-
cepting: A general purpose optimization al-
gorithm appearing superior to simulated an-
nealing. Journal of Computational Physics,
90(1):161-175.
Pascale Fung and Delcai Wu. 1995. Coerced
markov models for cross-lingual lexical-tag re-
lations. In The Sixth International Conference
on Theoretical and Methodological Issues in Ma-
chine Translation, pages 240-255, Leuven, Bel-
gium, July.
M. Jardino and G. Adda. 1993. Automatic Word
Classification Using Simulated Annealing. In
Proc. Int. Conf. on Acoustics, Speech, and Sig-
nal Processing, volume 2, pages 41-44, Min-
neapolis.
R. Kneser and H. Ney. 1991. Forming Word
Classes by Statistical Clustering for Statistical
Language Modelling. In 1. Quantitative Lin-
guistics Conference.
R. Kneser and H. Ney. 1993. Improved Cluster-
ing Techniques for Class-Based Statistical Lan-
guage Modelling. In European Conference on
Speech Communication and Technology, pages
973-976.
Sven Martin, Jorg Liermann, and Hermann Ney.
1998. Algorithms for bigram and trigram word
clustering. Speech Communication, 24(1):19-
37.
Franz Josef Och and Hermann Ney. 1999. The
alignment template approach to statistical ma-
chine translation. To appear.
Franz Josef Och and Hans Weber. 1998. Im-
proving statistical natural language translation
with categories and rules. In Proceedings of the
35th Annual Conference of the Association for
Computational Linguistics and the 17th Inter-
national Conference on Computational Linguis-
tics, pages 985-989, Montreal, Canada, August.
Enrique Vidal. 1997. Finite-state speech-to-
speech translation. In Proc. Int. Conf. on
Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 111-114.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in
statistical translation. In COLING &apos;96: The
16th Int. Conf. on Computational Linguistics,
pages 836-841, Copenhagen, August.
Ye-Yi Wang, John Lafferty, and Alex Waibel.
1996. Word clustering with parallel spoken lan-
guage corpora. In Proceedings of the 4th Inter-
national Conference on Spoken Language Pro-
cesing (ICSLP&apos;96), pages 2364-2367.
</reference>
<page confidence="0.991692">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407089">
<note confidence="0.57956">Proceedings of EACL &apos;99</note>
<title confidence="0.999562">An Efficient Method for Determining Bilingual Word Classes</title>
<author confidence="0.99957">Franz Josef Och</author>
<affiliation confidence="0.963723">Lehrstuhl für Informatik VI RWTH Aachen - University of Technology</affiliation>
<address confidence="0.958487666666667">Ahornstralk 55 52056 Aachen GERMANY</address>
<email confidence="0.996217">ochainformatik.rwth-aachen.de</email>
<abstract confidence="0.9914935">In statistical natural language processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="971" citStr="Brown et al., 1992" startWordPosition="148" endWordPosition="151">blem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. 1 Introduction Word classes are often used in language modelling to solve the problem of sparse data. Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms. In the field of statistical machine translation we also face the problem of sparse data. Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models. A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language. Unfortunately we can not expect these independently optimized classes to be correspondent. Therefore</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="5086" citStr="Brown et al., 1993" startWordPosition="849" endWordPosition="852">, .F) (5) E,Y arg max p(el lE) • p(felef; E, .F)(6) e,Y To perform the maximization of Eq. (6) we have to model the monolingual a priori probability p(ef1E) and the translation probability p(f lel; e, F). For the first we use the class-based bigram probability from Eq. (1). To model p(fillef; E, .7) we assume the existence of an alignment af. We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): P(fli rip(f,lea,) (7) j.1 The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996). The idea is to introduce the unknown alignment a as hidden variable into a statistical model of the translation probability p(glef). By applying the EMalgorithm we obtain the model parameters. The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996). By rewriting the translation probability using word classes, we obtain (corresponding to Eq. (1)): PC f E,7) =11P(T(fi)le(ea;)).P(.61.T(.6)) (8) The variables F and E denote special classes in and E. We use relative frequencies to estimate p(FIE) and p(fIF): nt(FIE)/ (E</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dueck</author>
<author>T Scheuer</author>
</authors>
<title>Threshold accepting: A general purpose optimization algorithm appearing superior to simulated annealing.</title>
<date>1990</date>
<journal>Journal of Computational Physics,</journal>
<pages>90--1</pages>
<contexts>
<context position="9383" citStr="Dueck and Scheuer, 1990" startWordPosition="1622" endWordPosition="1625">e number of classes and V for the vocabulary • • • • the co A0 m $4 g a a 0 Figure 2: Examples of alignment templates. size the computational complexity of this algorithm is roughly I (B • log2 (B IV) +V M • Mo). A detailed analysis of the complexity can be found in (Martin et al., 1998). The algorithm described above provides only a local optimum. The quality of the resulting local optima can be improved if we accept a short-term degradation of the optimization criterion during the optimization process. We do this in our implementation by applying the optimization method threshold accepting (Dueck and Scheuer, 1990) which is an efficient simplification of simulated annealing. onwards fifty-eight six from Hanover from hourly goes train AU 4.2 73 Proceedings of EACL &apos;99 Table 1: The EuTRANs-I corpus. Spanish English Train: Sentences 10 000 Words 97 131 99 292 Vocabulary Size 686 513 Test: Sentences 2 996 Words 35 023 35 590 Bigr. Perplexity — 5.2 Table 2: The EuTRANs-II corpus. German English Train: Sentences 16 226 Words 266 080 299 945 Vocabulary Size 39 511 25 751 Test: Sentences 187 Words 2 556 2 853 Bigr. Perplexity — 157 5 Results The statistical machine-translation method described in (Och and Weber</context>
</contexts>
<marker>Dueck, Scheuer, 1990</marker>
<rawString>G. Dueck and T. Scheuer. 1990. Threshold accepting: A general purpose optimization algorithm appearing superior to simulated annealing. Journal of Computational Physics, 90(1):161-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Delcai Wu</author>
</authors>
<title>Coerced markov models for cross-lingual lexical-tag relations.</title>
<date>1995</date>
<booktitle>In The Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>240--255</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="1688" citStr="Fung and Wu, 1995" startWordPosition="257" endWordPosition="260"> maximum-likelihood criterion with iterative clustering algorithms. In the field of statistical machine translation we also face the problem of sparse data. Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models. A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language. Unfortunately we can not expect these independently optimized classes to be correspondent. Therefore mono-lingually optimized word classes do not seem to be useful for machine translation (see also (Fung and Wu, 1995)). We define bilingual word clustering as the process of forming corresponding word classes suitable for machine translation purposes for a pair of languages using a parallel training corpus. The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998). Our approach is simpler and computationally more efficient than (Wang et al., 1996). 2 Monolingual Word Clustering The task of a statistical language model is to estimate the probability Pr(wiev) of a sequence of words wiv = wi &apos;WN. A simple approximation of Pr(41) is </context>
</contexts>
<marker>Fung, Wu, 1995</marker>
<rawString>Pascale Fung and Delcai Wu. 1995. Coerced markov models for cross-lingual lexical-tag relations. In The Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 240-255, Leuven, Belgium, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jardino</author>
<author>G Adda</author>
</authors>
<title>Automatic Word Classification Using Simulated Annealing.</title>
<date>1993</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>2</volume>
<pages>41--44</pages>
<location>Minneapolis.</location>
<contexts>
<context position="995" citStr="Jardino and Adda, 1993" startWordPosition="152" endWordPosition="155">ds into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. 1 Introduction Word classes are often used in language modelling to solve the problem of sparse data. Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms. In the field of statistical machine translation we also face the problem of sparse data. Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models. A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language. Unfortunately we can not expect these independently optimized classes to be correspondent. Therefore mono-lingually optimize</context>
</contexts>
<marker>Jardino, Adda, 1993</marker>
<rawString>M. Jardino and G. Adda. 1993. Automatic Word Classification Using Simulated Annealing. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing, volume 2, pages 41-44, Minneapolis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Forming Word Classes by Statistical Clustering for Statistical Language Modelling.</title>
<date>1991</date>
<booktitle>In 1. Quantitative Linguistics Conference.</booktitle>
<contexts>
<context position="3557" citStr="Kneser and Ney, 1991" startWordPosition="575" endWordPosition="578">en its predecessor class C&apos; and the membership probability p(wIC) for word w given class C. To determine the optimal classes C for a given number of classes M we perform a maximumlikelihood approach: = arg mrc p(wiv IC) (2) We estimate the probabilities of Eq. (1) by relative frequencies: p(CIC&amp;quot;) := n(C1C1&apos;)In(C&apos;), p(wIC) = n(w)In(C). The function n(-) provides the frequency of a uni- or bigram in the training corpus. If we insert this into Eq. (2) and apply the negative logarithm and change the summation order we arrive at the following optimization 71 Proceedings of EACL &apos;99 criterion LP,. (Kneser and Ney, 1991): LPi(C,n) = — E h(n(CICT +2 E h(n(C)) (3) c = arg min LPI(C, n). (4) The function h(n) is a shortcut for n • log(n). It is necessary to fix the number of classes in C in advance as the optimum is reached if every word is a class of its own. Because of this it is necessary to perform an additional optimization process which determines the number of classes. The use of leaving-one-out in a modified optimization criterion as in (Kneser and Ney, 1993) could in principle solve this problem. An efficient optimization algorithm for LPI is described in section 4. 3 Bilingual Word Clustering In biling</context>
</contexts>
<marker>Kneser, Ney, 1991</marker>
<rawString>R. Kneser and H. Ney. 1991. Forming Word Classes by Statistical Clustering for Statistical Language Modelling. In 1. Quantitative Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved Clustering Techniques for Class-Based Statistical Language Modelling.</title>
<date>1993</date>
<booktitle>In European Conference on Speech Communication and Technology,</booktitle>
<pages>973--976</pages>
<contexts>
<context position="4009" citStr="Kneser and Ney, 1993" startWordPosition="662" endWordPosition="665"> and apply the negative logarithm and change the summation order we arrive at the following optimization 71 Proceedings of EACL &apos;99 criterion LP,. (Kneser and Ney, 1991): LPi(C,n) = — E h(n(CICT +2 E h(n(C)) (3) c = arg min LPI(C, n). (4) The function h(n) is a shortcut for n • log(n). It is necessary to fix the number of classes in C in advance as the optimum is reached if every word is a class of its own. Because of this it is necessary to perform an additional optimization process which determines the number of classes. The use of leaving-one-out in a modified optimization criterion as in (Kneser and Ney, 1993) could in principle solve this problem. An efficient optimization algorithm for LPI is described in section 4. 3 Bilingual Word Clustering In bilingual word clustering we are interested in classes F and E which form partitions of the vocabulary of two languages. To perform bilingual word clustering we use a maximum-likelihood approach as in the monolingual case. We maximize the joint probability of a bilingual training corpus (ef (61, .fr) = arg max p(ef , .F) (5) E,Y arg max p(el lE) • p(felef; E, .F)(6) e,Y To perform the maximization of Eq. (6) we have to model the monolingual a priori prob</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>R. Kneser and H. Ney. 1993. Improved Clustering Techniques for Class-Based Statistical Language Modelling. In European Conference on Speech Communication and Technology, pages 973-976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering.</title>
<date>1998</date>
<journal>Speech Communication,</journal>
<pages>24--1</pages>
<contexts>
<context position="1017" citStr="Martin et al., 1998" startWordPosition="156" endWordPosition="159">ses which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. 1 Introduction Word classes are often used in language modelling to solve the problem of sparse data. Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms. In the field of statistical machine translation we also face the problem of sparse data. Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models. A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language. Unfortunately we can not expect these independently optimized classes to be correspondent. Therefore mono-lingually optimized word classes do not </context>
<context position="8043" citStr="Martin et al., 1998" startWordPosition="1378" endWordPosition="1381"> word clustering is to apply a two-step approach. In a first step we determine classes S optimizing only the monolingual part of Eq. (6) and secondly we determine classes F optimizing the bilingual part (without changing 6): = arg mein LP2(E, n) (11) argnin LP2((i&apos;,.7.),nt). (12) By using these two optimization processes we enforce that the classes E are mono-lingually &apos;good&apos; classes and that the classes .7- correspond to 6. Interestingly enough this results in a higher translation quality (see section 5). 4 Implementation An efficient optimization algorithm for LPI is the exchange algorithm (Martin et al., 1998). For the optimization of LP2 we can use the same algorithm with small modifications. Our starting point is a random partition of the training corpus vocabulary. This initial partition is improved iteratively by moving a single word from one class to another. The algorithm to determine bilingual classes is depicted in Figure 1. If only one word w is moved between the partitions C and C&apos; the change LP(C,n9)— LP(C&apos;,n9) can be computed efficiently looking only at classes C for which ng (w, C) &gt; 0 or ng(C,w) &gt;0. We define Mc, to be the average number of seen predecessor and successor word classes.</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. Speech Communication, 24(1):19-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>1999</date>
<note>To appear.</note>
<contexts>
<context position="11043" citStr="Och and Ney, 1999" startWordPosition="1897" endWordPosition="1900">explicitly taken into account. The alignment templates are automatically trained using a parallel training corpus. The translation of a sentence is done by a search process which determines the set of alignment templates which optimally cover the source sentence. The bilingual word classes are used to generalize the applicability of the alignment templates in search. If there exists a class which contains all cities in source and target language it is possible that an alignment template containing a special city can be generalized to all cities. More details are given in (Och and Weber, 1998; Och and Ney, 1999). We demonstrate results of our bilingual clustering method for two different bilingual corpora (see Tables 1 and 2). The EuTRANs-I corpus is a subtask of the &amp;quot;Traveller Task&amp;quot; (Vidal, 1997) which is an artificially generated Spanish-English corpus. The domain of the corpus is a humanto-human communication situation at a reception Table 3: Example of bilingual word classes (corpus EuTRANs-I, method BIL-2). El: how it pardon what when where which• who why E2: my our E3: today tomorrow E4: ask call make E5: carrying changing giving looking moving putting sending showing waking E6: full half quart</context>
</contexts>
<marker>Och, Ney, 1999</marker>
<rawString>Franz Josef Och and Hermann Ney. 1999. The alignment template approach to statistical machine translation. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hans Weber</author>
</authors>
<title>Improving statistical natural language translation with categories and rules.</title>
<date>1998</date>
<booktitle>In Proceedings of the 35th Annual Conference of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>985--989</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="2017" citStr="Och and Weber, 1998" startWordPosition="308" endWordPosition="311">e of mono-lingually optimized word classes in source and target language. Unfortunately we can not expect these independently optimized classes to be correspondent. Therefore mono-lingually optimized word classes do not seem to be useful for machine translation (see also (Fung and Wu, 1995)). We define bilingual word clustering as the process of forming corresponding word classes suitable for machine translation purposes for a pair of languages using a parallel training corpus. The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998). Our approach is simpler and computationally more efficient than (Wang et al., 1996). 2 Monolingual Word Clustering The task of a statistical language model is to estimate the probability Pr(wiev) of a sequence of words wiv = wi &apos;WN. A simple approximation of Pr(41) is to model it as a product of bigram probabilities: Pr (wPI) = HiN_, p(wi *2_1). If we want to estimate the bigram probabilities p(wlw&apos;) using a realistic natural language corpus we are faced with the problem that most of the bigrams are rarely seen. One possibility to solve this problem is to partition the set of all words into </context>
<context position="9990" citStr="Och and Weber, 1998" startWordPosition="1727" endWordPosition="1730">cheuer, 1990) which is an efficient simplification of simulated annealing. onwards fifty-eight six from Hanover from hourly goes train AU 4.2 73 Proceedings of EACL &apos;99 Table 1: The EuTRANs-I corpus. Spanish English Train: Sentences 10 000 Words 97 131 99 292 Vocabulary Size 686 513 Test: Sentences 2 996 Words 35 023 35 590 Bigr. Perplexity — 5.2 Table 2: The EuTRANs-II corpus. German English Train: Sentences 16 226 Words 266 080 299 945 Vocabulary Size 39 511 25 751 Test: Sentences 187 Words 2 556 2 853 Bigr. Perplexity — 157 5 Results The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes. The key element of this approach are the alignment templates (originally referred to as translation rules) which are pairs of phrases together with an alignment between the words of the phrases. Examples of alignment templates are shown in Figure 2. The advantage of the alignment template approach against word-based statistical translation models is that word context and local re-orderings are explicitly taken into account. The alignment templates are automatically trained using a parallel training corpus. The translation of a sentence is done by a search </context>
<context position="14957" citStr="Och and Weber, 1998" startWordPosition="2532" endWordPosition="2535">mall value means that the translation probability is very focused and that the knowledge of the source language class provides much information about the target language class. Table 4 shows the perplexity of the obtained translation lexicon without word classes, with monolingual and with bilingual word classes. As expected the bilingually optimized classes (BIL, BIL-2) achieve a significantly lower perplexity and a lower average &amp;mirror than the mono-lingually optimized classes (MONO). The tables 6 and 7 show the translation quality of the statistical machine translation system described in (Och and Weber, 1998) using no classes (WORD) at all, mono-lingually, and bilingually optimized word classes. The translation system was trained using the bilingual training corpus without any further knowledge sources. Our evaluation criterion is the word error rate (WER) - the minimum number of inTable 6: Word error rate (WER) and average alignment template length (AATL) on EUTRANSI. - Method WER [70] AATL WORD 6.31 2.85 MONO 5.64 5.03 BIL 5.38 4.40 BIL-2 4.76 5.19 Table 7: Word error rate (WER) and average alignment template length (AATL) on EUTRANSMethod WER [70] AATL WORD 64.3 1.36 MONO 63.5 1.74 BIL 63.2 1.5</context>
</contexts>
<marker>Och, Weber, 1998</marker>
<rawString>Franz Josef Och and Hans Weber. 1998. Improving statistical natural language translation with categories and rules. In Proceedings of the 35th Annual Conference of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 985-989, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
</authors>
<title>Finite-state speech-tospeech translation.</title>
<date>1997</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>111--114</pages>
<contexts>
<context position="11232" citStr="Vidal, 1997" startWordPosition="1930" endWordPosition="1931">et of alignment templates which optimally cover the source sentence. The bilingual word classes are used to generalize the applicability of the alignment templates in search. If there exists a class which contains all cities in source and target language it is possible that an alignment template containing a special city can be generalized to all cities. More details are given in (Och and Weber, 1998; Och and Ney, 1999). We demonstrate results of our bilingual clustering method for two different bilingual corpora (see Tables 1 and 2). The EuTRANs-I corpus is a subtask of the &amp;quot;Traveller Task&amp;quot; (Vidal, 1997) which is an artificially generated Spanish-English corpus. The domain of the corpus is a humanto-human communication situation at a reception Table 3: Example of bilingual word classes (corpus EuTRANs-I, method BIL-2). El: how it pardon what when where which• who why E2: my our E3: today tomorrow E4: ask call make E5: carrying changing giving looking moving putting sending showing waking E6: full half quarter Si: c&apos;omo cu&apos;al cu&apos;ando cu&apos;anta d&apos;onde dice dicho hace qu&apos;e qui&apos;en tiene S2: ll&apos;eveme mi mis nuestra nuestras nuestro nuestros s&apos;ubanme S3: hoy ma nana mismo S4: hacerme ll&apos;ameme ll&apos;amen</context>
</contexts>
<marker>Vidal, 1997</marker>
<rawString>Enrique Vidal. 1997. Finite-state speech-tospeech translation. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing, volume 1, pages 111-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING &apos;96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen,</location>
<contexts>
<context position="5107" citStr="Vogel et al., 1996" startWordPosition="853" endWordPosition="856">x p(el lE) • p(felef; E, .F)(6) e,Y To perform the maximization of Eq. (6) we have to model the monolingual a priori probability p(ef1E) and the translation probability p(f lel; e, F). For the first we use the class-based bigram probability from Eq. (1). To model p(fillef; E, .7) we assume the existence of an alignment af. We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): P(fli rip(f,lea,) (7) j.1 The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996). The idea is to introduce the unknown alignment a as hidden variable into a statistical model of the translation probability p(glef). By applying the EMalgorithm we obtain the model parameters. The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996). By rewriting the translation probability using word classes, we obtain (corresponding to Eq. (1)): PC f E,7) =11P(T(fi)le(ea;)).P(.61.T(.6)) (8) The variables F and E denote special classes in and E. We use relative frequencies to estimate p(FIE) and p(fIF): nt(FIE)/ (E nt(FIE)) nt(f)/ (E n</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING &apos;96: The 16th Int. Conf. on Computational Linguistics, pages 836-841, Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>John Lafferty</author>
<author>Alex Waibel</author>
</authors>
<title>Word clustering with parallel spoken language corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th International Conference on Spoken Language Procesing (ICSLP&apos;96),</booktitle>
<pages>2364--2367</pages>
<contexts>
<context position="2102" citStr="Wang et al., 1996" startWordPosition="322" endWordPosition="325"> we can not expect these independently optimized classes to be correspondent. Therefore mono-lingually optimized word classes do not seem to be useful for machine translation (see also (Fung and Wu, 1995)). We define bilingual word clustering as the process of forming corresponding word classes suitable for machine translation purposes for a pair of languages using a parallel training corpus. The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998). Our approach is simpler and computationally more efficient than (Wang et al., 1996). 2 Monolingual Word Clustering The task of a statistical language model is to estimate the probability Pr(wiev) of a sequence of words wiv = wi &apos;WN. A simple approximation of Pr(41) is to model it as a product of bigram probabilities: Pr (wPI) = HiN_, p(wi *2_1). If we want to estimate the bigram probabilities p(wlw&apos;) using a realistic natural language corpus we are faced with the problem that most of the bigrams are rarely seen. One possibility to solve this problem is to partition the set of all words into equivalence classes. The function C maps words w to their classes C(w). Rewriting the</context>
<context position="14000" citStr="Wang et al., 1996" startWordPosition="2375" endWordPosition="2378"> 74 Proceedings of EACL &apos;99 Table 4: Perplexity (PP) of different classes. Corpus MONO BIL BIL-2 EuTRANs-I 2.13 1.78 1.80 EuTRANs-II 13.2 9.3 9.8 Table 5: Average &amp;mirror of different classes. Corpus MONO BIL BIL-2 EuTRANs-I 3.5 2.6 2.6 EuTRANs-II 2.2 1.8 2.0 meaning like today and tomorrow does not imply that these words should be translated by the same Spanish word, but it does imply that the translations of these words are likely to be in the same Spanish word class. To measure the quality of our bilingual word classes we applied two different evaluation measures: 1. Average e-mirror size (Wang et al., 1996): The e-mirror of a class E is the set of classes which have a translation probability greater than c. We use E = 0.05. 2. The perplexity of the class transition probability on a bilingual test corpus: exp (J-1 E maxi log (p (C ( f j) (ei)))) 3=1 Both measures determine the extent to which the translation probability is spread out. A small value means that the translation probability is very focused and that the knowledge of the source language class provides much information about the target language class. Table 4 shows the perplexity of the obtained translation lexicon without word classes,</context>
</contexts>
<marker>Wang, Lafferty, Waibel, 1996</marker>
<rawString>Ye-Yi Wang, John Lafferty, and Alex Waibel. 1996. Word clustering with parallel spoken language corpora. In Proceedings of the 4th International Conference on Spoken Language Procesing (ICSLP&apos;96), pages 2364-2367.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>