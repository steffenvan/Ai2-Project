<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000763">
<title confidence="0.998949">
Document Similarity Measures to Distinguish
Native vs. Non-Native Essay Writers
</title>
<author confidence="0.787262">
Olga Gurevich
</author>
<affiliation confidence="0.5457935">
Educational Testing Service
Rosedale &amp; Carter Roads,
</affiliation>
<address confidence="0.9163665">
Turnbull 11R
Princeton, NJ 08541
</address>
<email confidence="0.998">
ogurevich@ets.org
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998855">
The ability to distinguish statistically dif-
ferent populations of speakers or writers
can be an important asset in many NLP
applications. In this paper, we describe a
method of using document similarity
measures to describe differences in be-
havior between native and non-native
speakers of English in a writing task.1
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988545454546">
The ability to distinguish statistically different
populations of speakers or writers can be an impor-
tant asset in many NLP applications. In this paper,
we describe a method of using document similarity
measures to describe differences in behavior be-
tween native and non-native speakers of English in
a prompt response task.
We analyzed results from the new TOEFL inte-
grated writing task, described in the next section.
All task participants received the same set of
prompts and were asked to summarize them. The
resulting essays are all trying to express the same
&amp;quot;gist&amp;quot; content, so that any measurable differences
between them must be due to differences in indi-
vidual language ability and style. Thus the task is
uniquely suited to measuring differences in linguis-
tic behavior between populations.
Our measure of document similarity, described
in section 3, is a combination of word overlap and
syntactic similarity, also serving as a measure of
syntactic variability. The results demonstrate sig-
nificant differences between native and non-native
</bodyText>
<footnote confidence="0.9343445">
1 This research was funded while the first author was a Re-
search Postdoctoral Fellow at ETS in Princeton, NJ.
</footnote>
<page confidence="0.989889">
49
</page>
<note confidence="0.702087">
Paul Deane
Educational Testing Service
Rosedale &amp; Carter Roads,
</note>
<address confidence="0.5690245">
Turnbull 11R
Princeton, NJ 08541
</address>
<email confidence="0.87577">
pdeane@ets.org
</email>
<bodyText confidence="0.9816055">
speakers that cannot be attributed to any demo-
graphic factor other than the language ability itself.
</bodyText>
<sectionHeader confidence="0.890725" genericHeader="method">
2 TOEFL Integrated Writing Task and
Scoring
</sectionHeader>
<bodyText confidence="0.999348966666667">
The Test of English as a Foreign Language
(TOEFL) is administered to foreign students wish-
ing to enroll in US or Canadian universities. It
aims to measure the extent to which a student has
acquired English; thus native speakers should on
average perform better on the test regardless of
their analytical abilities. The TOEFL now includes
a writing component, and pilot studies were con-
ducted with native as well as non-native speakers.
One of the writing components is an Integrated
Writing Task. Students first read an expository
passage, which remains on the screen throughout
the task. Students then hear a segment of a lecture
concerning the same topic. However, the lecture
contradicts and complements the information con-
tained in the reading. The lecture is heard once;
students then summarize the lecture and the read-
ing and describe any contradictions between them.
The resulting essays are scored by human raters
on a scale of 0 to 5, with 5 being the best possible
score2. The highest-scoring essays express ideas
from both the lecture and the reading using correct
grammar; the lowest-scoring essays rely on only
one of the prompts for information and have
grammatical problems; and the scores in between
show a combination of both types of deficiencies.
The test prompt contained passages about the
advantages and disadvantages of working in
groups; the reading was 260 words long, the lec-
ture 326 words. 540 non-native speakers and 950
</bodyText>
<footnote confidence="0.993089">
2 Native speaker essays were initially scored with possible
half-grades such as 2.5. For purposes of comparison, these
were rounded down to the nearest integer.
</footnote>
<note confidence="0.684796">
Proceedings of NAACL HLT 2007, Companion Volume, pages 49–52,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999065">
native speakers were tested by ETS in 2004. ETS
also collected essential demographic data such as
native language, educational level, etc., for each
student. For later validation, we excluded 1/3 of
each set, selected at random, thus involving 363
non-native speakers and 600 native speakers.
</bodyText>
<figureCaption confidence="0.997672">
Figure 1. Relative score distributions.
</figureCaption>
<bodyText confidence="0.99999625">
Among the non-native speakers, the most
common score was 1 (see Fig. 1 for a histogram).
By contrast, native speaker scores centered around
3 and showed a normal-type distribution. The dif-
ference in distributions confirms that the task is
effective at separating non-native speakers by skill
level, and is easier for native speakers. The poten-
tial sources of difficulty include comprehension of
the reading passage, listening ability and memory
for the lecture, and the analytical ability to find
commonalities and differences between the content
of the reading and the lecture.
</bodyText>
<sectionHeader confidence="0.981505" genericHeader="method">
3 Document Similarity Measure
</sectionHeader>
<bodyText confidence="0.999995814814815">
Due to the design of the TOEFL task, the content
of the student essays is highly constrained. The
aim of the computational measures is to extract
grammatical and stylistic differences between dif-
ferent essays. We do this by comparing the essays
to the reading and lecture prompts. Our end goal is
to determine to what extent speakers diverge from
the prompts while retaining the content.
The prediction is that native speakers are much
more likely to paraphrase the prompts while keep-
ing the same gist, whereas non-native speakers are
likely to either repeat the prompts close to verba-
tim, or diverge from them in ways that do not pre-
serve the gist. This intuition conforms to previous
studies of native vs. non-native speakers&apos; text
summarization (cf. Campbell 1987), although we
are not aware of any related computational work.
We begin by measuring lexico-grammatical
similarity between each essay and the two prompts.
An essay is represented as a set of features derived
from its lexico-grammatical content, as described
below. The resulting comparison measure goes
beyond simple word or n-gram overlap by provid-
ing a measure of structural similarity as well. In
essence, our method measures to what extent the
essay expresses the content of the prompt in the
same words, used in the same syntactic positions.
</bodyText>
<subsectionHeader confidence="0.997138">
3.1 C-rater tuples
</subsectionHeader>
<bodyText confidence="0.999941875">
In order to get a measure of syntactic similarity, we
relied on C-rater (Leacock &amp; Chodorow 2003), an
automatic scoring engine developed at ETS. C-
rater includes several basic NLP components, in-
cluding POS tagging, morphological processing,
anaphora resolution, and shallow parsing. The
parsing produces tuples for each clause, which de-
scribe each verb and its syntactic arguments (1).
</bodyText>
<listItem confidence="0.77701975">
(1) CLAUSE: the group spreads responsibil-
ity for a decision to all the members
TUPLE: :verb: spread :subj: the group :obj:
responsible :pp.for: for a decide :pp.to: to all
</listItem>
<bodyText confidence="0.996870666666667">
C-rater does not produce full-sentence trees or
prepositional phrase attachment. However, the
tuples are reasonably accurate on non-native input.
</bodyText>
<subsectionHeader confidence="0.998834">
3.2 Lexical and Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9999396">
C-rater produces tuples for each document, often
several per sentence. For the current experiment,
we used the main verb, its subject and object. We
then converted each tuple into a set of features,
which included the following:
</bodyText>
<listItem confidence="0.927054142857143">
• The verb, subject (pro)noun, and object
(pro)noun as individual words;
• All of the words together as a single feature;
• The verb, subject, and object words with
their argument roles.
Each document can now be represented as a set
of tuple-derived features, or feature vectors.
</listItem>
<subsectionHeader confidence="0.995648">
3.3 Document Comparison
</subsectionHeader>
<bodyText confidence="0.998787285714286">
Two feature vectors derived from tuples can be
compared using a cosine measure (Salton 1989).
The closer to 1 the cosine, the more similar the two
feature sets. To compensate for different frequen-
cies of the features and for varying document
lengths, the feature vectors are weighted using
standard tf*idf techniques.
</bodyText>
<figure confidence="0.998549461538461">
20
30
25
35
10
15
0
5
1 2 3 4 5
Percent score frequencies
Score
Non-native
Native
</figure>
<page confidence="0.979837">
50
</page>
<bodyText confidence="0.999972615384616">
In order to estimate the similarity between two
documents, we use the following procedure. For
each tuple vector in Document A, we find the tuple
in Document B with the maximum cosine to the
tuple in Document A. The maximum cosine val-
ues for each tuple are then averaged, resulting in a
single scalar value for Document A. We call this
measure Average Maximum Cosine (AMC).
We calculated AMCs for each student response
versus the reading, the lecture, and the reading +
lecture combined. This procedure was performed
for both native and non-native essays. A detailed
examination of the resulting trends is in section 4.
</bodyText>
<subsectionHeader confidence="0.883543">
3.4 Other Measures of Document Similarity
</subsectionHeader>
<bodyText confidence="0.999263">
We also performed several measures of document
similarity that did not include syntactic features.
</bodyText>
<subsectionHeader confidence="0.549127">
Content Vector Analysis
</subsectionHeader>
<bodyText confidence="0.999987789473684">
The student essays and the prompts were compared
using Content Vector Analysis (CVA), where each
document was represented as a vector consisting of
the words in it (Salton 1989). The tf*idf-weighted
vectors were compared by a cosine measure.
For non-native speakers, there was a noticeable
trend. At higher score levels (where the score is
determined by a human rater), student essays
showed more similarity to both the reading and the
lecture prompts. Both the reading and lecture
similarity trends were significant (linear trend; F=
MSlinear trend/MSwithin-subjects=63 for the reading; F=71
for the lecture at 0.05 significance level3). Thus,
the rate of vocabulary retention from both prompts
increases with higher English-language skill level.
Native speakers showed a similar pattern of in-
creasing cosine similarity between the essay and
the reading (F=35 at 0.05 significance for the
trend), and the lecture (F=35 at the 0.05 level).
</bodyText>
<sectionHeader confidence="0.450951" genericHeader="method">
BLEU score
</sectionHeader>
<bodyText confidence="0.999963">
In order to measure the extent to which whole
chunks of text from the prompt are reproduced in
the student essays, we used the BLEU score,
known from studies of machine translation (Pap-
ineni et al. 2002). We used whole essays as sec-
tions of text rather than individual sentences.
For non-native speakers, the trend was similar
to that found with CVA: at higher score levels, the
</bodyText>
<footnote confidence="0.839127">
3 All statistical calculations were performed as ANOVA-style
trend analyses using SPSS.
</footnote>
<bodyText confidence="0.999608">
overlap between the essays and both prompts in-
creased (F=52.4 at the 0.05 level for the reading;
F=53.6 for the lecture).
Native speakers again showed a similar pattern,
with a significant trend towards more similarity to
the reading (F=35.6) and the lecture (F=31.3).
These results were confirmed by a simple n-gram
overlap measure.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.998812">
4.1 Overall similarity to reading and lecture
</subsectionHeader>
<bodyText confidence="0.999995275862069">
The AMC similarity measure, which relies on syn-
tactic as well as lexical similarity, produced some-
what different results from simpler bag-of-word or
n-gram measures. In particular, there was a differ-
ence in behavior between native and non-native
speakers: non-native speakers showed increased
structural similarity to the lecture with increasing
scores, but native speakers did not.
For non-native speakers, the trend of increased
AMC between the essay and the lecture was sig-
nificant (F=10.9). On the other hand, there was no
significant increase in AMC between non-native
essays and the reading (F=3.4). Overall, for non-
native speakers the mean AMC was higher for the
reading than for the lecture (0.114 vs. 0.08).
Native speakers, by contrast, showed no sig-
nificant trends for either the reading or the lecture.
Overall, the average AMCs for the reading and the
lecture were comparable (0.08 vs. 0.075).
We know from results of CVA and BLEU
analyses that for both groups of speakers, higher-
scoring essays are more lexically similar to the
prompts. Thus, the lack of a trend for native
speakers must be due to lack of increase in struc-
tural similarity between higher-scoring essays and
the prompts. Since better essays are presumably
better at expressing the content of the prompts, we
can hypothesize that native speakers paraphrase the
content more than non-native speakers.
</bodyText>
<subsectionHeader confidence="0.99651">
4.2 Difference between lecture and reading
</subsectionHeader>
<bodyText confidence="0.999846666666667">
The most informative measure of speaker behavior
was the difference between the Average Maximum
Cosine with the reading and the lecture, calculated
by subtracting the lecture AMC from the reading
AMC. Here, non-native speakers showed a sig-
nificant downward linear trend with increasing
</bodyText>
<page confidence="0.996576">
51
</page>
<bodyText confidence="0.968671294117647">
score (F=6.5; partial eta-squared 0.08), whereas the
native speakers did not show any trend (F=1.5).
The AMC differences are plotted in Figure 3.
Figure 2 - AMC difference between reading and
lecture
Non-native speakers with lower scores rely
mostly on the reading to produce their response,
whereas speakers with higher scores rely some-
what more on the lecture than on the reading. By
contrast, native speakers show no correlation be-
tween score and reading vs. lecture similarity.
Thus, there is a significant difference in the overall
distribution and behavior between native and non-
native speaker populations. This difference also
shows that human raters rely on information other
than simple verbatim similarity to the lecture in
assigning the overall scores.
</bodyText>
<subsectionHeader confidence="0.994874">
4.3 Other parameters of variation
</subsectionHeader>
<bodyText confidence="0.999973909090909">
For non-native speakers, the best predictor of the
human-rated score is the difference in AMC be-
tween the reading and the lecture.
As demonstrated in the previous section, the
AMC difference does not predict the score for na-
tive speakers. We analyzed native speaker demo-
graphic data in order to find any other possible
predictors. The students&apos; overall listening score,
their status as monolingual vs. bilingual, their par-
ents&apos; educational levels all failed to predict the es-
say scores.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999817486486487">
The Average Maximum Cosine measure as de-
scribed in this paper successfully characterizes the
behavior of native vs. non-native speaker popula-
tions on an integrated writing task. Less skillful
non-native speakers show a significant trend of
relying on the easier, more available prompt (the
reading) than on the harder prompt (the lecture),
whereas more skillful readers view the lecture as
more relevant and rely on it more than on the read-
ing. This difference can be due to better listening
comprehension for the lecture and/or better mem-
ory. By contrast, native speakers rely on both the
reading and the lecture about the same, and show
no significant trend across skill levels. Native
speakers seem to deviate more from the structure
of the original prompts while keeping the same
content, signaling better paraphrasing skills.
While not a direct measure of gist similarity,
this technique represents a first step toward detect-
ing paraphrases in written text. In the immediate
future, we plan to extend the set of features to in-
clude non-verbatim similarity, such as synonyms
and words derived by LSA-type comparison (Lan-
dauer et al. 1998). In addition, the syntactic fea-
tures will be expanded to include frequent
grammatical alternations such as active / passive.
A rather simple measure such as AMC has al-
ready revealed differences in population distribu-
tions for native vs. non-native speakers.
Extensions of this method can potentially be used
to determine if a given essay was written by a na-
tive or a non-native speaker. For instance, a statis-
tical classifier can be trained to distinguish feature
sets characteristic for different populations. Such a
classifier can be useful in a number of NLP-related
fields, including information extraction, search,
and, of course, educational measurement.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999688235294118">
Campbell, C. 1987. Writing with Others&apos; Words: Native
and Non-Native University Students&apos; Use of Infor-
mation from a Background Reading Text in Aca-
demic Compositions. Technical Report, UCLA
Center for Language Education and Research.
Landauer, T.; Foltz, P. W; and Laham. D. 1998. Intro-
duction to Latent Semantic Analysis. Discourse
Processes 25: 259-284.
Leacock, C., &amp; Chodorow, M. 2003. C-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37(4), 389-405.
Papineni, K; Roukos, S.; Ward, T. and Zhu, W-J. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. ACL `02, p. 311-318.
Salton, G. 1989. Automatic Text Processing: The Trans-
formation, Analysis, and Retrieval of Information by
Computer. Reading, MA: Addison-Weley.
</reference>
<figure confidence="0.938537090909091">
AMC difference between reading and
lecture
0.15
0.1
0.05
0
-0.05
0 1 2 3 4 5
Score
Non-native
Native
</figure>
<page confidence="0.947823">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.154060">
<title confidence="0.993894">Document Similarity Measures to Native vs. Non-Native Essay Writers</title>
<author confidence="0.787069">Olga</author>
<affiliation confidence="0.689227">Educational Testing Rosedale &amp; Carter Turnbull</affiliation>
<address confidence="0.999647">Princeton, NJ 08541</address>
<email confidence="0.994077">ogurevich@ets.org</email>
<abstract confidence="0.9980805">The ability to distinguish statistically different populations of speakers or writers can be an important asset in many NLP applications. In this paper, we describe a method of using document similarity measures to describe differences in behavior between native and non-native</abstract>
<intro confidence="0.560795">of English in a writing</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Campbell</author>
</authors>
<title>Writing with Others&apos; Words: Native and Non-Native University Students&apos; Use of Information from a Background Reading Text in Academic Compositions.</title>
<date>1987</date>
<tech>Technical Report,</tech>
<institution>UCLA Center for Language Education and Research.</institution>
<contexts>
<context position="5416" citStr="Campbell 1987" startWordPosition="851" endWordPosition="852">ical and stylistic differences between different essays. We do this by comparing the essays to the reading and lecture prompts. Our end goal is to determine to what extent speakers diverge from the prompts while retaining the content. The prediction is that native speakers are much more likely to paraphrase the prompts while keeping the same gist, whereas non-native speakers are likely to either repeat the prompts close to verbatim, or diverge from them in ways that do not preserve the gist. This intuition conforms to previous studies of native vs. non-native speakers&apos; text summarization (cf. Campbell 1987), although we are not aware of any related computational work. We begin by measuring lexico-grammatical similarity between each essay and the two prompts. An essay is represented as a set of features derived from its lexico-grammatical content, as described below. The resulting comparison measure goes beyond simple word or n-gram overlap by providing a measure of structural similarity as well. In essence, our method measures to what extent the essay expresses the content of the prompt in the same words, used in the same syntactic positions. 3.1 C-rater tuples In order to get a measure of synta</context>
</contexts>
<marker>Campbell, 1987</marker>
<rawString>Campbell, C. 1987. Writing with Others&apos; Words: Native and Non-Native University Students&apos; Use of Information from a Background Reading Text in Academic Compositions. Technical Report, UCLA Center for Language Education and Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<marker>D, 1998</marker>
<rawString>Landauer, T.; Foltz, P. W; and Laham. D. 1998. Introduction to Latent Semantic Analysis. Discourse Processes 25: 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>C-rater: Scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>389--405</pages>
<contexts>
<context position="6079" citStr="Leacock &amp; Chodorow 2003" startWordPosition="956" endWordPosition="959">ted computational work. We begin by measuring lexico-grammatical similarity between each essay and the two prompts. An essay is represented as a set of features derived from its lexico-grammatical content, as described below. The resulting comparison measure goes beyond simple word or n-gram overlap by providing a measure of structural similarity as well. In essence, our method measures to what extent the essay expresses the content of the prompt in the same words, used in the same syntactic positions. 3.1 C-rater tuples In order to get a measure of syntactic similarity, we relied on C-rater (Leacock &amp; Chodorow 2003), an automatic scoring engine developed at ETS. Crater includes several basic NLP components, including POS tagging, morphological processing, anaphora resolution, and shallow parsing. The parsing produces tuples for each clause, which describe each verb and its syntactic arguments (1). (1) CLAUSE: the group spreads responsibility for a decision to all the members TUPLE: :verb: spread :subj: the group :obj: responsible :pp.for: for a decide :pp.to: to all C-rater does not produce full-sentence trees or prepositional phrase attachment. However, the tuples are reasonably accurate on non-native i</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Leacock, C., &amp; Chodorow, M. 2003. C-rater: Scoring of short-answer questions. Computers and the Humanities, 37(4), 389-405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<journal>ACL</journal>
<volume>02</volume>
<pages>311--318</pages>
<contexts>
<context position="9595" citStr="Papineni et al. 2002" startWordPosition="1519" endWordPosition="1523">= MSlinear trend/MSwithin-subjects=63 for the reading; F=71 for the lecture at 0.05 significance level3). Thus, the rate of vocabulary retention from both prompts increases with higher English-language skill level. Native speakers showed a similar pattern of increasing cosine similarity between the essay and the reading (F=35 at 0.05 significance for the trend), and the lecture (F=35 at the 0.05 level). BLEU score In order to measure the extent to which whole chunks of text from the prompt are reproduced in the student essays, we used the BLEU score, known from studies of machine translation (Papineni et al. 2002). We used whole essays as sections of text rather than individual sentences. For non-native speakers, the trend was similar to that found with CVA: at higher score levels, the 3 All statistical calculations were performed as ANOVA-style trend analyses using SPSS. overlap between the essays and both prompts increased (F=52.4 at the 0.05 level for the reading; F=53.6 for the lecture). Native speakers again showed a similar pattern, with a significant trend towards more similarity to the reading (F=35.6) and the lecture (F=31.3). These results were confirmed by a simple n-gram overlap measure. 4 </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K; Roukos, S.; Ward, T. and Zhu, W-J. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. ACL `02, p. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Weley.</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="7345" citStr="Salton 1989" startWordPosition="1155" endWordPosition="1156">ces tuples for each document, often several per sentence. For the current experiment, we used the main verb, its subject and object. We then converted each tuple into a set of features, which included the following: • The verb, subject (pro)noun, and object (pro)noun as individual words; • All of the words together as a single feature; • The verb, subject, and object words with their argument roles. Each document can now be represented as a set of tuple-derived features, or feature vectors. 3.3 Document Comparison Two feature vectors derived from tuples can be compared using a cosine measure (Salton 1989). The closer to 1 the cosine, the more similar the two feature sets. To compensate for different frequencies of the features and for varying document lengths, the feature vectors are weighted using standard tf*idf techniques. 20 30 25 35 10 15 0 5 1 2 3 4 5 Percent score frequencies Score Non-native Native 50 In order to estimate the similarity between two documents, we use the following procedure. For each tuple vector in Document A, we find the tuple in Document B with the maximum cosine to the tuple in Document A. The maximum cosine values for each tuple are then averaged, resulting in a si</context>
<context position="8618" citStr="Salton 1989" startWordPosition="1368" endWordPosition="1369">e Maximum Cosine (AMC). We calculated AMCs for each student response versus the reading, the lecture, and the reading + lecture combined. This procedure was performed for both native and non-native essays. A detailed examination of the resulting trends is in section 4. 3.4 Other Measures of Document Similarity We also performed several measures of document similarity that did not include syntactic features. Content Vector Analysis The student essays and the prompts were compared using Content Vector Analysis (CVA), where each document was represented as a vector consisting of the words in it (Salton 1989). The tf*idf-weighted vectors were compared by a cosine measure. For non-native speakers, there was a noticeable trend. At higher score levels (where the score is determined by a human rater), student essays showed more similarity to both the reading and the lecture prompts. Both the reading and lecture similarity trends were significant (linear trend; F= MSlinear trend/MSwithin-subjects=63 for the reading; F=71 for the lecture at 0.05 significance level3). Thus, the rate of vocabulary retention from both prompts increases with higher English-language skill level. Native speakers showed a simi</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Reading, MA: Addison-Weley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>