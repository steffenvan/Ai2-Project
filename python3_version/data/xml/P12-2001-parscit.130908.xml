<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059316">
<title confidence="0.987285">
Higher-Order Constituent Parsing and Parser Combination*
</title>
<author confidence="0.993511">
Xiao Chen and Chunyu Kit
</author>
<affiliation confidence="0.850305333333333">
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong SAR, China
</affiliation>
<email confidence="0.996371">
{cxiao2,ctckit}@cityu.edu.hk
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999167181818182">
This paper presents a higher-order model for
constituent parsing aimed at utilizing more lo-
cal structural context to decide the score of
a grammar rule instance in a parse tree. Ex-
periments on English and Chinese treebanks
confirm its advantage over its first-order ver-
sion. It achieves its best F1 scores of 91.86%
and 85.58% on the two languages, respec-
tively, and further pushes them to 92.80%
and 85.60% via combination with other high-
performance parsers.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999567411764706">
Factorization is crucial to discriminative parsing.
Previous discriminative parsing models usually fac-
tor a parse tree into a set of parts. Each part is scored
separately to ensure tractability. In dependency
parsing (DP), the number of dependencies in a part
is called the order of a DP model (Koo and Collins,
2010). Accordingly, existing graph-based DP mod-
els can be categorized into tree groups, namely, the
first-order (Eisner, 1996; McDonald et al., 2005a;
McDonald et al., 2005b), second-order (McDonald
and Pereira, 2006; Carreras, 2007) and third-order
(Koo and Collins, 2010) models.
Similarly, we can define the order of constituent
parsing in terms of the number of grammar rules
in a part. Then, the previous discriminative con-
stituent parsing models (Johnson, 2001; Henderson,
2004; Taskar et al., 2004; Petrov and Klein, 2008a;
</bodyText>
<footnote confidence="0.979685333333333">
*The research reported in this paper was partially supported
by the Research Grants Council of HKSAR, China, through the
GRF Grant 9041597 (CityU 144410).
</footnote>
<page confidence="0.889301">
1
</page>
<bodyText confidence="0.999522545454545">
Petrov and Klein, 2008b; Finkel et al., 2008) are the
first-order ones, because there is only one grammar
rule in a part. The discriminative re-scoring models
(Collins, 2000; Collins and Duffy, 2002; Charniak
and Johnson, 2005; Huang, 2008) can be viewed as
previous attempts to higher-order constituent pars-
ing, using some parts containing more than one
grammar rule as non-local features.
In this paper, we present a higher-order con-
stituent parsing model1 based on these previous
works. It allows multiple adjacent grammar rules
in each part of a parse tree, so as to utilize more
local structural context to decide the plausibility of
a grammar rule instance. Evaluated on the PTB
WSJ and Chinese Treebank, it achieves its best F1
scores of 91.86% and 85.58%, respectively. Com-
bined with other high-performance parsers under
the framework of constituent recombination (Sagae
and Lavie, 2006; Fossum and Knight, 2009), this
model further enhances the F1 scores to 92.80% and
85.60%, the highest ones achieved so far on these
two data sets.
</bodyText>
<sectionHeader confidence="0.973892" genericHeader="method">
2 Higher-order Constituent Parsing
</sectionHeader>
<bodyText confidence="0.986185666666667">
Discriminative parsing is aimed to learn a function
f : S —* T from a set of sentences S to a set of valid
parses T according to a given CFG, which maps an
input sentence s E S to a set of candidate parses
T (s). The function takes the following discrimina-
tive form:
</bodyText>
<equation confidence="0.997155">
f(s) = arg max g(t, s) (1)
tET(s)
</equation>
<footnote confidence="0.960597">
1http://code.google.com/p/gazaparser/
</footnote>
<note confidence="0.8847">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–5,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.510381">
begin(b) split(m) end(e)
</figure>
<figureCaption confidence="0.999652">
Figure 1: A part of a parse tree centered at NP → NP VP
</figureCaption>
<bodyText confidence="0.9998015">
where g(t, s) is a scoring function to evaluate the
event that t is the parse of s. Following Collins
(2002), this scoring function is formulated in the lin-
ear form
</bodyText>
<equation confidence="0.972042">
g(t, s) = B · Ψ(t, s), (2)
</equation>
<bodyText confidence="0.999041333333333">
where Ψ(t, s) is a vector of features and B the vector
of their associated weights. To ensure tractability,
this model is factorized as
</bodyText>
<equation confidence="0.9774995">
g(t, s) = � g(Q(r), s) = � B · Φ(Q(r), s), (3)
rEt rEt
</equation>
<bodyText confidence="0.999641166666667">
where g(Q(r), s) scores Q(r), a part centered at
grammar rule instance r in t, and Φ(Q(r), s) is the
vector of features for Q(r). Each Q(r) makes its
own contribution to g(t, s). A part in a parse tree
is illustrated in Figure 1. It consists of the center
grammar rule instance NP → NP VP and a set of im-
mediate neighbors, i.e., its parent PP → IN NP, its
children NP → DT QP and VP → VBN PP, and its
sibling IN → of. This set of neighboring rule in-
stances forms a local structural context to provide
useful information to determine the plausibility of
the center rule instance.
</bodyText>
<subsectionHeader confidence="0.957017">
2.1 Feature
</subsectionHeader>
<bodyText confidence="0.998705166666667">
The feature vector Φ(Q(r), s) consists of a series
of features {oi(Q(r), s))|i ≥ 0}. The first feature
Oo(Q(r), s) is calculated with a PCFG-based gen-
erative parsing model (Petrov and Klein, 2007), as
defined in (4) below, where r is the grammar rule in-
stance A → B C that covers the span from the b-th
</bodyText>
<equation confidence="0.882063">
00(Q(r), s) =
</equation>
<bodyText confidence="0.999653176470588">
to the e-th word, splitting at the m-th word, x, y and
z are latent variables in the PCFG-based model, and
I(·) and O(·) are the inside and outside probabili-
ties, respectively.
All other features Oi(Q(r), s) are binary func-
tions that indicate whether a configuration exists in
Q(r) and s. These features are by their own na-
ture in two categories, namely, lexical and structural.
All features extracted from the part in Figure 1 are
demonstrated in Table 1. Some back-off structural
features are used for smoothing, which cannot be
presented due to limited space. With only lexical
features in a part, this parsing model backs off to a
first-order one similar to those in the previous works.
Adding structural features, each involving a least a
neighboring rule instance, makes it a higher-order
parsing model.
</bodyText>
<subsectionHeader confidence="0.996481">
2.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999979125">
The factorization of the parsing model allows us to
develop an exact decoding algorithm for it. Follow-
ing Huang (2008), this algorithm traverses a parse
forest in a bottom-up manner. However, it deter-
mines and keeps the best derivation for every gram-
mar rule instance instead of for each node. Be-
cause all structures above the current rule instance
is not determined yet, the computation of its non-
local structural features, e.g., parent and sibling fea-
tures, has to be delayed until it joins an upper level
structure. For example, when computing the score
of a derivation under the center rule NP → NP VP
in Figure 1, the algorithm will extract child features
from its children NP → DT QP and VP → VBN PP.
The parent and sibling features of the two child rules
can also be extracted from the current derivation and
used to calculate the score of this derivation. But
parent and sibling features for the center rule will
not be computed until the decoding process reaches
the rule above, i.e., PP → IN NP.
This algorithm is more complex than the approx-
imate decoding algorithm of Huang (2008). How-
ever, its efficiency heavily depends on the size of the
parse forest it has to handle. Forest pruning (Char-
</bodyText>
<figure confidence="0.984180315789474">
PP
NP
NP VP
QP VBN PP
$ 32 million realized from the sales will be ...
...
a portion of
IN
the
DT
O(Ax, b, e)P(Ax → By Cz)I(By, b, m)I(Cz, m, e)
(4)
I(S,0,n)
�
x
�
z
E
y
</figure>
<page confidence="0.980732">
2
</page>
<table confidence="0.999671047619048">
Template Description Comments
Lexical N-gram on inner wb/e+l(l=0,1,2,3,4) &amp; b/e &amp; l &amp; NP Similar to the distributional
feature /outer edge wb/e−l(l=1,2,3,4,5) &amp; b/e &amp; l &amp; NP similarity cluster bigrams
wb/e+lwb/e+l+1(l=0,1,2,3) &amp; b/e &amp; l &amp; NP features in Finkel et al. (2008)
wb/e−l−1wb/e−l(l=1,2,3,4) &amp; b/e &amp; l &amp; NP
wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) &amp; b/e &amp; l &amp; NP
wb/e−l−2wb/e−l−1wb/e−l(l=1,2,3) &amp; b/e &amp; l &amp; NP
Bigram on edges wb/e−1wb/e &amp; NP Similar to the lexical span
features in Taskar et al. (2004)
and Petrov and Klein (2008b)
Split pair wm−1wm &amp; NP → NP VP
Inner/Outer pair wbwe−1 &amp; NP → NP VP
wb−1we &amp; NP → NP VP
Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features
Right &amp; NP &amp; NP in Collins (2000)
Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent
feature rules features in Collins (2000)
Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP
NP → DT QP &amp; NP → NP VP
VP → VBN PP &amp; NP → NP VP
Sibling Left &amp; IN → of &amp; NP → NP VP
</table>
<tableCaption confidence="0.999902">
Table 1: Examples of lexical and structural feature
</tableCaption>
<bodyText confidence="0.964355571428571">
niak and Johnson, 2005; Petrov and Klein, 2007)
is therefore adopted in our implementation for ef-
ficiency enhancement. A parallel decoding strategy
is also developed to further improve the efficiency
without loss of optimality. Interested readers can re-
fer to Chen (2012) for more technical details of this
algorithm.
</bodyText>
<sectionHeader confidence="0.990989" genericHeader="method">
3 Constituent Recombination
</sectionHeader>
<bodyText confidence="0.998781166666667">
Following Fossum and Knight (2009), our con-
stituent weighting scheme for parser combination
uses multiple outputs of independent parsers. Sup-
pose each parser generates a k-best parse list for an
input sentence, the weight of a candidate constituent
c is defined as
</bodyText>
<equation confidence="0.9936895">
�W(c) _ E AiS(c, ti,k)f(ti,k), (5)
i k
</equation>
<bodyText confidence="0.999901272727273">
where i is the index of an individual parser, Ai
the weight indicating the confidence of a parser,
S(c, ti,k) a binary function indicating whether c is
contained in ti,k, the k-th parse output from the i-
th parser, and f(ti,k) the score of the k-th parse as-
signed by the i-th parser, as defined in Fossum and
Knight (2009).
The weight of a recombined parse is defined as the
sum of weights of all constituents in the parse. How-
ever, this definition has a systematic bias towards se-
lecting a parse with as many constituents as possible
</bodyText>
<table confidence="0.85945125">
English Chinese
Train. Section 2-21 Art. 1-270,400-1151
Dev. Section 22/24 Art. 301-325
Test. Section 23 Art. 271-300
</table>
<tableCaption confidence="0.995704">
Table 2: Experiment Setup
</tableCaption>
<bodyText confidence="0.999971142857143">
for the highest weight. A pruning threshold p, simi-
lar to the one in Sagae and Lavie (2006), is therefore
needed to restrain the number of constituents in a re-
combined parse. The parameters Ai and p are tuned
by the Powell’s method (Powell, 1964) on a develop-
ment set, using the F1 score of PARSEVAL (Black
et al., 1991) as objective.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999920214285714">
Our parsing models are evaluated on both English
and Chinese treebanks, i.e., the WSJ section of Penn
Treebank 3.0 (LDC99T42) and the Chinese Tree-
bank 5.1 (LDC2005T01U01). In order to compare
with previous works, we opt for the same split as
in Petrov and Klein (2007), as listed in Table 2. For
parser combination, we follow the setting of Fossum
and Knight (2009), using Section 24 instead of Sec-
tion 22 of WSJ treebank as development set.
In this work, the lexical model of Chen and Kit
(2011) is combined with our syntactic model under
the framework of product-of-experts (Hinton, 2002).
A factor A is introduced to balance the two models.
It is tuned on a development set using the gold sec-
</bodyText>
<page confidence="0.997478">
3
</page>
<table confidence="0.999094333333333">
English Chinese
R(%) P(%) F1(%) R(%) P(%) F1(%)
Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22
First-order 91.33 91.79 91.56 84.14 86.23 85.17
Higher-order 91.62 92.11 91.86 84.24 86.54 85.37
Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58
Stanford parser - - - 77.40 79.57 78.47
C&amp;J parser 91.04 91.76 91.40 - - -
Conbination 92.02 93.60 92.80 82.44 89.01 85.60
</table>
<tableCaption confidence="0.99965">
Table 3: The performance of our parsing models on the English and Chinese test sets.
</tableCaption>
<table confidence="0.99993219047619">
System F1(%) EX(%)
Single
Charniak (2000) 89.70
Berkeley parser 89.87 36.7
Bod (2003) 90.70
Carreras et al. (2008) 91.1
Re-scoring
Collins (2000) 89.70
Charniak and Johnson (2005) 91.02
The parser of Charniak and Johnson 91.40 43.54
Huang (2008) 91.69 43.5
Combination
Fossum and Knight (2009) 92.4
Zhang et al. (2009) 92.3
Petrov (2010) 91.85 41.9
Self-training
Zhang et al. (2009) (s.t.+combo) 92.62
Huang et al. (2010) (single) 91.59 40.3
Huang et al. (2010) (combo) 92.39 43.1
Our single 91.86 40.89
Our combo 92.80 41.60
</table>
<tableCaption confidence="0.99987">
Table 4: Performance comparison on the English test set
</tableCaption>
<bodyText confidence="0.999566142857143">
tion search algorithm (Kiefer, 1953). The parame-
ters 0 of each parsing model are estimated from a
training set using an averaged perceptron algorithm,
following Collins (2002) and Huang (2008).
The performance of our first- and higher-order
parsing models on all sentences of the two test sets
is presented in Table 3, where A indicates a tuned
balance factor. This parser is also combined with
the parser of Charniak and Johnson (2005)2 and the
Stanford. parser3 The best combination results in
Table 3 are achieved with k=70 for English and
k=100 for Chinese for selecting the k-best parses.
Our results are compared with the best previous ones
on the same test sets in Tables 4 and 5. All scores
</bodyText>
<footnote confidence="0.9826635">
2ftp://ftp.cs.brown.edu/pub/nlparser/
3http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<table confidence="0.9997966">
System F1(%) EX(%)
Single
Charniak (2000) 80.85
Stanford parser 78.47 26.44
Berkeley parser 83.22 31.32
Burkett and Klein (2008) 84.24
Combination
Zhang et al. (2009) (combo) 85.45
Our single 85.56 31.61
Our combo 85.60 29.02
</table>
<tableCaption confidence="0.999669">
Table 5: Performance comparison on the Chinese test set
</tableCaption>
<bodyText confidence="0.885829">
listed in these tables are calculated with evalb,4
and EX is the complete match rate.
</bodyText>
<sectionHeader confidence="0.99356" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99996835">
This paper has presented a higher-order model for
constituent parsing that factorizes a parse tree into
larger parts than before, in hopes of increasing its
power of discriminating the true parse from the oth-
ers without losing tractability. A performance gain
of 0.3%-0.4% demonstrates its advantage over its
first-order version. Including a PCFG-based model
as its basic feature, this model achieves a better
performance than previous single and re-scoring
parsers, and its combination with other parsers per-
forms even better (by about 1%). More importantly,
it extends the existing works into a more general
framework of constituent parsing to utilize more
lexical and structural context and incorporate more
strength of various parsing techniques. However,
higher-order constituent parsing inevitably leads to
a high computational complexity. We intend to deal
with the efficiency problem of our model with some
advanced parallel computing technologies in our fu-
ture works.
</bodyText>
<footnote confidence="0.977829">
4http://nlp.cs.nyu.edu/evalb/
</footnote>
<page confidence="0.996551">
4
</page>
<sectionHeader confidence="0.990053" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896514563106">
E. Black, S. Abney, D. Flickenger, R. Grishman, P. Har-
rison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans,
M. Liberman, M. Marcus, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of DARPA Speech and Nat-
ural Language Workshop, pages 306–311.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In EACL 2003, pages 19–26.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In EMNLP
2008, pages 877–886.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In CoNLL 2008, pages
9–16.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP-CoNLL
2007, pages 957–961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In ACL 2005, pages 173–180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In NAACL 2000, pages 132–139.
Xiao Chen and Chunyu Kit. 2011. Improving part-of-
speech tagging for context-free parsing. In IJCNLP
2011, pages 1260–1268.
Xiao Chen. 2012. Discriminative Constituent Parsing
with Localized Features. Ph.D. thesis, City University
of Hong Kong.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In ACL
2002, pages 263–270.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In ICML 2000, pages 175–182.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In EMNLP 2002, pages
1–8.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996, pages 340–345.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In ACL-HLT 2008, pages 959–
967.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In NAACL-HLT 2009, pages 253–
256.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In ACL 2004, pages
95–102.
Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771–1800.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable gram-
mars. In EMNLP 2010, pages 12–22.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL-HLT 2008,
pages 586–594.
Mark Johnson. 2001. Joint and conditional estimation
of tagging and parsing models. In ACL 2001, pages
322–329.
J. Kiefer. 1953. Sequential minimax search for a maxi-
mum. Proceedings of the American Mathematical So-
ciety, 4:502–506.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL 2010, pages 1–11.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In EACL 2006, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In ACL 2005, pages 91–98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In EMNLP-HLT
2005, pages 523–530.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT 2007, pages
404–411.
Slav Petrov and Dan Klein. 2008a. Discriminative log-
linear grammars with latent variables. In NIPS 20,
pages 1–8.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing. In
EMNLP 2008, pages 867–876.
Slav Petrov. 2010. Products of random latent variable
grammars. In NAACL-HLT 2010, pages 19–27.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Computer Journal, 7(2):155–
162.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In NAACL-HLT 2006, pages 129–132.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
EMNLP 2004, pages 1–8.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In EMNLP 2009, pages 1552–1560.
</reference>
<page confidence="0.99436">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512497">
<title confidence="0.99861">Constituent Parsing and Parser</title>
<author confidence="0.986133">Xiao Chen</author>
<author confidence="0.986133">Chunyu</author>
<affiliation confidence="0.897005">Department of Chinese, Translation and City University of Hong</affiliation>
<author confidence="0.731485">Tat Chee Avenue</author>
<author confidence="0.731485">Hong Kong SAR Kowloon</author>
<abstract confidence="0.998148083333333">This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order ver- It achieves its best F1 scores of on the two languages, respecand further pushes them to via combination with other highperformance parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="9552" citStr="Black et al., 1991" startWordPosition="1671" endWordPosition="1674">f weights of all constituents in the parse. However, this definition has a systematic bias towards selecting a parse with as many constituents as possible English Chinese Train. Section 2-21 Art. 1-270,400-1151 Dev. Section 22/24 Art. 301-325 Test. Section 23 Art. 271-300 Table 2: Experiment Setup for the highest weight. A pruning threshold p, similar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. The parameters Ai and p are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-expe</context>
</contexts>
<marker>Black, Abney, Flickenger, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of DARPA Speech and Natural Language Workshop, pages 306–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of a new DOP model.</title>
<date>2003</date>
<booktitle>In EACL</booktitle>
<pages>pages</pages>
<contexts>
<context position="10818" citStr="Bod (2003)" startWordPosition="1888" endWordPosition="1889">the two models. It is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a train</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of a new DOP model. In EACL 2003, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<pages>877--886</pages>
<contexts>
<context position="12231" citStr="Burkett and Klein (2008)" startWordPosition="2106" endWordPosition="2109">s is presented in Table 3, where A indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005)2 and the Stanford. parser3 The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses. Our results are compared with the best previous ones on the same test sets in Tables 4 and 5. All scores 2ftp://ftp.cs.brown.edu/pub/nlparser/ 3http://nlp.stanford.edu/software/lex-parser.shtml System F1(%) EX(%) Single Charniak (2000) 80.85 Stanford parser 78.47 26.44 Berkeley parser 83.22 31.32 Burkett and Klein (2008) 84.24 Combination Zhang et al. (2009) (combo) 85.45 Our single 85.56 31.61 Our combo 85.60 29.02 Table 5: Performance comparison on the Chinese test set listed in these tables are calculated with evalb,4 and EX is the complete match rate. 5 Conclusion This paper has presented a higher-order model for constituent parsing that factorizes a parse tree into larger parts than before, in hopes of increasing its power of discriminating the true parse from the others without losing tractability. A performance gain of 0.3%-0.4% demonstrates its advantage over its first-order version. Including a PCFG-</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In EMNLP 2008, pages 877–886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In CoNLL</title>
<date>2008</date>
<pages>9--16</pages>
<contexts>
<context position="10847" citStr="Carreras et al. (2008)" startWordPosition="1891" endWordPosition="1894">t is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged per</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In CoNLL 2008, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser. In EMNLP-CoNLL</title>
<date>2007</date>
<pages>957--961</pages>
<contexts>
<context position="1267" citStr="Carreras, 2007" startWordPosition="192" endWordPosition="193">0% via combination with other highperformance parsers. 1 Introduction Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-s</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In EMNLP-CoNLL 2007, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1948" citStr="Charniak and Johnson, 2005" startWordPosition="298" endWordPosition="301">rly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectively. Combined with other high-performance parsers und</context>
<context position="10912" citStr="Charniak and Johnson (2005)" startWordPosition="1900" endWordPosition="1903">Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In ACL 2005, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL</booktitle>
<pages>132--139</pages>
<contexts>
<context position="10774" citStr="Charniak (2000)" startWordPosition="1881" endWordPosition="1882">nton, 2002). A factor A is introduced to balance the two models. It is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of e</context>
<context position="12144" citStr="Charniak (2000)" startWordPosition="2095" endWordPosition="2096">ur first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where A indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005)2 and the Stanford. parser3 The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses. Our results are compared with the best previous ones on the same test sets in Tables 4 and 5. All scores 2ftp://ftp.cs.brown.edu/pub/nlparser/ 3http://nlp.stanford.edu/software/lex-parser.shtml System F1(%) EX(%) Single Charniak (2000) 80.85 Stanford parser 78.47 26.44 Berkeley parser 83.22 31.32 Burkett and Klein (2008) 84.24 Combination Zhang et al. (2009) (combo) 85.45 Our single 85.56 31.61 Our combo 85.60 29.02 Table 5: Performance comparison on the Chinese test set listed in these tables are calculated with evalb,4 and EX is the complete match rate. 5 Conclusion This paper has presented a higher-order model for constituent parsing that factorizes a parse tree into larger parts than before, in hopes of increasing its power of discriminating the true parse from the others without losing tractability. A performance gain </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In NAACL 2000, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Chen</author>
<author>Chunyu Kit</author>
</authors>
<title>Improving part-ofspeech tagging for context-free parsing.</title>
<date>2011</date>
<booktitle>In IJCNLP 2011,</booktitle>
<pages>1260--1268</pages>
<contexts>
<context position="10076" citStr="Chen and Kit (2011)" startWordPosition="1763" endWordPosition="1766">method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002). A factor A is introduced to balance the two models. It is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of </context>
</contexts>
<marker>Chen, Kit, 2011</marker>
<rawString>Xiao Chen and Chunyu Kit. 2011. Improving part-ofspeech tagging for context-free parsing. In IJCNLP 2011, pages 1260–1268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Chen</author>
</authors>
<title>Discriminative Constituent Parsing with Localized Features.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>City University of Hong Kong.</institution>
<contexts>
<context position="8175" citStr="Chen (2012)" startWordPosition="1434" endWordPosition="1435">grams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. Interested readers can refer to Chen (2012) for more technical details of this algorithm. 3 Constituent Recombination Following Fossum and Knight (2009), our constituent weighting scheme for parser combination uses multiple outputs of independent parsers. Suppose each parser generates a k-best parse list for an input sentence, the weight of a candidate constituent c is defined as �W(c) _ E AiS(c, ti,k)f(ti,k), (5) i k where i is the index of an individual parser, Ai the weight indicating the confidence of a parser, S(c, ti,k) a binary function indicating whether c is contained in ti,k, the k-th parse output from the ith parser, and f(t</context>
</contexts>
<marker>Chen, 2012</marker>
<rawString>Xiao Chen. 2012. Discriminative Constituent Parsing with Localized Features. Ph.D. thesis, City University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1920" citStr="Collins and Duffy, 2002" startWordPosition="294" endWordPosition="297">ins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectively. Combined with other </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL 2002, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1895" citStr="Collins, 2000" startWordPosition="292" endWordPosition="293">r (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectiv</context>
<context position="7612" citStr="Collins (2000)" startWordPosition="1319" endWordPosition="1320">o the distributional feature /outer edge wb/e−l(l=1,2,3,4,5) &amp; b/e &amp; l &amp; NP similarity cluster bigrams wb/e+lwb/e+l+1(l=0,1,2,3) &amp; b/e &amp; l &amp; NP features in Finkel et al. (2008) wb/e−l−1wb/e−l(l=1,2,3,4) &amp; b/e &amp; l &amp; NP wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) &amp; b/e &amp; l &amp; NP wb/e−l−2wb/e−l−1wb/e−l(l=1,2,3) &amp; b/e &amp; l &amp; NP Bigram on edges wb/e−1wb/e &amp; NP Similar to the lexical span features in Taskar et al. (2004) and Petrov and Klein (2008b) Split pair wm−1wm &amp; NP → NP VP Inner/Outer pair wbwe−1 &amp; NP → NP VP wb−1we &amp; NP → NP VP Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. Interested readers can refer to Chen (2012) for more technical details of this a</context>
<context position="10878" citStr="Collins (2000)" startWordPosition="1897" endWordPosition="1898">he gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Co</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML 2000, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3518" citStr="Collins (2002)" startWordPosition="564" endWordPosition="565">id parses T according to a given CFG, which maps an input sentence s E S to a set of candidate parses T (s). The function takes the following discriminative form: f(s) = arg max g(t, s) (1) tET(s) 1http://code.google.com/p/gazaparser/ Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–5, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics begin(b) split(m) end(e) Figure 1: A part of a parse tree centered at NP → NP VP where g(t, s) is a scoring function to evaluate the event that t is the parse of s. Following Collins (2002), this scoring function is formulated in the linear form g(t, s) = B · Ψ(t, s), (2) where Ψ(t, s) is a vector of features and B the vector of their associated weights. To ensure tractability, this model is factorized as g(t, s) = � g(Q(r), s) = � B · Φ(Q(r), s), (3) rEt rEt where g(Q(r), s) scores Q(r), a part centered at grammar rule instance r in t, and Φ(Q(r), s) is the vector of features for Q(r). Each Q(r) makes its own contribution to g(t, s). A part in a parse tree is illustrated in Figure 1. It consists of the center grammar rule instance NP → NP VP and a set of immediate neighbors, i.</context>
<context position="11490" citStr="Collins (2002)" startWordPosition="1995" endWordPosition="1996">0) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The performance of our first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where A indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005)2 and the Stanford. parser3 The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses. Our results are compared with the best previous ones on the same test sets in Tables 4 and 5. All scores 2ftp://ftp.cs.brown.edu/pub/nlparser/ 3http://nlp.stanford.edu/software/lex-</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In EMNLP 2002, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In COLING</booktitle>
<pages>340--345</pages>
<contexts>
<context position="1159" citStr="Eisner, 1996" startWordPosition="177" endWordPosition="178">cores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers. 1 Introduction Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In COLING 1996, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In ACL-HLT</booktitle>
<pages>959--967</pages>
<contexts>
<context position="1767" citStr="Finkel et al., 2008" startWordPosition="270" endWordPosition="273">Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a gram</context>
<context position="7174" citStr="Finkel et al. (2008)" startWordPosition="1232" endWordPosition="1235">omplex than the approximate decoding algorithm of Huang (2008). However, its efficiency heavily depends on the size of the parse forest it has to handle. Forest pruning (CharPP NP NP VP QP VBN PP $ 32 million realized from the sales will be ... ... a portion of IN the DT O(Ax, b, e)P(Ax → By Cz)I(By, b, m)I(Cz, m, e) (4) I(S,0,n) � x � z E y 2 Template Description Comments Lexical N-gram on inner wb/e+l(l=0,1,2,3,4) &amp; b/e &amp; l &amp; NP Similar to the distributional feature /outer edge wb/e−l(l=1,2,3,4,5) &amp; b/e &amp; l &amp; NP similarity cluster bigrams wb/e+lwb/e+l+1(l=0,1,2,3) &amp; b/e &amp; l &amp; NP features in Finkel et al. (2008) wb/e−l−1wb/e−l(l=1,2,3,4) &amp; b/e &amp; l &amp; NP wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) &amp; b/e &amp; l &amp; NP wb/e−l−2wb/e−l−1wb/e−l(l=1,2,3) &amp; b/e &amp; l &amp; NP Bigram on edges wb/e−1wb/e &amp; NP Similar to the lexical span features in Taskar et al. (2004) and Petrov and Klein (2008b) Split pair wm−1wm &amp; NP → NP VP Inner/Outer pair wbwe−1 &amp; NP → NP VP wb−1we &amp; NP → NP VP Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In ACL-HLT 2008, pages 959– 967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Combining constituent parsers.</title>
<date>2009</date>
<booktitle>In NAACL-HLT 2009,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="2642" citStr="Fossum and Knight, 2009" startWordPosition="408" endWordPosition="411">tituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectively. Combined with other high-performance parsers under the framework of constituent recombination (Sagae and Lavie, 2006; Fossum and Knight, 2009), this model further enhances the F1 scores to 92.80% and 85.60%, the highest ones achieved so far on these two data sets. 2 Higher-order Constituent Parsing Discriminative parsing is aimed to learn a function f : S —* T from a set of sentences S to a set of valid parses T according to a given CFG, which maps an input sentence s E S to a set of candidate parses T (s). The function takes the following discriminative form: f(s) = arg max g(t, s) (1) tET(s) 1http://code.google.com/p/gazaparser/ Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–5, Jej</context>
<context position="8284" citStr="Fossum and Knight (2009)" startWordPosition="1447" endWordPosition="1450">ar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. Interested readers can refer to Chen (2012) for more technical details of this algorithm. 3 Constituent Recombination Following Fossum and Knight (2009), our constituent weighting scheme for parser combination uses multiple outputs of independent parsers. Suppose each parser generates a k-best parse list for an input sentence, the weight of a candidate constituent c is defined as �W(c) _ E AiS(c, ti,k)f(ti,k), (5) i k where i is the index of an individual parser, Ai the weight indicating the confidence of a parser, S(c, ti,k) a binary function indicating whether c is contained in ti,k, the k-th parse output from the ith parser, and f(ti,k) the score of the k-th parse assigned by the i-th parser, as defined in Fossum and Knight (2009). The wei</context>
<context position="9945" citStr="Fossum and Knight (2009)" startWordPosition="1738" endWordPosition="1741">6), is therefore needed to restrain the number of constituents in a recombined parse. The parameters Ai and p are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002). A factor A is introduced to balance the two models. It is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - </context>
</contexts>
<marker>Fossum, Knight, 2009</marker>
<rawString>Victoria Fossum and Kevin Knight. 2009. Combining constituent parsers. In NAACL-HLT 2009, pages 253– 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In ACL</booktitle>
<pages>95--102</pages>
<contexts>
<context position="1517" citStr="Henderson, 2004" startWordPosition="231" endWordPosition="232"> tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In t</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In ACL 2004, pages 95–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="10170" citStr="Hinton, 2002" startWordPosition="1778" endWordPosition="1779">bjective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002). A factor A is introduced to balance the two models. It is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In EMNLP 2010,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="11154" citStr="Huang et al. (2010)" startWordPosition="1939" endWordPosition="1942">arser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The performance of our first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where A indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005)2 a</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable grammars. In EMNLP 2010, pages 12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL-HLT</booktitle>
<pages>586--594</pages>
<contexts>
<context position="1962" citStr="Huang, 2008" startWordPosition="302" endWordPosition="303"> of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectively. Combined with other high-performance parsers under the framewo</context>
<context position="5642" citStr="Huang (2008)" startWordPosition="954" endWordPosition="955">n two categories, namely, lexical and structural. All features extracted from the part in Figure 1 are demonstrated in Table 1. Some back-off structural features are used for smoothing, which cannot be presented due to limited space. With only lexical features in a part, this parsing model backs off to a first-order one similar to those in the previous works. Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. 2.2 Decoding The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following Huang (2008), this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP → NP VP in Figure 1, the algorithm will extract child features from its children NP → DT QP and VP → VBN PP. The parent </context>
<context position="10978" citStr="Huang (2008)" startWordPosition="1913" endWordPosition="1914">4.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The performance of our first- and higher-order parsing models on all </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL-HLT 2008, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Joint and conditional estimation of tagging and parsing models.</title>
<date>2001</date>
<booktitle>In ACL</booktitle>
<pages>322--329</pages>
<contexts>
<context position="1500" citStr="Johnson, 2001" startWordPosition="229" endWordPosition="230">ately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-loc</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>Mark Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In ACL 2001, pages 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kiefer</author>
</authors>
<title>Sequential minimax search for a maximum.</title>
<date>1953</date>
<booktitle>Proceedings of the American Mathematical Society,</booktitle>
<pages>4--502</pages>
<contexts>
<context position="11351" citStr="Kiefer, 1953" startWordPosition="1973" endWordPosition="1974">m F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The performance of our first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where A indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005)2 and the Stanford. parser3 The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses. Our results are compared with the best pre</context>
</contexts>
<marker>Kiefer, 1953</marker>
<rawString>J. Kiefer. 1953. Sequential minimax search for a maximum. Proceedings of the American Mathematical Society, 4:502–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In ACL 2010,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1039" citStr="Koo and Collins, 2010" startWordPosition="158" endWordPosition="161">e tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers. 1 Introduction Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Resear</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In ACL 2010, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1250" citStr="McDonald and Pereira, 2006" startWordPosition="188" endWordPosition="191">shes them to 92.80% and 85.60% via combination with other highperformance parsers. 1 Introduction Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The di</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL 2006, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1182" citStr="McDonald et al., 2005" startWordPosition="179" endWordPosition="182">% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers. 1 Introduction Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In ACL 2005, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In EMNLP-HLT</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In EMNLP-HLT 2005, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In NAACL-HLT</title>
<date>2007</date>
<pages>404--411</pages>
<contexts>
<context position="4580" citStr="Petrov and Klein, 2007" startWordPosition="767" endWordPosition="770">on to g(t, s). A part in a parse tree is illustrated in Figure 1. It consists of the center grammar rule instance NP → NP VP and a set of immediate neighbors, i.e., its parent PP → IN NP, its children NP → DT QP and VP → VBN PP, and its sibling IN → of. This set of neighboring rule instances forms a local structural context to provide useful information to determine the plausibility of the center rule instance. 2.1 Feature The feature vector Φ(Q(r), s) consists of a series of features {oi(Q(r), s))|i ≥ 0}. The first feature Oo(Q(r), s) is calculated with a PCFG-based generative parsing model (Petrov and Klein, 2007), as defined in (4) below, where r is the grammar rule instance A → B C that covers the span from the b-th 00(Q(r), s) = to the e-th word, splitting at the m-th word, x, y and z are latent variables in the PCFG-based model, and I(·) and O(·) are the inside and outside probabilities, respectively. All other features Oi(Q(r), s) are binary functions that indicate whether a configuration exists in Q(r) and s. These features are by their own nature in two categories, namely, lexical and structural. All features extracted from the part in Figure 1 are demonstrated in Table 1. Some back-off structur</context>
<context position="7951" citStr="Petrov and Klein, 2007" startWordPosition="1398" endWordPosition="1401">/e−1wb/e &amp; NP Similar to the lexical span features in Taskar et al. (2004) and Petrov and Klein (2008b) Split pair wm−1wm &amp; NP → NP VP Inner/Outer pair wbwe−1 &amp; NP → NP VP wb−1we &amp; NP → NP VP Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. Interested readers can refer to Chen (2012) for more technical details of this algorithm. 3 Constituent Recombination Following Fossum and Knight (2009), our constituent weighting scheme for parser combination uses multiple outputs of independent parsers. Suppose each parser generates a k-best parse list for an input sentence, the weight of a candidate constituent c is defined as �W(c) _ E AiS(c, ti,k)f(ti,k), (5) i</context>
<context position="9848" citStr="Petrov and Klein (2007)" startWordPosition="1721" endWordPosition="1724"> Setup for the highest weight. A pruning threshold p, similar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. The parameters Ai and p are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002). A factor A is introduced to balance the two models. It is tuned on a development set using the gold sec3 English Chinese R(%) P(%) F1(%) R(%) P(%) F1(%) Berkeley parser 89.71 90.03 89.87 82.00 84.48 83.22 First-order 91.33 91.79 91.56 84.14 86.23 85.17 Higher-order 91.62 92.1</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In NAACL-HLT 2007, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2008</date>
<booktitle>In NIPS 20,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1562" citStr="Petrov and Klein, 2008" startWordPosition="237" endWordPosition="240">DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constitu</context>
<context position="7429" citStr="Petrov and Klein (2008" startWordPosition="1275" endWordPosition="1278">rtion of IN the DT O(Ax, b, e)P(Ax → By Cz)I(By, b, m)I(Cz, m, e) (4) I(S,0,n) � x � z E y 2 Template Description Comments Lexical N-gram on inner wb/e+l(l=0,1,2,3,4) &amp; b/e &amp; l &amp; NP Similar to the distributional feature /outer edge wb/e−l(l=1,2,3,4,5) &amp; b/e &amp; l &amp; NP similarity cluster bigrams wb/e+lwb/e+l+1(l=0,1,2,3) &amp; b/e &amp; l &amp; NP features in Finkel et al. (2008) wb/e−l−1wb/e−l(l=1,2,3,4) &amp; b/e &amp; l &amp; NP wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) &amp; b/e &amp; l &amp; NP wb/e−l−2wb/e−l−1wb/e−l(l=1,2,3) &amp; b/e &amp; l &amp; NP Bigram on edges wb/e−1wb/e &amp; NP Similar to the lexical span features in Taskar et al. (2004) and Petrov and Klein (2008b) Split pair wm−1wm &amp; NP → NP VP Inner/Outer pair wbwe−1 &amp; NP → NP VP wb−1we &amp; NP → NP VP Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A para</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008a. Discriminative loglinear grammars with latent variables. In NIPS 20, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Sparse multi-scale grammars for discriminative latent variable parsing.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<pages>867--876</pages>
<contexts>
<context position="1562" citStr="Petrov and Klein, 2008" startWordPosition="237" endWordPosition="240">DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constitu</context>
<context position="7429" citStr="Petrov and Klein (2008" startWordPosition="1275" endWordPosition="1278">rtion of IN the DT O(Ax, b, e)P(Ax → By Cz)I(By, b, m)I(Cz, m, e) (4) I(S,0,n) � x � z E y 2 Template Description Comments Lexical N-gram on inner wb/e+l(l=0,1,2,3,4) &amp; b/e &amp; l &amp; NP Similar to the distributional feature /outer edge wb/e−l(l=1,2,3,4,5) &amp; b/e &amp; l &amp; NP similarity cluster bigrams wb/e+lwb/e+l+1(l=0,1,2,3) &amp; b/e &amp; l &amp; NP features in Finkel et al. (2008) wb/e−l−1wb/e−l(l=1,2,3,4) &amp; b/e &amp; l &amp; NP wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) &amp; b/e &amp; l &amp; NP wb/e−l−2wb/e−l−1wb/e−l(l=1,2,3) &amp; b/e &amp; l &amp; NP Bigram on edges wb/e−1wb/e &amp; NP Similar to the lexical span features in Taskar et al. (2004) and Petrov and Klein (2008b) Split pair wm−1wm &amp; NP → NP VP Inner/Outer pair wbwe−1 &amp; NP → NP VP wb−1we &amp; NP → NP VP Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A para</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008b. Sparse multi-scale grammars for discriminative latent variable parsing. In EMNLP 2008, pages 867–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In NAACL-HLT 2010,</booktitle>
<pages>pages</pages>
<contexts>
<context position="11070" citStr="Petrov (2010)" startWordPosition="1928" endWordPosition="1929">4.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The performance of our first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where A indicates a tuned balance fa</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In NAACL-HLT 2010, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J D Powell</author>
</authors>
<title>An efficient method for finding the minimum of a function of several variables without calculating derivatives.</title>
<date>1964</date>
<journal>Computer Journal,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>162</pages>
<contexts>
<context position="9478" citStr="Powell, 1964" startWordPosition="1658" endWordPosition="1659">ght (2009). The weight of a recombined parse is defined as the sum of weights of all constituents in the parse. However, this definition has a systematic bias towards selecting a parse with as many constituents as possible English Chinese Train. Section 2-21 Art. 1-270,400-1151 Dev. Section 22/24 Art. 301-325 Test. Section 23 Art. 271-300 Table 2: Experiment Setup for the highest weight. A pruning threshold p, similar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. The parameters Ai and p are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) i</context>
</contexts>
<marker>Powell, 1964</marker>
<rawString>M. J. D. Powell. 1964. An efficient method for finding the minimum of a function of several variables without calculating derivatives. Computer Journal, 7(2):155– 162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In NAACL-HLT</booktitle>
<pages>129--132</pages>
<contexts>
<context position="2616" citStr="Sagae and Lavie, 2006" startWordPosition="404" endWordPosition="407">ts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectively. Combined with other high-performance parsers under the framework of constituent recombination (Sagae and Lavie, 2006; Fossum and Knight, 2009), this model further enhances the F1 scores to 92.80% and 85.60%, the highest ones achieved so far on these two data sets. 2 Higher-order Constituent Parsing Discriminative parsing is aimed to learn a function f : S —* T from a set of sentences S to a set of valid parses T according to a given CFG, which maps an input sentence s E S to a set of candidate parses T (s). The function takes the following discriminative form: f(s) = arg max g(t, s) (1) tET(s) 1http://code.google.com/p/gazaparser/ Proceedings of the 50th Annual Meeting of the Association for Computational L</context>
<context position="9323" citStr="Sagae and Lavie (2006)" startWordPosition="1629" endWordPosition="1632">c is contained in ti,k, the k-th parse output from the ith parser, and f(ti,k) the score of the k-th parse assigned by the i-th parser, as defined in Fossum and Knight (2009). The weight of a recombined parse is defined as the sum of weights of all constituents in the parse. However, this definition has a systematic bias towards selecting a parse with as many constituents as possible English Chinese Train. Section 2-21 Art. 1-270,400-1151 Dev. Section 22/24 Art. 301-325 Test. Section 23 Art. 271-300 Table 2: Experiment Setup for the highest weight. A pruning threshold p, similar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. The parameters Ai and p are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fo</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In NAACL-HLT 2006, pages 129–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Mike Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing. In EMNLP</title>
<date>2004</date>
<pages>1--8</pages>
<contexts>
<context position="1538" citStr="Taskar et al., 2004" startWordPosition="233" endWordPosition="236"> dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; *The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). 1 Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present</context>
<context position="7402" citStr="Taskar et al. (2004)" startWordPosition="1270" endWordPosition="1273">ales will be ... ... a portion of IN the DT O(Ax, b, e)P(Ax → By Cz)I(By, b, m)I(Cz, m, e) (4) I(S,0,n) � x � z E y 2 Template Description Comments Lexical N-gram on inner wb/e+l(l=0,1,2,3,4) &amp; b/e &amp; l &amp; NP Similar to the distributional feature /outer edge wb/e−l(l=1,2,3,4,5) &amp; b/e &amp; l &amp; NP similarity cluster bigrams wb/e+lwb/e+l+1(l=0,1,2,3) &amp; b/e &amp; l &amp; NP features in Finkel et al. (2008) wb/e−l−1wb/e−l(l=1,2,3,4) &amp; b/e &amp; l &amp; NP wb/e+lwb/e+l+1wb/e+l+2(l=0,1,2) &amp; b/e &amp; l &amp; NP wb/e−l−2wb/e−l−1wb/e−l(l=1,2,3) &amp; b/e &amp; l &amp; NP Bigram on edges wb/e−1wb/e &amp; NP Similar to the lexical span features in Taskar et al. (2004) and Petrov and Klein (2008b) Split pair wm−1wm &amp; NP → NP VP Inner/Outer pair wbwe−1 &amp; NP → NP VP wb−1we &amp; NP → NP VP Rule bigram Left &amp; NP &amp; NP Similar to the bigrams features Right &amp; NP &amp; NP in Collins (2000) Structural Parent PP → IN NP &amp; NP → NP VP Similar to the grandparent feature rules features in Collins (2000) Child NP → DT QP &amp; VP → VBN PP &amp; NP → NP VP NP → DT QP &amp; NP → NP VP VP → VBN PP &amp; NP → NP VP Sibling Left &amp; IN → of &amp; NP → NP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for eff</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In EMNLP 2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
<author>Haizhou Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>In EMNLP</booktitle>
<pages>1552--1560</pages>
<contexts>
<context position="11051" citStr="Zhang et al. (2009)" startWordPosition="1923" endWordPosition="1926">order 91.62 92.11 91.86 84.24 86.54 85.37 Higher-order+A 91.60 92.13 91.86 84.45 86.74 85.58 Stanford parser - - - 77.40 79.57 78.47 C&amp;J parser 91.04 91.76 91.40 - - - Conbination 92.02 93.60 92.80 82.44 89.01 85.60 Table 3: The performance of our parsing models on the English and Chinese test sets. System F1(%) EX(%) Single Charniak (2000) 89.70 Berkeley parser 89.87 36.7 Bod (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54 Huang (2008) 91.69 43.5 Combination Fossum and Knight (2009) 92.4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (s.t.+combo) 92.62 Huang et al. (2010) (single) 91.59 40.3 Huang et al. (2010) (combo) 92.39 43.1 Our single 91.86 40.89 Our combo 92.80 41.60 Table 4: Performance comparison on the English test set tion search algorithm (Kiefer, 1953). The parameters 0 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). The performance of our first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where A indicates</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-best combination of syntactic parsers. In EMNLP 2009, pages 1552–1560.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>