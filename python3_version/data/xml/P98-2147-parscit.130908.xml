<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001577">
<title confidence="0.866467">
Dynamic compilation of weighted context-free grammars
</title>
<author confidence="0.719232">
Mehryar Mohri and Fernando C. N. Pereira
</author>
<affiliation confidence="0.590327">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.733348">
180 Park Avenue
Florham Park, NJ 07932, USA
</address>
<email confidence="0.882929">
{mohri,pereira}Oresearch.att.com
</email>
<sectionHeader confidence="0.99071" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999737869565218">
Weighted context-free grammars are a conve-
nient formalism for representing grammatical
constructions and their likelihoods in a variety
of language-processing applications. In partic-
ular, speech understanding applications require
appropriate grammars both to constrain speech
recognition and to help extract the meaning
of utterances. In many of those applications,
the actual languages described are regular, but
context-free representations are much more con-
cise and easier to create. We describe an effi-
cient algorithm for compiling into weighted fi-
nite automata an interesting class of weighted
context-free grammars that represent regular
languages. The resulting automata can then be
combined with other speech recognition compo-
nents. Our method allows the recognizer to dy-
namically activate or deactivate grammar rules
and substitute a new regular language for some
terminal symbols, depending on previously rec-
ognized inputs, all without recompilation. We
also report experimental results showing the
practicality of the approach.
</bodyText>
<sectionHeader confidence="0.977984" genericHeader="keywords">
1. Motivation
</sectionHeader>
<bodyText confidence="0.995400098039216">
Context-free grammars (CFGs) are widely used
in language processing systems. In many appli-
cations, in particular in speech recognition, in
addition to recognizing grammatical sequences
it is necessary to provide some measure of the
probability of those sequences. It is then natu-
ral to use weighted CFGs, in which each rule is
given a weight from an appropriate weight alge-
bra (Salomaa and Soittola, 1978). Weights can
encode probabilities, for instance by setting a
rule&apos;s weight to the negative logarithm of the
probability of the rule. Rule probabilities can
be estimated in a variety of ways, which we will
not discuss further in this paper.
Since speech recognizers cannot be fully cer-
tain about the correct transcription of a spoken
utterance, they instead generate a range of al-
ternative hypotheses with associated probabil-
ities. An essential function of the grammar is
then to rank those hypotheses according to the
probability that they would be actually uttered.
The grammar is thus used together with other
information sources — pronunciation dictionary,
phonemic context-dependency model, acoustic
model (Bahl et al., 1983; Rabiner and Juang,
1993) — to generate an overall set of transcrip-
tion hypotheses with corresponding probabili-
ties.
General CFGs are computationally too de-
manding for real-time speech recognition sys-
tems, since the amount of work required to ex-
pand a recognition hypothesis in the way just
described would in general be unbounded for
an unrestricted grammar. Therefore, CFGs
used in spoken-dialogue applications often rem
resent regular languages (Church, 1983; Brown
and Buntschuh, 1994), either by construction or
as a result of a finite-state approximation of a
more general CFG (Pereira and Wright, 1997).&apos;
Assuming that the grammar can be efficiently
converted into a finite automaton, appropriate
techniques can then be used to combine it with
other finite-state recognition models for use in
real-time recognition (Mohri et al., 1998b).
There is no general algorithm that would map
an arbitrary CFG generating a regular language
into a corresponding finite-state automaton (U1-
lian, 1967). However, we will describe a use-
ful class of grammars that can be so trans-
formed, and a transformation algorithm that
avoids some of the potential for combinatorial
</bodyText>
<footnote confidence="0.750049">
&apos;Grammars representing regular languages have also
been used successfully in other areas of computational
linguistics (Karlsson et al., 1995).
</footnote>
<page confidence="0.998195">
891
</page>
<bodyText confidence="0.9993945">
explosion in the process.
Spoken dialogue systems require grammars
or language models to change as the dialogue
proceeds, because previous interactions set the
context for interpreting new utterances. For in-
stance, a previous request for a date might ac-
tivate the date grammar and lexicon and inac-
tivate the location grammar and lexicon in an
automated reservations task. Without such dy-
namic grammars, efficiency and accuracy would
be compromised because many irrelevant words
and constructions would be available when eval-
uating recognition hypotheses. We consider two
dynamic grammar mechanisms: activation and
deactivation of grammar rules, and the substi-
tution of a new regular language for a terminal
symbol when recognizing the next utterance.
We describe a new algorithm for compil-
ing weighted CFCs, based on representing the
grammar as a weighted transducer. This
representation provides opportunities for op-
timization, including optimizations involving
weights, which are not possible for general
CFGs. The algorithm also supports dynamic
grammar changes without recompilation. Fur-
thermore, the algorithm can be executed on de-
mand: states and transitions of the automa-
ton are expanded only as needed for the recog-
nition of the actual input utterances. More-
over, our lazy compilation algorithm is opti-
mal in the sense that the construction requires
work linear in the size of the input grammar,
which is the best one can expect given that
any algorithm needs to inspect the whole in-
put grammar. It is however possible to speed-
up grammar compilation further by applying
pre-compilation optimizations to the grammar,
as we will see later. The class of grammars
to which our algorithm applies includes right-
linear grammars, left-linear grammars and cer-
tain combinations thereof.
The algorithm has been fully implemented
and evaluated experimentally, demonstrating
its effectiveness.
</bodyText>
<sectionHeader confidence="0.991283" genericHeader="introduction">
2. Algorithm
</sectionHeader>
<bodyText confidence="0.999748352941176">
We will start by giving a precise definition of
dynamic grammars. We will then explain each
stage of grammar compilation. Grammar com-
pilation takes as input a weighted CFG repre-
sented as a weighted transducer (Salomaa and
Soittola, 1978), which may have been opti-
mized prior to compilation (preoptimized). The
weighted transducer is analyzed by the com-
pilation algorithm, and the analysis, if suc-
cessful, outputs a collection of weighted au-
tomata that are combined at runtime according
to the current dynamic grammar configuration
and the strings being recognized. Since not all
CFGs can be compiled into weighted automata,
the compilation algorithm may reject an input
grammar. The class of allowed grammars will
be defined later.
</bodyText>
<subsectionHeader confidence="0.984433">
2.1. Dynamic grammars
</subsectionHeader>
<bodyText confidence="0.970719619047619">
The following notation will be used in the rest
of the paper. A weighted CFG G = (V, P)
over the alphabet E, with real-number weights
consists of a finite alphabet V of variables or
nonterminals disjoint from E, and a finite set
PC VxR x (V U E)* of productions or deriva-
tion rules (Autebert et al., 1997). Given strings
U, v E (V U E)*, and real numbers c and c&apos;, we
write (u, c) 4 (v, c&apos;) when there is a derivation
from u with weight c to v with weight c&apos;. We
denote by LG(X) the weighted language gener-
ated by a nonterminal X:
LG(X)= {(w,C) E E* x R : (X, 0) 4 (w, c)}
We can now define the two grammar-changing
operations that we use.
Dynamic activation or deactivation of
rules 2 We augment the grammar with a set
of active nonterminals, which are those avail-
able as start symbols for derivations. More pre-
cisely, let A C V be the set of active nonter-
minals. The language generated by G is then
</bodyText>
<equation confidence="0.90989">
LG = UXEA
</equation>
<bodyText confidence="0.937809928571429">
LG(X). Note that inactive nonter-
minals, and the rules involving them, are avail-
able for use in derivations; they are just not
available as start symbols. Dynamic rule acti-
vation or deactivation is just the dynamic re-
definition of the set A in successive uses of the
grammar.
Dynamic substitution Let a be a weighted
rational transduction of E* to A* x R, E C A,
that is a regular weighted substitution (Berstel,
1979). a is a monoid morphism verifying:
2This is the terminology used in this area, though a
more appropriate expression would be dynamic activa-
tion or deactivation of nonterminal symbols.
</bodyText>
<page confidence="0.986245">
892
</page>
<bodyText confidence="0.988452888888889">
Vs E E, a(x) C Reg(er x R)
where Reg(* x R) denotes the set of
weighted regular languages over the alphabet
A. Thus a simply substitutes for each symbol
a E Ea weighted regular expression a(a). A
dynamic substitution consists of the application
of the substitution a to E, during the process
of recognition of a word sequence. Thus, after
substitution, the language generated by the new
</bodyText>
<equation confidence="0.872305">
grammar G&apos; is:3
.L01 = a(LG)
</equation>
<bodyText confidence="0.999844">
Our algorithm allows for both of those dy-
namic grammar changes without recompiling
the grammar.
</bodyText>
<subsectionHeader confidence="0.978027">
2.2. Preprocessing
</subsectionHeader>
<bodyText confidence="0.999990904761905">
Our compilation algorithm operates on a
weighted transducer r(G) encoding a factored
representation of the weighted CFG G, which
is generated from G by a separate preproces-
sor. This preprocessor is not strictly needed,
since we could use a version of the main algo-
rithm that works directly on G. However, pre-
processing can improve dramatically the time
and space needed for the main compilation step,
since the preprocessor uses determinization and
minimization algorithms for weighted transduc-
ers (Mohri, 1997) to increase the sharing — fac-
torirzg — among grammar rules that start or end
the same way.
The preprocessing step builds a weighted
transducer in which each path corresponds to a
grammar rule. Rule X a ...Yn has a cor-
responding path that maps X to the sequence
Y1 ...Yr, with weight a. For example, the small
CFG in Figure 1 is preprocessed into the com-
pacted transducer shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.978396">
2.3. Compilation
</subsectionHeader>
<bodyText confidence="0.999389833333333">
The compilation of weighted left-linear or right-
linear grammars into weighted automata is
straightforward (Aho and Ullman, 1973). In
the right-linear case, for instance, the states of
the automaton are the grammar nonterminals
together with a new final state F. There is a
</bodyText>
<equation confidence="0.946087666666667">
3er can be extended as usual to map Es x R to
Reg( A• x R).
Z .1 XY (1)
X .2 -4 aY
Y .3 —&gt; bX
V.4 —&gt; c
</equation>
<figureCaption confidence="0.999994">
Figure 1: Grammar GI.
Figure 2: Weighted transducer T(G1)•
</figureCaption>
<bodyText confidence="0.986067071428572">
transition labeled with a E E and weight a E R
from X E V to Y E V if the grammar con-
tains the rule X a aY. There is a transition
from X to F labeled with a and weight a if
X a —&gt; a is a rule of the grammar. The initial
states are the states corresponding to the active
nonterminals. For example, Figure 3 shows the
weighted automaton for grammar G2 consisting
of the last three rules of G1 with start symbol
X.
However, the standard methods for left- and
right-linear grammars cannot be used for gram-
mars such as G1 that generate regular sets but
have rules that are neither left- nor right-linear.
But we can use the methods for left- and right-
linear grammars as subroutines if the grammar
can be decomposed into left-linear and right-
linear components that do not call each other
recursively (Pereira and Wright, 1997). More
precisely, define a dependency graph DG for
G&apos;s nonterminals and examine the set of its
strongly-connected components (SCCs).4 The
nodes of DG are G&apos;s nonterminals, and there
is a directed edge from X to Y if Y appears
in the right-hand side of a rule with left-hand
side X, that is, if the definition of X depends
on Y. Each SCC S of DG has a corresponding
subgrammar of G consisting of those rules with
</bodyText>
<footnote confidence="0.98965">
4 Recall that the strongly connected components of a
directed graph are the equivalence classes of graph nodes
under the relation R defined by: q R q&apos; if q&apos; can be
reached from q and q from q&apos;.
</footnote>
<page confidence="0.996213">
893
</page>
<figureCaption confidence="0.969233666666667">
Figure 3: Compilation of G2.
Figure 4: Dependency graph DG, for grammar
G1.
</figureCaption>
<bodyText confidence="0.98385535">
left-hand nonterminals in S, with nonterminals
not in S treated as terminal symbols. If each of
these subgrammars is either left-linear or right-
linear, we shall see that compilation into a single
finite automaton is possible.
The dependency graph DG can be obtained
easily from the transducer r(G). For exam-
ple, Figure 4 shows the dependency graph for
our example grammar GI, with SCCs {Z} and
{X, Y}. It is clear that G1 satisfies our condi-
tion, and Figure 5 shows the result of compiling
G1 with A = {Z}.
• The SCCs of DG can be obtained in time lin-
ear in the size of G (Aho et al., 1974). Be-
fore starting the compilation, we check that
each subgrammar is left-linear or right-linear
(as noted above, nonterminals not in the SCC
of a subgrammar are treated as terminals). For
example, if {Xi, X2} is an SCC, then the sub-
grammar
</bodyText>
<equation confidence="0.996140333333333">
X1 -4 aY1bY2X1 (2)
X1 -4. bY2aY1 X2
X2 -4 bbY1abX1
</equation>
<figureCaption confidence="0.9724808">
Figure 5: Compilation of G1 with start symbol
Z.
Figure 6: Weighted automaton K({X,Y}) cor-
responding to the strongly connected compo-
nent {X, Y} of G1.
</figureCaption>
<bodyText confidence="0.989117540540541">
with X1, X27 Y1) Y2 E V and a, b E E is right-
linear, since expressions such as aY1bY2 can be
treated as elements of the terminal alphabet of
the subgrammar.
When the compilation condition holds, for
each SCC S we can build a weighted automa-
ton K(S) representing the language of S&apos;s sub-
grammar using the standard methods. Since
some nonterminals of G are treated as termi-
nal symbols within a subgrammar, the transi-
tions of an automaton K(S) may be labeled
with nonterminals not in 5.5 The nontermi-
nals not in S can then be replaced by their cor-
responding automata. The replacement opera-
tion is lazy, that is, the states and transitions of
the replacing automata are only expanded when
needed for a given input string. Another inter-
esting characteristic of our algorithm is that the
weighted automata K(S) can be made smaller
by determinization and minimization, leading
to improvements in runtime performance.
The automaton M(X) that represents the
language generated by nonterminal symbol X
can be defined using K(S), where S is the
strongly connected component containing X,
X E S. For instance, when the subgrammar
of S is right-linear, M(X) is the automaton
that has the same states, transitions, and final
states as K(S) and has the state correspond-
ing to X as initial state. For example, Figure
6 shows K({X,Y}) for G1. M(X) is then ob-
tained from K({X,Y}) by taking X as initial
state. The left-linear case can be treated in a
similar way. Thus, M(X) can always be de-
fined in constant time and space by editing the
automaton K(S). We use a lazy implementa-
tion of this editing operation for the definition
</bodyText>
<footnote confidence="0.9952244">
5More precisely, they can only be part of other
strongly connected components that come before S in
a reverse topological sort of the components. This guar-
antees the convergence of the replacement of the nonter-
minals by the corresponding automata.
</footnote>
<page confidence="0.997131">
894
</page>
<figureCaption confidence="0.943145714285714">
Figure 7: Automaton MG with activated non-
terminals: A = {X, Y, Z}.
of the automata M(X): the states and transi-
tions of M(X) are determined using K(S) only
when necessary for the given input string. This
allows us to save both space and time by avoid-
ing a copy of K(S) for each X E S.
</figureCaption>
<bodyText confidence="0.99946644">
Once the automaton representing the lan-
guage generated by each nonterminal is cre-
ated, we can define the language generated by G
by building an automaton MG with one initial
state and one final state, and transitions labeled
with active nonterminals from the initial to the
final state. Figure 7 illustrates this in the case
where A = {X, Y, Z}.
Given this construction, the dynamic activa-
tion or deactivation of nonterminals can be done
by modifying the automaton MG. This opera-
tion does not require any recompilation, since it
does not affect the automaton M(X) built for
each nonterminal X.
All the steps in building the automata M(X)
— construction of DG, finding the SCCs, and
computing for K(S) for each SCC S — require
linear time and space with respect to the size
of G. In fact, since we first convert G into
a compact weighted transducer r(G), the to-
tal work required is linear in the size of r(G).6
This leads to significant gains as shown by our
experiments.
In summary, the compilation algorithm has
the following steps:
</bodyText>
<listItem confidence="0.961891">
1. Build the dependency graph DG of the
grammar G.
2. Compute the SCCs of DG.7
3. For each SCC 5, construct the automaton
K(S). For each X E S, build M(X) from
</listItem>
<footnote confidence="0.887318">
6Applying the algorithm to a compacted weighted
transducer r(G) involves various subtleties that we omit
for simplicity.
7We order the SCCs in reverse topological order, but
this is not necessary for the correctness of the algorithm.
K(X). 8
</footnote>
<listItem confidence="0.7603908">
4. Create a simple automaton MG accepting
exactly the set of active nonterminals A.
5. The automaton is then expanded on-the-fly
for each input string using lazy replacement
and editing.
</listItem>
<bodyText confidence="0.9816367">
The dynamic substitution of a terminal sym-
bol a by a weighted automaton9 aa is done by
replacing the symbol a by the automaton aa, us-
ing the replacement operation discussed earlier.
This replacement is also done on demand, with
only the necessary part of Ca being expanded for
a given input string. In practice, the automaton
Ca can be large, a list of city or person names for
example. Thus a lazy implementation is crucial
for dynamic substitutions.
</bodyText>
<sectionHeader confidence="0.988836" genericHeader="method">
3. Optimizations, Experiments and
Results
</sectionHeader>
<bodyText confidence="0.99982">
We have a full implementation of the compila-
tion algorithm presented in the previous section,
including the lazy representations that are cru-
cial in reducing the space requirements of speech
recognition applications. Our implementation
of the compilation algorithm is part of a gen-
eral set of grammar tools, the GRM Library
(Mohri, 1998b), currently used in speech pro-
cessing projects at AT&amp;T Labs. The GRM Li-
brary also includes an efficient compilation tool
for weighted context-dependent rewrite rules
(Mohri and Sproat, 1996) that is used in text-
to-speech projects at Lucent Bell Laboratories.
Since the GRM library is compatible with the
FSM general-purpose finite-state machine li-
brary (Mohri et al., 1998a), we were able to use
the tools provided in FSM library to optimize
the input weighted transducers r(G) and the
weighted automata in the compilation output.
We did several experiments that show the ef-
ficiency of our compilation method. A key fea-
ture of our grammar compilation method is the
representation of the grammar by a weighted
transducer that can then be preoptimized using
weighted transducer determinization and mini-
mization (Mohri, 1997; Mohri, 1998a). To show
</bodyText>
<footnote confidence="0.9801298">
8For any X, this is a constant time operation. For
instance, if K(S) is right-linear, we just need to pick out
the state associated to X in K(X).
6In fact, our implementation allows more generally
dynamic substitutions by weighted transducers.
</footnote>
<page confidence="0.983443">
895
</page>
<table confidence="0.995964">
no optimization
- (size / 25)
optimization
50 100 150 200 250 50 100 150 200 250
VOCABULARY VOCABULARY
</table>
<figureCaption confidence="0.999363">
Figure 8: Advantage of transducer representation combined with preoptimization: time and space.
</figureCaption>
<bodyText confidence="0.969123419354839">
the benefits of this representation, we compared
the compilation time and the size of the re-
sulting lazy automata with and without preop-
timization. The advantage of preoptimization
would be even greater if the compilation output
were fully expanded rather than on-demand.
We did experiments with full bigram models
with various vocabulary sizes, and with two un-
weighted grammars derived by feature instanti-
ation from hand-built feature-based grammars
(Pereira and Wright, 1997). Figure 8 shows
the compilation times of full bigram models
with and without preoptimization, demonstrat-
ing the importance of the optimization allowed
by using a transducer representation of the
grammar. For a 250-word vocabulary model,
the compilation time is about 50 times faster
with the preoptimized representation.w Figure
8 also shows the sizes of the resulting lazy au-
tomata in the two cases. While in the preop-
timized case time and space grow linearly with
vocabulary size (0( IGI)), they grow quadrat-
ically in the unoptimized case (0(IGI)).
The bigram examples also show the advan-
tages of lazy replacement and editing over the
full expansion used in previous work (Pereira
and Wright, 1997). Indeed, the size of the
fully-expanded automaton for the preoptimized
&apos;For convenience, the compilation time for the unop-
timized case in Figure 8 was divided by 10, and the size
of the result by 25.
</bodyText>
<tableCaption confidence="0.997701">
Table 1: Feature-based grammars.
</tableCaption>
<table confidence="0.997518666666667">
IGI optim. time expanded expanded
(s) states transitions
431 no .04 9675 11470
431 yes .02 1535 2002
12657 no 9.76 274614 321615
12657 yes 2.02 112795 144083
</table>
<bodyText confidence="0.999888764705883">
case grows quadratically with the vocabulary
size (0(IGI)), while it grows with the cube of
the vocabulary size in the unoptimized case
(0(IG13/2)). For example, compilation is about
700 times faster in the optimized case for a fully
expanded automaton even for a 40-word vo-
cabulary model, and the result about 39 times
smaller.
Our experiments with a small and a medium-
sized CFG obtained from feature-based gram-
mars confirm these observations (Table 1).
If dynamic grammars and lazy expansion are
not needed, we can expand the result fully and
then apply weighted determinization and min-
imization algorithms. Additional experiments
show that this can yield dramatic reductions in
automata size.
</bodyText>
<sectionHeader confidence="0.999029" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.994827">
A new weighted CFG compilation algorithm has
been presented. It can be used to compile effi-
</bodyText>
<page confidence="0.995958">
896
</page>
<bodyText confidence="0.999331785714286">
ciently an interesting class of grammars repre-
senting weighted regular languages and allows
for dynamic modifications that are crucial in
many speech recognition applications.
While we focused here on CFGs with real
number weights, which are especially relevant in
speech recognition, weighted CFGs can be de-
fined more generally over an arbitrary semiring
(Salomaa and Soittola, 1978). Our compilation
algorithm applies to general semirings without
change. Both the grammar compilation algo-
rithms (GRM library) and our automata opti-
mization tools (FSM library) work in the most
general case.
</bodyText>
<sectionHeader confidence="0.996931" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999774">
We thank Bruce Buntschuh and Ted Roycraft
for their help with defining the dynamic gram-
mar features and for their comments on this
work.
</bodyText>
<sectionHeader confidence="0.996948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99854625974026">
Alfred V. Aho and Jeffrey D. Ullman. 1973.
The Theory of Parsing, Translation and
Compiling. Prentice-Hall.
Alfred V. Aho, John E. Hoperoft, and Jeffrey D.
Ullman. 1974. The design and analysis of
computer algorithms. Addison Wesley: Read-
ing, MA.
Jean-Michel Autebert, Jean Berstel, and Luc
Boasson. 1997. Context-free languages and
pushdown automata. In Grzegorz Rozenberg
and Arto Salomaa, editors, Handbook of For-
mal Languages, volume 1, pages 111-172.
Springer.
Lalit R. Bahl, Fred Jelinek, and Robert Mercer.
1983. A maximum likelihood approach to
continuous speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine In-
telligence (PAMI), 5(2):179-190.
Jean Berstel. 1979. Transductions and Context-
Free Languages. Teubner Studienbucher:
Stuttgart.
Michael K. Brown and Bruce M. Buntschuh.
1994. A context-free grammar compiler for
speech understanding systems. In Proceed-
ings of the International Conference on Spo-
ken Language Processing (ICSLP &apos;94), pages
21-24, Yokohama, Japan.
Kenneth W. Church. 1983. A finite-state parser
for use in speech recognition. In 21st Meet-
ing of the Association for Computational Lin-
guistics (ACL &apos;83), Proceedings of the Con-
ference. ACL.
Fred Karlsson, Atro Voutilainen, Juha Heikkila,
and Atro Anttila. 1995. Constraint Gram-
mar, A language-Independent System for
Parsing Unrestricted Text. Mouton de
Gruyter.
Mehryar Mohri and Richard Sproat. 1996. An
efficient compiler for weighted rewrite rules.
In 34th Meeting of the Association for Com-
putational Linguistics (ACL 96), Proceedings
of the Conference, Santa Cruz, California.
ACL.
Mehryar Mohri, Fernando C. N. Pereira, and
Michael Riley. 1998a. A rational design for a
weighted finite-state transducer library. Lec-
ture Notes in Computer Science, to appear.
Mehryar Mohri, Michael Riley, Don Hindle,
Andrej Ljolje, and Fernando C. N. Pereira.
1998b. Full expansion of context-dependent
networks in large vocabulary speech recogni-
tion. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP &apos;98), Seattle, Washington.
Mehryar Mohri. 1997. Finite-state transducers
in language and speech processing. Compu-
tational Linguistics, 23:2.
Mehryar Mohri. 1998a. Minimization algo-
rithms for sequential transducers. Theoretical
Computer Science, to appear.
Mehryar Mohri. 1998b. Weighted Grammar
Tools: the GRM Library. In preparation.
Fernando C. N. Pereira and Rebecca N. Wright.
1997. Finite-state approximation of phrase-
structure grammars. In Emmanuel Roche
and Yves Schabes, editors, Finite-State Lan-
guage Processing, pages 149-173. MIT Press,
Cambridge, Massachusetts.
Lawrence Rabiner and Biing-Hwang Juang.
1993. Fundamentals of Speech Recognition.
Prentice-Hall, Englewood Cliffs, NJ.
Arto Salomaa and Matti Soittola. 1978.
Automata-Theoretic Aspects of Formal Power
Series. Springer-Verlag: New York.
Joseph S. Ullian. 1967. Partial algorithm prob-
lems for context free languages. Information
and Control, 11:90-101.
</reference>
<page confidence="0.998218">
897
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000276">
<title confidence="0.999757">Dynamic compilation of weighted context-free grammars</title>
<author confidence="0.999964">Mohri C N Pereira</author>
<affiliation confidence="0.998763">AT&amp;T Labs - Research</affiliation>
<address confidence="0.999026">180 Park Avenue Florham Park, NJ 07932, USA</address>
<email confidence="0.999859">mohriOresearch.att.com</email>
<email confidence="0.999859">pereiraOresearch.att.com</email>
<abstract confidence="0.996269444007859">Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications. In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances. In many of those applications, actual are regular, but context-free representations are much more concise and easier to create. We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages. The resulting automata can then be combined with other speech recognition components. Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach. 1. Motivation Context-free grammars (CFGs) are widely used in language processing systems. In many applications, in particular in speech recognition, in addition to recognizing grammatical sequences it is necessary to provide some measure of the probability of those sequences. It is then natuto use in which each rule is given a weight from an appropriate weight algebra (Salomaa and Soittola, 1978). Weights can encode probabilities, for instance by setting a rule&apos;s weight to the negative logarithm of the probability of the rule. Rule probabilities can be estimated in a variety of ways, which we will not discuss further in this paper. Since speech recognizers cannot be fully certain about the correct transcription of a spoken utterance, they instead generate a range of alternative hypotheses with associated probabilities. An essential function of the grammar is then to rank those hypotheses according to the probability that they would be actually uttered. The grammar is thus used together with other information sources — pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) — to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1lian, 1967). However, we will describe a useful class of grammars that can be so transformed, and a transformation algorithm that avoids some of the potential for combinatorial &apos;Grammars representing regular languages have also been used successfully in other areas of computational linguistics (Karlsson et al., 1995). 891 explosion in the process. Spoken dialogue systems require grammars or language models to change as the dialogue proceeds, because previous interactions set the context for interpreting new utterances. For instance, a previous request for a date might activate the date grammar and lexicon and inactivate the location grammar and lexicon in an reservations task. Without such dygrammars, and accuracy would be compromised because many irrelevant words and constructions would be available when evaluating recognition hypotheses. We consider two dynamic grammar mechanisms: activation and deactivation of grammar rules, and the substitution of a new regular language for a terminal symbol when recognizing the next utterance. We describe a new algorithm for compiling weighted CFCs, based on representing the grammar as a weighted transducer. This representation provides opportunities for optimization, including optimizations involving weights, which are not possible for general CFGs. The algorithm also supports dynamic grammar changes without recompilation. Furthermore, the algorithm can be executed on demand: states and transitions of the automaton are expanded only as needed for the recognition of the actual input utterances. Moreover, our lazy compilation algorithm is optimal in the sense that the construction requires work linear in the size of the input grammar, which is the best one can expect given that any algorithm needs to inspect the whole input grammar. It is however possible to speedup grammar compilation further by applying pre-compilation optimizations to the grammar, as we will see later. The class of grammars to which our algorithm applies includes rightlinear grammars, left-linear grammars and certain combinations thereof. The algorithm has been fully implemented and evaluated experimentally, demonstrating its effectiveness. 2. Algorithm We will start by giving a precise definition of dynamic grammars. We will then explain each stage of grammar compilation. Grammar compilation takes as input a weighted CFG represented as a weighted transducer (Salomaa and Soittola, 1978), which may have been optiprior to compilation weighted transducer is analyzed by the compilation algorithm, and the analysis, if successful, outputs a collection of weighted automata that are combined at runtime according to the current dynamic grammar configuration and the strings being recognized. Since not all CFGs can be compiled into weighted automata, the compilation algorithm may reject an input grammar. The class of allowed grammars will be defined later. 2.1. Dynamic grammars The following notation will be used in the rest the paper. A weighted CFG = (V, P) over the alphabet E, with real-number weights consists of a finite alphabet V of variables or nonterminals disjoint from E, and a finite set PC VxR x (V U E)* of productions or derivation rules (Autebert et al., 1997). Given strings v (V U and real numbers (u, (v, there is a derivation u with weight v with weight by weighted language generby a nonterminal {(w,C) x R : 4 (w, We can now define the two grammar-changing operations that we use. Dynamic activation or deactivation of 2We augment the grammar with a set nonterminals, are those available as start symbols for derivations. More precisely, let A C V be the set of active nonter- The language generated by then = that inactive nonterminals, and the rules involving them, are available for use in derivations; they are just not available as start symbols. Dynamic rule activation or deactivation is just the dynamic redefinition of the set A in successive uses of the grammar. substitution a weighted transduction of E* to A* x C A, that is a regular weighted substitution (Berstel, a monoid morphism verifying: is the terminology used in this area, though a more appropriate expression would be dynamic activaor deactivation of symbols. 892 E E, C Reg(er R) Reg(*x denotes the set of weighted regular languages over the alphabet Thus substitutes for each symbol E Ea weighted regular expression dynamic substitution consists of the application the substitution E, during the process of recognition of a word sequence. Thus, after substitution, the language generated by the new .L01 = a(LG) Our algorithm allows for both of those dynamic grammar changes without recompiling the grammar. 2.2. Preprocessing Our compilation algorithm operates on a transducer a factored of the weighted CFG generated from a separate preprocessor. This preprocessor is not strictly needed, since we could use a version of the main algothat works directly on preprocessing can improve dramatically the time and space needed for the main compilation step, since the preprocessor uses determinization and minimization algorithms for weighted transduc- (Mohri, 1997) to increase the sharing — fac- — grammar rules that start or end the same way. The preprocessing step builds a weighted transducer in which each path corresponds to a rule. Rule a has a path that maps the sequence with weight a. For example, the small CFG in Figure 1 is preprocessed into the compacted transducer shown in Figure 2. 2.3. Compilation The compilation of weighted left-linear or rightlinear grammars into weighted automata is straightforward (Aho and Ullman, 1973). In the right-linear case, for instance, the states of the automaton are the grammar nonterminals with a new final state is a can be extended as usual to map x R to x R). XY -4 aY .3 —&gt; —&gt; Figure 1: Grammar GI. Figure 2: Weighted transducer T(G1)• labeled with a E E and weight a V to Y V the grammar conthe rule a There is a transition with a and weight a if a —&gt; a a rule of the grammar. The initial states are the states corresponding to the active nonterminals. For example, Figure 3 shows the automaton for grammar the last three rules of with start symbol X. However, the standard methods for leftand right-linear grammars cannot be used for gramsuch as that generate regular sets but have rules that are neither leftnor right-linear. But we can use the methods for leftand rightlinear grammars as subroutines if the grammar can be decomposed into left-linear and rightlinear components that do not call each other recursively (Pereira and Wright, 1997). More define a graph G&apos;s nonterminals and examine the set of its components The of G&apos;s nonterminals, and there a directed edge from Y if Y appears in the right-hand side of a rule with left-hand is, if the definition of X depends Y. Each SCC a corresponding of of those rules with 4Recall that the strongly connected components of a directed graph are the equivalence classes of graph nodes the relation by: R q&apos; be from 893 3: Compilation of 4: Dependency graph grammar G1. nonterminals in nonterminals in as terminal symbols. If each of these subgrammars is either left-linear or rightlinear, we shall see that compilation into a single finite automaton is possible. dependency graph be obtained from the transducer example, Figure 4 shows the dependency graph for example grammar with SCCs Y}. It is clear that satisfies our condition, and Figure 5 shows the result of compiling with A = {Z}. The SCCs of be obtained in time linin the size of et al., 1974). Before starting the compilation, we check that each subgrammar is left-linear or right-linear (as noted above, nonterminals not in the SCC of a subgrammar are treated as terminals). For if is an SCC, then the subgrammar X1 -4 aY1bY2X1 (2) -4. bY2aY1 5: Compilation of with start symbol Z. 6: Weighted automaton corresponding to the strongly connected component {X, Y} of G1. X1, Y1) Y2 a, is rightsince expressions such as can be treated as elements of the terminal alphabet of the subgrammar. When the compilation condition holds, for SCC can build a weighted automathe language of S&apos;s subgrammar using the standard methods. Since nonterminals of treated as terminal symbols within a subgrammar, the transiof an automaton be labeled nonterminals not in The nonterminot in then be their corresponding automata. The replacement operais is, the states and transitions of the replacing automata are only expanded when needed for a given input string. Another interesting characteristic of our algorithm is that the automata be made smaller by determinization and minimization, leading to improvements in runtime performance. automaton represents the generated by nonterminal symbol be defined using the connected component containing when the subgrammar right-linear, the automaton that has the same states, transitions, and final as has the state correspondto initial state. For example, Figure shows then obfrom taking initial state. The left-linear case can be treated in a way. Thus, always be dein constant time and space by use a lazy implementation of this editing operation for the definition precisely, they can only be part of other connected components that come before a reverse topological sort of the components. This guarantees the convergence of the replacement of the nonterminals by the corresponding automata. 894 7: Automaton activated non- A = {X, Y, the automata states and transiof determined using when necessary for the given input string. This allows us to save both space and time by avoida copy of each Once the automaton representing the language generated by each nonterminal is crewe can define the language generated by building an automaton one initial state and one final state, and transitions labeled with active nonterminals from the initial to the final state. Figure 7 illustrates this in the case A = {X, Y, Given this construction, the dynamic activation or deactivation of nonterminals can be done modifying the automaton operation does not require any recompilation, since it not affect the automaton for nonterminal the steps in building the automata construction of the SCCs, and for each SCC — linear time and space with respect to the size fact, since we first convert compact weighted transducer towork required is linear in the size of This leads to significant gains as shown by our experiments. In summary, the compilation algorithm has the following steps: Build the dependency graph the Compute the SCCs of 3. For each SCC 5, construct the automaton each the algorithm to a compacted weighted transducer r(G) involves various subtleties that we omit for simplicity. order the SCCs in reverse topological order, but this is not necessary for the correctness of the algorithm. 8 Create a simple automaton exactly the set of active nonterminals A. 5. The automaton is then expanded on-the-fly for each input string using lazy replacement and editing. The dynamic substitution of a terminal syma by a weighted is done by symbol a by the automaton using the replacement operation discussed earlier. This replacement is also done on demand, with the necessary part of being expanded for a given input string. In practice, the automaton can be large, a list of city or person names for example. Thus a lazy implementation is crucial for dynamic substitutions. 3. Optimizations, Experiments and Results We have a full implementation of the compilation algorithm presented in the previous section, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&amp;T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize input weighted transducers the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer determinization and minimization (Mohri, 1997; Mohri, 1998a). To show any is a constant time operation. For if right-linear, we just need to pick out state associated to fact, our implementation allows more generally dynamic substitutions by weighted transducers. 895 no optimization - (size / 25) optimization 50 100 150 200 250 50 100 150 200 250 VOCABULARY VOCABULARY Figure 8: Advantage of transducer representation combined with preoptimization: time and space. the benefits of this representation, we compared the compilation time and the size of the resulting lazy automata with and without preoptimization. The advantage of preoptimization would be even greater if the compilation output were fully expanded rather than on-demand. We did experiments with full bigram models with various vocabulary sizes, and with two unweighted grammars derived by feature instantiation from hand-built feature-based grammars (Pereira and Wright, 1997). Figure 8 shows the compilation times of full bigram models with and without preoptimization, demonstrating the importance of the optimization allowed by using a transducer representation of the grammar. For a 250-word vocabulary model, the compilation time is about 50 times faster with the preoptimized representation.w Figure 8 also shows the sizes of the resulting lazy automata in the two cases. While in the preoptimized case time and space grow linearly with size they grow quadratically in the unoptimized case (0(IGI)). The bigram examples also show the advantages of lazy replacement and editing over the full expansion used in previous work (Pereira and Wright, 1997). Indeed, the size of the fully-expanded automaton for the preoptimized &apos;For convenience, the compilation time for the unoptimized case in Figure 8 was divided by 10, and the size of the result by 25. Table 1: Feature-based grammars. IGI optim. time (s) expanded states expanded transitions 431 no .04 9675 11470 431 yes .02 1535 2002 12657 no 9.76 274614 321615 12657 yes 2.02 112795 144083 case grows quadratically with the vocabulary size (0(IGI)), while it grows with the cube of the vocabulary size in the unoptimized case For example, compilation is about 700 times faster in the optimized case for a fully expanded automaton even for a 40-word vocabulary model, and the result about 39 times smaller. Our experiments with a small and a mediumsized CFG obtained from feature-based grammars confirm these observations (Table 1). If dynamic grammars and lazy expansion are not needed, we can expand the result fully and then apply weighted determinization and minimization algorithms. Additional experiments show that this can yield dramatic reductions in automata size. 4. Conclusion A new weighted CFG compilation algorithm has presented. It can be used to compile effi- 896 ciently an interesting class of grammars representing weighted regular languages and allows for dynamic modifications that are crucial in many speech recognition applications. While we focused here on CFGs with real number weights, which are especially relevant in speech recognition, weighted CFGs can be demore generally over an arbitrary (Salomaa and Soittola, 1978). Our compilation algorithm applies to general semirings without change. Both the grammar compilation algorithms (GRM library) and our automata optimization tools (FSM library) work in the most general case. Acknowledgements We thank Bruce Buntschuh and Ted Roycraft for their help with defining the dynamic grammar features and for their comments on this work.</abstract>
<note confidence="0.783031047619048">References Alfred V. Aho and Jeffrey D. Ullman. 1973. The Theory of Parsing, Translation and Alfred V. Aho, John E. Hoperoft, and Jeffrey D. 1974. design and analysis of algorithms. Wesley: Reading, MA. Jean-Michel Autebert, Jean Berstel, and Luc Boasson. 1997. Context-free languages and pushdown automata. In Grzegorz Rozenberg Arto Salomaa, editors, of For- Languages, 1, pages 111-172. Springer. Lalit R. Bahl, Fred Jelinek, and Robert Mercer. 1983. A maximum likelihood approach to speech recognition. Transactions on Pattern Analysis and Machine In- (PAMI), Berstel. 1979. and Context- Languages. Studienbucher: Stuttgart. Michael K. Brown and Bruce M. Buntschuh. 1994. A context-free grammar compiler for understanding systems. In Proceedings of the International Conference on Spo- Language Processing (ICSLP &apos;94), 21-24, Yokohama, Japan. Kenneth W. Church. 1983. A finite-state parser use in speech recognition. In Meeting of the Association for Computational Linguistics (ACL &apos;83), Proceedings of the Con- Fred Karlsson, Atro Voutilainen, Juha Heikkila, Atro Anttila. 1995. Grammar, A language-Independent System for Unrestricted Text. de Gruyter. Mehryar Mohri and Richard Sproat. 1996. An efficient compiler for weighted rewrite rules. 34th of the Association for Computational Linguistics (ACL 96), Proceedings of the Conference, Santa Cruz, California. ACL. Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 1998a. A rational design for a finite-state transducer library. Lec- Notes in Computer Science, appear. Mehryar Mohri, Michael Riley, Don Hindle, Andrej Ljolje, and Fernando C. N. Pereira. 1998b. Full expansion of context-dependent networks in large vocabulary speech recogni- In of the International Conference on Acoustics, Speech, and Signal Pro- (ICASSP &apos;98), Washington. Mehryar Mohri. 1997. Finite-state transducers language and speech processing. Compu- Linguistics, Mehryar Mohri. 1998a. Minimization algofor sequential transducers. Science, appear. Mehryar Mohri. 1998b. Weighted Grammar Tools: the GRM Library. In preparation. Fernando C. N. Pereira and Rebecca N. Wright. 1997. Finite-state approximation of phrase-</note>
<author confidence="0.574549">In Emmanuel Roche</author>
<affiliation confidence="0.676461">Yves Schabes, editors, Lan-</affiliation>
<address confidence="0.8932175">Processing, 149-173. MIT Press, Cambridge, Massachusetts.</address>
<note confidence="0.8753692">Lawrence Rabiner and Biing-Hwang Juang. of Speech Recognition. Prentice-Hall, Englewood Cliffs, NJ. Arto Salomaa and Matti Soittola. 1978. Automata-Theoretic Aspects of Formal Power New York. Joseph S. Ullian. 1967. Partial algorithm probfor context free languages. Control, 897</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The Theory of Parsing, Translation and Compiling.</title>
<date>1973</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="9451" citStr="Aho and Ullman, 1973" startWordPosition="1508" endWordPosition="1511">es determinization and minimization algorithms for weighted transducers (Mohri, 1997) to increase the sharing — factorirzg — among grammar rules that start or end the same way. The preprocessing step builds a weighted transducer in which each path corresponds to a grammar rule. Rule X a ...Yn has a corresponding path that maps X to the sequence Y1 ...Yr, with weight a. For example, the small CFG in Figure 1 is preprocessed into the compacted transducer shown in Figure 2. 2.3. Compilation The compilation of weighted left-linear or rightlinear grammars into weighted automata is straightforward (Aho and Ullman, 1973). In the right-linear case, for instance, the states of the automaton are the grammar nonterminals together with a new final state F. There is a 3er can be extended as usual to map Es x R to Reg( A• x R). Z .1 XY (1) X .2 -4 aY Y .3 —&gt; bX V.4 —&gt; c Figure 1: Grammar GI. Figure 2: Weighted transducer T(G1)• transition labeled with a E E and weight a E R from X E V to Y E V if the grammar contains the rule X a aY. There is a transition from X to F labeled with a and weight a if X a —&gt; a is a rule of the grammar. The initial states are the states corresponding to the active nonterminals. For examp</context>
</contexts>
<marker>Aho, Ullman, 1973</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1973. The Theory of Parsing, Translation and Compiling. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The design and analysis of computer algorithms.</title>
<date>1974</date>
<publisher>Addison Wesley:</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="11854" citStr="Aho et al., 1974" startWordPosition="1968" endWordPosition="1971"> graph DG, for grammar G1. left-hand nonterminals in S, with nonterminals not in S treated as terminal symbols. If each of these subgrammars is either left-linear or rightlinear, we shall see that compilation into a single finite automaton is possible. The dependency graph DG can be obtained easily from the transducer r(G). For example, Figure 4 shows the dependency graph for our example grammar GI, with SCCs {Z} and {X, Y}. It is clear that G1 satisfies our condition, and Figure 5 shows the result of compiling G1 with A = {Z}. • The SCCs of DG can be obtained in time linear in the size of G (Aho et al., 1974). Before starting the compilation, we check that each subgrammar is left-linear or right-linear (as noted above, nonterminals not in the SCC of a subgrammar are treated as terminals). For example, if {Xi, X2} is an SCC, then the subgrammar X1 -4 aY1bY2X1 (2) X1 -4. bY2aY1 X2 X2 -4 bbY1abX1 Figure 5: Compilation of G1 with start symbol Z. Figure 6: Weighted automaton K({X,Y}) corresponding to the strongly connected component {X, Y} of G1. with X1, X27 Y1) Y2 E V and a, b E E is rightlinear, since expressions such as aY1bY2 can be treated as elements of the terminal alphabet of the subgrammar. W</context>
</contexts>
<marker>Aho, Hoperoft, Ullman, 1974</marker>
<rawString>Alfred V. Aho, John E. Hoperoft, and Jeffrey D. Ullman. 1974. The design and analysis of computer algorithms. Addison Wesley: Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Michel Autebert</author>
<author>Jean Berstel</author>
<author>Luc Boasson</author>
</authors>
<title>Context-free languages and pushdown automata.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages,</booktitle>
<volume>1</volume>
<pages>111--172</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6661" citStr="Autebert et al., 1997" startWordPosition="1017" endWordPosition="1020"> automata that are combined at runtime according to the current dynamic grammar configuration and the strings being recognized. Since not all CFGs can be compiled into weighted automata, the compilation algorithm may reject an input grammar. The class of allowed grammars will be defined later. 2.1. Dynamic grammars The following notation will be used in the rest of the paper. A weighted CFG G = (V, P) over the alphabet E, with real-number weights consists of a finite alphabet V of variables or nonterminals disjoint from E, and a finite set PC VxR x (V U E)* of productions or derivation rules (Autebert et al., 1997). Given strings U, v E (V U E)*, and real numbers c and c&apos;, we write (u, c) 4 (v, c&apos;) when there is a derivation from u with weight c to v with weight c&apos;. We denote by LG(X) the weighted language generated by a nonterminal X: LG(X)= {(w,C) E E* x R : (X, 0) 4 (w, c)} We can now define the two grammar-changing operations that we use. Dynamic activation or deactivation of rules 2 We augment the grammar with a set of active nonterminals, which are those available as start symbols for derivations. More precisely, let A C V be the set of active nonterminals. The language generated by G is then LG =</context>
</contexts>
<marker>Autebert, Berstel, Boasson, 1997</marker>
<rawString>Jean-Michel Autebert, Jean Berstel, and Luc Boasson. 1997. Context-free languages and pushdown automata. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, volume 1, pages 111-172. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Fred Jelinek</author>
<author>Robert Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<pages>5--2</pages>
<contexts>
<context position="2389" citStr="Bahl et al., 1983" startWordPosition="349" endWordPosition="352">bability of the rule. Rule probabilities can be estimated in a variety of ways, which we will not discuss further in this paper. Since speech recognizers cannot be fully certain about the correct transcription of a spoken utterance, they instead generate a range of alternative hypotheses with associated probabilities. An essential function of the grammar is then to rank those hypotheses according to the probability that they would be actually uttered. The grammar is thus used together with other information sources — pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) — to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wri</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit R. Bahl, Fred Jelinek, and Robert Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 5(2):179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
</authors>
<title>Transductions and ContextFree Languages. Teubner Studienbucher:</title>
<date>1979</date>
<location>Stuttgart.</location>
<contexts>
<context position="7684" citStr="Berstel, 1979" startWordPosition="1216" endWordPosition="1217">a set of active nonterminals, which are those available as start symbols for derivations. More precisely, let A C V be the set of active nonterminals. The language generated by G is then LG = UXEA LG(X). Note that inactive nonterminals, and the rules involving them, are available for use in derivations; they are just not available as start symbols. Dynamic rule activation or deactivation is just the dynamic redefinition of the set A in successive uses of the grammar. Dynamic substitution Let a be a weighted rational transduction of E* to A* x R, E C A, that is a regular weighted substitution (Berstel, 1979). a is a monoid morphism verifying: 2This is the terminology used in this area, though a more appropriate expression would be dynamic activation or deactivation of nonterminal symbols. 892 Vs E E, a(x) C Reg(er x R) where Reg(* x R) denotes the set of weighted regular languages over the alphabet A. Thus a simply substitutes for each symbol a E Ea weighted regular expression a(a). A dynamic substitution consists of the application of the substitution a to E, during the process of recognition of a word sequence. Thus, after substitution, the language generated by the new grammar G&apos; is:3 .L01 = a</context>
</contexts>
<marker>Berstel, 1979</marker>
<rawString>Jean Berstel. 1979. Transductions and ContextFree Languages. Teubner Studienbucher: Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Brown</author>
<author>Bruce M Buntschuh</author>
</authors>
<title>A context-free grammar compiler for speech understanding systems.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP &apos;94),</booktitle>
<pages>21--24</pages>
<location>Yokohama, Japan.</location>
<contexts>
<context position="2879" citStr="Brown and Buntschuh, 1994" startWordPosition="423" endWordPosition="426">together with other information sources — pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) — to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1- lian, 1967). However, we will describe a useful class of grammars that can be so transformed, and a tra</context>
</contexts>
<marker>Brown, Buntschuh, 1994</marker>
<rawString>Michael K. Brown and Bruce M. Buntschuh. 1994. A context-free grammar compiler for speech understanding systems. In Proceedings of the International Conference on Spoken Language Processing (ICSLP &apos;94), pages 21-24, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A finite-state parser for use in speech recognition.</title>
<date>1983</date>
<booktitle>In 21st Meeting of the Association for Computational Linguistics (ACL &apos;83), Proceedings of the Conference.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="2851" citStr="Church, 1983" startWordPosition="421" endWordPosition="422"> is thus used together with other information sources — pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) — to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1- lian, 1967). However, we will describe a useful class of grammars that can </context>
</contexts>
<marker>Church, 1983</marker>
<rawString>Kenneth W. Church. 1983. A finite-state parser for use in speech recognition. In 21st Meeting of the Association for Computational Linguistics (ACL &apos;83), Proceedings of the Conference. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkila, and Atro Anttila.</title>
<date>1995</date>
<marker>Karlsson, 1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Atro Anttila. 1995. Constraint Grammar, A language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Richard Sproat</author>
</authors>
<title>An efficient compiler for weighted rewrite rules.</title>
<date>1996</date>
<booktitle>In 34th Meeting of the Association for Computational Linguistics (ACL 96), Proceedings of the Conference,</booktitle>
<publisher>ACL.</publisher>
<location>Santa Cruz, California.</location>
<contexts>
<context position="17087" citStr="Mohri and Sproat, 1996" startWordPosition="2874" endWordPosition="2877">implementation is crucial for dynamic substitutions. 3. Optimizations, Experiments and Results We have a full implementation of the compilation algorithm presented in the previous section, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&amp;T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer determinizatio</context>
</contexts>
<marker>Mohri, Sproat, 1996</marker>
<rawString>Mehryar Mohri and Richard Sproat. 1996. An efficient compiler for weighted rewrite rules. In 34th Meeting of the Association for Computational Linguistics (ACL 96), Proceedings of the Conference, Santa Cruz, California. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>A rational design for a weighted finite-state transducer library.</title>
<date>1998</date>
<journal>Lecture Notes in Computer Science,</journal>
<note>to appear.</note>
<contexts>
<context position="3231" citStr="Mohri et al., 1998" startWordPosition="478" endWordPosition="481">of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1- lian, 1967). However, we will describe a useful class of grammars that can be so transformed, and a transformation algorithm that avoids some of the potential for combinatorial &apos;Grammars representing regular languages have also been used successfully in other areas of computational linguistics (Karlsson et al., 1995). 891 explosion in the process. Spoken dialogue systems require grammars or language models to change as the dialogue proceeds, because p</context>
<context position="17269" citStr="Mohri et al., 1998" startWordPosition="2903" endWordPosition="2906">, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&amp;T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer determinization and minimization (Mohri, 1997; Mohri, 1998a). To show 8For any X, this is a constant time operation. For instance, if K(S) is right-linear, we just need to pick out the state assoc</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1998</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 1998a. A rational design for a weighted finite-state transducer library. Lecture Notes in Computer Science, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
<author>Don Hindle</author>
<author>Andrej Ljolje</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Full expansion of context-dependent networks in large vocabulary speech recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP &apos;98),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="3231" citStr="Mohri et al., 1998" startWordPosition="478" endWordPosition="481">of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1- lian, 1967). However, we will describe a useful class of grammars that can be so transformed, and a transformation algorithm that avoids some of the potential for combinatorial &apos;Grammars representing regular languages have also been used successfully in other areas of computational linguistics (Karlsson et al., 1995). 891 explosion in the process. Spoken dialogue systems require grammars or language models to change as the dialogue proceeds, because p</context>
<context position="17269" citStr="Mohri et al., 1998" startWordPosition="2903" endWordPosition="2906">, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&amp;T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer determinization and minimization (Mohri, 1997; Mohri, 1998a). To show 8For any X, this is a constant time operation. For instance, if K(S) is right-linear, we just need to pick out the state assoc</context>
</contexts>
<marker>Mohri, Riley, Hindle, Ljolje, Pereira, 1998</marker>
<rawString>Mehryar Mohri, Michael Riley, Don Hindle, Andrej Ljolje, and Fernando C. N. Pereira. 1998b. Full expansion of context-dependent networks in large vocabulary speech recognition. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP &apos;98), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--2</pages>
<contexts>
<context position="8915" citStr="Mohri, 1997" startWordPosition="1417" endWordPosition="1418">allows for both of those dynamic grammar changes without recompiling the grammar. 2.2. Preprocessing Our compilation algorithm operates on a weighted transducer r(G) encoding a factored representation of the weighted CFG G, which is generated from G by a separate preprocessor. This preprocessor is not strictly needed, since we could use a version of the main algorithm that works directly on G. However, preprocessing can improve dramatically the time and space needed for the main compilation step, since the preprocessor uses determinization and minimization algorithms for weighted transducers (Mohri, 1997) to increase the sharing — factorirzg — among grammar rules that start or end the same way. The preprocessing step builds a weighted transducer in which each path corresponds to a grammar rule. Rule X a ...Yn has a corresponding path that maps X to the sequence Y1 ...Yr, with weight a. For example, the small CFG in Figure 1 is preprocessed into the compacted transducer shown in Figure 2. 2.3. Compilation The compilation of weighted left-linear or rightlinear grammars into weighted automata is straightforward (Aho and Ullman, 1973). In the right-linear case, for instance, the states of the auto</context>
<context position="17718" citStr="Mohri, 1997" startWordPosition="2977" endWordPosition="2978">tto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer determinization and minimization (Mohri, 1997; Mohri, 1998a). To show 8For any X, this is a constant time operation. For instance, if K(S) is right-linear, we just need to pick out the state associated to X in K(X). 6In fact, our implementation allows more generally dynamic substitutions by weighted transducers. 895 no optimization - (size / 25) optimization 50 100 150 200 250 50 100 150 200 250 VOCABULARY VOCABULARY Figure 8: Advantage of transducer representation combined with preoptimization: time and space. the benefits of this representation, we compared the compilation time and the size of the resulting lazy automata with and witho</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Minimization algorithms for sequential transducers.</title>
<date>1998</date>
<journal>Theoretical Computer Science,</journal>
<note>to appear.</note>
<contexts>
<context position="16895" citStr="Mohri, 1998" startWordPosition="2847" endWordPosition="2848">nd, with only the necessary part of Ca being expanded for a given input string. In practice, the automaton Ca can be large, a list of city or person names for example. Thus a lazy implementation is crucial for dynamic substitutions. 3. Optimizations, Experiments and Results We have a full implementation of the compilation algorithm presented in the previous section, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&amp;T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilat</context>
</contexts>
<marker>Mohri, 1998</marker>
<rawString>Mehryar Mohri. 1998a. Minimization algorithms for sequential transducers. Theoretical Computer Science, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted Grammar Tools: the GRM Library. In preparation.</title>
<date>1998</date>
<contexts>
<context position="16895" citStr="Mohri, 1998" startWordPosition="2847" endWordPosition="2848">nd, with only the necessary part of Ca being expanded for a given input string. In practice, the automaton Ca can be large, a list of city or person names for example. Thus a lazy implementation is crucial for dynamic substitutions. 3. Optimizations, Experiments and Results We have a full implementation of the compilation algorithm presented in the previous section, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&amp;T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilat</context>
</contexts>
<marker>Mohri, 1998</marker>
<rawString>Mehryar Mohri. 1998b. Weighted Grammar Tools: the GRM Library. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Rebecca N Wright</author>
</authors>
<title>Finite-state approximation of phrasestructure grammars.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing,</booktitle>
<pages>149--173</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2999" citStr="Pereira and Wright, 1997" startWordPosition="443" endWordPosition="446">hl et al., 1983; Rabiner and Juang, 1993) — to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1- lian, 1967). However, we will describe a useful class of grammars that can be so transformed, and a transformation algorithm that avoids some of the potential for combinatorial &apos;Grammars representing regular languages have </context>
<context position="10578" citStr="Pereira and Wright, 1997" startWordPosition="1729" endWordPosition="1732">the grammar. The initial states are the states corresponding to the active nonterminals. For example, Figure 3 shows the weighted automaton for grammar G2 consisting of the last three rules of G1 with start symbol X. However, the standard methods for left- and right-linear grammars cannot be used for grammars such as G1 that generate regular sets but have rules that are neither left- nor right-linear. But we can use the methods for left- and rightlinear grammars as subroutines if the grammar can be decomposed into left-linear and rightlinear components that do not call each other recursively (Pereira and Wright, 1997). More precisely, define a dependency graph DG for G&apos;s nonterminals and examine the set of its strongly-connected components (SCCs).4 The nodes of DG are G&apos;s nonterminals, and there is a directed edge from X to Y if Y appears in the right-hand side of a rule with left-hand side X, that is, if the definition of X depends on Y. Each SCC S of DG has a corresponding subgrammar of G consisting of those rules with 4 Recall that the strongly connected components of a directed graph are the equivalence classes of graph nodes under the relation R defined by: q R q&apos; if q&apos; can be reached from q and q fro</context>
<context position="18667" citStr="Pereira and Wright, 1997" startWordPosition="3123" endWordPosition="3126">50 200 250 50 100 150 200 250 VOCABULARY VOCABULARY Figure 8: Advantage of transducer representation combined with preoptimization: time and space. the benefits of this representation, we compared the compilation time and the size of the resulting lazy automata with and without preoptimization. The advantage of preoptimization would be even greater if the compilation output were fully expanded rather than on-demand. We did experiments with full bigram models with various vocabulary sizes, and with two unweighted grammars derived by feature instantiation from hand-built feature-based grammars (Pereira and Wright, 1997). Figure 8 shows the compilation times of full bigram models with and without preoptimization, demonstrating the importance of the optimization allowed by using a transducer representation of the grammar. For a 250-word vocabulary model, the compilation time is about 50 times faster with the preoptimized representation.w Figure 8 also shows the sizes of the resulting lazy automata in the two cases. While in the preoptimized case time and space grow linearly with vocabulary size (0( IGI)), they grow quadratically in the unoptimized case (0(IGI)). The bigram examples also show the advantages of </context>
</contexts>
<marker>Pereira, Wright, 1997</marker>
<rawString>Fernando C. N. Pereira and Rebecca N. Wright. 1997. Finite-state approximation of phrasestructure grammars. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, pages 149-173. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>Fundamentals of Speech Recognition.</title>
<date>1993</date>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="2415" citStr="Rabiner and Juang, 1993" startWordPosition="353" endWordPosition="356">e. Rule probabilities can be estimated in a variety of ways, which we will not discuss further in this paper. Since speech recognizers cannot be fully certain about the correct transcription of a spoken utterance, they instead generate a range of alternative hypotheses with associated probabilities. An essential function of the grammar is then to rank those hypotheses according to the probability that they would be actually uttered. The grammar is thus used together with other information sources — pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) — to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often rem resent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of a more general CFG (Pereira and Wright, 1997).&apos; Assuming that</context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Lawrence Rabiner and Biing-Hwang Juang. 1993. Fundamentals of Speech Recognition. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arto Salomaa</author>
<author>Matti Soittola</author>
</authors>
<title>Automata-Theoretic Aspects of Formal Power Series.</title>
<date>1978</date>
<publisher>Springer-Verlag:</publisher>
<location>New York.</location>
<contexts>
<context position="1660" citStr="Salomaa and Soittola, 1978" startWordPosition="236" endWordPosition="239">ubstitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach. 1. Motivation Context-free grammars (CFGs) are widely used in language processing systems. In many applications, in particular in speech recognition, in addition to recognizing grammatical sequences it is necessary to provide some measure of the probability of those sequences. It is then natural to use weighted CFGs, in which each rule is given a weight from an appropriate weight algebra (Salomaa and Soittola, 1978). Weights can encode probabilities, for instance by setting a rule&apos;s weight to the negative logarithm of the probability of the rule. Rule probabilities can be estimated in a variety of ways, which we will not discuss further in this paper. Since speech recognizers cannot be fully certain about the correct transcription of a spoken utterance, they instead generate a range of alternative hypotheses with associated probabilities. An essential function of the grammar is then to rank those hypotheses according to the probability that they would be actually uttered. The grammar is thus used togethe</context>
<context position="5839" citStr="Salomaa and Soittola, 1978" startWordPosition="878" endWordPosition="881">wever possible to speedup grammar compilation further by applying pre-compilation optimizations to the grammar, as we will see later. The class of grammars to which our algorithm applies includes rightlinear grammars, left-linear grammars and certain combinations thereof. The algorithm has been fully implemented and evaluated experimentally, demonstrating its effectiveness. 2. Algorithm We will start by giving a precise definition of dynamic grammars. We will then explain each stage of grammar compilation. Grammar compilation takes as input a weighted CFG represented as a weighted transducer (Salomaa and Soittola, 1978), which may have been optimized prior to compilation (preoptimized). The weighted transducer is analyzed by the compilation algorithm, and the analysis, if successful, outputs a collection of weighted automata that are combined at runtime according to the current dynamic grammar configuration and the strings being recognized. Since not all CFGs can be compiled into weighted automata, the compilation algorithm may reject an input grammar. The class of allowed grammars will be defined later. 2.1. Dynamic grammars The following notation will be used in the rest of the paper. A weighted CFG G = (V</context>
</contexts>
<marker>Salomaa, Soittola, 1978</marker>
<rawString>Arto Salomaa and Matti Soittola. 1978. Automata-Theoretic Aspects of Formal Power Series. Springer-Verlag: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph S Ullian</author>
</authors>
<title>Partial algorithm problems for context free languages.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<pages>11--90</pages>
<marker>Ullian, 1967</marker>
<rawString>Joseph S. Ullian. 1967. Partial algorithm problems for context free languages. Information and Control, 11:90-101.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>