<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002753">
<title confidence="0.994343">
Improving the Accuracy of Subcategorizations Acquired from Corpora
</title>
<author confidence="0.996564">
Naoki Yoshinaga
</author>
<affiliation confidence="0.998521">
Department of Computer Science,
University of Tokyo
</affiliation>
<address confidence="0.841855">
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033
</address>
<email confidence="0.999225">
yoshinag@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924066666667">
This paper presents a method of improv-
ing the accuracy of subcategorization
frames (SCFs) acquired from corpora to
augment existing lexicon resources. I
estimate a confidence value of each SCF
using corpus-based statistics, and then
perform clustering of SCF confidence-
value vectors for words to capture co-
occurrence tendency among SCFs in the
lexicon. I apply my method to SCFs
acquired from corpora using lexicons
of two large-scale lexicalized grammars.
The resulting SCFs achieve higher pre-
cision and recall compared to SCFs ob-
tained by naive frequency cut-off.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964928571428">
Recently, a variety of methods have been proposed
for acquisition of subcategorization frames (SCFs)
from corpora (surveyed in (Korhonen, 2002)).
One interesting possibility is to use these tech-
niques to improve the coverage of existing large-
scale lexicon resources such as lexicons of lexi-
calized grammars. However, there has been little
work on evaluating the impact of acquired SCFs
with the exception of (Carroll and Fang, 2004).
The problem when we integrate acquired SCFs
into existing lexicalized grammars is lower qual-
ity of the acquired SCFs, since they are acquired
in an unsupervised manner, rather than being man-
ually coded. If we attempt to compensate for the
poor precision by being less strict in filtering out
less likely SCFs, then we will end up with a larger
number of noisy lexical entries, which is problem-
atic for parsing with lexicalized grammars (Sarkar
et al., 2000). We thus need some method of select-
ing the most reliable set of SCFs from the system
output as demonstrated in (Korhonen, 2002).
In this paper, I present a method of improving
the accuracy of SCFs acquired from corpora in or-
der to augment existing lexicon resources. I first
estimate a confidence value that a word can have
each SCF, using corpus-based statistics. To cap-
ture latent co-occurrence tendency among SCFs
in the target lexicon, I next perform clustering of
SCF confidence-value vectors of words in the ac-
quired lexicon and the target lexicon. Since each
centroid value of the obtained clusters indicates
whether the words in that cluster have each SCF,
we can eliminate SCFs acquired in error and pre-
dict possible SCFs according to the centroids.
I applied my method to SCFs acquired from
a corpus of newsgroup posting about mobile
phones (Carroll and Fang, 2004), using the
XTAG English grammar (XTAG Research Group,
2001) and the LinGO English Resource Grammar
(ERG) (Copestake, 2002). I then compared the
resulting SCFs with SCFs obtained by naive fre-
quency cut-off to observe the effects of clustering.
</bodyText>
<sectionHeader confidence="0.712201333333333" genericHeader="introduction">
2 Background
2.1 SCF Acquisition for Lexicalized
Grammars
</sectionHeader>
<bodyText confidence="0.99958">
I start by acquiring SCFs for a lexicalized gram-
mar from corpora by the method described in (Car-
roll and Fang, 2004).
</bodyText>
<equation confidence="0.9845368">
#S(EPATTERN :TARGET |yield|
:SUBCAT (VSUBCAT NP)
:CLASSES ((24 51 161) 5293)
:RELIABILITY 0
:FREQSCORE 0.26861903
:FREQCNT 1 :TLTL (VV0)
:SLTL ((|route |NN1))
:OLT1L ((|result |NN2))
:OLT2L NIL
:OLT3L NIL :LRL 0))
</equation>
<figureCaption confidence="0.996889">
Figure 1: An acquired SCF for a verb “yield”
</figureCaption>
<bodyText confidence="0.999984129032258">
In their study, they first acquire fine-grained
SCFs using the unsupervised method proposed by
Briscoe and Carroll (1997) and Korhonen (2002).
Figure 1 shows an example of one acquired SCF
entry for a verb “yield.” Each SCF entry has
several fields about the observed SCF. I explain
here only its portion related to this study. The
TARGET field is a word stem, the first number in
the CLASSES field indicates an SCF type, and the
FREQCNT field shows how often words derivable
from the word stem appeared with the SCF type in
the training corpus. The obtained SCFs comprise
the total 163 SCF types which are originally based
on the SCFs in the ANLT (Boguraev and Briscoe,
1987) and COMLEX (Grishman et al., 1994) dic-
tionaries. In this example, the SCF type 24 corre-
sponds to an SCF of transitive verb.
They then obtain SCFs for the target lexicalized
grammar (the LinGO ERG (Copestake, 2002) in
their study) using a handcrafted translation map
from these 163 types to the SCF types in the target
grammar. They reported that they could achieve
a coverage improvement of 4.5% but that aver-
age parse time was doubled. This is because they
did not use any filtering method for the acquired
SCFs to suppress an increase of the lexical ambi-
guity. We definitely need some method to control
the quality of the acquired SCFs.
Their method is extendable to any lexicalized
grammars, if we could have a translation map from
these 163 types to the SCF types in the grammar.
</bodyText>
<subsectionHeader confidence="0.999897">
2.2 Clustering of Verb SCF Distributions
</subsectionHeader>
<bodyText confidence="0.99274625">
There is some related work on clustering of
verbs according to their SCF probability distri-
butions (Schulte im Walde and Brew, 2002; Ko-
rhonen et al., 2003). Schulte im Walde and
</bodyText>
<figure confidence="0.914023">
(true) probability distribution
NP None NP_to-PP NP_PP PP
subcategorization frame
</figure>
<figureCaption confidence="0.999597">
Figure 2: SCF probability distributions for apply
</figureCaption>
<bodyText confidence="0.9996199375">
Brew (2002) used the k-Means (Forgy, 1965) al-
gorithm to cluster SCF distributions for monose-
mous verbs while Korhonen et al. (2003) applied
other clustering methods to cluster polysemic SCF
data. These studies aim at obtaining verb seman-
tic classes, which are closely related to syntactic
behavior of argument selection (Levin, 1993).
Korhonen (2002) made use of SCF distributions
for representative verbs in Levin’s verb classes to
obtain accurate back-off estimates for all the verbs
in the classes. In this study, I assume that there
are classes whose element words have identical
SCF types. I then obtain these classes by clus-
tering acquired SCFs, using information available
in the target lexicon, and directly use the obtained
classes to eliminate implausible SCFs.
</bodyText>
<sectionHeader confidence="0.998002" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.999981">
3.1 Estimation of Confidence Values for SCFs
</subsectionHeader>
<bodyText confidence="0.999923388888889">
I first create an SCF confidence-value vector vi for
each word wi, an object for clustering. Each el-
ement vij in vi represents a confidence value of
SCF sj for a word wi, which expresses how strong
the evidence is that the word wi has SCF sj. Note
that a confidence value confij is not a probability
that a word wi appears with SCF sj but a proba-
bility of existence of SCF sj for the word wi. In
this study, I assume that a word wi appears with
each SCF sj with a certain (non-zero) probabil-
ity θij(= p(sij|wi) &gt; 0 where ∑ j θij = 1), but only
SCFs whose probabilities exceed a certain thresh-
old are recognized in the lexicon. I hereafter call
this threshold recognition threshold. Figure 2 de-
picts a probability distribution of SCF for apply.
In this context, I can regard a confidence value of
each SCF as a probability that the probability of
that SCF exceeds the recognition threshold.
</bodyText>
<figure confidence="0.996844266666667">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
recognition
threshold
probability
apply
</figure>
<bodyText confidence="0.999693523809524">
One intuitive way to estimate a confidence value
is to assume an observed probability, i.e., relative
frequency, is equal to a probability θij of SCF sj
for a word wi (θij = freqij/∑ j freqij where freqij
is a frequency that a word wi appears with SCF sj
in corpora). When the relative frequency of sj for
a word wi exceeds the recognition threshold, its
confidence value confij is set to 1, and otherwise
confij is set to 0. However, an observed probabil-
ity is unreliable for infrequent words. Moreover,
when we want to encode confidence values of re-
liable SCFs in the target grammar, we cannot dis-
tinguish the confidence values of those SCFs with
confidence values of acquired SCFs.
The other promising way to estimate a confi-
dence value, which I adopt in this study, is to as-
sume a probability θij as a stochastic variable in
the context of Bayesian statistics (Gelman et al.,
1995). In this context, a posteriori distribution of
the probability θij of an SCF sj for a word wi is
given by:
</bodyText>
<equation confidence="0.89708475">
P(θij)P(D|θij)
p(θij|D) = P(D)
P(θij)P(D|θij) (1)
f o P(θij)P(D |θij)dθij,
</equation>
<bodyText confidence="0.99842125">
where P(θij) is a priori distribution, and D is the
data we have observed. Since every occurrence
of SCFs in the data D is independent with each
other, the data D can be regarded as Bernoulli tri-
als. When we observe the data D that a word wi
appears n times in total and x(≤ n) times with SCF
sj,1 its conditional distribution is represented by
binominal distribution:
</bodyText>
<equation confidence="0.969979">
P(D |θij) = \xl θxij(1 − θij)(n−x) (2)
</equation>
<bodyText confidence="0.999876875">
To calculate this a posteriori distribution, I need
to define the apriori distribution P(θij). The ques-
tion is which probability distribution of θij can
appropriately reflects prior knowledge. In other
words, it should encode knowledge we use to es-
timate SCFs for unknown words. I simply deter-
mine it from distributions of observed probability
values of sj for words seen in corpora2 by using
</bodyText>
<footnote confidence="0.9878305">
1The values of FREQCNT is used to obtain n and x.
2I estimated a priori distribution separately for each type
of SCF from words that appeared more than 50 times in the
training corpus in the following experiments.
</footnote>
<bodyText confidence="0.996238666666667">
a method described in (Tsuruoka and Chikayama,
2001). In their study, they assume a priori distri-
bution as the beta distribution defined as:
</bodyText>
<equation confidence="0.989615">
θα−1
i j (1 − θij)β−1
, (3)
</equation>
<bodyText confidence="0.8923365">
where B(α,β) = f01 θα−1
i j (1 − θij)β−1dθij. The
value of α and β is determined by moment esti-
mation.3 By substituting Equations 2 and 3 into
Equation 1, I finally obtain the a posteriori distri-
bution p(θij|D) as:
</bodyText>
<equation confidence="0.84788425">
p(θij|α,β,D)=c · θx+α−1
i j (1−θij)n−x+β−1,(4)
where c = (n )/(B(α,β)f0 1 P(θij)P(D|θij)dθij).
x
</equation>
<bodyText confidence="0.9759015">
When I regard the recognition threshold as t, I
can calculate a confidence value confij that a word
wi can have sj by integrating the a posteriori dis-
tribution p(θij|D) from the threshold t to 1:
</bodyText>
<equation confidence="0.94161325">
nf
co
c· θi +α−1(1 − θij )n−x+β−1dθij.(5)
iJ = 1 j
</equation>
<bodyText confidence="0.98879575">
By using this confidence value, I represent an SCF
confidence-value vector vi for a word wi in the ac-
quired SCF lexicon (vij = confij).
In order to combine SCF confidence-value vec-
tors for words acquired from corpora and those for
words in the lexicon of the target grammar, I also
represent an SCF confidence-value vector vi for a
word wZ in the target grammar by:
</bodyText>
<equation confidence="0.941416">
� 1− ε w~ i has sj in the lexicon
v~ ij = otherwise, (6)
ε
</equation>
<bodyText confidence="0.999992333333333">
where ε expresses an unreliability of the lexicon.
In this study, I trust the lexicon as much as possible
by setting εto the machine epsilon.
</bodyText>
<subsectionHeader confidence="0.9906465">
3.2 Clustering of SCF Confidence-Value
Vectors
</subsectionHeader>
<bodyText confidence="0.9989006">
I next present a clustering algorithm of words
according to their SCF confidence-value vectors.
Given k initial representative vectors called cen-
troids, my algorithm iteratively updates clusters by
assigning each data object to its closest centroid
</bodyText>
<footnote confidence="0.704804333333333">
1
3The expectation and variance of the beta distribution are
made equal to those of the observed probability values.
</footnote>
<figure confidence="0.966267444444445">
p(θij|α,β) =
B(α,β)
Input: a set of SCF confidence-value
vectors V = {v1,v2,...,vn} C Rm
a distance function d : Rm xZm → R
a function to compute a centroid
- : {vj1,vj2,...,vjl } → Zm
initial centroids C = {c1,c2,...,ck} C Zm
Output: a set of clusters {Cj}
while cluster members are not stable do
foreach cluster Cj
Cj = {vi |dcl,d(vi,cj) ≥ d(vi,cl)} (1)
end foreach
foreach clusters Cj
cj=-(Cj) (2)
end foreach
end while
return {Cj}
</figure>
<figureCaption confidence="0.987727">
Figure 3: Clustering algorithm for SCF
confidence-value vectors
</figureCaption>
<bodyText confidence="0.996316727272727">
and recomputing centroids until cluster members
become stable, as depicted in Figure 3.
Although this algorithm is roughly based on the
k-Means algorithm, it is different from k-Means in
important respects. I assume the elements of the
centroids of the clusters as a discrete value of 0 or
1 because I want to obtain clusters whose element
words have the exactly same set of SCFs.
I then derive a distance function d to calculate
a probability that a data object vi should have an
SCF set represented by a centroid cm as follows:
</bodyText>
<equation confidence="0.9983505">
d(vi,cm) = ❑ vij · ❑ (1 −vij). (7)
cmj=1 cmj=0
</equation>
<bodyText confidence="0.980637315789474">
By using this function, I can determine the closest
d(vi,cm) ((1) in Figure 3).
After every assignment, I calculate a next cen-
troid cm of each cluster Cm ((2) in Figure 3) by
comparing a probability that the words in the clus-
ter have an SCF sj and a probability that the words
in the cluster do not have the SCF sj as follows:
I next address the way to determine the num-
ber of clusters and initial centroids. In this study,
I assume that the most of the possible set of SCFs
for words are included in the lexicon of the tar-
get grammar,4 and make use of the existing sets of
4When the lexicon is less accurate, I can determine the
number of clusters using other algorithms (Hamerly, 2003).
SCFs for the words in the lexicon to determine the
number of clusters and initial centroids. I first ex-
tract SCF confidence-value vectors from the lexi-
con of the grammar. By eliminating duplications
from them and regarding O= 0 in Equation 6, I ob-
tain initial centroids cm. I then initialize the num-
ber of clusters k to the number of cm.
I finally update the acquired SCFs using the ob-
tained clusters and the confidence values of SCFs
in this order. I call the following procedure cen-
troid cut-off t when the confidence values are es-
timated under the recognition threshold t. Since
the value cmj of a centroid cm in a cluster Cm rep-
resents whether the words in the cluster can have
SCF sj, I first obtain SCFs by collecting SCF sj
for a word wi E Cm when cmj is 1. I then elimi-
nate implausible SCFs sj for wi from the resulting
SCFs according to their confidence values confij.
In the following, I compare centroid cut-off
with frequency cut-off and confidence cut-off t,
which use relative frequencies and confidence val-
ues calculated under the recognition threshold t,
respectively. Note that these cut-offs use only
corpus-based statistics to eliminate SCFs.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99992925">
I applied my method to SCFs acquired from
135,902 sentences of mobile phone newsgroup
postings archived by Google.com, which is the
same data used in (Carroll and Fang, 2004). The
number of acquired SCFs was 14,783 for 3,864
word stems, while the number of SCF types in
the data was 97. I then translated the 163 SCF
types into the SCF types of the XTAG English
grammar (XTAG Research Group, 2001) and the
LinGO ERG (Copestake, 2002)5 using translation
mappings built by Ted Briscoe and Dan Flickinger
from 23 of the SCF types into 13 (out of 57 possi-
ble) XTAG SCF types, and 129 into 54 (out of 216
possible) ERG SCF types.
To evaluate my method, I split each lexicon of
the two grammars into the training SCFs and the
testing SCFs. The words in the testing SCFs were
included in the acquired SCFs. When I apply
my method to the acquired SCFs using the train-
ing SCFs and evaluate the resulting SCFs with the
</bodyText>
<footnote confidence="0.853367">
5I used the same version of the LinGO ERG as (Carroll
and Fang, 2004) (1.4; April 2003) but the map is updated.
cluster as argmax
</footnote>
<figure confidence="0.997005405405405">
Cm
1 when ❑ vij &gt; ❑ (1−vij)
viECm viECm (8)
0 otherwise.
cmj = {
Recall
0 0.2 0.4 0.6 0.8 1
Precision
0 0.2 0.4 0.6 0.8 1
Precision
0.8
0.6
0.4
0.2
0
1
A: frequency cut-off
B: confidence cut-off 0.01
C: confidence cut-off 0.03
D: confidence cut-off 0.05
B C D
A
A
A: frequency cut-off
B: confidence cut-off 0.01
C: confidence cut-off 0.03
D: confidence cut-off 0.05
B
D
C
Recall 1
0.8
0.6
0.4
0.2
0
XTAG ERG
</figure>
<figureCaption confidence="0.9904175">
Figure 4: Precision and recall of the resulting SCFs using confidence cut-offs and frequency cut-off: the
XTAG English grammar (left) the LinGO ERG (right)
</figureCaption>
<figure confidence="0.999649617647059">
0 0.2 0.4 0.6 0.8 1
Precision
0 0.2 0.4 0.6 0.8 1
Precision
Recall
0.8
0.6
0.4
0.2
0
1
A: frequency cut-off
B: centroid cut-off* 0.05
C: centroid cut-off 0.05
D: confidence cut-off 0.05
D
A
C
B
Recall
0.8
0.6
0.4
0.2
0
1
A B
D
A: frequency cut-off
B: centroid cut-off* 0.05
C: centroid cut-off 0.05
D: confidence cut-off 0.05
C
XTAG ERG
</figure>
<figureCaption confidence="0.9874485">
Figure 5: Precision and recall of the resulting SCFs using confidence cut-off and centroid cut-off: the
XTAG English grammar (left) the LinGO ERG (right)
</figureCaption>
<bodyText confidence="0.999819875">
testing SCFs, we can estimate to what extent my
method can preserve reliable SCFs for words un-
known to the grammar.6 The XTAG lexicon was
split into 9,437 SCFs for 8,399 word stems as
training and 423 SCFs for 280 word stems as test-
ing, while the ERG lexicon was split into 1,608
SCFs for 1,062 word stems as training and 292
SCFs for 179 word stems as testing. I extracted
SCF confidence-value vectors from the training
SCFs and the acquired SCFs for the words in the
testing SCFs. The number of the resulting data
objects was 8,679 for XTAG and 1,241 for ERG.
The number of initial centroids7 extracted from
the training SCFs was 49 for XTAG and 53 for
ERG. I then performed clustering of 8,679 data
objects into 49 clusters and 1,241 data objects into
</bodyText>
<footnote confidence="0.999206">
6I here assume that the existing SCFs for the words in the
lexicon is more reliable than the other SCFs for those words.
7I used the vectors that appeared for more than one word.
</footnote>
<bodyText confidence="0.999304625">
53 clusters, and then evaluated the resulting SCFs
by comparing them to the testing SCFs.
I first compare confidence cut-off with fre-
quency cut-off to observe the effects of Bayesian
estimation. Figure 4 shows precision and recall
of the SCFs obtained using frequency cut-off and
confidence cut-off 0.01, 0.03, and 0.05 by varying
threshold for the confidence values and the relative
frequencies from 0 to 1.8 The graph indicates that
the confidence cut-offs achieved higher recall than
the frequency cut-off, thanks to the a priori distri-
butions. When we compare the three confidence
cut-offs, we can improve precision using higher
recognition thresholds while we can improve re-
call using lower recognition thresholds. This is
quite consistent with our expectations.
</bodyText>
<footnote confidence="0.96217375">
Precision= Correct SCFs for the words in the resulting SCFs
8 All SCFs for the words in the resulting SCFs
Recall = Correct SCFs for the words in the resulting SCFs
All SCFs for the words in the test SCFs
</footnote>
<bodyText confidence="0.99992588">
I then compare centroid cut-off with confidence
cut-off to observe the effects of clustering. Fig-
ure 5 shows precision and recall of the resulting
SCFs using centroid cut-off 0.05 and the confi-
dence cut-off 0.05 by varying the threshold for the
confidence values. In order to show the effects
of the use of the training SCFs, I also performed
clustering of SCF confidence-value vectors in the
acquired SCFs with random initialization (k = 49
(for XTAG) and 53 (for ERG); centroid cut-off
0.05*). The graph shows that clustering is mean-
ingful only when we make use of the reliable SCFs
in the manually-coded lexicon. The centroid cut-
off using the lexicon of the grammar boosted pre-
cision compared to the confidence cut-off.
The difference between the effects of my
method on XTAG and ERG would be due to the
finer-grained SCF types of ERG. This resulted
in lower precision of the acquired SCFs for ERG,
which prevented us from distinguishing infrequent
(correct) SCFs from SCFs acquired in error. How-
ever, since unusual SCFs tend to be included in the
lexicon, we will be able to have accurate clusters
for unknown words with smaller SCF variations as
we achieved in the experiments with XTAG.
</bodyText>
<sectionHeader confidence="0.905073" genericHeader="conclusions">
5 Concluding Remarks and Future Work
</sectionHeader>
<bodyText confidence="0.999982214285714">
In this paper, I presented a method to improve
the quality of SCFs acquired from corpora using
existing lexicon resources. I applied my method
to SCFs acquired from corpora using lexicons of
the XTAG English grammar and the LinGO ERG,
and have shown that it can eliminate implausible
SCFs, preserving more reliable SCFs.
In the future, I need to evaluate the quality of
the resulting SCFs by manual analysis and by us-
ing the extended lexicons to improve parsing. I
will investigate other clustering methods such as
hierarchical clustering, and use other information
for clustering such as semantic preference of argu-
ments of SCFs to have more accurate clusters.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992588">
I thank Yoshimasa Tsuruoka and Takuya Mat-
suzaki for their advice on probabilistic modeling,
Alex Fang for his help in using the acquired SCFs,
and Anna Korhonen for her insightful suggestions
on evaluation. I am also grateful to Jun’ichi Tsujii,
Yusuke Miyao, John Carroll and the anonymous
reviewers for their valuable comments. This work
was supported in part by JSPS Research Fellow-
ships for Young Scientists and in part by CREST,
JST (Japan Science and Technology Agency).
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996598720930232">
B. Boguraev and T. Briscoe. 1987. Large lexicons for natural
language processing: utilising the grammar coding system
of LDOCE. Computational Linguistics, 13(4):203–218.
T. Briscoe and J. Carroll. 1997. Automatic extraction of
subcategorization from corpora. In Proc. the fifth ANLP,
pages 356–363.
J. Carroll and A. C. Fang. 2004. The automatic acquisition
of verb subcategorizations and their impact on the perfor-
mance of an HPSG parser. In Proc. thefirst ijc-NLP, pages
107–114.
A. Copestake. 2002. Implementing typed feature structure
grammars. CSLI publications.
E. W. Forgy. 1965. Cluster analysis of multivariate data: Ef-
ficiency vs. interpretability of classifications. Biometrics,
21:768–780.
A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, editors.
1995. Bayesian Data Analysis. Chapman and Hall.
R. Grishman, C. Macleod, and A. Meyers. 1994. Comlex
syntax: Building a computational lexicon. In Proc. the
15th COLING, pages 268–272.
G. Hamerly. 2003. Learning structure and concepts in data
through data clustering. Ph.D. thesis, University of Cali-
fornia, San Diego.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clus-
tering polysemic subcategorization frame distributions se-
mantically. In Proc. the 41stACL, pages 64–71.
A. Korhonen. 2002. Subcategorization Acquisition. Ph.D.
thesis, University of Cambridge.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press.
A. Sarkar, F. Xia, and A. K. Joshi. 2000. Some experiments
on indicators of parsing complexity for lexicalized gram-
mars. In Proc. the 18th COLING workshop, pages 37–42.
S. Schulte im Walde and C. Brew. 2002. Inducing German
semantic verb classes from purely syntactic subcategorisa-
tion information. In Proc. the 41stACL, pages 223–230.
Y. Tsuruoka and T. Chikayama. 2001. Estimating reliability
of contextual evidences in decision-list classifiers under
Bayesian learning. In Proc. the sixth NLPRS, pages 701–
707.
XTAG Research Group. 2001. A Lexicalized Tree Adjoin-
ing Grammar for English. Technical Report IRCS-01-03,
IRCS, University of Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.783432">
<title confidence="0.999551">Improving the Accuracy of Subcategorizations Acquired from Corpora</title>
<author confidence="0.996717">Naoki Yoshinaga</author>
<affiliation confidence="0.999924">Department of Computer Science, University of Tokyo</affiliation>
<address confidence="0.992938">7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033</address>
<email confidence="0.976449">yoshinag@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.98644175">This paper presents a method of improving the accuracy of subcategorization frames (SCFs) acquired from corpora to augment existing lexicon resources. I estimate a confidence value of each SCF using corpus-based statistics, and then perform clustering of SCF confidencevalue vectors for words to capture cooccurrence tendency among SCFs in the lexicon. I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars. The resulting SCFs achieve higher precision and recall compared to SCFs obtained by naive frequency cut-off.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<title>Large lexicons for natural language processing: utilising the grammar coding system of LDOCE.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="3915" citStr="Boguraev and Briscoe, 1987" startWordPosition="629" endWordPosition="632">CFs using the unsupervised method proposed by Briscoe and Carroll (1997) and Korhonen (2002). Figure 1 shows an example of one acquired SCF entry for a verb “yield.” Each SCF entry has several fields about the observed SCF. I explain here only its portion related to this study. The TARGET field is a word stem, the first number in the CLASSES field indicates an SCF type, and the FREQCNT field shows how often words derivable from the word stem appeared with the SCF type in the training corpus. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe, 1987) and COMLEX (Grishman et al., 1994) dictionaries. In this example, the SCF type 24 corresponds to an SCF of transitive verb. They then obtain SCFs for the target lexicalized grammar (the LinGO ERG (Copestake, 2002) in their study) using a handcrafted translation map from these 163 types to the SCF types in the target grammar. They reported that they could achieve a coverage improvement of 4.5% but that average parse time was doubled. This is because they did not use any filtering method for the acquired SCFs to suppress an increase of the lexical ambiguity. We definitely need some method to co</context>
</contexts>
<marker>Boguraev, Briscoe, 1987</marker>
<rawString>B. Boguraev and T. Briscoe. 1987. Large lexicons for natural language processing: utilising the grammar coding system of LDOCE. Computational Linguistics, 13(4):203–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proc. the fifth ANLP,</booktitle>
<pages>356--363</pages>
<contexts>
<context position="3360" citStr="Briscoe and Carroll (1997)" startWordPosition="529" endWordPosition="532">ined by naive frequency cut-off to observe the effects of clustering. 2 Background 2.1 SCF Acquisition for Lexicalized Grammars I start by acquiring SCFs for a lexicalized grammar from corpora by the method described in (Carroll and Fang, 2004). #S(EPATTERN :TARGET |yield| :SUBCAT (VSUBCAT NP) :CLASSES ((24 51 161) 5293) :RELIABILITY 0 :FREQSCORE 0.26861903 :FREQCNT 1 :TLTL (VV0) :SLTL ((|route |NN1)) :OLT1L ((|result |NN2)) :OLT2L NIL :OLT3L NIL :LRL 0)) Figure 1: An acquired SCF for a verb “yield” In their study, they first acquire fine-grained SCFs using the unsupervised method proposed by Briscoe and Carroll (1997) and Korhonen (2002). Figure 1 shows an example of one acquired SCF entry for a verb “yield.” Each SCF entry has several fields about the observed SCF. I explain here only its portion related to this study. The TARGET field is a word stem, the first number in the CLASSES field indicates an SCF type, and the FREQCNT field shows how often words derivable from the word stem appeared with the SCF type in the training corpus. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe, 1987) and COMLEX (Grishman et al., 1994) dictionar</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>T. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proc. the fifth ANLP, pages 356–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>A C Fang</author>
</authors>
<title>The automatic acquisition of verb subcategorizations and their impact on the performance of an HPSG parser.</title>
<date>2004</date>
<booktitle>In Proc. thefirst ijc-NLP,</booktitle>
<pages>107--114</pages>
<contexts>
<context position="1225" citStr="Carroll and Fang, 2004" startWordPosition="177" endWordPosition="180">om corpora using lexicons of two large-scale lexicalized grammars. The resulting SCFs achieve higher precision and recall compared to SCFs obtained by naive frequency cut-off. 1 Introduction Recently, a variety of methods have been proposed for acquisition of subcategorization frames (SCFs) from corpora (surveyed in (Korhonen, 2002)). One interesting possibility is to use these techniques to improve the coverage of existing largescale lexicon resources such as lexicons of lexicalized grammars. However, there has been little work on evaluating the impact of acquired SCFs with the exception of (Carroll and Fang, 2004). The problem when we integrate acquired SCFs into existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the poor precision by being less strict in filtering out less likely SCFs, then we will end up with a larger number of noisy lexical entries, which is problematic for parsing with lexicalized grammars (Sarkar et al., 2000). We thus need some method of selecting the most reliable set of SCFs from the system output as demonstrated in (Korhonen, 2002). In this pap</context>
<context position="2560" citStr="Carroll and Fang, 2004" startWordPosition="403" endWordPosition="406">con resources. I first estimate a confidence value that a word can have each SCF, using corpus-based statistics. To capture latent co-occurrence tendency among SCFs in the target lexicon, I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon. Since each centroid value of the obtained clusters indicates whether the words in that cluster have each SCF, we can eliminate SCFs acquired in error and predict possible SCFs according to the centroids. I applied my method to SCFs acquired from a corpus of newsgroup posting about mobile phones (Carroll and Fang, 2004), using the XTAG English grammar (XTAG Research Group, 2001) and the LinGO English Resource Grammar (ERG) (Copestake, 2002). I then compared the resulting SCFs with SCFs obtained by naive frequency cut-off to observe the effects of clustering. 2 Background 2.1 SCF Acquisition for Lexicalized Grammars I start by acquiring SCFs for a lexicalized grammar from corpora by the method described in (Carroll and Fang, 2004). #S(EPATTERN :TARGET |yield| :SUBCAT (VSUBCAT NP) :CLASSES ((24 51 161) 5293) :RELIABILITY 0 :FREQSCORE 0.26861903 :FREQCNT 1 :TLTL (VV0) :SLTL ((|route |NN1)) :OLT1L ((|result |NN2</context>
<context position="13753" citStr="Carroll and Fang, 2004" startWordPosition="2358" endWordPosition="2361"> sj for a word wi E Cm when cmj is 1. I then eliminate implausible SCFs sj for wi from the resulting SCFs according to their confidence values confij. In the following, I compare centroid cut-off with frequency cut-off and confidence cut-off t, which use relative frequencies and confidence values calculated under the recognition threshold t, respectively. Note that these cut-offs use only corpus-based statistics to eliminate SCFs. 4 Experiments I applied my method to SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived by Google.com, which is the same data used in (Carroll and Fang, 2004). The number of acquired SCFs was 14,783 for 3,864 word stems, while the number of SCF types in the data was 97. I then translated the 163 SCF types into the SCF types of the XTAG English grammar (XTAG Research Group, 2001) and the LinGO ERG (Copestake, 2002)5 using translation mappings built by Ted Briscoe and Dan Flickinger from 23 of the SCF types into 13 (out of 57 possible) XTAG SCF types, and 129 into 54 (out of 216 possible) ERG SCF types. To evaluate my method, I split each lexicon of the two grammars into the training SCFs and the testing SCFs. The words in the testing SCFs were inclu</context>
</contexts>
<marker>Carroll, Fang, 2004</marker>
<rawString>J. Carroll and A. C. Fang. 2004. The automatic acquisition of verb subcategorizations and their impact on the performance of an HPSG parser. In Proc. thefirst ijc-NLP, pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Implementing typed feature structure grammars.</title>
<date>2002</date>
<note>CSLI publications.</note>
<contexts>
<context position="2683" citStr="Copestake, 2002" startWordPosition="423" endWordPosition="424"> co-occurrence tendency among SCFs in the target lexicon, I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon. Since each centroid value of the obtained clusters indicates whether the words in that cluster have each SCF, we can eliminate SCFs acquired in error and predict possible SCFs according to the centroids. I applied my method to SCFs acquired from a corpus of newsgroup posting about mobile phones (Carroll and Fang, 2004), using the XTAG English grammar (XTAG Research Group, 2001) and the LinGO English Resource Grammar (ERG) (Copestake, 2002). I then compared the resulting SCFs with SCFs obtained by naive frequency cut-off to observe the effects of clustering. 2 Background 2.1 SCF Acquisition for Lexicalized Grammars I start by acquiring SCFs for a lexicalized grammar from corpora by the method described in (Carroll and Fang, 2004). #S(EPATTERN :TARGET |yield| :SUBCAT (VSUBCAT NP) :CLASSES ((24 51 161) 5293) :RELIABILITY 0 :FREQSCORE 0.26861903 :FREQCNT 1 :TLTL (VV0) :SLTL ((|route |NN1)) :OLT1L ((|result |NN2)) :OLT2L NIL :OLT3L NIL :LRL 0)) Figure 1: An acquired SCF for a verb “yield” In their study, they first acquire fine-grai</context>
<context position="4129" citStr="Copestake, 2002" startWordPosition="668" endWordPosition="669"> explain here only its portion related to this study. The TARGET field is a word stem, the first number in the CLASSES field indicates an SCF type, and the FREQCNT field shows how often words derivable from the word stem appeared with the SCF type in the training corpus. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe, 1987) and COMLEX (Grishman et al., 1994) dictionaries. In this example, the SCF type 24 corresponds to an SCF of transitive verb. They then obtain SCFs for the target lexicalized grammar (the LinGO ERG (Copestake, 2002) in their study) using a handcrafted translation map from these 163 types to the SCF types in the target grammar. They reported that they could achieve a coverage improvement of 4.5% but that average parse time was doubled. This is because they did not use any filtering method for the acquired SCFs to suppress an increase of the lexical ambiguity. We definitely need some method to control the quality of the acquired SCFs. Their method is extendable to any lexicalized grammars, if we could have a translation map from these 163 types to the SCF types in the grammar. 2.2 Clustering of Verb SCF Di</context>
<context position="14012" citStr="Copestake, 2002" startWordPosition="2408" endWordPosition="2409">equencies and confidence values calculated under the recognition threshold t, respectively. Note that these cut-offs use only corpus-based statistics to eliminate SCFs. 4 Experiments I applied my method to SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived by Google.com, which is the same data used in (Carroll and Fang, 2004). The number of acquired SCFs was 14,783 for 3,864 word stems, while the number of SCF types in the data was 97. I then translated the 163 SCF types into the SCF types of the XTAG English grammar (XTAG Research Group, 2001) and the LinGO ERG (Copestake, 2002)5 using translation mappings built by Ted Briscoe and Dan Flickinger from 23 of the SCF types into 13 (out of 57 possible) XTAG SCF types, and 129 into 54 (out of 216 possible) ERG SCF types. To evaluate my method, I split each lexicon of the two grammars into the training SCFs and the testing SCFs. The words in the testing SCFs were included in the acquired SCFs. When I apply my method to the acquired SCFs using the training SCFs and evaluate the resulting SCFs with the 5I used the same version of the LinGO ERG as (Carroll and Fang, 2004) (1.4; April 2003) but the map is updated. cluster as a</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>A. Copestake. 2002. Implementing typed feature structure grammars. CSLI publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Forgy</author>
</authors>
<title>Cluster analysis of multivariate data: Efficiency vs. interpretability of classifications.</title>
<date>1965</date>
<journal>Biometrics,</journal>
<pages>21--768</pages>
<contexts>
<context position="5093" citStr="Forgy, 1965" startWordPosition="830" endWordPosition="831"> definitely need some method to control the quality of the acquired SCFs. Their method is extendable to any lexicalized grammars, if we could have a translation map from these 163 types to the SCF types in the grammar. 2.2 Clustering of Verb SCF Distributions There is some related work on clustering of verbs according to their SCF probability distributions (Schulte im Walde and Brew, 2002; Korhonen et al., 2003). Schulte im Walde and (true) probability distribution NP None NP_to-PP NP_PP PP subcategorization frame Figure 2: SCF probability distributions for apply Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monosemous verbs while Korhonen et al. (2003) applied other clustering methods to cluster polysemic SCF data. These studies aim at obtaining verb semantic classes, which are closely related to syntactic behavior of argument selection (Levin, 1993). Korhonen (2002) made use of SCF distributions for representative verbs in Levin’s verb classes to obtain accurate back-off estimates for all the verbs in the classes. In this study, I assume that there are classes whose element words have identical SCF types. I then obtain these classes by clustering acqui</context>
</contexts>
<marker>Forgy, 1965</marker>
<rawString>E. W. Forgy. 1965. Cluster analysis of multivariate data: Efficiency vs. interpretability of classifications. Biometrics, 21:768–780.</rawString>
</citation>
<citation valid="true">
<title>Bayesian Data Analysis.</title>
<date>1995</date>
<editor>A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, editors.</editor>
<publisher>Chapman and Hall.</publisher>
<marker>1995</marker>
<rawString>A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, editors. 1995. Bayesian Data Analysis. Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>Comlex syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proc. the 15th COLING,</booktitle>
<pages>268--272</pages>
<contexts>
<context position="3950" citStr="Grishman et al., 1994" startWordPosition="635" endWordPosition="638">ed by Briscoe and Carroll (1997) and Korhonen (2002). Figure 1 shows an example of one acquired SCF entry for a verb “yield.” Each SCF entry has several fields about the observed SCF. I explain here only its portion related to this study. The TARGET field is a word stem, the first number in the CLASSES field indicates an SCF type, and the FREQCNT field shows how often words derivable from the word stem appeared with the SCF type in the training corpus. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe, 1987) and COMLEX (Grishman et al., 1994) dictionaries. In this example, the SCF type 24 corresponds to an SCF of transitive verb. They then obtain SCFs for the target lexicalized grammar (the LinGO ERG (Copestake, 2002) in their study) using a handcrafted translation map from these 163 types to the SCF types in the target grammar. They reported that they could achieve a coverage improvement of 4.5% but that average parse time was doubled. This is because they did not use any filtering method for the acquired SCFs to suppress an increase of the lexical ambiguity. We definitely need some method to control the quality of the acquired S</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>R. Grishman, C. Macleod, and A. Meyers. 1994. Comlex syntax: Building a computational lexicon. In Proc. the 15th COLING, pages 268–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hamerly</author>
</authors>
<title>Learning structure and concepts in data through data clustering.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>San Diego.</location>
<contexts>
<context position="12401" citStr="Hamerly, 2003" startWordPosition="2125" endWordPosition="2126">re 3). After every assignment, I calculate a next centroid cm of each cluster Cm ((2) in Figure 3) by comparing a probability that the words in the cluster have an SCF sj and a probability that the words in the cluster do not have the SCF sj as follows: I next address the way to determine the number of clusters and initial centroids. In this study, I assume that the most of the possible set of SCFs for words are included in the lexicon of the target grammar,4 and make use of the existing sets of 4When the lexicon is less accurate, I can determine the number of clusters using other algorithms (Hamerly, 2003). SCFs for the words in the lexicon to determine the number of clusters and initial centroids. I first extract SCF confidence-value vectors from the lexicon of the grammar. By eliminating duplications from them and regarding O= 0 in Equation 6, I obtain initial centroids cm. I then initialize the number of clusters k to the number of cm. I finally update the acquired SCFs using the obtained clusters and the confidence values of SCFs in this order. I call the following procedure centroid cut-off t when the confidence values are estimated under the recognition threshold t. Since the value cmj of</context>
</contexts>
<marker>Hamerly, 2003</marker>
<rawString>G. Hamerly. 2003. Learning structure and concepts in data through data clustering. Ph.D. thesis, University of California, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
<author>Z Marx</author>
</authors>
<title>Clustering polysemic subcategorization frame distributions semantically.</title>
<date>2003</date>
<booktitle>In Proc. the 41stACL,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="4896" citStr="Korhonen et al., 2003" startWordPosition="799" endWordPosition="803">chieve a coverage improvement of 4.5% but that average parse time was doubled. This is because they did not use any filtering method for the acquired SCFs to suppress an increase of the lexical ambiguity. We definitely need some method to control the quality of the acquired SCFs. Their method is extendable to any lexicalized grammars, if we could have a translation map from these 163 types to the SCF types in the grammar. 2.2 Clustering of Verb SCF Distributions There is some related work on clustering of verbs according to their SCF probability distributions (Schulte im Walde and Brew, 2002; Korhonen et al., 2003). Schulte im Walde and (true) probability distribution NP None NP_to-PP NP_PP PP subcategorization frame Figure 2: SCF probability distributions for apply Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monosemous verbs while Korhonen et al. (2003) applied other clustering methods to cluster polysemic SCF data. These studies aim at obtaining verb semantic classes, which are closely related to syntactic behavior of argument selection (Levin, 1993). Korhonen (2002) made use of SCF distributions for representative verbs in Levin’s verb classes to obtain accur</context>
</contexts>
<marker>Korhonen, Krymolowski, Marx, 2003</marker>
<rawString>A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clustering polysemic subcategorization frame distributions semantically. In Proc. the 41stACL, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="936" citStr="Korhonen, 2002" startWordPosition="132" endWordPosition="133">ra to augment existing lexicon resources. I estimate a confidence value of each SCF using corpus-based statistics, and then perform clustering of SCF confidencevalue vectors for words to capture cooccurrence tendency among SCFs in the lexicon. I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars. The resulting SCFs achieve higher precision and recall compared to SCFs obtained by naive frequency cut-off. 1 Introduction Recently, a variety of methods have been proposed for acquisition of subcategorization frames (SCFs) from corpora (surveyed in (Korhonen, 2002)). One interesting possibility is to use these techniques to improve the coverage of existing largescale lexicon resources such as lexicons of lexicalized grammars. However, there has been little work on evaluating the impact of acquired SCFs with the exception of (Carroll and Fang, 2004). The problem when we integrate acquired SCFs into existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the poor precision by being less strict in filtering out less likely SCFs,</context>
<context position="3380" citStr="Korhonen (2002)" startWordPosition="534" endWordPosition="535"> to observe the effects of clustering. 2 Background 2.1 SCF Acquisition for Lexicalized Grammars I start by acquiring SCFs for a lexicalized grammar from corpora by the method described in (Carroll and Fang, 2004). #S(EPATTERN :TARGET |yield| :SUBCAT (VSUBCAT NP) :CLASSES ((24 51 161) 5293) :RELIABILITY 0 :FREQSCORE 0.26861903 :FREQCNT 1 :TLTL (VV0) :SLTL ((|route |NN1)) :OLT1L ((|result |NN2)) :OLT2L NIL :OLT3L NIL :LRL 0)) Figure 1: An acquired SCF for a verb “yield” In their study, they first acquire fine-grained SCFs using the unsupervised method proposed by Briscoe and Carroll (1997) and Korhonen (2002). Figure 1 shows an example of one acquired SCF entry for a verb “yield.” Each SCF entry has several fields about the observed SCF. I explain here only its portion related to this study. The TARGET field is a word stem, the first number in the CLASSES field indicates an SCF type, and the FREQCNT field shows how often words derivable from the word stem appeared with the SCF type in the training corpus. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe, 1987) and COMLEX (Grishman et al., 1994) dictionaries. In this example</context>
<context position="5401" citStr="Korhonen (2002)" startWordPosition="877" endWordPosition="878">erbs according to their SCF probability distributions (Schulte im Walde and Brew, 2002; Korhonen et al., 2003). Schulte im Walde and (true) probability distribution NP None NP_to-PP NP_PP PP subcategorization frame Figure 2: SCF probability distributions for apply Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monosemous verbs while Korhonen et al. (2003) applied other clustering methods to cluster polysemic SCF data. These studies aim at obtaining verb semantic classes, which are closely related to syntactic behavior of argument selection (Levin, 1993). Korhonen (2002) made use of SCF distributions for representative verbs in Levin’s verb classes to obtain accurate back-off estimates for all the verbs in the classes. In this study, I assume that there are classes whose element words have identical SCF types. I then obtain these classes by clustering acquired SCFs, using information available in the target lexicon, and directly use the obtained classes to eliminate implausible SCFs. 3 Method 3.1 Estimation of Confidence Values for SCFs I first create an SCF confidence-value vector vi for each word wi, an object for clustering. Each element vij in vi represen</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>A. Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="5384" citStr="Levin, 1993" startWordPosition="875" endWordPosition="876">lustering of verbs according to their SCF probability distributions (Schulte im Walde and Brew, 2002; Korhonen et al., 2003). Schulte im Walde and (true) probability distribution NP None NP_to-PP NP_PP PP subcategorization frame Figure 2: SCF probability distributions for apply Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monosemous verbs while Korhonen et al. (2003) applied other clustering methods to cluster polysemic SCF data. These studies aim at obtaining verb semantic classes, which are closely related to syntactic behavior of argument selection (Levin, 1993). Korhonen (2002) made use of SCF distributions for representative verbs in Levin’s verb classes to obtain accurate back-off estimates for all the verbs in the classes. In this study, I assume that there are classes whose element words have identical SCF types. I then obtain these classes by clustering acquired SCFs, using information available in the target lexicon, and directly use the obtained classes to eliminate implausible SCFs. 3 Method 3.1 Estimation of Confidence Values for SCFs I first create an SCF confidence-value vector vi for each word wi, an object for clustering. Each element v</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>F Xia</author>
<author>A K Joshi</author>
</authors>
<title>Some experiments on indicators of parsing complexity for lexicalized grammars.</title>
<date>2000</date>
<booktitle>In Proc. the 18th COLING workshop,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="1684" citStr="Sarkar et al., 2000" startWordPosition="255" endWordPosition="258">as lexicons of lexicalized grammars. However, there has been little work on evaluating the impact of acquired SCFs with the exception of (Carroll and Fang, 2004). The problem when we integrate acquired SCFs into existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the poor precision by being less strict in filtering out less likely SCFs, then we will end up with a larger number of noisy lexical entries, which is problematic for parsing with lexicalized grammars (Sarkar et al., 2000). We thus need some method of selecting the most reliable set of SCFs from the system output as demonstrated in (Korhonen, 2002). In this paper, I present a method of improving the accuracy of SCFs acquired from corpora in order to augment existing lexicon resources. I first estimate a confidence value that a word can have each SCF, using corpus-based statistics. To capture latent co-occurrence tendency among SCFs in the target lexicon, I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon. Since each centroid value of the obtained cl</context>
</contexts>
<marker>Sarkar, Xia, Joshi, 2000</marker>
<rawString>A. Sarkar, F. Xia, and A. K. Joshi. 2000. Some experiments on indicators of parsing complexity for lexicalized grammars. In Proc. the 18th COLING workshop, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
<author>C Brew</author>
</authors>
<title>Inducing German semantic verb classes from purely syntactic subcategorisation information.</title>
<date>2002</date>
<booktitle>In Proc. the 41stACL,</booktitle>
<pages>223--230</pages>
<contexts>
<context position="4872" citStr="Walde and Brew, 2002" startWordPosition="795" endWordPosition="798">rted that they could achieve a coverage improvement of 4.5% but that average parse time was doubled. This is because they did not use any filtering method for the acquired SCFs to suppress an increase of the lexical ambiguity. We definitely need some method to control the quality of the acquired SCFs. Their method is extendable to any lexicalized grammars, if we could have a translation map from these 163 types to the SCF types in the grammar. 2.2 Clustering of Verb SCF Distributions There is some related work on clustering of verbs according to their SCF probability distributions (Schulte im Walde and Brew, 2002; Korhonen et al., 2003). Schulte im Walde and (true) probability distribution NP None NP_to-PP NP_PP PP subcategorization frame Figure 2: SCF probability distributions for apply Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monosemous verbs while Korhonen et al. (2003) applied other clustering methods to cluster polysemic SCF data. These studies aim at obtaining verb semantic classes, which are closely related to syntactic behavior of argument selection (Levin, 1993). Korhonen (2002) made use of SCF distributions for representative verbs in Levin’s verb</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>S. Schulte im Walde and C. Brew. 2002. Inducing German semantic verb classes from purely syntactic subcategorisation information. In Proc. the 41stACL, pages 223–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>T Chikayama</author>
</authors>
<title>Estimating reliability of contextual evidences in decision-list classifiers under Bayesian learning.</title>
<date>2001</date>
<booktitle>In Proc. the sixth NLPRS,</booktitle>
<pages>701--707</pages>
<contexts>
<context position="8982" citStr="Tsuruoka and Chikayama, 2001" startWordPosition="1507" endWordPosition="1510"> distribution, I need to define the apriori distribution P(θij). The question is which probability distribution of θij can appropriately reflects prior knowledge. In other words, it should encode knowledge we use to estimate SCFs for unknown words. I simply determine it from distributions of observed probability values of sj for words seen in corpora2 by using 1The values of FREQCNT is used to obtain n and x. 2I estimated a priori distribution separately for each type of SCF from words that appeared more than 50 times in the training corpus in the following experiments. a method described in (Tsuruoka and Chikayama, 2001). In their study, they assume a priori distribution as the beta distribution defined as: θα−1 i j (1 − θij)β−1 , (3) where B(α,β) = f01 θα−1 i j (1 − θij)β−1dθij. The value of α and β is determined by moment estimation.3 By substituting Equations 2 and 3 into Equation 1, I finally obtain the a posteriori distribution p(θij|D) as: p(θij|α,β,D)=c · θx+α−1 i j (1−θij)n−x+β−1,(4) where c = (n )/(B(α,β)f0 1 P(θij)P(D|θij)dθij). x When I regard the recognition threshold as t, I can calculate a confidence value confij that a word wi can have sj by integrating the a posteriori distribution p(θij|D) fr</context>
</contexts>
<marker>Tsuruoka, Chikayama, 2001</marker>
<rawString>Y. Tsuruoka and T. Chikayama. 2001. Estimating reliability of contextual evidences in decision-list classifiers under Bayesian learning. In Proc. the sixth NLPRS, pages 701– 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XTAG Research Group</author>
</authors>
<title>A Lexicalized Tree Adjoining Grammar for English.</title>
<date>2001</date>
<tech>Technical Report IRCS-01-03,</tech>
<institution>IRCS, University of Pennsylvania.</institution>
<contexts>
<context position="2620" citStr="Group, 2001" startWordPosition="414" endWordPosition="415"> each SCF, using corpus-based statistics. To capture latent co-occurrence tendency among SCFs in the target lexicon, I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon. Since each centroid value of the obtained clusters indicates whether the words in that cluster have each SCF, we can eliminate SCFs acquired in error and predict possible SCFs according to the centroids. I applied my method to SCFs acquired from a corpus of newsgroup posting about mobile phones (Carroll and Fang, 2004), using the XTAG English grammar (XTAG Research Group, 2001) and the LinGO English Resource Grammar (ERG) (Copestake, 2002). I then compared the resulting SCFs with SCFs obtained by naive frequency cut-off to observe the effects of clustering. 2 Background 2.1 SCF Acquisition for Lexicalized Grammars I start by acquiring SCFs for a lexicalized grammar from corpora by the method described in (Carroll and Fang, 2004). #S(EPATTERN :TARGET |yield| :SUBCAT (VSUBCAT NP) :CLASSES ((24 51 161) 5293) :RELIABILITY 0 :FREQSCORE 0.26861903 :FREQCNT 1 :TLTL (VV0) :SLTL ((|route |NN1)) :OLT1L ((|result |NN2)) :OLT2L NIL :OLT3L NIL :LRL 0)) Figure 1: An acquired SCF </context>
<context position="13976" citStr="Group, 2001" startWordPosition="2402" endWordPosition="2403">cut-off t, which use relative frequencies and confidence values calculated under the recognition threshold t, respectively. Note that these cut-offs use only corpus-based statistics to eliminate SCFs. 4 Experiments I applied my method to SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived by Google.com, which is the same data used in (Carroll and Fang, 2004). The number of acquired SCFs was 14,783 for 3,864 word stems, while the number of SCF types in the data was 97. I then translated the 163 SCF types into the SCF types of the XTAG English grammar (XTAG Research Group, 2001) and the LinGO ERG (Copestake, 2002)5 using translation mappings built by Ted Briscoe and Dan Flickinger from 23 of the SCF types into 13 (out of 57 possible) XTAG SCF types, and 129 into 54 (out of 216 possible) ERG SCF types. To evaluate my method, I split each lexicon of the two grammars into the training SCFs and the testing SCFs. The words in the testing SCFs were included in the acquired SCFs. When I apply my method to the acquired SCFs using the training SCFs and evaluate the resulting SCFs with the 5I used the same version of the LinGO ERG as (Carroll and Fang, 2004) (1.4; April 2003) </context>
</contexts>
<marker>Group, 2001</marker>
<rawString>XTAG Research Group. 2001. A Lexicalized Tree Adjoining Grammar for English. Technical Report IRCS-01-03, IRCS, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>