<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000821">
<title confidence="0.900264">
Sparsity in Dependency Grammar Induction
</title>
<note confidence="0.758884">
João Graça
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
</note>
<author confidence="0.935226">
Jennifer Gillenwater and Kuzman Ganchev
</author>
<affiliation confidence="0.984399">
University of Pennsylvania
</affiliation>
<address confidence="0.642466">
Philadelphia, PA, USA
</address>
<email confidence="0.992086">
{jengi,kuzman}@cis.upenn.edu
</email>
<author confidence="0.989378">
Fernando Pereira
</author>
<affiliation confidence="0.762348">
Google Inc.
Mountain View, CA, USA
</affiliation>
<email confidence="0.979432">
pereira@google.com
</email>
<author confidence="0.943247">
Ben Taskar
</author>
<affiliation confidence="0.989797">
University of Pennsylvania
</affiliation>
<address confidence="0.796664">
Philadelphia, PA, USA
</address>
<email confidence="0.996701">
taskar@cis.upenn.edu
</email>
<sectionHeader confidence="0.998583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990395">
A strong inductive bias is essential in un-
supervised grammar induction. We ex-
plore a particular sparsity bias in de-
pendency grammars that encourages a
small number of unique dependency
types. Specifically, we investigate
sparsity-inducing penalties on the poste-
rior distributions of parent-child POS tag
pairs in the posterior regularization (PR)
framework of Graça et al. (2007). In ex-
periments with 12 languages, we achieve
substantial gains over the standard expec-
tation maximization (EM) baseline, with
average improvement in attachment ac-
curacy of 6.3%. Further, our method
outperforms models based on a standard
Bayesian sparsity-inducing prior by an av-
erage of 4.9%. On English in particular,
we show that our approach improves on
several other state-of-the-art techniques.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999885978723404">
We investigate an unsupervised learning method
for dependency parsing models that imposes spar-
sity biases on the dependency types. We assume
a corpus annotated with POS tags, where the task
is to induce a dependency model from the tags for
corpus sentences. In this setting, the type of a de-
pendency is defined as a pair: tag of the dependent
(also known as the child), and tag of the head (also
known as the parent). Given that POS tags are de-
signed to convey information about grammatical
relations, it is reasonable to assume that only some
of the possible dependency types will be realized
for a given language. For instance, in English it
is ungrammatical for nouns to dominate verbs, ad-
jectives to dominate adverbs, and determiners to
dominate almost any part of speech. Thus, the re-
alized dependency types should be a sparse subset
of all possible types.
Previous work in unsupervised grammar induc-
tion has tried to achieve sparsity through priors.
Liang et al. (2007), Finkel et al. (2007) and John-
son et al. (2007) proposed hierarchical Dirichlet
process priors. Cohen et al. (2008) experimented
with a discounting Dirichlet prior, which encour-
ages a standard dependency parsing model (see
Section 2) to limit the number of dependent types
for each head type.
Our experiments show a more effective sparsity
pattern is one that limits the total number of unique
head-dependent tag pairs. This kind of sparsity
bias avoids inducing competition between depen-
dent types for each head type. We can achieve the
desired bias with a constraint on model posteri-
ors during learning, using the posterior regulariza-
tion (PR) framework (Graça et al., 2007). Specifi-
cally, to implement PR we augment the maximum
marginal likelihood objective of the dependency
model with a term that penalizes head-dependent
tag distributions that are too permissive.
Although not focused on sparsity, several other
studies use soft parameter sharing to couple dif-
ferent types of dependencies. To this end, Cohen
et al. (2008) and Cohen and Smith (2009) inves-
tigated logistic normal priors, and Headden III et
al. (2009) used a backoff scheme. We compare to
their results in Section 5.
The remainder of this paper is organized as fol-
</bodyText>
<page confidence="0.983009">
194
</page>
<note confidence="0.508236">
Proceedings of the ACL 2010 Conference Short Papers, pages 194–199,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999250666666667">
lows. Section 2 and 3 review the models and sev-
eral previous approaches for learning them. Sec-
tion 4 describes learning with PR. Section 5 de-
scribes experiments across 12 languages and Sec-
tion 6 analyzes the results. For additional details
on this work see Gillenwater et al. (2010).
</bodyText>
<sectionHeader confidence="0.99058" genericHeader="method">
2 Parsing Model
</sectionHeader>
<bodyText confidence="0.999988266666667">
The models we use are based on the generative de-
pendency model with valence (DMV) (Klein and
Manning, 2004). For a sentence with tags x, the
root POS r(x) is generated first. Then the model
decides whether to generate a right dependent con-
ditioned on the POS of the root and whether other
right dependents have already been generated for
this head. Upon deciding to generate a right de-
pendent, the POS of the dependent is selected by
conditioning on the head POS and the direction-
ality. After stopping on the right, the root gener-
ates left dependents using the mirror reversal of
this process. Once the root has generated all its
dependents, the dependents generate their own de-
pendents in the same manner.
</bodyText>
<subsectionHeader confidence="0.998533">
2.1 Model Extensions
</subsectionHeader>
<bodyText confidence="0.997841">
For better comparison with previous work we
implemented three model extensions, borrowed
from Headden III et al. (2009). The first exten-
sion alters the stopping probability by condition-
ing it not only on whether there are any depen-
dents in a particular direction already, but also on
how many such dependents there are. When we
talk about models with maximum stop valency V3
= 5, this means it distinguishes 5 different cases:
0, 1, ... , 5 −2, and &gt; 5 −1 dependents in a given
direction. The basic DMV has V3 = 2.
The second model extension we implement is
analogous to the first, but applies to dependent tag
probabilities instead of stop probabilities. Again,
we expand the conditioning such that the model
considers how many other dependents were al-
ready generated in the same direction. When we
talk about a model with maximum child valency
V, = C, this means we distinguish C different
cases. The basic DMV has V, = 1. Since this
extension to the dependent probabilities dramati-
cally increases model complexity, the third model
extension we implement is to add a backoff for the
dependent probabilities that does not condition on
the identity of the parent POS (see Equation 2).
More formally, under the extended DMV the
probability of a sentence with POS tags x and de-
pendency tree y is given by:
</bodyText>
<equation confidence="0.999115833333333">
pθ(x, y) = proot(r(x))x
ri pstop(false  |yp, yd, yv.)pchild(yc  |yp, yd, yv.)x
yEy
ri pstop(true  |x, left, xv�) pstop(true  |x, right, xv,)
xEx
(1)
</equation>
<bodyText confidence="0.9989816">
where y is the dependency of y, on head yp in di-
rection yd, and y„c, y„s, x„r, and x„l indicate va-
lence. For the third model extension, the backoff
to a probability not dependent on parent POS can
be formally expressed as:
</bodyText>
<equation confidence="0.732508">
λpchild(yc  |yp, yd, yv.) + (1 − λ)pchild(yc  |yd, yvJ (2)
</equation>
<bodyText confidence="0.996897">
for A E [0, 1]. We fix A = 1/3, which is a crude
approximation to the value learned by Headden III
et al. (2009).
</bodyText>
<sectionHeader confidence="0.978453" genericHeader="method">
3 Previous Learning Approaches
</sectionHeader>
<bodyText confidence="0.99956975">
In our experiments, we compare PR learning
to standard expectation maximization (EM) and
to Bayesian learning with a sparsity-inducing
prior. The EM algorithm optimizes marginal like-
lihood L(0) = log EY po(X, Y), where X =
{x1, ... , xn} denotes the entire unlabeled corpus
and Y = {y1, ... , yn} denotes a set of corre-
sponding parses for each sentence. Neal and Hin-
ton (1998) view EM as block coordinate ascent on
a function that lower-bounds L(0). Starting from
an initial parameter estimate 00, the algorithm it-
erates two steps:
</bodyText>
<equation confidence="0.992839">
E : qt+1 = arg min KL(q(Y) II pθt(Y  |X)) (3)
q
M : θt+1 = arg max Eqt+1 [log pθ(X, Y)] (4)
θ
</equation>
<bodyText confidence="0.999951214285714">
Note that the E-step just sets qt+1(Y) =
pot(Y|X), since it is an unconstrained minimiza-
tion of a KL-divergence. The PR method we
present modifies the E-step by adding constraints.
Besides EM, we also compare to learning with
several Bayesian priors that have been applied to
the DMV. One such prior is the Dirichlet, whose
hyperparameter we will denote by α. For α &lt; 0.5,
this prior encourages parameter sparsity. Cohen
et al. (2008) use this method with α = 0.25 for
training the DMV and achieve improvements over
basic EM. In this paper we will refer to our own
implementation of the Dirichlet prior as the “dis-
counting Dirichlet” (DD) method. In addition to
</bodyText>
<page confidence="0.994913">
195
</page>
<bodyText confidence="0.999951666666667">
the Dirichlet, other types of priors have been ap-
plied, in particular logistic normal priors (LN) and
shared logistic normal priors (SLN) (Cohen et al.,
2008; Cohen and Smith, 2009). LN and SLN aim
to tie parameters together. Essentially, this has a
similar goal to sparsity-inducing methods in that it
posits a more concise explanation for the grammar
of a language. Headden III et al. (2009) also im-
plement a sort of parameter tying for the E-DMV
through a learning a backoff distribution on child
probabilities. We compare against results from all
these methods.
</bodyText>
<sectionHeader confidence="0.679911" genericHeader="method">
4 Learning with Sparse Posteriors
</sectionHeader>
<bodyText confidence="0.999997736842105">
We would like to penalize models that predict a
large number of distinct dependency types. To en-
force this penalty, we use the posterior regular-
ization (PR) framework (Graça et al., 2007). PR
is closely related to generalized expectation con-
straints (Mann and McCallum, 2007; Mann and
McCallum, 2008; Bellare et al., 2009), and is also
indirectly related to a Bayesian view of learning
with constraints on posteriors (Liang et al., 2009).
The PR framework uses constraints on posterior
expectations to guide parameter estimation. Here,
PR allows a natural and tractable representation of
sparsity constraints based on edge type counts that
cannot easily be encoded in model parameters. We
use a version of PR where the desired bias is a
penalty on the log likelihood (see Ganchev et al.
(2010) for more details). For a distribution pg, we
define a penalty as the (generic) β-norm of expec-
tations of some features φ:
</bodyText>
<equation confidence="0.688745">
||Ep,, [φ(X, Y)]||a (5)
</equation>
<bodyText confidence="0.995682714285714">
For computational tractability, rather than penaliz-
ing the model’s posteriors directly, we use an aux-
iliary distribution q, and penalize the marginal log-
likelihood of a model by the KL-divergence of pg
from q, plus the penalty term with respect to q.
For a fixed set of model parameters θ the full PR
penalty term is:
</bodyText>
<equation confidence="0.9272105">
min
9 KL(q(Y) II pe(Y|X)) + σ ||E9[φ(X, Y)]||a (6)
</equation>
<bodyText confidence="0.9997494">
where σ is the strength of the regularization. PR
seeks to maximize L(θ) minus this penalty term.
The resulting objective can be optimized by a vari-
ant of the EM (Dempster et al., 1977) algorithm
used to optimize L(θ).
</bodyText>
<subsectionHeader confidence="0.739466">
4.1 `1/`� Regularization
</subsectionHeader>
<bodyText confidence="0.999990142857143">
We now define precisely how to count dependency
types. For each child tag c, let i range over an enu-
meration of all occurrences of c in the corpus, and
let p be another tag. Let the indicator φ pi(X, Y)
have value 1 if p is the parent tag of the ith occur-
rence of c, and value 0 otherwise. The number of
unique dependency types is then:
</bodyText>
<equation confidence="0.977227666666667">
E max φcpi(X, Y) (7)
i
cp
</equation>
<bodyText confidence="0.999890227272727">
Note there is an asymmetry in this count: occur-
rences of child type c are enumerated with i, but
all occurrences of parent type p are or-ed in φ pi.
That is, φ pi = 1 if any occurrence of p is the par-
ent of the ith occurrence of c. We will refer to PR
training with this constraint as PR-AS. Instead of
counting pairs of a child token and a parent type,
we can alternatively count pairs of a child token
and a parent token by letting p range over all to-
kens rather than types. Then each potential depen-
dency corresponds to a different indicator φ pig,
and the penalty is symmetric with respect to par-
ents and children. We will refer to PR training
with this constraint as PR-S. Both approaches per-
form very well, so we report results for both.
Equation 7 can be viewed as a mixed-norm
penalty on the features φ pi or φ pig: the sum cor-
responds to an `1 norm and the max to an `�
norm. Thus, the quantity we want to minimize
fits precisely into the PR penalty framework. For-
mally, to optimize the PR objective, we complete
the following E-step:
</bodyText>
<equation confidence="0.978781333333333">
arg min KL(q(Y)||pe(Y|X)) + σ �
9 cp
which can equivalently be written as:
KL(q(Y) II pe(Y|X)) + σ � ξcp
cp (9)
s. �. ξcp &lt; E9[φ(X, Y)]
</equation>
<bodyText confidence="0.999978">
where ξ p corresponds to the maximum expecta-
tion of φ over all instances of c and p. Note that
the projection problem can be solved efficiently in
the dual (Ganchev et al., 2010).
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999844">
We evaluate on 12 languages. Following the ex-
ample of Smith and Eisner (2006), we strip punc-
tuation from the sentences and keep only sen-
tences of length &lt; 10. For simplicity, for all mod-
els we use the “harmonic” initializer from Klein
</bodyText>
<equation confidence="0.778295">
E9[φ(X, Y)],
(8)
max
i
min
9(Y),Gp
</equation>
<page confidence="0.980063">
196
</page>
<table confidence="0.999566333333333">
Model EM PR Type v
DMV 45.8 62.1 PR-S 140
2-1 45.1 62.7 PR-S 100
2-2 54.4 62.9 PR-S 80
3-3 55.3 64.3 PR-S 140
4-4 55.1 64.4 PR-AS 140
</table>
<tableCaption confidence="0.999503">
Table 1: Attachment accuracy results. Column 1: V,
</tableCaption>
<bodyText confidence="0.791183">
V. used for the E-DMV models. Column 3: Best PR re-
sult for each model, which is chosen by applying each of
the two types of constraints (PR-S and PR-AS) and trying
v E {80, 100, 120, 140, 160, 180}. Columns 4 &amp; 5: Con-
straint type and v that produced the values in column 3.
</bodyText>
<table confidence="0.998114142857143">
Learning Method Accuracy
&lt; 10 &lt; 20 all
PR-S (v = 140) 62.1 53.8 49.1
LN families 59.3 45.1 39.0
SLN TieV &amp; N 61.3 47.4 41.4
PR-AS (v = 140) 64.4 55.2 50.5
DD (α = 1, A learned) 65.0 (f5.7)
</table>
<tableCaption confidence="0.964963">
Table 2: Comparison with previous published results. Rows
2 and 3 are taken from Cohen et al. (2008) and Cohen and
Smith (2009), and row 5 from Headden III et al. (2009).
</tableCaption>
<bodyText confidence="0.9997545">
and Manning (2004), which we refer to as K&amp;M.
We always train for 100 iterations and evaluate
on the test set using Viterbi parses. Before eval-
uating, we smooth the resulting models by adding
e−10 to each learned parameter, merely to remove
the chance of zero probabilities for unseen events.
(We did not tune this as it should make very little
difference for final parses.) We score models by
their attachment accuracy — the fraction of words
assigned the correct parent.
</bodyText>
<subsectionHeader confidence="0.872858">
5.1 Results on English
</subsectionHeader>
<bodyText confidence="0.999924409090909">
We start by comparing English performance for
EM, PR, and DD. To find a for DD we searched
over five values: {0.01, 0.1, 0.25,1}. We found
0.25 to be the best setting for the DMV, the same
as found by Cohen et al. (2008). DD achieves ac-
curacy 46.4% with this a. For the E-DMV we
tested four model complexities with valencies V,-
V3 of 2-1, 2-2, 3-3, and 4-4. DD’s best accuracy
was 53.6% with the 4-4 model at a = 0.1. A
comparison between EM and PR is shown in Ta-
ble 1. PR-S generally performs better than the PR-
AS for English. Comparing PR-S to EM, we also
found PR-S is always better, independent of the
particular a, with improvements ranging from 2%
to 17%. Note that in this work we do not perform
the PR projection at test time; we found it detri-
mental, probably due to a need to set the (corpus-
size-dependent) a differently for the test set. We
also note that development likelihood and the best
setting for a are not well-correlated, which un-
fortunately makes it hard to pick these parameters
without some supervision.
</bodyText>
<subsectionHeader confidence="0.999827">
5.2 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.999784222222222">
In this section we compare to previously published
unsupervised dependency parsing results for En-
glish. It might be argued that the comparison is
unfair since we do supervised selection of model
complexity and regularization strength. However,
we feel the comparison is not so unfair as we per-
form only a very limited search of the model-a
space. Specifically, the only values of a we search
over are {80,100,120,140,160,180}.
First, we consider the top three entries in Ta-
ble 2, which are for the basic DMV. The first en-
try was generated using our implementation of
PR-S. The second two entries are logistic nor-
mal and shared logistic normal parameter tying re-
sults (Cohen et al., 2008; Cohen and Smith, 2009).
The PR-S result is the clear winner, especially as
length of test sentences increases. For the bot-
tom two entries in the table, which are for the E-
DMV, the last entry is best, corresponding to us-
ing a DD prior with a = 1 (non-sparsifying), but
with a special “random pools” initialization and a
learned weight A for the child backoff probabil-
ity. The result for PR-AS is well within the vari-
ance range of this last entry, and thus we conjec-
ture that combining PR-AS with random pools ini-
tialization and learned A would likely produce the
best-performing model of all.
</bodyText>
<subsectionHeader confidence="0.96467">
5.3 Results on Other Languages
</subsectionHeader>
<bodyText confidence="0.9993868">
Here we describe experiments on 11 additional
languages. For each we set a and model complex-
ity (DMV versus one of the four E-DMV exper-
imented with previously) based on the best con-
figuration found for English. This likely will not
result in the ideal parameters for all languages, but
provides a realistic test setting: a user has avail-
able a labeled corpus in one language, and would
like to induce grammars for many other languages.
Table 3 shows the performance for all models and
training procedures. We see that the sparsifying
methods tend to improve over EM most of the
time. For the basic DMV, average improvements
are 1.6% for DD, 6.0% for PR-S, and 7.5% for
PR-AS. PR-AS beats PR-S in 8 out of 12 cases,
</bodyText>
<page confidence="0.992919">
197
</page>
<table confidence="0.997265636363636">
Bg Cz De Dk En Es Jp Nl Pt Se Si Tr
DMV Model
EM 37.8 29.6 35.7 47.2 45.8 40.3 52.8 37.1 35.7 39.4 42.3 46.8
DD 0.25 39.3 30.0 38.6 43.1 46.4 47.5 57.8 35.1 38.7 40.2 48.8 43.8
PR-S 140 53.7 31.5 39.6 44.0 62.1 61.1 58.8 31.0 47.0 42.2 39.9 51.4
PR-AS 140 54.0 32.0 39.6 42.4 61.9 62.4 60.2 37.9 47.8 38.7 50.3 53.4
Extended Model
EM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2
DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9
PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3
PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9
</table>
<tableCaption confidence="0.900958428571429">
Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters
(α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn
Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al.,
2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish
[Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese
[Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et
al., 2003).
</tableCaption>
<figureCaption confidence="0.9965375">
Figure 1: Posterior edge probabilities for an example sen-
tence from the Spanish test corpus. At the top are the gold
dependencies, the middle are EM posteriors, and bottom are
PR posteriors. Green indicates correct dependencies and red
indicates incorrect dependencies. The numbers on the edges
are the values of the posterior probabilities.
</figureCaption>
<bodyText confidence="0.999873714285714">
though the average increase is only 1.5%. PR-S
is also better than DD for 10 out of 12 languages.
If we instead consider these methods for the E-
DMV, DD performs worse, just 1.4% better than
the E-DMV EM, while both PR-S and PR-AS con-
tinue to show substantial average improvements
over EM, 6.5% and 6.3%, respectively.
</bodyText>
<sectionHeader confidence="0.975975" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999981533333333">
One common EM error that PR fixes in many lan-
guages is the directionality of the noun-determiner
relation. Figure 1 shows an example of a Span-
ish sentence where PR significantly outperforms
EM because of this. Sentences such as “Lleva
tiempo entenderlos” which has tags “main-verb
common-noun main-verb” (no determiner tag)
provide an explanation for PR’s improvement—
when PR sees that sometimes nouns can appear
without determiners but that the opposite situation
does not occur, it shifts the model parameters to
make nouns the parent of determiners instead of
the reverse. Then it does not have to pay the cost
of assigning a parent with a new tag to cover each
noun that doesn’t come with a determiner.
</bodyText>
<sectionHeader confidence="0.993485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9988936">
In this paper we presented a new method for unsu-
pervised learning of dependency parsers. In con-
trast to previous approaches that constrain model
parameters, we constrain model posteriors. Our
approach consistently outperforms the standard
EM algorithm and a discounting Dirichlet prior.
We have several ideas for further improving our
constraints, such as: taking into account the direc-
tionality of the edges, using different regulariza-
tion strengths for the root probabilities than for the
child probabilities, and working directly on word
types rather than on POS tags. In the future, we
would also like to try applying similar constraints
to the more complex task of joint induction of POS
tags and dependency parses.
</bodyText>
<sectionHeader confidence="0.998346" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.919185375">
J. Gillenwater was supported by NSF-IGERT
0504487. K. Ganchev was supported by
ARO MURI SUBTLE W911NF-07-1-0216.
J. Graça was supported by FCT fellowship
SFRH/BD/27528/2006 and by FCT project CMU-
PT/HuMach/0039/2008. B. Taskar was partly
supported by DARPA CSSG and ONR Young
Investigator Award N000141010746.
</bodyText>
<figure confidence="0.996831513513514">
Una
d
papelera
nc
objeto
nc
es
vs
un
d
civilizado
aq
Una
d
es
vs
objeto
nc
civilizado
aq
papelera
nc
un
d
0.51
0.43
1.00
1.00
1.00 1.00
0.49
0.35
0.83 0.75 0.92
1.00 0.99
0.48
Una papelera es un objeto civilizado
d nc vs d nc aq
0.57
</figure>
<page confidence="0.988631">
198
</page>
<sectionHeader confidence="0.995511" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999869082474227">
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta(c)tica: a treebank for Portuguese. In
Proc. LREC.
K. Bellare, G. Druck, and A. McCallum. 2009. Al-
ternating projections for learning with expectation
constraints. In Proc. UAI.
A. Bohomovà, J. Hajic, E. Hajicova, and B. Hladka.
2001. The prague dependency treebank: Three-level
annotation scenario. In Anne Abeillé, editor, Tree-
banks: Building and Using Syntactically Annotated
Corpora.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proc.
Workshop on Treebanks and Linguistic Theories.
M. Civit and M.A. Martí. 2004. Building cast3lb: A
Spanish Treebank. Research on Language &amp; Com-
putation.
S.B. Cohen and N.A. Smith. 2009. The shared logistic
normal distribution for grammar induction. In Proc.
NAACL.
S.B. Cohen, K. Gimpel, and N.A. Smith. 2008. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proc. NIPS.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1–38.
S. Džeroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. Žabokrtsky, and A. Žele. 2006. Towards a
Slovene dependency treebank. In Proc. LREC.
J. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Proc. ACL.
K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
J. Gillenwater, K. Ganchev, J. Graça, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, MS-CIS-10-
19, University of Pennsylvania.
J. Graça, K. Ganchev, and B. Taskar. 2007. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
W.P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency pars-
ing with richer contexts and smoothing. In Proc.
NAACL.
M. Johnson, T.L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
Proc. NIPS.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese Treebank in VERBMOBIL. Technical re-
port, Eberhard-Karls-Universitat Tubingen.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. ACL.
M.T. Kromann, L. Mikkelsen, and S.K. Lynge. 2003.
Danish Dependency Treebank. In Proc. TLT.
P. Liang, S. Petrov, M.I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proc. EMNLP.
P. Liang, M.I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families. In
Proc. ICML.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In Proc. ACL.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
R. Neal and G. Hinton. 1998. A new view of the EM
algorithm that justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in Graph-
ical Models, pages 355–368. MIT Press.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from
antiquity. NODALIDA Special Session on Tree-
banks.
K. Oflazer, B. Say, D.Z. Hakkani-Tür, and G. Tür.
2003. Building a Turkish treebank. Treebanks:
Building and Using Parsed Corpora.
K. Simov, P. Osenova, M. Slavcheva, S. Kolkovska,
E. Balabanova, D. Doikoff, K. Ivanova, A. Simov,
E. Simov, and M. Kouylekov. 2002. Building a lin-
guistically interpreted corpus of bulgarian: the bul-
treebank. In Proc. LREC.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers.
</reference>
<page confidence="0.998909">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897240">
<title confidence="0.999168">Sparsity in Dependency Grammar Induction</title>
<author confidence="0.994369">João Graça</author>
<affiliation confidence="0.99193">INESC-ID</affiliation>
<address confidence="0.980979">Lisboa, Portugal</address>
<email confidence="0.975014">joao.graca@l2f.inesc-id.pt</email>
<author confidence="0.961875">Gillenwater Ganchev</author>
<affiliation confidence="0.999922">University of Pennsylvania</affiliation>
<address confidence="0.999459">Philadelphia, PA, USA</address>
<email confidence="0.999717">jengi@cis.upenn.edu</email>
<email confidence="0.999717">kuzman@cis.upenn.edu</email>
<author confidence="0.998211">Fernando Pereira</author>
<affiliation confidence="0.999812">Google Inc.</affiliation>
<address confidence="0.999529">Mountain View, CA, USA</address>
<email confidence="0.999403">pereira@google.com</email>
<author confidence="0.999629">Ben Taskar</author>
<affiliation confidence="0.99989">University of Pennsylvania</affiliation>
<address confidence="0.9995">Philadelphia, PA, USA</address>
<email confidence="0.99986">taskar@cis.upenn.edu</email>
<abstract confidence="0.99963680952381">A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta Sinta(c)tica: a treebank for Portuguese. In</title>
<date>2002</date>
<booktitle>Proc. LREC.</booktitle>
<contexts>
<context position="17584" citStr="Afonso et al., 2002" startWordPosition="3096" endWordPosition="3099">cy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the posterior probabilities. though the average increase is only 1.5%. PR-S is also better than DD for 10 out of 12 languages. If we instead consider these methods for t</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta Sinta(c)tica: a treebank for Portuguese. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bellare</author>
<author>G Druck</author>
<author>A McCallum</author>
</authors>
<title>Alternating projections for learning with expectation constraints.</title>
<date>2009</date>
<booktitle>In Proc. UAI.</booktitle>
<contexts>
<context position="8665" citStr="Bellare et al., 2009" startWordPosition="1444" endWordPosition="1447">t posits a more concise explanation for the grammar of a language. Headden III et al. (2009) also implement a sort of parameter tying for the E-DMV through a learning a backoff distribution on child probabilities. We compare against results from all these methods. 4 Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. (2010) for more details). For a distribution pg, we define a penalty as the (generic) β-norm of expectations of some features φ: ||Ep,,</context>
</contexts>
<marker>Bellare, Druck, McCallum, 2009</marker>
<rawString>K. Bellare, G. Druck, and A. McCallum. 2009. Alternating projections for learning with expectation constraints. In Proc. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bohomovà</author>
<author>J Hajic</author>
<author>E Hajicova</author>
<author>B Hladka</author>
</authors>
<title>The prague dependency treebank: Three-level annotation scenario.</title>
<date>2001</date>
<editor>In Anne Abeillé, editor,</editor>
<contexts>
<context position="17354" citStr="Bohomovà et al., 2001" startWordPosition="3058" endWordPosition="3061">48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9 PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3 PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorr</context>
</contexts>
<marker>Bohomovà, Hajic, Hajicova, Hladka, 2001</marker>
<rawString>A. Bohomovà, J. Hajic, E. Hajicova, and B. Hladka. 2001. The prague dependency treebank: Three-level annotation scenario. In Anne Abeillé, editor, Treebanks: Building and Using Syntactically Annotated Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proc. Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="17389" citStr="Brants et al., 2002" startWordPosition="3064" endWordPosition="3067">.3 47.6 35.6 48.9 PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3 PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on th</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER treebank. In Proc. Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Civit</author>
<author>M A Martí</author>
</authors>
<title>Building cast3lb: A Spanish Treebank.</title>
<date>2004</date>
<journal>Research on Language &amp; Computation.</journal>
<contexts>
<context position="17463" citStr="Civit and Martí, 2004" startWordPosition="3076" endWordPosition="3079"> 54.3 45.6 49.1 56.3 PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the posterior probabilities. though the average </context>
</contexts>
<marker>Civit, Martí, 2004</marker>
<rawString>M. Civit and M.A. Martí. 2004. Building cast3lb: A Spanish Treebank. Research on Language &amp; Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>The shared logistic normal distribution for grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="3215" citStr="Cohen and Smith (2009)" startWordPosition="493" endWordPosition="496">sparsity bias avoids inducing competition between dependent types for each head type. We can achieve the desired bias with a constraint on model posteriors during learning, using the posterior regularization (PR) framework (Graça et al., 2007). Specifically, to implement PR we augment the maximum marginal likelihood objective of the dependency model with a term that penalizes head-dependent tag distributions that are too permissive. Although not focused on sparsity, several other studies use soft parameter sharing to couple different types of dependencies. To this end, Cohen et al. (2008) and Cohen and Smith (2009) investigated logistic normal priors, and Headden III et al. (2009) used a backoff scheme. We compare to their results in Section 5. The remainder of this paper is organized as fol194 Proceedings of the ACL 2010 Conference Short Papers, pages 194–199, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics lows. Section 2 and 3 review the models and several previous approaches for learning them. Section 4 describes learning with PR. Section 5 describes experiments across 12 languages and Section 6 analyzes the results. For additional details on this work see Gillenwa</context>
<context position="7924" citStr="Cohen and Smith, 2009" startWordPosition="1322" endWordPosition="1325"> several Bayesian priors that have been applied to the DMV. One such prior is the Dirichlet, whose hyperparameter we will denote by α. For α &lt; 0.5, this prior encourages parameter sparsity. Cohen et al. (2008) use this method with α = 0.25 for training the DMV and achieve improvements over basic EM. In this paper we will refer to our own implementation of the Dirichlet prior as the “discounting Dirichlet” (DD) method. In addition to 195 the Dirichlet, other types of priors have been applied, in particular logistic normal priors (LN) and shared logistic normal priors (SLN) (Cohen et al., 2008; Cohen and Smith, 2009). LN and SLN aim to tie parameters together. Essentially, this has a similar goal to sparsity-inducing methods in that it posits a more concise explanation for the grammar of a language. Headden III et al. (2009) also implement a sort of parameter tying for the E-DMV through a learning a backoff distribution on child probabilities. We compare against results from all these methods. 4 Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al.,</context>
<context position="12682" citStr="Cohen and Smith (2009)" startWordPosition="2209" endWordPosition="2212"> accuracy results. Column 1: V, V. used for the E-DMV models. Column 3: Best PR result for each model, which is chosen by applying each of the two types of constraints (PR-S and PR-AS) and trying v E {80, 100, 120, 140, 160, 180}. Columns 4 &amp; 5: Constraint type and v that produced the values in column 3. Learning Method Accuracy &lt; 10 &lt; 20 all PR-S (v = 140) 62.1 53.8 49.1 LN families 59.3 45.1 39.0 SLN TieV &amp; N 61.3 47.4 41.4 PR-AS (v = 140) 64.4 55.2 50.5 DD (α = 1, A learned) 65.0 (f5.7) Table 2: Comparison with previous published results. Rows 2 and 3 are taken from Cohen et al. (2008) and Cohen and Smith (2009), and row 5 from Headden III et al. (2009). and Manning (2004), which we refer to as K&amp;M. We always train for 100 iterations and evaluate on the test set using Viterbi parses. Before evaluating, we smooth the resulting models by adding e−10 to each learned parameter, merely to remove the chance of zero probabilities for unseen events. (We did not tune this as it should make very little difference for final parses.) We score models by their attachment accuracy — the fraction of words assigned the correct parent. 5.1 Results on English We start by comparing English performance for EM, PR, and DD</context>
<context position="14994" citStr="Cohen and Smith, 2009" startWordPosition="2620" endWordPosition="2623">ts for English. It might be argued that the comparison is unfair since we do supervised selection of model complexity and regularization strength. However, we feel the comparison is not so unfair as we perform only a very limited search of the model-a space. Specifically, the only values of a we search over are {80,100,120,140,160,180}. First, we consider the top three entries in Table 2, which are for the basic DMV. The first entry was generated using our implementation of PR-S. The second two entries are logistic normal and shared logistic normal parameter tying results (Cohen et al., 2008; Cohen and Smith, 2009). The PR-S result is the clear winner, especially as length of test sentences increases. For the bottom two entries in the table, which are for the EDMV, the last entry is best, corresponding to using a DD prior with a = 1 (non-sparsifying), but with a special “random pools” initialization and a learned weight A for the child backoff probability. The result for PR-AS is well within the variance range of this last entry, and thus we conjecture that combining PR-AS with random pools initialization and learned A would likely produce the best-performing model of all. 5.3 Results on Other Languages</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S.B. Cohen and N.A. Smith. 2009. The shared logistic normal distribution for grammar induction. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="2275" citStr="Cohen et al. (2008)" startWordPosition="344" endWordPosition="347"> about grammatical relations, it is reasonable to assume that only some of the possible dependency types will be realized for a given language. For instance, in English it is ungrammatical for nouns to dominate verbs, adjectives to dominate adverbs, and determiners to dominate almost any part of speech. Thus, the realized dependency types should be a sparse subset of all possible types. Previous work in unsupervised grammar induction has tried to achieve sparsity through priors. Liang et al. (2007), Finkel et al. (2007) and Johnson et al. (2007) proposed hierarchical Dirichlet process priors. Cohen et al. (2008) experimented with a discounting Dirichlet prior, which encourages a standard dependency parsing model (see Section 2) to limit the number of dependent types for each head type. Our experiments show a more effective sparsity pattern is one that limits the total number of unique head-dependent tag pairs. This kind of sparsity bias avoids inducing competition between dependent types for each head type. We can achieve the desired bias with a constraint on model posteriors during learning, using the posterior regularization (PR) framework (Graça et al., 2007). Specifically, to implement PR we augm</context>
<context position="7511" citStr="Cohen et al. (2008)" startWordPosition="1250" endWordPosition="1253">nds L(0). Starting from an initial parameter estimate 00, the algorithm iterates two steps: E : qt+1 = arg min KL(q(Y) II pθt(Y |X)) (3) q M : θt+1 = arg max Eqt+1 [log pθ(X, Y)] (4) θ Note that the E-step just sets qt+1(Y) = pot(Y|X), since it is an unconstrained minimization of a KL-divergence. The PR method we present modifies the E-step by adding constraints. Besides EM, we also compare to learning with several Bayesian priors that have been applied to the DMV. One such prior is the Dirichlet, whose hyperparameter we will denote by α. For α &lt; 0.5, this prior encourages parameter sparsity. Cohen et al. (2008) use this method with α = 0.25 for training the DMV and achieve improvements over basic EM. In this paper we will refer to our own implementation of the Dirichlet prior as the “discounting Dirichlet” (DD) method. In addition to 195 the Dirichlet, other types of priors have been applied, in particular logistic normal priors (LN) and shared logistic normal priors (SLN) (Cohen et al., 2008; Cohen and Smith, 2009). LN and SLN aim to tie parameters together. Essentially, this has a similar goal to sparsity-inducing methods in that it posits a more concise explanation for the grammar of a language. </context>
<context position="12655" citStr="Cohen et al. (2008)" startWordPosition="2204" endWordPosition="2207"> 140 Table 1: Attachment accuracy results. Column 1: V, V. used for the E-DMV models. Column 3: Best PR result for each model, which is chosen by applying each of the two types of constraints (PR-S and PR-AS) and trying v E {80, 100, 120, 140, 160, 180}. Columns 4 &amp; 5: Constraint type and v that produced the values in column 3. Learning Method Accuracy &lt; 10 &lt; 20 all PR-S (v = 140) 62.1 53.8 49.1 LN families 59.3 45.1 39.0 SLN TieV &amp; N 61.3 47.4 41.4 PR-AS (v = 140) 64.4 55.2 50.5 DD (α = 1, A learned) 65.0 (f5.7) Table 2: Comparison with previous published results. Rows 2 and 3 are taken from Cohen et al. (2008) and Cohen and Smith (2009), and row 5 from Headden III et al. (2009). and Manning (2004), which we refer to as K&amp;M. We always train for 100 iterations and evaluate on the test set using Viterbi parses. Before evaluating, we smooth the resulting models by adding e−10 to each learned parameter, merely to remove the chance of zero probabilities for unseen events. (We did not tune this as it should make very little difference for final parses.) We score models by their attachment accuracy — the fraction of words assigned the correct parent. 5.1 Results on English We start by comparing English per</context>
<context position="14970" citStr="Cohen et al., 2008" startWordPosition="2616" endWordPosition="2619">ndency parsing results for English. It might be argued that the comparison is unfair since we do supervised selection of model complexity and regularization strength. However, we feel the comparison is not so unfair as we perform only a very limited search of the model-a space. Specifically, the only values of a we search over are {80,100,120,140,160,180}. First, we consider the top three entries in Table 2, which are for the basic DMV. The first entry was generated using our implementation of PR-S. The second two entries are logistic normal and shared logistic normal parameter tying results (Cohen et al., 2008; Cohen and Smith, 2009). The PR-S result is the clear winner, especially as length of test sentences increases. For the bottom two entries in the table, which are for the EDMV, the last entry is best, corresponding to using a DD prior with a = 1 (non-sparsifying), but with a special “random pools” initialization and a learned weight A for the child backoff probability. The result for PR-AS is well within the variance range of this last entry, and thus we conjecture that combining PR-AS with random pools initialization and learned A would likely produce the best-performing model of all. 5.3 Re</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>S.B. Cohen, K. Gimpel, and N.A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="9837" citStr="Dempster et al., 1977" startWordPosition="1647" endWordPosition="1650"> β-norm of expectations of some features φ: ||Ep,, [φ(X, Y)]||a (5) For computational tractability, rather than penalizing the model’s posteriors directly, we use an auxiliary distribution q, and penalize the marginal loglikelihood of a model by the KL-divergence of pg from q, plus the penalty term with respect to q. For a fixed set of model parameters θ the full PR penalty term is: min 9 KL(q(Y) II pe(Y|X)) + σ ||E9[φ(X, Y)]||a (6) where σ is the strength of the regularization. PR seeks to maximize L(θ) minus this penalty term. The resulting objective can be optimized by a variant of the EM (Dempster et al., 1977) algorithm used to optimize L(θ). 4.1 `1/`� Regularization We now define precisely how to count dependency types. For each child tag c, let i range over an enumeration of all occurrences of c in the corpus, and let p be another tag. Let the indicator φ pi(X, Y) have value 1 if p is the parent tag of the ith occurrence of c, and value 0 otherwise. The number of unique dependency types is then: E max φcpi(X, Y) (7) i cp Note there is an asymmetry in this count: occurrences of child type c are enumerated with i, but all occurrences of parent type p are or-ed in φ pi. That is, φ pi = 1 if any occu</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Džeroski</author>
<author>T Erjavec</author>
<author>N Ledinek</author>
<author>P Pajas</author>
<author>Z Žabokrtsky</author>
<author>A Žele</author>
</authors>
<title>Towards a Slovene dependency treebank.</title>
<date>2006</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="17659" citStr="Džeroski et al., 2006" startWordPosition="3108" endWordPosition="3111">Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the posterior probabilities. though the average increase is only 1.5%. PR-S is also better than DD for 10 out of 12 languages. If we instead consider these methods for the EDMV, DD performs worse, just 1.4% better than the E-DMV EM, while both </context>
</contexts>
<marker>Džeroski, Erjavec, Ledinek, Pajas, Žabokrtsky, Žele, 2006</marker>
<rawString>S. Džeroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Žabokrtsky, and A. Žele. 2006. Towards a Slovene dependency treebank. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="2181" citStr="Finkel et al. (2007)" startWordPosition="329" endWordPosition="332"> of the head (also known as the parent). Given that POS tags are designed to convey information about grammatical relations, it is reasonable to assume that only some of the possible dependency types will be realized for a given language. For instance, in English it is ungrammatical for nouns to dominate verbs, adjectives to dominate adverbs, and determiners to dominate almost any part of speech. Thus, the realized dependency types should be a sparse subset of all possible types. Previous work in unsupervised grammar induction has tried to achieve sparsity through priors. Liang et al. (2007), Finkel et al. (2007) and Johnson et al. (2007) proposed hierarchical Dirichlet process priors. Cohen et al. (2008) experimented with a discounting Dirichlet prior, which encourages a standard dependency parsing model (see Section 2) to limit the number of dependent types for each head type. Our experiments show a more effective sparsity pattern is one that limits the total number of unique head-dependent tag pairs. This kind of sparsity bias avoids inducing competition between dependent types for each head type. We can achieve the desired bias with a constraint on model posteriors during learning, using the poste</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2007. The infinite tree. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Graça</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="9136" citStr="Ganchev et al. (2010)" startWordPosition="1521" endWordPosition="1524">a et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. (2010) for more details). For a distribution pg, we define a penalty as the (generic) β-norm of expectations of some features φ: ||Ep,, [φ(X, Y)]||a (5) For computational tractability, rather than penalizing the model’s posteriors directly, we use an auxiliary distribution q, and penalize the marginal loglikelihood of a model by the KL-divergence of pg from q, plus the penalty term with respect to q. For a fixed set of model parameters θ the full PR penalty term is: min 9 KL(q(Y) II pe(Y|X)) + σ ||E9[φ(X, Y)]||a (6) where σ is the strength of the regularization. PR seeks to maximize L(θ) minus this </context>
<context position="11617" citStr="Ganchev et al., 2010" startWordPosition="1997" endWordPosition="2000"> both. Equation 7 can be viewed as a mixed-norm penalty on the features φ pi or φ pig: the sum corresponds to an `1 norm and the max to an `� norm. Thus, the quantity we want to minimize fits precisely into the PR penalty framework. Formally, to optimize the PR objective, we complete the following E-step: arg min KL(q(Y)||pe(Y|X)) + σ � 9 cp which can equivalently be written as: KL(q(Y) II pe(Y|X)) + σ � ξcp cp (9) s. �. ξcp &lt; E9[φ(X, Y)] where ξ p corresponds to the maximum expectation of φ over all instances of c and p. Note that the projection problem can be solved efficiently in the dual (Ganchev et al., 2010). 5 Experiments We evaluate on 12 languages. Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length &lt; 10. For simplicity, for all models we use the “harmonic” initializer from Klein E9[φ(X, Y)], (8) max i min 9(Y),Gp 196 Model EM PR Type v DMV 45.8 62.1 PR-S 140 2-1 45.1 62.7 PR-S 100 2-2 54.4 62.9 PR-S 80 3-3 55.3 64.3 PR-S 140 4-4 55.1 64.4 PR-AS 140 Table 1: Attachment accuracy results. Column 1: V, V. used for the E-DMV models. Column 3: Best PR result for each model, which is chosen by applying each of the two types of c</context>
</contexts>
<marker>Ganchev, Graça, Gillenwater, Taskar, 2010</marker>
<rawString>K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Graça</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Posterior sparsity in unsupervised dependency parsing.</title>
<date>2010</date>
<tech>Technical report, MS-CIS-10-19,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3832" citStr="Gillenwater et al. (2010)" startWordPosition="595" endWordPosition="598">h (2009) investigated logistic normal priors, and Headden III et al. (2009) used a backoff scheme. We compare to their results in Section 5. The remainder of this paper is organized as fol194 Proceedings of the ACL 2010 Conference Short Papers, pages 194–199, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics lows. Section 2 and 3 review the models and several previous approaches for learning them. Section 4 describes learning with PR. Section 5 describes experiments across 12 languages and Section 6 analyzes the results. For additional details on this work see Gillenwater et al. (2010). 2 Parsing Model The models we use are based on the generative dependency model with valence (DMV) (Klein and Manning, 2004). For a sentence with tags x, the root POS r(x) is generated first. Then the model decides whether to generate a right dependent conditioned on the POS of the root and whether other right dependents have already been generated for this head. Upon deciding to generate a right dependent, the POS of the dependent is selected by conditioning on the head POS and the directionality. After stopping on the right, the root generates left dependents using the mirror reversal of th</context>
</contexts>
<marker>Gillenwater, Ganchev, Graça, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Graça, F. Pereira, and B. Taskar. 2010. Posterior sparsity in unsupervised dependency parsing. Technical report, MS-CIS-10-19, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Graça</author>
<author>K Ganchev</author>
<author>B Taskar</author>
</authors>
<title>Expectation maximization and posterior constraints.</title>
<date>2007</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="767" citStr="Graça et al. (2007)" startWordPosition="96" endWordPosition="99"> University of Pennsylvania Philadelphia, PA, USA {jengi,kuzman}@cis.upenn.edu Fernando Pereira Google Inc. Mountain View, CA, USA pereira@google.com Ben Taskar University of Pennsylvania Philadelphia, PA, USA taskar@cis.upenn.edu Abstract A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques. 1 Introduction We investigate an unsupervised learning method for dependency parsing models that imposes sparsity biases on the dependency types. We assume a corpus annotated with POS tags, where th</context>
<context position="2836" citStr="Graça et al., 2007" startWordPosition="434" endWordPosition="437">hierarchical Dirichlet process priors. Cohen et al. (2008) experimented with a discounting Dirichlet prior, which encourages a standard dependency parsing model (see Section 2) to limit the number of dependent types for each head type. Our experiments show a more effective sparsity pattern is one that limits the total number of unique head-dependent tag pairs. This kind of sparsity bias avoids inducing competition between dependent types for each head type. We can achieve the desired bias with a constraint on model posteriors during learning, using the posterior regularization (PR) framework (Graça et al., 2007). Specifically, to implement PR we augment the maximum marginal likelihood objective of the dependency model with a term that penalizes head-dependent tag distributions that are too permissive. Although not focused on sparsity, several other studies use soft parameter sharing to couple different types of dependencies. To this end, Cohen et al. (2008) and Cohen and Smith (2009) investigated logistic normal priors, and Headden III et al. (2009) used a backoff scheme. We compare to their results in Section 5. The remainder of this paper is organized as fol194 Proceedings of the ACL 2010 Conferenc</context>
<context position="8530" citStr="Graça et al., 2007" startWordPosition="1423" endWordPosition="1426"> Smith, 2009). LN and SLN aim to tie parameters together. Essentially, this has a similar goal to sparsity-inducing methods in that it posits a more concise explanation for the grammar of a language. Headden III et al. (2009) also implement a sort of parameter tying for the E-DMV through a learning a backoff distribution on child probabilities. We compare against results from all these methods. 4 Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. </context>
</contexts>
<marker>Graça, Ganchev, Taskar, 2007</marker>
<rawString>J. Graça, K. Ganchev, and B. Taskar. 2007. Expectation maximization and posterior constraints. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. NAACL.</booktitle>
<marker>Johnson, McClosky, 2009</marker>
<rawString>W.P. Headden III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="2207" citStr="Johnson et al. (2007)" startWordPosition="334" endWordPosition="338">as the parent). Given that POS tags are designed to convey information about grammatical relations, it is reasonable to assume that only some of the possible dependency types will be realized for a given language. For instance, in English it is ungrammatical for nouns to dominate verbs, adjectives to dominate adverbs, and determiners to dominate almost any part of speech. Thus, the realized dependency types should be a sparse subset of all possible types. Previous work in unsupervised grammar induction has tried to achieve sparsity through priors. Liang et al. (2007), Finkel et al. (2007) and Johnson et al. (2007) proposed hierarchical Dirichlet process priors. Cohen et al. (2008) experimented with a discounting Dirichlet prior, which encourages a standard dependency parsing model (see Section 2) to limit the number of dependent types for each head type. Our experiments show a more effective sparsity pattern is one that limits the total number of unique head-dependent tag pairs. This kind of sparsity bias avoids inducing competition between dependent types for each head type. We can achieve the desired bias with a constraint on model posteriors during learning, using the posterior regularization (PR) f</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T.L. Griffiths, and S. Goldwater. 2007. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kawata</author>
<author>J Bartels</author>
</authors>
<title>Stylebook for the Japanese Treebank in VERBMOBIL.</title>
<date>2000</date>
<tech>Technical report, Eberhard-Karls-Universitat Tubingen.</tech>
<contexts>
<context position="17505" citStr="Kawata and Bartels, 2000" startWordPosition="3082" endWordPosition="3085">.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the posterior probabilities. though the average increase is only 1.5%. PR-S is also better</context>
</contexts>
<marker>Kawata, Bartels, 2000</marker>
<rawString>Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese Treebank in VERBMOBIL. Technical report, Eberhard-Karls-Universitat Tubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="3957" citStr="Klein and Manning, 2004" startWordPosition="617" endWordPosition="620"> in Section 5. The remainder of this paper is organized as fol194 Proceedings of the ACL 2010 Conference Short Papers, pages 194–199, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics lows. Section 2 and 3 review the models and several previous approaches for learning them. Section 4 describes learning with PR. Section 5 describes experiments across 12 languages and Section 6 analyzes the results. For additional details on this work see Gillenwater et al. (2010). 2 Parsing Model The models we use are based on the generative dependency model with valence (DMV) (Klein and Manning, 2004). For a sentence with tags x, the root POS r(x) is generated first. Then the model decides whether to generate a right dependent conditioned on the POS of the root and whether other right dependents have already been generated for this head. Upon deciding to generate a right dependent, the POS of the dependent is selected by conditioning on the head POS and the directionality. After stopping on the right, the root generates left dependents using the mirror reversal of this process. Once the root has generated all its dependents, the dependents generate their own dependents in the same manner. </context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Kromann</author>
<author>L Mikkelsen</author>
<author>S K Lynge</author>
</authors>
<title>Danish Dependency Treebank. In</title>
<date>2003</date>
<booktitle>Proc. TLT.</booktitle>
<contexts>
<context position="17425" citStr="Kromann et al., 2003" startWordPosition="3070" endWordPosition="3073">.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3 PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the poster</context>
</contexts>
<marker>Kromann, Mikkelsen, Lynge, 2003</marker>
<rawString>M.T. Kromann, L. Mikkelsen, and S.K. Lynge. 2003. Danish Dependency Treebank. In Proc. TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>S Petrov</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="2159" citStr="Liang et al. (2007)" startWordPosition="325" endWordPosition="328">s the child), and tag of the head (also known as the parent). Given that POS tags are designed to convey information about grammatical relations, it is reasonable to assume that only some of the possible dependency types will be realized for a given language. For instance, in English it is ungrammatical for nouns to dominate verbs, adjectives to dominate adverbs, and determiners to dominate almost any part of speech. Thus, the realized dependency types should be a sparse subset of all possible types. Previous work in unsupervised grammar induction has tried to achieve sparsity through priors. Liang et al. (2007), Finkel et al. (2007) and Johnson et al. (2007) proposed hierarchical Dirichlet process priors. Cohen et al. (2008) experimented with a discounting Dirichlet prior, which encourages a standard dependency parsing model (see Section 2) to limit the number of dependent types for each head type. Our experiments show a more effective sparsity pattern is one that limits the total number of unique head-dependent tag pairs. This kind of sparsity bias avoids inducing competition between dependent types for each head type. We can achieve the desired bias with a constraint on model posteriors during lea</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>P. Liang, S. Petrov, M.I. Jordan, and D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="8780" citStr="Liang et al., 2009" startWordPosition="1463" endWordPosition="1466"> parameter tying for the E-DMV through a learning a backoff distribution on child probabilities. We compare against results from all these methods. 4 Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. (2010) for more details). For a distribution pg, we define a penalty as the (generic) β-norm of expectations of some features φ: ||Ep,, [φ(X, Y)]||a (5) For computational tractability, rather than penalizing the model’s posteriors directly, we use an</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M.I. Jordan, and D. Klein. 2009. Learning from measurements in exponential families. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Simple, robust, scalable semi-supervised learning via expectation regularization.</title>
<date>2007</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="8617" citStr="Mann and McCallum, 2007" startWordPosition="1436" endWordPosition="1439">imilar goal to sparsity-inducing methods in that it posits a more concise explanation for the grammar of a language. Headden III et al. (2009) also implement a sort of parameter tying for the E-DMV through a learning a backoff distribution on child probabilities. We compare against results from all these methods. 4 Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. (2010) for more details). For a distribution pg, we define a penalty as the (generic) β</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>G. Mann and A. McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation regularization. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="8642" citStr="Mann and McCallum, 2008" startWordPosition="1440" endWordPosition="1443">nducing methods in that it posits a more concise explanation for the grammar of a language. Headden III et al. (2009) also implement a sort of parameter tying for the E-DMV through a learning a backoff distribution on child probabilities. We compare against results from all these methods. 4 Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. (2010) for more details). For a distribution pg, we define a penalty as the (generic) β-norm of expectations of </context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>G. Mann and A. McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="17221" citStr="Marcus et al., 1993" startWordPosition="3034" endWordPosition="3037">0.2 37.9 47.8 38.7 50.3 53.4 Extended Model EM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2 DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9 PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3 PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold de</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
<author>G Hinton</author>
</authors>
<title>A new view of the EM algorithm that justifies incremental, sparse and other variants.</title>
<date>1998</date>
<booktitle>Learning in Graphical Models,</booktitle>
<pages>355--368</pages>
<editor>In M. I. Jordan, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6828" citStr="Neal and Hinton (1998)" startWordPosition="1126" endWordPosition="1130">on parent POS can be formally expressed as: λpchild(yc |yp, yd, yv.) + (1 − λ)pchild(yc |yd, yvJ (2) for A E [0, 1]. We fix A = 1/3, which is a crude approximation to the value learned by Headden III et al. (2009). 3 Previous Learning Approaches In our experiments, we compare PR learning to standard expectation maximization (EM) and to Bayesian learning with a sparsity-inducing prior. The EM algorithm optimizes marginal likelihood L(0) = log EY po(X, Y), where X = {x1, ... , xn} denotes the entire unlabeled corpus and Y = {y1, ... , yn} denotes a set of corresponding parses for each sentence. Neal and Hinton (1998) view EM as block coordinate ascent on a function that lower-bounds L(0). Starting from an initial parameter estimate 00, the algorithm iterates two steps: E : qt+1 = arg min KL(q(Y) II pθt(Y |X)) (3) q M : θt+1 = arg max Eqt+1 [log pθ(X, Y)] (4) θ Note that the E-step just sets qt+1(Y) = pot(Y|X), since it is an unconstrained minimization of a KL-divergence. The PR method we present modifies the E-step by adding constraints. Besides EM, we also compare to learning with several Bayesian priors that have been applied to the DMV. One such prior is the Dirichlet, whose hyperparameter we will deno</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. Neal and G. Hinton. 1998. A new view of the EM algorithm that justifies incremental, sparse and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nilsson</author>
<author>J Hall</author>
<author>J Nivre</author>
</authors>
<title>MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity.</title>
<date>2005</date>
<journal>NODALIDA Special Session on Treebanks.</journal>
<contexts>
<context position="17621" citStr="Nilsson et al., 2005" startWordPosition="3102" endWordPosition="3105">the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the posterior probabilities. though the average increase is only 1.5%. PR-S is also better than DD for 10 out of 12 languages. If we instead consider these methods for the EDMV, DD performs worse, just 1.4%</context>
</contexts>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. NODALIDA Special Session on Treebanks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>B Say</author>
<author>D Z Hakkani-Tür</author>
<author>G Tür</author>
</authors>
<title>Building a Turkish treebank. Treebanks: Building and Using Parsed Corpora.</title>
<date>2003</date>
<contexts>
<context position="17700" citStr="Oflazer et al., 2003" startWordPosition="3115" endWordPosition="3118">ven after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct dependencies and red indicates incorrect dependencies. The numbers on the edges are the values of the posterior probabilities. though the average increase is only 1.5%. PR-S is also better than DD for 10 out of 12 languages. If we instead consider these methods for the EDMV, DD performs worse, just 1.4% better than the E-DMV EM, while both PR-S and PR-AS continue to show substanti</context>
</contexts>
<marker>Oflazer, Say, Hakkani-Tür, Tür, 2003</marker>
<rawString>K. Oflazer, B. Say, D.Z. Hakkani-Tür, and G. Tür. 2003. Building a Turkish treebank. Treebanks: Building and Using Parsed Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>P Osenova</author>
<author>M Slavcheva</author>
<author>S Kolkovska</author>
<author>E Balabanova</author>
<author>D Doikoff</author>
<author>K Ivanova</author>
<author>A Simov</author>
<author>E Simov</author>
<author>M Kouylekov</author>
</authors>
<title>Building a linguistically interpreted corpus of bulgarian: the bultreebank. In</title>
<date>2002</date>
<booktitle>Proc. LREC.</booktitle>
<contexts>
<context position="17318" citStr="Simov et al., 2002" startWordPosition="3052" endWordPosition="3055">48.6 47.5 46.2 DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9 PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3 PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9 Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters (α or v) are given after the method name. For the extended model (V,, V3) are indicated in parentheses. En is the English Penn Treebank (Marcus et al., 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al., 2002), Czech [Cz] (Bohomovà et al., 2001), German [De] (Brants et al., 2002), Danish [Dk] (Kromann et al., 2003), Spanish [Es] (Civit and Martí, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al., 2002), Portuguese [Pt] (Afonso et al., 2002), Swedish [Se] (Nilsson et al., 2005), Slovene [Sl] (Džeroski et al., 2006), and Turkish [Tr] (Oflazer et al., 2003). Figure 1: Posterior edge probabilities for an example sentence from the Spanish test corpus. At the top are the gold dependencies, the middle are EM posteriors, and bottom are PR posteriors. Green indicates correct d</context>
</contexts>
<marker>Simov, Osenova, Slavcheva, Kolkovska, Balabanova, Doikoff, Ivanova, Simov, Simov, Kouylekov, 2002</marker>
<rawString>K. Simov, P. Osenova, M. Slavcheva, S. Kolkovska, E. Balabanova, D. Doikoff, K. Ivanova, A. Simov, E. Simov, and M. Kouylekov. 2002. Building a linguistically interpreted corpus of bulgarian: the bultreebank. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="11710" citStr="Smith and Eisner (2006)" startWordPosition="2013" endWordPosition="2016">sum corresponds to an `1 norm and the max to an `� norm. Thus, the quantity we want to minimize fits precisely into the PR penalty framework. Formally, to optimize the PR objective, we complete the following E-step: arg min KL(q(Y)||pe(Y|X)) + σ � 9 cp which can equivalently be written as: KL(q(Y) II pe(Y|X)) + σ � ξcp cp (9) s. �. ξcp &lt; E9[φ(X, Y)] where ξ p corresponds to the maximum expectation of φ over all instances of c and p. Note that the projection problem can be solved efficiently in the dual (Ganchev et al., 2010). 5 Experiments We evaluate on 12 languages. Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length &lt; 10. For simplicity, for all models we use the “harmonic” initializer from Klein E9[φ(X, Y)], (8) max i min 9(Y),Gp 196 Model EM PR Type v DMV 45.8 62.1 PR-S 140 2-1 45.1 62.7 PR-S 100 2-2 54.4 62.9 PR-S 80 3-3 55.3 64.3 PR-S 140 4-4 55.1 64.4 PR-AS 140 Table 1: Attachment accuracy results. Column 1: V, V. used for the E-DMV models. Column 3: Best PR result for each model, which is chosen by applying each of the two types of constraints (PR-S and PR-AS) and trying v E {80, 100, 120, 140, 160, 180}. Columns 4 &amp; 5: Cons</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>N. Smith and J. Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Van der Beek</author>
<author>G Bouma</author>
<author>R Malouf</author>
<author>G Van Noord</author>
</authors>
<title>The Alpino dependency treebank. Language and Computers.</title>
<date>2002</date>
<marker>Van der Beek, Bouma, Malouf, Van Noord, 2002</marker>
<rawString>L. Van der Beek, G. Bouma, R. Malouf, and G. Van Noord. 2002. The Alpino dependency treebank. Language and Computers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>