<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000163">
<title confidence="0.967317">
Joint Morphological-Lexical Language Modeling for Machine Translation
</title>
<author confidence="0.759084">
Ruhi Sarikaya Yonggang Deng
</author>
<affiliation confidence="0.482965">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.669759">
Yorktown Heights, NY 10598
</address>
<email confidence="0.980779">
sarikaya@us.ibm.com ydeng@us.ibm.com
</email>
<sectionHeader confidence="0.993505" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969">
We present a joint morphological-lexical language
model (JMLLM) for use in statistical machine trans-
lation (SMT) of language pairs where one or both of
the languages are morphologically rich. The pro-
posed JMLLM takes advantage of the rich morphol-
ogy to reduce the Out-Of-Vocabulary (OOV) rate,
while keeping the predictive power of the whole
words. It also allows incorporation of additional
available semantic, syntactic and linguistic informa-
tion about the morphemes and words into the lan-
guage model. Preliminary experiments with an
English to Dialectal-Arabic SMT system demon-
strate improved translation performance over trigram
based baseline language model.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999720181818182">
Statistical machine translation (SMT) methods have
evolved from using the simple word based models
(Brown et al., 1993) to phrase based models (Marcu and
Wong, 2002; Koehn et al., 2004; Och and Ney, 2004).
More recently, there is a significant effort focusing on
integrating richer knowledge, such as syntactic parse trees
(Huang and Knight, 2006) within the translation process
to overcome the limitations of the phrase based models.
The SMT has been formulated as a noisy channel model
in which the target language sentence, e is seen as dis-
torted by the channel into the foreign language f :
</bodyText>
<equation confidence="0.99416">
e =
ˆ argmax (  |) argmax
P e f =
e e
</equation>
<bodyText confidence="0.999841181818182">
where P(f  |e) is the translation model and P(e) is lan-
guage model of the target language. The overwhelming
proportion of the SMT research has been focusing on im-
proving the translation model. Despite several new studies
(Kirchhoff and Yang, 2004; Schwenk et al., 2006), lan-
guage modeling for SMT has not been receiving much
attention. Currently, the state-of-the-art SMT systems
have been using the standard word n-gram models. Since
n-gram models learn from given data, a severe drop in
performance may be observed if the target domain is not
adequately covered in the training data. The coverage
</bodyText>
<page confidence="0.991124">
145
</page>
<bodyText confidence="0.999909578947368">
problem is aggravated for morphologically rich lan-
guages. Arabic is such a language where affixes are
appended to the beginning or end of a stem to generate
new words that indicate case, gender, tense etc. associ-
ated with the stem. Hence, it is natural that this leads to
rapid vocabulary growth, which is accompanied by
worse language model probability estimation due to
data sparsity and high Out-Of-Vocabulary (OOV) rate.
Due to rich morphology, one would suspect that
words may not be the best lexical units for Arabic, and
perhaps morphological units would be a better choice.
Recently, there have been a number of new methods
using the morphological units to represent lexical items
(Ghaoui et al., 2005; Xiang et al., 2006; Choueiter et al.,
2006). Factored Language Models (FLMs) (Kirchhoff
and Yang, 2004) share the same idea to some extent but
here words are decomposed into a number of features
and the resulting representation is used in a generalized
back-off scheme to improve the robustness of probabil-
ity estimates for rarely observed word n-grams.
In this study we propose a tree structure called Mor-
phological-Lexical Parse Tree (MLPT) to combine the
information provided by a morphological analyzer with
the lexical information within a single Joint Morpho-
logical-Lexical Language Model (JMLLM). The MLPT
allows us to include available syntactic and semantic
information about the morphological segments1 (i.e.
prefix/stem/suffix), words or group of words. The
JMLLM can also be used to guide the recognition for
selecting high probability morphological sentence seg-
mentations.
The rest of the paper is organized as follows. Section
2 provides a description of the morphological segmenta-
tion method. A short overview of Maximum Entropy
modeling is given in Section 3. The proposed JMLLM
is presented in Section 4. Section 5 introduces the SMT
system and Section 6 describes the experimental results
followed by the conclusions in Section 7.
</bodyText>
<sectionHeader confidence="0.966666" genericHeader="method">
2 Morphological Segmentation
</sectionHeader>
<bodyText confidence="0.97814">
Applying the morphological segmentation to data
improves the coverage and reduces the OOV rate. In
</bodyText>
<footnote confidence="0.7885355">
1 We use “Morphological Segment” and “Morpheme” inter-
changeably.
</footnote>
<equation confidence="0.7975974">
P ( |
f
e)P ( e)
Proceedings of NAACL HLT 2007, Companion Volume, pages 145–148,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</equation>
<bodyText confidence="0.998507125">
this study we use a rule-based morphological segmenta-
tion algorithm for Iraqi-Arabic (Afify et. al., 2006). This
algorithm analyzes a given surface word, and generates
one of the four possible segmentations: {stem, pre-
fix+stem, suffix+stem, prefix+stem+suffix}. Here, stem
includes those words that do not have any affixes. We use
the longest prefixes (suffixes). Using finer affixes re-
duces the n-gram language model span, and leads to poor
performance for a fixed n-gram size. Therefore, we prede-
fine a set of prefixes and suffixes and perform blind word
segmentation. In order to minimize the illegitimate seg-
mentations we employ the following algorithm. Using the
given set of prefixes and suffixes, a word is first blindly
chopped to one of the four segmentations mentioned
above. This segmentation is accepted if the following
three rules apply:
</bodyText>
<listItem confidence="0.9955265">
(1) The resulting stem has more than two characters.
(2) The resulting stem is accepted by the Buckwalter
morphological analyzer (Buckwalter, 2002).
(3) The resulting stem exists in the original dictionary.
</listItem>
<bodyText confidence="0.9994766">
The first rule eliminates many of the illegitimate segmen-
tations. The second rule ensures that the word is a valid
Arabic stem, given that the Buckwalter morphological
analyzer covers all words in the Arabic language. Unfor-
tunately, the fact that the stem is a valid Arabic stem does
not always imply that the segmentation is valid. The third
rule, while still not offering such guarantee, simply pre-
fers keeping the word intact if its stem does not occur in
the lexicon. In our implementation we used the following
set of prefixes and suffixes for dialectal Iraqi:
</bodyText>
<listItem confidence="0.9027192">
• Prefix list: {chAl, bhAl, lhAl, whAl, wbAl, wAl, bAl,
hAl, EAl, fAl, Al, cd, ll, b, f, c, d, w}.
• Suffix list: {thmA, tynA, hmA, thA, thm, tkm, tnA,
tny,whA, whm, wkm, wnA, wny, An, hA, hm, hn, km,
kn, nA, ny, tm, wA, wh, wk, wn, yn, tk, th, h, k, t, y}.
</listItem>
<bodyText confidence="0.999915888888889">
These affixes are selected based on our knowledge of
their adequacy for dialectal Iraqi Arabic. In addition, we
found in preliminary experiments that keeping the top-N
frequent decomposable words intact led to better per-
formance. A value of N=5000 was experimentally found
to work well in practice. Using this segmentation method
will produce prefixes and suffixes on the SMT output that
are glued to the following or previous word to form mean-
ingful words.
</bodyText>
<sectionHeader confidence="0.991535" genericHeader="method">
3 Maximum Entropy Modeling
</sectionHeader>
<bodyText confidence="0.999803285714286">
The Maximum Entropy (MaxEnt) method is an effec-
tive method to combine multiple information sources
(features) in statistical modeling and has been used widely
in many areas of natural language processing (Berger et
al.,, 2000). The MaxEnt modeling produces a probability
model that is as uniform as possible while matching em-
pirical feature expectations exactly:
</bodyText>
<equation confidence="0.991538">
( , )
o h
P o h
(  |) _ Eifj
j
E
o
</equation>
<bodyText confidence="0.884841285714286">
indicator functions
or features are
when
certain outcomes are generated for certain context. The
MaxEnt model is trained using the Improved Iterative
Scaling algori
fi
“activated”
thm.
which describes the probability of a particular outcome
(e.g. one of the morphemes) given the history (h) or
context. Notice that the denominator includes a sum
over all possible outcomes, o&apos;, which is essentially a
normalization factor for probabilities to sum to 1. The
</bodyText>
<sectionHeader confidence="0.408277" genericHeader="method">
4 Joint Morphological-Lexical Language
</sectionHeader>
<subsectionHeader confidence="0.244749">
Modeling
</subsectionHeader>
<bodyText confidence="0.760090615384616">
The purpose of morphological analysis is to split a
word into its constituting segments. Hence, a set of
segments can form a meaningful lexical unit such as a
word. There
additional information for words or
group of words, such as part-of-speech (POS) tags, syn-
tactic (from parse tree) and semantic information, or
morpheme and word attributes. For example, in Arabic
and to a certain extent in French, some words can be
masculine/feminine or singular/plural. All of these in-
formation sources can be represented using a -what we
call- Morphological-Lexical Parse Tree (MLPT).
MLPT is a tree structured joint representation of lexical,
morphological, attribute, syntactic and semantic content
of the sentence. An example of a MLPT for an Arabic
sentence is shown in Fig. 1. The leaves of the tree are
morphemes that are predicted by the language model.
Each morphological segment has one of the three attrib-
utes: {prefix, stem, suffix} as generated by the morpho-
logical analysis mentioned in Sec. 2. Each word can
take three sets of attributes: {type, gender, number}.
Word type can be considered as POS, but here we con-
sider only nouns (N), verbs (V) and the rest are labeled
as
Gender can be masculine (M) or femi-
nine (F). Number can be singular (S), plural (P) or dou-
ble (D) (this is specific to Arabic). For example, NMP
label for the first2 word,
shows that this word is a
noun (N), male (M), plural (P). Using the information
represented in MLPT for Arabic language modeling
provides
for smooth probability estimation
even for those words that are not seen before.
The JMLLM integrates the local morpheme and
word n-grams, morphological dependencies and attrib-
ute information associated with morphological segments
and words, which are all represented in the MLPT using
the MaxEnt fr
</bodyText>
<figure confidence="0.907943933333333">
may be
“other”(O).
بﺎﺒﺷ,
a back-off
amework. We trained JMLLM for Iraqi-
Arabic text is written (read) from ri
2In
ght-to-left.
e
E
i
fi
i
(o&apos;, h)
e
</figure>
<page confidence="0.996267">
146
</page>
<bodyText confidence="0.9995402">
Arabic speech recognition task (Sarikaya et al., 2007),
and obtained significant improvements over word and
morpheme based trigram language models.
We can construct a single probability model that mod-
els the joint probability of all of the available information
sources in the MLPT. To compute the joint probability of
the morpheme sequence and its MLPT, we use features
extracted from MLPT. Even though the framework is
generic to jointly represent the information sources in the
MLPT, in this study we limit ourselves to using only lexi-
cal and morphological content of the sentence, along with
the morphological attributes simply because the lexical
attributes are not available yet and we are in the process
of labeling them. Therefore, the information we used from
MLPT in Fig. 1 uses everything but the second row that
contains lexical attributes (NFS, VFP, NFS, and NMP).
Using the morphological segmentation improves the
coverage, for example, splitting the word, ةﻮﮭﻘﻟﺎﺑ as لﺎﺑ
(prefix) and ةﻮﮭﻗ (stem) as in Fig. 1, allows us to decode
other combinations of this stem with the prefix and suffix
list provided in Sec.2. These additional combinations
certainly cover those words that are not seen in the un-
segmented training data.
The first step in building the MaxEnt model is to rep-
resent a MLPT as a sequence of morphological segments,
morphological attributes, words, and word attributes using
a bracket notation. Converting the MLPT into a text se-
quence allows us to group the semantically related mor-
phological segments and their attributes. In this notation,
each morphological segment is associated (this associa-
tion is denoted by “=&amp;quot;) with an attribute (i.e. pre-
fix/stem/suffix) and the lexical items are represented by
opening and closing tokens, [WORD and WORD] respec-
tively. The parse tree given in Fig. 1 can be converted into
a token sequence in text format as follows:
</bodyText>
<figure confidence="0.644654666666667">
[!S! [NMP بﺎﺒﺷ=stem NMP] [NFS [ﺔﻘﻄﻨﻤﻟا لا=prefix ﺔﻘﻄﻨﻣ=stem
ﺔﻘﻄﻨﻤﻟا] NFS] [VFP [نوﺪﻌﻘﯾ ي=prefix ﺪﻌﻗ=stem نو=suffix نوﺪﻌﻘﯾ]
VFP] [NFS [ةﻮﮭﻘﻟﺎﺑ لﺎﺑ=prefix ةﻮﮭﻘﻟﺎﺑ=stem ةﻮﮭﻘﻟﺎﺑ] NFS] !S!]
</figure>
<figureCaption confidence="0.53669725">
This representation uniquely defines the MLPT given in
Fig. 1. Given the bracket notation of the text, JMLLM can
be trained in two ways with varying degrees of “tightness
of integration”. A relatively “loose integration” involves
</figureCaption>
<bodyText confidence="0.945589434782609">
using only the leaves of the MLPT as the model output
and estimating P(M|MLPT), where M is the morpheme
sequence. In this case JMLLM predicts only morphemes.
A tight integration method would require every token in
the bracket representation to be an outcome of the joint
model. In our preliminary experiments we chose the
loose integration method, simply because the model
training time was significantly faster than that for the tight
integration. segment. The JMLLM can employ any type
of questions one can derive from MLPT for predicting the
next morphological segment. In addition to regular tri-
gram questions about previous morphological segments,
questions about the attributes of the previous morpho-
Fig 1. Morphological-Lexical Parse Tree.
logical segments, parent lexical item and attributes of
the parent lexical item can be used. Obviously joint
questions combining these information sources are also
used. Obviously joint questions combining these infor-
mation sources are also used. These questions include
(1) previous morpheme i1
m and current active parent
word ( wi ) (2) mi , wi
1 and previous morpheme attribute
</bodyText>
<equation confidence="0.93581175">
( i1
ma ). (3) mai1, ma i2 ,wi ,lexical attribute ( i
wa ) and
m i1, m i2 .
</equation>
<bodyText confidence="0.999917272727273">
The history given in P(o  |h) consists of answers to
these questions. In our experiments, we have not ex-
haustively searched for the best feature set but rather
used a small subset of these features which we believed
to be helpful in predicting the next morpheme. The lan-
guage model score for a given morpheme using JMLLM
is conditioned not only on the previous morphemes but
also on their attributes, and the lexical items and their
attributes. As such, the language model scores are
smoother compared to n-gram models especially for
unseen lexical items.
</bodyText>
<sectionHeader confidence="0.973523" genericHeader="method">
5 Statistical Machine Translation System
</sectionHeader>
<bodyText confidence="0.999913352941176">
Starting from a collection of parallel sentences, we
trained word alignment models in two translation direc-
tions, from English to Iraqi Arabic and from Iraqi Ara-
bic to English, and derived two sets of Viterbi
alignments. By combining word alignments in two di-
rections using heuristics (Och and Ney, 2003), a single
set of static word alignments was then formed. All
phrase pairs which respect to the word alignment
boundary constraint were identified and pooled together
to build phrase translation tables with the Maximum
Likelihood criterion. The maximum number of words in
Arabic phrases was set to 5.
Our decoder is the phrase-based multi-stack imple-
mentation of log-linear models similar to Pharaoh
(Koehn et al, 2004). Like most other MaxEnt-based
decoders, active features in our decoder include transla-
tion models in two directions, lexicon weights in two
</bodyText>
<figure confidence="0.998056083333334">
stem
ةﻮﮭﻗ
NFS
ةﻮﮭﻘﻟﺎﺑ
prefix
لﺎﺑ
suffix
نو
VFP
نوﺪﻌﻘﯾ
stem
ﺪﻌﻗ
prefix
!S!
ي
stem
ﺔﻘﻄﻨﻣ
ﺔﻘﻄﻨﻤﻟا
NFS
prefix
لا
NMP
stem
بﺎﺒﺷ
</figure>
<page confidence="0.984514">
147
</page>
<bodyText confidence="0.994258">
directions, language model, distortion model, and sen-
tence length penalty.
</bodyText>
<sectionHeader confidence="0.999563" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999995027027027">
The parallel corpus has 459K utterance pairs with 90K
words (50K morphemes). The Iraqi-Arabic language
model training data is slightly larger than the Iraqi-Arabic
side of the parallel corpus and it has 2.8M words with
98K unique lexical items. The morphologically analyzed
training data has 2.6M words with 58K unique vocabulary
items. A statistical trigram language model using Modi-
fied Knesser-Ney smoothing has been built for the mor-
phologically segmented data. The test data consists of
2242 utterances (3474 unique words). The OOV rate for
the unsegmented test data is 8.7%, the corresponding
number for the morphologically analyzed data is 7.4%.
Hence, morphological segmentation reduces the OOV
rate by 1.3% (15% relative), which is not as large reduc-
tion as compared to training data (about 40% relative re-
duction). We believe this would limit the potential
improvement we could get from JMLLM, since JMLLM
is expected to be more effective compared to word n-
gram models, when the OOV rate is significantly reduced
after segmentation.
We measure translation performance by the BLEU
score (Papineni et al, 2002) with one reference for each
hypothesis. In order to evaluate the performance of the
JMLLM, a translation N-best list (N=10) is generated
using the baseline Morpheme-trigram language model.
First, on a heldout development data all feature weights
including the language model weight are optimized to
maximize the BLEU score using the downhill simplex
method (Och and Hey, 2002). These weights are fixed
when the language models are used on the test data. The
translation BLEU (%) scores are given in Table 1. The
first entry (37.59) is the oracle BLEU score for the N-best
list. The baseline morpheme-trigram achieved 29.63,
word-trigram rescoring improved the BLEU score to
29.91. The JMLLM achieved 30.20 and log-linear inter-
polation with the morpheme-trigram improved the BLEU
score to 30.41.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999905777777778">
We presented a new language modeling technique called
Joint Morphological-Lexical Language Modeling
(JMLLM) for use in SMT. JMLLM allows joint modeling
of lexical, morphological and additional information
sources about morphological segments, lexical items
and sentence. The translation results demonstrate that
joint modeling provides encouraging improvement over
morpheme based language model. Our future work
will be directed towards tight integration of all available
</bodyText>
<tableCaption confidence="0.997099">
Table 1. SMT N-best list rescoring.
</tableCaption>
<table confidence="0.945981166666667">
LANGUAGE MODELS BLEU (%)
N-best Oracle 37.59
Morpheme-trigram 29.63
Word-trigram 29.91
JMLLM 30.20
JMLLM + Morpheme-Trigram 30.41
</table>
<bodyText confidence="0.9676765">
information by predicting the entire MLPT (besides
leaves).
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848975">
P. Brown er al.,. 1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263–311.
A. Berger, S. Della Pietra and V. Della Pietra, &amp;quot;A Maximum Entropy
Approach to Natural Language Processing,&amp;quot; Computational Lin-
guistics, vol. 22, no. 1, March 1996
T. Buckwalter. 2002. Buckwalter Arabic morphological analyzer
version 1.0, LDC2002L49 and ISBN 1-58563-257-0, 2002.
G. Choueiter, D. Povey, S.F. Chen, and G. Zweig, 2006. Morpheme-
based language modeling for Arabic LVCSR. ICASSP’06, Tou-
louse, France, 2006.
A. Ghaoui, F. Yvon, C. Mokbel, and G. Chollet, 2005. On the use of
morphological constraints in N-gram statistical language model,
Eurospeech’05, Lisbon, Portugal, 2005.
B. Huang and K. Knight. 2006. Relabeling Syntax Trees to Improve
Syntax-Based Machine Translation Quality. In HLT/NAACL.
B. Xiang, K. Nguyen, L. Nguyen, R. Schwartz, J. Makhoul, 2006.
Morphological decomposition for Arabic broadcast news tran-
scription”, ICASSP’06, Toulouse, France, 2006.
K. Kirchhoff and M. Yang. 2005. Improved language modeling for
statistical machine translation. In ACL’05 workshop on Building
and Using Parallel Text, pages 125–128.
P. Koehn, F. J. Och, and D. Marcu. 2004. Pharaoh: A beam search
decoder for phrase based statistical machine translation models. In
Proc. of 6th Conf. of AMTA.
F. J. Och and H. Ney. 2002. Discriminative training and maximum
entropy models for statistical machine translation. In ACL, pages
295–302, University of Pennsylvania.
F. J. Och and H. Ney. 2003. A Systematic Comparison of Various
Statistical Alignment Models. Comp. Linguistics, 29(1):9--51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a
Method for Automatic Evaluation of machine translation. In ACL
02, pages 311–318
H. Schwenk, D. D´echelotte and J-L. Gauvain. 2006. Continuous
space language models for statistical machine translation. In
ACL/COLING, pages 723–730.
M. Afify, R. Sarikaya, H-K. J. Kuo, L. Besacier and Y. Gao. 2006. On
the Use of Morphological Analysis for Dialectal Arabic Speech
Recognition, In Interspeech-2006, Pittsburgh PA.
R. Sarikaya, M .Afify and Y. Gao. 2007. Joint Morphological-Lexical
Modeling (JMLLM) for Arabic. ICASSP 2007, Honolulu Hawaii.
</reference>
<page confidence="0.996818">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649359">
<title confidence="0.999467">Joint Morphological-Lexical Language Modeling for Machine Translation</title>
<author confidence="0.983354">Ruhi Sarikaya Yonggang Deng</author>
<affiliation confidence="0.999181">IBM T.J. Watson Research</affiliation>
<address confidence="0.88718">Yorktown Heights, NY 10598</address>
<email confidence="0.997298">sarikaya@us.ibm.comydeng@us.ibm.com</email>
<abstract confidence="0.982896533333333">We present a joint morphological-lexical language model (JMLLM) for use in statistical machine translation (SMT) of language pairs where one or both of the languages are morphologically rich. The proposed JMLLM takes advantage of the rich morphology to reduce the Out-Of-Vocabulary (OOV) rate, while keeping the predictive power of the whole words. It also allows incorporation of additional available semantic, syntactic and linguistic information about the morphemes and words into the language model. Preliminary experiments with an English to Dialectal-Arabic SMT system demonstrate improved translation performance over trigram baseline language</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown er al</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>al, 1993</marker>
<rawString>P. Brown er al.,. 1993. The mathematics of statistical machine translation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing,&amp;quot;</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra and V. Della Pietra, &amp;quot;A Maximum Entropy Approach to Natural Language Processing,&amp;quot; Computational Linguistics, vol. 22, no. 1, March 1996</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Buckwalter Arabic morphological analyzer version 1.0,</title>
<date>2002</date>
<booktitle>LDC2002L49 and ISBN</booktitle>
<pages>1--58563</pages>
<contexts>
<context position="5425" citStr="Buckwalter, 2002" startWordPosition="849" endWordPosition="850">s reduces the n-gram language model span, and leads to poor performance for a fixed n-gram size. Therefore, we predefine a set of prefixes and suffixes and perform blind word segmentation. In order to minimize the illegitimate segmentations we employ the following algorithm. Using the given set of prefixes and suffixes, a word is first blindly chopped to one of the four segmentations mentioned above. This segmentation is accepted if the following three rules apply: (1) The resulting stem has more than two characters. (2) The resulting stem is accepted by the Buckwalter morphological analyzer (Buckwalter, 2002). (3) The resulting stem exists in the original dictionary. The first rule eliminates many of the illegitimate segmentations. The second rule ensures that the word is a valid Arabic stem, given that the Buckwalter morphological analyzer covers all words in the Arabic language. Unfortunately, the fact that the stem is a valid Arabic stem does not always imply that the segmentation is valid. The third rule, while still not offering such guarantee, simply prefers keeping the word intact if its stem does not occur in the lexicon. In our implementation we used the following set of prefixes and suff</context>
</contexts>
<marker>Buckwalter, 2002</marker>
<rawString>T. Buckwalter. 2002. Buckwalter Arabic morphological analyzer version 1.0, LDC2002L49 and ISBN 1-58563-257-0, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Choueiter</author>
<author>D Povey</author>
<author>S F Chen</author>
<author>G Zweig</author>
</authors>
<date>2006</date>
<booktitle>Morphemebased language modeling for Arabic LVCSR. ICASSP’06,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="2870" citStr="Choueiter et al., 2006" startWordPosition="455" endWordPosition="458">end of a stem to generate new words that indicate case, gender, tense etc. associated with the stem. Hence, it is natural that this leads to rapid vocabulary growth, which is accompanied by worse language model probability estimation due to data sparsity and high Out-Of-Vocabulary (OOV) rate. Due to rich morphology, one would suspect that words may not be the best lexical units for Arabic, and perhaps morphological units would be a better choice. Recently, there have been a number of new methods using the morphological units to represent lexical items (Ghaoui et al., 2005; Xiang et al., 2006; Choueiter et al., 2006). Factored Language Models (FLMs) (Kirchhoff and Yang, 2004) share the same idea to some extent but here words are decomposed into a number of features and the resulting representation is used in a generalized back-off scheme to improve the robustness of probability estimates for rarely observed word n-grams. In this study we propose a tree structure called Morphological-Lexical Parse Tree (MLPT) to combine the information provided by a morphological analyzer with the lexical information within a single Joint Morphological-Lexical Language Model (JMLLM). The MLPT allows us to include available</context>
</contexts>
<marker>Choueiter, Povey, Chen, Zweig, 2006</marker>
<rawString>G. Choueiter, D. Povey, S.F. Chen, and G. Zweig, 2006. Morphemebased language modeling for Arabic LVCSR. ICASSP’06, Toulouse, France, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghaoui</author>
<author>F Yvon</author>
<author>C Mokbel</author>
<author>G Chollet</author>
</authors>
<title>On the use of morphological constraints in N-gram statistical language model,</title>
<date>2005</date>
<location>Eurospeech’05, Lisbon, Portugal,</location>
<contexts>
<context position="2825" citStr="Ghaoui et al., 2005" startWordPosition="447" endWordPosition="450">affixes are appended to the beginning or end of a stem to generate new words that indicate case, gender, tense etc. associated with the stem. Hence, it is natural that this leads to rapid vocabulary growth, which is accompanied by worse language model probability estimation due to data sparsity and high Out-Of-Vocabulary (OOV) rate. Due to rich morphology, one would suspect that words may not be the best lexical units for Arabic, and perhaps morphological units would be a better choice. Recently, there have been a number of new methods using the morphological units to represent lexical items (Ghaoui et al., 2005; Xiang et al., 2006; Choueiter et al., 2006). Factored Language Models (FLMs) (Kirchhoff and Yang, 2004) share the same idea to some extent but here words are decomposed into a number of features and the resulting representation is used in a generalized back-off scheme to improve the robustness of probability estimates for rarely observed word n-grams. In this study we propose a tree structure called Morphological-Lexical Parse Tree (MLPT) to combine the information provided by a morphological analyzer with the lexical information within a single Joint Morphological-Lexical Language Model (JM</context>
</contexts>
<marker>Ghaoui, Yvon, Mokbel, Chollet, 2005</marker>
<rawString>A. Ghaoui, F. Yvon, C. Mokbel, and G. Chollet, 2005. On the use of morphological constraints in N-gram statistical language model, Eurospeech’05, Lisbon, Portugal, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Huang</author>
<author>K Knight</author>
</authors>
<title>Relabeling Syntax Trees to Improve Syntax-Based Machine Translation Quality.</title>
<date>2006</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="1229" citStr="Huang and Knight, 2006" startWordPosition="174" endWordPosition="177">emantic, syntactic and linguistic information about the morphemes and words into the language model. Preliminary experiments with an English to Dialectal-Arabic SMT system demonstrate improved translation performance over trigram based baseline language model. 1 Introduction Statistical machine translation (SMT) methods have evolved from using the simple word based models (Brown et al., 1993) to phrase based models (Marcu and Wong, 2002; Koehn et al., 2004; Och and Ney, 2004). More recently, there is a significant effort focusing on integrating richer knowledge, such as syntactic parse trees (Huang and Knight, 2006) within the translation process to overcome the limitations of the phrase based models. The SMT has been formulated as a noisy channel model in which the target language sentence, e is seen as distorted by the channel into the foreign language f : e = ˆ argmax ( |) argmax P e f = e e where P(f |e) is the translation model and P(e) is language model of the target language. The overwhelming proportion of the SMT research has been focusing on improving the translation model. Despite several new studies (Kirchhoff and Yang, 2004; Schwenk et al., 2006), language modeling for SMT has not been receiv</context>
</contexts>
<marker>Huang, Knight, 2006</marker>
<rawString>B. Huang and K. Knight. 2006. Relabeling Syntax Trees to Improve Syntax-Based Machine Translation Quality. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Xiang</author>
<author>K Nguyen</author>
<author>L Nguyen</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
</authors>
<title>Morphological decomposition for Arabic broadcast news transcription”, ICASSP’06,</title>
<date>2006</date>
<location>Toulouse, France,</location>
<contexts>
<context position="2845" citStr="Xiang et al., 2006" startWordPosition="451" endWordPosition="454">to the beginning or end of a stem to generate new words that indicate case, gender, tense etc. associated with the stem. Hence, it is natural that this leads to rapid vocabulary growth, which is accompanied by worse language model probability estimation due to data sparsity and high Out-Of-Vocabulary (OOV) rate. Due to rich morphology, one would suspect that words may not be the best lexical units for Arabic, and perhaps morphological units would be a better choice. Recently, there have been a number of new methods using the morphological units to represent lexical items (Ghaoui et al., 2005; Xiang et al., 2006; Choueiter et al., 2006). Factored Language Models (FLMs) (Kirchhoff and Yang, 2004) share the same idea to some extent but here words are decomposed into a number of features and the resulting representation is used in a generalized back-off scheme to improve the robustness of probability estimates for rarely observed word n-grams. In this study we propose a tree structure called Morphological-Lexical Parse Tree (MLPT) to combine the information provided by a morphological analyzer with the lexical information within a single Joint Morphological-Lexical Language Model (JMLLM). The MLPT allow</context>
</contexts>
<marker>Xiang, Nguyen, Nguyen, Schwartz, Makhoul, 2006</marker>
<rawString>B. Xiang, K. Nguyen, L. Nguyen, R. Schwartz, J. Makhoul, 2006. Morphological decomposition for Arabic broadcast news transcription”, ICASSP’06, Toulouse, France, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>M Yang</author>
</authors>
<title>Improved language modeling for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL’05 workshop on Building and Using Parallel Text,</booktitle>
<pages>125--128</pages>
<marker>Kirchhoff, Yang, 2005</marker>
<rawString>K. Kirchhoff and M. Yang. 2005. Improved language modeling for statistical machine translation. In ACL’05 workshop on Building and Using Parallel Text, pages 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. of 6th Conf. of AMTA.</booktitle>
<contexts>
<context position="1066" citStr="Koehn et al., 2004" startWordPosition="149" endWordPosition="152">logy to reduce the Out-Of-Vocabulary (OOV) rate, while keeping the predictive power of the whole words. It also allows incorporation of additional available semantic, syntactic and linguistic information about the morphemes and words into the language model. Preliminary experiments with an English to Dialectal-Arabic SMT system demonstrate improved translation performance over trigram based baseline language model. 1 Introduction Statistical machine translation (SMT) methods have evolved from using the simple word based models (Brown et al., 1993) to phrase based models (Marcu and Wong, 2002; Koehn et al., 2004; Och and Ney, 2004). More recently, there is a significant effort focusing on integrating richer knowledge, such as syntactic parse trees (Huang and Knight, 2006) within the translation process to overcome the limitations of the phrase based models. The SMT has been formulated as a noisy channel model in which the target language sentence, e is seen as distorted by the channel into the foreign language f : e = ˆ argmax ( |) argmax P e f = e e where P(f |e) is the translation model and P(e) is language model of the target language. The overwhelming proportion of the SMT research has been focus</context>
<context position="14475" citStr="Koehn et al, 2004" startWordPosition="2338" endWordPosition="2341">ections, from English to Iraqi Arabic and from Iraqi Arabic to English, and derived two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments was then formed. All phrase pairs which respect to the word alignment boundary constraint were identified and pooled together to build phrase translation tables with the Maximum Likelihood criterion. The maximum number of words in Arabic phrases was set to 5. Our decoder is the phrase-based multi-stack implementation of log-linear models similar to Pharaoh (Koehn et al, 2004). Like most other MaxEnt-based decoders, active features in our decoder include translation models in two directions, lexicon weights in two stem ةﻮﮭﻗ NFS ةﻮﮭﻘﻟﺎﺑ prefix لﺎﺑ suffix نو VFP نوﺪﻌﻘﯾ stem ﺪﻌﻗ prefix !S! ي stem ﺔﻘﻄﻨﻣ ﺔﻘﻄﻨﻤﻟا NFS prefix لا NMP stem بﺎﺒﺷ 147 directions, language model, distortion model, and sentence length penalty. 6 Experiments The parallel corpus has 459K utterance pairs with 90K words (50K morphemes). The Iraqi-Arabic language model training data is slightly larger than the Iraqi-Arabic side of the parallel corpus and it has 2.8M words with 98K unique lexical items</context>
</contexts>
<marker>Koehn, Och, Marcu, 2004</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2004. Pharaoh: A beam search decoder for phrase based statistical machine translation models. In Proc. of 6th Conf. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>295--302</pages>
<institution>University of Pennsylvania.</institution>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL, pages 295–302, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Comp. Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14056" citStr="Och and Ney, 2003" startWordPosition="2272" endWordPosition="2275">core for a given morpheme using JMLLM is conditioned not only on the previous morphemes but also on their attributes, and the lexical items and their attributes. As such, the language model scores are smoother compared to n-gram models especially for unseen lexical items. 5 Statistical Machine Translation System Starting from a collection of parallel sentences, we trained word alignment models in two translation directions, from English to Iraqi Arabic and from Iraqi Arabic to English, and derived two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments was then formed. All phrase pairs which respect to the word alignment boundary constraint were identified and pooled together to build phrase translation tables with the Maximum Likelihood criterion. The maximum number of words in Arabic phrases was set to 5. Our decoder is the phrase-based multi-stack implementation of log-linear models similar to Pharaoh (Koehn et al, 2004). Like most other MaxEnt-based decoders, active features in our decoder include translation models in two directions, lexicon weights in two stem ةﻮﮭﻗ NFS ةﻮﮭﻘﻟﺎﺑ prefix لﺎﺑ suffix </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Comp. Linguistics, 29(1):9--51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL 02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="15950" citStr="Papineni et al, 2002" startWordPosition="2573" endWordPosition="2576">utterances (3474 unique words). The OOV rate for the unsegmented test data is 8.7%, the corresponding number for the morphologically analyzed data is 7.4%. Hence, morphological segmentation reduces the OOV rate by 1.3% (15% relative), which is not as large reduction as compared to training data (about 40% relative reduction). We believe this would limit the potential improvement we could get from JMLLM, since JMLLM is expected to be more effective compared to word ngram models, when the OOV rate is significantly reduced after segmentation. We measure translation performance by the BLEU score (Papineni et al, 2002) with one reference for each hypothesis. In order to evaluate the performance of the JMLLM, a translation N-best list (N=10) is generated using the baseline Morpheme-trigram language model. First, on a heldout development data all feature weights including the language model weight are optimized to maximize the BLEU score using the downhill simplex method (Och and Hey, 2002). These weights are fixed when the language models are used on the test data. The translation BLEU (%) scores are given in Table 1. The first entry (37.59) is the oracle BLEU score for the N-best list. The baseline morpheme</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of machine translation. In ACL 02, pages 311–318</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>D D´echelotte</author>
<author>J-L Gauvain</author>
</authors>
<title>Continuous space language models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL/COLING,</booktitle>
<pages>723--730</pages>
<marker>Schwenk, D´echelotte, Gauvain, 2006</marker>
<rawString>H. Schwenk, D. D´echelotte and J-L. Gauvain. 2006. Continuous space language models for statistical machine translation. In ACL/COLING, pages 723–730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Afify</author>
<author>R Sarikaya</author>
<author>H-K J Kuo</author>
<author>L Besacier</author>
<author>Y Gao</author>
</authors>
<title>On the Use of Morphological Analysis for Dialectal Arabic Speech Recognition,</title>
<date>2006</date>
<booktitle>In Interspeech-2006,</booktitle>
<location>Pittsburgh PA.</location>
<marker>Afify, Sarikaya, Kuo, Besacier, Gao, 2006</marker>
<rawString>M. Afify, R. Sarikaya, H-K. J. Kuo, L. Besacier and Y. Gao. 2006. On the Use of Morphological Analysis for Dialectal Arabic Speech Recognition, In Interspeech-2006, Pittsburgh PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sarikaya</author>
<author>M Afify</author>
<author>Y Gao</author>
</authors>
<date>2007</date>
<booktitle>Joint Morphological-Lexical Modeling (JMLLM) for Arabic. ICASSP</booktitle>
<location>Honolulu Hawaii.</location>
<contexts>
<context position="9692" citStr="Sarikaya et al., 2007" startWordPosition="1567" endWordPosition="1570">s a noun (N), male (M), plural (P). Using the information represented in MLPT for Arabic language modeling provides for smooth probability estimation even for those words that are not seen before. The JMLLM integrates the local morpheme and word n-grams, morphological dependencies and attribute information associated with morphological segments and words, which are all represented in the MLPT using the MaxEnt fr may be “other”(O). بﺎﺒﺷ, a back-off amework. We trained JMLLM for IraqiArabic text is written (read) from ri 2In ght-to-left. e E i fi i (o&apos;, h) e 146 Arabic speech recognition task (Sarikaya et al., 2007), and obtained significant improvements over word and morpheme based trigram language models. We can construct a single probability model that models the joint probability of all of the available information sources in the MLPT. To compute the joint probability of the morpheme sequence and its MLPT, we use features extracted from MLPT. Even though the framework is generic to jointly represent the information sources in the MLPT, in this study we limit ourselves to using only lexical and morphological content of the sentence, along with the morphological attributes simply because the lexical at</context>
</contexts>
<marker>Sarikaya, Afify, Gao, 2007</marker>
<rawString>R. Sarikaya, M .Afify and Y. Gao. 2007. Joint Morphological-Lexical Modeling (JMLLM) for Arabic. ICASSP 2007, Honolulu Hawaii.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>