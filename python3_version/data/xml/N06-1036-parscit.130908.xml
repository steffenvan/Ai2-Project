<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007366">
<title confidence="0.999189">
Backoff Model Training using Partially Observed Data:
Application to Dialog Act Tagging
</title>
<author confidence="0.99288">
Gang Ji and Jeff Bilmes
</author>
<affiliation confidence="0.995166">
Department of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.942398">
Seattle, WA 98105-2500
</address>
<email confidence="0.999784">
{gang,bilmes}@ee.washington.edu
</email>
<sectionHeader confidence="0.996674" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985708333333">
Dialog act (DA) tags are useful for many
applications in natural language process-
ing and automatic speech recognition. In
this work, we introduce hidden backoff
models (HBMs) where a large generalized
backoff model is trained, using an embed-
ded expectation-maximization (EM) pro-
cedure, on data that is partially observed.
We use HBMs as word models condi-
tioned on both DAs and (hidden) DA-
segments. Experimental results on the
ICSI meeting recorder dialog act corpus
show that our procedure can strictly in-
crease likelihood on training data and can
effectively reduce errors on test data. In
the best case, test error can be reduced by
6.1% relative to our baseline, an improve-
ment on previously reported models that
also use prosody. We also compare with
our own prosody-based model, and show
that our HBM is competitive even without
the use of prosody. We have not yet suc-
ceeded, however, in combining the bene-
fits of both prosody and the HBM.
</bodyText>
<sectionHeader confidence="0.999153" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889538461539">
Discourse patterns in natural conversations and
meetings are well known to provide interesting and
useful information about human conversational be-
havior. They thus attract research from many differ-
ent and beneficial perspectives. Dialog acts (DAs)
(Searle, 1969), which reflect the functions that ut-
terances serve in a discourse, are one type of such
patterns. Detecting and understanding dialog act
patterns can provide benefit to systems such as au-
tomatic speech recognition (ASR) (Stolcke et al.,
1998), machine dialog translation (Lee et al., 1998),
and general natural language processing (NLP) (Ju-
rafsky et al., 1997b; He and Young, 2003). DA pat-
tern recognition is an instance of “tagging.” Many
different techniques have been quite successful in
this endeavor, including hidden Markov models (Ju-
rafsky et al., 1997a; Stolcke et al., 1998), seman-
tic classification trees and polygrams (Mast et al.,
1996), maximum entropy models (Ang et al., 2005),
and other language models (Reithinger et al., 1996;
Reithinger and Klesen, 1997). Like other tagging
tasks, DA recognition can also be achieved using
conditional random fields (Lafferty et al., 2001; Sut-
ton et al., 2004) and general discriminative model-
ing on structured outputs (Bartlett et al., 2004). In
many sequential data analysis tasks (speech, lan-
guage, or DNA sequence analysis), standard dy-
namic Bayesian networks (DBNs) (Murphy, 2002)
have shown great flexibility and are widely used. In
(Ji and Bilmes, 2005), for example, an analysis of
DA tagging using DBNs is performed, where the
models avoid label bias by structural changes and
avoid data sparseness by using a generalized back-
off procedures (Bilmes and Kirchhoff, 2003).
Most DA classification procedures assume that
within a sentence of a particular fixed DA type,
there is a fixed word distribution over the entire sen-
tence. Similar to (Ma et al., 2000) (and see cita-
tions therein), we have found, however, that intra-
</bodyText>
<page confidence="0.951453">
280
</page>
<note confidence="0.9952445">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 280–287,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999467358974359">
sentence discourse patterns are inherently dynamic.
Moreover, the patterns are specific to each type of
DA, meaning a sentence will go through a DA-
specific sequence of sub-DA phases or “states.” A
generative description of this phenomena is that a
DA is first chosen, and then words are generated
according to both the DA and to the relative posi-
tion of the word in that sentence. For example, a
“statement” (one type of DA) can consist of a sub-
ject (noun phrase), verb phrase, and object (noun
phrase). This particular sequence might be different
for a different DA (e.g., a “back-channel”). Our be-
lief is that explicitly modeling these internal states
can help a DA-classification system in conversa-
tional meetings or dialogs.
In this work, we describe an approach that is
motivated by several aspects of the typical DA-
classification procedure. First, it is rare to have sub-
DAs labeled in training data, and indeed this is true
of the corpus (Shriberg et al., 2004) that we use.
Therefore, some form of unsupervised clustering or
pre-shallow-parsing of sub-DAs must be performed.
In such a model, these sub-DAs are essentially un-
known hidden variables that ideally could be trained
with an expectation-maximization (EM) procedure.
Second, when training models of language, it is nec-
essary to employ some form of smoothing method-
ology since otherwise data-sparseness would render
standard maximum-likelihood trained models use-
less. Third, discrete conditional probability distri-
butions formed using backoff models that have been
smoothed (particularly using modified Kneser-Ney
(Chen and Goodman, 1998)) have been extremely
successful in many language modeling tasks. Train-
ing backoff models, however, requires that all data
is observed so that data counts can be formed. In-
deed, our DA-specific word models (implemented
via backoff) will also need to condition on the cur-
rent sub-DA, which at training time is unknown.
We therefore have developed a procedure that al-
lows us to train generalized backoff models (Bilmes
and Kirchhoff, 2003), even when some or all of the
variables involved in the model are hidden. We thus
call our models hidden backoff models (HBMs). Our
method is indeed a form of embedded EM training
(Morgan and Bourlard, 1990), and more generally
is a specific form of EM (Neal and Hinton, 1998).
Our approach is similar to (Ma et al., 2000), except
our underlying language models are backoff-based
and thus retain the benefits of advanced smoothing
methods, and we utilize both a normal and a backoff
EM step as will be seen. We moreover wrap up the
above ideas in the framework of dynamic Bayesian
networks, which are used to represent and train all
of our models.
We evaluate our methods on the ICSI meeting
recorder dialog act (MRDA) (Shriberg et al., 2004)
corpus, and find that our novel hidden backoff model
can significantly improve dialog tagging accuracy.
With a different number of hidden states for each
DA, a relative reduction in tagging error rate as
much as 6.1% can be achieved. Our best HBM result
shows an accuracy that improves on the best known
(to our knowledge) result on this corpora which is
one that uses acoustic prosody as a feature. We have
moreover developed our own prosody model and
while we have not been able to usefully employ both
prosody and the HBM technique together, our HBM
is competitive in this case as well. Furthermore, our
results show the effectiveness of our embedded EM
procedure, as we demonstrate that it increases train-
ing log likelihoods, while simultaneously reducing
error rate.
Section 2 briefly summarizes our baseline DBN-
based models for DA tagging tasks. In Section 3,
we introduce our HBMs. Section 4 contains experi-
mental evaluations on the MRDA corpus and finally
Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.974492" genericHeader="method">
2 DBN-based Models for Tagging
</sectionHeader>
<bodyText confidence="0.997043933333333">
Dynamic Bayesian networks (DBNs) (Murphy,
2002) are widely used in sequential data analysis
such as automatic speech recognition (ASR) and
DNA sequencing analysis (Durbin et al., 1999). A
hidden Markov model (HMM) for DA tagging as in
(Stolcke et al., 1998) is one such instance.
Figure 1 shows a generative DBN model that will
be taken as our baseline. This DBN shows a pro-
logue (the first time slice of the model), an epilogue
(the last slice), and a chunk that is repeated suffi-
ciently to fit the entire data stream. In this case,
the data stream consists of the words of a meet-
ing conversation, where individuals within the meet-
ing (hopefully) take turns speaking. In our model,
the entire meeting conversation, and all turns of all
</bodyText>
<page confidence="0.998336">
281
</page>
<figureCaption confidence="0.999336">
Figure 1: Baseline generative DBN for DA tagging.
</figureCaption>
<bodyText confidence="0.999919307692308">
speakers, are strung together into a single stream
rather than treating each turn in the meeting indi-
vidually. This approach has the benefit that we are
able to integrate a temporal DA-to-DA model (such
as a DA bigram).
In all our models, to simplify we assume that the
sentence change information is known (as is com-
mon with this corpus (Shriberg et al., 2004)). We
next describe Figure 1 in detail. Normally, the sen-
tence change variable is not set, so that we are within
a sentence (or a particular DA). When a sentence
change does not occur, the DA stays the same from
slice to slice. During this time, we use a DA-specific
language model (implemented via a backoff strat-
egy) to score the words within the current DA.
When a sentence change event does occur, a new
DA is predicted based on the DA from the previous
sentence (using a DA bigram). At the beginning of
a sentence, rather than conditioning on the last word
of the previous sentence, we condition on the special
start of sentence &lt;s&gt; token, as shown in the figure
by having a special parent that is used only when
sentence change is true. Lastly, at the very beginning
of a meeting, a special start of DA token is used.
The joint probability under this baseline model is
written as follows:
</bodyText>
<equation confidence="0.995903">
P (W, D) = H P(dk|dk−1)
·ri
k (1)
P(wk,i|wk,i−1, dk),
</equation>
<bodyText confidence="0.999976166666667">
where W = {wk,i} is the word sequence, D = {dk}
is the DA sequence, dk is the DA of the k-th sen-
tence, and wk,i is the i-th word of the k-th sentence
in the meeting.
Because all variables are observed when training
our baseline, we use the SRILM toolkit (Stolcke,
2002), modified Kneser-Ney smoothing (Chen and
Goodman, 1998), and factored extensions (Bilmes
and Kirchhoff, 2003). In evaluations, the Viterbi al-
gorithm (Viterbi, 1967) can be used to find the best
DA sequence path from the words of the meeting
according to the joint distribution in Equation (1).
</bodyText>
<sectionHeader confidence="0.980625" genericHeader="method">
3 Hidden Backoff Models
</sectionHeader>
<bodyText confidence="0.976567210526316">
When analyzing discourse patterns, it can be seen
that sentences with different DAs usually have dif-
ferent internal structures. Accordingly, in this work
we do not assume sentences for each dialog act have
the same hidden state patterns. For instance (and as
mentioned above), a statement can consist of a noun
followed by a verb phase.
A problem, however, is that sub-DAs are not an-
notated in our training corpus. While clustering and
annotation of these phrases is already a widely de-
veloped research topic (Pieraccini and Levin, 1991;
Lee et al., 1997; Gildea and Jurafsky, 2002), in our
approach we use an EM algorithm to learn these hid-
den sub-DAs in a data-driven fashion. Pictorially,
we add a layer of hidden states to our baseline DBN
as illustrated in Figure 2.
prologue chunk epilogue
Figure 2: Hidden backoff model for DA tagging.
Under this model, the joint probability is:
</bodyText>
<equation confidence="0.995834333333333">
P (W, �, D) = H P(dk|dk−1)
k
·ri [P(sk,i|sk,i−1, dk) (2)
</equation>
<page confidence="0.415504">
· P(wk,i|wk,i−1, sk,i, dk)I ,
</page>
<figure confidence="0.967036357142857">
dialog act
DA &lt;s&gt;
sentence change
word &lt;s&gt;
word
prologue
chunk
epilogue
dialog act
DA &lt;s&gt;
hidden state
sentence change
word &lt;s&gt;
word
</figure>
<page confidence="0.986992">
282
</page>
<bodyText confidence="0.9998476">
where 5 = {sk,i} is the hidden state sequence, sk,i
is the hidden state at the i-th position of the k-th sen-
tence, and other variables are the same as before.
Similar to our baseline model, the DA bigram
P(dk|dk−1) can be modeled using a backoff bi-
gram. Moreover, if the states {sk,i} are known
during training, the word prediction probability
P(wk,i|wk,i−1, sk,i, dk) can also use backoff and be
trained accordingly. The hidden state sequence is
unknown, however, and thus cannot be used to pro-
duce a standard backoff model. What we desire is
an ability to utilize a backoff model (to mitigate data
sparseness effects) while simultaneously retaining
the state as a hidden (rather than an observed) vari-
able, and also have a procedure that trains the entire
model to improve overall model likelihood.
Expectation-maximization (EM) algorithms are
well-known to be able to train models with hidden
states. Furthermore, standard advanced smoothing
methods such as modified Kneser-Ney smoothing
(Chen and Goodman, 1998) utilize integer counts
(rather than fractional ones), and they moreover
need “meta” counts (or counts of counts). There-
fore, in order to train this model, we propose an
embedded training algorithm that cycles between a
standard EM training procedure (to train the hidden
state distribution), and a stage where the most likely
hidden states (and their counts and meta counts) are
used externally to train a backoff model. This pro-
cedure can be described in detail as follows:
</bodyText>
<figure confidence="0.928352363636364">
Input : W — meeting word sequence
Input : D — DA sequence
Output : P(sk,i|sk,i−1) - state transition CPT
Output: P(wk,i|wk,i−1, sk,i, dk) -word model
1 randomly generate a sequence 5;
2 backoff train P(wk,i|wk,i−1, sk,i, dk);
3 while not “converged” do
EM train P(sk,i|sk,i−1);
calculate best 5 sequence by Viterbi;
backoff train P(wk,i|wk,i−1, �sk,i, dk);
7 end
</figure>
<figureCaption confidence="0.798091">
Algorithm 1: Embedded training for HBMs
</figureCaption>
<bodyText confidence="0.999971178571428">
In the algorithm, the input contains words and
a DA for each sentence in the meeting. The out-
put is the corresponding conditional probability ta-
ble (CPT) for hidden state transitions, and a back-
off model for word prediction. Because we train the
backoff model when some of the variables are hid-
den, we call the result a hidden backoff model. While
we have seen embedded Viterbi training used in the
past for simultaneously training heterogeneous mod-
els (e.g., Markov chains and Neural Networks (Mor-
gan and Bourlard, 1990)), this is the first instance
of training backoff-models that involve hidden vari-
ables that we are aware of.
While embedded Viterbi estimation is not guar-
anteed to have the same convergence (or fixed-point
under convergence) as normal EM (Lember and
Koloydenko, 2004), we find empirically this to be
the case (see examples below). Moreover, our algo-
rithm can easily be modified so that instead of tak-
ing a Viterbi alignment in step 5, we instead use a
set of random samples generated under the current
model. In this case, it can be shown using a law-of-
large numbers argument that having sufficient sam-
ples guarantees the algorithm will converge (we will
investigate this modification in future work).
Of course, when decoding with such a model, a
conventional Viterbi algorithm can still be used to
calculate the best DA sequence.
</bodyText>
<sectionHeader confidence="0.998578" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.995715181818182">
We evaluated our hidden backoff model on the
ICSI meeting recorder dialog act (MRDA) corpus
(Shriberg et al., 2004). MRDA is a rich data set that
contains 75 natural meetings on different topics with
each meeting involving about 6 participants. DA an-
notations from ICSI were based on a previous ap-
proach in (Jurafsky et al., 1997b) with some adapta-
tion for meetings in a number of ways described in
(Bhagat et al., 2003). Each DA contains a main tag,
several optional special tags and an optional “disrup-
tion” form. The total number of distinct DAs in the
corpus is as large as 1260. In order to make the prob-
lem comparable to other work (Ang et al., 2005), a
DA tag sub-set is used in our experiments that con-
tains back channels (b), place holders (h), questions
(q), statements (s), and disruptions (x). In our eval-
uations, among the entire 75 conversations, 51 are
used as the training set, 11 are used as the develop-
ment set, 11 are used as test set, and the remaining
3 are not used. For each experiment, we used a ge-
netic algorithm to search for the best factored lan-
guage model structure on the development set and
</bodyText>
<figure confidence="0.878271333333333">
4
5
6
</figure>
<page confidence="0.996115">
283
</page>
<bodyText confidence="0.9998147">
we report the best results.
Our baseline system is the generative model
shown in Figure 1 and uses a backoff implementa-
tion of the word model, and is optimized on the de-
velopment set. We use the SRILM toolkit with ex-
tensions (Bilmes and Kirchhoff, 2003) to train, and
use GMTK (Bilmes and Zweig, 2002) for decoding.
Our baseline system has an error rate of 19.7% on
the test set, which is comparable to other approaches
on the same task (Ang et al., 2005).
</bodyText>
<subsectionHeader confidence="0.999779">
4.1 Same number of states for all DAs
</subsectionHeader>
<bodyText confidence="0.9916415">
To compare against our baseline, we use HBMs in
the model shown in Figure 2. To train, we followed
Algorithm 1 as described before and as is here de-
tailed in Figure 3.
</bodyText>
<table confidence="0.485114444444444">
Initialization:
- randomize states
- train word FLM
Yes
No
3-epoch
EM training
- find best state path
- train word FLM
</table>
<figureCaption confidence="0.99759">
Figure 3: Embedded training: llh = log likelihood
</figureCaption>
<bodyText confidence="0.999930194444445">
In this implementation, an upper triangular ma-
trix (with self-transitions along the diagonal) is used
for the hidden state transition probability table so
that sub-DA states only propagate in one direction.
When initializing the hidden state sequence of a DA,
we expanded the states uniformly along the sen-
tence. This initial alignment is then used for HBM
training. In the word models used in our experi-
ments, the backoff path first drops previous words,
then does a parallel backoff to hidden state and DA
using a mean combination strategy.
The HBM thus obtained was then fed into the
main loop of our embedded EM algorithm. The
training was considered to have “converged” if ei-
ther it exceeded 10 iterations (which never hap-
pened) or the relative log likelihood change was less
than 0.2%. Within each embedded iteration, three
EM epochs were used. After each EM iteration,
a Viterbi alignment was performed thus obtaining
what we expect to be a better hidden state alignment.
This updated alignment, was then used to train a
new HBM. The newly generated model was then fed
back into the embedded training loop until it con-
verged. After the procedure met our convergence
criteria, an additional five EM epochs were carried
out in order to provide a good hidden state transi-
tion probability table. Finally, after Viterbi align-
ment and text generation was performed, the word
HBM was trained from the best state sequence.
To evaluate our hidden backoff model, the Viterbi
algorithm was used to find the best DA sequence ac-
cording to test data, and the tagging error rates were
calculated. In our first experiment, an equal num-
ber of hidden states for all DAs were used in each
model. The effect of this number on the accuracy of
DA tagging is shown in Table 1.
</bodyText>
<tableCaption confidence="0.999436">
Table 1: HBMs, different numbers of hidden states.
</tableCaption>
<table confidence="0.966279">
# states error improvement
baseline 19.7% –
2-state 18.7% 5.1%
3-state 19.5% 1.0%
</table>
<bodyText confidence="0.999240666666667">
For the baseline system, the backoff path first
drops dialog act, and for the HBMs, all backoff
paths drop hidden state first and drop DA sec-
ond. From Table 1 we see that with two hidden states
for every DA the system can reduce the tagging error
rate by more than 5% relative. As a comparison, in
(Ang et al., 2005), where conditional maximum en-
tropy models (which are conditionally trained) are
used, the error rate is 18.8% when using both word
and acoustic prosody features, and and 20.5% with-
out prosody. When the number of hidden states in-
creases to 3, the improvement decreases even though
it is still (very slightly) better than the baseline. We
believe the reasons are as follows: First, assuming
different DAs have the same number of hidden states
may not be appropriate. For example, back chan-
nels usually have shorter sentences and are constant
in discourse pattern over a DA. On the other hand,
</bodyText>
<figure confidence="0.959701571428572">
Convergence:
- llh change &lt; 0.2%
- or 10 iterations
5-epoch
EM training
- find best state path
- train word FLM
</figure>
<page confidence="0.994948">
284
</page>
<bodyText confidence="0.9998539">
questions and statements typically have longer, and
more complex, discourse structures. Second, even
under the same DA, the structure and inherent length
of sentence can vary. For example, “yes” can also be
a statement even though it has only one word. There-
fore, one-word statements need completely differ-
ent hidden state patterns than those in subject-verb-
object like statements — having one monolithic 3-
state model for statements might be inappropriate.
This issue is discussed further in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.991242">
4.2 Different states for different DAs
</subsectionHeader>
<bodyText confidence="0.999879285714286">
In order to mitigate the first problem described
above, we allow different numbers of hidden states
for each DA. This, however, leads to a combinato-
rial explosion of possibilities if done in a naive fash-
ion. Therefore, we attempted only a small number
of combinations based on the statistics of numbers
of words in each DA given in Table 2.
</bodyText>
<tableCaption confidence="0.995994">
Table 2: Length statistics of different DAs.
</tableCaption>
<table confidence="0.996055833333333">
DA mean median std p
(b) 1.0423 1 0.2361 0.4954
(h) 1.3145 1 0.7759 0.4660
(q) 6.5032 5 6.3323 0.3377
(s) 8.6011 7 7.8380 0.3013
(x) 1.7201 1 1.1308 0.4257
</table>
<bodyText confidence="0.995049210526316">
Table 2 shows the mean and median number of
words per sentence for each DA as well as the stan-
dard deviation. Also, the last column provides the p
value according to fitting the length histogram to a
geometric distribution (1 − p)p. As we expected,
back channels (b) and place holders (h) tend to have
shorter sentences while questions (q) and statements
(s) have longer ones. From this analysis, we use
fewer states for (b) and (h) and more states for (q)
and (s). For disruptions (x), the standard deviation of
number of words histogram is relatively high com-
pared with (b) and (h), so we also used more hidden
states in this case. In our experimental results below,
we used one state for (b) and (h), and various num-
bers of hidden states for other DAs. Tagging error
rates are shown in Table 3.
From Table 3, we see that using different num-
bers of hidden states for different DAs can produce
better models. Among all the experiments we per-
</bodyText>
<tableCaption confidence="0.99794">
Table 3: Number of hidden states for different DAs.
</tableCaption>
<table confidence="0.9963045">
b h q s x error improvement
1 1 4 4 1 18.9% 4.1%
1 1 3 3 2 18.9% 4.1%
1 1 2 2 2 18.7% 5.1%
1 1 3 2 2 18.6% 5.6%
1 1 3 2 2 18.5% 6.1%
</table>
<bodyText confidence="0.9934445">
formed, the best case is given by three states for (q),
two states for (s) and (x), and one state for (b) and
(h). This combination gives 6.1% relative reduction
of error rate from the baseline.
</bodyText>
<subsectionHeader confidence="0.999322">
4.3 Effect of embedded EM training
</subsectionHeader>
<bodyText confidence="0.970134266666667">
Incorporating backoff smoothing procedures into
Bayesian networks (and hidden variable training in
particular) can show benefits for any data domain
where smoothing is necessary. To understand the
properties of our algorithm a bit better, after each
training iteration using a partially trained model, we
calculated both the log likelihood of the training set
and the tagging error rate of the test data. Figure 4
shows these results using the best configuration from
the previous section (three states for (q), two for
(s)/(x) and one for (b)/(h)). This example is typical
of the convergence we see of Algorithm 1, which
empirically suggests that our procedure may be sim-
ilar to a generalized EM (Neal and Hinton, 1998).
iterations
</bodyText>
<figureCaption confidence="0.990416">
Figure 4: Embedded EM training performance.
</figureCaption>
<bodyText confidence="0.997566333333333">
We find that the log likelihood after each EM
training is strictly increasing, suggesting that our
embedded EM algorithm for hidden backoff models
</bodyText>
<figure confidence="0.954735588235294">
24
6
x −1.05
23
llh
error rate (y)
baseline
error rate
19
18 −1.2
1 2 3 4 5 6 7
−1.1
log likelihood
−1.15
22
21
20
</figure>
<page confidence="0.997134">
285
</page>
<bodyText confidence="0.999974133333333">
is improving the overall joint likelihood of the train-
ing data according to the model. This strict increase
of likelihood combined with the fact that Viterbi
training does not have the same theoretical conver-
gence guarantees as does normal EM indicates that
more detailed theoretical analysis of this algorithm
used with these particular models is desirable.
From the figure we also see that both the log
likelihood and tagging error rate “converge” af-
ter around four iterations of embedded training.
This quick convergence indicates that our embedded
training procedure is effective. The leveling of the
error rates after several iterations shows that model
over-fitting appears not to be an issue presumably
due to the smoothed embedded backoff models.
</bodyText>
<subsectionHeader confidence="0.990929">
4.4 Discussion and Error Analysis
</subsectionHeader>
<bodyText confidence="0.99985672">
A large portion of our tagging errors are due to con-
fusing the DA of short sentences such as “yeah”, and
“right”. The sentence, “yeah” can either be a back
channel or an affirmative statement. There are also
cases where “yeah?” is a question. These types of
confusions are difficult to remove in the prosody-
less framework but there are several possibilities.
First, we can allow the use of a “fork and join” tran-
sition matrix, where we fork to each DA-specific
condition (e.g., short or long) and join thereafter.
Alternatively, hidden Markov chain structuring al-
gorithms or context (i.e., conditioning the number
of sub-DAs on the previous DA) might be helpful.
Finding a proper number of hidden states for each
DA is also challenging. In our preliminary work, we
simply explored different combinations using sim-
ple statistics of the data. A systematic procedure
would be more beneficial. In this work, we also
did not perform any hidden state tying within dif-
ferent DAs. In practice, some states in statements
should be able to be beneficially tied with other
states within questions. Our results show that having
three states for all DAs is not as good as two states
for all. But with tying, more states might be more
successfully used.
</bodyText>
<subsectionHeader confidence="0.984499">
4.5 Influence of Prosody Cues
</subsectionHeader>
<bodyText confidence="0.999772777777778">
It has been shown that prosody cues provide use-
ful information in DA tagging tasks (Shriberg et
al., 1998; Ang et al., 2005). We also incorporated
prosody features in our models. We used ESPS
get f0 based on RAPT algorithm (Talkin, 1995) to
get Fo values. For each speaker, mean and variance
normalization is performed. For each word, a linear
regression is carried on the normalized Fo values.
We quantize the slope values into 20 bins and treat
those as prosody features associated with each word.
After adding the prosody features, the simple gener-
ative model as shown in Figure 5 gives 18.4% error
rate, which is 6.6% improvement over our baseline.
There is no statistical difference between the best
performance of this prosody model and the earlier
best HBM. This implies that the HBM can obtain
as good performance as a prosody-based model but
without using prosody.
</bodyText>
<figureCaption confidence="0.974648">
Figure 5: Generative prosody model for DA tagging.
</figureCaption>
<bodyText confidence="0.9999315">
The next obvious step is to combine an HBM with
the prosody information. Strangely, even after ex-
perimenting with many different models (including
ones where prosody depends on DA; prosody de-
pends on DA and the hidden state; prosody depends
on DA, hidden state, and word; and many varia-
tions thereof), we were unsuccessful in obtaining
a complementary benefit when using both prosody
and an HBM. One hypothesis is that our prosody
features are at the word-level (rather than at the DA
level). Another problem might be the small size of
the MRDA corpus relative to the model complexity.
Yet a third hypothesis is that the errors corrected by
both methods are the same — indeed, we have ver-
ified that the corrected errors overlap by more than
50%. We plan further investigations in future work.
</bodyText>
<figure confidence="0.998725888888889">
sentence change
word &lt;s&gt;
dialog act
DA &lt;s&gt;
prosody
word
prologue
chunk
epilogue
</figure>
<page confidence="0.994508">
286
</page>
<sectionHeader confidence="0.999546" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999957619047619">
In this work, we introduced a training method for
hidden backoff models (HBMs) to solve a problem
in DA tagging where smoothed backoff models in-
volving training-time hidden variables are useful.
We tested this procedure in the context of dynamic
Bayesian networks. Different hidden states were
used to model different positions in a DA. According
to empirical evaluations, our embedded EM algo-
rithm effectively increases log likelihood on training
data and reduces DA tagging error rate on test data.
If different numbers of hidden states are used for dif-
ferent DAs, we find that our prosody-independent
HBM reduces the tagging error rate by 6.1% rela-
tive to the baseline, a result that improves upon pre-
viously reported work that uses prosody, and that is
comparable to our own new result that also incorpo-
rates prosody. We have not yet been able to combine
the benefits of both an HBM and prosody informa-
tion. This material is based upon work supported
by the National Science Foundation under Grant No.
IIS-0121396.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99940978021978">
J. Ang et al. 2005. Automatic dialog act segmentation and
classification in multiparty meetings. In ICASSP.
P. Bartlett et al. 2004. Exponentiated gradient algorithms for
large-margin structured classification. In NIPS.
S. Bhagat et al. 2003. Labeling guide for dialog act tags in the
meeting recordering meetings. Technical Report 2, Interna-
tional Computer Science Insititute.
J. Bilmes and K. Kirchhoff. 2003. Factored language mod-
els and generalized parallel backoff. In Human Lang. Tech.,
North American Chapter ofAsssoc. Comp. Ling., Edmonton,
Alberta, May/June.
J. Bilmes and G. Zweig. 2002. The Graphical Models Toolkit:
An open source software system for speech and time-series
processing. Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Processing.
S. Chen and J. Goodman. 1998. An empirical study of smooth-
ing techniques for language modeling. Technical report,
Computer Science Group, Harvard University.
R. Durbin et al. 1999. Biological Sequence Analysis: Prob-
abilistic Models of Proteins and Nucleic Acids. Cambridge
University Press.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245–288.
Y. He and S. Young. 2003. A data-driven spoken language un-
derstanding system. In Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 583–588.
G. Ji and J. Bilmes. 2005. Dialog act tagging using graphical
models. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Processing, Philadelphia, PA, March.
D. Jurafsky et al. 1997a. Automatic detection of discourse
structure for speech recognition and understanding. In Proc.
IEEE Workshop on Speech Recognition and Understanding.
D. Jurafsky et al. 1997b. Switchboard SWBD-
DAMSL shallow-discourse-function annotation coders man-
ual. Technical Report 97-02, Institute of Cognitive Science,
University of Colorado.
J. Lafferty et al. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In ICML.
K. Lee et al. 1997. Restricted representation of phrase struc-
ture grammar for building a tree annotated corpus of Korean.
Natural Language Engineering, 3(2-3):215–230.
H. Lee et al. 1998. Speech act analysis model of Korean utter-
ances for automatic dialog translation. J. KISS(B) (Software
and Applications), 25(10):1443–1452.
J. Lember and A. Koloydenko. 2004. Adjusted viterbi training.
a proof of concept. In Submission.
K. Ma et al. 2000. Bi-modal sentence structure for language
modeling. Speech Communication, 31(1):51–67.
M. Mast et al. 1996. Automatic classification of dialog acts
with semantic classification trees and polygrams. Connec-
tionist, Statistical and Symbolic Approaches to Learning for
Natural Language Processing, pages 217–229.
N. Morgan and H. Bourlard. 1990. Continuous speech recogni-
tion using multilayer perceptrons with hidden Markov mod-
els. In ICASSP, pages 413–416.
K. Murphy. 2002. Dynamic Bayesian Networks, Represen-
tation, Inference, and Learning. Ph.D. thesis, MIT, Dept.
Computer Science.
R. Neal and G. Hinton. 1998. A view of the EM algorithm that
justifies incremental, sparse, and other variants. In Learning
in Graphical Models, pages 355–368. Dordrecht: Kluwer
Academic Publishers.
R. Pieraccini and E. Levin. 1991. Stochastic representation of
semantic structure for speech understanding. In Eurospeech,
volume 2, pages 383–386.
N. Reithinger and M. Klesen. 1997. Dialogue act classification
using language models. In Eurospeech.
N. Reithinger et al. 1996. Predicting dialogue acts for a speech-
to-speech translation system. In ICLSP, pages 654–657.
J. Searle. 1969. Speech Acts: An Essay in the Philosophy of
Language. Cambridge University Press.
E. Shriberg et al. 1998. Can prosody aid the automatic classi-
fication of dialog acts in conversational speech? Language
and Speech, 41(3–4):439–487.
E. Shriberg et al. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proc. of the 5th SIGdial Workshop on
Discourse and Dialogue, pages 97–100.
A. Stolcke et al. 1998. Dialog act modeling for conversa-
tional speech. In Proc. AAAI Spring Symp. on Appl. Machine
Learning to Discourse Processing, pages 98–105.
A. Stolcke. 2002. SRILM – an extensible language modeling
toolkit. In ICLSP, volume 2, pages 901–904.
C. Sutton et al. 2004. Dynamic conditional random fields: fac-
torized probabilistic models for labeling and segmenting se-
quence data. In ICML.
D. Talkin. 1995. A robust algorithm for pitch tracking (rapt).
In W. B. Kleijn and K.K. Paliwal, editors, Speech Coding
and Synthesis, pages 495–518. Elsevier Science.
A. Viterbi. 1967. Error bounds for convolutional codes and an
asymptotically optimum decoding algorithm. IEEE Trans.
on Information Theory, 13(2):260–269.
</reference>
<page confidence="0.997399">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932077">
<title confidence="0.99792">Backoff Model Training using Partially Observed Application to Dialog Act Tagging</title>
<author confidence="0.998612">Gang Ji</author>
<author confidence="0.998612">Jeff</author>
<affiliation confidence="0.999789">Department of Electrical University of</affiliation>
<address confidence="0.996343">Seattle, WA</address>
<abstract confidence="0.99758256">Dialog act (DA) tags are useful for many applications in natural language processing and automatic speech recognition. In work, we introduce backoff where a large generalized backoff model is trained, using an embedded expectation-maximization (EM) procedure, on data that is partially observed. We use HBMs as word models conditioned on both DAs and (hidden) DAsegments. Experimental results on the ICSI meeting recorder dialog act corpus show that our procedure can strictly increase likelihood on training data and can effectively reduce errors on test data. In the best case, test error can be reduced by 6.1% relative to our baseline, an improvement on previously reported models that also use prosody. We also compare with our own prosody-based model, and show that our HBM is competitive even without the use of prosody. We have not yet succeeded, however, in combining the benefits of both prosody and the HBM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In ICASSP.</booktitle>
<marker>Ang, 2005</marker>
<rawString>J. Ang et al. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bartlett</author>
</authors>
<title>Exponentiated gradient algorithms for large-margin structured classification.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<marker>Bartlett, 2004</marker>
<rawString>P. Bartlett et al. 2004. Exponentiated gradient algorithms for large-margin structured classification. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bhagat</author>
</authors>
<title>Labeling guide for dialog act tags in the meeting recordering meetings.</title>
<date>2003</date>
<tech>Technical Report 2,</tech>
<institution>International Computer Science Insititute.</institution>
<marker>Bhagat, 2003</marker>
<rawString>S. Bhagat et al. 2003. Labeling guide for dialog act tags in the meeting recordering meetings. Technical Report 2, International Computer Science Insititute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Human Lang. Tech., North American Chapter ofAsssoc. Comp.</booktitle>
<location>Ling., Edmonton, Alberta, May/June.</location>
<contexts>
<context position="2893" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="449" endWordPosition="452">recognition can also be achieved using conditional random fields (Lafferty et al., 2001; Sutton et al., 2004) and general discriminative modeling on structured outputs (Bartlett et al., 2004). In many sequential data analysis tasks (speech, language, or DNA sequence analysis), standard dynamic Bayesian networks (DBNs) (Murphy, 2002) have shown great flexibility and are widely used. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003). Most DA classification procedures assume that within a sentence of a particular fixed DA type, there is a fixed word distribution over the entire sentence. Similar to (Ma et al., 2000) (and see citations therein), we have found, however, that intra280 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 280–287, New York, June 2006. c�2006 Association for Computational Linguistics sentence discourse patterns are inherently dynamic. Moreover, the patterns are specific to each type of DA, meaning a sentence will go through a DAspecific sequenc</context>
<context position="5370" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="844" endWordPosition="847">ned models useless. Third, discrete conditional probability distributions formed using backoff models that have been smoothed (particularly using modified Kneser-Ney (Chen and Goodman, 1998)) have been extremely successful in many language modeling tasks. Training backoff models, however, requires that all data is observed so that data counts can be formed. Indeed, our DA-specific word models (implemented via backoff) will also need to condition on the current sub-DA, which at training time is unknown. We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden. We thus call our models hidden backoff models (HBMs). Our method is indeed a form of embedded EM training (Morgan and Bourlard, 1990), and more generally is a specific form of EM (Neal and Hinton, 1998). Our approach is similar to (Ma et al., 2000), except our underlying language models are backoff-based and thus retain the benefits of advanced smoothing methods, and we utilize both a normal and a backoff EM step as will be seen. We moreover wrap up the above ideas in the framework of dynamic Bayesian networks, which ar</context>
<context position="9610" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="1586" endWordPosition="1589">s used only when sentence change is true. Lastly, at the very beginning of a meeting, a special start of DA token is used. The joint probability under this baseline model is written as follows: P (W, D) = H P(dk|dk−1) ·ri k (1) P(wk,i|wk,i−1, dk), where W = {wk,i} is the word sequence, D = {dk} is the DA sequence, dk is the DA of the k-th sentence, and wk,i is the i-th word of the k-th sentence in the meeting. Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmes and Kirchhoff, 2003). In evaluations, the Viterbi algorithm (Viterbi, 1967) can be used to find the best DA sequence path from the words of the meeting according to the joint distribution in Equation (1). 3 Hidden Backoff Models When analyzing discourse patterns, it can be seen that sentences with different DAs usually have different internal structures. Accordingly, in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned above), a statement can consist of a noun followed by a verb phase. A problem, however, is that sub-DAs are not annotated </context>
<context position="15585" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="2606" endWordPosition="2609">tions (q), statements (s), and disruptions (x). In our evaluations, among the entire 75 conversations, 51 are used as the training set, 11 are used as the development set, 11 are used as test set, and the remaining 3 are not used. For each experiment, we used a genetic algorithm to search for the best factored language model structure on the development set and 4 5 6 283 we report the best results. Our baseline system is the generative model shown in Figure 1 and uses a backoff implementation of the word model, and is optimized on the development set. We use the SRILM toolkit with extensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding. Our baseline system has an error rate of 19.7% on the test set, which is comparable to other approaches on the same task (Ang et al., 2005). 4.1 Same number of states for all DAs To compare against our baseline, we use HBMs in the model shown in Figure 2. To train, we followed Algorithm 1 as described before and as is here detailed in Figure 3. Initialization: - randomize states - train word FLM Yes No 3-epoch EM training - find best state path - train word FLM Figure 3: Embedded training: llh = log likelihood In this implementatio</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J. Bilmes and K. Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Human Lang. Tech., North American Chapter ofAsssoc. Comp. Ling., Edmonton, Alberta, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>G Zweig</author>
</authors>
<title>The Graphical Models Toolkit: An open source software system for speech and time-series processing.</title>
<date>2002</date>
<booktitle>Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="15633" citStr="Bilmes and Zweig, 2002" startWordPosition="2615" endWordPosition="2618">ur evaluations, among the entire 75 conversations, 51 are used as the training set, 11 are used as the development set, 11 are used as test set, and the remaining 3 are not used. For each experiment, we used a genetic algorithm to search for the best factored language model structure on the development set and 4 5 6 283 we report the best results. Our baseline system is the generative model shown in Figure 1 and uses a backoff implementation of the word model, and is optimized on the development set. We use the SRILM toolkit with extensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding. Our baseline system has an error rate of 19.7% on the test set, which is comparable to other approaches on the same task (Ang et al., 2005). 4.1 Same number of states for all DAs To compare against our baseline, we use HBMs in the model shown in Figure 2. To train, we followed Algorithm 1 as described before and as is here detailed in Figure 3. Initialization: - randomize states - train word FLM Yes No 3-epoch EM training - find best state path - train word FLM Figure 3: Embedded training: llh = log likelihood In this implementation, an upper triangular matrix (with self-transit</context>
</contexts>
<marker>Bilmes, Zweig, 2002</marker>
<rawString>J. Bilmes and G. Zweig. 2002. The Graphical Models Toolkit: An open source software system for speech and time-series processing. Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="4933" citStr="Chen and Goodman, 1998" startWordPosition="773" endWordPosition="776"> Therefore, some form of unsupervised clustering or pre-shallow-parsing of sub-DAs must be performed. In such a model, these sub-DAs are essentially unknown hidden variables that ideally could be trained with an expectation-maximization (EM) procedure. Second, when training models of language, it is necessary to employ some form of smoothing methodology since otherwise data-sparseness would render standard maximum-likelihood trained models useless. Third, discrete conditional probability distributions formed using backoff models that have been smoothed (particularly using modified Kneser-Ney (Chen and Goodman, 1998)) have been extremely successful in many language modeling tasks. Training backoff models, however, requires that all data is observed so that data counts can be formed. Indeed, our DA-specific word models (implemented via backoff) will also need to condition on the current sub-DA, which at training time is unknown. We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden. We thus call our models hidden backoff models (HBMs). Our method is indeed a form of emb</context>
<context position="9556" citStr="Chen and Goodman, 1998" startWordPosition="1579" endWordPosition="1582">wn in the figure by having a special parent that is used only when sentence change is true. Lastly, at the very beginning of a meeting, a special start of DA token is used. The joint probability under this baseline model is written as follows: P (W, D) = H P(dk|dk−1) ·ri k (1) P(wk,i|wk,i−1, dk), where W = {wk,i} is the word sequence, D = {dk} is the DA sequence, dk is the DA of the k-th sentence, and wk,i is the i-th word of the k-th sentence in the meeting. Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmes and Kirchhoff, 2003). In evaluations, the Viterbi algorithm (Viterbi, 1967) can be used to find the best DA sequence path from the words of the meeting according to the joint distribution in Equation (1). 3 Hidden Backoff Models When analyzing discourse patterns, it can be seen that sentences with different DAs usually have different internal structures. Accordingly, in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned above), a statement can consist of a noun followed by a verb phase. </context>
<context position="11945" citStr="Chen and Goodman, 1998" startWordPosition="1977" endWordPosition="1980">d be trained accordingly. The hidden state sequence is unknown, however, and thus cannot be used to produce a standard backoff model. What we desire is an ability to utilize a backoff model (to mitigate data sparseness effects) while simultaneously retaining the state as a hidden (rather than an observed) variable, and also have a procedure that trains the entire model to improve overall model likelihood. Expectation-maximization (EM) algorithms are well-known to be able to train models with hidden states. Furthermore, standard advanced smoothing methods such as modified Kneser-Ney smoothing (Chen and Goodman, 1998) utilize integer counts (rather than fractional ones), and they moreover need “meta” counts (or counts of counts). Therefore, in order to train this model, we propose an embedded training algorithm that cycles between a standard EM training procedure (to train the hidden state distribution), and a stage where the most likely hidden states (and their counts and meta counts) are used externally to train a backoff model. This procedure can be described in detail as follows: Input : W — meeting word sequence Input : D — DA sequence Output : P(sk,i|sk,i−1) - state transition CPT Output: P(wk,i|wk,i</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Durbin</author>
</authors>
<title>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.</title>
<date>1999</date>
<publisher>Cambridge University Press.</publisher>
<marker>Durbin, 1999</marker>
<rawString>R. Durbin et al. 1999. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="10401" citStr="Gildea and Jurafsky, 2002" startWordPosition="1719" endWordPosition="1722">on in Equation (1). 3 Hidden Backoff Models When analyzing discourse patterns, it can be seen that sentences with different DAs usually have different internal structures. Accordingly, in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned above), a statement can consist of a noun followed by a verb phase. A problem, however, is that sub-DAs are not annotated in our training corpus. While clustering and annotation of these phrases is already a widely developed research topic (Pieraccini and Levin, 1991; Lee et al., 1997; Gildea and Jurafsky, 2002), in our approach we use an EM algorithm to learn these hidden sub-DAs in a data-driven fashion. Pictorially, we add a layer of hidden states to our baseline DBN as illustrated in Figure 2. prologue chunk epilogue Figure 2: Hidden backoff model for DA tagging. Under this model, the joint probability is: P (W, �, D) = H P(dk|dk−1) k ·ri [P(sk,i|sk,i−1, dk) (2) · P(wk,i|wk,i−1, sk,i, dk)I , dialog act DA &lt;s&gt; sentence change word &lt;s&gt; word prologue chunk epilogue dialog act DA &lt;s&gt; hidden state sentence change word &lt;s&gt; word 282 where 5 = {sk,i} is the hidden state sequence, sk,i is the hidden state</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>A data-driven spoken language understanding system.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>583--588</pages>
<contexts>
<context position="1843" citStr="He and Young, 2003" startWordPosition="284" endWordPosition="287">ral conversations and meetings are well known to provide interesting and useful information about human conversational behavior. They thus attract research from many different and beneficial perspectives. Dialog acts (DAs) (Searle, 1969), which reflect the functions that utterances serve in a discourse, are one type of such patterns. Detecting and understanding dialog act patterns can provide benefit to systems such as automatic speech recognition (ASR) (Stolcke et al., 1998), machine dialog translation (Lee et al., 1998), and general natural language processing (NLP) (Jurafsky et al., 1997b; He and Young, 2003). DA pattern recognition is an instance of “tagging.” Many different techniques have been quite successful in this endeavor, including hidden Markov models (Jurafsky et al., 1997a; Stolcke et al., 1998), semantic classification trees and polygrams (Mast et al., 1996), maximum entropy models (Ang et al., 2005), and other language models (Reithinger et al., 1996; Reithinger and Klesen, 1997). Like other tagging tasks, DA recognition can also be achieved using conditional random fields (Lafferty et al., 2001; Sutton et al., 2004) and general discriminative modeling on structured outputs (Bartlett</context>
</contexts>
<marker>He, Young, 2003</marker>
<rawString>Y. He and S. Young. 2003. A data-driven spoken language understanding system. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding, pages 583–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ji</author>
<author>J Bilmes</author>
</authors>
<title>Dialog act tagging using graphical models.</title>
<date>2005</date>
<booktitle>In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2675" citStr="Ji and Bilmes, 2005" startWordPosition="414" endWordPosition="417"> classification trees and polygrams (Mast et al., 1996), maximum entropy models (Ang et al., 2005), and other language models (Reithinger et al., 1996; Reithinger and Klesen, 1997). Like other tagging tasks, DA recognition can also be achieved using conditional random fields (Lafferty et al., 2001; Sutton et al., 2004) and general discriminative modeling on structured outputs (Bartlett et al., 2004). In many sequential data analysis tasks (speech, language, or DNA sequence analysis), standard dynamic Bayesian networks (DBNs) (Murphy, 2002) have shown great flexibility and are widely used. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003). Most DA classification procedures assume that within a sentence of a particular fixed DA type, there is a fixed word distribution over the entire sentence. Similar to (Ma et al., 2000) (and see citations therein), we have found, however, that intra280 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 280–287, New York, June </context>
</contexts>
<marker>Ji, Bilmes, 2005</marker>
<rawString>G. Ji and J. Bilmes. 2005. Dialog act tagging using graphical models. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Philadelphia, PA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>Automatic detection of discourse structure for speech recognition and understanding.</title>
<date>1997</date>
<booktitle>In Proc. IEEE Workshop on Speech Recognition and Understanding.</booktitle>
<marker>Jurafsky, 1997</marker>
<rawString>D. Jurafsky et al. 1997a. Automatic detection of discourse structure for speech recognition and understanding. In Proc. IEEE Workshop on Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>Switchboard SWBDDAMSL shallow-discourse-function annotation coders manual.</title>
<date>1997</date>
<tech>Technical Report 97-02,</tech>
<institution>Institute of Cognitive Science, University of Colorado.</institution>
<marker>Jurafsky, 1997</marker>
<rawString>D. Jurafsky et al. 1997b. Switchboard SWBDDAMSL shallow-discourse-function annotation coders manual. Technical Report 97-02, Institute of Cognitive Science, University of Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<marker>Lafferty, 2001</marker>
<rawString>J. Lafferty et al. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lee</author>
</authors>
<title>Restricted representation of phrase structure grammar for building a tree annotated corpus of Korean.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<pages>3--2</pages>
<marker>Lee, 1997</marker>
<rawString>K. Lee et al. 1997. Restricted representation of phrase structure grammar for building a tree annotated corpus of Korean. Natural Language Engineering, 3(2-3):215–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lee</author>
</authors>
<title>Speech act analysis model of Korean utterances for automatic dialog translation.</title>
<date>1998</date>
<journal>J. KISS(B) (Software and Applications),</journal>
<volume>25</volume>
<issue>10</issue>
<marker>Lee, 1998</marker>
<rawString>H. Lee et al. 1998. Speech act analysis model of Korean utterances for automatic dialog translation. J. KISS(B) (Software and Applications), 25(10):1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lember</author>
<author>A Koloydenko</author>
</authors>
<title>Adjusted viterbi training. a proof of concept. In Submission.</title>
<date>2004</date>
<contexts>
<context position="13612" citStr="Lember and Koloydenko, 2004" startWordPosition="2251" endWordPosition="2254">dden state transitions, and a backoff model for word prediction. Because we train the backoff model when some of the variables are hidden, we call the result a hidden backoff model. While we have seen embedded Viterbi training used in the past for simultaneously training heterogeneous models (e.g., Markov chains and Neural Networks (Morgan and Bourlard, 1990)), this is the first instance of training backoff-models that involve hidden variables that we are aware of. While embedded Viterbi estimation is not guaranteed to have the same convergence (or fixed-point under convergence) as normal EM (Lember and Koloydenko, 2004), we find empirically this to be the case (see examples below). Moreover, our algorithm can easily be modified so that instead of taking a Viterbi alignment in step 5, we instead use a set of random samples generated under the current model. In this case, it can be shown using a law-oflarge numbers argument that having sufficient samples guarantees the algorithm will converge (we will investigate this modification in future work). Of course, when decoding with such a model, a conventional Viterbi algorithm can still be used to calculate the best DA sequence. 4 Experimental Results We evaluated</context>
</contexts>
<marker>Lember, Koloydenko, 2004</marker>
<rawString>J. Lember and A. Koloydenko. 2004. Adjusted viterbi training. a proof of concept. In Submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ma</author>
</authors>
<title>Bi-modal sentence structure for language modeling.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Ma, 2000</marker>
<rawString>K. Ma et al. 2000. Bi-modal sentence structure for language modeling. Speech Communication, 31(1):51–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mast</author>
</authors>
<title>Automatic classification of dialog acts with semantic classification trees and polygrams. Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing,</title>
<date>1996</date>
<pages>217--229</pages>
<marker>Mast, 1996</marker>
<rawString>M. Mast et al. 1996. Automatic classification of dialog acts with semantic classification trees and polygrams. Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing, pages 217–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan</author>
<author>H Bourlard</author>
</authors>
<title>Continuous speech recognition using multilayer perceptrons with hidden Markov models.</title>
<date>1990</date>
<booktitle>In ICASSP,</booktitle>
<pages>413--416</pages>
<contexts>
<context position="5578" citStr="Morgan and Bourlard, 1990" startWordPosition="881" endWordPosition="884">ly successful in many language modeling tasks. Training backoff models, however, requires that all data is observed so that data counts can be formed. Indeed, our DA-specific word models (implemented via backoff) will also need to condition on the current sub-DA, which at training time is unknown. We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden. We thus call our models hidden backoff models (HBMs). Our method is indeed a form of embedded EM training (Morgan and Bourlard, 1990), and more generally is a specific form of EM (Neal and Hinton, 1998). Our approach is similar to (Ma et al., 2000), except our underlying language models are backoff-based and thus retain the benefits of advanced smoothing methods, and we utilize both a normal and a backoff EM step as will be seen. We moreover wrap up the above ideas in the framework of dynamic Bayesian networks, which are used to represent and train all of our models. We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al., 2004) corpus, and find that our novel hidden backoff model can signifi</context>
<context position="13345" citStr="Morgan and Bourlard, 1990" startWordPosition="2208" endWordPosition="2212">uence by Viterbi; backoff train P(wk,i|wk,i−1, �sk,i, dk); 7 end Algorithm 1: Embedded training for HBMs In the algorithm, the input contains words and a DA for each sentence in the meeting. The output is the corresponding conditional probability table (CPT) for hidden state transitions, and a backoff model for word prediction. Because we train the backoff model when some of the variables are hidden, we call the result a hidden backoff model. While we have seen embedded Viterbi training used in the past for simultaneously training heterogeneous models (e.g., Markov chains and Neural Networks (Morgan and Bourlard, 1990)), this is the first instance of training backoff-models that involve hidden variables that we are aware of. While embedded Viterbi estimation is not guaranteed to have the same convergence (or fixed-point under convergence) as normal EM (Lember and Koloydenko, 2004), we find empirically this to be the case (see examples below). Moreover, our algorithm can easily be modified so that instead of taking a Viterbi alignment in step 5, we instead use a set of random samples generated under the current model. In this case, it can be shown using a law-oflarge numbers argument that having sufficient s</context>
</contexts>
<marker>Morgan, Bourlard, 1990</marker>
<rawString>N. Morgan and H. Bourlard. 1990. Continuous speech recognition using multilayer perceptrons with hidden Markov models. In ICASSP, pages 413–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Murphy</author>
</authors>
<title>Dynamic Bayesian Networks, Representation, Inference, and Learning.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT, Dept. Computer Science.</institution>
<contexts>
<context position="2600" citStr="Murphy, 2002" startWordPosition="403" endWordPosition="404">rkov models (Jurafsky et al., 1997a; Stolcke et al., 1998), semantic classification trees and polygrams (Mast et al., 1996), maximum entropy models (Ang et al., 2005), and other language models (Reithinger et al., 1996; Reithinger and Klesen, 1997). Like other tagging tasks, DA recognition can also be achieved using conditional random fields (Lafferty et al., 2001; Sutton et al., 2004) and general discriminative modeling on structured outputs (Bartlett et al., 2004). In many sequential data analysis tasks (speech, language, or DNA sequence analysis), standard dynamic Bayesian networks (DBNs) (Murphy, 2002) have shown great flexibility and are widely used. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003). Most DA classification procedures assume that within a sentence of a particular fixed DA type, there is a fixed word distribution over the entire sentence. Similar to (Ma et al., 2000) (and see citations therein), we have found, however, that intra280 Proceedings of the Human Language Technology Conferen</context>
<context position="7177" citStr="Murphy, 2002" startWordPosition="1152" endWordPosition="1153">nd while we have not been able to usefully employ both prosody and the HBM technique together, our HBM is competitive in this case as well. Furthermore, our results show the effectiveness of our embedded EM procedure, as we demonstrate that it increases training log likelihoods, while simultaneously reducing error rate. Section 2 briefly summarizes our baseline DBNbased models for DA tagging tasks. In Section 3, we introduce our HBMs. Section 4 contains experimental evaluations on the MRDA corpus and finally Section 5 concludes. 2 DBN-based Models for Tagging Dynamic Bayesian networks (DBNs) (Murphy, 2002) are widely used in sequential data analysis such as automatic speech recognition (ASR) and DNA sequencing analysis (Durbin et al., 1999). A hidden Markov model (HMM) for DA tagging as in (Stolcke et al., 1998) is one such instance. Figure 1 shows a generative DBN model that will be taken as our baseline. This DBN shows a prologue (the first time slice of the model), an epilogue (the last slice), and a chunk that is repeated sufficiently to fit the entire data stream. In this case, the data stream consists of the words of a meeting conversation, where individuals within the meeting (hopefully)</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>K. Murphy. 2002. Dynamic Bayesian Networks, Representation, Inference, and Learning. Ph.D. thesis, MIT, Dept. Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
<author>G Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning in Graphical Models,</booktitle>
<pages>355--368</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="5647" citStr="Neal and Hinton, 1998" startWordPosition="894" endWordPosition="897">owever, requires that all data is observed so that data counts can be formed. Indeed, our DA-specific word models (implemented via backoff) will also need to condition on the current sub-DA, which at training time is unknown. We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden. We thus call our models hidden backoff models (HBMs). Our method is indeed a form of embedded EM training (Morgan and Bourlard, 1990), and more generally is a specific form of EM (Neal and Hinton, 1998). Our approach is similar to (Ma et al., 2000), except our underlying language models are backoff-based and thus retain the benefits of advanced smoothing methods, and we utilize both a normal and a backoff EM step as will be seen. We moreover wrap up the above ideas in the framework of dynamic Bayesian networks, which are used to represent and train all of our models. We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al., 2004) corpus, and find that our novel hidden backoff model can significantly improve dialog tagging accuracy. With a different number of hi</context>
<context position="22247" citStr="Neal and Hinton, 1998" startWordPosition="3784" endWordPosition="3787"> particular) can show benefits for any data domain where smoothing is necessary. To understand the properties of our algorithm a bit better, after each training iteration using a partially trained model, we calculated both the log likelihood of the training set and the tagging error rate of the test data. Figure 4 shows these results using the best configuration from the previous section (three states for (q), two for (s)/(x) and one for (b)/(h)). This example is typical of the convergence we see of Algorithm 1, which empirically suggests that our procedure may be similar to a generalized EM (Neal and Hinton, 1998). iterations Figure 4: Embedded EM training performance. We find that the log likelihood after each EM training is strictly increasing, suggesting that our embedded EM algorithm for hidden backoff models 24 6 x −1.05 23 llh error rate (y) baseline error rate 19 18 −1.2 1 2 3 4 5 6 7 −1.1 log likelihood −1.15 22 21 20 285 is improving the overall joint likelihood of the training data according to the model. This strict increase of likelihood combined with the fact that Viterbi training does not have the same theoretical convergence guarantees as does normal EM indicates that more detailed theor</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. Neal and G. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in Graphical Models, pages 355–368. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>E Levin</author>
</authors>
<title>Stochastic representation of semantic structure for speech understanding.</title>
<date>1991</date>
<booktitle>In Eurospeech,</booktitle>
<volume>2</volume>
<pages>383--386</pages>
<contexts>
<context position="10355" citStr="Pieraccini and Levin, 1991" startWordPosition="1711" endWordPosition="1714"> the meeting according to the joint distribution in Equation (1). 3 Hidden Backoff Models When analyzing discourse patterns, it can be seen that sentences with different DAs usually have different internal structures. Accordingly, in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned above), a statement can consist of a noun followed by a verb phase. A problem, however, is that sub-DAs are not annotated in our training corpus. While clustering and annotation of these phrases is already a widely developed research topic (Pieraccini and Levin, 1991; Lee et al., 1997; Gildea and Jurafsky, 2002), in our approach we use an EM algorithm to learn these hidden sub-DAs in a data-driven fashion. Pictorially, we add a layer of hidden states to our baseline DBN as illustrated in Figure 2. prologue chunk epilogue Figure 2: Hidden backoff model for DA tagging. Under this model, the joint probability is: P (W, �, D) = H P(dk|dk−1) k ·ri [P(sk,i|sk,i−1, dk) (2) · P(wk,i|wk,i−1, sk,i, dk)I , dialog act DA &lt;s&gt; sentence change word &lt;s&gt; word prologue chunk epilogue dialog act DA &lt;s&gt; hidden state sentence change word &lt;s&gt; word 282 where 5 = {sk,i} is the h</context>
</contexts>
<marker>Pieraccini, Levin, 1991</marker>
<rawString>R. Pieraccini and E. Levin. 1991. Stochastic representation of semantic structure for speech understanding. In Eurospeech, volume 2, pages 383–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Reithinger</author>
<author>M Klesen</author>
</authors>
<title>Dialogue act classification using language models.</title>
<date>1997</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="2235" citStr="Reithinger and Klesen, 1997" startWordPosition="345" endWordPosition="348"> can provide benefit to systems such as automatic speech recognition (ASR) (Stolcke et al., 1998), machine dialog translation (Lee et al., 1998), and general natural language processing (NLP) (Jurafsky et al., 1997b; He and Young, 2003). DA pattern recognition is an instance of “tagging.” Many different techniques have been quite successful in this endeavor, including hidden Markov models (Jurafsky et al., 1997a; Stolcke et al., 1998), semantic classification trees and polygrams (Mast et al., 1996), maximum entropy models (Ang et al., 2005), and other language models (Reithinger et al., 1996; Reithinger and Klesen, 1997). Like other tagging tasks, DA recognition can also be achieved using conditional random fields (Lafferty et al., 2001; Sutton et al., 2004) and general discriminative modeling on structured outputs (Bartlett et al., 2004). In many sequential data analysis tasks (speech, language, or DNA sequence analysis), standard dynamic Bayesian networks (DBNs) (Murphy, 2002) have shown great flexibility and are widely used. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a g</context>
</contexts>
<marker>Reithinger, Klesen, 1997</marker>
<rawString>N. Reithinger and M. Klesen. 1997. Dialogue act classification using language models. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Reithinger</author>
</authors>
<title>Predicting dialogue acts for a speechto-speech translation system.</title>
<date>1996</date>
<booktitle>In ICLSP,</booktitle>
<pages>654--657</pages>
<marker>Reithinger, 1996</marker>
<rawString>N. Reithinger et al. 1996. Predicting dialogue acts for a speechto-speech translation system. In ICLSP, pages 654–657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosophy of Language.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1461" citStr="Searle, 1969" startWordPosition="225" endWordPosition="226"> error can be reduced by 6.1% relative to our baseline, an improvement on previously reported models that also use prosody. We also compare with our own prosody-based model, and show that our HBM is competitive even without the use of prosody. We have not yet succeeded, however, in combining the benefits of both prosody and the HBM. 1 Introduction Discourse patterns in natural conversations and meetings are well known to provide interesting and useful information about human conversational behavior. They thus attract research from many different and beneficial perspectives. Dialog acts (DAs) (Searle, 1969), which reflect the functions that utterances serve in a discourse, are one type of such patterns. Detecting and understanding dialog act patterns can provide benefit to systems such as automatic speech recognition (ASR) (Stolcke et al., 1998), machine dialog translation (Lee et al., 1998), and general natural language processing (NLP) (Jurafsky et al., 1997b; He and Young, 2003). DA pattern recognition is an instance of “tagging.” Many different techniques have been quite successful in this endeavor, including hidden Markov models (Jurafsky et al., 1997a; Stolcke et al., 1998), semantic class</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>J. Searle. 1969. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<title>Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech,</title>
<date>1998</date>
<pages>41--3</pages>
<marker>Shriberg, 1998</marker>
<rawString>E. Shriberg et al. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech, 41(3–4):439–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In Proc. of the 5th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--100</pages>
<marker>Shriberg, 2004</marker>
<rawString>E. Shriberg et al. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Proc. of the 5th SIGdial Workshop on Discourse and Dialogue, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Dialog act modeling for conversational speech.</title>
<date>1998</date>
<booktitle>In Proc. AAAI Spring Symp. on Appl. Machine Learning to Discourse Processing,</booktitle>
<pages>98--105</pages>
<marker>Stolcke, 1998</marker>
<rawString>A. Stolcke et al. 1998. Dialog act modeling for conversational speech. In Proc. AAAI Spring Symp. on Appl. Machine Learning to Discourse Processing, pages 98–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICLSP,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="9500" citStr="Stolcke, 2002" startWordPosition="1574" endWordPosition="1575">the special start of sentence &lt;s&gt; token, as shown in the figure by having a special parent that is used only when sentence change is true. Lastly, at the very beginning of a meeting, a special start of DA token is used. The joint probability under this baseline model is written as follows: P (W, D) = H P(dk|dk−1) ·ri k (1) P(wk,i|wk,i−1, dk), where W = {wk,i} is the word sequence, D = {dk} is the DA sequence, dk is the DA of the k-th sentence, and wk,i is the i-th word of the k-th sentence in the meeting. Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmes and Kirchhoff, 2003). In evaluations, the Viterbi algorithm (Viterbi, 1967) can be used to find the best DA sequence path from the words of the meeting according to the joint distribution in Equation (1). 3 Hidden Backoff Models When analyzing discourse patterns, it can be seen that sentences with different DAs usually have different internal structures. Accordingly, in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned above), a st</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In ICLSP, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
</authors>
<title>Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<marker>Sutton, 2004</marker>
<rawString>C. Sutton et al. 2004. Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talkin</author>
</authors>
<title>A robust algorithm for pitch tracking (rapt).</title>
<date>1995</date>
<booktitle>Speech Coding and Synthesis,</booktitle>
<pages>495--518</pages>
<editor>In W. B. Kleijn and K.K. Paliwal, editors,</editor>
<publisher>Elsevier Science.</publisher>
<contexts>
<context position="24866" citStr="Talkin, 1995" startWordPosition="4227" endWordPosition="4228">his work, we also did not perform any hidden state tying within different DAs. In practice, some states in statements should be able to be beneficially tied with other states within questions. Our results show that having three states for all DAs is not as good as two states for all. But with tying, more states might be more successfully used. 4.5 Influence of Prosody Cues It has been shown that prosody cues provide useful information in DA tagging tasks (Shriberg et al., 1998; Ang et al., 2005). We also incorporated prosody features in our models. We used ESPS get f0 based on RAPT algorithm (Talkin, 1995) to get Fo values. For each speaker, mean and variance normalization is performed. For each word, a linear regression is carried on the normalized Fo values. We quantize the slope values into 20 bins and treat those as prosody features associated with each word. After adding the prosody features, the simple generative model as shown in Figure 5 gives 18.4% error rate, which is 6.6% improvement over our baseline. There is no statistical difference between the best performance of this prosody model and the earlier best HBM. This implies that the HBM can obtain as good performance as a prosody-ba</context>
</contexts>
<marker>Talkin, 1995</marker>
<rawString>D. Talkin. 1995. A robust algorithm for pitch tracking (rapt). In W. B. Kleijn and K.K. Paliwal, editors, Speech Coding and Synthesis, pages 495–518. Elsevier Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Trans. on Information Theory,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="9665" citStr="Viterbi, 1967" startWordPosition="1596" endWordPosition="1597">ing of a meeting, a special start of DA token is used. The joint probability under this baseline model is written as follows: P (W, D) = H P(dk|dk−1) ·ri k (1) P(wk,i|wk,i−1, dk), where W = {wk,i} is the word sequence, D = {dk} is the DA sequence, dk is the DA of the k-th sentence, and wk,i is the i-th word of the k-th sentence in the meeting. Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmes and Kirchhoff, 2003). In evaluations, the Viterbi algorithm (Viterbi, 1967) can be used to find the best DA sequence path from the words of the meeting according to the joint distribution in Equation (1). 3 Hidden Backoff Models When analyzing discourse patterns, it can be seen that sentences with different DAs usually have different internal structures. Accordingly, in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned above), a statement can consist of a noun followed by a verb phase. A problem, however, is that sub-DAs are not annotated in our training corpus. While clustering and annotation</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Trans. on Information Theory, 13(2):260–269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>