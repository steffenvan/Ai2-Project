<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.281104">
<title confidence="0.9968265">
Getting More Mileage from Web Text Sources for Conversational
Speech Language Modeling using Class-Dependent Mixtures
</title>
<author confidence="0.999595">
Ivan Bulyko, Mari Ostendorf Andreas Stolcke
</author>
<affiliation confidence="0.998907">
Department of Electrical Engineering SRI International
University of Washington, Seattle, WA 98195. Menlo Park, CA 94025.
</affiliation>
<email confidence="0.998648">
{bulyko,mo}@ssli.ee.washington.edu stolcke@speech.sri.com
</email>
<sectionHeader confidence="0.994125" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9928565">
Sources of training data suitable for language
modeling of conversational speech are limited.
In this paper, we show how training data can be
supplemented with text from the web filtered to
match the style and/or topic of the target recog-
nition task, but also that it is possible to get big-
ger performance gains from the data by using
class-dependent interpolation ofN-grams.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995301724138">
Language models constitute one of the key components
in modern speech recognition systems. Training an N-
gram language model, the most commonly used type of
model, requires large quantities of text that is matched
to the target recognition task both in terms of style and
topic. In tasks involving conversational speech the ideal
training material, i.e. transcripts of conversational speech,
is costly to produce, which limits the amount of training
data currently available.
Methods have been developed for the purpose of lan-
guage model adaptation, i.e. the adaptation of an exist-
ing model to new topics, domains, or tasks for which
little or no training material may be available. Since
out-of-domain data can contain relevant as well as irrele-
vant information, various methods are used to identify the
most relevant portions of the out-of-domain data prior to
combination. Past work on pre-selection has been based
on word frequency counts (Rudnicky, 1995), probabil-
ity (or perplexity) of word or part-of-speech sequences
(Iyer and Ostendorf, 1999), latent semantic analysis (Bel-
legarda, 1998), and information retrieval techniques (Ma-
hajan et al., 1999; Iyer and Ostendorf, 1999). Perplexity-
based clustering has also been used for defining topic-
specific subsets of in-domain data (Clarkson and Robin-
son, 1997; Martin et al, 1997), and test set perplexity
has been used to prune documents from a training corpus
(Klakow, 2000). The most common method for using the
additional text sources is to train separate language mod-
els on a small amount of in-domain and large amounts
of out-of-domain data and to combine them by interpola-
tion, also referred to as mixtures of language models. The
technique was reported by IBM in 1995 (Liu et al, 1995),
and has been used by many sites since then. An alter-
native approach involves decomposition of the language
model into a class n-gram for interpolation (Iyer and Os-
tendorf, 1997; Ries, 1997), allowing content words to be
interpolated with different weights than filled pauses, for
example, which gives an improvement over standard mix-
ture modeling for conversational speech.
Recently researchers have turned to the World Wide
Web as an additional source of training data for language
modeling. For “just-in-time” language modeling (Berger
and Miller, 1998), adaptation data is obtained by submit-
ting words from initial hypotheses of user utterances as
queries to a web search engine. Their queries, however,
treated words as individual tokens and ignored function
words. Such a search strategy typically generates text of
a non-conversational style, hence not ideally suited for
ASR. In (Zhu and Rosenfeld, 2001), instead of down-
loading the actual web pages, the authors retrieved N-
gram counts provided by the search engine. Such an ap-
proach generates valuable statistics but limits the set of
N-grams to ones occurring in the baseline model.
In this paper, we present an approach to extracting ad-
ditional training data from the web by searching for text
that is better matched to a conversational speaking style.
We also show how we can make better use of this new
data by applying class-dependent interpolation.
</bodyText>
<sectionHeader confidence="0.476569" genericHeader="method">
2 Collecting Text from the Web
</sectionHeader>
<bodyText confidence="0.999912514285714">
The amount of text available on the web is enormous
(over 3 billion web pages are indexed via Google alone)
and continues to grow. Most of the text on the web is
non-conversational, but there is a fair amount of chat-like
material that is similar to conversational speech though
often omitting disfluencies. This was our primary target
when extracting data from the web. Queries submitted to
Google were composed of N-grams that occur most fre-
quently in the switchboard training corpus, e.g. “I never
thought I would”, “I would think so”, etc. We were
searching for the exact match to one or more of these
N-grams within the text of the web pages. Web pages
returned by Google for the most part consisted of conver-
sational style phrases like “we were friends but we don’t
actually have a relationship” and “well I actually I I really
haven’t seen her for years.”
We used a slightly different search strategy when col-
lecting topic-specific data. First we extended the base-
line vocabulary with words from a small in-domain train-
ing corpus (Schwarm and Ostendorf, 2002), and then we
used N-grams with these new words in our web queries,
e.g. “wireless mikes like”, “I know that recognizer” for
a meeting transcription task (Morgan et al, 2001). Web
pages returned by Google mostly contained technical ma-
terial related to topics similar to what was discussed in the
meetings, e.g. “we were inspired by the weighted count
scheme...”, “for our experiments we used the Bellman-
Ford algorithm...”, etc.
The retrieved web pages were filtered before their con-
tent could be used for language modeling. First we
stripped the HTML tags and ignored any pages with a
very high OOV rate. We then piped the text through
a maximum entropy sentence boundary detector (Rat-
naparkhi, 1996) and performed text normalization using
NSW tools (Sproat et al, 2001).
</bodyText>
<sectionHeader confidence="0.976704" genericHeader="method">
3 Class-dependent Mixture of LMs
</sectionHeader>
<bodyText confidence="0.999928655172414">
Linear interpolation is a standard approach to combin-
ing language models, where the probability of a word
wi given history h is computed as a linear combination
of the corresponding N-gram probabilities from S dif-
ferent models: p(wi|h) = EsES asps(wi|h). Depend-
ing on how much adaptation data is available it may be
beneficial to estimate a larger number of mixture weights
as (more than one per data source) in order to handle
source mismatch, specifically letting the mixture weight
depend on the context h. One approach is to use a mixture
weight corresponding to the source posterior probability
as(h) = p(s|h) (Weintraub et al, 1996). Here, we instead
choose to let the weight vary as a function of the previous
word class, i.e. p(wi|h) = EsES as(c(wi_1))ps(wi|h),
where classes c(wi_1) are part-of-speech tags except for
the 100 most frequent words which form their own indi-
vidual classes. Such a scheme can generalize across do-
mains by tapping into the syntactic structure (POS tags),
already shown to be useful for cross-domain language
modeling (Iyer and Ostendorf, 1997), and at the same
time target conversational speech since the top 100 words
cover 70% of tokens in Switchboard training corpus.
Combining several N-grams can produce a model with
a very large number of parameters, which is costly in de-
coding. In such cases N-grams are typically pruned. Here
we use entropy-based pruning (Stolcke, 1998) after mix-
ing unpruned models, and reduce the model aggressively
to about 15% of its original size. The same pruning pa-
rameters were applied to all models in our experiments.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99966551923077">
We evaluated on two tasks: 1) Switchboard (Godfrey et
al., 1992), specifically the HUB5 eval 2001 set having a
total of 60K words spoken by 120 speakers, and 2) an
ICSI Meeting recorder (Morgan et al, 2001) eval set hav-
ing a total of 44K words spoken by 25 speakers. Both
sets featured spontaneous conversational speech. There
were 45K words of held-out data for each task.
Text corpora of conversational telephone speech
(CTS) available for training language models consisted
of Switchboard, Callhome English, and Switchboard-
cellular, a total of 3 million words. In addition to that we
used 150 million words of Broadcast News (BN) tran-
scripts, and we collected 191 million words of “con-
versational” text from the web. For the Meetings task,
there were 200K words of meeting transcripts available
for training, and we collected 28 million words of “topic-
related” text from the web.
The experiments were conducted using the SRI large
vocabulary speech recognizer (Stolcke et al, 2000) in
the N-best rescoring mode. A baseline bigram language
model was used to generate N-best lists, which were then
rescored with various trigram models.
Table 1 shows word error rates (WER) on the HUB5
test set, comparing performance of the class-based mix-
ture against standard (i.e. class-independent) interpola-
tion. The class-based mixture gave better results in all
cases except when only CTS sources were used, probably
because these sources are similar to each other and the
class-based mixture is mainly useful when data sources
are more diverse. We also obtained lower WER by using
the web data instead of BN, which indicates that the web
data is better matched to our task (i.e. it is more “conver-
sational”). If training data is completely arbitrary, then its
benefits to the recognition task are minimal, as shown by
an example of using a 66M-word corpus collected from
random web pages. The baseline Switchboard model
gave test set perplexity of 96, which is reduced to 87 with
a standard mixture CTS and BN data, reduced further to
83 by adding the web data, and to a best case of 82 with
class-dependent interpolation and the added web data.
Increasing the amount of web training data from 61M
to 191M gave relatively small performance gains. We
“trimmed” the 191M-word web corpus down to 61M
words by choosing documents with lowest perplexity
according to the combined CTS model, yielding the
“Web2” data source. The model that used Web2 gave
the same WER as the one trained with the original 61M
web corpus. It could be that the web text obtained
with “Google” filtering is fairly homogeneous, so little
is gained by further perplexity filtering. Or, it could be
that when choosing better matched data, we also exclude
new N-grams that may occur only in testing.
</bodyText>
<tableCaption confidence="0.962341">
Table 1: HUB5 (eval 2001) WER results using standard
and class-based mixtures.
</tableCaption>
<table confidence="0.999929">
LM Data Sources Std. mix Class mix
Baseline CTS 38.9% 38.9%
+ 150M BN 37.9% 37.8%
+ 66M Web (Random) 38.6% 38.3%
+ 61M Web 37.7% 37.6%
+ 191M Web 37.6% 37.4%
+ 150M BN + 61M Web 37.7% 37.3%
+ 150M BN + 191M Web 37.5% 37.2%
+ 150M BN + 61M Web2 37.7% 37.3%
</table>
<tableCaption confidence="0.793538">
Table 2: Meetings results (WER).
</tableCaption>
<table confidence="0.9995426">
LM Data Sources Std. mix Class mix
Baseline 38.2%
+ 0.2M Meetings 37.2% 36.9%
+ 28M Web (Topic) 36.9% 36.7%
+ Meetings + Web (Topic) 36.2% 35.9%
</table>
<bodyText confidence="0.997828">
Results on the Meeting test set are shown in Table
2, where the baseline model was trained on CTS and
BN sources. As in the HUB5 experiments, the class-
based mixture outperformed standard interpolation. We
achieved lower WER by using the web data instead of
the meeting transcripts, but the best results are obtained
by using all data sources. Language model perplexity is
reduced from 122 for the baseline to a best case of 95.
We also tried different class assignments for the class-
based mixture on the HUB5 set and we found that using
automatically derived classes instead of part-of-speech
tags does not lead to performance degradation as long
as we allocate individual classes for the top 100 words.
Automatic class mapping can make class-based mixtures
feasible for other languages where part-of-speech tags are
difficult to derive.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999962916666667">
In summary, we have shown that, if filtered, web text
can be successfully used for training language models
of conversational speech, outperforming some other out-
of-domain (BN) and small domain-specific (Meetings)
sources of data. We have also found that by combin-
ing LMs from different domains with class-dependent in-
terpolation (particularly when each of the top 100 words
forms its own class), we achieve lower WER than if we
use the standard approach where mixture weights depend
only on the data source. Recognition experiments show a
significant reduction in WER (1.3-2.3% absolute) due to
additional training data and class-based interpolation.
</bodyText>
<sectionHeader confidence="0.989815" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997325678571428">
J. Bellegarda. 1998. Exploiting both local and global con-
straints for multispan statistical language modeling. In Proc.
ICASSP, pages II:677–680.
A. Berger and R. Miller. 1998. Just-in-time language modeling.
In Proc. ICASSP, pages II:705–708.
P. Clarkson and A. Robinson. 1997. Language model adapta-
tion using mixtures and an exponentially decaying cache. In
Proc. ICASSP, pages II:799–802.
J. Godfrey, E. Holliman, and J. McDaniel. 1992. Switchboard:
telephone speech corpus for research and development. In
Proc. ICASSP, pages I:517–520.
R. Iyer and M. Ostendorf. 1997. Transforming out-of-domain
estimates to improve in-domain language models. In Proc.
Eurospeech, volume 4, pages 1975–1978.
R. Iyer and M. Ostendorf. 1999. Relevance weighting for
combining multi-domain data for n-gram language model-
ing. Computer Speech and Language, 13(3):267–282.
D. Klakow. 2000. Selecting articles from the language model
training corpus. In Proc. ICASSP, pages III:1695–1698.
F. Liu et al. 1995. IBM Switchboard progress and evaluation
site report. In LVCSR Workshop, Gaithersburg, MD. Na-
tional Institute of Standards and Technology.
M. Mahajan, D. Beeferman, and D. Huang. 1999. Improved
topic-dependent language modeling using information re-
trieval techniques. In Proc. ICASSP, pages I:541–544.
S. Martin et al. 1997. Adaptive topic-dependent language
modeling using word-based varigrams. In Proc. Eurospeech,
pages 3:1447–1450.
N. Morgan et al. 2001. The meeting project at ICSI. In Proc.
Conf. on Human Language Technology, pages 246–252.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proc. Empirical Methods in Natural Language Pro-
cessing Conference, pages 133–141.
K. Ries. 1997. A class based approach to domain adaptation
and constraint integration for empirical m-gram models. In
Proc. Eurospeech, pages 4:1983–1986.
A. Rudnicky. 1995. Language modeling with limited domain
data. In Proc. ARPA Spoken Language Technology Work-
shop, pages 66–69.
S. Schwarm and M. Ostendorf. 2002. Text normalization with
varied data sources for conversational speech language mod-
eling. In Proc. ICASSP, pages I:789–792.
R. Sproat et al. 2001. Normalization of non-standard words.
Computer Speech and Language, 15(3):287–333.
A. Stolcke et al. 2000. The SRI March 2000 Hub-5 conver-
sational speech transcription system. In Proc. NIST Speech
Transcription Workshop.
A. Stolcke. 1998. Entropy-based pruning of backoff language
models. In Proc. DARPA Broadcast News Transcription and
Understanding Workshop, pages 270–274.
M. Weintraub et al. 1996. LM95 Project Report: Fast training
and portability. Technical Report 1, Center for Language and
Speech Processing, Johns Hopkins University, Baltimore.
X. Zhu and R. Rosenfeld. 2001. Improving trigram language
modeling with the world wide web. In Proc. ICASSP, pages
I:533–536.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788585">
<title confidence="0.9997665">Getting More Mileage from Web Text Sources for Speech Language Modeling using Class-Dependent Mixtures</title>
<author confidence="0.999792">Ivan Bulyko</author>
<author confidence="0.999792">Mari Ostendorf Andreas Stolcke</author>
<affiliation confidence="0.8963005">Department of Electrical Engineering SRI International University of Washington, Seattle, WA 98195. Menlo Park, CA 94025.</affiliation>
<email confidence="0.999387">stolcke@speech.sri.com</email>
<abstract confidence="0.999578111111111">Sources of training data suitable for language modeling of conversational speech are limited. In this paper, we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task, but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation ofN-grams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Exploiting both local and global constraints for multispan statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>677--680</pages>
<contexts>
<context position="1837" citStr="Bellegarda, 1998" startWordPosition="271" endWordPosition="273">hods have been developed for the purpose of language model adaptation, i.e. the adaptation of an existing model to new topics, domains, or tasks for which little or no training material may be available. Since out-of-domain data can contain relevant as well as irrelevant information, various methods are used to identify the most relevant portions of the out-of-domain data prior to combination. Past work on pre-selection has been based on word frequency counts (Rudnicky, 1995), probability (or perplexity) of word or part-of-speech sequences (Iyer and Ostendorf, 1999), latent semantic analysis (Bellegarda, 1998), and information retrieval techniques (Mahajan et al., 1999; Iyer and Ostendorf, 1999). Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data (Clarkson and Robinson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models. The techn</context>
</contexts>
<marker>Bellegarda, 1998</marker>
<rawString>J. Bellegarda. 1998. Exploiting both local and global constraints for multispan statistical language modeling. In Proc. ICASSP, pages II:677–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>R Miller</author>
</authors>
<title>Just-in-time language modeling.</title>
<date>1998</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>705--708</pages>
<contexts>
<context position="3044" citStr="Berger and Miller, 1998" startWordPosition="464" endWordPosition="467">models. The technique was reported by IBM in 1995 (Liu et al, 1995), and has been used by many sites since then. An alternative approach involves decomposition of the language model into a class n-gram for interpolation (Iyer and Ostendorf, 1997; Ries, 1997), allowing content words to be interpolated with different weights than filled pauses, for example, which gives an improvement over standard mixture modeling for conversational speech. Recently researchers have turned to the World Wide Web as an additional source of training data for language modeling. For “just-in-time” language modeling (Berger and Miller, 1998), adaptation data is obtained by submitting words from initial hypotheses of user utterances as queries to a web search engine. Their queries, however, treated words as individual tokens and ignored function words. Such a search strategy typically generates text of a non-conversational style, hence not ideally suited for ASR. In (Zhu and Rosenfeld, 2001), instead of downloading the actual web pages, the authors retrieved Ngram counts provided by the search engine. Such an approach generates valuable statistics but limits the set of N-grams to ones occurring in the baseline model. In this paper</context>
</contexts>
<marker>Berger, Miller, 1998</marker>
<rawString>A. Berger and R. Miller. 1998. Just-in-time language modeling. In Proc. ICASSP, pages II:705–708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>A Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>799--802</pages>
<contexts>
<context position="2053" citStr="Clarkson and Robinson, 1997" startWordPosition="302" endWordPosition="306">nce out-of-domain data can contain relevant as well as irrelevant information, various methods are used to identify the most relevant portions of the out-of-domain data prior to combination. Past work on pre-selection has been based on word frequency counts (Rudnicky, 1995), probability (or perplexity) of word or part-of-speech sequences (Iyer and Ostendorf, 1999), latent semantic analysis (Bellegarda, 1998), and information retrieval techniques (Mahajan et al., 1999; Iyer and Ostendorf, 1999). Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data (Clarkson and Robinson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models. The technique was reported by IBM in 1995 (Liu et al, 1995), and has been used by many sites since then. An alternative approach involves decomposition of the language model into a class n-gram for interpolation (Iyer and Ost</context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>P. Clarkson and A. Robinson. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In Proc. ICASSP, pages II:799–802.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>Switchboard: telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="7473" citStr="Godfrey et al., 1992" startWordPosition="1201" endWordPosition="1204">eling (Iyer and Ostendorf, 1997), and at the same time target conversational speech since the top 100 words cover 70% of tokens in Switchboard training corpus. Combining several N-grams can produce a model with a very large number of parameters, which is costly in decoding. In such cases N-grams are typically pruned. Here we use entropy-based pruning (Stolcke, 1998) after mixing unpruned models, and reduce the model aggressively to about 15% of its original size. The same pruning parameters were applied to all models in our experiments. 4 Experiments We evaluated on two tasks: 1) Switchboard (Godfrey et al., 1992), specifically the HUB5 eval 2001 set having a total of 60K words spoken by 120 speakers, and 2) an ICSI Meeting recorder (Morgan et al, 2001) eval set having a total of 44K words spoken by 25 speakers. Both sets featured spontaneous conversational speech. There were 45K words of held-out data for each task. Text corpora of conversational telephone speech (CTS) available for training language models consisted of Switchboard, Callhome English, and Switchboardcellular, a total of 3 million words. In addition to that we used 150 million words of Broadcast News (BN) transcripts, and we collected 1</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J. McDaniel. 1992. Switchboard: telephone speech corpus for research and development. In Proc. ICASSP, pages I:517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Transforming out-of-domain estimates to improve in-domain language models.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<volume>4</volume>
<pages>1975--1978</pages>
<contexts>
<context position="2665" citStr="Iyer and Ostendorf, 1997" startWordPosition="408" endWordPosition="412">binson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models. The technique was reported by IBM in 1995 (Liu et al, 1995), and has been used by many sites since then. An alternative approach involves decomposition of the language model into a class n-gram for interpolation (Iyer and Ostendorf, 1997; Ries, 1997), allowing content words to be interpolated with different weights than filled pauses, for example, which gives an improvement over standard mixture modeling for conversational speech. Recently researchers have turned to the World Wide Web as an additional source of training data for language modeling. For “just-in-time” language modeling (Berger and Miller, 1998), adaptation data is obtained by submitting words from initial hypotheses of user utterances as queries to a web search engine. Their queries, however, treated words as individual tokens and ignored function words. Such a</context>
<context position="6884" citStr="Iyer and Ostendorf, 1997" startWordPosition="1103" endWordPosition="1106">ly letting the mixture weight depend on the context h. One approach is to use a mixture weight corresponding to the source posterior probability as(h) = p(s|h) (Weintraub et al, 1996). Here, we instead choose to let the weight vary as a function of the previous word class, i.e. p(wi|h) = EsES as(c(wi_1))ps(wi|h), where classes c(wi_1) are part-of-speech tags except for the 100 most frequent words which form their own individual classes. Such a scheme can generalize across domains by tapping into the syntactic structure (POS tags), already shown to be useful for cross-domain language modeling (Iyer and Ostendorf, 1997), and at the same time target conversational speech since the top 100 words cover 70% of tokens in Switchboard training corpus. Combining several N-grams can produce a model with a very large number of parameters, which is costly in decoding. In such cases N-grams are typically pruned. Here we use entropy-based pruning (Stolcke, 1998) after mixing unpruned models, and reduce the model aggressively to about 15% of its original size. The same pruning parameters were applied to all models in our experiments. 4 Experiments We evaluated on two tasks: 1) Switchboard (Godfrey et al., 1992), specifica</context>
</contexts>
<marker>Iyer, Ostendorf, 1997</marker>
<rawString>R. Iyer and M. Ostendorf. 1997. Transforming out-of-domain estimates to improve in-domain language models. In Proc. Eurospeech, volume 4, pages 1975–1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Relevance weighting for combining multi-domain data for n-gram language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="1792" citStr="Iyer and Ostendorf, 1999" startWordPosition="264" endWordPosition="267"> the amount of training data currently available. Methods have been developed for the purpose of language model adaptation, i.e. the adaptation of an existing model to new topics, domains, or tasks for which little or no training material may be available. Since out-of-domain data can contain relevant as well as irrelevant information, various methods are used to identify the most relevant portions of the out-of-domain data prior to combination. Past work on pre-selection has been based on word frequency counts (Rudnicky, 1995), probability (or perplexity) of word or part-of-speech sequences (Iyer and Ostendorf, 1999), latent semantic analysis (Bellegarda, 1998), and information retrieval techniques (Mahajan et al., 1999; Iyer and Ostendorf, 1999). Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data (Clarkson and Robinson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred</context>
</contexts>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>R. Iyer and M. Ostendorf. 1999. Relevance weighting for combining multi-domain data for n-gram language modeling. Computer Speech and Language, 13(3):267–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
</authors>
<title>Selecting articles from the language model training corpus.</title>
<date>2000</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>1695--1698</pages>
<contexts>
<context position="2170" citStr="Klakow, 2000" startWordPosition="325" endWordPosition="326">nt portions of the out-of-domain data prior to combination. Past work on pre-selection has been based on word frequency counts (Rudnicky, 1995), probability (or perplexity) of word or part-of-speech sequences (Iyer and Ostendorf, 1999), latent semantic analysis (Bellegarda, 1998), and information retrieval techniques (Mahajan et al., 1999; Iyer and Ostendorf, 1999). Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data (Clarkson and Robinson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models. The technique was reported by IBM in 1995 (Liu et al, 1995), and has been used by many sites since then. An alternative approach involves decomposition of the language model into a class n-gram for interpolation (Iyer and Ostendorf, 1997; Ries, 1997), allowing content words to be interpolated with different weights than filled pauses, for e</context>
</contexts>
<marker>Klakow, 2000</marker>
<rawString>D. Klakow. 2000. Selecting articles from the language model training corpus. In Proc. ICASSP, pages III:1695–1698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
</authors>
<title>IBM Switchboard progress and evaluation site report.</title>
<date>1995</date>
<booktitle>In LVCSR Workshop,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, MD.</location>
<marker>Liu, 1995</marker>
<rawString>F. Liu et al. 1995. IBM Switchboard progress and evaluation site report. In LVCSR Workshop, Gaithersburg, MD. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mahajan</author>
<author>D Beeferman</author>
<author>D Huang</author>
</authors>
<title>Improved topic-dependent language modeling using information retrieval techniques.</title>
<date>1999</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>541--544</pages>
<contexts>
<context position="1897" citStr="Mahajan et al., 1999" startWordPosition="278" endWordPosition="282">l adaptation, i.e. the adaptation of an existing model to new topics, domains, or tasks for which little or no training material may be available. Since out-of-domain data can contain relevant as well as irrelevant information, various methods are used to identify the most relevant portions of the out-of-domain data prior to combination. Past work on pre-selection has been based on word frequency counts (Rudnicky, 1995), probability (or perplexity) of word or part-of-speech sequences (Iyer and Ostendorf, 1999), latent semantic analysis (Bellegarda, 1998), and information retrieval techniques (Mahajan et al., 1999; Iyer and Ostendorf, 1999). Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data (Clarkson and Robinson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models. The technique was reported by IBM in 1995 (Liu et al, 1995), and has </context>
</contexts>
<marker>Mahajan, Beeferman, Huang, 1999</marker>
<rawString>M. Mahajan, D. Beeferman, and D. Huang. 1999. Improved topic-dependent language modeling using information retrieval techniques. In Proc. ICASSP, pages I:541–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Martin</author>
</authors>
<title>Adaptive topic-dependent language modeling using word-based varigrams.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>3--1447</pages>
<marker>Martin, 1997</marker>
<rawString>S. Martin et al. 1997. Adaptive topic-dependent language modeling using word-based varigrams. In Proc. Eurospeech, pages 3:1447–1450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan</author>
</authors>
<title>The meeting project at ICSI.</title>
<date>2001</date>
<booktitle>In Proc. Conf. on Human Language Technology,</booktitle>
<pages>246--252</pages>
<marker>Morgan, 2001</marker>
<rawString>N. Morgan et al. 2001. The meeting project at ICSI. In Proc. Conf. on Human Language Technology, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proc. Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>133--141</pages>
<contexts>
<context position="5699" citStr="Ratnaparkhi, 1996" startWordPosition="911" endWordPosition="913"> mikes like”, “I know that recognizer” for a meeting transcription task (Morgan et al, 2001). Web pages returned by Google mostly contained technical material related to topics similar to what was discussed in the meetings, e.g. “we were inspired by the weighted count scheme...”, “for our experiments we used the BellmanFord algorithm...”, etc. The retrieved web pages were filtered before their content could be used for language modeling. First we stripped the HTML tags and ignored any pages with a very high OOV rate. We then piped the text through a maximum entropy sentence boundary detector (Ratnaparkhi, 1996) and performed text normalization using NSW tools (Sproat et al, 2001). 3 Class-dependent Mixture of LMs Linear interpolation is a standard approach to combining language models, where the probability of a word wi given history h is computed as a linear combination of the corresponding N-gram probabilities from S different models: p(wi|h) = EsES asps(wi|h). Depending on how much adaptation data is available it may be beneficial to estimate a larger number of mixture weights as (more than one per data source) in order to handle source mismatch, specifically letting the mixture weight depend on </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tagger. In Proc. Empirical Methods in Natural Language Processing Conference, pages 133–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ries</author>
</authors>
<title>A class based approach to domain adaptation and constraint integration for empirical m-gram models.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>4--1983</pages>
<contexts>
<context position="2678" citStr="Ries, 1997" startWordPosition="413" endWordPosition="414">, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models. The technique was reported by IBM in 1995 (Liu et al, 1995), and has been used by many sites since then. An alternative approach involves decomposition of the language model into a class n-gram for interpolation (Iyer and Ostendorf, 1997; Ries, 1997), allowing content words to be interpolated with different weights than filled pauses, for example, which gives an improvement over standard mixture modeling for conversational speech. Recently researchers have turned to the World Wide Web as an additional source of training data for language modeling. For “just-in-time” language modeling (Berger and Miller, 1998), adaptation data is obtained by submitting words from initial hypotheses of user utterances as queries to a web search engine. Their queries, however, treated words as individual tokens and ignored function words. Such a search strat</context>
</contexts>
<marker>Ries, 1997</marker>
<rawString>K. Ries. 1997. A class based approach to domain adaptation and constraint integration for empirical m-gram models. In Proc. Eurospeech, pages 4:1983–1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
</authors>
<title>Language modeling with limited domain data. In</title>
<date>1995</date>
<booktitle>Proc. ARPA Spoken Language Technology Workshop,</booktitle>
<pages>66--69</pages>
<contexts>
<context position="1700" citStr="Rudnicky, 1995" startWordPosition="253" endWordPosition="254">ial, i.e. transcripts of conversational speech, is costly to produce, which limits the amount of training data currently available. Methods have been developed for the purpose of language model adaptation, i.e. the adaptation of an existing model to new topics, domains, or tasks for which little or no training material may be available. Since out-of-domain data can contain relevant as well as irrelevant information, various methods are used to identify the most relevant portions of the out-of-domain data prior to combination. Past work on pre-selection has been based on word frequency counts (Rudnicky, 1995), probability (or perplexity) of word or part-of-speech sequences (Iyer and Ostendorf, 1999), latent semantic analysis (Bellegarda, 1998), and information retrieval techniques (Mahajan et al., 1999; Iyer and Ostendorf, 1999). Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data (Clarkson and Robinson, 1997; Martin et al, 1997), and test set perplexity has been used to prune documents from a training corpus (Klakow, 2000). The most common method for using the additional text sources is to train separate language models on a small amount of in-domain</context>
</contexts>
<marker>Rudnicky, 1995</marker>
<rawString>A. Rudnicky. 1995. Language modeling with limited domain data. In Proc. ARPA Spoken Language Technology Workshop, pages 66–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Text normalization with varied data sources for conversational speech language modeling.</title>
<date>2002</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>789--792</pages>
<contexts>
<context position="4999" citStr="Schwarm and Ostendorf, 2002" startWordPosition="793" endWordPosition="796">t frequently in the switchboard training corpus, e.g. “I never thought I would”, “I would think so”, etc. We were searching for the exact match to one or more of these N-grams within the text of the web pages. Web pages returned by Google for the most part consisted of conversational style phrases like “we were friends but we don’t actually have a relationship” and “well I actually I I really haven’t seen her for years.” We used a slightly different search strategy when collecting topic-specific data. First we extended the baseline vocabulary with words from a small in-domain training corpus (Schwarm and Ostendorf, 2002), and then we used N-grams with these new words in our web queries, e.g. “wireless mikes like”, “I know that recognizer” for a meeting transcription task (Morgan et al, 2001). Web pages returned by Google mostly contained technical material related to topics similar to what was discussed in the meetings, e.g. “we were inspired by the weighted count scheme...”, “for our experiments we used the BellmanFord algorithm...”, etc. The retrieved web pages were filtered before their content could be used for language modeling. First we stripped the HTML tags and ignored any pages with a very high OOV r</context>
</contexts>
<marker>Schwarm, Ostendorf, 2002</marker>
<rawString>S. Schwarm and M. Ostendorf. 2002. Text normalization with varied data sources for conversational speech language modeling. In Proc. ICASSP, pages I:789–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>3</issue>
<marker>Sproat, 2001</marker>
<rawString>R. Sproat et al. 2001. Normalization of non-standard words. Computer Speech and Language, 15(3):287–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>The SRI</title>
<date>2000</date>
<booktitle>In Proc. NIST Speech Transcription Workshop.</booktitle>
<marker>Stolcke, 2000</marker>
<rawString>A. Stolcke et al. 2000. The SRI March 2000 Hub-5 conversational speech transcription system. In Proc. NIST Speech Transcription Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="7220" citStr="Stolcke, 1998" startWordPosition="1160" endWordPosition="1161">f-speech tags except for the 100 most frequent words which form their own individual classes. Such a scheme can generalize across domains by tapping into the syntactic structure (POS tags), already shown to be useful for cross-domain language modeling (Iyer and Ostendorf, 1997), and at the same time target conversational speech since the top 100 words cover 70% of tokens in Switchboard training corpus. Combining several N-grams can produce a model with a very large number of parameters, which is costly in decoding. In such cases N-grams are typically pruned. Here we use entropy-based pruning (Stolcke, 1998) after mixing unpruned models, and reduce the model aggressively to about 15% of its original size. The same pruning parameters were applied to all models in our experiments. 4 Experiments We evaluated on two tasks: 1) Switchboard (Godfrey et al., 1992), specifically the HUB5 eval 2001 set having a total of 60K words spoken by 120 speakers, and 2) an ICSI Meeting recorder (Morgan et al, 2001) eval set having a total of 44K words spoken by 25 speakers. Both sets featured spontaneous conversational speech. There were 45K words of held-out data for each task. Text corpora of conversational teleph</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>A. Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Weintraub</author>
</authors>
<title>LM95 Project Report: Fast training and portability.</title>
<date>1996</date>
<tech>Technical Report 1,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University,</institution>
<location>Baltimore.</location>
<marker>Weintraub, 1996</marker>
<rawString>M. Weintraub et al. 1996. LM95 Project Report: Fast training and portability. Technical Report 1, Center for Language and Speech Processing, Johns Hopkins University, Baltimore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>R Rosenfeld</author>
</authors>
<title>Improving trigram language modeling with the world wide web. In</title>
<date>2001</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>533--536</pages>
<contexts>
<context position="3400" citStr="Zhu and Rosenfeld, 2001" startWordPosition="519" endWordPosition="522">, which gives an improvement over standard mixture modeling for conversational speech. Recently researchers have turned to the World Wide Web as an additional source of training data for language modeling. For “just-in-time” language modeling (Berger and Miller, 1998), adaptation data is obtained by submitting words from initial hypotheses of user utterances as queries to a web search engine. Their queries, however, treated words as individual tokens and ignored function words. Such a search strategy typically generates text of a non-conversational style, hence not ideally suited for ASR. In (Zhu and Rosenfeld, 2001), instead of downloading the actual web pages, the authors retrieved Ngram counts provided by the search engine. Such an approach generates valuable statistics but limits the set of N-grams to ones occurring in the baseline model. In this paper, we present an approach to extracting additional training data from the web by searching for text that is better matched to a conversational speaking style. We also show how we can make better use of this new data by applying class-dependent interpolation. 2 Collecting Text from the Web The amount of text available on the web is enormous (over 3 billion</context>
</contexts>
<marker>Zhu, Rosenfeld, 2001</marker>
<rawString>X. Zhu and R. Rosenfeld. 2001. Improving trigram language modeling with the world wide web. In Proc. ICASSP, pages I:533–536.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>