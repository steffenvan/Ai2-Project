<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009259">
<title confidence="0.981298">
Learning Alignments and Leveraging Natural Logic
</title>
<author confidence="0.974344666666667">
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
</author>
<affiliation confidence="0.9840805">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.930531">
Stanford, CA 94305
</address>
<email confidence="0.998705">
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975916666667">
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999991">
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
</bodyText>
<sectionHeader confidence="0.982013" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.999840904761905">
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
</bodyText>
<page confidence="0.98236">
165
</page>
<bodyText confidence="0.906148">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165–170,
Prague, June 2007. c�2007 Association for Computational Linguistics
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al.
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
</bodyText>
<sectionHeader confidence="0.997294" genericHeader="method">
3 Alignment Model
</sectionHeader>
<bodyText confidence="0.999701111111111">
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
</bodyText>
<subsectionHeader confidence="0.997962">
3.1 Manual Alignment Annotation
</subsectionHeader>
<bodyText confidence="0.99997223076923">
While work such as Raina et al. (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al. (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
</bodyText>
<subsectionHeader confidence="0.999411">
3.2 Improving Alignment Search
</subsectionHeader>
<bodyText confidence="0.999945333333333">
In order to find “good” alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full dataset D, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a: n(h) H n(t) U
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
</bodyText>
<equation confidence="0.9947615">
�s(t, h, a) _ sw(hi, a(hi))
hiEn(h)
+ � se((hi, hj), (a(hi), a(hj)))
(hi,hj)Ee(h)
</equation>
<bodyText confidence="0.999980956521739">
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)1
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj. Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
</bodyText>
<page confidence="0.993586">
166
</page>
<bodyText confidence="0.9998315">
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
</bodyText>
<subsectionHeader confidence="0.99958">
3.3 Learning Alignment Models
</subsectionHeader>
<bodyText confidence="0.999983142857143">
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al., 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
</bodyText>
<equation confidence="0.966665">
sw(hi, tj) = θw - f(hi, tj), and
se((hi, hj), (tk, t`)) = θe - f((hi, hj), (tk, t`)).
</equation>
<bodyText confidence="0.99959659375">
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). The MIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
</bodyText>
<tableCaption confidence="0.9972165">
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
</tableCaption>
<bodyText confidence="0.997057333333333">
RTE3 dev alignment data. We believe this is due
to a larger proportion of “irrelevant” and “relation”
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
</bodyText>
<sectionHeader confidence="0.999017" genericHeader="method">
4 Coreference
</sectionHeader>
<bodyText confidence="0.995146857142857">
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as “long,” with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first “he” as referring to “Yunus”
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of “microcredit.”
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age’s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p &lt; 0.1, McNemar’s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
</bodyText>
<footnote confidence="0.890926">
1http://opennlp.sourceforge.net/
</footnote>
<table confidence="0.542540666666667">
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron
MIRA
4675 271
4775 283
</table>
<page confidence="0.980892">
167
</page>
<bodyText confidence="0.9995434">
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
</bodyText>
<sectionHeader confidence="0.974545" genericHeader="method">
5 Semgrex Language
</sectionHeader>
<bodyText confidence="0.9999612">
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al.,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
</bodyText>
<subsectionHeader confidence="0.993796">
5.1 Semgrex Features
</subsectionHeader>
<bodyText confidence="0.999928">
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/ˆNN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} &lt;nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
</bodyText>
<subsectionHeader confidence="0.993677">
5.2 Entailment Patterns
</subsectionHeader>
<bodyText confidence="0.997314333333333">
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
</bodyText>
<figure confidence="0.93557">
Semgrex Relations
Symbol #Description
{A} &gt;reln {B} A is the governor of a reln relation
with B
{A} &lt;reln {B} A is the dependent of a reln relation
with B
{A} &gt;&gt;reln {B} A dominates a node that is the
governor of a reln relation with B
{A} &lt;&lt;reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
</figure>
<figureCaption confidence="0.999967">
Figure 1: Semgrex relations between nodes.
</figureCaption>
<bodyText confidence="0.854244166666667">
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton’s wife Hillary was in Wichita today, continuing
her campaign.
</bodyText>
<equation confidence="0.742795">
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
&lt;nsubjpass ({word:married} &gt;pp to {}=2))
@ ({} &gt;poss ({lemma:/wife/} &gt;appos {}=3))
</equation>
<bodyText confidence="0.999713888888889">
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
</bodyText>
<subsectionHeader confidence="0.99941">
5.3 Range of Application
</subsectionHeader>
<bodyText confidence="0.999986571428572">
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system’s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
</bodyText>
<listItem confidence="0.999956666666666">
• Work: works for, holds the position of
• Location: lives in, is located in
• Relative: wife/husband of, are relatives
• Membership: is an employee of, is part of
• Business: is a partner of, owns
• Base: is based in, headquarters in
</listItem>
<bodyText confidence="0.878126">
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
</bodyText>
<page confidence="0.997998">
168
</page>
<sectionHeader confidence="0.987601" genericHeader="method">
6 Natural Logic
</sectionHeader>
<bodyText confidence="0.999976717391304">
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (S´anchez Va-
lencia, 1995).
We define an entailment relation C between
nouns (hammer C tool), adjectives (deafening C
loud), verbs (sprint C run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris C dance
in France, since tango C dance and in Paris C in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn’t sing C didn’t yodel), restrictive quanti-
fiers (few beetles C few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
</bodyText>
<table confidence="0.930615833333333">
relation symbol in terms of C RTE
equivalent p = h p C h, h C p yes
forward p ❁ h p C h, h C= p yes
reverse p ❂ h h C p, p C= h no
independent p # h p C= h, h C= p no
exclusive p  |h p C -,h, h C -,p no
</table>
<tableCaption confidence="0.996922">
Table 2: NatLog’s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
</tableCaption>
<bodyText confidence="0.999274342105263">
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields ❁, substitution of a hypernym in a downward-
monotone context yields ❂, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= o r) - r, but (# o r) -
#. ❁ and ❂ are transitive, but (❁ o ❂) - #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either ❁ or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
</bodyText>
<page confidence="0.994133">
169
</page>
<table confidence="0.9982584">
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in The French railway company is called SNCF. yes
the project.
601 NUCOR has pioneered a giant mini-mill in which steel Nucor has pioneered the first mini-mill. no
is poured into continuous casting machines.
</table>
<tableCaption confidence="0.937039">
Table 4: Illustrative examples from the RTE3 test suite
</tableCaption>
<table confidence="0.986765071428572">
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
</table>
<tableCaption confidence="0.943946333333333">
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
</tableCaption>
<bodyText confidence="0.63245">
ning, 2007) for more details on NatLog.
</bodyText>
<sectionHeader confidence="0.989847" genericHeader="method">
7 System Results
</sectionHeader>
<bodyText confidence="0.999983344827586">
Our core system makes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, −x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p &lt; 0.01, McNemar’s test, 2-tailed).
The gain cannot be fully attributed to NatLog’s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
</bodyText>
<sectionHeader confidence="0.997662" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.932243333333333">
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)’s
AQUAINT Phase III Program.
</bodyText>
<sectionHeader confidence="0.995984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999115666666667">
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn’t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC’s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099–1105.
Victor S´anchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258–285.
</reference>
<page confidence="0.997424">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579200">
<title confidence="0.999768">Learning Alignments and Leveraging Natural Logic</title>
<author confidence="0.969266">Nathanael Chambers</author>
<author confidence="0.969266">Daniel Cer</author>
<author confidence="0.969266">Trond Grenager</author>
<author confidence="0.969266">David Hall</author>
<author confidence="0.969266">Chloe Bill MacCartney</author>
<author confidence="0.969266">Marie-Catherine de_Marneffe</author>
<author confidence="0.969266">Daniel Eric Yeh</author>
<author confidence="0.969266">D Christopher</author>
<affiliation confidence="0.8359215">Computer Science Stanford</affiliation>
<address confidence="0.998782">Stanford, CA 94305</address>
<abstract confidence="0.995728846153846">We describe an approach to textual inference that improves alignments at both the typed dependency level and at a deeper semantic level. We present a machine learning approach to alignment scoring, a stochastic search procedure, and a new tool that finds deeper semantic alignments, allowing rapid development of semantic features over the aligned graphs. Further, we describe a complementary semantic component based on natural logic, which shows an added gain of 3.13% accuracy on the RTE3 test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>When logical inference helps determining textual entailment (and when it doesn’t).</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="16371" citStr="Bos and Markert, 2006" startWordPosition="2670" endWordPosition="2673">g the following: • Work: works for, holds the position of • Location: lives in, is located in • Relative: wife/husband of, are relatives • Membership: is an employee of, is part of • Business: is a partner of, owns • Base: is based in, headquarters in These relations make up at least 7% of the data, suggesting utility from capturing other relations. 168 6 Natural Logic We developed a computational model of natural logic, the NatLog system, as another inference engine for our RTE system. NatLog complements our core broad-coverage system by trading lower recall for higher precision, similar to (Bos and Markert, 2006). Natural logic avoids difficulties with translating natural language into first-order logic (FOL) by forgoing logical notation and model theory in favor of natural language. Proofs are expressed as incremental edits to natural language expressions. Edits represent conceptual contractions and expansions, with truth preservation specified natural logic. For further details, we refer the reader to (S´anchez Valencia, 1995). We define an entailment relation C between nouns (hammer C tool), adjectives (deafening C loud), verbs (sprint C run), modifiers, connectives and quantifiers. In ordinary (up</context>
<context position="20263" citStr="Bos and Markert, 2006" startWordPosition="3304" endWordPosition="3307">extraction. Most pairs have large edit distances, and more atomic edits means a greater chance of errors propagating to the final output: given the entailment composition rules, the system can answer yes only if all atomic-level predictions are either ❁ or =. Instead, we hope to make reliable predictions on a subset of the RTE problems. Table 3 shows NatLog performance on RTE3. It makes positive predictions on few problems (18% on development set, 24% on test), but achieves good precision relative to our RTE system (76% and 68%, respectively). For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a precision of 76% on RTE2, but made a positive prediction in only 4% of cases. This high precision suggests that superior performance can be achieved by hybridizing NatLog with our core RTE system. The reader is referred to (MacCartney and Man169 ID Premise(s) Hypothesis Answer 518 The French railway company SNCF is cooperating in The French railway company is called SNCF. yes the project. 601 NUCOR has pioneered a giant mini-mill in which steel Nucor has pioneered the first mini-mill. no is poured into continuous casting machines. Table 4: Illustrative examples from the RTE3 test s</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos and Katja Markert. 2006. When logical inference helps determining textual entailment (and when it doesn’t). In Proceedings of the Second PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002.</booktitle>
<contexts>
<context position="9303" citStr="Collins, 2002" startWordPosition="1483" endWordPosition="1484">h iteration step over the training set. For each datum, they use the current weight vector to make a prediction which is compared to the correct label. The weight vector is updated as a function of the difference. We compared two different update rules: the perceptron update and the MIRA update. In the perceptron update, for an incorrect prediction, the weight vector is modified by adding a multiple of the difference between the feature vector of the correct label and the feature vector of the predicted label. We use the adaptation of this algorithm to structure prediction, first proposed by (Collins, 2002). The MIRA update is a proposed improvement that attempts to make the minimal modification to the weight vector such that the score of the incorrect prediction for the example is lower than the score of the correct label (Crammer and Singer, 2001). We compare the performance of the perceptron and MIRA algorithms on 10-fold cross-validation on the RTE2 dev dataset. Both algorithms improve with each pass over the dataset. Most improvement is within the first five passes. Table 1 shows runs for both algorithms over 10 passes through the dataset. MIRA consistently outperforms perceptron learning. </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2001</date>
<booktitle>In Proceedings of COLT-2001.</booktitle>
<contexts>
<context position="9550" citStr="Crammer and Singer, 2001" startWordPosition="1523" endWordPosition="1526">ent update rules: the perceptron update and the MIRA update. In the perceptron update, for an incorrect prediction, the weight vector is modified by adding a multiple of the difference between the feature vector of the correct label and the feature vector of the predicted label. We use the adaptation of this algorithm to structure prediction, first proposed by (Collins, 2002). The MIRA update is a proposed improvement that attempts to make the minimal modification to the weight vector such that the score of the incorrect prediction for the example is lower than the score of the correct label (Crammer and Singer, 2001). We compare the performance of the perceptron and MIRA algorithms on 10-fold cross-validation on the RTE2 dev dataset. Both algorithms improve with each pass over the dataset. Most improvement is within the first five passes. Table 1 shows runs for both algorithms over 10 passes through the dataset. MIRA consistently outperforms perceptron learning. Moreover, scoring alignments based on the learned weights marginally outperforms our handconstructed scoring function by 1.7% absolute. A puzzling problem is that our overall performance decreased 0.87% with the addition of Table 1: Perceptron and</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Koby Crammer and Yoram Singer. 2001. Ultraconservative online algorithms for multiclass problems. In Proceedings of COLT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Second Pascal RTE Challenge Workshop.</booktitle>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher D. Manning. 2006a. Learning to distinguish valid textual entailments. In Second Pascal RTE Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In 5th Int. Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006b. Generating typed dependency parses from phrase structure parses. In 5th Int. Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCC’s GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="4515" citStr="Hickl et al. (2006)" startWordPosition="669" endWordPosition="672">construction of manually aligned data which enables automatic learning of alignment models, and effectively decouples the alignment and inference development efforts, (2) the development of new search procedures for finding high-quality alignments, and (3) the use of machine learning techniques to automatically learn the parameters of alignment scoring models. 3.1 Manual Alignment Annotation While work such as Raina et al. (2005) has tried to learn feature alignment weights by credit assignment backward from whether an item is answered correctly, this can be very difficult, and here we follow Hickl et al. (2006) in using supervised goldstandard alignments, which help us to evaluate and improve alignment and inference independently. We built a web-based tool that allows annotators to mark semantic relationships between text and hypothesis words. A table with the hypothesis words on one axis and the text on the other allows relationships to be marked in the corresponding table cell with one of four options. These relationships include text to hypothesis entailment, hypothesis to text entailment, synonymy, and antonymy. Examples of entailment (from the RTE 2005 dataset) include pairs such as drinking/co</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCC’s GROUNDHOG system. In Proceedings of the Second PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="13017" citStr="Levy and Andrew, 2006" startWordPosition="2096" endWordPosition="2099">age A core part of an entailment system is the ability to find semantically equivalent patterns in text. Previously, we wrote tedious graph traversal code by hand for each desired pattern. As a remedy, we wrote Semgrex, a pattern language for dependency graphs. We use Semgrex atop the typed dependencies from the Stanford Parser (de Marneffe et al., 2006b), as aligned in the alignment phase, to identify both semantic patterns in a single text and over two aligned pieces of text. The syntax of the language was modeled after tgrep/Tregex, query languages used to find syntactic patterns in trees (Levy and Andrew, 2006). This speeds up the process of graph search and reduces errors that occur in complicated traversal code. 5.1 Semgrex Features Rather than providing regular expression matching of atomic tree labels, as in most tree pattern languages, Semgrex represents nodes as a (nonrecursive) attribute-value matrix. It then uses regular expressions for subsets of attribute values. For example, {word:run;tag:/ˆNN/} refers to any node that has a value run for the attribute word and a tag that starts with NN, while {} refers to any node in the graph. However, the most important part of Semgrex is that it allow</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of the Fifth International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In ACL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<marker>MacCartney, Manning, 2007</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2007. Natural logic for textual inference. In ACL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning. In AAAI</title>
<date>2005</date>
<pages>1099--1105</pages>
<contexts>
<context position="4329" citStr="Raina et al. (2005)" startWordPosition="636" endWordPosition="639">es natural logic is also used to make the final entailment decisions, described in section 6. 3 Alignment Model We examine three tasks undertaken to improve the alignment phase: (1) the construction of manually aligned data which enables automatic learning of alignment models, and effectively decouples the alignment and inference development efforts, (2) the development of new search procedures for finding high-quality alignments, and (3) the use of machine learning techniques to automatically learn the parameters of alignment scoring models. 3.1 Manual Alignment Annotation While work such as Raina et al. (2005) has tried to learn feature alignment weights by credit assignment backward from whether an item is answered correctly, this can be very difficult, and here we follow Hickl et al. (2006) in using supervised goldstandard alignments, which help us to evaluate and improve alignment and inference independently. We built a web-based tool that allows annotators to mark semantic relationships between text and hypothesis words. A table with the hypothesis words on one axis and the text on the other allows relationships to be marked in the corresponding table cell with one of four options. These relati</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via learning and abductive reasoning. In AAAI 2005, pages 1099–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S´anchez Valencia</author>
</authors>
<date>1995</date>
<booktitle>Parsing-driven inference: Natural logic. Linguistic Analysis,</booktitle>
<pages>25--258</pages>
<contexts>
<context position="16795" citStr="Valencia, 1995" startWordPosition="2731" endWordPosition="2733">tLog system, as another inference engine for our RTE system. NatLog complements our core broad-coverage system by trading lower recall for higher precision, similar to (Bos and Markert, 2006). Natural logic avoids difficulties with translating natural language into first-order logic (FOL) by forgoing logical notation and model theory in favor of natural language. Proofs are expressed as incremental edits to natural language expressions. Edits represent conceptual contractions and expansions, with truth preservation specified natural logic. For further details, we refer the reader to (S´anchez Valencia, 1995). We define an entailment relation C between nouns (hammer C tool), adjectives (deafening C loud), verbs (sprint C run), modifiers, connectives and quantifiers. In ordinary (upward-monotone) contexts, the entailment relation between compound expressions mirrors the entailment relations between their parts. Thus tango in Paris C dance in France, since tango C dance and in Paris C in France. However, many linguistic constructions create downward-monotone contexts, including negation (didn’t sing C didn’t yodel), restrictive quantifiers (few beetles C few insects) and many others. NatLog uses a t</context>
</contexts>
<marker>Valencia, 1995</marker>
<rawString>Victor S´anchez Valencia. 1995. Parsing-driven inference: Natural logic. Linguistic Analysis, 25:258–285.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>