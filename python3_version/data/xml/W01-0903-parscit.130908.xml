<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057008">
<title confidence="0.999046">
Usability Evaluation in
Spoken Language Dialogue Systems
</title>
<author confidence="0.952028">
Laila Dybkjær and Niels Ole Bernsen
</author>
<affiliation confidence="0.963396">
Natural Interactive Systems Laboratory, University of Southern Denmark
</affiliation>
<address confidence="0.913412">
Science Park 10, 5230 Odense M, Denmark
</address>
<email confidence="0.99592">
laila@nis.sdu.dk, nob@nis.sdu.dk
</email>
<sectionHeader confidence="0.993783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974272727273">
The paper first addresses a series of
issues basic to evaluating the usability
of spoken language dialogue systems,
including types and purpose of
evaluation, when to evaluate and
which methods to use, user
involvement, how to evaluate and
what to evaluate. We then go on to
present and discuss a comprehensive
set of usability evaluation criteria for
spoken language dialogue systems.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996927566037736">
Usability is becoming an increasingly important
issue in the development and evaluation of
spoken language dialogue systems (SLDSs).
Many companies would pay large amounts to
know exactly which features make SLDSs
attractive to users and how to evaluate whether
their system has these features. In spite of its
key importance far less resources have been
invested in the usability aspect of SLDSs over
the years than in SLDS component technologies.
The usability aspect has often been neglected in
SLDS development and evaluation and there has
been surprisingly little research in important
user-related issues, such as user reactions to
SLDSs in the field, users’ linguistic behaviour,
or the main factors which determine overall user
satisfaction. However, there now seems to be
growing recognition that usability is as
important as, and partly independent of, the
technical quality of any SLDS component and
that quality usability constitutes an important
competitive parameter.
Most of today&apos;s SLDSs are walk-up-and-use
systems for shared-goal tasks. Usability of walk-
up-and-use systems is of utmost importance,
since users of such systems cannot be expected
to undertake extensive training about the system
or to read the user manual. Help must be
available online and needed as infrequently as
possible.
There is at present no systematic
understanding of which factors must be taken
into account to optimise SLDS usability and
thus also no consensus as to which usability
evaluation criteria to use. Ideally, such an
understanding should be comprehensive, i.e.
include all major usability perspectives on
SLDSs, and exhaustive, i.e. describe each
perspective as it pertains to the detailed
development and evaluation of any possible
SLDS. This paper addresses the aspect of
comprehensiveness by proposing a set of
usability evaluation criteria. The criteria are
derived from a set of usability issues that have
resulted from a decomposition of the complex
space of SLDS usability best practice.
The present paper focuses on walk-up-and-
use SLDSs for shared-goal tasks and reviews
substantial parts of the authors’ work as
presented in, e.g., (Dybkjær and Bernsen 2000).
For additional examples the reader is referred to
this paper. Due to space limitations few
examples are included in the present paper.
In the following we first briefly address types
and purpose of evaluation (Section 2), when to
evaluate and which methods to use (Section 3),
user involvement (Section 4), and how to
evaluate (Section 5). Section 6 presents the
proposed set of evaluation criteria and discusses
the usability issues behind these. Section 7
concludes the paper.
2 Types and purpose of evaluation
Evaluation can be quantitative or qualitative,
subjective or objective. Quantitative evaluation
consists in quantifying some parameter through
an independently meaningful number,
percentage etc. which in principle allows
comparison across systems. Qualitative
evaluation consists in estimating or judging
some parameter by reference to expert standards
and rules. Subjective evaluation consists in
judging some parameter by reference to users’
opinions. Objective evaluation produces subject-
independent parameter assessment. Ideally, we
would like to obtain quantitative and objective
progress evaluation scores for usability which
can be objectively compared to scores obtained
from evaluation of other SLDSs. This is what
has been attempted in the PARADISE
framework based on the claim that task success
and dialogue cost are potentially relevant
contributors to user satisfaction (Walker,
Litman, Kamm and Abella 1997). However,
many important usability issues cannot be
subjected to quantification and objective expert
evaluation is sometimes highly uncertain or non-
existent.
The purpose of evaluation may be to detect
and analyse design and implementation errors
(diagnostic evaluation), measure SLDS
performance in terms of a set of quantitative
and/or qualitative parameters (performance
evaluation), or evaluate how well the system fits
its purpose and meets actual user needs and
expectations (adequacy evaluation), cf.
(Hirschmann and Thompson 1996, Gibbon,
Moore. and Winski 1997, Bernsen et al. 1998).
The latter purpose is the more important one
from a usability point of view although the
others are relevant as well. Which type of
evaluation to use and for which purpose,
depends on the evaluation criterion which is
being applied (see below). Other general
references to natural language systems
evaluation are (EAGLES 1996, Gaizauskas
1997, Sparck Jones and Galliers 1996).
</bodyText>
<sectionHeader confidence="0.9257625" genericHeader="method">
3 When to evaluate and methods to
use
</sectionHeader>
<bodyText confidence="0.999924028571429">
Usability evaluation should start as early as
possible and continue throughout development.
In general, the earlier design errors are being
identified, the easier and cheaper it is to correct
them. Different methods of evaluation may have
to be applied for evaluating a particular
parameter depending on the phase in the
lifecycle in which evaluation takes place. Early
design evaluation can be based on mock-up
experiments with users and on design walk-
throughs. Wizard of Oz simulations with
representative task scenarios can provide
valuable evaluation data. When the system has
been implemented, controlled scenario-based
tests with representative users and field tests can
be used. Recorded dialogues with the
(simulated) system should be carefully analysed
for indications that the users have problems or
expectations which exceed the capabilities of the
system. Human-system interaction data should
be complemented by interviews and
questionnaires to enable assessment of user
satisfaction. If users are interacting with the
prototype on the basis of scenarios, there are at
least two issues to be aware of. Firstly, scenarios
should be designed to avoid priming the users
on how to interact with the system. Secondly,
sub-tasks covered by the scenarios will not
necessarily be representative of the sub-tasks
which real users (not using scenarios) would
expect the system to cover.
The final test of the system is often called the
acceptance test. It involves real users and must
satisfy the evaluation criteria defined as part of
the requirements specification.
</bodyText>
<sectionHeader confidence="0.994646" genericHeader="method">
4 User involvement
</sectionHeader>
<bodyText confidence="0.999955641025641">
In general, representative users from the target
user group(s) should be involved in evaluation
from early on. The developers themselves can
certainly discover many of the usability
problems with the early design and
implementation, especially when supported by
state-of-the-art usability standards, evaluation
criteria and design support tools. The problem is
that they know too well how to interact with the
system in order to avoid creating interaction
problems which the system cannot handle. For
the time being, there is no alternative to
involving the target users in all or most system
evaluation phases and for most usability
evaluation purposes. This is costly and complex
to do. However, the data analysis which is
crucial to benefiting from trials with the system,
is as necessary after trials with developers as it
is after trials with representative users. Even the
early involvement of representative users is no
guarantee that the system will ultimately
produce sufficient user satisfaction. For one
thing, the data distribution they generate may
not match the behaviour of the users of the
system, once installed. For another,
experimental user trials are different from real
situations of use in which time, money and trust
are really at stake. For these reasons, and
particularly when introducing SLDSs which are
innovative in some respect, it is necessary to
prepare and budget for field trials with the
implemented system as well as for the
subsequent data analysis and fine-tuning of the
system. Users who are “only” involved in a test
can be much more indifferent to, or more
positive towards, a system with poor usability
characteristics than real users who have
something to loose if the system lets them down
(Bernsen et al. 1998).
</bodyText>
<sectionHeader confidence="0.863161" genericHeader="method">
5 How to evaluate
</sectionHeader>
<bodyText confidence="0.999949153846154">
Evaluation, including usability evaluation, is
non-trivial and cannot be explained simply by
stating what to evaluate (cf. Section 6) and what
the developers’ options are. One of the most
difficult questions in evaluation probably is how
to do it properly. We have developed a template
which supports consistent and detailed
description of each evaluation criterion (Bernsen
and Dybkjær 2000). The template includes the
following issues: what is being evaluated (e.g.
feedback adequacy), the system part evaluated
(e.g. the dialogue manager), type of evaluation
(e.g. qualitative), method(s) of evaluation (e.g.
controlled user experiments), symptoms to look
for (e.g. user clarification questions), life cycle
phase(s) (e.g. simulation), importance of
evaluation (e.g. crucial), difficulty of evaluation
(e.g. easy), cost of evaluation (e.g. expensive),
and support tools (e.g. SMALTO), see
(www.disc2.dk/tools). The idea is that the
combined set of (i) design options for SLDS
usability, (ii) usability evaluation criteria, and
(iii) template-based characterisation of each
criterion, will provide developers with sufficient
information for proper evaluation of their
SLDSs.
</bodyText>
<sectionHeader confidence="0.961375" genericHeader="method">
6 What to evaluate
</sectionHeader>
<bodyText confidence="0.99999174">
In general terms, a usable SLDS must satisfy
user needs which are similar to those which
must be satisfied by other interactive systems.
The SLDS must be easy to understand and
interact with. Interaction should be smooth and
the user should feel in control throughout the
dialogue with the system. It is the task of the
SLDS developer to meet those user needs
considered as overall usability design goals.
However, SLDSs are very different from more
traditional interactive systems whose usability
aspects have been investigated for decades, such
as systems controlled through graphical user
interfaces involving screen, keyboard and
mouse. Perhaps the most important difference is
that speech is perceptually transient rather than
static. This means that the user must pick up the
output information provided by the system the
moment it is being provided or else miss it
altogether. Moreover, the user has no way of
inspecting the interface prior to interaction. If
the interface is not self-evident all the way
through the dialogue it must be learnt by trial-
and-error through repeated interaction, which is
unsatisfactory for the casual walk-up-and-use
user. Secondly, the processing (recognition,
language understanding, dialogue management)
of spoken input remains difficult to design and
error-prone in execution, which is why SLDSs
must be crafted with extreme care to ensure that
users do not produce spoken input which the
system is incapable of handling.
In the following we present a set of usability
evaluation criteria which are based on results
achieved in the European DISC project
(www.disc2.dk) on best practice in the
development and evaluation of SLDSs
(Failenschmid et al. 1999). Our claim is that
quality usability of SLDSs may be pursued by
focusing on a comprehensive set of 15 usability
issues which include all major usability
perspectives on SLDSs. These usability issues -
or a subset thereof, depending on how advanced
and complex the system is to be – should be
represented in the system specification and
should therefore also be reflected in a set of
evaluation criteria for the system which would
appear mandatory for evaluating the usability of
the SLDS. More details on the usability issues
can be found in (Dybkjær and Bernsen 2000).
</bodyText>
<subsectionHeader confidence="0.998863">
6.1 Modality appropriateness
</subsectionHeader>
<bodyText confidence="0.999975260869565">
The majority of task-oriented SLDSs have so far
used speech as the only input/output modality.
However, an increasing number of systems now
combine spoken input/output with other
modalities.
It is well-known that speech-only interaction
is not appropriate for all tasks and applications,
and the same is true for any particular modality
combination which includes speech input and
speech output. Few users would e.g. be happy to
speak aloud their pin code to the bank teller
machine in the street. The developers should
attempt to make sure that spoken input and
output, possibly combined with other
input/output modalities, is an appropriate
modality choice for the planned application. If
the chosen modalities are inappropriate, chances
are that the users either will not accept the
application or will refrain from using some of
the modalities it offers. Common sense,
experimentation and/or the use of the tool
SMALTO (www.disc2.dk/tools) may help the
developers in making the right modality choice.
</bodyText>
<subsectionHeader confidence="0.994237">
6.2 Input recognition adequacy
</subsectionHeader>
<bodyText confidence="0.999982666666667">
From the user’s point of view, good speech
recognition means that the system rarely gets the
user’s spoken input wrong or fails to recognise
what the user just said. Recognition success, as
perceived by the user, not only depends on
recogniser quality but also on how other parts of
the SLDS handle the user’s input. Good
recogniser quality nevertheless remains the key
factor in making users confident that the system
will successfully get what they say.
Walk-up-and-use systems may be used by
many different users in highly different
environments. The speech recogniser, therefore,
and depending on more specific information on
its intended users and environments of
interaction, must be trained to recognise a
variety of dialects and accents, speakers of
different gender, age and voice quality, speaking
with a low or a loud voice, in noisy or quiet
environments, and with varying channel quality.
Adequate information on users and
environments is essential input to the selection
and creation of training data. To assess the
quality of the system’s recognition capabilities
prior to running the full system, speech
recognition accuracy may be tested on the
recogniser with users from the target group(s).
</bodyText>
<subsectionHeader confidence="0.999788">
6.3 Naturalness of user speech
</subsectionHeader>
<bodyText confidence="0.999289366666667">
Speaking to an SLDS should feel as easy and
natural as possible. It does not help the user that
the system’s speech recognition is perfect in
principle if the input vocabulary and grammar
expected from the user are not the ones which
the user is likely to use and thus cannot be
understood. Depending on, e.g., the task and
users’ experience, what is “natural” input
language may vary considerably.
What is being experienced as natural input
speech is also highly relative to the system’s
output phrasing. For example, lengthy and/or
overly polite system utterances are likely to
invite similar linguistic user behaviour, thereby
burdening input recognition and understanding
unnecessarily. The system’s output language
thus should be used to control users’ input
language so that the latter becomes manageable
for the system whilst still feeling natural to the
user. If the minimal constraints imposed by the
task are satisfied and the system’s output
language adequately controls the user’s input
language, users may well feel that the dialogue
is natural even if they are not inclined to engage
in lengthy conversation.
Analysis of data from system simulations,
questionnaires and interviews is a useful tool for
obtaining information on users’ input language
and on what they perceive as being natural input
language.
</bodyText>
<subsectionHeader confidence="0.957133">
6.4 Output voice quality
</subsectionHeader>
<bodyText confidence="0.99995025">
From the user’s point of view, good SLDS
output voice quality means that the system’s
speech is clear and intelligible, does not demand
an extra listening effort, is not particularly noise
sensitive or distorted by clicks and other
extraneous sounds, has natural intonation and
prosody, uses an appropriate speaking rate, and
is pleasant to listen to (Karlsson 1999). Taken
together, these requirements are difficult to meet
today.
There are three main types of output speech:
recordings of entire system utterances,
concatenation of recorded words and phrases,
and text-to-speech (TTS). Concatenated speech
is the most frequently used type of speech in
today’s SLDSs. For walk-up-and-use systems in
particular, TTS may simply be too difficult to
understand for infrequent users while full
recordings are much too inflexible. Moreover,
too natural output speech, like full recordings,
may suggest to users that the system is far more
capable and human-like than it actually is,
encouraging them to address the system in a
way which is more conversational and talkative
than it can handle.
The type of output voice chosen is likely to
affect users’ perception of the system as a
whole. In particular, and together with the
quality of the speech output, the voice type has a
major influence on how pleasant users find the
“system’s voice”. Voice type includes features
such as male/female, deep/high voice, speaking
rate, and emotions.
In order to gather input on user preferences
with respect to the system’s output voice,
representative users of the system under
development may be asked to listen to different
“system voices” and provide feedback on which
one they prefer and what they like and dislike
about each of them.
</bodyText>
<subsectionHeader confidence="0.97654">
6.5 Output phrasing adequacy
</subsectionHeader>
<bodyText confidence="0.996012129032258">
Regardless of the topic, the system should
express itself co-operatively in order to
maximise the likelihood that the task is achieved
as smoothly and efficiently as possible. To
facilitate successful interaction, the contents of
the system’s output should be correct, relevant
and sufficiently informative without being over-
informative. Users have good reason for
dissatisfaction if the system provides false
information, e.g., if the database is not being
properly updated. Lack of relevance of system
output caused by, e.g., misrecognition, will
typically lead to meta-communication dialogue.
System output should be sufficiently
informative. Otherwise, misunderstandings may
occur which are only detected much later during
interaction, if at all, or which, at best, lead to
immediate requests for clarification by the user.
Conversely, the system should not provide too
much or overly verbose information. Users may
then e.g. become inattentive, try to take the
dialogue initiative, or become confused and
initiate clarification meta-communication.
The form of system expressions should be
clear and unambiguous, and language and, as far
as possible, terminology should be consistent
and familiar to the user (Bernsen et al. 1998).
Unclarity naturally leads to uncertainty and need
for clarification. So does ambiguity if detected
by the user. If undetected, as often happens, the
effects of ambiguity can be severe. If the user
unknowingly selects a non-intended meaning of
a word or phrase uttered by the system, all sorts
of things can go wrong. To help avoid ambiguity
it is, moreover, advisable to use the same
expressions for the same purposes throughout
the dialogue. The system preferably should not
use terms and expressions which are not familiar
to most or all of its users. If the system must do
that, unfamiliar terminology should be explained
either proactively (before users ask) or through
adequate measures for clarification meta-
communication.
Developers may use CODIAL - a tool based
on Cooperativity Theory – as support for the
design and evaluation of co-operative system
dialogue (www.disc2.dk/tools).
It is important to realise that the system’s
output language tends to have a massive priming
effect on the user’s language. It is, therefore,
crucial that the words and grammar used in
system output can be recognised and understood
by the system itself. Similarly, the system
should have a speaking style which induces
users to provide input that is to the point and can
be handled by the system.
Interaction data analysis is needed to assess
the efficiency of the input control strategies
adopted. User contacts through interviews and
questionnaires are good means for obtaining
early input on how users experience the
system’s output.
</bodyText>
<subsectionHeader confidence="0.996982">
6.6 Feedback adequacy
</subsectionHeader>
<bodyText confidence="0.999931">
Adequate feedback is essential for users to feel
in control during interaction. The user must feel
confident that the system has understood the
information input in the way it was intended,
and the user must be told which actions the
system has taken and what the system is
currently doing. A difficult thing is that telling
the user is not always good enough – the user
must be told in such a way that the user notices
what the system says. It may therefore be a good
thing for SLDSs to provide several different
kinds of feedback to their users. We distinguish
between process feedback and information
feedback.
When the system processes information
received from the user and hence may not be
speaking for a while, process feedback – which
may be provided in many different ways - keeps
the user informed on what is going on. A user
who is uncertain about what the system is doing,
if anything, is liable to produce unwanted input
or to believe that the system has crashed and
decide to hang up. Moreover, the uncertainty
itself is likely to affect negatively the user’s
satisfaction with the system.
Feedback on the system’s understanding of
what the user just said and on the actions taken
by the system helps ensure that, throughout the
dialogue, the user is left in no doubt as to what
the system has understood and is doing.
Information feedback can be provided in
different ways and more or less explicitly. The
amount and nature of the information feedback
depends e.g. on the cost and risk involved in the
user-system transaction. A user who is uncertain
as to what the system has understood, or done, is
liable to produce unwanted input and to react
negatively to the way the system works.
</bodyText>
<subsectionHeader confidence="0.999381">
6.7 Adequacy of dialogue initiative
</subsectionHeader>
<bodyText confidence="0.999994068181818">
To support natural interaction, an SLDS needs a
reasonable choice of dialogue initiative, an
appropriate dialogue structure, sufficient task
and domain coverage, and sufficient reasoning
capabilities.
Spoken human-human dialogue is
prototypically mixed-initiative. However, many
task-oriented dialogues tend to be directed
primarily by one of the interlocutors. Users may
even feel satisfied with less initiative when
interacting with an SLDS than when talking to a
person as long as the dialogue initiative distri-
bution fits the task(s) the system and the user
must solve together, and provided that the rest of
the best practice issues proposed in this paper
are properly attended to. Thus, system directed
dialogue can work well for tasks in which the
system simply requires a series of specific
pieces of information from the user, in particular
if the user is new to the system. To satisfy
experienced users, the system may have to be
able to cope with the larger packages of input
information which are natural to these users.
In principle, a (mainly) user directed
dialogue is as much of an aberration from mixed
initiative dialogue as is the (mainly) system
directed dialogue. Currently, user directed
dialogue would seem to be appropriate primarily
for applications designed for experienced users
who know how to make themselves understood
by the system. Unless supported by screen
graphics or other additional modalities,
inexperienced users are likely to address the
system in ways it cannot cope with.
Mixed initiative dialogue, i.e. a mixture of
system and user initiative, is often both desirable
and technically feasible. At some points in the
dialogue it may be appropriate that the system
takes the initiative to guide the user, obtain
missing information, or handle an error. At other
points, such as when the user needs information
from the system, is already familiar with the
system or wants to correct an error, it is
appropriate for the user to take the initiative.
</bodyText>
<subsectionHeader confidence="0.968064">
6.8 Naturalness of the dialogue
structure
</subsectionHeader>
<bodyText confidence="0.999997416666667">
As long as we cannot build fully conversational
systems, dialogue designers may have to impose
some kind of structure onto the dialogue,
determining which topics (or sub-tasks) could be
addressed when. It is important that the structure
imposed on the dialogue is natural to the user,
reflecting the user’s intuitive expectations,
especially in system directed dialogue in which
the user is not supposed to interfere with the
dialogue structure. Unnatural dialogue structure
will often cause users to try to take the initiative
in ways which the system cannot cope with.
</bodyText>
<subsectionHeader confidence="0.976575">
6.9 Sufficiency of task and domain
coverage
</subsectionHeader>
<bodyText confidence="0.999974727272727">
Sufficient task and domain coverage is also
crucial to natural interaction. Even if unfamiliar
with SLDSs, users normally have rather
detailed expectations to the information or
service which they should be able to obtain
from the system. It is important that the system
meet these expectations. If, for some reason, the
system is not able to perform a certain sub-task
which users would expect the system to handle,
this has to be stated clearly. Even then, user
satisfaction is likely to suffer.
</bodyText>
<subsectionHeader confidence="0.9889215">
6.10 Sufficiency of the system’s
reasoning capabilities
</subsectionHeader>
<bodyText confidence="0.999992545454546">
Contextually adequate reasoning is a classical
problem in the design of natural interaction.
Even when users have been appropriately
primed to expect a rather primitive interlocutor,
they tend to assume that the system is able to
perform the bits and pieces of reasoning which
humans are able to do without thinking and
which are inseparable parts of natural dialogue
about the task. Typically, therefore, SLDSs must
incorporate both facts and inferences about the
task as well as general world knowledge in order
to act as adequate interlocutors. Defining which
kinds of reasoning the system must be capable
of is part and parcel of defining the system’s
task and domain coverage and subject to
similarly difficult decisions on task delimitation.
It is possible to get rough ideas on initiative
distribution, users’ models of the task, and how
to delimit the domain from studying recorded
human-human dialogues on tasks similar to
those which the system is intended to cover.
However, the recordings should only be
considered possible starting points. In particular,
as task complexity grows, developers are likely
to find themselves forced to adopt more
restrictive task delimitations and impose a more
rigid dialogue structure than those which they
found in the human-human dialogues. Having
done that, the resulting interaction model needs
early testing and evaluation. In particular, if the
developer is into relatively high task complexity
compared to the state of the art, early testing is
strongly recommended.
</bodyText>
<subsectionHeader confidence="0.994975">
6.11 Sufficiency of interaction guidance
</subsectionHeader>
<bodyText confidence="0.999973644067797">
Sufficient interaction guidance is essential for
users to feel in control during interaction.
Interaction guidance can be particularly hard to
get right in speech-only, walk-up-and-use
SLDSs. Speech is inappropriate for providing
lengthy and complex “user manual” instructions
up front for first-time users. Moreover, at any
given time some users will already be familiar
with the system whereas others will be novices.
Issues to consider include cues for turn-taking
vs. barge-in; help facilities; and highlighting of
non-obvious system behaviour.
Barge-in allows the user to speed up the
interaction, e.g. by interrupting already familiar
instruction prompts. If the system does not allow
barge-in, it must provide clear cues for turn-
taking, making it completely clear to the user
when to speak and when to refrain from
speaking because the system does not listen.
Cues can be explicit or implicit. If the user starts
speaking while the system is still listening but
processing the previous user input, the user’s
new input may cause problems for the dialogue
manager which has to generate an appropriate
response to disjoint pieces of user input. And if
the system is not listening any more, important
input could be lost in cases when users do not
merely repeat themselves.
General and explicit instructions on what the
system can and cannot do and how to interact
with it may be provided in a spoken introduction
which can be repeated on request or be skipped
by experienced users. In fact, most speech-only
SLDSs strictly need some up-front introduction
to guide interaction. We already mentioned the
when-(not)-to-speak issue above. Just as
importantly, the system should be perfectly clear
about the task(s) which the user can accomplish
through interaction. The introduction should not
be too long because then users cannot remember
the instructions. Moreover, the instructions must
be feasible for the user. If the instructions
needed by the walk-up-and-use user are too
many to be presented in the system’s
introduction, some of them may be relocated for
presentation at particular points during
interaction and only when needed.
Providing useful help mechanisms is a
difficult interaction design task. Help may be an
implicit part of the dialogue, be available on
request by saying “help”; or be automatically
enabled if the user is having problems
repeatedly, for instance in being recognised. In
this case the system may, e.g., propose how to
express input or inform the user on what can be
said.
Sufficiency of interaction guidance should be
carefully evaluated by exposing the SLDS to
interaction with representative users.
</bodyText>
<subsectionHeader confidence="0.529088">
6.12 Error handling adequacy
</subsectionHeader>
<bodyText confidence="0.99985993814433">
Even if the best practice issues discussed so far
have been taken into account carefully during
specification, design and implementation, the
SLDS and its users will still make errors during
dialogue. In human-system interaction, error
prevention is far preferable to error correction,
and what those best practice issues do is to help
prevent errors from occurring during interaction.
Also as regards error handling current SLDSs
are far inferior to their human interlocutors. This
is why adequate error handling remains a
difficult issue in SLDS development. Intuitively,
this issue can be decomposed along two
dimensions: (a) either the system initiates error-
handling meta-communication or the user
initiates error-handling meta-communication.
And (b) when error-handling meta-
communication is initiated, it is either because
one party has failed to hear or understand the
other or because what was heard or understood
is false, or it is because what was heard or
understood is somehow in need of clarification.
We distinguish, therefore, between system or
user initiated repair meta-communication and
system or user initiated clarification meta-
communication.
System-initiated repair meta-communication
is needed whenever the system either did not
understand or was uncertain that it understood
correctly what was said. In such cases, the
system must ask for repetition, ask the user to
speak louder or modify the way the input is
being expressed in other specified ways, or tell
the user what it did understand and ask for
confirmation or correction. In case of a repeated
misunderstanding the system may either choose
to fall back on a human operator, close the
dialogue, or, better, start graceful degradation,
i.e. change the level of interaction into a simpler
one. If users simply fail to respond, then the
system should tell that it is expecting their input.
Users may also be understood by the system to
have said something which is false and hence
needs to be corrected. User-initiated repair
meta-communication can be designed in several
different ways. Ideally, users should just initiate
repair the same way they would have done in
dialogue with a human, but since users may
express their corrections in many different ways
this is very difficult. Some systems require the
user to use specifically designed keywords. The
problem is that using keywords for correction is
unnatural and hence difficult for the user to
remember. A third approach is the “eraser”
principle where the user simply repeats his input
until the system has received the message.
Whilst this solution may work well for low-
complexity tasks, it may be difficult to keep
track of in high-complexity tasks. And it will not
work if the system cannot recognise input on
any sub-task all the time.
Very roughly speaking, clarification meta-
communication is more difficult to design for
than repair meta-communication, and user-
initiated clarification meta-communication is
more difficult to design for than system-initiated
clarification meta-communication. System-
initiated clarification is needed when the user’s
input is inconsistent, ambiguous or
underspecified. In such cases, the system must
ask for clarification, for instance by pointing out
that an expression is inconsistent. User-initiated
clarification is needed whenever the system
produces inconsistent or ambiguous utterances,
or uses terms with which the user is not familiar.
Unfortunately, handling user clarification
questions is difficult for SLDSs and the system
developers might not have discovered all the
potential problems in the first place. If they had,
they could have tried to prevent all or most of
the problems from occurring through adequate
output phrasing or other means. Due to the
nature of their domain, some tasks inherently
require facilities for clarifying the terminology
used because it may not be a practical option for
e.g. a car sales system to explain all domain
terms as it goes along.
Most SLDSs need abilities for handling
system- and user-initiated repair, and many
SLDSs need system-initiated clarification
abilities. There is no simple decision procedure
for deciding which mechanisms to include in a
particular SLDS. Sensible decisions very much
depend on factors such as domain, task
complexity, user population and peculiarities of
user behaviour which can only be discovered
through interaction data analysis.
</bodyText>
<subsectionHeader confidence="0.609103">
6.13 Sufficiency of adaptation to user
differences
</subsectionHeader>
<bodyText confidence="0.99998575">
It is useful to distinguish between four types of
user: system expert/domain expert, system
expert/domain novice, system novice/domain
expert and system novice/domain novice. An
SLDS needs not support all four groups, of
course. If the target user group is domain and
system experts only, then, obviously, the system
is not a walk-up-and-use system and thus falls
outside the group of SLDSs considered in this
paper. If the primary target group is system
novice users, on-line instructions and other help
information is likely to be needed. This need
tends to increase even further when the system
novices are also domain novices who need
explanation of domain technicalities.
Given the relative simplicity of current
SLDSs, walk-up-and-use users may quickly
become (system) experts. This means that
interaction should be supported and facilitated
for both system novices and system experts.
Special shortcuts for expert interaction can be a
good solution. Such shortcuts include e.g.
introductions which can be skipped easily
through barge-in or explicit de-selection.
</bodyText>
<subsectionHeader confidence="0.807288">
6.14 Number of interaction problems
</subsectionHeader>
<bodyText confidence="0.999955071428572">
Lack of co-operativity in the system’s output
may be diagnosed from the occurrence of
communication problems in simulated or real
user-system interaction. Data capture and
analysis is costly, however, especially because
large amounts of data may be needed for
triggering most of the communication problems
which the system is likely to cause. To reduce
cost, and to help identify those kinds of lack of
cooperativity which are less likely to cause
communication problems, CODIAL may be
used both for walk-throughs through the
interaction design prior to data capture and for
the actual data analysis.
</bodyText>
<sectionHeader confidence="0.711549" genericHeader="method">
6.15 User satisfaction
</sectionHeader>
<bodyText confidence="0.9999385">
Objectively measured quality, technical and
otherwise, does have an impact on user
satisfaction, but this is far from being the whole
story. User satisfaction is inherently subjective,
building on personal preferences and contextual
factors. Unfortunately, some of the most
difficult usability issues exactly concern
contextual adequacy, i.e. adequacy of the full set
of contextual factors which contribute to making
an SLDS acceptable to its users. These factors
remain insufficiently explored both as regards
which they are and as regards their individual
contributions to user satisfaction. It is possible
that contextual factors, such as service
improvements or economical benefits, are
among the most important factors influencing
users’ satisfaction with SLDSs.
Much still remains to be discovered about
how the behaviour of SLDSs affect the
satisfaction of their users. Therefore, subjective
evaluation remains a cornerstone in SLDS
evaluation. User questionnaires and interviews
remain core tools for gathering information on
user satisfaction.
</bodyText>
<sectionHeader confidence="0.975536" genericHeader="conclusions">
7 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9999812">
We have presented a brief guide to practical
evaluation of walk-up-and-use SLDSs for
shared-goal tasks followed by a set of usability
evaluation criteria. Within this framework, many
issues remain unresolved or even unaddressed.
Deployment usability issues are still poorly
understood as are the usability issues arising
from multimodal and natural interactive
applications which integrate speech-only SLDSs
into larger systems. Usability questionnaire
design remains poorly understood. The same
applies to cultural differences in the perception
of SLDS usability.
Much work remains to be done before we
have a solid all-round understanding of usability
evaluation of SLDSs. The authors work in two
recently started projects on walk-up-and-use
SLDSs, one commercial and one research
system, in which the guidelines presented above
will be tested and, very likely, extended.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995805">
Bernsen, N. O., Dybkjær, H. and Dybkjær, L.
1998. Designing Interactive Speech Systems.
From First Ideas to User Testing. Berlin,
Springer.
Bernsen, N. O. and Dybkjær, L. 2000. A
Methodology for Evaluating Spoken Language
Dialogue Systems and Their Components.
Proceedings of LREC 2000, Athens, May 2000,
183-188.
Cole, R. A., Mariani, J., Uszkoreit, H.,
Zaenen, A. and Zue, V. W. (Editorial Board),
Varile, G. and Zampolli, A. (Managing Editors).
1996. Survey of the State of the Art in Human
Language Technology. Sponsors: National
Science Foundation, Directorate XIII-E of the
Commission of the European Communities,
Center for Spoken Language Understanding,
Oregon Graduate Institute. URL:
http://www.cse.ogi.edu/CSLU/HLTsurvey/.
Dybkjær, L. and Bernsen, N.O. 2000.
Usability Issues in Spoken Language Dialogue
Systems. In Natural Language Engineering,
Special Issue on Best Practice in Spoken
Language Dialogue System Engineering,
Volume 6 Parts 3 &amp; 4 September 2000, 243-
272.
EAGLES. 1996. Evaluation of Natural
Language Processing Systems. Final Report,
EAGLES Document EAG-EWG-PR2.
Copenhagen, Center for Sprogteknologi,
http://issco-
www.unige.ch/projects/eagles/ewg99/index.html
Failenschmid, K., Williams, D., Dybkjær, L.
and Bernsen, N. O. 1999. Draft proposal on best
practice methods and procedures in human
factors. DISC Deliverable D3.6.
http://www.disc2.dk.
Gaizauskas, R. (Ed.) 1997. Proceedings of
the SALT Club Workshop on Evaluation in
Speech and Language Technology, Sheffield.
Gibbon, D., Moore, R. and Winski, R. (Eds.).
1997. Handbook of standards and resources for
spoken language systems. Mouton de Gruyter,
Berlin, New York.
Hirschmann, L. and Thompson, H. S. 1996.
Overview of evaluation in speech and natural
language processing. In Cole et al. 1996, Section
13.1.
Karlsson, I. 1999. Draft proposal on best
practice methods and procedures in speech
generation. DISC Deliverable D3.3.
http://www.disc2.dk
Sparck Jones, K. and Galliers, J. 1996.
Evaluating natural language processing
systems. Lecture Notes in Artificial Intelligence
1083. Berlin, Springer.
Walker, M. A., Litman, D. J., Kamm, C. A.
and Abella, A. 1997. Evaluating interactive
dialogue systems: Extending component
evaluation to integrated system evaluation.
Proceedings of the ACL/EACL Workshop on
Spoken Dialog Systems, Madrid, 1-8.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.410271">
<title confidence="0.9986345">Usability Evaluation in Spoken Language Dialogue Systems</title>
<author confidence="0.985">Laila Dybkjær</author>
<author confidence="0.985">Niels Ole</author>
<affiliation confidence="0.986916">Natural Interactive Systems Laboratory, University of Southern</affiliation>
<address confidence="0.540948">Science Park 10, 5230 Odense M,</address>
<email confidence="0.974272">laila@nis.sdu.dk,nob@nis.sdu.dk</email>
<abstract confidence="0.999414549450552">The paper first addresses a series of issues basic to evaluating the usability of spoken language dialogue systems, including types and purpose of evaluation, when to evaluate and which methods to use, user involvement, how to evaluate and what to evaluate. We then go on to present and discuss a comprehensive set of usability evaluation criteria for spoken language dialogue systems. Usability is becoming an increasingly important issue in the development and evaluation of spoken language dialogue systems (SLDSs). Many companies would pay large amounts to know exactly which features make SLDSs attractive to users and how to evaluate whether their system has these features. In spite of its key importance far less resources have been invested in the usability aspect of SLDSs over the years than in SLDS component technologies. The usability aspect has often been neglected in SLDS development and evaluation and there has been surprisingly little research in important user-related issues, such as user reactions to SLDSs in the field, users’ linguistic behaviour, or the main factors which determine overall user satisfaction. However, there now seems to be growing recognition that usability is as important as, and partly independent of, the technical quality of any SLDS component and that quality usability constitutes an important competitive parameter. Most of today&apos;s SLDSs are walk-up-and-use systems for shared-goal tasks. Usability of walkup-and-use systems is of utmost importance, since users of such systems cannot be expected to undertake extensive training about the system or to read the user manual. Help must be available online and needed as infrequently as possible. There is at present no systematic understanding of which factors must be taken into account to optimise SLDS usability and thus also no consensus as to which usability evaluation criteria to use. Ideally, such an understanding should be comprehensive, i.e. include all major usability perspectives on SLDSs, and exhaustive, i.e. describe each perspective as it pertains to the detailed development and evaluation of any possible SLDS. This paper addresses the aspect of comprehensiveness by proposing a set of usability evaluation criteria. The criteria are derived from a set of usability issues that have resulted from a decomposition of the complex space of SLDS usability best practice. The present paper focuses on walk-up-anduse SLDSs for shared-goal tasks and reviews substantial parts of the authors’ work as presented in, e.g., (Dybkjær and Bernsen 2000). For additional examples the reader is referred to this paper. Due to space limitations few examples are included in the present paper. In the following we first briefly address types and purpose of evaluation (Section 2), when to evaluate and which methods to use (Section 3), user involvement (Section 4), and how to evaluate (Section 5). Section 6 presents the proposed set of evaluation criteria and discusses the usability issues behind these. Section 7 concludes the paper. and purpose of evaluation Evaluation can be quantitative or qualitative, or objective. evaluation consists in quantifying some parameter through an independently meaningful number, percentage etc. which in principle allows across systems. in estimating or judging some parameter by reference to expert standards rules. evaluation in judging some parameter by reference to users’ evaluation subjectindependent parameter assessment. Ideally, we would like to obtain quantitative and objective progress evaluation scores for usability which can be objectively compared to scores obtained from evaluation of other SLDSs. This is what has been attempted in the PARADISE framework based on the claim that task success and dialogue cost are potentially relevant contributors to user satisfaction (Walker, Litman, Kamm and Abella 1997). However, many important usability issues cannot be subjected to quantification and objective expert evaluation is sometimes highly uncertain or nonexistent. The purpose of evaluation may be to detect and analyse design and implementation errors (diagnostic evaluation), measure SLDS performance in terms of a set of quantitative and/or qualitative parameters (performance evaluation), or evaluate how well the system fits its purpose and meets actual user needs and expectations (adequacy evaluation), cf. (Hirschmann and Thompson 1996, Gibbon, Moore. and Winski 1997, Bernsen et al. 1998). The latter purpose is the more important one from a usability point of view although the others are relevant as well. Which type of evaluation to use and for which purpose, depends on the evaluation criterion which is being applied (see below). Other general references to natural language systems evaluation are (EAGLES 1996, Gaizauskas 1997, Sparck Jones and Galliers 1996). to evaluate and methods to use Usability evaluation should start as early as possible and continue throughout development. In general, the earlier design errors are being identified, the easier and cheaper it is to correct them. Different methods of evaluation may have to be applied for evaluating a particular parameter depending on the phase in the lifecycle in which evaluation takes place. Early design evaluation can be based on mock-up experiments with users and on design walkthroughs. Wizard of Oz simulations with representative task scenarios can provide valuable evaluation data. When the system has been implemented, controlled scenario-based tests with representative users and field tests can be used. Recorded dialogues with the (simulated) system should be carefully analysed for indications that the users have problems or expectations which exceed the capabilities of the system. Human-system interaction data should be complemented by interviews and questionnaires to enable assessment of user satisfaction. If users are interacting with the prototype on the basis of scenarios, there are at least two issues to be aware of. Firstly, scenarios should be designed to avoid priming the users on how to interact with the system. Secondly, sub-tasks covered by the scenarios will not necessarily be representative of the sub-tasks which real users (not using scenarios) would expect the system to cover. The final test of the system is often called the acceptance test. It involves real users and must satisfy the evaluation criteria defined as part of the requirements specification. involvement In general, representative users from the target user group(s) should be involved in evaluation from early on. The developers themselves can certainly discover many of the usability problems with the early design and implementation, especially when supported by state-of-the-art usability standards, evaluation criteria and design support tools. The problem is that they know too well how to interact with the system in order to avoid creating interaction problems which the system cannot handle. For the time being, there is no alternative to involving the target users in all or most system evaluation phases and for most usability evaluation purposes. This is costly and complex to do. However, the data analysis which is crucial to benefiting from trials with the system, is as necessary after trials with developers as it is after trials with representative users. Even the early involvement of representative users is no guarantee that the system will ultimately produce sufficient user satisfaction. For one thing, the data distribution they generate may not match the behaviour of the users of the system, once installed. For another, experimental user trials are different from real situations of use in which time, money and trust are really at stake. For these reasons, and particularly when introducing SLDSs which are innovative in some respect, it is necessary to prepare and budget for field trials with the implemented system as well as for the subsequent data analysis and fine-tuning of the system. Users who are “only” involved in a test can be much more indifferent to, or more positive towards, a system with poor usability characteristics than real users who have something to loose if the system lets them down (Bernsen et al. 1998). to evaluate Evaluation, including usability evaluation, is non-trivial and cannot be explained simply by stating what to evaluate (cf. Section 6) and what the developers’ options are. One of the most difficult questions in evaluation probably is how to do it properly. We have developed a template which supports consistent and detailed description of each evaluation criterion (Bernsen and Dybkjær 2000). The template includes the following issues: what is being evaluated (e.g. feedback adequacy), the system part evaluated (e.g. the dialogue manager), type of evaluation (e.g. qualitative), method(s) of evaluation (e.g. controlled user experiments), symptoms to look for (e.g. user clarification questions), life cycle phase(s) (e.g. simulation), importance of evaluation (e.g. crucial), difficulty of evaluation (e.g. easy), cost of evaluation (e.g. expensive), and support tools (e.g. SMALTO), see (www.disc2.dk/tools). The idea is that the combined set of (i) design options for SLDS usability, (ii) usability evaluation criteria, and (iii) template-based characterisation of each criterion, will provide developers with sufficient information for proper evaluation of their SLDSs. to evaluate In general terms, a usable SLDS must satisfy user needs which are similar to those which must be satisfied by other interactive systems. The SLDS must be easy to understand and interact with. Interaction should be smooth and the user should feel in control throughout the dialogue with the system. It is the task of the SLDS developer to meet those user needs considered as overall usability design goals. However, SLDSs are very different from more traditional interactive systems whose usability aspects have been investigated for decades, such as systems controlled through graphical user interfaces involving screen, keyboard and mouse. Perhaps the most important difference is that speech is perceptually transient rather than static. This means that the user must pick up the provided by the system the moment it is being provided or else miss it altogether. Moreover, the user has no way of inspecting the interface prior to interaction. If the interface is not self-evident all the way through the dialogue it must be learnt by trialand-error through repeated interaction, which is unsatisfactory for the casual walk-up-and-use user. Secondly, the processing (recognition, language understanding, dialogue management) spoken difficult to design and error-prone in execution, which is why SLDSs must be crafted with extreme care to ensure that users do not produce spoken input which the system is incapable of handling. In the following we present a set of usability evaluation criteria which are based on results achieved in the European DISC project (www.disc2.dk) on best practice in the development and evaluation of SLDSs (Failenschmid et al. 1999). Our claim is that quality usability of SLDSs may be pursued by focusing on a comprehensive set of 15 usability issues which include all major usability perspectives on SLDSs. These usability issues or a subset thereof, depending on how advanced and complex the system is to be – should be represented in the system specification and should therefore also be reflected in a set of evaluation criteria for the system which would appear mandatory for evaluating the usability of the SLDS. More details on the usability issues can be found in (Dybkjær and Bernsen 2000). 6.1 Modality appropriateness The majority of task-oriented SLDSs have so far used speech as the only input/output modality. However, an increasing number of systems now combine spoken input/output with other modalities. It is well-known that speech-only interaction is not appropriate for all tasks and applications, and the same is true for any particular modality combination which includes speech input and speech output. Few users would e.g. be happy to speak aloud their pin code to the bank teller machine in the street. The developers should attempt to make sure that spoken input and output, possibly combined with other input/output modalities, is an appropriate modality choice for the planned application. If the chosen modalities are inappropriate, chances are that the users either will not accept the application or will refrain from using some of the modalities it offers. Common sense, experimentation and/or the use of the tool SMALTO (www.disc2.dk/tools) may help the developers in making the right modality choice. 6.2 Input recognition adequacy From the user’s point of view, good speech recognition means that the system rarely gets the user’s spoken input wrong or fails to recognise what the user just said. Recognition success, as perceived by the user, not only depends on recogniser quality but also on how other parts of the SLDS handle the user’s input. Good recogniser quality nevertheless remains the key factor in making users confident that the system will successfully get what they say. Walk-up-and-use systems may be used by many different users in highly different environments. The speech recogniser, therefore, and depending on more specific information on its intended users and environments of interaction, must be trained to recognise a variety of dialects and accents, speakers of different gender, age and voice quality, speaking with a low or a loud voice, in noisy or quiet environments, and with varying channel quality. Adequate information on users and environments is essential input to the selection and creation of training data. To assess the quality of the system’s recognition capabilities prior to running the full system, speech recognition accuracy may be tested on the recogniser with users from the target group(s). 6.3 Naturalness of user speech Speaking to an SLDS should feel as easy and natural as possible. It does not help the user that the system’s speech recognition is perfect in principle if the input vocabulary and grammar expected from the user are not the ones which the user is likely to use and thus cannot be understood. Depending on, e.g., the task and users’ experience, what is “natural” input language may vary considerably. What is being experienced as natural input speech is also highly relative to the system’s output phrasing. For example, lengthy and/or overly polite system utterances are likely to invite similar linguistic user behaviour, thereby burdening input recognition and understanding unnecessarily. The system’s output language thus should be used to control users’ input language so that the latter becomes manageable for the system whilst still feeling natural to the user. If the minimal constraints imposed by the task are satisfied and the system’s output language adequately controls the user’s input language, users may well feel that the dialogue is natural even if they are not inclined to engage in lengthy conversation. Analysis of data from system simulations, questionnaires and interviews is a useful tool for obtaining information on users’ input language and on what they perceive as being natural input language. 6.4 Output voice quality From the user’s point of view, good SLDS output voice quality means that the system’s speech is clear and intelligible, does not demand an extra listening effort, is not particularly noise sensitive or distorted by clicks and other extraneous sounds, has natural intonation and prosody, uses an appropriate speaking rate, and is pleasant to listen to (Karlsson 1999). Taken together, these requirements are difficult to meet today. There are three main types of output speech: recordings of entire system utterances, concatenation of recorded words and phrases, and text-to-speech (TTS). Concatenated speech is the most frequently used type of speech in today’s SLDSs. For walk-up-and-use systems in particular, TTS may simply be too difficult to understand for infrequent users while full recordings are much too inflexible. Moreover, too natural output speech, like full recordings, may suggest to users that the system is far more capable and human-like than it actually is, encouraging them to address the system in a way which is more conversational and talkative than it can handle. The type of output voice chosen is likely to affect users’ perception of the system as a whole. In particular, and together with the quality of the speech output, the voice type has a major influence on how pleasant users find the “system’s voice”. Voice type includes features such as male/female, deep/high voice, speaking rate, and emotions. In order to gather input on user preferences with respect to the system’s output voice, representative users of the system under development may be asked to listen to different “system voices” and provide feedback on which one they prefer and what they like and dislike about each of them. 6.5 Output phrasing adequacy Regardless of the topic, the system should express itself co-operatively in order to maximise the likelihood that the task is achieved as smoothly and efficiently as possible. To facilitate successful interaction, the contents of the system’s output should be correct, relevant and sufficiently informative without being overinformative. Users have good reason for dissatisfaction if the system provides false information, e.g., if the database is not being properly updated. Lack of relevance of system output caused by, e.g., misrecognition, will typically lead to meta-communication dialogue. System output should be sufficiently informative. Otherwise, misunderstandings may occur which are only detected much later during interaction, if at all, or which, at best, lead to immediate requests for clarification by the user. Conversely, the system should not provide too much or overly verbose information. Users may then e.g. become inattentive, try to take the dialogue initiative, or become confused and initiate clarification meta-communication. The form of system expressions should be clear and unambiguous, and language and, as far as possible, terminology should be consistent and familiar to the user (Bernsen et al. 1998). Unclarity naturally leads to uncertainty and need clarification. So does ambiguity by the user. If undetected, as often happens, the effects of ambiguity can be severe. If the user unknowingly selects a non-intended meaning of a word or phrase uttered by the system, all sorts of things can go wrong. To help avoid ambiguity it is, moreover, advisable to use the same expressions for the same purposes throughout the dialogue. The system preferably should not use terms and expressions which are not familiar to most or all of its users. If the system must do that, unfamiliar terminology should be explained either proactively (before users ask) or through adequate measures for clarification metacommunication. Developers may use CODIAL a tool based on Cooperativity Theory – as support for the design and evaluation of co-operative system dialogue (www.disc2.dk/tools). It is important to realise that the system’s output language tends to have a massive priming effect on the user’s language. It is, therefore, crucial that the words and grammar used in system output can be recognised and understood by the system itself. Similarly, the system should have a speaking style which induces users to provide input that is to the point and can be handled by the system. Interaction data analysis is needed to assess the efficiency of the input control strategies adopted. User contacts through interviews and questionnaires are good means for obtaining early input on how users experience the system’s output. 6.6 Feedback adequacy Adequate feedback is essential for users to feel in control during interaction. The user must feel confident that the system has understood the information input in the way it was intended, and the user must be told which actions the system has taken and what the system is doing. A difficult thing is that the user is not always good enough – the user be told in such a way that the user what the system says. It may therefore be a good thing for SLDSs to provide several different kinds of feedback to their users. We distinguish between process feedback and information feedback. When the system processes information received from the user and hence may not be speaking for a while, process feedback – which may be provided in many different ways keeps the user informed on what is going on. A user who is uncertain about what the system is doing, if anything, is liable to produce unwanted input or to believe that the system has crashed and decide to hang up. Moreover, the uncertainty itself is likely to affect negatively the user’s satisfaction with the system. Feedback on the system’s understanding of what the user just said and on the actions taken by the system helps ensure that, throughout the dialogue, the user is left in no doubt as to what the system has understood and is doing. Information feedback can be provided in different ways and more or less explicitly. The amount and nature of the information feedback depends e.g. on the cost and risk involved in the user-system transaction. A user who is uncertain as to what the system has understood, or done, is liable to produce unwanted input and to react negatively to the way the system works. 6.7 Adequacy of dialogue initiative To support natural interaction, an SLDS needs a reasonable choice of dialogue initiative, an appropriate dialogue structure, sufficient task and domain coverage, and sufficient reasoning capabilities. Spoken human-human dialogue is prototypically mixed-initiative. However, many task-oriented dialogues tend to be directed primarily by one of the interlocutors. Users may even feel satisfied with less initiative when interacting with an SLDS than when talking to a person as long as the dialogue initiative distribution fits the task(s) the system and the user must solve together, and provided that the rest of the best practice issues proposed in this paper properly attended to. Thus, directed work well for tasks in which the system simply requires a series of specific pieces of information from the user, in particular if the user is new to the system. To satisfy experienced users, the system may have to be able to cope with the larger packages of input information which are natural to these users. principle, a (mainly) directed as much of an aberration from mixed initiative dialogue as is the (mainly) system directed dialogue. Currently, user directed dialogue would seem to be appropriate primarily for applications designed for experienced users who know how to make themselves understood by the system. Unless supported by screen graphics or other additional modalities, inexperienced users are likely to address the system in ways it cannot cope with. initiative dialogue, a mixture of system and user initiative, is often both desirable and technically feasible. At some points in the dialogue it may be appropriate that the system takes the initiative to guide the user, obtain missing information, or handle an error. At other points, such as when the user needs information from the system, is already familiar with the system or wants to correct an error, it is appropriate for the user to take the initiative. 6.8 Naturalness of the dialogue structure As long as we cannot build fully conversational systems, dialogue designers may have to impose some kind of structure onto the dialogue, determining which topics (or sub-tasks) could be addressed when. It is important that the structure imposed on the dialogue is natural to the user, reflecting the user’s intuitive expectations, especially in system directed dialogue in which the user is not supposed to interfere with the dialogue structure. Unnatural dialogue structure will often cause users to try to take the initiative in ways which the system cannot cope with. 6.9 Sufficiency of task and domain coverage Sufficient task and domain coverage is also crucial to natural interaction. Even if unfamiliar with SLDSs, users normally have rather detailed expectations to the information or service which they should be able to obtain from the system. It is important that the system meet these expectations. If, for some reason, the system is not able to perform a certain sub-task which users would expect the system to handle, this has to be stated clearly. Even then, user satisfaction is likely to suffer. 6.10 Sufficiency of the system’s reasoning capabilities Contextually adequate reasoning is a classical problem in the design of natural interaction. Even when users have been appropriately primed to expect a rather primitive interlocutor, they tend to assume that the system is able to perform the bits and pieces of reasoning which humans are able to do without thinking and which are inseparable parts of natural dialogue about the task. Typically, therefore, SLDSs must incorporate both facts and inferences about the task as well as general world knowledge in order to act as adequate interlocutors. Defining which kinds of reasoning the system must be capable of is part and parcel of defining the system’s task and domain coverage and subject to similarly difficult decisions on task delimitation. It is possible to get rough ideas on initiative distribution, users’ models of the task, and how to delimit the domain from studying recorded human-human dialogues on tasks similar to those which the system is intended to cover. However, the recordings should only be considered possible starting points. In particular, as task complexity grows, developers are likely to find themselves forced to adopt more restrictive task delimitations and impose a more rigid dialogue structure than those which they found in the human-human dialogues. Having done that, the resulting interaction model needs early testing and evaluation. In particular, if the developer is into relatively high task complexity compared to the state of the art, early testing is strongly recommended. 6.11 Sufficiency of interaction guidance Sufficient interaction guidance is essential for users to feel in control during interaction. Interaction guidance can be particularly hard to get right in speech-only, walk-up-and-use SLDSs. Speech is inappropriate for providing lengthy and complex “user manual” instructions up front for first-time users. Moreover, at any given time some users will already be familiar with the system whereas others will be novices. Issues to consider include cues for turn-taking vs. barge-in; help facilities; and highlighting of non-obvious system behaviour. Barge-in allows the user to speed up the interaction, e.g. by interrupting already familiar instruction prompts. If the system does not allow barge-in, it must provide clear cues for turntaking, making it completely clear to the user when to speak and when to refrain from speaking because the system does not listen. Cues can be explicit or implicit. If the user starts speaking while the system is still listening but processing the previous user input, the user’s new input may cause problems for the dialogue manager which has to generate an appropriate response to disjoint pieces of user input. And if the system is not listening any more, important input could be lost in cases when users do not merely repeat themselves. General and explicit instructions on what the system can and cannot do and how to interact</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N O Bernsen</author>
<author>H Dybkjær</author>
<author>L Dybkjær</author>
</authors>
<title>Designing Interactive Speech Systems. From First Ideas to User Testing.</title>
<date>1998</date>
<publisher>Berlin, Springer.</publisher>
<contexts>
<context position="4847" citStr="Bernsen et al. 1998" startWordPosition="714" endWordPosition="717">Litman, Kamm and Abella 1997). However, many important usability issues cannot be subjected to quantification and objective expert evaluation is sometimes highly uncertain or nonexistent. The purpose of evaluation may be to detect and analyse design and implementation errors (diagnostic evaluation), measure SLDS performance in terms of a set of quantitative and/or qualitative parameters (performance evaluation), or evaluate how well the system fits its purpose and meets actual user needs and expectations (adequacy evaluation), cf. (Hirschmann and Thompson 1996, Gibbon, Moore. and Winski 1997, Bernsen et al. 1998). The latter purpose is the more important one from a usability point of view although the others are relevant as well. Which type of evaluation to use and for which purpose, depends on the evaluation criterion which is being applied (see below). Other general references to natural language systems evaluation are (EAGLES 1996, Gaizauskas 1997, Sparck Jones and Galliers 1996). 3 When to evaluate and methods to use Usability evaluation should start as early as possible and continue throughout development. In general, the earlier design errors are being identified, the easier and cheaper it is to</context>
<context position="8608" citStr="Bernsen et al. 1998" startWordPosition="1304" endWordPosition="1307">nother, experimental user trials are different from real situations of use in which time, money and trust are really at stake. For these reasons, and particularly when introducing SLDSs which are innovative in some respect, it is necessary to prepare and budget for field trials with the implemented system as well as for the subsequent data analysis and fine-tuning of the system. Users who are “only” involved in a test can be much more indifferent to, or more positive towards, a system with poor usability characteristics than real users who have something to loose if the system lets them down (Bernsen et al. 1998). 5 How to evaluate Evaluation, including usability evaluation, is non-trivial and cannot be explained simply by stating what to evaluate (cf. Section 6) and what the developers’ options are. One of the most difficult questions in evaluation probably is how to do it properly. We have developed a template which supports consistent and detailed description of each evaluation criterion (Bernsen and Dybkjær 2000). The template includes the following issues: what is being evaluated (e.g. feedback adequacy), the system part evaluated (e.g. the dialogue manager), type of evaluation (e.g. qualitative)</context>
<context position="18732" citStr="Bernsen et al. 1998" startWordPosition="2867" endWordPosition="2870">ld be sufficiently informative. Otherwise, misunderstandings may occur which are only detected much later during interaction, if at all, or which, at best, lead to immediate requests for clarification by the user. Conversely, the system should not provide too much or overly verbose information. Users may then e.g. become inattentive, try to take the dialogue initiative, or become confused and initiate clarification meta-communication. The form of system expressions should be clear and unambiguous, and language and, as far as possible, terminology should be consistent and familiar to the user (Bernsen et al. 1998). Unclarity naturally leads to uncertainty and need for clarification. So does ambiguity if detected by the user. If undetected, as often happens, the effects of ambiguity can be severe. If the user unknowingly selects a non-intended meaning of a word or phrase uttered by the system, all sorts of things can go wrong. To help avoid ambiguity it is, moreover, advisable to use the same expressions for the same purposes throughout the dialogue. The system preferably should not use terms and expressions which are not familiar to most or all of its users. If the system must do that, unfamiliar termi</context>
</contexts>
<marker>Bernsen, Dybkjær, Dybkjær, 1998</marker>
<rawString>Bernsen, N. O., Dybkjær, H. and Dybkjær, L. 1998. Designing Interactive Speech Systems. From First Ideas to User Testing. Berlin, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N O Bernsen</author>
<author>L Dybkjær</author>
</authors>
<title>A Methodology for Evaluating Spoken Language Dialogue Systems and Their Components.</title>
<date>2000</date>
<booktitle>Proceedings of LREC 2000,</booktitle>
<pages>183--188</pages>
<location>Athens,</location>
<contexts>
<context position="9020" citStr="Bernsen and Dybkjær 2000" startWordPosition="1367" endWordPosition="1370">lved in a test can be much more indifferent to, or more positive towards, a system with poor usability characteristics than real users who have something to loose if the system lets them down (Bernsen et al. 1998). 5 How to evaluate Evaluation, including usability evaluation, is non-trivial and cannot be explained simply by stating what to evaluate (cf. Section 6) and what the developers’ options are. One of the most difficult questions in evaluation probably is how to do it properly. We have developed a template which supports consistent and detailed description of each evaluation criterion (Bernsen and Dybkjær 2000). The template includes the following issues: what is being evaluated (e.g. feedback adequacy), the system part evaluated (e.g. the dialogue manager), type of evaluation (e.g. qualitative), method(s) of evaluation (e.g. controlled user experiments), symptoms to look for (e.g. user clarification questions), life cycle phase(s) (e.g. simulation), importance of evaluation (e.g. crucial), difficulty of evaluation (e.g. easy), cost of evaluation (e.g. expensive), and support tools (e.g. SMALTO), see (www.disc2.dk/tools). The idea is that the combined set of (i) design options for SLDS usability, (i</context>
</contexts>
<marker>Bernsen, Dybkjær, 2000</marker>
<rawString>Bernsen, N. O. and Dybkjær, L. 2000. A Methodology for Evaluating Spoken Language Dialogue Systems and Their Components. Proceedings of LREC 2000, Athens, May 2000, 183-188.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R A Cole</author>
<author>J Mariani</author>
<author>H Uszkoreit</author>
<author>A Zaenen</author>
<author>V W Zue</author>
</authors>
<title>(Managing Editors).</title>
<date>1996</date>
<booktitle>Survey of the State of the Art in Human Language Technology. Sponsors: National Science Foundation, Directorate XIII-E of the Commission of the European Communities,</booktitle>
<location>Center</location>
<marker>Cole, Mariani, Uszkoreit, Zaenen, Zue, 1996</marker>
<rawString>Cole, R. A., Mariani, J., Uszkoreit, H., Zaenen, A. and Zue, V. W. (Editorial Board), Varile, G. and Zampolli, A. (Managing Editors). 1996. Survey of the State of the Art in Human Language Technology. Sponsors: National Science Foundation, Directorate XIII-E of the Commission of the European Communities, Center for Spoken Language Understanding, Oregon Graduate Institute. URL: http://www.cse.ogi.edu/CSLU/HLTsurvey/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dybkjær</author>
<author>N O Bernsen</author>
</authors>
<title>Usability Issues in Spoken Language Dialogue Systems.</title>
<date>2000</date>
<booktitle>In Natural Language Engineering, Special Issue on Best Practice in Spoken Language Dialogue System Engineering, Volume 6 Parts</booktitle>
<volume>3</volume>
<pages>243--272</pages>
<contexts>
<context position="2820" citStr="Dybkjær and Bernsen 2000" startWordPosition="422" endWordPosition="425">omprehensive, i.e. include all major usability perspectives on SLDSs, and exhaustive, i.e. describe each perspective as it pertains to the detailed development and evaluation of any possible SLDS. This paper addresses the aspect of comprehensiveness by proposing a set of usability evaluation criteria. The criteria are derived from a set of usability issues that have resulted from a decomposition of the complex space of SLDS usability best practice. The present paper focuses on walk-up-anduse SLDSs for shared-goal tasks and reviews substantial parts of the authors’ work as presented in, e.g., (Dybkjær and Bernsen 2000). For additional examples the reader is referred to this paper. Due to space limitations few examples are included in the present paper. In the following we first briefly address types and purpose of evaluation (Section 2), when to evaluate and which methods to use (Section 3), user involvement (Section 4), and how to evaluate (Section 5). Section 6 presents the proposed set of evaluation criteria and discusses the usability issues behind these. Section 7 concludes the paper. 2 Types and purpose of evaluation Evaluation can be quantitative or qualitative, subjective or objective. Quantitative </context>
<context position="12090" citStr="Dybkjær and Bernsen 2000" startWordPosition="1837" endWordPosition="1840">pment and evaluation of SLDSs (Failenschmid et al. 1999). Our claim is that quality usability of SLDSs may be pursued by focusing on a comprehensive set of 15 usability issues which include all major usability perspectives on SLDSs. These usability issues - or a subset thereof, depending on how advanced and complex the system is to be – should be represented in the system specification and should therefore also be reflected in a set of evaluation criteria for the system which would appear mandatory for evaluating the usability of the SLDS. More details on the usability issues can be found in (Dybkjær and Bernsen 2000). 6.1 Modality appropriateness The majority of task-oriented SLDSs have so far used speech as the only input/output modality. However, an increasing number of systems now combine spoken input/output with other modalities. It is well-known that speech-only interaction is not appropriate for all tasks and applications, and the same is true for any particular modality combination which includes speech input and speech output. Few users would e.g. be happy to speak aloud their pin code to the bank teller machine in the street. The developers should attempt to make sure that spoken input and output</context>
</contexts>
<marker>Dybkjær, Bernsen, 2000</marker>
<rawString>Dybkjær, L. and Bernsen, N.O. 2000. Usability Issues in Spoken Language Dialogue Systems. In Natural Language Engineering, Special Issue on Best Practice in Spoken Language Dialogue System Engineering, Volume 6 Parts 3 &amp; 4 September 2000, 243-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES</author>
</authors>
<title>Evaluation of Natural Language Processing Systems.</title>
<date>1996</date>
<tech>Final Report, EAGLES Document EAG-EWG-PR2.</tech>
<institution>Copenhagen, Center for Sprogteknologi,</institution>
<location>http://issco-</location>
<contexts>
<context position="5174" citStr="EAGLES 1996" startWordPosition="769" endWordPosition="770">f a set of quantitative and/or qualitative parameters (performance evaluation), or evaluate how well the system fits its purpose and meets actual user needs and expectations (adequacy evaluation), cf. (Hirschmann and Thompson 1996, Gibbon, Moore. and Winski 1997, Bernsen et al. 1998). The latter purpose is the more important one from a usability point of view although the others are relevant as well. Which type of evaluation to use and for which purpose, depends on the evaluation criterion which is being applied (see below). Other general references to natural language systems evaluation are (EAGLES 1996, Gaizauskas 1997, Sparck Jones and Galliers 1996). 3 When to evaluate and methods to use Usability evaluation should start as early as possible and continue throughout development. In general, the earlier design errors are being identified, the easier and cheaper it is to correct them. Different methods of evaluation may have to be applied for evaluating a particular parameter depending on the phase in the lifecycle in which evaluation takes place. Early design evaluation can be based on mock-up experiments with users and on design walkthroughs. Wizard of Oz simulations with representative ta</context>
</contexts>
<marker>EAGLES, 1996</marker>
<rawString>EAGLES. 1996. Evaluation of Natural Language Processing Systems. Final Report, EAGLES Document EAG-EWG-PR2. Copenhagen, Center for Sprogteknologi, http://issco-</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Failenschmid</author>
<author>D Williams</author>
<author>L Dybkjær</author>
<author>N O Bernsen</author>
</authors>
<title>Draft proposal on best practice methods and procedures in human factors.</title>
<date>1999</date>
<booktitle>DISC Deliverable D3.6. http://www.disc2.dk.</booktitle>
<contexts>
<context position="11521" citStr="Failenschmid et al. 1999" startWordPosition="1741" endWordPosition="1744">rough repeated interaction, which is unsatisfactory for the casual walk-up-and-use user. Secondly, the processing (recognition, language understanding, dialogue management) of spoken input remains difficult to design and error-prone in execution, which is why SLDSs must be crafted with extreme care to ensure that users do not produce spoken input which the system is incapable of handling. In the following we present a set of usability evaluation criteria which are based on results achieved in the European DISC project (www.disc2.dk) on best practice in the development and evaluation of SLDSs (Failenschmid et al. 1999). Our claim is that quality usability of SLDSs may be pursued by focusing on a comprehensive set of 15 usability issues which include all major usability perspectives on SLDSs. These usability issues - or a subset thereof, depending on how advanced and complex the system is to be – should be represented in the system specification and should therefore also be reflected in a set of evaluation criteria for the system which would appear mandatory for evaluating the usability of the SLDS. More details on the usability issues can be found in (Dybkjær and Bernsen 2000). 6.1 Modality appropriateness </context>
</contexts>
<marker>Failenschmid, Williams, Dybkjær, Bernsen, 1999</marker>
<rawString>Failenschmid, K., Williams, D., Dybkjær, L. and Bernsen, N. O. 1999. Draft proposal on best practice methods and procedures in human factors. DISC Deliverable D3.6. http://www.disc2.dk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
</authors>
<date>1997</date>
<booktitle>Proceedings of the SALT Club Workshop on Evaluation in Speech and Language Technology,</booktitle>
<location>Sheffield.</location>
<contexts>
<context position="5191" citStr="Gaizauskas 1997" startWordPosition="771" endWordPosition="772">antitative and/or qualitative parameters (performance evaluation), or evaluate how well the system fits its purpose and meets actual user needs and expectations (adequacy evaluation), cf. (Hirschmann and Thompson 1996, Gibbon, Moore. and Winski 1997, Bernsen et al. 1998). The latter purpose is the more important one from a usability point of view although the others are relevant as well. Which type of evaluation to use and for which purpose, depends on the evaluation criterion which is being applied (see below). Other general references to natural language systems evaluation are (EAGLES 1996, Gaizauskas 1997, Sparck Jones and Galliers 1996). 3 When to evaluate and methods to use Usability evaluation should start as early as possible and continue throughout development. In general, the earlier design errors are being identified, the easier and cheaper it is to correct them. Different methods of evaluation may have to be applied for evaluating a particular parameter depending on the phase in the lifecycle in which evaluation takes place. Early design evaluation can be based on mock-up experiments with users and on design walkthroughs. Wizard of Oz simulations with representative task scenarios can </context>
</contexts>
<marker>Gaizauskas, 1997</marker>
<rawString>Gaizauskas, R. (Ed.) 1997. Proceedings of the SALT Club Workshop on Evaluation in Speech and Language Technology, Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gibbon</author>
<author>R Moore</author>
<author>R Winski</author>
</authors>
<title>Handbook of standards and resources for spoken language systems. Mouton de Gruyter,</title>
<date>1997</date>
<location>Berlin, New York.</location>
<marker>Gibbon, Moore, Winski, 1997</marker>
<rawString>Gibbon, D., Moore, R. and Winski, R. (Eds.). 1997. Handbook of standards and resources for spoken language systems. Mouton de Gruyter, Berlin, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschmann</author>
<author>H S Thompson</author>
</authors>
<title>Overview of evaluation in speech and natural language processing.</title>
<date>1996</date>
<journal>Section</journal>
<booktitle>In Cole et</booktitle>
<volume>13</volume>
<contexts>
<context position="4793" citStr="Hirschmann and Thompson 1996" startWordPosition="705" endWordPosition="708">tentially relevant contributors to user satisfaction (Walker, Litman, Kamm and Abella 1997). However, many important usability issues cannot be subjected to quantification and objective expert evaluation is sometimes highly uncertain or nonexistent. The purpose of evaluation may be to detect and analyse design and implementation errors (diagnostic evaluation), measure SLDS performance in terms of a set of quantitative and/or qualitative parameters (performance evaluation), or evaluate how well the system fits its purpose and meets actual user needs and expectations (adequacy evaluation), cf. (Hirschmann and Thompson 1996, Gibbon, Moore. and Winski 1997, Bernsen et al. 1998). The latter purpose is the more important one from a usability point of view although the others are relevant as well. Which type of evaluation to use and for which purpose, depends on the evaluation criterion which is being applied (see below). Other general references to natural language systems evaluation are (EAGLES 1996, Gaizauskas 1997, Sparck Jones and Galliers 1996). 3 When to evaluate and methods to use Usability evaluation should start as early as possible and continue throughout development. In general, the earlier design errors</context>
</contexts>
<marker>Hirschmann, Thompson, 1996</marker>
<rawString>Hirschmann, L. and Thompson, H. S. 1996. Overview of evaluation in speech and natural language processing. In Cole et al. 1996, Section 13.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Karlsson</author>
</authors>
<title>Draft proposal on best practice methods and procedures in speech generation.</title>
<date>1999</date>
<booktitle>DISC Deliverable D3.3. http://www.disc2.dk</booktitle>
<contexts>
<context position="16111" citStr="Karlsson 1999" startWordPosition="2468" endWordPosition="2469">n lengthy conversation. Analysis of data from system simulations, questionnaires and interviews is a useful tool for obtaining information on users’ input language and on what they perceive as being natural input language. 6.4 Output voice quality From the user’s point of view, good SLDS output voice quality means that the system’s speech is clear and intelligible, does not demand an extra listening effort, is not particularly noise sensitive or distorted by clicks and other extraneous sounds, has natural intonation and prosody, uses an appropriate speaking rate, and is pleasant to listen to (Karlsson 1999). Taken together, these requirements are difficult to meet today. There are three main types of output speech: recordings of entire system utterances, concatenation of recorded words and phrases, and text-to-speech (TTS). Concatenated speech is the most frequently used type of speech in today’s SLDSs. For walk-up-and-use systems in particular, TTS may simply be too difficult to understand for infrequent users while full recordings are much too inflexible. Moreover, too natural output speech, like full recordings, may suggest to users that the system is far more capable and human-like than it a</context>
</contexts>
<marker>Karlsson, 1999</marker>
<rawString>Karlsson, I. 1999. Draft proposal on best practice methods and procedures in speech generation. DISC Deliverable D3.3. http://www.disc2.dk</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
<author>J Galliers</author>
</authors>
<title>Evaluating natural language processing systems.</title>
<date>1996</date>
<journal>Lecture Notes in Artificial Intelligence</journal>
<volume>1083</volume>
<publisher>Berlin, Springer.</publisher>
<marker>Jones, K, Galliers, 1996</marker>
<rawString>Sparck Jones, K. and Galliers, J. 1996. Evaluating natural language processing systems. Lecture Notes in Artificial Intelligence 1083. Berlin, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D J Litman</author>
<author>C A Kamm</author>
<author>A Abella</author>
</authors>
<title>Evaluating interactive dialogue systems: Extending component evaluation to integrated system evaluation.</title>
<date>1997</date>
<booktitle>Proceedings of the ACL/EACL Workshop on Spoken Dialog Systems,</booktitle>
<pages>1--8</pages>
<location>Madrid,</location>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Walker, M. A., Litman, D. J., Kamm, C. A. and Abella, A. 1997. Evaluating interactive dialogue systems: Extending component evaluation to integrated system evaluation. Proceedings of the ACL/EACL Workshop on Spoken Dialog Systems, Madrid, 1-8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>