<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9982225">
Graph Branch Algorithm: An Optimum Tree Search Method for Scored
Dependency Graph with Arc Co-occurrence Constraints
</title>
<author confidence="0.962545">
Hideki Hirakawa
</author>
<affiliation confidence="0.888968">
Toshiba R&amp;D Center
</affiliation>
<address confidence="0.8783105">
1 Komukai Toshiba-cho, Saiwai-ku,
Kawasaki 210, JAPAN
</address>
<email confidence="0.999121">
hideki.hirakawa@toshiba.co.jp
</email>
<sectionHeader confidence="0.99388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999444666666667">
Various kinds of scored dependency
graphs are proposed as packed shared data
structures in combination with optimum
dependency tree search algorithms. This
paper classifies the scored dependency
graphs and discusses the specific features
of the “Dependency Forest” (DF) which is
the packed shared data structure adopted
in the “Preference Dependency Grammar”
(PDG), and proposes the “Graph Branch
Algorithm” for computing the optimum
dependency tree from a DF. This paper
also reports the experiment showing the
computational amount and behavior of the
graph branch algorithm.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.956557282051282">
The dependency graph (DG) is a packed shared
data structure which consists of the nodes corre-
sponding to the words in a sentence and the arcs
showing dependency relations between the nodes.
The scored DG has preference scores attached to
the arcs and is widely used as a basis of the opti-
mum tree search method. For example, the scored
DG is used in Japanese Kakari-uke analysis1
to represent all possible kakari-uke(dependency)
trees(Ozeki, 1994),(Hirakawa, 2001). (McDon-
ald et al., 2005) proposed a dependency analysis
method using a scored DG and some maximum
spanning tree search algorithms. In this method,
scores on arcs are computed from a set of features
obtained from the dependency trees based on the
1Kakari-uke relation, widely adopted in Japanese sen-
tence analysis, is projective dependency relation with a con-
straint such that the dependent word is located at the left-hand
side of its governor word.
optimum parameters for scoring dependency arcs
obtained by the discriminative learning method.
There are various kinds of dependency analy-
sis methods based on the scored DGs. This pa-
per classifies these methods based on the types
of the DGs and the basic well-formed constraints
and explains the features of the DF adopted in
PDG(Hirakawa, 2006). This paper proposes the
graph branch algorithm which searches the opti-
mum dependency tree from a DF based on the
branch and bound (B&amp;B) method(Ibaraki, 1978)
and reports the experiment showing the computa-
tional amount and behavior of the graph branch
algorithm. As shown below, the combination of
the DF and the graph branch algorithm enables the
treatment of non-projective dependency analysis
and optimum solution search satisfying the single
valence occupation constraint, which are out of the
scope of most of the DP(dynamic programming)-
based parsing methods.
</bodyText>
<sectionHeader confidence="0.925911" genericHeader="method">
2 Optimum Tree Search in a Scored DG
</sectionHeader>
<subsectionHeader confidence="0.892845">
2.1 Basic Framework
</subsectionHeader>
<bodyText confidence="0.9996414">
Figure 1 shows the basic framework of the opti-
mum dependency tree search in a scored DG. In
general, nodes in a DG correspond to words in
the sentence and the arcs show some kind of de-
pendency relations between nodes. Each arc has
</bodyText>
<figureCaption confidence="0.999905">
Figure 1: The optimum tree search in a scored DG
</figureCaption>
<figure confidence="0.94533805">
Dependency
Tree
Set of Scored Well-
formed Dependency
Trees
Well-formed
dependency tree
constraint
Scored Dependency
Graph
Optimum Tree
Search
Algorithm
s1
s3
s4 s5
s2
Well-formed Dependency
Tree with the highest score
(score=s1+s2+s3+s4+s5 )
</figure>
<page confidence="0.970484">
361
</page>
<note confidence="0.725685">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 361–368,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998384">
a preference score representing plausibility of the
relation. The well-formed dependency tree con-
straint is a set of well-formed constraints which
should be satisfied by all dependency trees repre-
senting sentence interpretations. A DG and a well-
formed dependency tree constraint prescribe a set
of well-formed dependency trees. The score of a
dependency tree is the sum total of arc scores. The
optimum tree is a dependency tree with the highest
score in the set of dependency trees.
</bodyText>
<subsectionHeader confidence="0.999366">
2.2 Dependency Graph
</subsectionHeader>
<bodyText confidence="0.997386888888889">
DGs are classified into some classes based on the
types of nodes and arcs. This paper assumes three
types of nodes, i.e. word-type, WPP-type2 and
concept-type3. The types of DGs are called a word
DG, a WPP DG and a concept DG, respectively.
DGs are also classified into non-labeled and la-
beled DGs. There are some types of arc labels
such as syntactic label (ex. “subject”,“object”)
and semantic label (ex. “agent”,“target”). Var-
ious types of DGs are used in existing sys-
tems according to these classifications, such as
non-label word DG(Lee and Choi, 1997; Eisner,
1996; McDonald et al., 2005)4, syntactic-label
word DG (Maruyama, 1990), semantic-label word
DG(Hirakawa, 2001), non-label WPP DG(Ozeki,
1994; Katoh and Ehara, 1989), syntactic-label
WPP DG(Wang and Harper, 2004), semantic-label
concept DG(Harada and Mizuno, 2001).
</bodyText>
<subsectionHeader confidence="0.9998195">
2.3 Well-formedness Constraints and Graph
Search Algorithms
</subsectionHeader>
<bodyText confidence="0.997007857142857">
There can be a variety of well-formedness con-
straints from very basic and language-independent
constraints to specific and language-dependent
constraints. This paper focuses on the following
four basic and language-independent constraints
which may be embedded in data structure and/or
the optimum tree search algorithm.
</bodyText>
<listItem confidence="0.9919336">
(C1) Coverage constraint: Every input word has
a corresponding node in the tree
(C2) Single role constraint(SRC): No two nodes
in a dependency tree occupy the same input
position
</listItem>
<footnote confidence="0.999303428571428">
2WPP is a pair of a word and a part of speech (POS). The
word “time” has WPPs such as “time/n” and “time/v”.
3One WPP (ex. “time/n”) can be categorized into one or
more concepts semantically (ex. “time/n/period time” and
“time/n/clock time”).
4This does not mean that these algorithms can not handle
labeled DGs.
</footnote>
<listItem confidence="0.9331846">
(C3) Projectivity constraint(PJC): No arc crosses
another arc5
(C4) Single valence occupation constraint(SVOC):
No two arcs in a tree occupy the same valence
of a predicate
</listItem>
<bodyText confidence="0.943441935483871">
(C1) and (C2), collectively referred to as “cover-
ing constraint”, are basic constraints adopted by
almost all dependency parsers. (C3) is adopted
by the majority of dependency parsers which are
called projective dependency parsers. A projective
dependency parser fails to analyze non-projective
sentences. (C4) is a basic constraint for valency
but is not adopted by the majority of dependency
parsers.
Graph search algorithms, such as the Chu-
Liu-Edmonds maximum spanning tree algorithm
(Chu and Liu, 1965; Edmonds, 1967), algorithms
based on the dynamic programming (DP) princi-
ple (Ozeki, 1994; Eisner, 1996) and the algorithm
based on the B&amp;B method (Hirakawa, 2001), are
used for the optimum tree search in scored DGs.
The applicability of these algorithms is closely re-
lated to the types of DGs and/or well-formedness
constraints. The Chu-Liu-Edmonds algorithm is
very fast (O(n2)for sentence lengthn), but it
works correctly only on word DGs. DP-based al-
gorithms can satisfy (C1)-(C3) and run efficiently,
but seems not to satisfy (C4) as shown in 2.4.
(C2)-(C4) can be described as a set of co-
occurrence constraints between two arcs in a DG.
As described in Section 2.6, the DF can represent
(C2)-(C4) and more precise constraints because it
can handle co-occurrence constraints between two
arbitrary arcs in a DG. The graph branch algorithm
described in Section 3 can find the optimum tree
from the DF.
</bodyText>
<subsectionHeader confidence="0.984783">
2.4 SVOC and DP
</subsectionHeader>
<bodyText confidence="0.9997672">
(Ozeki and Zhang, 1999) proposed the minimum
cost partitioning method (MCPM) which is a parti-
tioning computation based on the recurrence equa-
tion where the cost of joining two partitions is
the cost of these partitions plus the cost of com-
bining these partitions. MCPM is a generaliza-
tion of (Ozeki, 1994) and (Katoh and Ehara, 1989)
which compute the optimum dependency tree in a
scored DG. MCPM is also a generalization of the
probabilistic CKY algorithm and the Viterbi algo-
</bodyText>
<footnote confidence="0.7976865">
5Another condition for projectivity, i.e. “no arc covers top
node” is equivalent to the crossing arc constraint if special
root node , which is a governor of top node, is introduced at
the top (or end) of a sentence.
</footnote>
<page confidence="0.977726">
362
</page>
<figure confidence="0.298645375">
target6,5
agent5,15
OS1[15]: (agent1,15)
OS3[22]: (agent1,15) + (target4,7)
OS4[25]: (agent5,15) + (in-state7,10)
NOS1[10]: (target2,10) OS4[25]: (agent5,15) + (in-state7,10)
OS1[15]: (agent1,15) NOS2[20]: (target4,10) + (in-state7,10)
Well-formed optimum solutions for covering whole phrase
</figure>
<figureCaption confidence="0.997005">
Figure 2: Optimum tree search satisfying SVOC
</figureCaption>
<bodyText confidence="0.999928583333333">
rithm6. The minimum cost partition of the whole
sentence is calculated very efficiently by the DP
principle. The optimum partitioning obtained by
MCPM constitutes a tree covering the whole sen-
tence satisfying the SRC and PJC. However, it is
not assured that the SVOC is satisfied by MCPM.
Figure 2 shows a DG for the Japanese phrase
“Isha-mo Wakaranai Byouki-no Kanja” encom-
passing dependency trees corresponding to “a pa-
tient suffering from a disease that the doctor
doesn’t know”, “a sick patient who does not know
the doctor”, and so on. OS1-OS4 represent the op-
timum solutions for the phrases specified by their
brackets computed based on MCPM. For exam-
ple, OS3 gives an optimum tree with a score of 22
(consisting of agent1 and target4) for the phrase
“Isha-mo Wakaranai Byouki-no”. The optimum
solution for the whole phrase is either OS1 + OS4
or OS3 + OS2 due to MCPM. The former has the
highest score 40(= 15 + 25) but does not satisfy
the SVOC because it has agent1 and agent5 si-
multaneously. The optimum solutions satisfying
the SVOC are NOS1 + OS4 and OS1 + NOS2
shown at the bottom of Figure 2. NOS1 and
NOS2 are not optimum solutions for their word
coverages. This shows that it is not assured that
MCPM will obtain the optimum solution satisfy-
ing the SVOC.
On the contrary, it is assured that the graph
branch algorithm computes the optimum solu-
tion(s) satisfying the SVOC because it com-
putes the optimum solution(s) satisfying any co-
occurrence constraints in the constraint matrix. It
is an open problem whether an algorithm based
on the DP framework exists which can handle the
SVOC and arbitrary arc co-occurrence constraints.
</bodyText>
<footnote confidence="0.731756333333333">
6Specifically, MTCM corresponds to probabilistic CKY
and the Viterbi algorithm because it computes both the opti-
mum tree score and its structure.
</footnote>
<table confidence="0.5539287">
nc2 npp19 rt29 obj16
Meaning of Arc Name
sub : subject
obj : object
npp : noun-preposition
vpp : verb-preposition
pre : preposition
nc : noun compound
det : determiner
rt : root
</table>
<figure confidence="0.5699375">
Constraint
Matrix
</figure>
<figureCaption confidence="0.996225">
Figure 3: Scored dependency forest
</figureCaption>
<subsectionHeader confidence="0.980207">
2.5 Semantic Dependency Graph (SDG)
</subsectionHeader>
<bodyText confidence="0.999986666666667">
The SDG is a semantic-label word DG designed
for Japanese sentence analysis. The optimum tree
search algorithm searches for the optimum tree
satisfying the well-formed constraints (C1)-(C4)
in a SDG(Hirakawa, 2001). This method is lack-
ing in terms of generality in that it cannot handle
backward dependency and multiple WPP because
it depends on some linguistic features peculiar to
Japanese. Therefore, this method is inherently in-
applicable to languages like English that require
backward dependency and multiple POS analysis.
The DF described below can be seen as the ex-
tension of the SDG. Since the DF has none of the
language-dependent premises that the SDG has, it
is applicable to English and other languages.
</bodyText>
<subsectionHeader confidence="0.940725">
2.6 Dependency Forest (DF)
</subsectionHeader>
<bodyText confidence="0.999747526315789">
The DF is a packed shared data structure en-
compassing all possible dependency trees for a
sentence adopted in PDG. The DF consists of a
dependency graph (DG) and a constraint matrix
(CM). Figure 3 shows a DF for the example sen-
tence “Time flies like an arrow.” The DG consists
of nodes and directed arcs. A node represents a
WPP and an arc shows the dependency relation
between nodes. An arc has its ID and preference
score. CM is a matrix whose rows and columns
are a set of arcs in DG and prescribes the co-
occurrence constraint between arcs. Only when
CM(i,j) is ○,arc2andarchare co-occurrable in
one dependency tree.
The DF is generated by using a phrase structure
parser in PDG. PDG grammar rule is an extended
CFG rule, which defines the mapping between
a sequence of constituents (the body of a CFG
rule) and a set of arcs (a partial dependency tree).
</bodyText>
<figure confidence="0.999346151515151">
target2,10
agent1,15
target4,7
agent3,5 in-state7,10
Isha-mo
(doctor)
Wakaranai
(not_know)
Byouki-no
(sickness)
Kanja
(patient)
OS2[10]: (in-state7,10)
rt32
vpp20
root
pre15
rt31
0,time/n
sub24
1,fly/v
vpp18
2,like/p
3,an/det
det14
4,arrow/n
0,time/v
obj4
1,fly/n
sub23
2,like/v
Dependency
Graph
</figure>
<page confidence="0.996405">
363
</page>
<bodyText confidence="0.999925666666667">
The generated CM assures that the parse trees in
the parse forest and the dependency trees in the
DF have mutual correspondence(Hirakawa, 2006).
CM can represent (C2)-(C4) in 2.3 and more pre-
cise constraints. For example, PDG can generate
a DF encompassing non-projective dependency
trees by introducing the grammar rules defining
non-projective constructions. This is called the
controlled non-projectivity in this paper. Treat-
ment of non-projectivity as described in (Kanahe
et al., 1998; Nivre and Nilsson, 2005) is an impor-
tant topic out of the scope of this paper.
</bodyText>
<sectionHeader confidence="0.987205" genericHeader="method">
3 The Optimum Tree Search in DF
</sectionHeader>
<bodyText confidence="0.99998225">
This section shows the graph branch algorithm
based on the B&amp;B principle, which searches for
the optimum well-formed tree in a DF by apply-
ing problem expansions called graph branching.
</bodyText>
<subsectionHeader confidence="0.992629">
3.1 Outline of B&amp;B Method
</subsectionHeader>
<bodyText confidence="0.999904">
The B&amp;B method(Ibaraki, 1978) is a principle
for solving computationally hard problems such
as NP-complete problems. The basic strategy is
that the original problem is decomposed into eas-
ier partial-problems (branching) and the original
problem is solved by solving them. Pruning called
a bound operation is applied if it turns out that the
optimum solution to a partial-problem is inferior
to the solution obtained from some other partial-
problem (dominance test)7, or if it turns out that
a partial-problem gives no optimum solutions to
the original problem (maximum value test). Usu-
ally, the B&amp;B algorithm is constructed to mini-
mize the value of the solution. The graph branch
algorithm in this paper is constructed to maximize
the score of the solution because the best solution
is the maximum tree in the DF.
</bodyText>
<subsectionHeader confidence="0.999474">
3.2 Graph Branch Algorithm
</subsectionHeader>
<bodyText confidence="0.9997515">
The graph branch algorithm is obtained by defin-
ing the components of the original B&amp;B skeleton
algorithm, i.e. the partial-problem, the feasible so-
lution, the lower bound value, the upper bound
value, the branch operation, and so on(Ibaraki,
1978). Figure 4 shows the graph branch algorithm
which has been extended from the original B&amp;B
skeleton algorithm to search for all the optimum
trees in a DF. The following sections explain the
B&amp;B components of the graph branch algorithm.
</bodyText>
<footnote confidence="0.973225">
7The dominance test is not used in the graph branch algo-
rithm.
</footnote>
<figureCaption confidence="0.944859">
Figure 4: Graph branch algorithm
(1) Partial-problem
</figureCaption>
<bodyText confidence="0.880924">
Partial-problem Pi in the graph branch algo-
rithm is a problem searching for all the well-
formed optimum trees in a DF DFi consisting of
the dependency graph DGi and constraint matrix
CMi. Pi consists of the following elements.
</bodyText>
<listItem confidence="0.9860216">
(a) Dependency graph DGi
(b) Constraint matrixCMi
(c) Feasible solution valueLBi
(d) Upper bound value UBi
(e) Inconsistent arc pair list IAPLi
</listItem>
<bodyText confidence="0.9997397">
The constraint matrix is common to all partial-
problems, so one CM is shared by all partial-
problems. DGi is represented by “rem[..]” which
shows a set of arcs to be removed from the whole
dependency graph DG. For example, “rem[b, d]”
represents a partial dependency graph [a, c, e] in
the case DG = [a, b, c, d, e]. IAPLi is a list of
inconsistent arc pairs. An inconsistent arc pair
is an arc pair which does not satisfy some co-
occurrence constraint.
</bodyText>
<page confidence="0.995897">
364
</page>
<listItem confidence="0.8586615">
(2) Algorithm for Obtaining Feasible Solution
and Lower Bound Value
</listItem>
<bodyText confidence="0.999912333333333">
In the graph branch algorithm, a well-formed
dependency tree in the dependency graph DG of
the partial-problem P is assigned as the feasible
solution FS of P8. The score of the feasible solu-
tion FS is assigned as the lower bound value LB.
The function for computing these values get f s is
called a feasible solution/lower bound value func-
tion. The details are not shown due to space lim-
itations, but get f s is realized by the backtrack-
based depth-first search algorithm with the opti-
mization based on the arc scores. get f s assures
that the obtained solution satisfies the covering
constraint and the arc co-occurrence constraint.
The incumbent value z (the best score so far) is
replaced by the LB at S3 in Figure 4 if needed.
</bodyText>
<listItem confidence="0.646095">
(3) Algorithm for Obtaining Upper Bound
</listItem>
<bodyText confidence="0.999969285714286">
Given a set of arcs A which is a subset of DG,
if the set of dependent nodes9 of arcs in A satisfies
the covering constraint, the arc set A is called the
well-covered arc set. The maximum well-covered
arc set is defined as a well-covered arc set with
the highest score. In general, the maximum well-
covered arc set does not satisfy the SRC and does
not form a tree. In the graph branch algorithm, the
score of the maximum well-covered arc set of a de-
pendency graphGis assigned as the upper bound
valueUBof the partial-problemP. Upper bound
functiongetubcalculatesUBby scanning the arc
lists sorted by the surface position of the depen-
dent nodes of the arcs.
</bodyText>
<listItem confidence="0.730505">
(4) Branch Operation
</listItem>
<bodyText confidence="0.998517333333333">
Figure 5 shows a branch operation called a
graph branch operation. Child partial-problems of
Pare constructed as follows:
</bodyText>
<listItem confidence="0.99073">
(a) Search for an inconsistent arc pair(arci,arcj)
in the maximum well-covered arc set of the
DG of P.
(b) Create child partial-problems Pi, Pj which
have new DGs DGi = DG — {arcj } and
DGj = DG — {arci} respectively.
</listItem>
<bodyText confidence="0.9987155">
Since a solution to P cannot have both arci and
arcj simultaneously due to the co-occurrence con-
straint, the optimum solution of P is obtained
from either/both Pi or/and Pj. The child partial-
</bodyText>
<footnote confidence="0.96904">
8A feasible solution may not be optimum but is a possible
interpretation of a sentence. Therefore, it can be used as an
approximate output when the search process is aborted.
9The dependent node of an arc is the node located at the
source of the arc.
</footnote>
<figureCaption confidence="0.999285">
Figure 5: Graph branching
</figureCaption>
<bodyText confidence="0.971713">
problem is easier than the parent partial-problem
because the size of the DG of the child partial-
problem is less than that of its parent.
In Figure 4, get iapl computes the list of incon-
sistent arc pairs IAPL(Inconsistent Arc Pair List)
for the maximum well-covered arc set of Pi. Then
the graph branch function graph branch selects
one inconsistent arc pair (arci, arcj) from IAPL
for branch operation. The selection criteria for
(arci, arcj) affects the efficiency of the algorithm.
graph branch selects the inconsistent arc pair
containing the highest score arc in BACL(Branch
Arc Candidates List). graph branch calculates
the upper bound value for a child partial-problem
by get ub and sets it to the child partial-problem.
</bodyText>
<listItem confidence="0.607205">
(5) Selection of Partial-problem
</listItem>
<bodyText confidence="0.999758142857143">
selectproblememploys the best bound search
strategy, i.e. it selects the partial-problem which
has the maximum bound value among the active
partial-problems. It is known that the number of
partial-problems decomposed during computation
is minimized by this strategy in the case that no
dominance tests are applied (Ibaraki, 1978).
</bodyText>
<listItem confidence="0.744244">
(6) Computing All Optimum Solutions
</listItem>
<bodyText confidence="0.999795533333333">
In order to obtain all optimum solutions, partial-
problems whose upper bound values are equal to
the score of the optimum solution(s) are expanded
atS8in Figure 4. In the case that at least one
inconsistent arc pair remains in a partial-problem
(i.e. IAPL=y�{}), graph branch is performed
based on the inconsistent arc pair. Otherwise,
the obtained optimum solution FS is checked if
one of the arcs in FS has an equal rival arc by
arcs with alternatives function. The equal ri-
val arc of arc A is an arc whose position and score
are equal to those of arcA. If an equal rival arc
of an arc inFSexists, a new partial-problem is
generated by removing the arc inFS.S8assures
that no partial-problem has an upper bound value
</bodyText>
<table confidence="0.7143472">
DG: Dependency graph
of parent problem
Remove arcj arcj Remove arci
arci
arcj
arci
DGi: Dependency graph
for child problem Pi
DGj: Dependency graph
for child problem Pj
</table>
<page confidence="0.920912">
365
</page>
<note confidence="0.5944">
P0
</note>
<figureCaption confidence="0.999675">
Figure 6: Search diagram
</figureCaption>
<bodyText confidence="0.9965885">
greater than or equal to the score of the optimum
solutions when the computation stopped.
</bodyText>
<sectionHeader confidence="0.897025" genericHeader="method">
4 Example of Optimum Tree Search
</sectionHeader>
<bodyText confidence="0.9997945">
This section shows an example of the graph branch
algorithm execution using the DF in Fig.3.
</bodyText>
<subsectionHeader confidence="0.99955">
4.1 Example of Graph Branch Algorithm
</subsectionHeader>
<bodyText confidence="0.99978811627907">
The search process of the B&amp;B method can be
shown as a search diagram constructing a partial-
problem tree representing the parent-child relation
between the partial-problems. Figure 6 is a search
diagram for the example DF showing the search
process of the graph branch algorithm.
In this figure, box Pi is a partial-problem with
its dependency graph rem, upper bound value
UB, feasible solution and lower bound value LB
and inconsistent arc pair list IAPL. Suffix i of Pi
indicates the generation order of partial-problems.
Updating of global variable z (incumbent value)
and O (set of incumbent solutions) is shown un-
der the box. The value of the left-hand side of the
arrow is updated to that of right-hand side of the
arrow during the partial-problem processing. De-
tails of the behavior of the algorithm in Figure 4
are described below.
In S1(initialize), z, O and AP are set to
—1, { } and {P0 } respectively. The DG of P0 is
that of the example DF. This is represented by
rem = []. get ub sets the upper bound value
(=63) of P0 to UB. In practice, this is calcu-
lated by obtaining the maximum well-covered arc
set of P0. In S2 (search), select problem selects
P0 and get f s(P0) is executed. The feasible so-
lution FS and its score LB are calculated to set
FS = [14, 2,16, 23, 29], LB = 50 (P0 in the
search diagram). S3 (incumbent value update)
updates z and O to new values. Then,
get iapl(P0) computes the inconsistent arc pair
list [(2,15), (15, 23), (23,18), (2,18)] from the
maximum well-covered arc set [14, 2,15, 23,18]
and set it to IAPL. S5(maximum value test)
compares the upper bound value UB and the fea-
sible solution value LB. In this case, LB &lt; UB
holds, so BACL is assigned the value of IAPL.
The next step S6 (branch operation) executes the
graph branch function. graph branch selects
the arc pair with the highest arc score and performs
the graph branch operation with the selected arc
pair. The following is a BACL shown with the
arc names and arc scores.
</bodyText>
<equation confidence="0.965035">
[(nc2[17],pre15[10]), (pre15[10], sub23[10]),
(sub23[10], vpp18[9]), (nc2[17], vpp18[9])]
</equation>
<bodyText confidence="0.999834111111111">
Scores are shown in [ ]. The arc pair contain-
ing the highest arc score is (2,15) and (2,18)
containing nc2[17]. Here, (2,15) is selected and
partial-problems P1 (rem [2]) and P2 (rem [15]) are
generated. P0 is removed from AP and the new
two partial-problems are added to AP resulting in
AP = {P1, P2}. Then, based on the best bound
search strategy, S2 (search) is tried again.
P1 updates z and O because P1 obtained a
feasible solution better than that in O obtained
by P0. P2 and P4 are terminated because they
have no feasible solution. P3 generates a feasi-
ble solution but z and O are not updated. This
is because the obtained feasible solution is infe-
rior to the incumbent solution in O. The optimum
solution(={[14, 24,15, 31,18]}) is obtained by P1.
The computation from P2 to P4 is required to as-
sure that the feasible solution ofP1is optimum.
</bodyText>
<sectionHeader confidence="0.995572" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999703666666667">
This section describes some experimental results
showing the computational amount of the graph
branch algorithm.
</bodyText>
<subsectionHeader confidence="0.996568">
5.1 Environment and Performance Metric
</subsectionHeader>
<bodyText confidence="0.999095">
An existing sentence analysis system10 (called the
oracle system) is used as a generator of the test
corpus, the preference knowledge source and the
correct answers. Experiment data of 125,320 sen-
tences11 extracted from English technical docu-
</bodyText>
<footnote confidence="0.91870775">
10A real-world rule-based machine translation system with
a long development history
11Sentences ending with a period and parsable by the ora-
cle system.
</footnote>
<figure confidence="0.7197835">
P3
P4
P1
P2
</figure>
<page confidence="0.996534">
366
</page>
<bodyText confidence="0.991759685714286">
ments is divided into open data (8605 sentences)
and closed data (116,715 sentences). The prefer-
ence score source, i.e. the WPP frequencies and
the dependency relation frequencies are obtained
from the closed data. The basic PDG grammar
(907 extended CFG rules) is used for generating
the DFs for the open data.
The expanded problem number (EPN), a prin-
cipal computational amount factor of the B&amp;B
method, is adopted as the base metric. The fol-
lowing three metrics are used in this experiment.
(a) EPN in total (EPN-T): The number of the ex-
panded problems which are generated in the
entire search process.
(b) EPN for the first optimum solution (EPN-F):
The number of the expanded problems when
the first optimum solution is obtained.
(c) EPN for the last optimum solution (EPN-L):
The number of the expanded problems when
the last optimum solution is obtained. At this
point, all optimum solutions are obtained.
Optimum solution number (OSN) for a problem,
i.e. the number of optimum dependency trees in
a given DF, gives the lower bound value for all
these metrics because one problem generates at
most one solution. The minimum value of OSN
is 1 because every DF has at least one dependency
tree. As the search process proceeds, the algorithm
finds the first optimum solution, then the last opti-
mum solution, and finally terminates the process
by confirming no better solution is left. There-
fore, the three metrics and OSN have the relation
OSN&lt;EPN-F&lt;EPN-L&lt;EPN-T. Average val-
ues for these are described as Ave:OSN, Ave:EPN-
F, Ave:EPN-L and Ave:EPN-T.
</bodyText>
<subsectionHeader confidence="0.993799">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999873666666667">
An evaluation experiment for the open data is
performed using a prototype PDG system imple-
mented in Prolog. The sentences containing more
than 22 words are neglected due to the limita-
tion of Prolog system resources in the parsing pro-
cess. 4334 sentences out of the remaining 6882
test sentences are parsable. Unparsable sentences
(2584 sentences) are simply neglected in this ex-
periment. The arc precision ratio12 of the oracle
</bodyText>
<footnote confidence="0.5567945">
12Correct arc ratio with respect to arcs in the output depen-
dency trees (Hirakawa, 2005).
</footnote>
<figureCaption confidence="0.999096">
Figure 7: EPN-T, EPN-F EPN-F and OSN
</figureCaption>
<bodyText confidence="0.99986895">
system for 136 sentences in this sentence set is
97.2% with respect to human analysis results.
All optimum trees are computed by the graph
branch algorithm described in Section 3.2. Fig-
ure 7 shows averages of EPN-T, EPN-L, EPN-F
and OSN with respect to sentence length. Over-
all averages of EPN-T, EPN-L, EPN-F and OSN
for the test sentences are 3.0, 1.67, 1.43 and 1.15.
The result shows that the average number of prob-
lems required is relatively small. The gap between
Ave:EPN-T and Ave:EPN-L (3.0-1.67=1.33) is
much greater than the gap between Ave:EPN-L
and Ave:OSN(1.67-1.15=0.52). This means that
the major part of the computation is performed
only for checking if the obtained feasible solutions
are optimum or not.
According to (Hirakawa, 2001), the experiment
on the B&amp;B-based algorithm for the SDG shows
the overall averages of AVE:EPN-T, AVE:EPN-
F are 2.91, 1.33 and the average CPU time is
305.8ms (on EWS). These values are close to
those in the experiment based on the graph branch
algorithm. Two experiments show a tendency for
the optimum solution to be obtained in the early
stage of the search process. The graph branch al-
gorithm is expected to obtain the comparable per-
formance with the SDG search algorithm.
(Hirakawa, 2001) introduced the improved up-
per bound function g’(P) into the B&amp;B-based al-
gorithm for the SDG and found Ave:EPN-T is re-
duced from 2.91 to 1.82. The same technique
is introduced to the graph branch algorithm and
has obtained the reduction of the Ave:EPN-T from
3.00 to 2.68.
The tendency for the optimum solution to be
obtained in the early stage of the search process
suggests that limiting the number of problems to
expand is an effective pruning strategy. Figure
8 shows the ratios of the sentences obtaining the
whole problem expansion, the first optimum solu-
</bodyText>
<page confidence="0.996544">
367
</page>
<figureCaption confidence="0.999621">
Figure 8: ARs for EPS-F, EPS-A, EPS-T
</figureCaption>
<bodyText confidence="0.999944714285714">
tion and the last optimum solution to whole sen-
tences with respect to the EPNs. This kind of ratio
is called an achievement ratio (AR) in this paper.
From Figure 8, the ARs for EPN-T, EPN-L, EPN-
F (plotted in solid lines) are 97.1%,99.6%,99.8%
respectively at the EPN 10. The dotted line shows
the AR for EPN-T of the improved algorithm us-
ing g’(P). The use of g’(P) increases the AR for
EPN-T from 97.1% to 99.1% at the EPN 10. How-
ever, the effect of g’(P) is quite small for EPN-
F and EPN-L. This result shows that the pruning
strategy based on the EPN is effective and g’(P)
works for the reduction of the problems generated
in the posterior part of the search processes.
</bodyText>
<sectionHeader confidence="0.99347" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999983529411765">
This paper has described the graph branch algo-
rithm for obtaining the optimum solution for a
DF used in PDG. The well-formedness depen-
dency tree constraints are represented by the con-
straint matrix of the DF, which has flexible and
precise description ability so that controlled non-
projectivity is available in PDG framework. The
graph branch algorithm assures the search for the
optimum trees with arbitrary arc co-occurrence
constraints, including the SVOC which has not
been treated in DP-based algorithms so far. The
experimental result shows the averages of EPN-
T, EPN-L and EPN-F for English test sentences
are 3.0, 1.67 and 1.43, respectively. The practi-
cal code implementation of the graph branch algo-
rithm and its performance evaluation are subjects
for future work.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999584789473684">
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233–240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings
of COLING’96, pages 340–345.
M. Harada and T. Mizuno. 2001. Japanese semantic
analysis system sage using edr (in japanese). Trans-
actions ofJSAI, 16(1):85–93.
H. Hirakawa. 2001. Semantic dependency analysis
method for japanese based on optimum tree search
algorithm. In Proceedings of the PACLING2001.
H. Hirakawa. 2005. Evaluation measures for natural
language analyser based on preference dependency
grammar. In Proceedings of the PACLING2005.
H. Hirakawa. 2006. Preference dependency grammar
and its packed shared data structure ’dependency
forest’ (in japanese). To appear in Natural Lan-
guage Processing, 13(3).
T. Ibaraki. 1978. Branch-and-bounding procedure
and state-space representation of combinatorial opti-
mization problems. Information and Control, 36,1-
27.
S. Kanahe, A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-
ACL’98, pages 646–52.
N. Katoh and T. Ehara. 1989. A fast algorithm for
dependency structure analysis (in japanese). In Pro-
ceedings of 39th Annual Convention of the Informa-
tion Processing Society.
S. Lee and K. S. Choi. 1997. Reestimation and best-
first parsing algorithm for probablistic dependency
grammars. In Proceedings of the Fifth Workshop on
Very Large Corpora, pages 41–55.
H. Maruyama. 1990. Constraint dependency grammar
and its weak generative capacity. Computer Soft-
ware.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of HLT-
EMNLP, pages 523–530.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In ACL-05, pages 99–106.
K. Ozeki and Y. Zhang. 1999. Jk+=�1 3��PP99
LLZ6D*,h_�x&apos;V11W*. In Proceeding of the Work-
shop of The Fifth Annual Meeting of The Association
for Natural Language Processing, pages 9–14.
K. Ozeki. 1994. Dependency structure analysis as
combinatorial optimization. Information Sciences,
78(1-2):77–99.
W. Wang and M. P. Harper. 2004. A statistical con-
straint dependency grammar (cdg) parser. In Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 42–49.
</reference>
<page confidence="0.998238">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549948">
<title confidence="0.9985995">Graph Branch Algorithm: An Optimum Tree Search Method for Scored Dependency Graph with Arc Co-occurrence Constraints</title>
<author confidence="0.939116">Hideki Hirakawa</author>
<affiliation confidence="0.873297">Toshiba R&amp;D Center</affiliation>
<address confidence="0.8448735">1 Komukai Toshiba-cho, Saiwai-ku, Kawasaki 210, JAPAN</address>
<email confidence="0.951847">hideki.hirakawa@toshiba.co.jp</email>
<abstract confidence="0.9927206875">Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms. This paper classifies the scored dependency graphs and discusses the specific features of the “Dependency Forest” (DF) which is the packed shared data structure adopted in the “Preference Dependency Grammar” (PDG), and proposes the “Graph Branch Algorithm” for computing the optimum dependency tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<contexts>
<context position="6286" citStr="Chu and Liu, 1965" startWordPosition="968" endWordPosition="971">5 (C4) Single valence occupation constraint(SVOC): No two arcs in a tree occupy the same valence of a predicate (C1) and (C2), collectively referred to as “covering constraint”, are basic constraints adopted by almost all dependency parsers. (C3) is adopted by the majority of dependency parsers which are called projective dependency parsers. A projective dependency parser fails to analyze non-projective sentences. (C4) is a basic constraint for valency but is not adopted by the majority of dependency parsers. Graph search algorithms, such as the ChuLiu-Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967), algorithms based on the dynamic programming (DP) principle (Ozeki, 1994; Eisner, 1996) and the algorithm based on the B&amp;B method (Hirakawa, 2001), are used for the optimum tree search in scored DGs. The applicability of these algorithms is closely related to the types of DGs and/or well-formedness constraints. The Chu-Liu-Edmonds algorithm is very fast (O(n2)for sentence lengthn), but it works correctly only on word DGs. DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4. (C2)-(C4) can be described as a set of cooccurr</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y. J. Chu and T. H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<pages>71--233</pages>
<contexts>
<context position="6302" citStr="Edmonds, 1967" startWordPosition="972" endWordPosition="973">ce occupation constraint(SVOC): No two arcs in a tree occupy the same valence of a predicate (C1) and (C2), collectively referred to as “covering constraint”, are basic constraints adopted by almost all dependency parsers. (C3) is adopted by the majority of dependency parsers which are called projective dependency parsers. A projective dependency parser fails to analyze non-projective sentences. (C4) is a basic constraint for valency but is not adopted by the majority of dependency parsers. Graph search algorithms, such as the ChuLiu-Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967), algorithms based on the dynamic programming (DP) principle (Ozeki, 1994; Eisner, 1996) and the algorithm based on the B&amp;B method (Hirakawa, 2001), are used for the optimum tree search in scored DGs. The applicability of these algorithms is closely related to the types of DGs and/or well-formedness constraints. The Chu-Liu-Edmonds algorithm is very fast (O(n2)for sentence lengthn), but it works correctly only on word DGs. DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4. (C2)-(C4) can be described as a set of cooccurrence constraints</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING’96,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="4473" citStr="Eisner, 1996" startWordPosition="701" endWordPosition="702">of dependency trees. 2.2 Dependency Graph DGs are classified into some classes based on the types of nodes and arcs. This paper assumes three types of nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure</context>
<context position="6390" citStr="Eisner, 1996" startWordPosition="985" endWordPosition="986">ate (C1) and (C2), collectively referred to as “covering constraint”, are basic constraints adopted by almost all dependency parsers. (C3) is adopted by the majority of dependency parsers which are called projective dependency parsers. A projective dependency parser fails to analyze non-projective sentences. (C4) is a basic constraint for valency but is not adopted by the majority of dependency parsers. Graph search algorithms, such as the ChuLiu-Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967), algorithms based on the dynamic programming (DP) principle (Ozeki, 1994; Eisner, 1996) and the algorithm based on the B&amp;B method (Hirakawa, 2001), are used for the optimum tree search in scored DGs. The applicability of these algorithms is closely related to the types of DGs and/or well-formedness constraints. The Chu-Liu-Edmonds algorithm is very fast (O(n2)for sentence lengthn), but it works correctly only on word DGs. DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4. (C2)-(C4) can be described as a set of cooccurrence constraints between two arcs in a DG. As described in Section 2.6, the DF can represent (C2)-(C4) a</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING’96, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Harada</author>
<author>T Mizuno</author>
</authors>
<title>Japanese semantic analysis system sage using edr (in japanese). Transactions ofJSAI,</title>
<date>2001</date>
<pages>16--1</pages>
<contexts>
<context position="4733" citStr="Harada and Mizuno, 2001" startWordPosition="732" endWordPosition="735">DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm. (C1) Coverage constraint: Every input word has a corresponding node in the tree (C2) Single role constraint(SRC): No two nodes in a dependency tree occupy the same input position 2WPP is a pair of a word and a part of</context>
</contexts>
<marker>Harada, Mizuno, 2001</marker>
<rawString>M. Harada and T. Mizuno. 2001. Japanese semantic analysis system sage using edr (in japanese). Transactions ofJSAI, 16(1):85–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hirakawa</author>
</authors>
<title>Semantic dependency analysis method for japanese based on optimum tree search algorithm.</title>
<date>2001</date>
<booktitle>In Proceedings of the PACLING2001.</booktitle>
<contexts>
<context position="1301" citStr="Hirakawa, 2001" startWordPosition="189" endWordPosition="190">ncy tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm. 1 Introduction The dependency graph (DG) is a packed shared data structure which consists of the nodes corresponding to the words in a sentence and the arcs showing dependency relations between the nodes. The scored DG has preference scores attached to the arcs and is widely used as a basis of the optimum tree search method. For example, the scored DG is used in Japanese Kakari-uke analysis1 to represent all possible kakari-uke(dependency) trees(Ozeki, 1994),(Hirakawa, 2001). (McDonald et al., 2005) proposed a dependency analysis method using a scored DG and some maximum spanning tree search algorithms. In this method, scores on arcs are computed from a set of features obtained from the dependency trees based on the 1Kakari-uke relation, widely adopted in Japanese sentence analysis, is projective dependency relation with a constraint such that the dependent word is located at the left-hand side of its governor word. optimum parameters for scoring dependency arcs obtained by the discriminative learning method. There are various kinds of dependency analysis methods</context>
<context position="4580" citStr="Hirakawa, 2001" startWordPosition="714" endWordPosition="715">es and arcs. This paper assumes three types of nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm. (C1) Coverage constraint: Every input word has a corresponding n</context>
<context position="6449" citStr="Hirakawa, 2001" startWordPosition="995" endWordPosition="996">constraint”, are basic constraints adopted by almost all dependency parsers. (C3) is adopted by the majority of dependency parsers which are called projective dependency parsers. A projective dependency parser fails to analyze non-projective sentences. (C4) is a basic constraint for valency but is not adopted by the majority of dependency parsers. Graph search algorithms, such as the ChuLiu-Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967), algorithms based on the dynamic programming (DP) principle (Ozeki, 1994; Eisner, 1996) and the algorithm based on the B&amp;B method (Hirakawa, 2001), are used for the optimum tree search in scored DGs. The applicability of these algorithms is closely related to the types of DGs and/or well-formedness constraints. The Chu-Liu-Edmonds algorithm is very fast (O(n2)for sentence lengthn), but it works correctly only on word DGs. DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4. (C2)-(C4) can be described as a set of cooccurrence constraints between two arcs in a DG. As described in Section 2.6, the DF can represent (C2)-(C4) and more precise constraints because it can handle co-occurr</context>
<context position="10514" citStr="Hirakawa, 2001" startWordPosition="1663" endWordPosition="1664">y, MTCM corresponds to probabilistic CKY and the Viterbi algorithm because it computes both the optimum tree score and its structure. nc2 npp19 rt29 obj16 Meaning of Arc Name sub : subject obj : object npp : noun-preposition vpp : verb-preposition pre : preposition nc : noun compound det : determiner rt : root Constraint Matrix Figure 3: Scored dependency forest 2.5 Semantic Dependency Graph (SDG) The SDG is a semantic-label word DG designed for Japanese sentence analysis. The optimum tree search algorithm searches for the optimum tree satisfying the well-formed constraints (C1)-(C4) in a SDG(Hirakawa, 2001). This method is lacking in terms of generality in that it cannot handle backward dependency and multiple WPP because it depends on some linguistic features peculiar to Japanese. Therefore, this method is inherently inapplicable to languages like English that require backward dependency and multiple POS analysis. The DF described below can be seen as the extension of the SDG. Since the DF has none of the language-dependent premises that the SDG has, it is applicable to English and other languages. 2.6 Dependency Forest (DF) The DF is a packed shared data structure encompassing all possible dep</context>
<context position="26283" citStr="Hirakawa, 2001" startWordPosition="4320" endWordPosition="4321">ph branch algorithm described in Section 3.2. Figure 7 shows averages of EPN-T, EPN-L, EPN-F and OSN with respect to sentence length. Overall averages of EPN-T, EPN-L, EPN-F and OSN for the test sentences are 3.0, 1.67, 1.43 and 1.15. The result shows that the average number of problems required is relatively small. The gap between Ave:EPN-T and Ave:EPN-L (3.0-1.67=1.33) is much greater than the gap between Ave:EPN-L and Ave:OSN(1.67-1.15=0.52). This means that the major part of the computation is performed only for checking if the obtained feasible solutions are optimum or not. According to (Hirakawa, 2001), the experiment on the B&amp;B-based algorithm for the SDG shows the overall averages of AVE:EPN-T, AVE:EPNF are 2.91, 1.33 and the average CPU time is 305.8ms (on EWS). These values are close to those in the experiment based on the graph branch algorithm. Two experiments show a tendency for the optimum solution to be obtained in the early stage of the search process. The graph branch algorithm is expected to obtain the comparable performance with the SDG search algorithm. (Hirakawa, 2001) introduced the improved upper bound function g’(P) into the B&amp;B-based algorithm for the SDG and found Ave:EP</context>
</contexts>
<marker>Hirakawa, 2001</marker>
<rawString>H. Hirakawa. 2001. Semantic dependency analysis method for japanese based on optimum tree search algorithm. In Proceedings of the PACLING2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hirakawa</author>
</authors>
<title>Evaluation measures for natural language analyser based on preference dependency grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of the PACLING2005.</booktitle>
<contexts>
<context position="25493" citStr="Hirakawa, 2005" startWordPosition="4190" endWordPosition="4191">es for these are described as Ave:OSN, Ave:EPNF, Ave:EPN-L and Ave:EPN-T. 5.2 Experimental Results An evaluation experiment for the open data is performed using a prototype PDG system implemented in Prolog. The sentences containing more than 22 words are neglected due to the limitation of Prolog system resources in the parsing process. 4334 sentences out of the remaining 6882 test sentences are parsable. Unparsable sentences (2584 sentences) are simply neglected in this experiment. The arc precision ratio12 of the oracle 12Correct arc ratio with respect to arcs in the output dependency trees (Hirakawa, 2005). Figure 7: EPN-T, EPN-F EPN-F and OSN system for 136 sentences in this sentence set is 97.2% with respect to human analysis results. All optimum trees are computed by the graph branch algorithm described in Section 3.2. Figure 7 shows averages of EPN-T, EPN-L, EPN-F and OSN with respect to sentence length. Overall averages of EPN-T, EPN-L, EPN-F and OSN for the test sentences are 3.0, 1.67, 1.43 and 1.15. The result shows that the average number of problems required is relatively small. The gap between Ave:EPN-T and Ave:EPN-L (3.0-1.67=1.33) is much greater than the gap between Ave:EPN-L and </context>
</contexts>
<marker>Hirakawa, 2005</marker>
<rawString>H. Hirakawa. 2005. Evaluation measures for natural language analyser based on preference dependency grammar. In Proceedings of the PACLING2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hirakawa</author>
</authors>
<title>Preference dependency grammar and its packed shared data structure ’dependency forest’ (in japanese).</title>
<date>2006</date>
<journal>Natural Language Processing,</journal>
<volume>13</volume>
<issue>3</issue>
<note>To appear in</note>
<contexts>
<context position="2097" citStr="Hirakawa, 2006" startWordPosition="318" endWordPosition="319">a set of features obtained from the dependency trees based on the 1Kakari-uke relation, widely adopted in Japanese sentence analysis, is projective dependency relation with a constraint such that the dependent word is located at the left-hand side of its governor word. optimum parameters for scoring dependency arcs obtained by the discriminative learning method. There are various kinds of dependency analysis methods based on the scored DGs. This paper classifies these methods based on the types of the DGs and the basic well-formed constraints and explains the features of the DF adopted in PDG(Hirakawa, 2006). This paper proposes the graph branch algorithm which searches the optimum dependency tree from a DF based on the branch and bound (B&amp;B) method(Ibaraki, 1978) and reports the experiment showing the computational amount and behavior of the graph branch algorithm. As shown below, the combination of the DF and the graph branch algorithm enables the treatment of non-projective dependency analysis and optimum solution search satisfying the single valence occupation constraint, which are out of the scope of most of the DP(dynamic programming)- based parsing methods. 2 Optimum Tree Search in a Score</context>
<context position="12345" citStr="Hirakawa, 2006" startWordPosition="1964" endWordPosition="1965"> grammar rule is an extended CFG rule, which defines the mapping between a sequence of constituents (the body of a CFG rule) and a set of arcs (a partial dependency tree). target2,10 agent1,15 target4,7 agent3,5 in-state7,10 Isha-mo (doctor) Wakaranai (not_know) Byouki-no (sickness) Kanja (patient) OS2[10]: (in-state7,10) rt32 vpp20 root pre15 rt31 0,time/n sub24 1,fly/v vpp18 2,like/p 3,an/det det14 4,arrow/n 0,time/v obj4 1,fly/n sub23 2,like/v Dependency Graph 363 The generated CM assures that the parse trees in the parse forest and the dependency trees in the DF have mutual correspondence(Hirakawa, 2006). CM can represent (C2)-(C4) in 2.3 and more precise constraints. For example, PDG can generate a DF encompassing non-projective dependency trees by introducing the grammar rules defining non-projective constructions. This is called the controlled non-projectivity in this paper. Treatment of non-projectivity as described in (Kanahe et al., 1998; Nivre and Nilsson, 2005) is an important topic out of the scope of this paper. 3 The Optimum Tree Search in DF This section shows the graph branch algorithm based on the B&amp;B principle, which searches for the optimum well-formed tree in a DF by applying</context>
</contexts>
<marker>Hirakawa, 2006</marker>
<rawString>H. Hirakawa. 2006. Preference dependency grammar and its packed shared data structure ’dependency forest’ (in japanese). To appear in Natural Language Processing, 13(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ibaraki</author>
</authors>
<title>Branch-and-bounding procedure and state-space representation of combinatorial optimization problems.</title>
<date>1978</date>
<journal>Information and Control,</journal>
<pages>36--1</pages>
<contexts>
<context position="2256" citStr="Ibaraki, 1978" startWordPosition="344" endWordPosition="345">elation with a constraint such that the dependent word is located at the left-hand side of its governor word. optimum parameters for scoring dependency arcs obtained by the discriminative learning method. There are various kinds of dependency analysis methods based on the scored DGs. This paper classifies these methods based on the types of the DGs and the basic well-formed constraints and explains the features of the DF adopted in PDG(Hirakawa, 2006). This paper proposes the graph branch algorithm which searches the optimum dependency tree from a DF based on the branch and bound (B&amp;B) method(Ibaraki, 1978) and reports the experiment showing the computational amount and behavior of the graph branch algorithm. As shown below, the combination of the DF and the graph branch algorithm enables the treatment of non-projective dependency analysis and optimum solution search satisfying the single valence occupation constraint, which are out of the scope of most of the DP(dynamic programming)- based parsing methods. 2 Optimum Tree Search in a Scored DG 2.1 Basic Framework Figure 1 shows the basic framework of the optimum dependency tree search in a scored DG. In general, nodes in a DG correspond to words</context>
<context position="13044" citStr="Ibaraki, 1978" startWordPosition="2076" endWordPosition="2077">n generate a DF encompassing non-projective dependency trees by introducing the grammar rules defining non-projective constructions. This is called the controlled non-projectivity in this paper. Treatment of non-projectivity as described in (Kanahe et al., 1998; Nivre and Nilsson, 2005) is an important topic out of the scope of this paper. 3 The Optimum Tree Search in DF This section shows the graph branch algorithm based on the B&amp;B principle, which searches for the optimum well-formed tree in a DF by applying problem expansions called graph branching. 3.1 Outline of B&amp;B Method The B&amp;B method(Ibaraki, 1978) is a principle for solving computationally hard problems such as NP-complete problems. The basic strategy is that the original problem is decomposed into easier partial-problems (branching) and the original problem is solved by solving them. Pruning called a bound operation is applied if it turns out that the optimum solution to a partial-problem is inferior to the solution obtained from some other partialproblem (dominance test)7, or if it turns out that a partial-problem gives no optimum solutions to the original problem (maximum value test). Usually, the B&amp;B algorithm is constructed to min</context>
<context position="18693" citStr="Ibaraki, 1978" startWordPosition="3031" endWordPosition="3032">gorithm. graph branch selects the inconsistent arc pair containing the highest score arc in BACL(Branch Arc Candidates List). graph branch calculates the upper bound value for a child partial-problem by get ub and sets it to the child partial-problem. (5) Selection of Partial-problem selectproblememploys the best bound search strategy, i.e. it selects the partial-problem which has the maximum bound value among the active partial-problems. It is known that the number of partial-problems decomposed during computation is minimized by this strategy in the case that no dominance tests are applied (Ibaraki, 1978). (6) Computing All Optimum Solutions In order to obtain all optimum solutions, partialproblems whose upper bound values are equal to the score of the optimum solution(s) are expanded atS8in Figure 4. In the case that at least one inconsistent arc pair remains in a partial-problem (i.e. IAPL=y�{}), graph branch is performed based on the inconsistent arc pair. Otherwise, the obtained optimum solution FS is checked if one of the arcs in FS has an equal rival arc by arcs with alternatives function. The equal rival arc of arc A is an arc whose position and score are equal to those of arcA. If an e</context>
</contexts>
<marker>Ibaraki, 1978</marker>
<rawString>T. Ibaraki. 1978. Branch-and-bounding procedure and state-space representation of combinatorial optimization problems. Information and Control, 36,1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kanahe</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar.</title>
<date>1998</date>
<booktitle>In COLINGACL’98,</booktitle>
<pages>646--52</pages>
<contexts>
<context position="12691" citStr="Kanahe et al., 1998" startWordPosition="2012" endWordPosition="2015">e15 rt31 0,time/n sub24 1,fly/v vpp18 2,like/p 3,an/det det14 4,arrow/n 0,time/v obj4 1,fly/n sub23 2,like/v Dependency Graph 363 The generated CM assures that the parse trees in the parse forest and the dependency trees in the DF have mutual correspondence(Hirakawa, 2006). CM can represent (C2)-(C4) in 2.3 and more precise constraints. For example, PDG can generate a DF encompassing non-projective dependency trees by introducing the grammar rules defining non-projective constructions. This is called the controlled non-projectivity in this paper. Treatment of non-projectivity as described in (Kanahe et al., 1998; Nivre and Nilsson, 2005) is an important topic out of the scope of this paper. 3 The Optimum Tree Search in DF This section shows the graph branch algorithm based on the B&amp;B principle, which searches for the optimum well-formed tree in a DF by applying problem expansions called graph branching. 3.1 Outline of B&amp;B Method The B&amp;B method(Ibaraki, 1978) is a principle for solving computationally hard problems such as NP-complete problems. The basic strategy is that the original problem is decomposed into easier partial-problems (branching) and the original problem is solved by solving them. Prun</context>
</contexts>
<marker>Kanahe, Nasr, Rambow, 1998</marker>
<rawString>S. Kanahe, A. Nasr, and O. Rambow. 1998. Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar. In COLINGACL’98, pages 646–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Katoh</author>
<author>T Ehara</author>
</authors>
<title>A fast algorithm for dependency structure analysis (in japanese).</title>
<date>1989</date>
<booktitle>In Proceedings of 39th Annual Convention of the Information Processing</booktitle>
<publisher>Society.</publisher>
<contexts>
<context position="4634" citStr="Katoh and Ehara, 1989" startWordPosition="720" endWordPosition="723">nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm. (C1) Coverage constraint: Every input word has a corresponding node in the tree (C2) Single role constraint(SRC): No t</context>
<context position="7540" citStr="Katoh and Ehara, 1989" startWordPosition="1177" endWordPosition="1180">s in a DG. As described in Section 2.6, the DF can represent (C2)-(C4) and more precise constraints because it can handle co-occurrence constraints between two arbitrary arcs in a DG. The graph branch algorithm described in Section 3 can find the optimum tree from the DF. 2.4 SVOC and DP (Ozeki and Zhang, 1999) proposed the minimum cost partitioning method (MCPM) which is a partitioning computation based on the recurrence equation where the cost of joining two partitions is the cost of these partitions plus the cost of combining these partitions. MCPM is a generalization of (Ozeki, 1994) and (Katoh and Ehara, 1989) which compute the optimum dependency tree in a scored DG. MCPM is also a generalization of the probabilistic CKY algorithm and the Viterbi algo5Another condition for projectivity, i.e. “no arc covers top node” is equivalent to the crossing arc constraint if special root node , which is a governor of top node, is introduced at the top (or end) of a sentence. 362 target6,5 agent5,15 OS1[15]: (agent1,15) OS3[22]: (agent1,15) + (target4,7) OS4[25]: (agent5,15) + (in-state7,10) NOS1[10]: (target2,10) OS4[25]: (agent5,15) + (in-state7,10) OS1[15]: (agent1,15) NOS2[20]: (target4,10) + (in-state7,10)</context>
</contexts>
<marker>Katoh, Ehara, 1989</marker>
<rawString>N. Katoh and T. Ehara. 1989. A fast algorithm for dependency structure analysis (in japanese). In Proceedings of 39th Annual Convention of the Information Processing Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lee</author>
<author>K S Choi</author>
</authors>
<title>Reestimation and bestfirst parsing algorithm for probablistic dependency grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Workshop on Very Large Corpora,</booktitle>
<pages>41--55</pages>
<contexts>
<context position="4459" citStr="Lee and Choi, 1997" startWordPosition="697" endWordPosition="700">st score in the set of dependency trees. 2.2 Dependency Graph DGs are classified into some classes based on the types of nodes and arcs. This paper assumes three types of nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in </context>
</contexts>
<marker>Lee, Choi, 1997</marker>
<rawString>S. Lee and K. S. Choi. 1997. Reestimation and bestfirst parsing algorithm for probablistic dependency grammars. In Proceedings of the Fifth Workshop on Very Large Corpora, pages 41–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
</authors>
<title>Constraint dependency grammar and its weak generative capacity.</title>
<date>1990</date>
<journal>Computer Software.</journal>
<contexts>
<context position="4540" citStr="Maruyama, 1990" startWordPosition="710" endWordPosition="711">o some classes based on the types of nodes and arcs. This paper assumes three types of nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm. (C1) Coverage constraint</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>H. Maruyama. 1990. Constraint dependency grammar and its weak generative capacity. Computer Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="1326" citStr="McDonald et al., 2005" startWordPosition="191" endWordPosition="195">. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm. 1 Introduction The dependency graph (DG) is a packed shared data structure which consists of the nodes corresponding to the words in a sentence and the arcs showing dependency relations between the nodes. The scored DG has preference scores attached to the arcs and is widely used as a basis of the optimum tree search method. For example, the scored DG is used in Japanese Kakari-uke analysis1 to represent all possible kakari-uke(dependency) trees(Ozeki, 1994),(Hirakawa, 2001). (McDonald et al., 2005) proposed a dependency analysis method using a scored DG and some maximum spanning tree search algorithms. In this method, scores on arcs are computed from a set of features obtained from the dependency trees based on the 1Kakari-uke relation, widely adopted in Japanese sentence analysis, is projective dependency relation with a constraint such that the dependent word is located at the left-hand side of its governor word. optimum parameters for scoring dependency arcs obtained by the discriminative learning method. There are various kinds of dependency analysis methods based on the scored DGs.</context>
<context position="4497" citStr="McDonald et al., 2005" startWordPosition="703" endWordPosition="706">trees. 2.2 Dependency Graph DGs are classified into some classes based on the types of nodes and arcs. This paper assumes three types of nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLTEMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In ACL-05,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="12717" citStr="Nivre and Nilsson, 2005" startWordPosition="2016" endWordPosition="2019">24 1,fly/v vpp18 2,like/p 3,an/det det14 4,arrow/n 0,time/v obj4 1,fly/n sub23 2,like/v Dependency Graph 363 The generated CM assures that the parse trees in the parse forest and the dependency trees in the DF have mutual correspondence(Hirakawa, 2006). CM can represent (C2)-(C4) in 2.3 and more precise constraints. For example, PDG can generate a DF encompassing non-projective dependency trees by introducing the grammar rules defining non-projective constructions. This is called the controlled non-projectivity in this paper. Treatment of non-projectivity as described in (Kanahe et al., 1998; Nivre and Nilsson, 2005) is an important topic out of the scope of this paper. 3 The Optimum Tree Search in DF This section shows the graph branch algorithm based on the B&amp;B principle, which searches for the optimum well-formed tree in a DF by applying problem expansions called graph branching. 3.1 Outline of B&amp;B Method The B&amp;B method(Ibaraki, 1978) is a principle for solving computationally hard problems such as NP-complete problems. The basic strategy is that the original problem is decomposed into easier partial-problems (branching) and the original problem is solved by solving them. Pruning called a bound operati</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In ACL-05, pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ozeki</author>
<author>Y Zhang</author>
</authors>
<date>1999</date>
<booktitle>Jk+=�1 3��PP99 LLZ6D*,h_�x&apos;V11W*. In Proceeding of the Workshop of The Fifth Annual Meeting of The Association for Natural Language Processing,</booktitle>
<pages>9--14</pages>
<contexts>
<context position="7230" citStr="Ozeki and Zhang, 1999" startWordPosition="1124" endWordPosition="1127">raints. The Chu-Liu-Edmonds algorithm is very fast (O(n2)for sentence lengthn), but it works correctly only on word DGs. DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4. (C2)-(C4) can be described as a set of cooccurrence constraints between two arcs in a DG. As described in Section 2.6, the DF can represent (C2)-(C4) and more precise constraints because it can handle co-occurrence constraints between two arbitrary arcs in a DG. The graph branch algorithm described in Section 3 can find the optimum tree from the DF. 2.4 SVOC and DP (Ozeki and Zhang, 1999) proposed the minimum cost partitioning method (MCPM) which is a partitioning computation based on the recurrence equation where the cost of joining two partitions is the cost of these partitions plus the cost of combining these partitions. MCPM is a generalization of (Ozeki, 1994) and (Katoh and Ehara, 1989) which compute the optimum dependency tree in a scored DG. MCPM is also a generalization of the probabilistic CKY algorithm and the Viterbi algo5Another condition for projectivity, i.e. “no arc covers top node” is equivalent to the crossing arc constraint if special root node , which is a </context>
</contexts>
<marker>Ozeki, Zhang, 1999</marker>
<rawString>K. Ozeki and Y. Zhang. 1999. Jk+=�1 3��PP99 LLZ6D*,h_�x&apos;V11W*. In Proceeding of the Workshop of The Fifth Annual Meeting of The Association for Natural Language Processing, pages 9–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ozeki</author>
</authors>
<title>Dependency structure analysis as combinatorial optimization.</title>
<date>1994</date>
<journal>Information Sciences,</journal>
<pages>78--1</pages>
<contexts>
<context position="1284" citStr="Ozeki, 1994" startWordPosition="188" endWordPosition="189">ptimum dependency tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm. 1 Introduction The dependency graph (DG) is a packed shared data structure which consists of the nodes corresponding to the words in a sentence and the arcs showing dependency relations between the nodes. The scored DG has preference scores attached to the arcs and is widely used as a basis of the optimum tree search method. For example, the scored DG is used in Japanese Kakari-uke analysis1 to represent all possible kakari-uke(dependency) trees(Ozeki, 1994),(Hirakawa, 2001). (McDonald et al., 2005) proposed a dependency analysis method using a scored DG and some maximum spanning tree search algorithms. In this method, scores on arcs are computed from a set of features obtained from the dependency trees based on the 1Kakari-uke relation, widely adopted in Japanese sentence analysis, is projective dependency relation with a constraint such that the dependent word is located at the left-hand side of its governor word. optimum parameters for scoring dependency arcs obtained by the discriminative learning method. There are various kinds of dependency</context>
<context position="4610" citStr="Ozeki, 1994" startWordPosition="718" endWordPosition="719">ree types of nodes, i.e. word-type, WPP-type2 and concept-type3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm. (C1) Coverage constraint: Every input word has a corresponding node in the tree (C2) Single ro</context>
<context position="6375" citStr="Ozeki, 1994" startWordPosition="983" endWordPosition="984">e of a predicate (C1) and (C2), collectively referred to as “covering constraint”, are basic constraints adopted by almost all dependency parsers. (C3) is adopted by the majority of dependency parsers which are called projective dependency parsers. A projective dependency parser fails to analyze non-projective sentences. (C4) is a basic constraint for valency but is not adopted by the majority of dependency parsers. Graph search algorithms, such as the ChuLiu-Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967), algorithms based on the dynamic programming (DP) principle (Ozeki, 1994; Eisner, 1996) and the algorithm based on the B&amp;B method (Hirakawa, 2001), are used for the optimum tree search in scored DGs. The applicability of these algorithms is closely related to the types of DGs and/or well-formedness constraints. The Chu-Liu-Edmonds algorithm is very fast (O(n2)for sentence lengthn), but it works correctly only on word DGs. DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4. (C2)-(C4) can be described as a set of cooccurrence constraints between two arcs in a DG. As described in Section 2.6, the DF can repres</context>
</contexts>
<marker>Ozeki, 1994</marker>
<rawString>K. Ozeki. 1994. Dependency structure analysis as combinatorial optimization. Information Sciences, 78(1-2):77–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>M P Harper</author>
</authors>
<title>A statistical constraint dependency grammar (cdg) parser.</title>
<date>2004</date>
<booktitle>In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL),</booktitle>
<pages>42--49</pages>
<contexts>
<context position="4681" citStr="Wang and Harper, 2004" startWordPosition="726" endWordPosition="729">pe3. The types of DGs are called a word DG, a WPP DG and a concept DG, respectively. DGs are also classified into non-labeled and labeled DGs. There are some types of arc labels such as syntactic label (ex. “subject”,“object”) and semantic label (ex. “agent”,“target”). Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001). 2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints. This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm. (C1) Coverage constraint: Every input word has a corresponding node in the tree (C2) Single role constraint(SRC): No two nodes in a dependency tree occupy the same i</context>
</contexts>
<marker>Wang, Harper, 2004</marker>
<rawString>W. Wang and M. P. Harper. 2004. A statistical constraint dependency grammar (cdg) parser. In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL), pages 42–49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>