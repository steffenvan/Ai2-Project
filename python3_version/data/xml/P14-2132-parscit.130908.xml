<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001531">
<title confidence="0.971188">
Observational Initialization of Type-Supervised Taggers
</title>
<author confidence="0.995739">
Hui Zhang∗ John DeNero
</author>
<affiliation confidence="0.994448">
Department of Computer Science Google, Inc.
University of Southern California denero@google.com
</affiliation>
<email confidence="0.990947">
hzhang@isi.edu
</email>
<sectionHeader confidence="0.997314" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956294117647">
Recent work has sparked new interest
in type-supervised part-of-speech tagging,
a data setting in which no labeled sen-
tences are available, but the set of allowed
tags is known for each word type. This
paper describes observational initializa-
tion, a novel technique for initializing EM
when training a type-supervised HMM
tagger. Our initializer allocates probabil-
ity mass to unambiguous transitions in an
unlabeled corpus, generating token-level
observations from type-level supervision.
Experimentally, observational initializa-
tion gives state-of-the-art type-supervised
tagging accuracy, providing an error re-
duction of 56% over uniform initialization
on the Penn English Treebank.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999455703703704">
For many languages, there exist comprehensive
dictionaries that list the possible parts-of-speech
for each word type, but there are no corpora la-
beled with the part-of-speech of each token in con-
text. Type-supervised tagging (Merialdo, 1994)
explores this scenario; a model is provided with
type-level information, such as the fact that “only”
can be an adjective, adverb, or conjunction, but
not any token-level information about which in-
stances of “only” in a corpus are adjectives. Re-
cent research has focused on using type-level su-
pervision to infer token-level tags. For instance,
Li et al. (2012) derive type-level supervision from
Wiktionary, Das and Petrov (2011) and T¨ackstr¨om
et al. (2013) project type-level tag sets across lan-
guages, and Garrette and Baldridge (2013) solicit
type-level annotations directly from speakers. In
all of these efforts, a probabilistic sequence model
is trained to disambiguate token-level tags that are
∗Research conducted during an internship at Google.
constrained to match type-level tag restrictions.
This paper describes observational initialization,
a simple but effective learning technique for train-
ing type-supervised taggers.
A hidden Markov model (HMM) can be used
to disambiguate tags of individual tokens by max-
imizing corpus likelihood using the expectation
maximization (EM) algorithm. Our approach is
motivated by a suite of oracle experiments that
demonstrate the effect of initialization on the fi-
nal tagging accuracy of an EM-trained HMM tag-
ger. We show that initializing EM with accurate
transition model parameters is sufficient to guide
learning toward a high-accuracy final model.
Inspired by this finding, we introduce obser-
vational initialization, which is a simple method
to heuristically estimate transition parameters for
a corpus using type-level supervision. Transi-
tion probabilities are estimated from unambiguous
consecutive tag pairs that arise when two consec-
utive words each have only a single allowed tag.
These unambiguous word pairs can be tagged cor-
rectly without any statistical inference. Initializing
EM with the relative frequency of these unambigu-
ous pairs improves tagging accuracy dramatically
over uniform initialization, reducing errors by
56% in English and 29% in German. This efficient
and data-driven approach gives the best reported
tagging accuracy for type-supervised sequence
models, outperforming the minimized model of
Ravi and Knight (2009), the Bayesian LDA-based
model of Toutanova and Johnson (2008), and an
HMM trained with language-specific initialization
described by Goldberg et al. (2008).
</bodyText>
<sectionHeader confidence="0.994" genericHeader="method">
2 Type-Supervised Tagging
</sectionHeader>
<bodyText confidence="0.9995688">
A first-order Markov model for part-of-speech
tagging defines a distribution over sentences for
which a single tag is given to each word token.
Let wi ∈ W refer to the ith word in a sentence w,
drawn from language vocabulary W. Likewise,
</bodyText>
<page confidence="0.978071">
816
</page>
<bodyText confidence="0.959480666666667">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816–821,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
ti ∈ T is the tag in tag sequence t of the ith word,
drawn from tag inventory T. The joint probabil-
ity of a sentence can be expressed in terms of two
sets of parameters for conditional multinomial dis-
tributions: φ defines the probability of a tag given
its previous tag and θ defines the probability of a
word given its tag.
</bodyText>
<equation confidence="0.997407">
Pφ,θ(w,t) = Y |w |Pφ(ti|ti−1) · Pθ(wi|ti)
i=1
Above, t0 is a fixed start-of-sentence tag.
</equation>
<bodyText confidence="0.899375666666667">
For a set of sentences S, the EM algorithm can
be used to iteratively find a local maximum of the
corpus log-likelihood:
</bodyText>
<equation confidence="0.998563333333333">
&amp;quot;X #
ln Pφ,θ(w, t)
t
</equation>
<bodyText confidence="0.993062">
The parameters φ and θ can then be used to predict
the most likely sequence of tags for each sentence
under the model:
</bodyText>
<equation confidence="0.9990785">
ˆt(w) = arg max
t
</equation>
<bodyText confidence="0.998404">
Tagging accuracy is the fraction of these tags in
ˆt(w) that match hand-labeled oracle tags t∗(w).
Type Supervision. In addition to an unlabeled
corpus of sentences, type-supervised models also
have access to a tag dictionary D ⊆ W × T that
contains all allowed word-tag pairs. For an EM-
trained HMM, initially setting Pθ(w|t) = 0 for all
(w, t) ∈/ D ensures that all words will be labeled
with allowed tags.
Tag dictionaries can be derived from various
sources, such as lexicographic resources (Li et
al., 2012) and cross-lingual projections (Das and
Petrov, 2011). In this paper, we will follow pre-
vious work in deriving the tag dictionary from
a labeled corpus (Smith and Eisner, 2005); this
synthetic setting maximizes experiment repeata-
bility and allows for direct comparison of type-
supervised learning techniques.
Transductive Applications. We consider a
transductive data setting in which the test set is
available during training. In this case, the model
is not required to generalize to unseen examples or
unknown words, as in the typical inductive setting.
Transductive learning arises in document clus-
tering and corpus analysis applications. For ex-
ample, before running a document clustering al-
gorithm on a fixed corpus of documents, it may be
useful to tag each word with its most likely part-
of-speech in context, disambiguating the lexical
features in a bag-of-words representation. In cor-
pus analysis or genre detection, it may be useful
to determine for a fixed corpus the most common
part-of-speech for each word type, which could be
inferred by tagging each word with its most likely
part-of-speech. In both cases, the set of sentences
to tag is known in advance of learning.
</bodyText>
<sectionHeader confidence="0.991095" genericHeader="method">
3 Initializing HMM Taggers
</sectionHeader>
<bodyText confidence="0.999974304347826">
The EM algorithm is sensitive to initialization. In
a latent variable model, different parameter values
may yield similar data likelihoods but very differ-
ent predictions. We explore this issue via exper-
iments on the Wall Street Journal section of the
English Penn Treebank (Marcus et al., 1993). We
adopt the transductive data setting introduced by
Smith and Eisner (2005) and used by Goldwa-
ter and Griffiths (2007), Toutanova and Johnson
(2008) and Ravi and Knight (2009); models are
trained on all sections 00-24, the tag dictionary D
is constructed by allowing all word-tag pairs ap-
pearing in the entire labeled corpus, and the tag-
ging accuracy is evaluated on a 1005 sentence sub-
set sampled from the corpus.
The degree of variation in tagging accuracy due
to initialization can be observed most clearly by
two contrasting initializations. UNIFORM initial-
izes the model with uniform distributions over al-
lowed outcomes:
SUPERVISED is an oracle setting that initializes
the model with the relative frequency of observed
pairs in a labeled corpus:
</bodyText>
<equation confidence="0.9999085">
Pφ(t|t0) ∝ X X |w |δ((t∗i, t∗i−1), (t, t0))
(w,t*) i=1
Pθ(w|t) ∝ X X|w |δ((wi, t∗i), (w, t))
(w,t*) i=1
</equation>
<bodyText confidence="0.9987706">
where the Kronecker δ(x, y) function is 1 if x and
y are equal and 0 otherwise.
Figure 1 shows that while UNIFORM and
SUPERVISED achieve nearly identical data log-
likelihoods, their final tagging accuracy differs by
</bodyText>
<equation confidence="0.9995203">
X
`(φ, θ; S) =
w∈S
Pφ,θ(w, t)
|T|
Pθ(w|t)
= 1
|{w : (w, t) ∈ D}|
1
Pφ(t|t0) =
</equation>
<page confidence="0.9879">
817
</page>
<figure confidence="0.9857455">
0 5 10 15 20 25 30
Number of Iterations of Expectation Maximization
</figure>
<figureCaption confidence="0.9854545">
Figure 1: The data log-likelihood (top) and tag-
ging accuracy (bottom) of two contrasting initial-
izers, UNIFORM and SUPERVISED, compared on
the Penn Treebank.
</figureCaption>
<figure confidence="0.988999">
0 5 10 15 20 25 30
Number of Iterations of Expectation Maximization
</figure>
<figureCaption confidence="0.998885">
Figure 2: The data log-likelihood (top) and tag-
</figureCaption>
<bodyText confidence="0.899421285714286">
ging accuracy (bottom) of two partially supervised
initializers, one with SUPERVISED TRANSITIONS
and one with SUPERVISED EMISSIONS, compared
on the Penn Treebank.
12%. Accuracy degrades somewhat from the SU-
PERVISED initialization, since the data likelihood
objective differs from the objective of maximizing
tagging accuracy. However, the final SUPERVISED
performance of 94.1% shows that there is substan-
tial room for improvement over the UNIFORM ini-
tializer.
Figure 2 compares two partially supervised ini-
tializations. SUPERVISED TRANSITIONS initial-
izes the transition model with oracle counts, but
the emission model uniformly. Conversely, SU-
PERVISED EMISSIONS initializes the emission pa-
rameters from oracle counts, but initializes the
transition model uniformly. There are many more
emission parameters (57,390) than transition pa-
rameters (1,858). Thus, it is not surprising that
SUPERVISED EMISSIONS gives a higher initial
</bodyText>
<figure confidence="0.96804525">
Data LogLikelihood (106)
7
likelihood. Again, both initializers lead to solu-
-9 SUPERVISED TRANSTIONS
tions with nearly the same likelihood as SUPER-
-11 UPERVISED EMISSION
VISED and UNIFORM.
-13
</figure>
<figureCaption confidence="0.805955">
Figure 2 shows that SUPERVISED TRANSI-
</figureCaption>
<figure confidence="0.90097425">
Taggng Accuracy (%)
100
TIONS outperforms SUPERVISED EMISSION
935%
tagging accuracy, despite the fact that fewer
90 2.8%
rameters are set with supervision. With fixed D,
80
an accurate initialization of the transition distribu-
tions leads to accurate tagging after EM training
70
0 5 10 15 20 25
</figure>
<bodyText confidence="0.693801333333333">
We therefore concentrate on developing an effec-
Nube of Itrtion of Expecttio Maxiztion
tive initialization for the transition distribution.
</bodyText>
<sectionHeader confidence="0.99834" genericHeader="method">
4 Observational Initialization
</sectionHeader>
<bodyText confidence="0.9986155">
The SUPERVISED TRANSITIONS initialization is
estimated from observations of consecutive tags in
a labeled corpus. Our OBSERVATIONAL initializer
is likewise estimated from the relative frequency
</bodyText>
<figure confidence="0.9206126">
Data LogLikelihood (106)
-7
of consecutive tags, taking advantage of the struc-
-9 SUPERVISED TRANSITIONS
-11
ture of the tag dictionary D. However, it does not
OBSERVATIONAL
require a labeled corpus.
13
Let D(w, ·) = {t : (w, t) ∈ D} denote the
aggn Accurac (%
allowed tags for word w. The set
93.5%
U = {w : |D(w, ·) |= 1}
contains all words that have only one allowed tag.
When a token of some w ∈ U is observed in a
70
0 5 0 15 20 25 30
corpus,its tag is unambiguous. Therefore, its tag
Number of Ierations of Expectaon Maximizaton
</figure>
<bodyText confidence="0.992184">
is observed as well, and a portion of the tag se-
quence is known. When consecutive pairs of to-
kens are both in U, we can observe a transition in
the latent tag sequence. The OBSERVATIONAL ini-
tializer simply estimates a transition distribution
from the relative frequency of these unambiguous
observations that occur whenever two consecutive
tokens both have a unique tag.
We now formally define the observational ini-
tializer. Let g(w, t) = δ(D(w, ·), {t}) be an indi-
cator function that is 1 whenever w ∈ U and its
single allowed tag is t, and 0 otherwise. Then, we
initialize φ such that:
g(wi, t) · g(wi−1, t,)
The emission parameters θ are set to be uniform
over allowed words for each tag, as in UNIFORM
initialization.
Figure 3 compares the OBSERVATIONAL ini-
tializer to the SUPERVISED TRANSITIONS initial-
izer, and the top of Table 1 summarizes the perfor-
mance of all initializers discussed so far for the
</bodyText>
<figure confidence="0.989893214285714">
-7 Data Log-Likelihood (106)
-13
100
96.7% Tagging Accuracy (%) 94.1%
90
80
70
82.1%
72.0%
SUPERVISED
UNIFORM
-9
-11
-9
-11
Data Log-Likelihood (106)
-7
Tagging Accuracy (%)
93.5%
92.8%
93.7%
91.0%
80
70
-13
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
100
90
937%
S in
pa-
10%
.
�
Pφ(t|t,) ∝
WES
� |W|
i=1
818
0 5 10 15 20 25 30
Number of Iterations of Expectation Maximization
</figure>
<figureCaption confidence="0.923665">
Figure 3: The data log-likelihood (top) and tag-
ging accuracy (bottom) of initializing with SU-
PERVISED TRANSITIONS compared to the unsu-
pervised OBSERVATIONAL initialization that re-
quires only a tag dictionary and an unlabeled train-
ing corpus.
</figureCaption>
<bodyText confidence="0.998734714285714">
English Penn Treebank. The OBSERVATIONAL
initializer provides an error reduction over UNI-
FORM of 56%, surpassing the performance of an
initially supervised emission model and nearing
the performance of a supervised transition model.
The bottom of Table 1 shows a similar compar-
ison on the T¨ubingen treebank of spoken German
(Telljohann et al., 2006). Both training and test-
ing were performed on the entire treebank. The
observational initializer provides an error reduc-
tion over UNIFORM of 29%, and again outper-
forms SUPERVISED EMISSIONS. On this dataset
OBSERVATIONAL initialization matches the final
performance of SUPERVISED TRANSITIONS.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9985774">
The fact that observations and prior knowledge are
useful for part-of-speech tagging is well under-
stood (Brill, 1995), but the approach of estimating
an initial transition model only from unambiguous
word pairs is novel.
Our experiments show that for EM-trained
HMM taggers in a type-supervised transductive
data setting, observational initialization is an ef-
fective technique for guiding training toward high-
accuracy solutions, approaching the oracle accu-
racy of SUPERVISED TRANSITIONS initialization.
The fact that models with similar data likeli-
hood can vary dramatically in accuracy has been
observed in other learning problems. For instance,
Toutanova and Galley (2011) show that optimal
</bodyText>
<table confidence="0.999660333333333">
English Initial EM-trained
UNIFORM 72.0 82.1
OBSERVATIONAL 89.2 92.1
SUP. EMISSIONS 92.8 91.0
SUP. TRANSITIONS 93.5 93.7
FULLY SUPERVISED 96.7 94.1
German Initial EM-trained
UNIFORM 77.2 88.8
OBSERVATIONAL 92.7 92.1
SUP. EMISSIONS 90.7 89.0
SUP. TRANSITIONS 94.8 92.0
FULLY SUPERVISED 97.0 92.9
</table>
<tableCaption confidence="0.999139">
Table 1: Accuracy of English (top) and German
</tableCaption>
<bodyText confidence="0.97877495">
(bottom) tagging models at initialization (left) and
after 30 iterations of EM training (right) using var-
ious initializers.
parameters for IBM Model 1 are not unique, and
alignments predicted from different optimal pa-
rameters vary significantly in accuracy.
However, the effectiveness of observational ini-
tialization is somewhat surprising because EM
training includes these unambiguous tag pairs in
its expected counts, even with uniform initializa-
tion. Our experiments indicate that this signal is
not used effectively unless explicitly encoded in
the initialization.
In our English data, 48% of tokens and 74% of
word types have only one allowed tag. 28% of
pairs of adjacent tokens have only one allowed tag
pair and contribute to observational initialization.
In German, 49% of tokens and 87% of word types
are unambiguous, and 26% of adjacent token pairs
are unambiguous.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999956785714286">
We now compare with several previous published
results on type-supervised part-of-speech tagging
trained using the same data setting on the English
WSJ Penn Treebank, introduced by Smith and Eis-
ner (2005).
Contrastive estimation (Smith and Eisner, 2005)
is a learning technique that approximates the par-
tition function of the EM objective in a log-linear
model by considering a neighborhood around ob-
served training examples. The Bayesian HMM
of Goldwater and Griffiths (2007) is a second-
order HMM (i.e., likelihood factors over triples
of tags) that is estimated using a prior distribu-
tion that promotes sparsity. Sparse priors have
</bodyText>
<figure confidence="0.998398">
Data Log-Likelihood (106)
SUPERVISED TRANSITIONS
OBSERVATIONAL
-13
100
Tagging Accuracy (%)
-7
-9
-11
80
70
93.7%
92.1%
93.5%
90
89.2%
</figure>
<page confidence="0.988717">
819
</page>
<table confidence="0.835717875">
45 tag set 17 tag set
All train 973k train All train 973k train
Observational initialization (this work) 92.1 92.8 93.9 94.8
Contrastive Estimation (Smith and Eisner, 2005) – – 88.7 –
Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 – 87.3 –
Bayesian LDA-HMM (Toutanova and Johnson, 2008) – – 93.4 –
Linguistic initialization (Goldberg et al., 2008) 91.4 – 93.8 –
Minimal models (Ravi and Knight, 2009) – 92.3 – 96.8
</table>
<tableCaption confidence="0.932184">
Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k
train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009).
</tableCaption>
<bodyText confidence="0.999935441860465">
been motivated empirically for this task (Johnson,
2007). The Bayesian HMM model predicts tag se-
quences via Gibbs sampling, integrating out model
parameters. The Bayesian LDA-based model of
Toutanova and Johnson (2008) models ambiguity
classes of words, which allows information shar-
ing among words in the tag dictionary. In addition,
it incorporates morphology features and a sparse
prior of tags for a word. Inference approximations
are required to predict tags, integrating out model
parameters.
Ravi and Knight (2009) employs integer linear
programming to select a minimal set of parame-
ters that can generate the test sentences, followed
by EM to set parameter values. This technique
requires the additional information of which sen-
tences will be used for evaluation, and its scalabil-
ity is limited. In addition, this work used a sub-
set of the WSJ Penn Treebank for training and se-
lecting a tag dictionary. This restriction actually
tends to improve performance, because a smaller
tag dictionary further constrains model optimiza-
tion. We compare directly to their training set,
kindly provided to us by the authors.
The linguistic initialization of Goldberg et al.
(2008) is most similar to the current work, in
that it estimates maximum likelihood parameters
of an HMM using EM, but starting with a well-
chosen initialization with language specific lin-
guistic knowledge. That work estimates emission
distributions using a combination of suffix mor-
phology rules and corpus context counts.
Table 2 compares our results to these related
techniques. Each column represents a variant of
the experimental setting used in prior work. Smith
and Eisner (2005) introduced a mapping from the
full 45 tag set of the Penn Treebank to 17 coarse
tags. We report results on this coarse set by pro-
jecting from the full set after learning and infer-
ence.1 Using the full tag set or the full training
data, our method offers the best published perfor-
mance without language-specific assumptions or
approximate inference.
</bodyText>
<sectionHeader confidence="0.99983" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999983916666667">
This paper has demonstrated a simple and effec-
tive learning method for type-supervised, trans-
ductive part-of-speech tagging. However, it is an
open question whether the technique is as effec-
tive for tag dictionaries derived from more natural
sources than the labels of an existing treebank.
All of the methods to which we compare ex-
cept Goldberg et al. (2008) focus on learning and
modeling techniques, while our method only ad-
dresses initialization. We look forward to inves-
tigating whether our technique can be used as an
initialization or prior for these other methods.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.924291789473684">
Eric Brill. 1995. Unsupervised learning of disam-
biguation rules for part of speech tagging. In In Nat-
ural Language Processing Using Very Large Cor-
pora, pages 1–13. Kluwer Academic Press.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the Assocation for
Computational Linguistics.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the North American Chapter of
the Assocation for Computational Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
1Training with the reduced tag set led to lower perfor-
mance of 91.0% accuracy, likely because the coarse projec-
tion drops critical information about allowable English tran-
sitions, such as what verb forms can follow to be (Goldberg
et al., 2008).
</reference>
<page confidence="0.978372">
820
</page>
<reference confidence="0.999069957446809">
(when given a good start). In Proceedings of the As-
sociation for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the Association for Com-
putational Linguistics.
Mark Johnson. 2007. Why doesnt EM nd good HMM
POS-taggers? In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics.
Heike Telljohann, Erhard Hinrichs, Sandra K¨ubler, and
Heike Zinsmeister. 2006. Stylebook for the tbingen
treebank of written german.
Kristina Toutanova and Michel Galley. 2011. Why
initialization matters for ibm model 1: Multiple op-
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 461–466, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of Neural
and Information Processing Systems.
</reference>
<page confidence="0.998389">
821
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539730">
<title confidence="0.998964">Observational Initialization of Type-Supervised Taggers</title>
<author confidence="0.924213">DeNero</author>
<affiliation confidence="0.8053165">Department of Computer Science Google, Inc. of Southern California</affiliation>
<email confidence="0.999705">hzhang@isi.edu</email>
<abstract confidence="0.997521222222222">Recent work has sparked new interest in type-supervised part-of-speech tagging, a data setting in which no labeled sentences are available, but the set of allowed tags is known for each word type. This describes initializaa novel technique for initializing EM when training a type-supervised HMM tagger. Our initializer allocates probability mass to unambiguous transitions in an unlabeled corpus, generating token-level observations from type-level supervision. Experimentally, observational initialization gives state-of-the-art type-supervised tagging accuracy, providing an error reduction of 56% over uniform initialization on the Penn English Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Unsupervised learning of disambiguation rules for part of speech tagging.</title>
<date>1995</date>
<booktitle>In In Natural Language Processing Using Very Large Corpora,</booktitle>
<pages>1--13</pages>
<publisher>Kluwer Academic Press.</publisher>
<contexts>
<context position="12831" citStr="Brill, 1995" startWordPosition="2040" endWordPosition="2041">sion model and nearing the performance of a supervised transition model. The bottom of Table 1 shows a similar comparison on the T¨ubingen treebank of spoken German (Telljohann et al., 2006). Both training and testing were performed on the entire treebank. The observational initializer provides an error reduction over UNIFORM of 29%, and again outperforms SUPERVISED EMISSIONS. On this dataset OBSERVATIONAL initialization matches the final performance of SUPERVISED TRANSITIONS. 5 Discussion The fact that observations and prior knowledge are useful for part-of-speech tagging is well understood (Brill, 1995), but the approach of estimating an initial transition model only from unambiguous word pairs is novel. Our experiments show that for EM-trained HMM taggers in a type-supervised transductive data setting, observational initialization is an effective technique for guiding training toward highaccuracy solutions, approaching the oracle accuracy of SUPERVISED TRANSITIONS initialization. The fact that models with similar data likelihood can vary dramatically in accuracy has been observed in other learning problems. For instance, Toutanova and Galley (2011) show that optimal English Initial EM-train</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Unsupervised learning of disambiguation rules for part of speech tagging. In In Natural Language Processing Using Very Large Corpora, pages 1–13. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the Assocation for Computational Linguistics.</booktitle>
<contexts>
<context position="1570" citStr="Das and Petrov (2011)" startWordPosition="218" endWordPosition="221">ionaries that list the possible parts-of-speech for each word type, but there are no corpora labeled with the part-of-speech of each token in context. Type-supervised tagging (Merialdo, 1994) explores this scenario; a model is provided with type-level information, such as the fact that “only” can be an adjective, adverb, or conjunction, but not any token-level information about which instances of “only” in a corpus are adjectives. Recent research has focused on using type-level supervision to infer token-level tags. For instance, Li et al. (2012) derive type-level supervision from Wiktionary, Das and Petrov (2011) and T¨ackstr¨om et al. (2013) project type-level tag sets across languages, and Garrette and Baldridge (2013) solicit type-level annotations directly from speakers. In all of these efforts, a probabilistic sequence model is trained to disambiguate token-level tags that are ∗Research conducted during an internship at Google. constrained to match type-level tag restrictions. This paper describes observational initialization, a simple but effective learning technique for training type-supervised taggers. A hidden Markov model (HMM) can be used to disambiguate tags of individual tokens by maximiz</context>
<context position="5236" citStr="Das and Petrov, 2011" startWordPosition="798" endWordPosition="801">ags for each sentence under the model: ˆt(w) = arg max t Tagging accuracy is the fraction of these tags in ˆt(w) that match hand-labeled oracle tags t∗(w). Type Supervision. In addition to an unlabeled corpus of sentences, type-supervised models also have access to a tag dictionary D ⊆ W × T that contains all allowed word-tag pairs. For an EMtrained HMM, initially setting Pθ(w|t) = 0 for all (w, t) ∈/ D ensures that all words will be labeled with allowed tags. Tag dictionaries can be derived from various sources, such as lexicographic resources (Li et al., 2012) and cross-lingual projections (Das and Petrov, 2011). In this paper, we will follow previous work in deriving the tag dictionary from a labeled corpus (Smith and Eisner, 2005); this synthetic setting maximizes experiment repeatability and allows for direct comparison of typesupervised learning techniques. Transductive Applications. We consider a transductive data setting in which the test set is available during training. In this case, the model is not required to generalize to unseen examples or unknown words, as in the typical inductive setting. Transductive learning arises in document clustering and corpus analysis applications. For example,</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the Assocation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Assocation for Computational Linguistics.</booktitle>
<contexts>
<context position="1680" citStr="Garrette and Baldridge (2013)" startWordPosition="235" endWordPosition="238">ith the part-of-speech of each token in context. Type-supervised tagging (Merialdo, 1994) explores this scenario; a model is provided with type-level information, such as the fact that “only” can be an adjective, adverb, or conjunction, but not any token-level information about which instances of “only” in a corpus are adjectives. Recent research has focused on using type-level supervision to infer token-level tags. For instance, Li et al. (2012) derive type-level supervision from Wiktionary, Das and Petrov (2011) and T¨ackstr¨om et al. (2013) project type-level tag sets across languages, and Garrette and Baldridge (2013) solicit type-level annotations directly from speakers. In all of these efforts, a probabilistic sequence model is trained to disambiguate token-level tags that are ∗Research conducted during an internship at Google. constrained to match type-level tag restrictions. This paper describes observational initialization, a simple but effective learning technique for training type-supervised taggers. A hidden Markov model (HMM) can be used to disambiguate tags of individual tokens by maximizing corpus likelihood using the expectation maximization (EM) algorithm. Our approach is motivated by a suite </context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proceedings of the North American Chapter of the Assocation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers 1Training with the reduced tag set led to lower performance of 91.0% accuracy, likely because the coarse projection drops critical information about allowable English transitions, such as what verb forms can follow to be (Goldberg et al.,</title>
<date>2008</date>
<contexts>
<context position="3497" citStr="Goldberg et al. (2008)" startWordPosition="496" endWordPosition="499">wed tag. These unambiguous word pairs can be tagged correctly without any statistical inference. Initializing EM with the relative frequency of these unambiguous pairs improves tagging accuracy dramatically over uniform initialization, reducing errors by 56% in English and 29% in German. This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al. (2008). 2 Type-Supervised Tagging A first-order Markov model for part-of-speech tagging defines a distribution over sentences for which a single tag is given to each word token. Let wi ∈ W refer to the ith word in a sentence w, drawn from language vocabulary W. Likewise, 816 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816–821, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ti ∈ T is the tag in tag sequence t of the ith word, drawn from tag inventory T. The joint probability of a sentence ca</context>
<context position="15764" citStr="Goldberg et al., 2008" startWordPosition="2487" endWordPosition="2490">e., likelihood factors over triples of tags) that is estimated using a prior distribution that promotes sparsity. Sparse priors have Data Log-Likelihood (106) SUPERVISED TRANSITIONS OBSERVATIONAL -13 100 Tagging Accuracy (%) -7 -9 -11 80 70 93.7% 92.1% 93.5% 90 89.2% 819 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 Contrastive Estimation (Smith and Eisner, 2005) – – 88.7 – Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 – 87.3 – Bayesian LDA-HMM (Toutanova and Johnson, 2008) – – 93.4 – Linguistic initialization (Goldberg et al., 2008) 91.4 – 93.8 – Minimal models (Ravi and Knight, 2009) – 92.3 – 96.8 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). been motivated empirically for this task (Johnson, 2007). The Bayesian HMM model predicts tag sequences via Gibbs sampling, integrating out model parameters. The Bayesian LDA-based model of Toutanova and Johnson (2008) models ambiguity classes of words, which allows information sharing among words in the tag dictionary. In addition, it incorp</context>
<context position="17195" citStr="Goldberg et al. (2008)" startWordPosition="2715" endWordPosition="2718">ing to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. This technique requires the additional information of which sentences will be used for evaluation, and its scalability is limited. In addition, this work used a subset of the WSJ Penn Treebank for training and selecting a tag dictionary. This restriction actually tends to improve performance, because a smaller tag dictionary further constrains model optimization. We compare directly to their training set, kindly provided to us by the authors. The linguistic initialization of Goldberg et al. (2008) is most similar to the current work, in that it estimates maximum likelihood parameters of an HMM using EM, but starting with a wellchosen initialization with language specific linguistic knowledge. That work estimates emission distributions using a combination of suffix morphology rules and corpus context counts. Table 2 compares our results to these related techniques. Each column represents a variant of the experimental setting used in prior work. Smith and Eisner (2005) introduced a mapping from the full 45 tag set of the Penn Treebank to 17 coarse tags. We report results on this coarse s</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers 1Training with the reduced tag set led to lower performance of 91.0% accuracy, likely because the coarse projection drops critical information about allowable English transitions, such as what verb forms can follow to be (Goldberg et al., 2008).</rawString>
</citation>
<citation valid="false">
<title>(when given a good start).</title>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker></marker>
<rawString>(when given a good start). In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6806" citStr="Goldwater and Griffiths (2007)" startWordPosition="1050" endWordPosition="1054">of-speech for each word type, which could be inferred by tagging each word with its most likely part-of-speech. In both cases, the set of sentences to tag is known in advance of learning. 3 Initializing HMM Taggers The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. UNIFORM initializes the model with uniform distributions over allowed outcomes: SUPERVISED is an oracle setting that initializes the model with the relative frequency of ob</context>
<context position="15117" citStr="Goldwater and Griffiths (2007)" startWordPosition="2381" endWordPosition="2384">rvational initialization. In German, 49% of tokens and 87% of word types are unambiguous, and 26% of adjacent token pairs are unambiguous. 6 Related Work We now compare with several previous published results on type-supervised part-of-speech tagging trained using the same data setting on the English WSJ Penn Treebank, introduced by Smith and Eisner (2005). Contrastive estimation (Smith and Eisner, 2005) is a learning technique that approximates the partition function of the EM objective in a log-linear model by considering a neighborhood around observed training examples. The Bayesian HMM of Goldwater and Griffiths (2007) is a secondorder HMM (i.e., likelihood factors over triples of tags) that is estimated using a prior distribution that promotes sparsity. Sparse priors have Data Log-Likelihood (106) SUPERVISED TRANSITIONS OBSERVATIONAL -13 100 Tagging Accuracy (%) -7 -9 -11 80 70 93.7% 92.1% 93.5% 90 89.2% 819 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 Contrastive Estimation (Smith and Eisner, 2005) – – 88.7 – Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 – 87.3 – Bayesian LDA-HMM (Toutanova and Johnson, 2008) – – 93.4 – Li</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesnt EM nd good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16076" citStr="Johnson, 2007" startWordPosition="2541" endWordPosition="2542">rain 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 Contrastive Estimation (Smith and Eisner, 2005) – – 88.7 – Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 – 87.3 – Bayesian LDA-HMM (Toutanova and Johnson, 2008) – – 93.4 – Linguistic initialization (Goldberg et al., 2008) 91.4 – 93.8 – Minimal models (Ravi and Knight, 2009) – 92.3 – 96.8 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). been motivated empirically for this task (Johnson, 2007). The Bayesian HMM model predicts tag sequences via Gibbs sampling, integrating out model parameters. The Bayesian LDA-based model of Toutanova and Johnson (2008) models ambiguity classes of words, which allows information sharing among words in the tag dictionary. In addition, it incorporates morphology features and a sparse prior of tags for a word. Inference approximations are required to predict tags, integrating out model parameters. Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set p</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesnt EM nd good HMM POS-taggers? In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="6685" citStr="Marcus et al., 1993" startWordPosition="1031" endWordPosition="1034">. In corpus analysis or genre detection, it may be useful to determine for a fixed corpus the most common part-of-speech for each word type, which could be inferred by tagging each word with its most likely part-of-speech. In both cases, the set of sentences to tag is known in advance of learning. 3 Initializing HMM Taggers The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. UNIFORM initializes the model with uniform distribu</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model. Computational Linguistics.</title>
<date>1994</date>
<contexts>
<context position="1140" citStr="Merialdo, 1994" startWordPosition="152" endWordPosition="153">er. Our initializer allocates probability mass to unambiguous transitions in an unlabeled corpus, generating token-level observations from type-level supervision. Experimentally, observational initialization gives state-of-the-art type-supervised tagging accuracy, providing an error reduction of 56% over uniform initialization on the Penn English Treebank. 1 Introduction For many languages, there exist comprehensive dictionaries that list the possible parts-of-speech for each word type, but there are no corpora labeled with the part-of-speech of each token in context. Type-supervised tagging (Merialdo, 1994) explores this scenario; a model is provided with type-level information, such as the fact that “only” can be an adjective, adverb, or conjunction, but not any token-level information about which instances of “only” in a corpus are adjectives. Recent research has focused on using type-level supervision to infer token-level tags. For instance, Li et al. (2012) derive type-level supervision from Wiktionary, Das and Petrov (2011) and T¨ackstr¨om et al. (2013) project type-level tag sets across languages, and Garrette and Baldridge (2013) solicit type-level annotations directly from speakers. In a</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3341" citStr="Ravi and Knight (2009)" startWordPosition="474" endWordPosition="477">upervision. Transition probabilities are estimated from unambiguous consecutive tag pairs that arise when two consecutive words each have only a single allowed tag. These unambiguous word pairs can be tagged correctly without any statistical inference. Initializing EM with the relative frequency of these unambiguous pairs improves tagging accuracy dramatically over uniform initialization, reducing errors by 56% in English and 29% in German. This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al. (2008). 2 Type-Supervised Tagging A first-order Markov model for part-of-speech tagging defines a distribution over sentences for which a single tag is given to each word token. Let wi ∈ W refer to the ith word in a sentence w, drawn from language vocabulary W. Likewise, 816 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816–821, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Assoc</context>
<context position="6863" citStr="Ravi and Knight (2009)" startWordPosition="1060" endWordPosition="1063">each word with its most likely part-of-speech. In both cases, the set of sentences to tag is known in advance of learning. 3 Initializing HMM Taggers The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. UNIFORM initializes the model with uniform distributions over allowed outcomes: SUPERVISED is an oracle setting that initializes the model with the relative frequency of observed pairs in a labeled corpus: Pφ(t|t0) ∝ X X |w |δ((t</context>
<context position="15817" citStr="Ravi and Knight, 2009" startWordPosition="2497" endWordPosition="2500">estimated using a prior distribution that promotes sparsity. Sparse priors have Data Log-Likelihood (106) SUPERVISED TRANSITIONS OBSERVATIONAL -13 100 Tagging Accuracy (%) -7 -9 -11 80 70 93.7% 92.1% 93.5% 90 89.2% 819 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 Contrastive Estimation (Smith and Eisner, 2005) – – 88.7 – Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 – 87.3 – Bayesian LDA-HMM (Toutanova and Johnson, 2008) – – 93.4 – Linguistic initialization (Goldberg et al., 2008) 91.4 – 93.8 – Minimal models (Ravi and Knight, 2009) – 92.3 – 96.8 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). been motivated empirically for this task (Johnson, 2007). The Bayesian HMM model predicts tag sequences via Gibbs sampling, integrating out model parameters. The Bayesian LDA-based model of Toutanova and Johnson (2008) models ambiguity classes of words, which allows information sharing among words in the tag dictionary. In addition, it incorporates morphology features and a sparse prior of tags</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5359" citStr="Smith and Eisner, 2005" startWordPosition="820" endWordPosition="823">ch hand-labeled oracle tags t∗(w). Type Supervision. In addition to an unlabeled corpus of sentences, type-supervised models also have access to a tag dictionary D ⊆ W × T that contains all allowed word-tag pairs. For an EMtrained HMM, initially setting Pθ(w|t) = 0 for all (w, t) ∈/ D ensures that all words will be labeled with allowed tags. Tag dictionaries can be derived from various sources, such as lexicographic resources (Li et al., 2012) and cross-lingual projections (Das and Petrov, 2011). In this paper, we will follow previous work in deriving the tag dictionary from a labeled corpus (Smith and Eisner, 2005); this synthetic setting maximizes experiment repeatability and allows for direct comparison of typesupervised learning techniques. Transductive Applications. We consider a transductive data setting in which the test set is available during training. In this case, the model is not required to generalize to unseen examples or unknown words, as in the typical inductive setting. Transductive learning arises in document clustering and corpus analysis applications. For example, before running a document clustering algorithm on a fixed corpus of documents, it may be useful to tag each word with its </context>
<context position="6763" citStr="Smith and Eisner (2005)" startWordPosition="1043" endWordPosition="1046">a fixed corpus the most common part-of-speech for each word type, which could be inferred by tagging each word with its most likely part-of-speech. In both cases, the set of sentences to tag is known in advance of learning. 3 Initializing HMM Taggers The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. UNIFORM initializes the model with uniform distributions over allowed outcomes: SUPERVISED is an oracle setting that initializes </context>
<context position="14845" citStr="Smith and Eisner (2005)" startWordPosition="2339" endWordPosition="2343">icate that this signal is not used effectively unless explicitly encoded in the initialization. In our English data, 48% of tokens and 74% of word types have only one allowed tag. 28% of pairs of adjacent tokens have only one allowed tag pair and contribute to observational initialization. In German, 49% of tokens and 87% of word types are unambiguous, and 26% of adjacent token pairs are unambiguous. 6 Related Work We now compare with several previous published results on type-supervised part-of-speech tagging trained using the same data setting on the English WSJ Penn Treebank, introduced by Smith and Eisner (2005). Contrastive estimation (Smith and Eisner, 2005) is a learning technique that approximates the partition function of the EM objective in a log-linear model by considering a neighborhood around observed training examples. The Bayesian HMM of Goldwater and Griffiths (2007) is a secondorder HMM (i.e., likelihood factors over triples of tags) that is estimated using a prior distribution that promotes sparsity. Sparse priors have Data Log-Likelihood (106) SUPERVISED TRANSITIONS OBSERVATIONAL -13 100 Tagging Accuracy (%) -7 -9 -11 80 70 93.7% 92.1% 93.5% 90 89.2% 819 45 tag set 17 tag set All train</context>
<context position="17674" citStr="Smith and Eisner (2005)" startWordPosition="2790" endWordPosition="2793">zation. We compare directly to their training set, kindly provided to us by the authors. The linguistic initialization of Goldberg et al. (2008) is most similar to the current work, in that it estimates maximum likelihood parameters of an HMM using EM, but starting with a wellchosen initialization with language specific linguistic knowledge. That work estimates emission distributions using a combination of suffix morphology rules and corpus context counts. Table 2 compares our results to these related techniques. Each column represents a variant of the experimental setting used in prior work. Smith and Eisner (2005) introduced a mapping from the full 45 tag set of the Penn Treebank to 17 coarse tags. We report results on this coarse set by projecting from the full set after learning and inference.1 Using the full tag set or the full training data, our method offers the best published performance without language-specific assumptions or approximate inference. 7 Future Work This paper has demonstrated a simple and effective learning method for type-supervised, transductive part-of-speech tagging. However, it is an open question whether the technique is as effective for tag dictionaries derived from more na</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Heike Zinsmeister</author>
</authors>
<title>Stylebook for the tbingen treebank of written german.</title>
<date>2006</date>
<marker>Telljohann, Hinrichs, K¨ubler, Zinsmeister, 2006</marker>
<rawString>Heike Telljohann, Erhard Hinrichs, Sandra K¨ubler, and Heike Zinsmeister. 2006. Stylebook for the tbingen treebank of written german.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Michel Galley</author>
</authors>
<title>Why initialization matters for ibm model 1: Multiple optima and non-strict convexity.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>461--466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="13388" citStr="Toutanova and Galley (2011)" startWordPosition="2118" endWordPosition="2121">are useful for part-of-speech tagging is well understood (Brill, 1995), but the approach of estimating an initial transition model only from unambiguous word pairs is novel. Our experiments show that for EM-trained HMM taggers in a type-supervised transductive data setting, observational initialization is an effective technique for guiding training toward highaccuracy solutions, approaching the oracle accuracy of SUPERVISED TRANSITIONS initialization. The fact that models with similar data likelihood can vary dramatically in accuracy has been observed in other learning problems. For instance, Toutanova and Galley (2011) show that optimal English Initial EM-trained UNIFORM 72.0 82.1 OBSERVATIONAL 89.2 92.1 SUP. EMISSIONS 92.8 91.0 SUP. TRANSITIONS 93.5 93.7 FULLY SUPERVISED 96.7 94.1 German Initial EM-trained UNIFORM 77.2 88.8 OBSERVATIONAL 92.7 92.1 SUP. EMISSIONS 90.7 89.0 SUP. TRANSITIONS 94.8 92.0 FULLY SUPERVISED 97.0 92.9 Table 1: Accuracy of English (top) and German (bottom) tagging models at initialization (left) and after 30 iterations of EM training (right) using various initializers. parameters for IBM Model 1 are not unique, and alignments predicted from different optimal parameters vary significa</context>
</contexts>
<marker>Toutanova, Galley, 2011</marker>
<rawString>Kristina Toutanova and Michel Galley. 2011. Why initialization matters for ibm model 1: Multiple optima and non-strict convexity. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 461–466, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of Neural and Information Processing Systems.</booktitle>
<contexts>
<context position="3403" citStr="Toutanova and Johnson (2008)" startWordPosition="483" endWordPosition="486">unambiguous consecutive tag pairs that arise when two consecutive words each have only a single allowed tag. These unambiguous word pairs can be tagged correctly without any statistical inference. Initializing EM with the relative frequency of these unambiguous pairs improves tagging accuracy dramatically over uniform initialization, reducing errors by 56% in English and 29% in German. This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al. (2008). 2 Type-Supervised Tagging A first-order Markov model for part-of-speech tagging defines a distribution over sentences for which a single tag is given to each word token. Let wi ∈ W refer to the ith word in a sentence w, drawn from language vocabulary W. Likewise, 816 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816–821, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ti ∈ T is the tag in tag </context>
<context position="6836" citStr="Toutanova and Johnson (2008)" startWordPosition="1055" endWordPosition="1058">ich could be inferred by tagging each word with its most likely part-of-speech. In both cases, the set of sentences to tag is known in advance of learning. 3 Initializing HMM Taggers The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. UNIFORM initializes the model with uniform distributions over allowed outcomes: SUPERVISED is an oracle setting that initializes the model with the relative frequency of observed pairs in a labeled corp</context>
<context position="15703" citStr="Toutanova and Johnson, 2008" startWordPosition="2477" endWordPosition="2480">sian HMM of Goldwater and Griffiths (2007) is a secondorder HMM (i.e., likelihood factors over triples of tags) that is estimated using a prior distribution that promotes sparsity. Sparse priors have Data Log-Likelihood (106) SUPERVISED TRANSITIONS OBSERVATIONAL -13 100 Tagging Accuracy (%) -7 -9 -11 80 70 93.7% 92.1% 93.5% 90 89.2% 819 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 Contrastive Estimation (Smith and Eisner, 2005) – – 88.7 – Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 – 87.3 – Bayesian LDA-HMM (Toutanova and Johnson, 2008) – – 93.4 – Linguistic initialization (Goldberg et al., 2008) 91.4 – 93.8 – Minimal models (Ravi and Knight, 2009) – 92.3 – 96.8 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). been motivated empirically for this task (Johnson, 2007). The Bayesian HMM model predicts tag sequences via Gibbs sampling, integrating out model parameters. The Bayesian LDA-based model of Toutanova and Johnson (2008) models ambiguity classes of words, which allows information shar</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A Bayesian LDA-based model for semi-supervised part-of-speech tagging. In Proceedings of Neural and Information Processing Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>