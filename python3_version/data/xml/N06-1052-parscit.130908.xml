<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005686">
<title confidence="0.986693">
Language Model Information Retrieval with Document Expansion
</title>
<author confidence="0.999746">
Tao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang Zhai
</author>
<affiliation confidence="0.998491">
Department of Computer Science
University of Illinois at Urbana Champaign
</affiliation>
<sectionHeader confidence="0.987927" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999695555555556">
Language model information retrieval de-
pends on accurate estimation of document
models. In this paper, we propose a docu-
ment expansion technique to deal with the
problem of insufficient sampling of docu-
ments. We construct a probabilistic neigh-
borhood for each document, and expand
the document with its neighborhood infor-
mation. The expanded document provides
a more accurate estimation of the docu-
ment model, thus improves retrieval ac-
curacy. Moreover, since document expan-
sion and pseudo feedback exploit different
corpus structures, they can be combined to
further improve performance. The experi-
ment results on several different data sets
demonstrate the effectiveness of the pro-
posed document expansion method.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998992304347826">
Information retrieval with statistical language mod-
els (Lafferty and Zhai, 2003) has recently attracted
much more attention because of its solid theoreti-
cal background as well as its good empirical per-
formance. In this approach, queries and documents
are assumed to be sampled from hidden generative
models, and the similarity between a document and
a query is then calculated through the similarity be-
tween their underlying models.
Clearly, good retrieval performance relies on the
accurate estimation of the query and document mod-
els. Indeed, smoothing of document models has
been proved to be very critical (Chen and Good-
man, 1998; Kneser and Ney, 1995; Zhai and Laf-
ferty, 2001b). The need for smoothing originated
from the zero count problem: when a term does not
occur in a document, the maximum likelihood esti-
mator would give it a zero probability. This is un-
reasonable because the zero count is often due to in-
sufficient sampling, and a larger sample of the data
would likely contain the term. Smoothing is pro-
posed to address the problem.
While most smoothing methods utilize the global
collection information with a simple interpolation
(Ponte and Croft, 1998; Miller et al., 1999; Hiemstra
and Kraaij, 1998; Zhai and Lafferty, 2001b), sev-
eral recent studies (Liu and Croft, 2004; Kurland and
Lee, 2004) have shown that local corpus structures
can be exploited to improve retrieval performance.
In this paper, we further study the use of local cor-
pus structures for document model estimation and
propose to use document expansion to better exploit
local corpus structures for estimating document lan-
guage models.
According to statistical principles, the accuracy of
a statistical estimator is largely determined by the
sampling size of the observed data; a small data
set generally would result in large variances, thus
can not be trusted completely. Unfortunately, in re-
trieval, we often have to estimate a model based on a
single document. Since a document is a small sam-
ple, our estimate is unlikely to be very accurate.
A natural improvement is to enlarge the data sam-
ple, ideally in a document-specific way. Ideally, the
enlarged data sample should come from the same
original generative model. In reality, however, since
</bodyText>
<page confidence="0.963864">
407
</page>
<note confidence="0.995287">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407–414,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999907166666667">
the underlying model is unknown to us, we would
not really be able to obtain such extra data. The
essence of this paper is to use document expansion
to obtain high quality extra data to enlarge the sam-
ple of a document so as to improve the accuracy
of the estimated document language model. Docu-
ment expansion was previously explored in (Sing-
hal and Pereira, 1999) in the context of the vec-
tor space retrieval model, mainly involving selecting
more terms from similar documents. Our work dif-
fers from this previous work in that we study doc-
ument expansion in the language modeling frame-
work and implement the idea quite differently.
Our main idea is to augment a document prob-
abilistically with potentially all other documents in
the collection that are similar to the document. The
probability associated with each neighbor document
reflects how likely the neighbor document is from
the underlying distribution of the original document,
thus we have a “probabilistic neighborhood”, which
can serve as “extra data” for the document for es-
timating the underlying language model. From the
viewpoint of smoothing, our method extends the ex-
isting work on using clusters for smoothing (Liu and
Croft, 2004) to allow each document to have its own
cluster for smoothing.
We evaluated our method using six representative
retrieval test sets. The experiment results show that
document expansion smoothing consistently outper-
forms the baseline smoothing methods in all the data
sets. It also outperforms a state-of-the-art cluster-
ing smoothing method. Analysis shows that the
improvement tends to be more significant for short
documents, indicating that the improvement indeed
comes from the improved estimation of the docu-
ment language model, since a short document pre-
sumably would benefit more from the neighborhood
smoothing. Moreover, since document expansion
and pseudo feedback exploit different corpus struc-
tures, they can be combined to further improve per-
formance. As document expansion can be done in
the indexing stage, it is scalable to large collections.
</bodyText>
<sectionHeader confidence="0.979781" genericHeader="method">
2 Document Expansion Retrieval Model
</sectionHeader>
<subsectionHeader confidence="0.984835">
2.1 The KL-divergence retrieval model
</subsectionHeader>
<bodyText confidence="0.999982315789474">
We first briefly review the KL-divergence retrieval
model, on which we will develop the document
expansion technique. The KL-divergence model
is a representative state-of-the-art language model-
ing approach for retrieval. It covers the basic lan-
guage modeling approach (i.e., the query likelihood
method) as a special case and can support feedback
more naturally.
In this approach, a query and a document are as-
sumed to be generated from a unigram query lan-
guage model OQ and a unigram document language
model OD, respectively. Given a query and a docu-
ment, we would first compute an estimate of the cor-
responding query model (OQ) and document model
(OD), and then score the document w.r.t. the query
based on the KL-divergence of the two models (Laf-
ferty and Zhai, 2001):
where V is the set of all the words in our vocabulary.
The documents can then be ranked according to the
ascending order of the KL-divergence values.
Clearly, the two fundamental problems in such a
model are to estimate the query model and the doc-
ument model, and the accuracy of our estimation of
these models would affect the retrieval performance
significantly. The estimation of the query model
can often be improved by exploiting the local cor-
pus structure in a way similar to pseudo-relevance
feedback (Lafferty and Zhai, 2001; Lavrenko and
Croft, 2001; Zhai and Lafferty, 2001a). The esti-
mation of the document model is most often done
through smoothing with the global collection lan-
guage model (Zhai and Lafferty, 2001b), though re-
cently there has been some work on using clusters
for smoothing (Liu and Croft, 2004). Our work is
mainly to extend the previous work on document
smoothing and improve the accuracy of estimation
by better exploiting the local corpus structure. We
now discuss all these in detail.
</bodyText>
<subsectionHeader confidence="0.999958">
2.2 Smoothing of document models
</subsectionHeader>
<bodyText confidence="0.941652142857143">
Given a document d, the simplest way to estimate
the document language model is to treat the docu-
ment as a sample from the underlying multinomial
word distribution and use the maximum likelihood
estimator: P(w|�Od) = c(w,d)
|d |, where c(w, d) is
the count of word w in document d, and |d |is the
</bodyText>
<equation confidence="0.840913125">
D( �OQ  ||�Od) = 1:
wEV
OQ) × log Aw|
Aw|
OQ)
Aw|
�
�Od)
</equation>
<page confidence="0.991433">
408
</page>
<bodyText confidence="0.999913157894737">
length of d. However, as discussed in virtually all
the existing work on using language models for re-
trieval, such an estimate is problematic and inaccu-
rate; indeed, it would assign zero probability to any
word not present in document d, causing problems
in scoring a document with query likelihood or KL-
divergence (Zhai and Lafferty, 2001b). Intuitively,
such an estimate is inaccurate because the document
is a small sample.
To solve this problem, many different smoothing
techniques have been proposed and studied, usually
involving some kind of interpolation of the maxi-
mum likelihood estimate and a global collection lan-
guage model (Hiemstra and Kraaij, 1998; Miller et
al., 1999; Zhai and Lafferty, 2001b). For exam-
ple, Jelinek-Mercer(JM) and Dirichlet are two com-
monly used smoothing methods (Zhai and Lafferty,
2001b). JM smoothing uses a fixed parameter λ to
control the interpolation:
</bodyText>
<equation confidence="0.999328">
P(wj�Od) = λc(w, d)
jdj + (1 − λ)P(wjOC),
</equation>
<bodyText confidence="0.998600666666667">
while the Dirichlet smoothing uses a document-
dependent coefficient (parameterized with µ) to con-
trol the interpolation:
</bodyText>
<equation confidence="0.9964865">
Od) = c(w, d) + µP(wjOC) .
jdj + µ
</equation>
<bodyText confidence="0.981248">
Here P(wjOC) is the probability of word w given by
the collection language model OC, which is usually
estimated using the whole collection of documents
C, e.g., P(wjOC) = EdEC |d |).
</bodyText>
<subsectionHeader confidence="0.9951">
2.3 Cluster-based document model (CBDM)
</subsectionHeader>
<bodyText confidence="0.9999623">
Recently, the cluster structure of the corpus has been
exploited to improve language models for retrieval
(Kurland and Lee, 2004; Liu and Croft, 2004). In
particular, the cluster-based language model pro-
posed in (Liu and Croft, 2004) uses clustering infor-
mation to further smooth a document model. It di-
vides all documents into K different clusters (K =
1000 in their experiments). Both cluster informa-
tion and collection information are used to improve
the estimate of the document model:
</bodyText>
<equation confidence="0.999726666666667">
Od) = λc(w, d)
jdj + (1 − λ)
x[βP(wjOLd) + (1 − β)P(wjOC)],
</equation>
<bodyText confidence="0.9999672">
where OLd stands for document d’s cluster model
and λ and β are smoothing parameters. In this
clustering-based smoothing method, we first smooth
a cluster model with the collection model using
Dirichlet smoothing, and then use smoothed cluster
model as a new reference model to further smooth
the document model using JM smoothing; empirical
results show that the added cluster information in-
deed enhances retrieval performance (Liu and Croft,
2004).
</bodyText>
<subsectionHeader confidence="0.998419">
2.4 Document expansion
</subsectionHeader>
<bodyText confidence="0.999996138888889">
From the viewpoint of data augmentation, the
clustering-based language model can be regarded as
“expanding” a document with more data from the
cluster that contains the document. This is intu-
itively better than simply expanding every document
with the same collection language model as in the
case of JM or Dirichlet smoothing. Looking at it
from this perspective, we see that, as the “extra data”
for smoothing a document model, the cluster con-
taining the document is often not optimal. Indeed,
the purpose of clustering is to group similar doc-
uments together, hence a cluster model represents
well the overall property of all the documents in the
cluster. However, such an average model is often not
accurate for smoothing each individual document.
We illustrate this problem in Figure 1(a), where we
show two documents d and a in cluster D. Clearly
the generative model of cluster D is more suitable
for smoothing document a than document d. In gen-
eral, the cluster model is more suitable for smooth-
ing documents close to the centroid, such as a, but is
inaccurate for smoothing a document at the bound-
ary, such as d.
To achieve optimal smoothing, each document
should ideally have its own cluster centered on the
document, as shown in Figure 1(b). This is pre-
cisely what we propose – expanding each document
with a probabilistic neighborhood around the doc-
ument and estimate the document model based on
such a virtual, expanded document. We can then ap-
ply any simple interpolation-based method (e.g., JM
or Dirichlet) to such a “virtual document” and treat
the word counts given by this “virtual document” as
if they were the original word counts.
The use of neighborhood information is worth
more discussion. First of all, neighborhood is not a
</bodyText>
<equation confidence="0.7567565">
P(wj
P(wj
</equation>
<page confidence="0.951349">
409
</page>
<figure confidence="0.999247833333333">
d’s neighbors
cluster D
a
d d
d
(a) (b)
</figure>
<figureCaption confidence="0.971008">
Figure 1: Clusters, neighborhood, and document ex-
pansion
</figureCaption>
<bodyText confidence="0.999791965517241">
clearly defined concept. In the narrow sense, only
a few documents close to the original one should
be included in the neighborhood, while in the wide
sense, the whole collection can be potentially in-
cluded. It is thus a challenge to define the neighbor-
hood concept reasonably. Secondly, the assumption
that neighbor documents are sampled from the same
generative model as the original document is not
completely valid. We probably do not want to trust
them so much as the original one. We solve these
two problems by associating a confidence value with
every document in the collection, which reflects our
belief that the document is sampled from the same
underlying model as the original document. When a
document is close to the original one, we have high
confidence, but when it is farther apart, our confi-
dence would fade away. In this way, we construct
a probabilistic neighborhood which can potentially
include all the documents with different confidence
values. We call a language model based on such a
neighborhood document expansion language model
(DELM).
Technically, we are looking for a new enlarged
document d&apos; for each document d in a text collec-
tion, such that the new document d&apos; can be used
to estimate the hidden generative model of d more
accurately. Since a good d&apos; should presumably be
based on both the original document d and its neigh-
borhood N(d), we define a function φ:
</bodyText>
<equation confidence="0.854548">
d&apos; = φ(d, N(d)). (1)
</equation>
<bodyText confidence="0.9989515">
The precise definition of the neighborhood con-
cept N(d) relies on the distance or similarity be-
tween each pair of documents. Here, we simply
choose the commonly used cosine similarity, though
other choices may also be possible. Given any two
document models X and Y , the cosine similarity is
</bodyText>
<figureCaption confidence="0.988598">
Figure 2: Normal distribution of confidence values.
</figureCaption>
<bodyText confidence="0.99979552173913">
To model the uncertainty of neighborhood, we as-
sign a confidence value γd(b) to every document b in
the collection to indicate how strongly we believe b
is sampled from d’s hidden model. In general, γd(b)
can be set based on the similarity of b and d – the
more similar b and d are, the larger γd(b) would
be. With these confidence values, we construct a
probabilistic neighborhood with every document in
it, each with a different weight. The whole problem
is thus reduced to how to define γd(b) exactly.
Intuitively, an exponential decay curve can help
regularize the influence from remote documents. We
therefore want γd(b) to satisfy a normal distribution
centered around d. Figure 2 illustrates the shape
of this distribution. The black dots are neighbor-
hood documents centered around d. Their proba-
bility values are determined by their distances to the
center. We fortunately observe that the cosine sim-
ilarities, which we use to decide the neighborhood,
are roughly of this decay shape. We thus use them
directly without further transformation because that
would introduce unnecessary parameters. We set
γd(b) by normalizing the cosine similarity scores :
</bodyText>
<equation confidence="0.920642">
γd(b) = EVEC−{d} sim(d, b&apos;).
</equation>
<bodyText confidence="0.999979142857143">
Function φ serves to balance the confidence be-
tween d and its neighborhood N(d) in the model es-
timation step. Intuitively, a shorter document is less
sufficient, hence needs more help from its neighbor-
hood. Conversely, a longer one can rely more on
itself. We use a parameter α to control this balance.
Thus finally, we obtain a pseudo document d&apos; with
</bodyText>
<equation confidence="0.97139725">
defined as:
sim(X, Y ) = Ei xi X yi
V Ei(xi)2 X Ei(yi)2 .
sim(d, b)
</equation>
<page confidence="0.969245">
410
</page>
<table confidence="0.978968727272727">
#document queries #total qrel
AP 242918 51-150 21819
LA 131896 301-400 2350
WSJ 173252 51-100 and 151-200 10141
SJMN 90257 51-150 4881
TREC8 528155 401-450 4728
DOE 226087 DOE queries 2047
the following pseudo term count:
c(w, d&apos;) = αc(w, d) + (1 − α)
�x (-yd(b) x c(w, b)),
bEC−{d}
</table>
<bodyText confidence="0.999940230769231">
We hypothesize that, in general, Od can be estimated
more accurately from d&apos; rather than d itself because
d&apos; contains more complete information about Od.
This hypothesis can be tested by by comparing the
retrieval results of applying any smoothing method
to d with those of applying the same method to d&apos;.
In our experiments, we will test this hypothesis with
both JM smoothing and Dirichlet smoothing.
Note that the proposed document expansion tech-
nique is quite general. Indeed, since it transforms
the original document to a potentially better “ex-
panded document”, it can presumably be used to-
gether with any retrieval method, including the vec-
tor space model. In this paper, we focus on evalu-
ating this technique with the language modeling ap-
proach.
Because of the decay shape of the neighborhood
and for the sake of efficiency, we do not have to ac-
tually use all documents in C −{d}. Instead, we can
safely cut off the documents on the tail, and only use
the top M closest neighbors for each document. We
show in the experiment section that the performance
is not sensitive to the choice of M when M is suf-
ficiently large (for example 100). Also, since doc-
ument expansion can be done completely offline, it
can scale up to large collections.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999676357142857">
We evaluate the proposed method over six repre-
sentative TREC data sets (Voorhees and Harman,
2001): AP (Associated Press news 1988-90), LA
(LA Times), WSJ (Wall Street Journal 1987-92),
SJMN (San Jose Mercury News 1991), DOE (De-
partment of Energy), and TREC8 (the ad hoc data
used in TREC8). Table 1 shows the statistics of these
data.
We choose the first four TREC data sets for per-
formance comparison with (Liu and Croft, 2004).
To ensure that the comparison is meaningful, we use
identical sources (after all preprocessing). In addi-
tion, we use the large data set TREC8 to show that
our algorithm can scale up, and use DOE because its
</bodyText>
<tableCaption confidence="0.997152">
Table 1: Experiment data sets
</tableCaption>
<bodyText confidence="0.8743345">
documents are usually short, and our previous expe-
rience shows that it is a relatively difficult data set.
</bodyText>
<subsectionHeader confidence="0.99786">
3.1 Neighborhood document expansion
</subsectionHeader>
<bodyText confidence="0.999977314285714">
Our model boils down to a standard query likelihood
model when no neighborhood document is used. We
therefore use two most commonly used smoothing
methods, JM and Dirichlet, as our baselines. The re-
sults are shown in Table 2, where we report both the
mean average precision (MAP) and precision at 10
documents. JM and Dirichlet indicate the standard
language models with JM smoothing and Dirichlet
smoothing respectively, and the other two are the
ones combined with our document expansion. For
both baselines, we tune the parameters (A for JM,
and p for Dirichlet) to be optimal. We then use the
same values of A or p without further tuning for the
document expansion runs, which means that the pa-
rameters may not necessarily optimal for the docu-
ment expansion runs. Despite this disadvantage, we
see that the document expansion runs significantly
outperform their corresponding baselines, with more
than 15% relative improvement on AP. The parame-
ters M and α were set to 100 and 0.5, respectively.
To understand the improvement in more detail, we
show the precision values at different levels of recall
for the AP data in Table 3. Here we see that our
method significantly outperforms the baseline at ev-
ery precision point.
In our model, we introduce two additional param-
eters: M and α. We first examine M here, and then
study α in Section 3.3. Figure 3 shows the perfor-
mance trend with respect to the values of M. The
x-axis is the values of M, and the y-axis is the non-
interpolated precision averaging over all 50 queries.
We draw two conclusions from this plot: (1) Neigh-
borhood information improves retrieval accuracy;
adding more documents leads to better retrieval re-
sults. (2) The performance becomes insensitive to
</bodyText>
<page confidence="0.997068">
411
</page>
<table confidence="0.999915571428571">
Data JM DELM+JM (impr. %) Dirichlet DELM + Diri.(impr. %)
AP AvgPrec 0.2058 0.2405 (16.8%***) 0.2168 0.2505 (15.5%***)
P@10 0.3990 0.4444 (11.4%***) 0.4323 0.4515 (4.4%**)
DOE AvgPrec 0.1759 0.1904 (8.3%***) 0.1804 0.1898 (5.2%**)
P@10 0.2629 0.2943 (11.9%*) 0.2600 0.2800 (7.7%*)
TREC8 AvgPrec 0.2392 0.2539 (6.01%**) 0.2567 0.2671 (4.05%*)
P@10 0.4300 0.4460 (3.7%) 0.4500 0.4740 (5.3%*)
</table>
<tableCaption confidence="0.9755785">
Table 2: Comparisons with baselines. *,**,*** indicate that we accept the improvement hypothesis by
Wilcoxon test at significance level 0.1, 0.05, 0.01 respectively.
</tableCaption>
<table confidence="0.99994705882353">
AP, TREC queries 51-150
Dirichlet DELM+Diri Improvement(%)
Rel. 21819 21819
Rel.Retr. 10126 10917 7.81% ***
Prec.
0.0 0.6404 0.6605 3.14% *
0.1 0.4333 0.4785 10.4% ***
0.2 0.3461 0.3983 15.1% ***
0.3 0.2960 0.3496 18.1% ***
0.4 0.2436 0.2962 21.6% ***
0.5 0.2060 0.2418 17.4% ***
0.6 0.1681 0.1975 17.5% ***
0.7 0.1290 0.1580 22.5% ***
0.8 0.0862 0.1095 27.0% **
0.9 0.0475 0.0695 46.3% **
1.0 0.0220 0.0257 16.8%
ave. 0.2168 0.2505 15.5% ***
</table>
<tableCaption confidence="0.997987">
Table 3: PR curve on AP data. *,**,*** indicate that
</tableCaption>
<bodyText confidence="0.892509">
we accept the improvement hypothesis by Wilcoxon
test at significant level 0.1, 0.05, 0.01 respectively.
M when M is sufficiently large, namely 100. The
reason is twofold: First, since the neighborhood is
centered around the original document, when M is
large, the expansion may be evenly magnified on all
term dimensions. Second, the exponentially decay-
ing confidence values reduce the influence of remote
documents.
</bodyText>
<subsectionHeader confidence="0.999981">
3.2 Comparison with CBDM
</subsectionHeader>
<bodyText confidence="0.934982363636364">
In this section, we compare the CBDM method us-
ing the model performing the best in (Liu and Croft,
2004)1. Furthermore, we also set Dirichlet prior pa-
rameter p = 1000, as mentioned in (Liu and Croft,
2004), to rule out any potential influence of Dirichlet
smoothing.
Table 4 shows that our model outperforms CBDM
in MAP values on four data sets; the improvement
&apos;We use the exact same data, queries, stemming and all
other preprocessing techniques. The baseline results in (Liu and
Croft, 2004) are confirmed.
</bodyText>
<figure confidence="0.501895">
0 100 200 300 400 500 600 700 800
M : the number of neighborhood documents
</figure>
<figureCaption confidence="0.976235">
Figure 3: Performance change with respect to M
</figureCaption>
<table confidence="0.9995994">
CBDM DELM+Diri. improvement(%)
AP 0.2326 0.2505 7.7%
LA 0.2590 0.2655 2.5%
WSJ 0.3006 0.3113 3.6%
SJMN 0.2171 0.2266 4.3%
</table>
<tableCaption confidence="0.99993">
Table 4: Comparisons with CBDM.
</tableCaption>
<bodyText confidence="0.9998885">
presumably comes from a more principled way of
exploiting corpus structures. Given that clustering
can at least capture the local structure to some ex-
tent, it should not be very surprising that the im-
provement of document expansion over CBDM is
much less than that over the baselines.
Note that we cannot fulfill Wilcoxon test because
of the lack of the individual query results of CBDM.
</bodyText>
<subsectionHeader confidence="0.999791">
3.3 Impact on short documents
</subsectionHeader>
<bodyText confidence="0.999910545454545">
Document expansion is to solve the insufficient sam-
pling problem. Intuitively, a short document is less
sufficient than a longer one, hence would need more
“help” from its neighborhood. We design experi-
ments to test this hypothesis.
Specifically, we randomly shrink each document
in AP88-89 to a certain percentage of its original
length. For example, a shrinkage factor of 30%
means each term has 30% chance to stay, or 70%
chance to be filtered out. In this way, we reduce the
original data set to a new one with the same number
</bodyText>
<figure confidence="0.998797533333333">
average precesion
0.27
0.26
0.25
0.24
0.23
0.22
0.21
0.19
0.18
0.17
0.2
AP
DOE
TREC8
</figure>
<page confidence="0.992653">
412
</page>
<table confidence="0.9914212">
average doc length 30% 50% 70% 100%
baseline 0.1273 0.1672 0.1916 0.2168
document expansion 0.1794 0.2137 0.2307 0.2505
optimal α 0.2 0.3 0.3 0.4
improvement(%) 41% 28% 20% 16%
</table>
<figureCaption confidence="0.994307">
Figure 4: Performance change with respect to α
</figureCaption>
<bodyText confidence="0.999882875">
of documents but a shorter average document length.
Table 5 shows the experiment results over docu-
ment sets with different average document lengths.
The results indeed support our hypothesis that doc-
ument expansion does help short documents more
than longer ones. While we can manage to improve
41% on a 30%-length corpus, the same model only
gets 16% improvement on the full length corpus.
To understand how α affects the performance we
plot the sensitivity curves in Figure 4. The curves all
look similar, but the optimal points slightly migrate
when the average document length becomes shorter.
A 100% corpus gets optimal at α = 0.4, but 30%
corpus has to use α = 0.2 to obtain its optimum.
(All optimal α values are presented in the fourth row
of Table 5.)
</bodyText>
<sectionHeader confidence="0.863897" genericHeader="method">
3.4 Further improvement with pseudo
feedback
</sectionHeader>
<bodyText confidence="0.9995234">
Query expansion has been proved to be an effec-
tive way of utilizing corpus information to improve
the query representation (Rocchio, 1971; Zhai and
Lafferty, 2001a). It is thus interesting to examine
whether our model can be combined with query ex-
pansion to further improve the retrieval accuracy.
We use the model-based feedback proposed in (Zhai
and Lafferty, 2001a) and take top 5 returned docu-
ments for feedback. There are two parameters in the
model-based pseudo feedback process: the noisy pa-
</bodyText>
<table confidence="0.9995444">
DELM pseudo DELM+pseudo Impr.(%)
AP 0.2505 0.2643 0.2726 3.14%*
LA 0.2655 0.2769 0.2901 4.77%
TREC8 0.2671 0.2716 0.2809 3.42%**
DOE 0.1898 0.1918 0.2046 6.67%***
</table>
<tableCaption confidence="0.997425">
Table 6: Combination with pseudo feed-
</tableCaption>
<bodyText confidence="0.783093">
back.*,**,*** indicate that we accept the improve-
ment hypothesis by Wilcoxon test at significant
level 0.1, 0.05, 0.01 respectively.
</bodyText>
<table confidence="0.99152">
pseu. inter. combined (%) z-score
AP 0.2643 0.2450 0.2660 (0.64%) -0.2888
LA 0.2769 0.2662 0.2636 (-0.48%) -1.0570
TREC8 0.2716 0.2702 0.2739 (0.84%) -1.6938
</table>
<tableCaption confidence="0.873094">
Table 7: Performance of the interpolation algorithm
combined with the pseudo feedback.
</tableCaption>
<bodyText confidence="0.979180966666667">
rameter ρ and the interpolation parameter σ2. We fix
ρ = 0.9 and tune σ to optimal, and use them directly
in the feedback process combined with our models.
(It again means that σ is probably not optimal in our
results.) The combination is conducted in the fol-
lowing way: (1) Retrieve documents by our DELM
method; (2) Choose top 5 document to do the model-
based feedback; (3) Use the expanded query model
to retrieve documents again with DELM method.
Table 6 shows the experiment results (MAP); in-
deed, by combining DELM with pseudo feedback,
we can obtain significant further improvement of
performance.
As another baseline, we also tested the algorithm
proposed in (Kurland and Lee, 2004). Since the al-
gorithm overlaps with pseudo feedback process, it is
not easy to further combine them. We implement its
best-performing algorithm, “interpolation” (labeled
as inter. ), and show the results in Table 7. Here,
we use the same three data sets as used in (Kurland
and Lee, 2004). We tune the feedback parameters to
optimal in each experiment. The second last column
in Table 7 shows the performance of combination of
the “interpolation” model with the pseudo feedback
and its improvement percentage. The last column is
the z-scores of Wilcoxon test. The negative z-scores
indicate that none of the improvement is significant.
2 (Zhai and Lafferty, 2001a) uses different notations. We
change them because α has already been used in our own
model.
</bodyText>
<tableCaption confidence="0.884324">
Table 5: Impact on short documents (in MAP)
</tableCaption>
<figure confidence="0.998437642857143">
0.26
0.24
0.22
0.2
0.18
0.16
0.14
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
alpha
average precision
30%
50%
70%
100%
</figure>
<page confidence="0.998611">
413
</page>
<sectionHeader confidence="0.999522" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999565">
In this paper, we proposed a novel document expan-
sion method to enrich the document sample through
exploiting the local corpus structure. Unlike pre-
vious cluster-based models, we smooth each doc-
ument using a probabilistic neighborhood centered
around the document itself.
Experiment results show that (1) The proposed
document expansion method outperforms both the
“no expansion” baselines and the cluster-based mod-
els. (2) Our model is relatively insensitive to the set-
ting of parameter M as long as it is sufficiently large,
while the parameter α should be set according to the
document length; short documents need a smaller
α to obtain more help from its neighborhood. (3)
Document expansion can be combined with pseudo
feedback to further improve performance. Since any
retrieval model can be presumably applied on top of
the expanded documents, we believe that the pro-
posed technique can be potentially useful for any re-
trieval model.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999937">
This work is in part supported by the National Sci-
ence Foundation under award number IIS-0347933.
We thank Xiaoyong Liu for kindly providing us sev-
eral processed data sets for our performance com-
parison. We thank Jing Jiang and Azadeh Shakery
for helping improve the paper writing, and thank the
anonymous reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.99912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999245711864407">
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal Report TR-10-98, Harvard University.
D. Hiemstra and W. Kraaij. 1998. Twenty-one at trec-7:
Ad-hoc and cross-language track. In Proc. of Seventh
Text REtrieval Conference (TREC-7).
R. Kneser and H. Ney. 1995. Improved smoothing for m-
gram languagemodeling. In Proceedings of the Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Oren Kurland and Lillian Lee. 2004. Corpus structure,
language models, and ad hoc information retrieval. In
SIGIR ’04: Proceedings of the 27th annual interna-
tional conference on Research and development in in-
formation retrieval, pages 194–201. ACM Press.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of SI-
GIR’2001, pages 111–119, Sept.
John Lafferty and ChengXiang Zhai. 2003. Probabilistic
relevance models based on document and query gen-
eration.
Victor Lavrenko and Bruce Croft. 2001. Relevance-
based language models. In Proceedings of SI-
GIR’2001, Sept.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based
retrieval using language models. In SIGIR ’04: Pro-
ceedings of the 27th annual international conference
on Research and development in information retrieval,
pages 186–193. ACM Press.
D. H. Miller, T. Leek, and R. Schwartz. 1999. A hid-
den markov model information retrieval system. In
Proceedings of the 1999 ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 214–221.
J. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
the ACM SIGIR, pages 275–281.
J. Rocchio. 1971. Relevance feedback in information re-
trieval. In The SMART Retrieval System: Experiments
in Automatic Document Processing, pages 313–323.
Prentice-Hall Inc.
Amit Singhal and Fernando Pereira. 1999. Document
expansion for speech retrieval. In SIGIR ’99: Pro-
ceedings of the 22nd annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 34–41. ACM Press.
E. Voorhees and D. Harman, editors. 2001. Proceedings
of Text REtrieval Conference (TREC1-9). NIST Spe-
cial Publications. http://trec.nist.gov/pubs.html.
Chengxiang Zhai and John Lafferty. 2001a. Model-
based feedback in the KL-divergence retrieval model.
In Tenth International Conference on Information and
Knowledge Management (CIKM 2001), pages 403–
410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of SI-
GIR’2001, pages 334–342, Sept.
</reference>
<page confidence="0.998119">
414
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970985">
<title confidence="0.999895">Language Model Information Retrieval with Document Expansion</title>
<author confidence="0.999112">Tao Tao</author>
<author confidence="0.999112">Xuanhui Wang</author>
<author confidence="0.999112">Qiaozhu Mei</author>
<author confidence="0.999112">ChengXiang</author>
<affiliation confidence="0.998615">Department of Computer University of Illinois at Urbana Champaign</affiliation>
<abstract confidence="0.998660789473685">Language model information retrieval depends on accurate estimation of document models. In this paper, we propose a document expansion technique to deal with the problem of insufficient sampling of documents. We construct a probabilistic neighborhood for each document, and expand the document with its neighborhood information. The expanded document provides a more accurate estimation of the document model, thus improves retrieval accuracy. Moreover, since document expansion and pseudo feedback exploit different corpus structures, they can be combined to further improve performance. The experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="1560" citStr="Chen and Goodman, 1998" startWordPosition="231" endWordPosition="235"> retrieval with statistical language models (Lafferty and Zhai, 2003) has recently attracted much more attention because of its solid theoretical background as well as its good empirical performance. In this approach, queries and documents are assumed to be sampled from hidden generative models, and the similarity between a document and a query is then calculated through the similarity between their underlying models. Clearly, good retrieval performance relies on the accurate estimation of the query and document models. Indeed, smoothing of document models has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Laffe</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hiemstra</author>
<author>W Kraaij</author>
</authors>
<title>Twenty-one at trec-7: Ad-hoc and cross-language track.</title>
<date>1998</date>
<booktitle>In Proc. of Seventh Text REtrieval Conference (TREC-7).</booktitle>
<contexts>
<context position="2144" citStr="Hiemstra and Kraaij, 1998" startWordPosition="331" endWordPosition="334"> be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Croft, 2004; Kurland and Lee, 2004) have shown that local corpus structures can be exploited to improve retrieval performance. In this paper, we further study the use of local corpus structures for document model estimation and propose to use document expansion to better exploit local corpus structures for estimating document language models. According to statistical principles, the accuracy of a statistical estimator is largely determined by the sampling size of the observed data; a small data set generally would result in large var</context>
<context position="8330" citStr="Hiemstra and Kraaij, 1998" startWordPosition="1343" endWordPosition="1346">virtually all the existing work on using language models for retrieval, such an estimate is problematic and inaccurate; indeed, it would assign zero probability to any word not present in document d, causing problems in scoring a document with query likelihood or KLdivergence (Zhai and Lafferty, 2001b). Intuitively, such an estimate is inaccurate because the document is a small sample. To solve this problem, many different smoothing techniques have been proposed and studied, usually involving some kind of interpolation of the maximum likelihood estimate and a global collection language model (Hiemstra and Kraaij, 1998; Miller et al., 1999; Zhai and Lafferty, 2001b). For example, Jelinek-Mercer(JM) and Dirichlet are two commonly used smoothing methods (Zhai and Lafferty, 2001b). JM smoothing uses a fixed parameter λ to control the interpolation: P(wj�Od) = λc(w, d) jdj + (1 − λ)P(wjOC), while the Dirichlet smoothing uses a documentdependent coefficient (parameterized with µ) to control the interpolation: Od) = c(w, d) + µP(wjOC) . jdj + µ Here P(wjOC) is the probability of word w given by the collection language model OC, which is usually estimated using the whole collection of documents C, e.g., P(wjOC) = </context>
</contexts>
<marker>Hiemstra, Kraaij, 1998</marker>
<rawString>D. Hiemstra and W. Kraaij. 1998. Twenty-one at trec-7: Ad-hoc and cross-language track. In Proc. of Seventh Text REtrieval Conference (TREC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved smoothing for mgram languagemodeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="1582" citStr="Kneser and Ney, 1995" startWordPosition="236" endWordPosition="239">cal language models (Lafferty and Zhai, 2003) has recently attracted much more attention because of its solid theoretical background as well as its good empirical performance. In this approach, queries and documents are assumed to be sampled from hidden generative models, and the similarity between a document and a query is then calculated through the similarity between their underlying models. Clearly, good retrieval performance relies on the accurate estimation of the query and document models. Indeed, smoothing of document models has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several r</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved smoothing for mgram languagemodeling. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Kurland</author>
<author>Lillian Lee</author>
</authors>
<title>Corpus structure, language models, and ad hoc information retrieval.</title>
<date>2004</date>
<booktitle>In SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval,</booktitle>
<pages>194--201</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2240" citStr="Kurland and Lee, 2004" startWordPosition="347" endWordPosition="350">d for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Croft, 2004; Kurland and Lee, 2004) have shown that local corpus structures can be exploited to improve retrieval performance. In this paper, we further study the use of local corpus structures for document model estimation and propose to use document expansion to better exploit local corpus structures for estimating document language models. According to statistical principles, the accuracy of a statistical estimator is largely determined by the sampling size of the observed data; a small data set generally would result in large variances, thus can not be trusted completely. Unfortunately, in retrieval, we often have to estima</context>
<context position="9110" citStr="Kurland and Lee, 2004" startWordPosition="1472" endWordPosition="1475">1b). JM smoothing uses a fixed parameter λ to control the interpolation: P(wj�Od) = λc(w, d) jdj + (1 − λ)P(wjOC), while the Dirichlet smoothing uses a documentdependent coefficient (parameterized with µ) to control the interpolation: Od) = c(w, d) + µP(wjOC) . jdj + µ Here P(wjOC) is the probability of word w given by the collection language model OC, which is usually estimated using the whole collection of documents C, e.g., P(wjOC) = EdEC |d |). 2.3 Cluster-based document model (CBDM) Recently, the cluster structure of the corpus has been exploited to improve language models for retrieval (Kurland and Lee, 2004; Liu and Croft, 2004). In particular, the cluster-based language model proposed in (Liu and Croft, 2004) uses clustering information to further smooth a document model. It divides all documents into K different clusters (K = 1000 in their experiments). Both cluster information and collection information are used to improve the estimate of the document model: Od) = λc(w, d) jdj + (1 − λ) x[βP(wjOLd) + (1 − β)P(wjOC)], where OLd stands for document d’s cluster model and λ and β are smoothing parameters. In this clustering-based smoothing method, we first smooth a cluster model with the collecti</context>
<context position="25444" citStr="Kurland and Lee, 2004" startWordPosition="4215" endWordPosition="4218">ne σ to optimal, and use them directly in the feedback process combined with our models. (It again means that σ is probably not optimal in our results.) The combination is conducted in the following way: (1) Retrieve documents by our DELM method; (2) Choose top 5 document to do the modelbased feedback; (3) Use the expanded query model to retrieve documents again with DELM method. Table 6 shows the experiment results (MAP); indeed, by combining DELM with pseudo feedback, we can obtain significant further improvement of performance. As another baseline, we also tested the algorithm proposed in (Kurland and Lee, 2004). Since the algorithm overlaps with pseudo feedback process, it is not easy to further combine them. We implement its best-performing algorithm, “interpolation” (labeled as inter. ), and show the results in Table 7. Here, we use the same three data sets as used in (Kurland and Lee, 2004). We tune the feedback parameters to optimal in each experiment. The second last column in Table 7 shows the performance of combination of the “interpolation” model with the pseudo feedback and its improvement percentage. The last column is the z-scores of Wilcoxon test. The negative z-scores indicate that none</context>
</contexts>
<marker>Kurland, Lee, 2004</marker>
<rawString>Oren Kurland and Lillian Lee. 2004. Corpus structure, language models, and ad hoc information retrieval. In SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 194–201. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR’2001,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="6255" citStr="Lafferty and Zhai, 2001" startWordPosition="989" endWordPosition="993"> representative state-of-the-art language modeling approach for retrieval. It covers the basic language modeling approach (i.e., the query likelihood method) as a special case and can support feedback more naturally. In this approach, a query and a document are assumed to be generated from a unigram query language model OQ and a unigram document language model OD, respectively. Given a query and a document, we would first compute an estimate of the corresponding query model (OQ) and document model (OD), and then score the document w.r.t. the query based on the KL-divergence of the two models (Lafferty and Zhai, 2001): where V is the set of all the words in our vocabulary. The documents can then be ranked according to the ascending order of the KL-divergence values. Clearly, the two fundamental problems in such a model are to estimate the query model and the document model, and the accuracy of our estimation of these models would affect the retrieval performance significantly. The estimation of the query model can often be improved by exploiting the local corpus structure in a way similar to pseudo-relevance feedback (Lafferty and Zhai, 2001; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001a). The estimat</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR’2001, pages 111–119, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Probabilistic relevance models based on document and query generation.</title>
<date>2003</date>
<contexts>
<context position="1007" citStr="Lafferty and Zhai, 2003" startWordPosition="143" endWordPosition="146">ampling of documents. We construct a probabilistic neighborhood for each document, and expand the document with its neighborhood information. The expanded document provides a more accurate estimation of the document model, thus improves retrieval accuracy. Moreover, since document expansion and pseudo feedback exploit different corpus structures, they can be combined to further improve performance. The experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method. 1 Introduction Information retrieval with statistical language models (Lafferty and Zhai, 2003) has recently attracted much more attention because of its solid theoretical background as well as its good empirical performance. In this approach, queries and documents are assumed to be sampled from hidden generative models, and the similarity between a document and a query is then calculated through the similarity between their underlying models. Clearly, good retrieval performance relies on the accurate estimation of the query and document models. Indeed, smoothing of document models has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001</context>
</contexts>
<marker>Lafferty, Zhai, 2003</marker>
<rawString>John Lafferty and ChengXiang Zhai. 2003. Probabilistic relevance models based on document and query generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Bruce Croft</author>
</authors>
<title>Relevancebased language models.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR’2001,</booktitle>
<contexts>
<context position="6815" citStr="Lavrenko and Croft, 2001" startWordPosition="1084" endWordPosition="1087">on the KL-divergence of the two models (Lafferty and Zhai, 2001): where V is the set of all the words in our vocabulary. The documents can then be ranked according to the ascending order of the KL-divergence values. Clearly, the two fundamental problems in such a model are to estimate the query model and the document model, and the accuracy of our estimation of these models would affect the retrieval performance significantly. The estimation of the query model can often be improved by exploiting the local corpus structure in a way similar to pseudo-relevance feedback (Lafferty and Zhai, 2001; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001a). The estimation of the document model is most often done through smoothing with the global collection language model (Zhai and Lafferty, 2001b), though recently there has been some work on using clusters for smoothing (Liu and Croft, 2004). Our work is mainly to extend the previous work on document smoothing and improve the accuracy of estimation by better exploiting the local corpus structure. We now discuss all these in detail. 2.2 Smoothing of document models Given a document d, the simplest way to estimate the document language model is to treat the document as </context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and Bruce Croft. 2001. Relevancebased language models. In Proceedings of SIGIR’2001, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyong Liu</author>
<author>W Bruce Croft</author>
</authors>
<title>Cluster-based retrieval using language models.</title>
<date>2004</date>
<booktitle>In SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval,</booktitle>
<pages>186--193</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2216" citStr="Liu and Croft, 2004" startWordPosition="343" endWordPosition="346">erty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Croft, 2004; Kurland and Lee, 2004) have shown that local corpus structures can be exploited to improve retrieval performance. In this paper, we further study the use of local corpus structures for document model estimation and propose to use document expansion to better exploit local corpus structures for estimating document language models. According to statistical principles, the accuracy of a statistical estimator is largely determined by the sampling size of the observed data; a small data set generally would result in large variances, thus can not be trusted completely. Unfortunately, in retrieval,</context>
<context position="4552" citStr="Liu and Croft, 2004" startWordPosition="723" endWordPosition="726">framework and implement the idea quite differently. Our main idea is to augment a document probabilistically with potentially all other documents in the collection that are similar to the document. The probability associated with each neighbor document reflects how likely the neighbor document is from the underlying distribution of the original document, thus we have a “probabilistic neighborhood”, which can serve as “extra data” for the document for estimating the underlying language model. From the viewpoint of smoothing, our method extends the existing work on using clusters for smoothing (Liu and Croft, 2004) to allow each document to have its own cluster for smoothing. We evaluated our method using six representative retrieval test sets. The experiment results show that document expansion smoothing consistently outperforms the baseline smoothing methods in all the data sets. It also outperforms a state-of-the-art clustering smoothing method. Analysis shows that the improvement tends to be more significant for short documents, indicating that the improvement indeed comes from the improved estimation of the document language model, since a short document presumably would benefit more from the neigh</context>
<context position="7082" citStr="Liu and Croft, 2004" startWordPosition="1129" endWordPosition="1132">re to estimate the query model and the document model, and the accuracy of our estimation of these models would affect the retrieval performance significantly. The estimation of the query model can often be improved by exploiting the local corpus structure in a way similar to pseudo-relevance feedback (Lafferty and Zhai, 2001; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001a). The estimation of the document model is most often done through smoothing with the global collection language model (Zhai and Lafferty, 2001b), though recently there has been some work on using clusters for smoothing (Liu and Croft, 2004). Our work is mainly to extend the previous work on document smoothing and improve the accuracy of estimation by better exploiting the local corpus structure. We now discuss all these in detail. 2.2 Smoothing of document models Given a document d, the simplest way to estimate the document language model is to treat the document as a sample from the underlying multinomial word distribution and use the maximum likelihood estimator: P(w|�Od) = c(w,d) |d |, where c(w, d) is the count of word w in document d, and |d |is the D( �OQ ||�Od) = 1: wEV OQ) × log Aw| Aw| OQ) Aw| � �Od) 408 length of d. Ho</context>
<context position="9132" citStr="Liu and Croft, 2004" startWordPosition="1476" endWordPosition="1479">a fixed parameter λ to control the interpolation: P(wj�Od) = λc(w, d) jdj + (1 − λ)P(wjOC), while the Dirichlet smoothing uses a documentdependent coefficient (parameterized with µ) to control the interpolation: Od) = c(w, d) + µP(wjOC) . jdj + µ Here P(wjOC) is the probability of word w given by the collection language model OC, which is usually estimated using the whole collection of documents C, e.g., P(wjOC) = EdEC |d |). 2.3 Cluster-based document model (CBDM) Recently, the cluster structure of the corpus has been exploited to improve language models for retrieval (Kurland and Lee, 2004; Liu and Croft, 2004). In particular, the cluster-based language model proposed in (Liu and Croft, 2004) uses clustering information to further smooth a document model. It divides all documents into K different clusters (K = 1000 in their experiments). Both cluster information and collection information are used to improve the estimate of the document model: Od) = λc(w, d) jdj + (1 − λ) x[βP(wjOLd) + (1 − β)P(wjOC)], where OLd stands for document d’s cluster model and λ and β are smoothing parameters. In this clustering-based smoothing method, we first smooth a cluster model with the collection model using Dirichl</context>
<context position="17219" citStr="Liu and Croft, 2004" startWordPosition="2853" endWordPosition="2856">ensitive to the choice of M when M is sufficiently large (for example 100). Also, since document expansion can be done completely offline, it can scale up to large collections. 3 Experiments We evaluate the proposed method over six representative TREC data sets (Voorhees and Harman, 2001): AP (Associated Press news 1988-90), LA (LA Times), WSJ (Wall Street Journal 1987-92), SJMN (San Jose Mercury News 1991), DOE (Department of Energy), and TREC8 (the ad hoc data used in TREC8). Table 1 shows the statistics of these data. We choose the first four TREC data sets for performance comparison with (Liu and Croft, 2004). To ensure that the comparison is meaningful, we use identical sources (after all preprocessing). In addition, we use the large data set TREC8 to show that our algorithm can scale up, and use DOE because its Table 1: Experiment data sets documents are usually short, and our previous experience shows that it is a relatively difficult data set. 3.1 Neighborhood document expansion Our model boils down to a standard query likelihood model when no neighborhood document is used. We therefore use two most commonly used smoothing methods, JM and Dirichlet, as our baselines. The results are shown in T</context>
<context position="20929" citStr="Liu and Croft, 2004" startWordPosition="3462" endWordPosition="3465">68 0.2505 15.5% *** Table 3: PR curve on AP data. *,**,*** indicate that we accept the improvement hypothesis by Wilcoxon test at significant level 0.1, 0.05, 0.01 respectively. M when M is sufficiently large, namely 100. The reason is twofold: First, since the neighborhood is centered around the original document, when M is large, the expansion may be evenly magnified on all term dimensions. Second, the exponentially decaying confidence values reduce the influence of remote documents. 3.2 Comparison with CBDM In this section, we compare the CBDM method using the model performing the best in (Liu and Croft, 2004)1. Furthermore, we also set Dirichlet prior parameter p = 1000, as mentioned in (Liu and Croft, 2004), to rule out any potential influence of Dirichlet smoothing. Table 4 shows that our model outperforms CBDM in MAP values on four data sets; the improvement &apos;We use the exact same data, queries, stemming and all other preprocessing techniques. The baseline results in (Liu and Croft, 2004) are confirmed. 0 100 200 300 400 500 600 700 800 M : the number of neighborhood documents Figure 3: Performance change with respect to M CBDM DELM+Diri. improvement(%) AP 0.2326 0.2505 7.7% LA 0.2590 0.2655 2.</context>
</contexts>
<marker>Liu, Croft, 2004</marker>
<rawString>Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based retrieval using language models. In SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 186–193. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Miller</author>
<author>T Leek</author>
<author>R Schwartz</author>
</authors>
<title>A hidden markov model information retrieval system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="2117" citStr="Miller et al., 1999" startWordPosition="327" endWordPosition="330">ls has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Croft, 2004; Kurland and Lee, 2004) have shown that local corpus structures can be exploited to improve retrieval performance. In this paper, we further study the use of local corpus structures for document model estimation and propose to use document expansion to better exploit local corpus structures for estimating document language models. According to statistical principles, the accuracy of a statistical estimator is largely determined by the sampling size of the observed data; a small data set generall</context>
<context position="8351" citStr="Miller et al., 1999" startWordPosition="1347" endWordPosition="1350">work on using language models for retrieval, such an estimate is problematic and inaccurate; indeed, it would assign zero probability to any word not present in document d, causing problems in scoring a document with query likelihood or KLdivergence (Zhai and Lafferty, 2001b). Intuitively, such an estimate is inaccurate because the document is a small sample. To solve this problem, many different smoothing techniques have been proposed and studied, usually involving some kind of interpolation of the maximum likelihood estimate and a global collection language model (Hiemstra and Kraaij, 1998; Miller et al., 1999; Zhai and Lafferty, 2001b). For example, Jelinek-Mercer(JM) and Dirichlet are two commonly used smoothing methods (Zhai and Lafferty, 2001b). JM smoothing uses a fixed parameter λ to control the interpolation: P(wj�Od) = λc(w, d) jdj + (1 − λ)P(wjOC), while the Dirichlet smoothing uses a documentdependent coefficient (parameterized with µ) to control the interpolation: Od) = c(w, d) + µP(wjOC) . jdj + µ Here P(wjOC) is the probability of word w given by the collection language model OC, which is usually estimated using the whole collection of documents C, e.g., P(wjOC) = EdEC |d |). 2.3 Clust</context>
</contexts>
<marker>Miller, Leek, Schwartz, 1999</marker>
<rawString>D. H. Miller, T. Leek, and R. Schwartz. 1999. A hidden markov model information retrieval system. In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACM SIGIR,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="2096" citStr="Ponte and Croft, 1998" startWordPosition="323" endWordPosition="326">othing of document models has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Croft, 2004; Kurland and Lee, 2004) have shown that local corpus structures can be exploited to improve retrieval performance. In this paper, we further study the use of local corpus structures for document model estimation and propose to use document expansion to better exploit local corpus structures for estimating document language models. According to statistical principles, the accuracy of a statistical estimator is largely determined by the sampling size of the observed data; a sm</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J. Ponte and W. B. Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the ACM SIGIR, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<booktitle>In The SMART Retrieval System: Experiments in Automatic Document Processing,</booktitle>
<pages>313--323</pages>
<publisher>Prentice-Hall Inc.</publisher>
<contexts>
<context position="23816" citStr="Rocchio, 1971" startWordPosition="3956" endWordPosition="3957">e model only gets 16% improvement on the full length corpus. To understand how α affects the performance we plot the sensitivity curves in Figure 4. The curves all look similar, but the optimal points slightly migrate when the average document length becomes shorter. A 100% corpus gets optimal at α = 0.4, but 30% corpus has to use α = 0.2 to obtain its optimum. (All optimal α values are presented in the fourth row of Table 5.) 3.4 Further improvement with pseudo feedback Query expansion has been proved to be an effective way of utilizing corpus information to improve the query representation (Rocchio, 1971; Zhai and Lafferty, 2001a). It is thus interesting to examine whether our model can be combined with query expansion to further improve the retrieval accuracy. We use the model-based feedback proposed in (Zhai and Lafferty, 2001a) and take top 5 returned documents for feedback. There are two parameters in the model-based pseudo feedback process: the noisy paDELM pseudo DELM+pseudo Impr.(%) AP 0.2505 0.2643 0.2726 3.14%* LA 0.2655 0.2769 0.2901 4.77% TREC8 0.2671 0.2716 0.2809 3.42%** DOE 0.1898 0.1918 0.2046 6.67%*** Table 6: Combination with pseudo feedback.*,**,*** indicate that we accept t</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J. Rocchio. 1971. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313–323. Prentice-Hall Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Fernando Pereira</author>
</authors>
<title>Document expansion for speech retrieval.</title>
<date>1999</date>
<booktitle>In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>34--41</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="3715" citStr="Singhal and Pereira, 1999" startWordPosition="588" endWordPosition="592">d come from the same original generative model. In reality, however, since 407 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407–414, New York, June 2006. c�2006 Association for Computational Linguistics the underlying model is unknown to us, we would not really be able to obtain such extra data. The essence of this paper is to use document expansion to obtain high quality extra data to enlarge the sample of a document so as to improve the accuracy of the estimated document language model. Document expansion was previously explored in (Singhal and Pereira, 1999) in the context of the vector space retrieval model, mainly involving selecting more terms from similar documents. Our work differs from this previous work in that we study document expansion in the language modeling framework and implement the idea quite differently. Our main idea is to augment a document probabilistically with potentially all other documents in the collection that are similar to the document. The probability associated with each neighbor document reflects how likely the neighbor document is from the underlying distribution of the original document, thus we have a “probabilis</context>
</contexts>
<marker>Singhal, Pereira, 1999</marker>
<rawString>Amit Singhal and Fernando Pereira. 1999. Document expansion for speech retrieval. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 34–41. ACM Press.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Proceedings of Text REtrieval Conference</booktitle>
<editor>E. Voorhees and D. Harman, editors.</editor>
<note>Special Publications. http://trec.nist.gov/pubs.html.</note>
<marker>2001</marker>
<rawString>E. Voorhees and D. Harman, editors. 2001. Proceedings of Text REtrieval Conference (TREC1-9). NIST Special Publications. http://trec.nist.gov/pubs.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Modelbased feedback in the KL-divergence retrieval model.</title>
<date>2001</date>
<booktitle>In Tenth International Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>403--410</pages>
<contexts>
<context position="1607" citStr="Zhai and Lafferty, 2001" startWordPosition="240" endWordPosition="244">afferty and Zhai, 2003) has recently attracted much more attention because of its solid theoretical background as well as its good empirical performance. In this approach, queries and documents are assumed to be sampled from hidden generative models, and the similarity between a document and a query is then calculated through the similarity between their underlying models. Clearly, good retrieval performance relies on the accurate estimation of the query and document models. Indeed, smoothing of document models has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Cr</context>
<context position="6840" citStr="Zhai and Lafferty, 2001" startWordPosition="1088" endWordPosition="1091">e two models (Lafferty and Zhai, 2001): where V is the set of all the words in our vocabulary. The documents can then be ranked according to the ascending order of the KL-divergence values. Clearly, the two fundamental problems in such a model are to estimate the query model and the document model, and the accuracy of our estimation of these models would affect the retrieval performance significantly. The estimation of the query model can often be improved by exploiting the local corpus structure in a way similar to pseudo-relevance feedback (Lafferty and Zhai, 2001; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001a). The estimation of the document model is most often done through smoothing with the global collection language model (Zhai and Lafferty, 2001b), though recently there has been some work on using clusters for smoothing (Liu and Croft, 2004). Our work is mainly to extend the previous work on document smoothing and improve the accuracy of estimation by better exploiting the local corpus structure. We now discuss all these in detail. 2.2 Smoothing of document models Given a document d, the simplest way to estimate the document language model is to treat the document as a sample from the underly</context>
<context position="8376" citStr="Zhai and Lafferty, 2001" startWordPosition="1351" endWordPosition="1354">e models for retrieval, such an estimate is problematic and inaccurate; indeed, it would assign zero probability to any word not present in document d, causing problems in scoring a document with query likelihood or KLdivergence (Zhai and Lafferty, 2001b). Intuitively, such an estimate is inaccurate because the document is a small sample. To solve this problem, many different smoothing techniques have been proposed and studied, usually involving some kind of interpolation of the maximum likelihood estimate and a global collection language model (Hiemstra and Kraaij, 1998; Miller et al., 1999; Zhai and Lafferty, 2001b). For example, Jelinek-Mercer(JM) and Dirichlet are two commonly used smoothing methods (Zhai and Lafferty, 2001b). JM smoothing uses a fixed parameter λ to control the interpolation: P(wj�Od) = λc(w, d) jdj + (1 − λ)P(wjOC), while the Dirichlet smoothing uses a documentdependent coefficient (parameterized with µ) to control the interpolation: Od) = c(w, d) + µP(wjOC) . jdj + µ Here P(wjOC) is the probability of word w given by the collection language model OC, which is usually estimated using the whole collection of documents C, e.g., P(wjOC) = EdEC |d |). 2.3 Cluster-based document model (</context>
<context position="23841" citStr="Zhai and Lafferty, 2001" startWordPosition="3958" endWordPosition="3961">ts 16% improvement on the full length corpus. To understand how α affects the performance we plot the sensitivity curves in Figure 4. The curves all look similar, but the optimal points slightly migrate when the average document length becomes shorter. A 100% corpus gets optimal at α = 0.4, but 30% corpus has to use α = 0.2 to obtain its optimum. (All optimal α values are presented in the fourth row of Table 5.) 3.4 Further improvement with pseudo feedback Query expansion has been proved to be an effective way of utilizing corpus information to improve the query representation (Rocchio, 1971; Zhai and Lafferty, 2001a). It is thus interesting to examine whether our model can be combined with query expansion to further improve the retrieval accuracy. We use the model-based feedback proposed in (Zhai and Lafferty, 2001a) and take top 5 returned documents for feedback. There are two parameters in the model-based pseudo feedback process: the noisy paDELM pseudo DELM+pseudo Impr.(%) AP 0.2505 0.2643 0.2726 3.14%* LA 0.2655 0.2769 0.2901 4.77% TREC8 0.2671 0.2716 0.2809 3.42%** DOE 0.1898 0.1918 0.2046 6.67%*** Table 6: Combination with pseudo feedback.*,**,*** indicate that we accept the improvement hypothesis</context>
<context position="26106" citStr="Zhai and Lafferty, 2001" startWordPosition="4323" endWordPosition="4326">udo feedback process, it is not easy to further combine them. We implement its best-performing algorithm, “interpolation” (labeled as inter. ), and show the results in Table 7. Here, we use the same three data sets as used in (Kurland and Lee, 2004). We tune the feedback parameters to optimal in each experiment. The second last column in Table 7 shows the performance of combination of the “interpolation” model with the pseudo feedback and its improvement percentage. The last column is the z-scores of Wilcoxon test. The negative z-scores indicate that none of the improvement is significant. 2 (Zhai and Lafferty, 2001a) uses different notations. We change them because α has already been used in our own model. Table 5: Impact on short documents (in MAP) 0.26 0.24 0.22 0.2 0.18 0.16 0.14 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 alpha average precision 30% 50% 70% 100% 413 4 Conclusions In this paper, we proposed a novel document expansion method to enrich the document sample through exploiting the local corpus structure. Unlike previous cluster-based models, we smooth each document using a probabilistic neighborhood centered around the document itself. Experiment results show that (1) The proposed document expans</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001a. Modelbased feedback in the KL-divergence retrieval model. In Tenth International Conference on Information and Knowledge Management (CIKM 2001), pages 403– 410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR’2001,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="1607" citStr="Zhai and Lafferty, 2001" startWordPosition="240" endWordPosition="244">afferty and Zhai, 2003) has recently attracted much more attention because of its solid theoretical background as well as its good empirical performance. In this approach, queries and documents are assumed to be sampled from hidden generative models, and the similarity between a document and a query is then calculated through the similarity between their underlying models. Clearly, good retrieval performance relies on the accurate estimation of the query and document models. Indeed, smoothing of document models has been proved to be very critical (Chen and Goodman, 1998; Kneser and Ney, 1995; Zhai and Lafferty, 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum likelihood estimator would give it a zero probability. This is unreasonable because the zero count is often due to insufficient sampling, and a larger sample of the data would likely contain the term. Smoothing is proposed to address the problem. While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Lafferty, 2001b), several recent studies (Liu and Cr</context>
<context position="6840" citStr="Zhai and Lafferty, 2001" startWordPosition="1088" endWordPosition="1091">e two models (Lafferty and Zhai, 2001): where V is the set of all the words in our vocabulary. The documents can then be ranked according to the ascending order of the KL-divergence values. Clearly, the two fundamental problems in such a model are to estimate the query model and the document model, and the accuracy of our estimation of these models would affect the retrieval performance significantly. The estimation of the query model can often be improved by exploiting the local corpus structure in a way similar to pseudo-relevance feedback (Lafferty and Zhai, 2001; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001a). The estimation of the document model is most often done through smoothing with the global collection language model (Zhai and Lafferty, 2001b), though recently there has been some work on using clusters for smoothing (Liu and Croft, 2004). Our work is mainly to extend the previous work on document smoothing and improve the accuracy of estimation by better exploiting the local corpus structure. We now discuss all these in detail. 2.2 Smoothing of document models Given a document d, the simplest way to estimate the document language model is to treat the document as a sample from the underly</context>
<context position="8376" citStr="Zhai and Lafferty, 2001" startWordPosition="1351" endWordPosition="1354">e models for retrieval, such an estimate is problematic and inaccurate; indeed, it would assign zero probability to any word not present in document d, causing problems in scoring a document with query likelihood or KLdivergence (Zhai and Lafferty, 2001b). Intuitively, such an estimate is inaccurate because the document is a small sample. To solve this problem, many different smoothing techniques have been proposed and studied, usually involving some kind of interpolation of the maximum likelihood estimate and a global collection language model (Hiemstra and Kraaij, 1998; Miller et al., 1999; Zhai and Lafferty, 2001b). For example, Jelinek-Mercer(JM) and Dirichlet are two commonly used smoothing methods (Zhai and Lafferty, 2001b). JM smoothing uses a fixed parameter λ to control the interpolation: P(wj�Od) = λc(w, d) jdj + (1 − λ)P(wjOC), while the Dirichlet smoothing uses a documentdependent coefficient (parameterized with µ) to control the interpolation: Od) = c(w, d) + µP(wjOC) . jdj + µ Here P(wjOC) is the probability of word w given by the collection language model OC, which is usually estimated using the whole collection of documents C, e.g., P(wjOC) = EdEC |d |). 2.3 Cluster-based document model (</context>
<context position="23841" citStr="Zhai and Lafferty, 2001" startWordPosition="3958" endWordPosition="3961">ts 16% improvement on the full length corpus. To understand how α affects the performance we plot the sensitivity curves in Figure 4. The curves all look similar, but the optimal points slightly migrate when the average document length becomes shorter. A 100% corpus gets optimal at α = 0.4, but 30% corpus has to use α = 0.2 to obtain its optimum. (All optimal α values are presented in the fourth row of Table 5.) 3.4 Further improvement with pseudo feedback Query expansion has been proved to be an effective way of utilizing corpus information to improve the query representation (Rocchio, 1971; Zhai and Lafferty, 2001a). It is thus interesting to examine whether our model can be combined with query expansion to further improve the retrieval accuracy. We use the model-based feedback proposed in (Zhai and Lafferty, 2001a) and take top 5 returned documents for feedback. There are two parameters in the model-based pseudo feedback process: the noisy paDELM pseudo DELM+pseudo Impr.(%) AP 0.2505 0.2643 0.2726 3.14%* LA 0.2655 0.2769 0.2901 4.77% TREC8 0.2671 0.2716 0.2809 3.42%** DOE 0.1898 0.1918 0.2046 6.67%*** Table 6: Combination with pseudo feedback.*,**,*** indicate that we accept the improvement hypothesis</context>
<context position="26106" citStr="Zhai and Lafferty, 2001" startWordPosition="4323" endWordPosition="4326">udo feedback process, it is not easy to further combine them. We implement its best-performing algorithm, “interpolation” (labeled as inter. ), and show the results in Table 7. Here, we use the same three data sets as used in (Kurland and Lee, 2004). We tune the feedback parameters to optimal in each experiment. The second last column in Table 7 shows the performance of combination of the “interpolation” model with the pseudo feedback and its improvement percentage. The last column is the z-scores of Wilcoxon test. The negative z-scores indicate that none of the improvement is significant. 2 (Zhai and Lafferty, 2001a) uses different notations. We change them because α has already been used in our own model. Table 5: Impact on short documents (in MAP) 0.26 0.24 0.22 0.2 0.18 0.16 0.14 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 alpha average precision 30% 50% 70% 100% 413 4 Conclusions In this paper, we proposed a novel document expansion method to enrich the document sample through exploiting the local corpus structure. Unlike previous cluster-based models, we smooth each document using a probabilistic neighborhood centered around the document itself. Experiment results show that (1) The proposed document expans</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001b. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR’2001, pages 334–342, Sept.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>