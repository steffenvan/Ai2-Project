<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.99873">
Discourse Parsing: Learning FOL Rules based on Rich Verb Semantic
Representations to automatically label Rhetorical Relations
</title>
<author confidence="0.918768">
Rajen Subba
</author>
<affiliation confidence="0.904989">
Computer Science
University of Illinois
</affiliation>
<address confidence="0.653194">
Chicago, IL, USA
</address>
<email confidence="0.998013">
rsubba@cs.uic.edu
</email>
<author confidence="0.991673">
Barbara Di Eugenio
</author>
<affiliation confidence="0.9886825">
Computer Science
University of Illinois
</affiliation>
<address confidence="0.756537">
Chicago, IL, USA
</address>
<email confidence="0.997361">
bdieugen@cs.uic.edu
</email>
<author confidence="0.939317">
Su Nam Kim
</author>
<affiliation confidence="0.9513915">
Department of CSSE
University of Melbourne
</affiliation>
<address confidence="0.496">
Carlton, VIC, Australia
</address>
<email confidence="0.995974">
snkim@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878230769231">
We report on our work to build a dis-
course parser (SemDP) that uses seman-
tic features of sentences. We use an In-
ductive Logic Programming (ILP) System
to exploit rich verb semantics of clauses
to induce rules for discourse parsing. We
demonstrate that ILP can be used to learn
from highly structured natural language
data and that the performance of a dis-
course parsing model that only uses se-
mantic information is comparable to that
of the state of the art syntactic discourse
parsers.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910176470589">
The availability of corpora annotated with syntac-
tic information have facilitated the use of prob-
abilistic models on tasks such as syntactic pars-
ing. Current state of the art syntactic parsers
reach accuracies between 86% and 90%, as mea-
sured by different types of precision and recall
(for more details see (Collins, 2003)). Recent
semantic (Kingsbury and Palmer, 2002) and dis-
course (Carlson et al., 2003) annotation projects
are paving the way for developments in seman-
tic and discourse parsing as well. However unlike
syntactic parsing, significant development in dis-
course parsing remains at large.
Previous work on discourse parsing ((Soricut
and Marcu, 2003) and (Forbes et al., 2001))
have focused on syntactic and lexical features
only. However, discourse relations connect
clauses/sentences, hence, descriptions of events
and states. It makes linguistic sense that the
semantics of the two clauses —generally built
around the semantics of the verbs, composed with
that of their arguments— affects the discourse re-
lation(s) connecting the clauses. This may be
even more evident in our instructional domain,
where relations derived from planning such as
Precondition-Act may relate clauses.
Of course, since semantic information is hard
to come by, it is not surprising that previous work
on discourse parsing did not use it, or only used
shallow word level ontological semantics as spec-
ified in WordNet (Polanyi et al., 2004). But when
rich sentence level semantics is available, it makes
sense to experiment with it for discourse parsing.
A second major difficulty with using such rich
verb semantic information, is that it is rep-
resented using complex data structures. Tradi-
tional Machine Learning methods cannot han-
dle highly structured data such as First Or-
der Logic (FOL), a representation that is suit-
ably used to represent sentence level seman-
tics. Such FOL representations cannot be reduced
to a vector of attribute/value pairs as the rela-
tions/interdependencies that exist among the pred-
icates would be lost.
Inductive Logic Programming (ILP) can learn
structured descriptions since it learns FOL de-
scriptions. In this paper, we present our first steps
using ILP to learn semantic descriptions of dis-
course relations. Also of relevance to the topic of
this workshop, is that discourse structure is inher-
ently highly structured, since discourse structure
is generally described in hierarchical terms: ba-
sic units of analysis, generally clauses, are related
by discourse relations, resulting in more complex
units, which in turn can be related via discourse re-
lations. At the moment, we do not yet address the
problem of parsing at higher levels of discourse.
We intend to build on the work we present in this
paper to achieve that goal.
The task of discourse parsing can be di-
vided into two disjoint sub-problems ((Soricut and
Marcu, 2003) and (Polanyi et al., 2004)). The two
sub-problems are automatic identification of seg-
ment boundaries and the labeling of rhetorical re-
lations. Though we consider the problem of auto-
matic segmentation to be an important part in dis-
course parsing, we have focused entirely on the
latter problem of automatically labeling rhetorical
</bodyText>
<page confidence="0.999357">
33
</page>
<figureCaption confidence="0.999705">
Figure 1: SemDP System Architecture (Discourse Parser)
</figureCaption>
<bodyText confidence="0.9874947">
relations only. Our approach uses rich verb seman-
tics1 of elementary discourse units (EDUs)2 based
on VerbNet(Kipper et al., 2000) as background
knowledge and manually annotated rhetorical re-
lations as training examples. It is trained on a lot
fewer examples than the state of the art syntax-
based discourse parser (Soricut and Marcu, 2003).
Nevertheless, it achieves a comparable level of
performance with an F-Score of 60.24. Figure 1
shows a block diagram of SemDP’s system archi-
tecture. Segmentation, annotation of rhetorical re-
lations and parsing constitute the data collection
phase of the system. Learning is accomplished
using an ILP based system, Progol (Muggleton,
1995). As can be seen in Figure 1, Progol takes
as input both rich verb semantic information of
pairs of EDUs and the rhetorical relations between
them. The goal was to learn rules using the se-
mantic information from pairs of EDUs as in Ex-
ample 1:
</bodyText>
<listItem confidence="0.58025">
(1) EDU1: ”Sometimes, you can add a liquid to the water
EDU2: ”to hasten the process”
relation(EDU1,EDU2,”Act:goal”).
</listItem>
<bodyText confidence="0.9898256">
to automatically label unseen examples with the
correct rhetorical relation.
The rest of the paper is organized as follows.
Section 2 describes our data collection methodol-
ogy. In section 3, Progol, the ILP system that we
</bodyText>
<footnote confidence="0.73933675">
1The semantic information we used is composed of Verb-
Net semantic predicates that capture event semantics as well
as thematic roles.
2EDUs are minimal discourse units produced as a result
</footnote>
<subsectionHeader confidence="0.49397">
of discourse segmentation.
</subsectionHeader>
<bodyText confidence="0.989695">
used to induce rules for discourse parsing is de-
tailed. Evaluation results are presented in section
4 followed by the conclusion in section 5.
</bodyText>
<sectionHeader confidence="0.974765" genericHeader="method">
2 Data Collection
</sectionHeader>
<bodyText confidence="0.999471631578947">
The lack of corpora annotated with both rhetorical
relations as well as sentence level semantic rep-
resentation led us to create our own corpus. Re-
sources such as (Kingsbury and Palmer, 2002) and
(Carlson et al., 2003) have been developed man-
ually. Since such efforts are time consuming and
costly, we decided to semi-automatically build our
annotated corpus. We used an existing corpus of
instructional text that is about 9MB in size and is
made up entirely of written English instructions.
The two largest components are home repair man-
uals (5Mb) and cooking recipes (1.7Mb). 3
Segmentation. The segmentation of the corpus
was done manually by a human coder. Our seg-
mentation rules are based on those defined in
(Mann and Thompson, 1988). For example, (as
shown in Example 2) we segment sentences in
which a conjunction is used with a clause at the
conjunction site.
</bodyText>
<listItem confidence="0.967032">
(2) You can copy files (//) as well as cut messages.
(//) is the segmentation marker. Sentences are
segmented into EDUs. Not all the segmentation
</listItem>
<footnote confidence="0.997079666666667">
3It was collected opportunistically off the internet and
from other sources, and originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
</footnote>
<page confidence="0.999367">
34
</page>
<bodyText confidence="0.9984245">
rules from (Mann and Thompson, 1988) are im-
ported into our coding scheme. For example, we
do not segment relative clauses. In total, our seg-
mentation resulted in 10,084 EDUs. The seg-
mented EDUs were then annotated with rhetorical
relations by the human coder4 and also forwarded
to the parser as they had to be annotated with se-
mantic information.
</bodyText>
<subsectionHeader confidence="0.999897">
2.1 Parsing of Verb Semantics
</subsectionHeader>
<bodyText confidence="0.999614025641026">
We integrated LCFLEX (Ros´e and Lavie, 2000),
a robust left-corner parser, with VerbNet (Kipper
et al., 2000) and CoreLex (Buitelaar, 1998). Our
interest in decompositional theories of lexical se-
mantics led us to base our semantic representation
on VerbNet.
VerbNet operationalizes Levin’s work and ac-
counts for 4962 distinct verbs classified into 237
main classes. Moreover, VerbNet’s strong syntac-
tic components allow it to be easily coupled with a
parser in order to automatically generate a seman-
tically annotated corpus.
To provide semantics for nouns, we use
CoreLex (Buitelaar, 1998), in turn based on the
generative lexicon(Pustejovsky, 1991). CoreLex
defines basic types such as art (artifact) or com
(communication). Nouns that share the same bun-
dle of basic types are grouped in the same System-
atic Polysemous Class (SPC). The resulting 126
SPCs cover about 40,000 nouns.
We modified and augmented LCFLEX’s exist-
ing lexicon to incorporate VerbNet and CoreLex.
The lexicon is based on COMLEX (Grishman et
al., 1994). Verb and noun entries in the lexicon
contain a link to a semantic type defined in the on-
tology. VerbNet classes (including subclasses and
frames) and CoreLex SPCs are realized as types in
the ontology. The deep syntactic roles are mapped
to the thematic roles, which are defined as vari-
ables in the ontology types. For more details on
the parser see (Terenzi and Di Eugenio, 2003).
Each of the 10,084 EDUs was parsed using the
parser. The parser generates both a syntactic tree
and the associated semantic representation – for
the purpose of this paper, we only focus on the
latter. Figure 2 shows the semantic representation
generated for EDU1 from Example 1, ”sometimes,
you can add a liquid to the water”.
The semantic representation in Figure 2 is part
</bodyText>
<footnote confidence="0.426013">
4Double annotation and segmentation is currently being
done to assess inter-annotator agreement using kappa.
</footnote>
<figure confidence="0.993707153846154">
(*SEM*
((AGENT YOU)
(VERBCLASS ((VNCLASS MIX-22.1-2))) (EVENT +)
(EVENT0
((END
((ARG1 (LIQUID))
(FRAME *TOGETHER) (ARG0 PHYSICAL)
(ARG2 (WATER)))))))
(EVENTSEM
((FRAME *CAUSE) (ARG1 E) (ARG0 (YOU)))))
(PATIENT1 LIQUID)
(PATIENT2 WATER)
(ROOT-VERB ADD))
</figure>
<figureCaption confidence="0.999703">
Figure 2: Parser Output (Semantic Information)
</figureCaption>
<bodyText confidence="0.999971923076923">
of the F-Structure produced by the parser. The
verb add is parsed for a transitive frame with a PP
modifier that belongs to the verb class ’MIX-22.1-
2’. The sentence contains two PATIENTs, namely
liquid and water. you is identified as the AGENT
by the parser. *TOGETHER and *CAUSE are the
primitive semantic predicates used by VerbNet.
Verb Semantics in VerbNet are defined as events
that are decomposed into stages, namely start, end,
during and result. The semantic representation in
Figure 2 states that there is an event EVENT0 in
which the two PATIENTs are together at the end.
An independent evaluation on a set of 200 sen-
tences from our instructional corpus was con-
ducted. 5 It was able to generate complete parses
for 72.2% and partial parses for 10.9% of the verb
frames that we expected it to parse, given the re-
sources. The parser cannot parse those sentences
(or EDUs) that contain a verb that is not cov-
ered by VerbNet. This coverage issue, coupled
with parser errors, exacerbates the problem of data
sparseness. This is further worsened by the fact
that we require both the EDUs in a relation set
to be parsed for the Machine Learning part of our
work. Addressing data sparseness is an issue left
for future work.
</bodyText>
<subsectionHeader confidence="0.999944">
2.2 Annotation of Rhetorical Relations
</subsectionHeader>
<bodyText confidence="0.99951175">
The annotation of rhetorical relations was done
manually by a human coder. Our coding scheme
builds on Relational Discourse Analysis (RDA)
(Moser and Moore, 1995), to which we made mi-
</bodyText>
<footnote confidence="0.533707666666667">
5The parser evaluation was not based on EDUs but rather
on unsegmented sentences. A sentence contained one or
more EDUs.
</footnote>
<page confidence="0.997855">
35
</page>
<bodyText confidence="0.998579155555556">
nor modifications; in turn, as far as discourse rela-
tions are concerned, RDA was inspired by Rhetor-
ical Structure Theory (RST) (Mann and Thomp-
son, 1988).
Rhetorical relations were categorized as infor-
mational, elaborational, temporal and others. In-
formational relations describe how contents in
two relata are related in the domain. These re-
lations are further subdivided into two groups;
causality and similarity. The former group con-
sists of relations between an action and other ac-
tions or between actions and their conditions or
effects. Relations like ’act:goal’, ’criterion:act’
fall under this group. The latter group con-
sists of relations between two EDUs according
to some notion of similarity such as ’restate-
ment’ and ’contrast1:contrast2’. Elaborational
relations are interpropositional relations in which
a proposition(s) provides detail relating to some
aspect of another proposition (Mann and Thomp-
son, 1988). Relations like ’general:specific’ and
’circumstance:situation’ belong to this category.
Temporal relations like ’before:after’ capture time
differences between two EDUs. Lastly, the cate-
gory others includes relations not covered by the
previous three categories such as ’joint’ and ’inde-
terminate&apos;. inde-
terminate&apos;.
Based on the modified coding scheme manual,
we segmented and annotated our instructional cor-
pus using the augmented RST tool from (Marcu et
al., 1999). The RST tool was modified to incor-
porate our relation set. Since we were only inter-
ested in rhetorical relations that spanned between
two adjacent EDUs 6, we obtained 3115 sets of
potential relations from the set of all relations that
we could use as training and testing data.
The parser was able to provide complete parses
for both EDUs in 908 of the 3115 relation sets.
These constitute the training and test set for Pro-
gol.
The semantic representation for the EDUs along
with the manually annotated rhetorical relations
were further processed (as shown in Figure 4) and
used by Progol as input.
</bodyText>
<sectionHeader confidence="0.984284" genericHeader="method">
3 The Inductive Logic Programming
Framework
</sectionHeader>
<bodyText confidence="0.943530777777778">
We chose to use Progol, an Inductive Logic Pro-
gramming system (ILP), to learn rules based on
6At the moment, we are concerned with learning relations
between two EDUs at the base level of a Discourse Parse Tree
(DPT) and not at higher levels of the hierarchy.
the data we collected. ILP is an area of research
at the intersection of Machine Learning (ML) and
Logic Programming. The general problem speci-
fication in ILP is given by the following property:
</bodyText>
<equation confidence="0.984115">
B ∧ H |= E (3)
</equation>
<bodyText confidence="0.999493857142857">
Given the background knowledge B and the ex-
amples E, ILP systems find the simplest consistent
hypothesis H, such that B and H entails E.
While most of the work in NLP that involves
learning has used more traditional ML paradigms
like decision-tree algorithms and SVMs, we did
not find them suitable for our data which is rep-
resented as Horn clauses. The requirement of us-
ing a ML system that could handle first order logic
data led us to explore ILP based systems of which
we found Progol most appropriate.
Progol combines Inverse Entailment with
general-to-specific search through a refinement
graph. A most specific clause is derived using
mode declarations along with Inverse Entailment.
All clauses that subsume the most specific clause
form the hypothesis space. An A*-like search
is used to search for the most probable theory
through the hypothesis space. Progol allows arbi-
trary programs as background knowledge and ar-
bitrary definite clauses as examples.
</bodyText>
<subsectionHeader confidence="0.999264">
3.1 Learning from positive data only
</subsectionHeader>
<bodyText confidence="0.99993952173913">
One of the features we found appealing about Pro-
gol, besides being able to handle first order logic
data, is that it can learn from positive examples
alone.
Learning in natural language is a universal hu-
man process based on positive data only. How-
ever, the usual traditional learning models do not
work well without negative examples. On the
other hand, negative examples are not easy to ob-
tain. Moreover, we found learning from positive
data only to be a natural way to model the task of
discourse parsing.
To make the learning from positive data only
feasible, Progol uses a Bayesian framework. Pro-
gol learns logic programs with an arbitrarily low
expected error using only positive data. Of course,
we could have synthetically labeled examples of
relation sets (pairs of EDUs), that did not belong
to a particular relation, as negative examples. We
plan to explore this approach in the future.
A key issue in learning from positive data
only using a Bayesian framework is the ability
to learn complex logic programs. Without any
</bodyText>
<page confidence="0.991644">
36
</page>
<bodyText confidence="0.999967111111111">
negative examples, the simplest rule or logic
program, which in our case would be a single
definite clause, would be assigned the highest
score as it captures the most number of examples.
In order to handle this problem, Progol’s scoring
function exercises a trade-off between the size of
the function and the generality of the hypothesis.
The score for a given hypothesis is calculated
according to formula 4.
</bodyText>
<equation confidence="0.9976225">
ln p(H  |E) = m lnGH)
1 I −sz(H)+dm (4)
</equation>
<bodyText confidence="0.9968445">
sz(H) and g(H) computes the size of the hy-
pothesis and the its generality respectively. The
size of a hypothesis is measured as the number
of atoms in the hypothesis whereas generality is
measured by the number of positive examples the
hypothesis covers. m is the number of examples
covered by the hypothesis and dm is a normaliz-
ing constant. The function ln p(H|E) decreases
with increases in sz(H) and g(H). As the number
of examples covered (m) grow, the requirements
on g(H) become even stricter. This property fa-
cilitates the ability to learn more complex rules
as they are supported by more positive examples.
For more information on Progol and the computa-
tion of Bayes’ posterior estimation, please refer to
(Muggleton, 1995).
</bodyText>
<subsectionHeader confidence="0.993479">
3.2 Discourse Parsing with Progol
</subsectionHeader>
<bodyText confidence="0.999915235294117">
We model the problem of assigning the correct
rhetorical relation as a classification task within
the ILP framework. The rich verb semantic repre-
sentation of pairs of EDUs, as shown in Figure 3 7,
form the background knowledge and the manually
annotated rhetorical relations between the pairs of
EDUs, as shown in Figure 4, serve as the positive
examples in our learning framework. The num-
bers in the definite clauses are ids used to identify
the EDUs.
Progol constructs logic programs based on the
background knowledge and the examples in Fig-
ures 3 and 4. Mode declarations in the Progol in-
put file determines which clause to be used as the
head (i.e. modeh) and which ones to be used in
the body (i.e. modeb) of the hypotheses. Figure 5
shows an abridged set of our mode declarations.
</bodyText>
<footnote confidence="0.950006">
7The output from the parser was further processed into
definite clauses.
</footnote>
<figure confidence="0.9987001">
...
agent(97,you).
together(97,event0,end,physical,liquid,water).
cause(97,you,e).
patient1(97,liquid).
patient2(97,water).
theme(98,process).
rushed(98,event0,during,process).
cause(98,AGENT98,e).
...
</figure>
<figureCaption confidence="0.982705">
Figure 3: Background Knowledge for Example 1
</figureCaption>
<figure confidence="0.976910777777778">
...
relation(18,19,’Act:goal’).
relation(97,98,’Act:goal’).
relation(1279,1280,’Step1:step2’).
relation(1300,1301,’Step1:step2’).
relation(1310,1311,’Step1:step2’).
relation(412,413,’Before:after’).
relation(441,442,’Before:after’).
...
</figure>
<figureCaption confidence="0.999822">
Figure 4: Positive Examples
</figureCaption>
<bodyText confidence="0.863441608695652">
Our mode declarations dictate that the predicate
relation be used as the head and the other pred-
icates (has possession, transfer and visible) form
the body of the hypotheses. ’*’ indicates that the
number of hypotheses to learn for a given relation
is unlimited. ’+’ and ’-’ signs indicate variables
within the predicates of which the former is an in-
put variable and the latter an output variable. ’#’
is used to denote a constant. Each argument of the
predicate is a type, whether a constant or a vari-
able. Types are defined as a single definite clause.
Our goal is to learn rules where the LHS of the
rule contains the relation that we wish to learn and
:- modeh(*,relation(+edu,+edu,#relationtype))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,+verbarg))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,-verbarg))?
:- modeb(*,transfer(+edu,#event,#eventstage,-verbarg))?
:- modeb(*,visible(+edu,#event,#eventstage,+verbarg))?
:- modeb(*,together(+edu,#event,
#eventstage,+verbarg,+verbarg,+verbarg))?
:- modeb(*,rushed(+edu,#event,#eventstage,+verbarg))?
</bodyText>
<figureCaption confidence="0.995357">
Figure 5: Mode Declarations
</figureCaption>
<page confidence="0.952824">
37
</page>
<figure confidence="0.99795315">
RULE1:
relation(EDU1,EDU2,’Act:goal’) :-
degradation material integrity(EDU1,event0,result,C),
allow(EDU2,event0,during,C,D).
RULE2:
relation(EDU1,EDU2,’Act:goal’) :-
cause(EDU1,C,D),
together(EDU1,event0,end,E,F,G),
cause(EDU2,C,D).
RULE3:
relation(EDU1,EDU2,’Step1:step2’) :-
together(EDU2,event0,end,C,D,E),
has possession(EDU1,event0,during,C,F).
RULE4:
relation(EDU1,EDU2,’Before:after’) :-
motion(EDU1,event0,during,C),
location(EDU2,event0,start,C,D).
RULE6:
relation(EDU1,EDU2,’Act:goal’) :-
motion(EDU1,event0,during,C).
</figure>
<figureCaption confidence="0.999875">
Figure 6: Rules Learned
</figureCaption>
<bodyText confidence="0.999966681818182">
the RHS is a CNF of the semantic predicates de-
fined in VerbNet with their arguments. Given the
amount of training data we have, the nature of the
data itself and the Bayesian framework used, Pro-
gol learns simple rules that contain just one or two
clauses on the RHS. 6 of the 68 rules that Progol
manages to learn are shown in Figure 6. RULE4
states that there is a theme in motion during the
event in EDU A (which is the first EDU) and that
the theme is located in location D at the start of
the event in EDU B (the second EDU). RULE2 is
learned from pairs of EDUs such as in Example
1. The simple rules in Figure 6 may not readily
appeal to our intuitive notion of what such rules
should include. It is not clear at this point as to
how elaborate these rules should be, in order to
correctly identify the relation in question. One
of the reasons why more complex rules are not
learned by Progol is that there aren’t enough train-
ing examples. As we add more training data in the
future, we will see if rules that are more elaborate
than the ones in Figure 6 are learned .
</bodyText>
<sectionHeader confidence="0.618352" genericHeader="evaluation">
4 Evaluation of the Discourse Parser
</sectionHeader>
<bodyText confidence="0.566976333333333">
Table 1 shows the sets of relations for which we
managed to obtain semantic representations (i.e.
for both the EDUs).
</bodyText>
<table confidence="0.988013034482759">
Relations like Preparation:act did not yield any
Relation Total Train Test
Set Set
Step1:step2: 232 188 44
Joint: 190
Goal:act: 170 147 23
General:specific: 77
Criterion:act: 53 46 7
Before:after: 53 42 11
Act:side-effect: 38
Co-temp1:co-temp2: 22
Cause:effect: 19
Prescribe-act:wrong-act: 14
Obstacle:situation: 11
Reason:act: 9
Restatement: 6
Contrast1:contrast2: 6
Circumstance:situation: 3
Act:constraint: 2
Criterion:wrong-act: 2
Set:member: 1
Act:justification: 0
Comparison: 0
Preparation:act: 0
Object:attribute: 0
Part:whole: 0
Same-unit: 0
Indeterminate: 0
908 423 85
</table>
<tableCaption confidence="0.8083575">
Table 1: Relation Set Count (Total Counts include ex-
amples that yielded semantic representations for both EDUs)
</tableCaption>
<bodyText confidence="0.99916745">
examples that could potentially be used. For a
number of relations, the total number of examples
we could use were less than 50. For the time being,
we decided to use only those relation sets that had
more than 50 examples. In addition, we chose not
to use Joint and General:specific relations. They
will be included in the future. Hence, our training
and testing data consisted of the following four re-
lations: Goal:act, Step1:step2, Criterion:act and
Before:after. The total number of examples we
used was 508 of which 423 were used for training
and 85 were used for testing.
Table 2, Table 3 and Table 4 show the results
from running the system on our test data. A total
of 85 positive examples were used for testing the
system.
Table 2 evaluates our SemDP system against a
baseline. Our baseline is the majority function,
which performs at a 51.7 F-Score. SemDP outper-
forms the baseline by almost 10 percentage points
</bodyText>
<page confidence="0.996774">
38
</page>
<table confidence="0.99987325">
Discourse Precision Recall F-Score
Parser
SemDP 61.7 58.8 60.24
Baseline* 51.7 51.7 51.7
</table>
<tableCaption confidence="0.989347">
Table 2: Evaluation vs Baseline (* our baseline is
</tableCaption>
<table confidence="0.942172428571428">
the majority function)
Relation Precision Recall F-Score
Goal:act 31.57 26.08 28.57
Step1:step2 75 75 75
Before:after 54.5 54.5 54.5
Criterion:act 71.4 71.4 71.4
Total 61.7 58.8 60.24
</table>
<tableCaption confidence="0.999789">
Table 3: Test Results for SemDP
</tableCaption>
<bodyText confidence="0.999971765957447">
with an F-Score of 60.24. To the best of our
knowledge, we are also not aware of any work that
uses rich semantic information for discourse pars-
ing. (Polanyi et al., 2004) do not provide any eval-
uation results at all. (Soricut and Marcu, 2003) re-
port that their SynDP parser achieved up to 63.8 F-
Score on human-segmented test data. Our result of
60.24 F-Score shows that a Discourse Parser based
purely on semantics can perform as well. How-
ever, since the corpus, the size of training data and
the set of rhetorical relations we have used differ
from (Soricut and Marcu, 2003), a direct compar-
ison cannot be made.
Table 3 breaks down the results in detail for
each of the four rhetorical relations we tested on.
Since we are learning from positive data only and
the rules we learn depend heavily on the amount
of training data we have, we expected the system
to be more accurate with the relations that have
more training examples. As expected, SemDP did
very well in labeling Step1:step2 relations. Sur-
prisingly though, it did not perform as well with
Goal:act, even though it had the second highest
number of training examples (147 in total). In fact,
SemDP misclassified more positive test examples
for Goal:act than Before:after or Criterion:act, re-
lations which had almost one third the number of
training examples. Overall SemDP achieved a pre-
cision of 61.7 and a Recall of 58.8.
In order to find out how the positive test exam-
ples were misclassified, we investigated the dis-
tribution of the relations classified by SemDP. Ta-
ble 4 is the confusion matrix that highlights this
issue. A majority of the actual Goal:act relations
are incorrectly classified as Step1:step1 and Be-
fore:after. Likewise, most of the misclassification
of actual Step1:step1 seems to labeled as Goal:act
or Before:after. Such misclassification occurs be-
cause the simple rules learned by SemDP are not
able to accurately distinguish cases where positive
examples of two different relations share similar
semantic predicates. Moreover, since we are learn-
ing using positive examples only, it is possible that
a positive example may satisfy two or more rules
for different relations. In such cases, the rule that
has the highest score (as calculated by formula 4)
is used to label the unseen example.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.98494162962963">
We have shown that it is possible to learn First Or-
der Logic rules from complex semantic data us-
ing an ILP based methodology. These rules can
be used to automatically label rhetorical relations.
Moreover, our results show that a Discourse Parser
that uses only semantic information can perform
as well as the state of the art Discourse Parsers
based on syntactic and lexical information.
Future work will involve the use of syntactic in-
formation as well. We also plan to run a more thor-
ough evaluation on the complete set of relations
that we have used in our coding scheme. It is also
important that the manual segmentation and an-
notation of rhetorical relations be subject to inter-
annotator agreement. A second human annotator
is currently annotating a sample of the annotated
corpus. Upon completion, the annotated corpus
will be checked for reliability.
Data sparseness is a well known problem in Ma-
chine Learning. Like most paradigms, our learn-
ing model is also affected by it. We also plan to
explore techniques to deal with this issue.
Relation Goal:act Step1:step2 Before:after Criterion:act
Goal:act 6 8 5 0
Step1:step2 6 33 5 0
Before:after 0 4 6 1
Criterion:act 0 0 2 5
</bodyText>
<tableCaption confidence="0.995719">
Table 4: Confusion Matrix for SemDP Test Result
</tableCaption>
<page confidence="0.99898">
39
</page>
<bodyText confidence="0.999981666666667">
Lastly, we have not tackled the problem of dis-
course parsing at higher levels of the DPT and seg-
mentation in this paper. Our ultimate goal is to
build a Discourse Parser that will automatically
segment a full text as well as annotate it with
rhetorical relations at every level of the DPT using
semantic as well as syntactic information. Much
work needs to be done but we are excited to see
what the aforesaid future work will yield.
</bodyText>
<sectionHeader confidence="0.998207" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.94522675">
This work is supported by award 0133123 from the National
Science Foundation. Thanks to C.P. Ros´e for LCFLEX, M.
Palmer and K. Kipper for VerbNet, C. Buitelaar for CoreLex,
and Stephen Muggleton for Progol.
</bodyText>
<sectionHeader confidence="0.995574" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946548780488">
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy
and Underspecification. Ph.D. thesis, Computer Science,
Brandeis University, February.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. In Current Direc-
tions in Discourse and Dialogue, pp. 85-112, Jan van Kup-
pevelt and Ronnie Smith eds., Kluwer Academic Publish-
ers.
Michael Collins. 2003. Head-driven statistical methods for
natural language parsing. Computational Linguistics, 29.
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi and Bonnie Webber. 2001. D-
LTAG System - Discourse Parsing with a Lexicalized Tree
Adjoining Grammar. Information Stucture, Discourse
Structure and Discourse Semantics, ESSLLI, 2001.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational lex-
icon. In COLING 94, Proceedings of the 15th Interna-
tional Conference on Computational Linguistics, pages
472–477, Kyoto, Japan, August.
Paul Kingsbury and Martha Palmer. 2000. From Treebank
to Propbank. In Third International Conference on Lan-
guage Resources and Evaluation, LREC-02, Las Palmas,
Canary Islands, Spain, May 28 - June 3, 2002.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In AAAI-2000,
Proceedings of the Seventeenth National Conference on
Artificial Intelligence, Austin, TX.
Beth Levin and Malka Rappaport Hovav. 1992. Wiping the
slate clean: a lexical semantic exploration. In Beth Levin
and Steven Pinker, editors, Lexical and Conceptual Se-
mantics, Special Issue of Cognition: International Journal
of Cognitive Science. Blackwell Publishers.
William C. Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory: toward a Functional Theory of Text Or-
ganization. Text, 8(3):243–281.
Daniel Marcu and Abdessamad Echihabi. 2002. An unsuper-
vised approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), Philadelphia, PA,
July.
Daniel Marcu, Magdalena Romera and Estibaliz Amorrortu.
1999. Experiments in Constructing a Corpus of Discourse
Trees: Problems, Annotation Choices, Issues. In The
Workshop on Levels of Representation in Discourse, pages
71-78, Edinburgh, Scotland, July.
M. G. Moser, and J. D. Moore. 1995. Using Discourse
Analysis and Automatic Text Generation to Study Dis-
course Cue Usage. In AAAI Spring Symposium on Empir-
ical Methods in Discourse Interpretation and Generation,
1995.
Stephen H. Muggleton. 1995. Inverse Entailment and Pro-
gol. In New Generation Computing Journal, Vol. 13, pp.
245-286, 1995.
Martha Palmer, Daniel Gildea and, Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71–105.
Livia Polanyi, Christopher Culy, Martin H. van den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. Senten-
tial Structure and Discourse Parsing. Proceedings of the
ACL2004 Workshop on Discourse Annotation, Barcelona,
Spain, July 25, 2004.
James Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4):409–441.
Carolyn Penstein Ros´e and Alon Lavie. 2000. Balancing ro-
bustness and efficiency in unification-augmented context-
free parsers for large practical applications. In Jean-
Clause Junqua and Gertjan van Noord, editors, Robustness
in Language and Speech Technology. Kluwer Academic
Press.
Radu Soricut and Daniel Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Information.
In Proceedings of the Human Language Technology and
North American Assiciation for Computational Linguis-
tics Conference (HLT/NAACL-2003), Edmonton, Canada,
May-June.
Elena Terenzi and Barbara Di Eugenio. 2003. Building lex-
ical semantic representations for natural language instruc-
tions. In HLT-NAACL03, 2003 Human Language Tech-
nology Conference, pages 100–102, Edmonton, Canada,
May. (Short Paper).
</reference>
<page confidence="0.998638">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.781105">
<title confidence="0.949798">Discourse Parsing: Learning FOL Rules based on Rich Verb Representations to automatically label Rhetorical Relations</title>
<author confidence="0.950593">Rajen</author>
<affiliation confidence="0.986839">Computer University of</affiliation>
<address confidence="0.962649">Chicago, IL, USA</address>
<email confidence="0.998285">rsubba@cs.uic.edu</email>
<author confidence="0.998815">Barbara Di_Eugenio</author>
<affiliation confidence="0.996976">Computer University of Illinois</affiliation>
<address confidence="0.992023">Chicago, IL, USA</address>
<email confidence="0.999236">bdieugen@cs.uic.edu</email>
<author confidence="0.995718">Su Nam</author>
<affiliation confidence="0.999636">Department of University of</affiliation>
<address confidence="0.998913">Carlton, VIC, Australia</address>
<email confidence="0.998705">snkim@csse.unimelb.edu.au</email>
<abstract confidence="0.993695285714286">We report on our work to build a discourse parser (SemDP) that uses semantic features of sentences. We use an Inductive Logic Programming (ILP) System to exploit rich verb semantics of clauses to induce rules for discourse parsing. We demonstrate that ILP can be used to learn from highly structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Buitelaar</author>
</authors>
<title>CoreLex: Systematic Polysemy and Underspecification.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science, Brandeis University,</institution>
<contexts>
<context position="7517" citStr="Buitelaar, 1998" startWordPosition="1198" endWordPosition="1199"> originally assembled at the Information Technology Research Institute, University of Brighton. 34 rules from (Mann and Thompson, 1988) are imported into our coding scheme. For example, we do not segment relative clauses. In total, our segmentation resulted in 10,084 EDUs. The segmented EDUs were then annotated with rhetorical relations by the human coder4 and also forwarded to the parser as they had to be annotated with semantic information. 2.1 Parsing of Verb Semantics We integrated LCFLEX (Ros´e and Lavie, 2000), a robust left-corner parser, with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998). Our interest in decompositional theories of lexical semantics led us to base our semantic representation on VerbNet. VerbNet operationalizes Levin’s work and accounts for 4962 distinct verbs classified into 237 main classes. Moreover, VerbNet’s strong syntactic components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that s</context>
</contexts>
<marker>Buitelaar, 1998</marker>
<rawString>Paul Buitelaar. 1998. CoreLex: Systematic Polysemy and Underspecification. Ph.D. thesis, Computer Science, Brandeis University, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory.</title>
<date>2003</date>
<booktitle>In Current Directions in Discourse and Dialogue,</booktitle>
<pages>85--112</pages>
<editor>van Kuppevelt and Ronnie Smith eds.,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1331" citStr="Carlson et al., 2003" startWordPosition="202" endWordPosition="205"> structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers. 1 Introduction The availability of corpora annotated with syntactic information have facilitated the use of probabilistic models on tasks such as syntactic parsing. Current state of the art syntactic parsers reach accuracies between 86% and 90%, as measured by different types of precision and recall (for more details see (Collins, 2003)). Recent semantic (Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotation projects are paving the way for developments in semantic and discourse parsing as well. However unlike syntactic parsing, significant development in discourse parsing remains at large. Previous work on discourse parsing ((Soricut and Marcu, 2003) and (Forbes et al., 2001)) have focused on syntactic and lexical features only. However, discourse relations connect clauses/sentences, hence, descriptions of events and states. It makes linguistic sense that the semantics of the two clauses —generally built around the semantics of the verbs, composed with that of their arguments— affects </context>
<context position="6024" citStr="Carlson et al., 2003" startWordPosition="951" endWordPosition="954">gol, the ILP system that we 1The semantic information we used is composed of VerbNet semantic predicates that capture event semantics as well as thematic roles. 2EDUs are minimal discourse units produced as a result of discourse segmentation. used to induce rules for discourse parsing is detailed. Evaluation results are presented in section 4 followed by the conclusion in section 5. 2 Data Collection The lack of corpora annotated with both rhetorical relations as well as sentence level semantic representation led us to create our own corpus. Resources such as (Kingsbury and Palmer, 2002) and (Carlson et al., 2003) have been developed manually. Since such efforts are time consuming and costly, we decided to semi-automatically build our annotated corpus. We used an existing corpus of instructional text that is about 9MB in size and is made up entirely of written English instructions. The two largest components are home repair manuals (5Mb) and cooking recipes (1.7Mb). 3 Segmentation. The segmentation of the corpus was done manually by a human coder. Our segmentation rules are based on those defined in (Mann and Thompson, 1988). For example, (as shown in Example 2) we segment sentences in which a conjunct</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2003. Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. In Current Directions in Discourse and Dialogue, pp. 85-112, Jan van Kuppevelt and Ronnie Smith eds., Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical methods for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context position="1247" citStr="Collins, 2003" startWordPosition="191" endWordPosition="192">r discourse parsing. We demonstrate that ILP can be used to learn from highly structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers. 1 Introduction The availability of corpora annotated with syntactic information have facilitated the use of probabilistic models on tasks such as syntactic parsing. Current state of the art syntactic parsers reach accuracies between 86% and 90%, as measured by different types of precision and recall (for more details see (Collins, 2003)). Recent semantic (Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotation projects are paving the way for developments in semantic and discourse parsing as well. However unlike syntactic parsing, significant development in discourse parsing remains at large. Previous work on discourse parsing ((Soricut and Marcu, 2003) and (Forbes et al., 2001)) have focused on syntactic and lexical features only. However, discourse relations connect clauses/sentences, hence, descriptions of events and states. It makes linguistic sense that the semantics of the two clauses —generally buil</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical methods for natural language parsing. Computational Linguistics, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Forbes</author>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Anoop Sarkar</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<date>2001</date>
<booktitle>DLTAG System - Discourse Parsing with a Lexicalized Tree Adjoining Grammar. Information Stucture, Discourse Structure and Discourse Semantics, ESSLLI,</booktitle>
<contexts>
<context position="1615" citStr="Forbes et al., 2001" startWordPosition="245" endWordPosition="248">acilitated the use of probabilistic models on tasks such as syntactic parsing. Current state of the art syntactic parsers reach accuracies between 86% and 90%, as measured by different types of precision and recall (for more details see (Collins, 2003)). Recent semantic (Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotation projects are paving the way for developments in semantic and discourse parsing as well. However unlike syntactic parsing, significant development in discourse parsing remains at large. Previous work on discourse parsing ((Soricut and Marcu, 2003) and (Forbes et al., 2001)) have focused on syntactic and lexical features only. However, discourse relations connect clauses/sentences, hence, descriptions of events and states. It makes linguistic sense that the semantics of the two clauses —generally built around the semantics of the verbs, composed with that of their arguments— affects the discourse relation(s) connecting the clauses. This may be even more evident in our instructional domain, where relations derived from planning such as Precondition-Act may relate clauses. Of course, since semantic information is hard to come by, it is not surprising that previous</context>
</contexts>
<marker>Forbes, Miltsakaki, Prasad, Sarkar, Joshi, Webber, 2001</marker>
<rawString>Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop Sarkar, Aravind Joshi and Bonnie Webber. 2001. DLTAG System - Discourse Parsing with a Lexicalized Tree Adjoining Grammar. Information Stucture, Discourse Structure and Discourse Semantics, ESSLLI, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Catherine Macleod</author>
<author>Adam Meyers</author>
</authors>
<title>COMLEX syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In COLING 94, Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>472--477</pages>
<location>Kyoto, Japan,</location>
<contexts>
<context position="8403" citStr="Grishman et al., 1994" startWordPosition="1335" endWordPosition="1338">components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX’s existing lexicon to incorporate VerbNet and CoreLex. The lexicon is based on COMLEX (Grishman et al., 1994). Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic roles are mapped to the thematic roles, which are defined as variables in the ontology types. For more details on the parser see (Terenzi and Di Eugenio, 2003). Each of the 10,084 EDUs was parsed using the parser. The parser generates both a syntactic tree and the associated semantic representation – for the purpose of this paper, we only focus on the latter. Figure 2 shows </context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Ralph Grishman, Catherine Macleod, and Adam Meyers. 1994. COMLEX syntax: Building a computational lexicon. In COLING 94, Proceedings of the 15th International Conference on Computational Linguistics, pages 472–477, Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to Propbank.</title>
<date>2000</date>
<booktitle>In Third International Conference on Language Resources and Evaluation, LREC-02,</booktitle>
<location>Las Palmas, Canary Islands, Spain,</location>
<marker>Kingsbury, Palmer, 2000</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2000. From Treebank to Propbank. In Third International Conference on Language Resources and Evaluation, LREC-02, Las Palmas, Canary Islands, Spain, May 28 - June 3, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In AAAI-2000, Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="4299" citStr="Kipper et al., 2000" startWordPosition="670" endWordPosition="673">he task of discourse parsing can be divided into two disjoint sub-problems ((Soricut and Marcu, 2003) and (Polanyi et al., 2004)). The two sub-problems are automatic identification of segment boundaries and the labeling of rhetorical relations. Though we consider the problem of automatic segmentation to be an important part in discourse parsing, we have focused entirely on the latter problem of automatically labeling rhetorical 33 Figure 1: SemDP System Architecture (Discourse Parser) relations only. Our approach uses rich verb semantics1 of elementary discourse units (EDUs)2 based on VerbNet(Kipper et al., 2000) as background knowledge and manually annotated rhetorical relations as training examples. It is trained on a lot fewer examples than the state of the art syntaxbased discourse parser (Soricut and Marcu, 2003). Nevertheless, it achieves a comparable level of performance with an F-Score of 60.24. Figure 1 shows a block diagram of SemDP’s system architecture. Segmentation, annotation of rhetorical relations and parsing constitute the data collection phase of the system. Learning is accomplished using an ILP based system, Progol (Muggleton, 1995). As can be seen in Figure 1, Progol takes as input</context>
<context position="7487" citStr="Kipper et al., 2000" startWordPosition="1192" endWordPosition="1195">ternet and from other sources, and originally assembled at the Information Technology Research Institute, University of Brighton. 34 rules from (Mann and Thompson, 1988) are imported into our coding scheme. For example, we do not segment relative clauses. In total, our segmentation resulted in 10,084 EDUs. The segmented EDUs were then annotated with rhetorical relations by the human coder4 and also forwarded to the parser as they had to be annotated with semantic information. 2.1 Parsing of Verb Semantics We integrated LCFLEX (Ros´e and Lavie, 2000), a robust left-corner parser, with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998). Our interest in decompositional theories of lexical semantics led us to base our semantic representation on VerbNet. VerbNet operationalizes Levin’s work and accounts for 4962 distinct verbs classified into 237 main classes. Moreover, VerbNet’s strong syntactic components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In AAAI-2000, Proceedings of the Seventeenth National Conference on Artificial Intelligence, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Malka Rappaport Hovav</author>
</authors>
<title>Wiping the slate clean: a lexical semantic exploration.</title>
<date>1992</date>
<booktitle>In Beth Levin and Steven Pinker, editors, Lexical and Conceptual Semantics, Special Issue of Cognition: International Journal of Cognitive Science.</booktitle>
<publisher>Blackwell Publishers.</publisher>
<marker>Levin, Hovav, 1992</marker>
<rawString>Beth Levin and Malka Rappaport Hovav. 1992. Wiping the slate clean: a lexical semantic exploration. In Beth Levin and Steven Pinker, editors, Lexical and Conceptual Semantics, Special Issue of Cognition: International Journal of Cognitive Science. Blackwell Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical Structure Theory: toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="6545" citStr="Mann and Thompson, 1988" startWordPosition="1037" endWordPosition="1040"> us to create our own corpus. Resources such as (Kingsbury and Palmer, 2002) and (Carlson et al., 2003) have been developed manually. Since such efforts are time consuming and costly, we decided to semi-automatically build our annotated corpus. We used an existing corpus of instructional text that is about 9MB in size and is made up entirely of written English instructions. The two largest components are home repair manuals (5Mb) and cooking recipes (1.7Mb). 3 Segmentation. The segmentation of the corpus was done manually by a human coder. Our segmentation rules are based on those defined in (Mann and Thompson, 1988). For example, (as shown in Example 2) we segment sentences in which a conjunction is used with a clause at the conjunction site. (2) You can copy files (//) as well as cut messages. (//) is the segmentation marker. Sentences are segmented into EDUs. Not all the segmentation 3It was collected opportunistically off the internet and from other sources, and originally assembled at the Information Technology Research Institute, University of Brighton. 34 rules from (Mann and Thompson, 1988) are imported into our coding scheme. For example, we do not segment relative clauses. In total, our segmenta</context>
<context position="11298" citStr="Mann and Thompson, 1988" startWordPosition="1813" endWordPosition="1817">tion set to be parsed for the Machine Learning part of our work. Addressing data sparseness is an issue left for future work. 2.2 Annotation of Rhetorical Relations The annotation of rhetorical relations was done manually by a human coder. Our coding scheme builds on Relational Discourse Analysis (RDA) (Moser and Moore, 1995), to which we made mi5The parser evaluation was not based on EDUs but rather on unsegmented sentences. A sentence contained one or more EDUs. 35 nor modifications; in turn, as far as discourse relations are concerned, RDA was inspired by Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Rhetorical relations were categorized as informational, elaborational, temporal and others. Informational relations describe how contents in two relata are related in the domain. These relations are further subdivided into two groups; causality and similarity. The former group consists of relations between an action and other actions or between actions and their conditions or effects. Relations like ’act:goal’, ’criterion:act’ fall under this group. The latter group consists of relations between two EDUs according to some notion of similarity such as ’restatement’ and ’contrast1:contrast2’. </context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra Thompson. 1988. Rhetorical Structure Theory: toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<location>Philadelphia, PA,</location>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Magdalena Romera</author>
<author>Estibaliz Amorrortu</author>
</authors>
<title>Experiments in Constructing a Corpus of Discourse Trees: Problems, Annotation Choices, Issues.</title>
<date>1999</date>
<booktitle>In The Workshop on Levels of Representation in Discourse,</booktitle>
<pages>71--78</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="12536" citStr="Marcu et al., 1999" startWordPosition="1992" endWordPosition="1995">ions are interpropositional relations in which a proposition(s) provides detail relating to some aspect of another proposition (Mann and Thompson, 1988). Relations like ’general:specific’ and ’circumstance:situation’ belong to this category. Temporal relations like ’before:after’ capture time differences between two EDUs. Lastly, the category others includes relations not covered by the previous three categories such as ’joint’ and ’indeterminate&apos;. indeterminate&apos;. Based on the modified coding scheme manual, we segmented and annotated our instructional corpus using the augmented RST tool from (Marcu et al., 1999). The RST tool was modified to incorporate our relation set. Since we were only interested in rhetorical relations that spanned between two adjacent EDUs 6, we obtained 3115 sets of potential relations from the set of all relations that we could use as training and testing data. The parser was able to provide complete parses for both EDUs in 908 of the 3115 relation sets. These constitute the training and test set for Progol. The semantic representation for the EDUs along with the manually annotated rhetorical relations were further processed (as shown in Figure 4) and used by Progol as input.</context>
</contexts>
<marker>Marcu, Romera, Amorrortu, 1999</marker>
<rawString>Daniel Marcu, Magdalena Romera and Estibaliz Amorrortu. 1999. Experiments in Constructing a Corpus of Discourse Trees: Problems, Annotation Choices, Issues. In The Workshop on Levels of Representation in Discourse, pages 71-78, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Moser</author>
<author>J D Moore</author>
</authors>
<title>Using Discourse Analysis and Automatic Text Generation to Study Discourse Cue Usage.</title>
<date>1995</date>
<booktitle>In AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<contexts>
<context position="11001" citStr="Moser and Moore, 1995" startWordPosition="1762" endWordPosition="1765">e, given the resources. The parser cannot parse those sentences (or EDUs) that contain a verb that is not covered by VerbNet. This coverage issue, coupled with parser errors, exacerbates the problem of data sparseness. This is further worsened by the fact that we require both the EDUs in a relation set to be parsed for the Machine Learning part of our work. Addressing data sparseness is an issue left for future work. 2.2 Annotation of Rhetorical Relations The annotation of rhetorical relations was done manually by a human coder. Our coding scheme builds on Relational Discourse Analysis (RDA) (Moser and Moore, 1995), to which we made mi5The parser evaluation was not based on EDUs but rather on unsegmented sentences. A sentence contained one or more EDUs. 35 nor modifications; in turn, as far as discourse relations are concerned, RDA was inspired by Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Rhetorical relations were categorized as informational, elaborational, temporal and others. Informational relations describe how contents in two relata are related in the domain. These relations are further subdivided into two groups; causality and similarity. The former group consists of relations b</context>
</contexts>
<marker>Moser, Moore, 1995</marker>
<rawString>M. G. Moser, and J. D. Moore. 1995. Using Discourse Analysis and Automatic Text Generation to Study Discourse Cue Usage. In AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen H Muggleton</author>
</authors>
<title>Inverse Entailment and Progol.</title>
<date>1995</date>
<journal>In New Generation Computing Journal,</journal>
<volume>13</volume>
<pages>245--286</pages>
<contexts>
<context position="4848" citStr="Muggleton, 1995" startWordPosition="757" endWordPosition="758">ntary discourse units (EDUs)2 based on VerbNet(Kipper et al., 2000) as background knowledge and manually annotated rhetorical relations as training examples. It is trained on a lot fewer examples than the state of the art syntaxbased discourse parser (Soricut and Marcu, 2003). Nevertheless, it achieves a comparable level of performance with an F-Score of 60.24. Figure 1 shows a block diagram of SemDP’s system architecture. Segmentation, annotation of rhetorical relations and parsing constitute the data collection phase of the system. Learning is accomplished using an ILP based system, Progol (Muggleton, 1995). As can be seen in Figure 1, Progol takes as input both rich verb semantic information of pairs of EDUs and the rhetorical relations between them. The goal was to learn rules using the semantic information from pairs of EDUs as in Example 1: (1) EDU1: ”Sometimes, you can add a liquid to the water EDU2: ”to hasten the process” relation(EDU1,EDU2,”Act:goal”). to automatically label unseen examples with the correct rhetorical relation. The rest of the paper is organized as follows. Section 2 describes our data collection methodology. In section 3, Progol, the ILP system that we 1The semantic inf</context>
<context position="16869" citStr="Muggleton, 1995" startWordPosition="2731" endWordPosition="2732">measured as the number of atoms in the hypothesis whereas generality is measured by the number of positive examples the hypothesis covers. m is the number of examples covered by the hypothesis and dm is a normalizing constant. The function ln p(H|E) decreases with increases in sz(H) and g(H). As the number of examples covered (m) grow, the requirements on g(H) become even stricter. This property facilitates the ability to learn more complex rules as they are supported by more positive examples. For more information on Progol and the computation of Bayes’ posterior estimation, please refer to (Muggleton, 1995). 3.2 Discourse Parsing with Progol We model the problem of assigning the correct rhetorical relation as a classification task within the ILP framework. The rich verb semantic representation of pairs of EDUs, as shown in Figure 3 7, form the background knowledge and the manually annotated rhetorical relations between the pairs of EDUs, as shown in Figure 4, serve as the positive examples in our learning framework. The numbers in the definite clauses are ids used to identify the EDUs. Progol constructs logic programs based on the background knowledge and the examples in Figures 3 and 4. Mode de</context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>Stephen H. Muggleton. 1995. Inverse Entailment and Progol. In New Generation Computing Journal, Vol. 13, pp. 245-286, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea and</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, and, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea and, Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Christopher Culy</author>
<author>Martin H van den Berg</author>
<author>Gian Lorenzo Thione</author>
<author>David Ahn</author>
</authors>
<date>2004</date>
<booktitle>Sentential Structure and Discourse Parsing. Proceedings of the ACL2004 Workshop on Discourse Annotation,</booktitle>
<location>Barcelona, Spain,</location>
<marker>Polanyi, Culy, van den Berg, Thione, Ahn, 2004</marker>
<rawString>Livia Polanyi, Christopher Culy, Martin H. van den Berg, Gian Lorenzo Thione, and David Ahn. 2004. Sentential Structure and Discourse Parsing. Proceedings of the ACL2004 Workshop on Discourse Annotation, Barcelona, Spain, July 25, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="8028" citStr="Pustejovsky, 1991" startWordPosition="1275" endWordPosition="1276">Lavie, 2000), a robust left-corner parser, with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998). Our interest in decompositional theories of lexical semantics led us to base our semantic representation on VerbNet. VerbNet operationalizes Levin’s work and accounts for 4962 distinct verbs classified into 237 main classes. Moreover, VerbNet’s strong syntactic components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX’s existing lexicon to incorporate VerbNet and CoreLex. The lexicon is based on COMLEX (Grishman et al., 1994). Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic ro</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>James Pustejovsky. 1991. The generative lexicon. Computational Linguistics, 17(4):409–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Penstein Ros´e</author>
<author>Alon Lavie</author>
</authors>
<title>Balancing robustness and efficiency in unification-augmented contextfree parsers for large practical applications.</title>
<date>2000</date>
<booktitle>In JeanClause Junqua and Gertjan</booktitle>
<editor>van Noord, editors,</editor>
<publisher>Kluwer Academic Press.</publisher>
<marker>Ros´e, Lavie, 2000</marker>
<rawString>Carolyn Penstein Ros´e and Alon Lavie. 2000. Balancing robustness and efficiency in unification-augmented contextfree parsers for large practical applications. In JeanClause Junqua and Gertjan van Noord, editors, Robustness in Language and Speech Technology. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence Level Discourse Parsing using Syntactic and Lexical Information.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Assiciation for Computational Linguistics Conference (HLT/NAACL-2003),</booktitle>
<location>Edmonton, Canada, May-June.</location>
<contexts>
<context position="1589" citStr="Soricut and Marcu, 2003" startWordPosition="240" endWordPosition="243">h syntactic information have facilitated the use of probabilistic models on tasks such as syntactic parsing. Current state of the art syntactic parsers reach accuracies between 86% and 90%, as measured by different types of precision and recall (for more details see (Collins, 2003)). Recent semantic (Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotation projects are paving the way for developments in semantic and discourse parsing as well. However unlike syntactic parsing, significant development in discourse parsing remains at large. Previous work on discourse parsing ((Soricut and Marcu, 2003) and (Forbes et al., 2001)) have focused on syntactic and lexical features only. However, discourse relations connect clauses/sentences, hence, descriptions of events and states. It makes linguistic sense that the semantics of the two clauses —generally built around the semantics of the verbs, composed with that of their arguments— affects the discourse relation(s) connecting the clauses. This may be even more evident in our instructional domain, where relations derived from planning such as Precondition-Act may relate clauses. Of course, since semantic information is hard to come by, it is no</context>
<context position="3780" citStr="Soricut and Marcu, 2003" startWordPosition="590" endWordPosition="593">ns. Also of relevance to the topic of this workshop, is that discourse structure is inherently highly structured, since discourse structure is generally described in hierarchical terms: basic units of analysis, generally clauses, are related by discourse relations, resulting in more complex units, which in turn can be related via discourse relations. At the moment, we do not yet address the problem of parsing at higher levels of discourse. We intend to build on the work we present in this paper to achieve that goal. The task of discourse parsing can be divided into two disjoint sub-problems ((Soricut and Marcu, 2003) and (Polanyi et al., 2004)). The two sub-problems are automatic identification of segment boundaries and the labeling of rhetorical relations. Though we consider the problem of automatic segmentation to be an important part in discourse parsing, we have focused entirely on the latter problem of automatically labeling rhetorical 33 Figure 1: SemDP System Architecture (Discourse Parser) relations only. Our approach uses rich verb semantics1 of elementary discourse units (EDUs)2 based on VerbNet(Kipper et al., 2000) as background knowledge and manually annotated rhetorical relations as training </context>
<context position="23391" citStr="Soricut and Marcu, 2003" startWordPosition="3676" endWordPosition="3679"> 10 percentage points 38 Discourse Precision Recall F-Score Parser SemDP 61.7 58.8 60.24 Baseline* 51.7 51.7 51.7 Table 2: Evaluation vs Baseline (* our baseline is the majority function) Relation Precision Recall F-Score Goal:act 31.57 26.08 28.57 Step1:step2 75 75 75 Before:after 54.5 54.5 54.5 Criterion:act 71.4 71.4 71.4 Total 61.7 58.8 60.24 Table 3: Test Results for SemDP with an F-Score of 60.24. To the best of our knowledge, we are also not aware of any work that uses rich semantic information for discourse parsing. (Polanyi et al., 2004) do not provide any evaluation results at all. (Soricut and Marcu, 2003) report that their SynDP parser achieved up to 63.8 FScore on human-segmented test data. Our result of 60.24 F-Score shows that a Discourse Parser based purely on semantics can perform as well. However, since the corpus, the size of training data and the set of rhetorical relations we have used differ from (Soricut and Marcu, 2003), a direct comparison cannot be made. Table 3 breaks down the results in detail for each of the four rhetorical relations we tested on. Since we are learning from positive data only and the rules we learn depend heavily on the amount of training data we have, we expe</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence Level Discourse Parsing using Syntactic and Lexical Information. In Proceedings of the Human Language Technology and North American Assiciation for Computational Linguistics Conference (HLT/NAACL-2003), Edmonton, Canada, May-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Terenzi</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Building lexical semantic representations for natural language instructions.</title>
<date>2003</date>
<booktitle>In HLT-NAACL03, 2003 Human Language Technology Conference,</booktitle>
<tech>(Short Paper).</tech>
<pages>100--102</pages>
<location>Edmonton, Canada,</location>
<marker>Terenzi, Di Eugenio, 2003</marker>
<rawString>Elena Terenzi and Barbara Di Eugenio. 2003. Building lexical semantic representations for natural language instructions. In HLT-NAACL03, 2003 Human Language Technology Conference, pages 100–102, Edmonton, Canada, May. (Short Paper).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>