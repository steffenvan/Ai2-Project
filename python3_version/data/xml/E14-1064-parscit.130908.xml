<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.992905">
Bilingual Sentiment Consistency for Statistical Machine Translation
</title>
<author confidence="0.984091">
Boxing Chen and Xiaodan Zhu
</author>
<affiliation confidence="0.991854">
National Research Council Canada
</affiliation>
<address confidence="0.817836">
1200 Montreal Road, Ottawa, Canada, K1A 0R6
</address>
<email confidence="0.677943">
{Boxing.Chen, Xiaodan.Zhu}@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.987161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994395">
In this paper, we explore bilingual sentiment
knowledge for statistical machine translation
(SMT). We propose to explicitly model the
consistency of sentiment between the source
and target side with a lexicon-based approach.
The experiments show that the proposed mod-
el significantly improves Chinese-to-English
NIST translation over a competitive baseline.
</bodyText>
<sectionHeader confidence="0.998095" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867076923077">
The expression of sentiment is an interesting and
integral part of human languages. In written text
sentiment is conveyed by senses and in speech also
via prosody. Sentiment is associated with both
evaluative (positive or negative) and potency (de-
gree of sentiment) ― involving two of the three
major semantic differential categories identified by
Osgood et al. (1957).
Automatically analyzing the sentiment of mono-
lingual text has attracted a large bulk of research,
which includes, but is not limited to, the early ex-
ploration of (Turney, 2002; Pang et al., 2002; Hat-
zivassiloglou &amp; McKeown, 1997). Since then,
research has involved a variety of approaches and
been conducted on various type of data, e.g., prod-
uct reviews, news, blogs, and the more recent so-
cial media text.
As sentiment has been an important concern in
monolingual settings, better translation of such
information between languages could be of interest
to help better cross language barriers, particularly
for sentiment-abundant data. Even when we ran-
domly sampled a subset of sentence pairs from the
NIST Open MT1 training data, we found that about
48.2% pairs contain at least one sentiment word on
both sides, and 22.4% pairs contain at least one
</bodyText>
<footnote confidence="0.488838">
1 http://www.nist.gov/speech/tests/mt
</footnote>
<bodyText confidence="0.599976">
intensifier word on both sides, which suggests a
non-trivial percent of sentences may potentially
involve sentiment in some degree2.
</bodyText>
<table confidence="0.960046333333333">
# snt. % snt. with % snt. with
pairs sentiment words intensifiers
103,369 48.2% 22.4%
</table>
<tableCaption confidence="0.979762">
Table 1. Percentages of sentence pairs that contain sen-
timent words on both sides or intensifiers3 on both sides.
</tableCaption>
<bodyText confidence="0.999659071428572">
One expects that sentiment has been implicitly
captured in SMT through the statistics learned
from parallel corpus, e.g., the phrase tables in a
phrase-based system. In this paper, we are interest-
ed in explicitly modeling sentiment knowledge for
translation. We propose a lexicon-based approach
that examines the consistency of bilingual subjec-
tivity, sentiment polarity, intensity, and negation.
The experiments show that the proposed approach
improves the NIST Chinese-to-English translation
over a strong baseline.
In general, we hope this line of work will help
achieve better MT quality, especially for data with
more abundant sentiment, such as social media text.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.953283466666667">
Sentiment analysis and lexicon-based approach-
es Research on monolingual sentiment analysis can
be found under different names such as opinion,
stance, appraisal, and semantic orientation, among
others. The overall goal is to label a span of text
either as positive, negative, or neutral ― some-
times the strength of sentiment is a concern too.
2 The numbers give a rough idea of sentiment coverage; it
would be more ideal if the estimation could be conducted on
senses instead of words, which, however, requires reliable
sense labeling and is not available at this stage. Also, accord-
ing to our human evaluation on a smaller dataset, two thirds of
such potentially sentimental sentences do convey sentiment.
3 The sentiment and intensifier lexicons used to acquire these
numbers are discussed in Section 3.2.
</bodyText>
<page confidence="0.521647">
607
</page>
<note confidence="0.9996825">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 607–615,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999668927272727">
The granularities of text have spanned from words
and phrases to passages and documents.
Sentiment analysis has been approached mainly
as an unsupervised or supervised problem, alt-
hough the middle ground, semi-supervised ap-
proaches, exists. In this paper, we take a lexicon-
based, unsupervised approach to considering sen-
timent consistency for translation, although the
translation system itself is supervised. The ad-
vantages of such an approach have been discussed
in (Taboada et al., 2011). Briefly, it is good at cap-
turing the basic sentiment expressions common to
different domains, and certainly it requires no bi-
lingual sentiment-annotated data for our study. It
suits our purpose here of exploring the basic role
of sentiment for translation. Also, such a method
has been reported to achieve a good cross-domain
performance (Taboada et al., 2011) comparable
with that of other state-of-the-art models.
Translation for sentiment analysis A very inter-
esting line of research has leveraged labeled data in
a resource-rich language (e.g., English) to help
sentiment analysis in a resource-poorer language.
This includes the idea of constructing sentiment
lexicons automatically by using a translation dic-
tionary (Mihalcea et al., 2007), as well as the idea
of utilizing parallel corpora or automatically trans-
lated documents to incorporate sentiment-labeled
data from different languages (Wan, 2009; Mihal-
cea et al., 2007).
Our concern here is different ― instead of uti-
lizing translation for sentiment analysis; we are
interested in the SMT quality itself, by modeling
bilingual sentiment in translation. As mentioned
above, while we expect that statistics learned from
parallel corpora have implicitly captured sentiment
in some degree, we are curious if better modeling
is possible.
Considering semantic similarity in translation
The literature has included interesting ideas of in-
corporating different types of semantic knowledge
for SMT. A main stream of recent efforts have
been leveraging semantic roles (Wu and Fung,
2009; Liu and Gildea, 2010; Li et al., 2013) to im-
prove translation, e.g., through improving reorder-
ing. Also, Chen et al. (2010) have leveraged sense
similarity between source and target side as addi-
tional features. In this work, we view a different
dimension, i.e., semantic orientation, and show that
incorporating such knowledge improves the trans-
lation performance. We hope this work would add
more evidences to the existing literature of lever-
aging semantics for SMT, and shed some light on
further exploration of semantic consistency, e.g.,
examining other semantic differential factors.
</bodyText>
<sectionHeader confidence="0.988808" genericHeader="method">
3 Problem &amp; Approach
</sectionHeader>
<subsectionHeader confidence="0.999974">
3.1 Consistency of sentiment
</subsectionHeader>
<bodyText confidence="0.999968130434783">
Ideally, sentiment should be properly preserved in
high-quality translation. An interesting study con-
ducted by Mihalcea et al. (2007) suggests that in
most cases the sentence-level subjectivity is pre-
served by human translators. In their experiments,
one English and two Romanian native speakers
were asked to independently annotate the senti-
ment of English-Romanian sentence pairs from the
SemCor corpus (Miller et al., 1993), a balanced
corpus covering a number of topics in sports, poli-
tics, fashion, education, and others. These human
subjects were restricted to only access and annotate
the sentiment of their native-language side of sen-
tence pairs. The sentiment consistency was ob-
served by examining the annotation on both sides.
Automatic translation should conform to such a
consistency too, which could be of interest for
many applications, particularly for sentiment-
abundant data. On the other hand, if consistency is
not preserved for some reason, e.g., alignment
noise, enforcing consistency may help improve the
translation performance. In this paper, we explore
bilingual sentiment consistency for translation.
</bodyText>
<subsectionHeader confidence="0.999488">
3.2 Lexicon-based bilingual sentiment analysis
</subsectionHeader>
<bodyText confidence="0.999947866666667">
To capture bilingual sentiment consistency, we use
a lexicon-based approach to sentiment analysis.
Based on this, we design four groups of features to
represent the consistency.
The basic idea of the lexicon-based approach is
first identifying the sentiment words, intensifiers,
and negation words with lexicons, and then calcu-
lating the sentiment value using manually designed
formulas. To this end, we adapted the approaches
of (Taboada et al., 2011) and (Zhang et al., 2012)
so as to use the same formulas to analyze the sen-
timent on both the source and the target side.
The English and Chinese sentiment lexicons we
used are from (Wilson et al. 2005) and (Xu and Lin,
2007), respectively. We further use 75 English in-
</bodyText>
<page confidence="0.881713">
608
</page>
<bodyText confidence="0.999559117647059">
tensifiers listed in (Benzinger, 1971; page 171) and
81 Chinese intensifiers from (Zhang et al., 2012).
We use 17 English and 13 Chinese negation words.
Similar to (Taboada et al., 2011) and (Zhang et
al., 2012), we assigned a numerical score to each
sentiment word, intensifier, and negation word.
More specifically, one of the five values: -0.8, -0.4,
0, 0.4, and 0.8, was assigned to each sentiment
word in both the source and target sentiment lexi-
cons, according to the strength information anno-
tated in these lexicons. The scores indicate the
strength of sentiment. Table 2 lists some examples.
Similarly, one of the 4 values, i.e., -0.5, 0.5, 0.7
and 0.9, was manually assigned to each intensifier
word, and a -0.8 or -0.6 to the negation words. All
these scores will be used below to modify and shift
the sentiment value of a sentiment unit.
</bodyText>
<table confidence="0.975751666666667">
Sentiment words Intensifiers Negation words
impressive (0.8) extremely (0.9) not (-0.8)
good (0.4) very (0.7) rarely (-0.6)
actually (0.0) pretty (0.5)
worn (-0.4) slightly (-0.5)
depressing (-0.8)
</table>
<tableCaption confidence="0.748565">
Table 2: Examples of sentiment words and their senti-
ment strength; intensifiers and their modify rate; nega-
tion words and their negation degree.
</tableCaption>
<bodyText confidence="0.999843818181818">
Each sentiment word and its modifiers (negation
words and intensifiers) form a sentiment unit. We
first found all sentiment units by identifying senti-
ment words with the sentiment lexicons and their
modifiers with the corresponding lexicon in a 7-
word window. Then, for different patterns of sen-
timent unit, we calculated the sentiment values
using the formulas listed in Table 3, where these
formulas are adapted from (Taboada et al., 2011)
and (Zhang et al., 2012) so as to be applied to both
languages.
</bodyText>
<table confidence="0.991701571428571">
Sen. Sen. value Example Sen.
unit formula value
ws S(ws) good 0.40
wnws D(wn)S(ws) not good -0.32
wiws (1+R(wi))S(ws) very good 0.68
wnwiws (1+ D(wn)R(wi))S(ws) not very good 0.176
wiwnws D(wn)(1+R(wi))S(ws) very not good4 -0.544
</table>
<tableCaption confidence="0.998635">
Table 3: Heuristics used to compute the lexicon-based
sentiment values for different types of sentiment units.
</tableCaption>
<bodyText confidence="0.984898214285714">
4 The expression “very not good” is ungrammatical in English.
However, in Chinese, it is possible to have this kind of expres-
sion, such as “TVTXn”, whose transliteration is “very not
beautiful”, meaning “very ugly”.
For notation, S(ws) stands for the strength of
sentiment word ws, R(wi) is degree of the intensifi-
er word wi, and D(wn) is the negation degree of the
negation word wn.
Above, we have calculated the lexicon based
sentiment value (LSV) for any given unit ui, and
we call it lsv(ui) below. If a sentence or phrase s
contains multiple sentiment units, its lsv-score is a
merge of the individual lsv-scores of all its senti-
ment units:
</bodyText>
<equation confidence="0.953981">
(1)
</equation>
<bodyText confidence="0.99999812">
where the function basis(.) is a normalization func-
tion that performs on each lsv(ui). For example, the
basis(.) function could be a standard sign function
that just examines if a sentiment unit is positive or
negative, or simply an identity function (using the
lsv-scores directly). The merg(.) is a function that
merge the lsv-scores of individual sentiment units,
which may take several different forms below in
our feature design. For example, it can be a mean
function to take the average of the sentiment units’
lsv-scores, or a logic OR function to examine if a
sentence or phrase contains positive or negative
units (depending on the basis function). It can also
be a linear function that gives different weights to
different units according to further knowledge, e.g.,
syntactic information. In this paper, we only lever-
age the basic, surface-level analysis5.
In brief, our model here can be thought of as a
unification and simplification of both (Taboada et
al., 2011) and (Zhang et al., 2012), for our bilin-
gual task. We suspect that better sentiment model-
ing may further improve the general translation
performance or the quality of sentiment in transla-
tion. We will discuss some directions we think in-
teresting in the future work section.
</bodyText>
<subsectionHeader confidence="0.9332565">
3.3 Incorporating sentiment consistency into
phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.988206666666667">
In this paper, we focus on exploring sentiment
consistency for phrase-based SMT. However, the
approach might be used in other translation
framework. For example, consistency may be con-
sidered in the variables used in hierarchical transla-
tion rules (Chiang, 2005).
5 Note that when sentiment-annotated training data are availa-
ble, merg(.) can be trained, e.g., if assuming it to be the wide-
ly-used (log-) linear form.
</bodyText>
<page confidence="0.780374">
609
</page>
<bodyText confidence="0.987268285714286">
We will examine the role of sentiment con-
sistency in two ways: designing features for the
translation model and using them for re-ranking.
Before discussing the details of our features, we
briefly recap phrase-based SMT for completeness.
Given a source sentence f, the goal of statistical
machine translation is to select a target language
string e which maximizes the posterior probability
P(e|f). In a phrase-based SMT system, the transla-
tion unit is the phrases, where a &amp;quot;phrase&amp;quot; is a se-
quence of words. Phrase-based statistical machine
translation systems are usually modeled through a
log-linear framework (Och and Ney, 2002) by in-
troducing the hidden word alignment variable a
(Brown et al., 1993).
(2)
where is a string of phrases in the target lan-
guage, is the source language string,
Hm (e , f , a) are feature functions, and weights
A. are typically optimized to maximize the scoring
function (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.988698">
3.4 Feature design
</subsectionHeader>
<bodyText confidence="0.999834588235294">
In Section 3.2 above, we have discussed our lexi-
con-based approach, which leverages lexicon-
based sentiment consistency. Below, we describe
the specific features we designed for our experi-
ments. For a phrase pair ( 7, e ) or a sentence pair
(f, e)6, we propose the following four groups of
consistency features.
Subjectivity The first group of features is designed
to check the subjectivity of a phrase or a sentence
pair (f, e). This set of features examines if the
source or target side contains sentiment units. As
the name suggests, these features only capture if
subjectivity exists, but not if a sentiment is positive,
negative, or neutral. We include four binary fea-
tures that are triggered in the following condi-
tions―satisfaction of each condition gives the
corresponding feature a value of 1 and otherwise 0.
</bodyText>
<listItem confidence="0.998936666666667">
• F1: if neither side of the pair (f, e) contains at
least one sentiment unit;
• F2: if only one side contains sentiment units;
• F3: if the source side contains sentiment
units;
• F4: if the target side contains sentiment units.
</listItem>
<bodyText confidence="0.6674315">
Sentiment polarity The second group of features
check the sentiment polarity. These features are
still binary; they check if the polarities of the
source and target side are the same.
</bodyText>
<listItem confidence="0.940873666666667">
• F5: if the two sides of the pair (f, e) have the
same polarity;
• F6: if at least one side has a neutral senti-
ment;
• F7: if the polarity is opposite on the two
sides, i.e., one is positive and one is negative.
</listItem>
<bodyText confidence="0.999888210526316">
Note that examining the polarity on each side
can be regarded as a special case of applying Equa-
tion 1 above. For example, examining the positive
sentiment corresponds to using an indicator func-
tion as the basis function: it takes a value of 1 if
the lsv-score of a sentiment unit is positive or 0
otherwise, while the merge function is the logic
OR function. The subjectivity features above can
also be thought of similarly.
Sentiment intensity The third group of features is
designed to capture the degree of sentiment and
these features are numerical. We designed two
types of features in this group.
Feature F8 measures the difference of the LSV
scores on the two sides. As shown in Equation (3),
we use a mean function7 as our merge function
when computing the lsv-scores with Equation (1),
where the basis function is simply the identity
function.
</bodyText>
<equation confidence="0.998982666666667">
1  ilsv ui
0 ( ) (3)
lsv1 (s)  n
</equation>
<bodyText confidence="0.99970825">
Feature F9, F10, and F11 are the second type in
this group of features, which compute the ratio of
sentiment units on each side and examine their dif-
ference.
</bodyText>
<listItem confidence="0.989652">
• F8: H8 (f, e)  |lsv1 (f)  lsv1 (e) |
• F9: H9 (f, e)  |lsv+ (f)  lsv+ (e) |
</listItem>
<figure confidence="0.30419075">
n
6 For simplicity, we hereafter use the same notation (f, e) to
represent both a phrase pair and a sentence pair, when no con-
fusion arises.
7 We studied several different options but found the average
function is better than others for our translation task here, e.g.,
better than giving more weight to the last unit.
610
</figure>
<listItem confidence="0.997989">
• F10:
• F11:
</listItem>
<bodyText confidence="0.999797882352942">
lsv+(.) calculates the ratio of a positive sentiment
units in a phrase or a sentence, i.e., the number of
positive sentiment units divided by the total num-
ber of words of the phrase or the sentence. It corre-
sponds to a special form of Equation 1, in which
the basis function is an indicator function as dis-
cussed above, and the merge function adds up all
the counts and normalizes the sum by the length of
the phrase or the sentence concerned. Similarly,
lsv-(.) calculates the ratio of negative units and
lsv+-(.) calculates that for both types of units. The
length of sentence here means the number of word
tokens. We experimented with and without remov-
ing stop words when counting them, and found that
decision has little impact on the performance. We
also used the part-of-speech (POS) information in
the sentiment lexicons to help decide if a word is a
sentiment word or not, when we extract features;
i.e., a word is considered to have sentiment only if
its POS tag also matches what is specified in the
lexicons8. Using POS tags, however, did not im-
prove our translation performance.
Negation The fourth group of features checks the
consistency of negation words on the source and
target side. Note that negation words have already
been considered in computing the lsv-scores of
sentiment units. One motivation is that a negation
word may appear far from the sentiment word it
modifies, as mentioned in (Taboada et al., 2011)
and may be outside the window we used to calcu-
late the lsv-score above. The features here addi-
tionally check the counts of negation words. This
group of features is binary and triggered by the
following conditions.
</bodyText>
<listItem confidence="0.971666466666667">
• F12: if neither side of the pair (f, e) contain
negation words;
• F13: if both sides have an odd number of
negation words or both sides have an even
number of them;
• F14: if both sides have an odd number of
negation words not appearing outside any
sentiment units, or if both sides have an even
number of such negation words;
8 The Stanford POS tagger (Toutanova et al., 2003) was
used to tag phrase and sentence pairs for this purpose.
• F15: if both sides have an odd number of
negation words appearing in all sentiment
units, or if both sides have an even number
of such negation words.
</listItem>
<sectionHeader confidence="0.999148" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999231">
4.1 Translation experimental settings
</subsectionHeader>
<bodyText confidence="0.999985178571429">
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007). Each corpus was word-aligned using
IBM model 2, HMM, and IBM model 4, and the
phrase table was the union of phrase pairs extract-
ed from these separate alignments, with a length
limit of 7. The translation model was smoothed in
both directions with Kneser-Ney smoothing (Chen
et al., 2011). We use the hierarchical lexicalized
reordering model (Galley and Manning, 2008),
with a distortion limit of 7. Other features include
lexical weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram Eng-
lish Gigaword LM. The system was tuned with
batch lattice MIRA (Cherry and Foster, 2012).
We conducted experiments on NIST Chinese-to-
English translation task. The training data are from
NIST Open MT 2012. All allowed bilingual corpo-
ra were used to train the translation model and re-
ordering models. There are about 283M target
word tokens. The development (dev) set comprised
mainly data from the NIST 2005 test set, and also
some balanced-genre web-text from NIST training
data. Evaluation was performed on NIST 2006 and
2008, which have 1,664 and 1,357 sentences,
39.7K and 33.7K source words respectively. Four
references were provided for all dev and test sets.
</bodyText>
<sectionHeader confidence="0.601408" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.998295222222222">
Our evaluation metric is case-insensitive IBM
BLEU (Papineni et al., 2002), which performs
matching of n-grams up to n = 4; we report BLEU
scores on two test sets NIST06 and NIST08. Fol-
lowing (Koehn, 2004), we use the bootstrap
resampling test to do significance testing. In Table
4-6, the sign * and ** denote statistically signifi-
cant gains over the baseline at the p &lt; 0.05 and p &lt;
0.01 level, respectively.
</bodyText>
<page confidence="0.706656">
611
</page>
<table confidence="0.998378333333333">
NIST06 NIST08 Avg.
Baseline 35.1 28.4 31.7
+feat. group1 35.6** 29.0** 32.3
+feat. group2 35.3* 28.7* 32.0
+feat. group3 35.3 28.7* 32.0
+feat. group4 35.5* 28.8* 32.1
+feat. group1+2 35.8** 29.1** 32.5
+feat. group1+2+3 36.1** 29.3** 32.7
+feat. group1+2+3+4 36.2** 29.4** 32.8
</table>
<tableCaption confidence="0.692309">
Table 4: BLEU(%) scores on two original test sets for
different feature combinations. The sign * and ** indi-
cate statistically significant gains over the baseline at
the p &lt; 0.05 and p &lt; 0.01 level, respectively.
</tableCaption>
<bodyText confidence="0.999531481481482">
Table 4 summarizes the results of the baseline
and the results of adding each group of features
and their combinations. We can see that each indi-
vidual feature group improves the BLEU scores of
the baseline, and most of these gains are signifi-
cant. Among the feature groups, the largest im-
provement is associated with the first feature
group, i.e., the subjectivity features, which sug-
gests the significant role of modeling the basic sub-
jectivity. Adding more features results in further
improvement; the best performance was achieved
when using all these sentiment consistency fea-
tures, where we observed a 1.1 point improvement
on the NIST06 set and a 1.0 point improvement on
the NIST08 set, which yields an overall improve-
ment of about 1.1 BLEU score.
To further observe the results, we split each of
the two (i.e., the NIST06 and NIST08) test sets
into three subsets according to the ratio of senti-
ment words in the reference. We call them low-
sen, mid-sen and high-sen subsets, denoting lower,
middle, and higher sentiment-word ratios, respec-
tively. The three subsets contain roughly equal
number of sentences. Then we merged the two
low-sen subsets together, and similarly the two
mid-sen and high-sen subsets together, respective-
ly. Each subset has roughly 1007 sentences.
</bodyText>
<table confidence="0.9262505">
low-sen mid-sen high-sen
baseline 33.4 32.3 29.3
+all feat. 34.4 ** 33.5 ** 30.4 **
improvement 1.0 1.2 1.1
</table>
<tableCaption confidence="0.983118">
Table 5: BLEU(%) scores on three sub test sets with
different sentiment ratios.
</tableCaption>
<bodyText confidence="0.999696727272727">
Table 5 shows the performance of baseline and
the system with sentiment features (the last system
of Table 4) on these subsets. First, we can see that
both systems perform worse as the ratio of senti-
ment words increases. This probably indicates that
text with more sentiment is harder to translate than
text with less sentiment. Second, it is interesting
that the largest improvement is seen on the mid-sen
sub-set. The larger improvement on the mid-
sen/high-sen subsets than on the low-sen may indi-
cate the usefulness of the proposed features in cap-
turing sentiment information. The lower
improvement on high-sen than on mid-sen proba-
bly indicates that the high-sen subset is hard any-
way and using simple lexicon-level features is not
sufficient.
Sentence-level rerankdng Above, we have incor-
porated sentiment features into the phrase tables.
To further confirm the usefulness of the sentiment
consistency features, we explore their role for sen-
tence-level reranking. To this end, we re-rank
1000-best hypotheses for each sentence that were
generated with the baseline system. All the senti-
ment features were recalculated for each hypothe-
sis. We then re-learned the weights for the
decoding and sentiment features to select the best
hypothesis. The results are shown in Table 6. We
can see that sentiment features improve the per-
formance via re-ranking. The improvement is sta-
tistically significant, although the absolute
improvement is less than that obtained by incorpo-
rating the sentiment features in decoding. Not that
as widely known, the limited variety of candidates
in reranking may confine the improvement that
could be achieved. Better models on the sentence
level are possible. In addition, we feel that ensur-
ing sentiment and its target to be correctly paired is
of interest. Note that we have also combined the
last system in Table 4 with the reranking system
here; i.e., sentiment consistency was incorporated
in both ways, but we did not see further improve-
ment, which suggests that the benefit of the senti-
ment features has mainly been captured in the
phrase tables already.
</bodyText>
<table confidence="0.996570333333333">
feature NIST06 NIST08 Avg.
baseline 35.1 28.4 31.7
+ all feat. 35.4* 28.9** 32.1
</table>
<tableCaption confidence="0.984699">
Table 6: BLEU(%) scores on two original test sets on
sentence-level sentiment features.
</tableCaption>
<page confidence="0.788548">
612
</page>
<bodyText confidence="0.999762483870968">
Human evaluation We conducted a human evalu-
ation on the output of the baseline and the system
that incorporates all the proposed sentiment fea-
tures (the last system in Table 4). For this purpose,
we randomly sampled 250 sentences from the two
NIST test sets according to the following condi-
tions. First, the selected sentences should contain
at least one sentiment word―in this evaluation, we
target the sentences that may convey some senti-
ment. Second, we do not consider sentences short-
er than 5 words or longer than 50 words; or where
outputs of the baseline system and the system with
sentiment feature were identical. The 250 selected
sentences were split into 9 subsets, as we have 9
human evaluators (none of the authors of this paper
took part in this experiment). Each subset contains
26 randomly selected sentences, which are 234
sentences in total. The other 16 sentences are ran-
domly selected to serve as a common data set: they
are added to each of the 9 subsets in order to ob-
serve agreements between the 9 annotators. In
short, each human evaluator was presented with 42
evaluation samples. Each sample is a tuple contain-
ing the output of the baseline system, that of the
system considering sentiment, and the reference
translation. The two automatic translations were
presented in a random order to the evaluators.
As in (Callison-Burch et al., 2012), we per-
formed a pairwise comparison of the translations
produced by the systems. We asked the annotators
the following two questions Q1 and Q2:
</bodyText>
<listItem confidence="0.833342555555556">
• Q1(general preference): For any reason,
which of the two translations do you prefer
according to the provided references, other-
wise mark “no preference”?
• Q2 (sentiment preference): Does the refer-
ence contains sentiment? If so, in terms of
the translations of the sentiment, which of
the two translations do you prefer, otherwise
mark “no preference”?
</listItem>
<bodyText confidence="0.960247444444444">
ment, Ka ll . Then, we excluded one and only one
We computed Fleiss’s Kappa (Fleiss, 1971) on
the common set to measure inter-annotator agree-
annotator at a time to compute (Kappa score
without i-th annotator, i.e., from the other eight).
Finally, we removed the annotation of the two an-
notators whose answers were most different from
the others’: i.e., annotators with the biggest
aaa — xc` values. As a result, we got a Kappa score
</bodyText>
<table confidence="0.818934285714286">
0.432 on question Q1 and 0.415 on question Q2,
which both mean moderate agreement.
base win bsc win equal total
Translation 58 82 42 182
(31.86%) (45.05 %) (23.09%)
Sentiment 30 49 55 134
(22.39%) (36.57 %) (41.04%)
</table>
<tableCaption confidence="0.983388">
Table 7: Human evaluation preference for outputs from
baseline vs. system with sentiment features.
</tableCaption>
<bodyText confidence="0.999954166666667">
This left 7 files from 7 evaluators. We threw
away the common set in each file, leaving 182
pairwise comparisons. Table 6 shows that the eval-
uators preferred the output from the system with
sentiment features 82 times, the output from the
baseline system 58 times, and had no preference
the other 42 times. This indicates that there is a
human preference for the output from the system
that incorporated the sentiment features over those
from the baseline system at the p&lt;0.05 significance
level (in cases where people prefer one of them).
For question Q2, the human annotators regarded 48
sentences as conveying no sentiment according to
the provided reference, although each of them con-
tains at least one sentiment word (a criterion we
described above in constructing the evaluation set).
Among the remaining 134 sentences, the human
annotators preferred the proposed system 49 times
and the baseline system 30 times, while they mark
no-preference 55 times. The result shows a human
preference for the proposed model that considers
sentiment features at the p&lt;0.05 significance level
(in the cases where the evaluators did mark a pref-
erence).
</bodyText>
<subsectionHeader confidence="0.988917">
4.3 Examples
</subsectionHeader>
<bodyText confidence="0.867896352941177">
We have also manually examined the translations
generated by our best model (the last model of Ta-
ble 4, named BSC below) and the baseline model
(BSL), and we attribute the improvement to two
main reasons: (1) checking sentiment consistency
on a phrase pair helps punish low-quality phrase
pairs caused by word alignment error, (2) such
consistency checking also improves the sentiment
of the translation to better match the sentiment of
the source.
613
REF
BSL
BSC
... help the palestinians and the israelis to resume peace talks ...
... help the israelis and palestinians to resumption of the talks ...
... help the israelis and palestinians to resume peace talks ...
</bodyText>
<table confidence="0.5829173125">
REF
BSL
BSC
... the national team is preparing for matches with palestine and Iraq ...
... the national team &apos;s match with the palestinians and the iraq war ...
... the national team preparing for the match with the palestinian and iraq ...
REF ... in china we have top-quality people , ever-improving facilities ...
BSL
BSC
... we have talents in china , an increasing number of facilities ...
... we have outstanding talent in china , more and better facilities ...
REF ... continue to strive for that ...
BSL
BSC
... continue to struggle ...
... continue to work hard to achieve ...
</table>
<tableCaption confidence="0.9960785">
Table 8: Examples that show how sentiment helps improve our baseline model. REF is a reference translation, BSL
stands for baseline model, and BSC (bilingual sentiment consistency) is the last model of Table 4.
</tableCaption>
<bodyText confidence="0.999985625">
In the first two examples of Table 8, the first
line shows two phrase pairs that are finally chosen
by the baseline and BSC system, respectively. The
next three lines correspond to a reference (REF),
translation from BSL, and that from the BSC sys-
tem. The correct translations of “和谈” should be
“peace negotiations” or “peace talks”, which have
a positive sentiment, while the word “talks”
doesn’t convey sentiment at all. By punishing the
able to generate a better translation. In the second
example, the correct translation of “备战” should
be “prepare for”, where neither side conveys sen-
generated from incorrect word alignment. Since
“war” is a negative word in our sentiment lexicon,
checking sentiment consistency helps down-weight
such incorrect translations. Note also that the in-
tional, as the literal translation of “ 备 战 ” is
“prepare for war”.
Similarly, in the third example, “outstanding tal-
ent” is closer with respect to sentiment to the refer-
ence “top-quality people” than “talent” is; “more
and better” is closer with respect to sentiment to
the reference “ever-improving” than “an increasing
number” is. These three examples also help us un-
derstand the benefit of the subjectivity features
discussed in Section 3.4. In the fourth example,
“work hard to achieve” has a positive sentiment,
same as “strive”, while “struggle” is negative. We
can see that the BSC model is able to preserve the
original sentiment better (the 9 human evaluators
who were involved in our human evaluation (Sec-
tion 4.3) all agreed with this).
</bodyText>
<sectionHeader confidence="0.994558" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999985310344827">
We explore lexicon-based sentiment consistency
for statistical machine translation. By incorporating
lexicon-based subjectivity, polarity, intensity, and
negation features into the phrase-pair translation
model, we observed a 1.1-point improvement of
BLEU score on NIST Chinese-to-English transla-
tion. Among the four individual groups of features,
subjectivity consistency yields the largest im-
provement. The usefulness of the sentiment fea-
tures has also been confirmed when they are used
for re-ranking, for which we observed a 0.4-point
improvement on the BLEU score. In addition, hu-
man evaluation shows the preference of the human
subjects towards the translations generated by the
proposed model, in terms of both the general trans-
lation quality and the sentiment conveyed.
In the paper, we propose a lexicon-based ap-
proach to the problem. It is possible to employ
more complicated models. For example, with the
involvement of proper sentiment-annotated data, if
available, one may train a better sentiment-analysis
model even for the often-ungrammatical phrase
pairs or sentence candidates. Another direction we
feel interesting is ensuring that sentiment and its
target are not only better translated but also better
paired, i.e., their semantic relation is preserved.
This is likely to need further syntactic or semantic
analysis at the sentence level, and the semantic role
labeling work reviewed in Section 2 is relevant.
</bodyText>
<page confidence="0.905891">
614
</page>
<sectionHeader confidence="0.997877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999978892473119">
C. Banea, R. Mihalcea, J. Wiebe and S. Hassan. 2008.
Multilingual subjectivity analysis using machine
translation. In Proc. of EMNLP.
E. M. Benzinger. 1971. Intensifiers in current English.
PhD. Thesis. University of Florida.
P. F. Brown, S. Della Pietra, V. Della J. Pietra, and R.
Mercer. 1993. The mathematics of Machine Transla-
tion: Parameter estimation. Computational Linguis-
tics, 19(2): 263-312.
C. Callison-Burch, P. Koehn, C. Monz, R. Soricut, and
L. Specia. 2012. Findings of the 2012 Workshop on
Statistical Machine Translation. In Proc. of WMT.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL, 263–
270.
B. Chen, G. Foster, and R. Kuhn. 2010. Bilingual Sense
Similarity for Statistical Machine Translation. In
Proc. of ACL, 834-843.
B. Chen, R. Kuhn, G. Foster, and H. Johnson. 2011.
Unpacking and transforming feature functions: New
ways to smooth phrase tables. In Proc. of MT Sum-
mit.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proc. of
NAACL.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76(5):
378–382.
M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc.
of EMNLP: 848–856.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In Proc. of
EACL: 174-181.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of EMNLP:
388–395.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E.
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proc. of ACL, 177-180.
J. Li, P. Resnik and H. Daume III. 2013. Modeling Syn-
tactic and Semantic Structures in Hierarchical
Phrase-based Translation. In Proc. of NAACL, 540-
549.
D. Liu and D. Gildea. 2010. Semantic role features for
machine translation. In Proc. of COLING, 716–724.
R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In Proc. of ACL.
F. J. Och and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Ma-
chine Translation. In Proc. of ACL.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of ACL.
C. E. Osgood, G. J. Suci, and P. H. Tannenbaum. 1957.
The measurement of meaning. University of Illinois
Press.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up?:
sentiment classification using machine learning tech-
niques. In Proc. of EMNLP, 79-86.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-chine
translation. In Proc. of ACL, 311–318.
M. Taboada, M. Tofiloski, J. Brooke, K. Voll, and M.
Stede. 2011. Lexicon-Based Methods for Sentiment
Analysis. Computational Linguistics. 37(2): 267-307.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-Rich Part-of-Speech Tagging with a
Cyclic Dependency Network. In Proc. of HLT-
NAACL, 252-259.
P. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of ACL, 417-424.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In Proc. of
COLING.
X. Wan. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In proc. of ACL, 235-243.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In Proc. of EMNLP.
D. Wu and P. Fung. 2009. Semantic Roles for SMT: A
Hybrid Two-Pass Model. In Proc. of NAACL, 13-16.
L. Xu and H. Lin. 2007. Ontology-Driven Affective
Chinese Text Analysis and Evaluation Method. In
Lecture Notes in Computer Science Vol. 4738, 723-
724, Springer.
C. Zhang, P. Liu, Z. Zhu, and M. Fang. 2012. A Senti-
ment Analysis Method Based on a Polarity Lexicon.
Journal of Shangdong University (Natural Science).
47(3): 47-50.
</reference>
<page confidence="0.947694">
615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699719">
<title confidence="0.999779">Bilingual Sentiment Consistency for Statistical Machine Translation</title>
<author confidence="0.994293">Chen</author>
<affiliation confidence="0.999045">National Research Council Canada</affiliation>
<address confidence="0.998233">1200 Montreal Road, Ottawa, Canada, K1A 0R6</address>
<email confidence="0.751119">Boxing.Chen@nrc-cnrc.gc.ca</email>
<email confidence="0.751119">Xiaodan.Zhu@nrc-cnrc.gc.ca</email>
<abstract confidence="0.992152222222222">In this paper, we explore bilingual sentiment knowledge for statistical machine translation (SMT). We propose to explicitly model the consistency of sentiment between the source and target side with a lexicon-based approach. The experiments show that the proposed model significantly improves Chinese-to-English NIST translation over a competitive baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This is likely to need further syntactic or semantic analysis at the sentence level, and the semantic role labeling work reviewed in Section 2 is relevant.</title>
<marker></marker>
<rawString>This is likely to need further syntactic or semantic analysis at the sentence level, and the semantic role labeling work reviewed in Section 2 is relevant.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Banea</author>
<author>R Mihalcea</author>
<author>J Wiebe</author>
<author>S Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>C. Banea, R. Mihalcea, J. Wiebe and S. Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Benzinger</author>
</authors>
<title>Intensifiers in current English.</title>
<date>1971</date>
<tech>PhD. Thesis.</tech>
<institution>University of Florida.</institution>
<contexts>
<context position="8504" citStr="Benzinger, 1971" startWordPosition="1290" endWordPosition="1291"> to represent the consistency. The basic idea of the lexicon-based approach is first identifying the sentiment words, intensifiers, and negation words with lexicons, and then calculating the sentiment value using manually designed formulas. To this end, we adapted the approaches of (Taboada et al., 2011) and (Zhang et al., 2012) so as to use the same formulas to analyze the sentiment on both the source and the target side. The English and Chinese sentiment lexicons we used are from (Wilson et al. 2005) and (Xu and Lin, 2007), respectively. We further use 75 English in608 tensifiers listed in (Benzinger, 1971; page 171) and 81 Chinese intensifiers from (Zhang et al., 2012). We use 17 English and 13 Chinese negation words. Similar to (Taboada et al., 2011) and (Zhang et al., 2012), we assigned a numerical score to each sentiment word, intensifier, and negation word. More specifically, one of the five values: -0.8, -0.4, 0, 0.4, and 0.8, was assigned to each sentiment word in both the source and target sentiment lexicons, according to the strength information annotated in these lexicons. The scores indicate the strength of sentiment. Table 2 lists some examples. Similarly, one of the 4 values, i.e.,</context>
</contexts>
<marker>Benzinger, 1971</marker>
<rawString>E. M. Benzinger. 1971. Intensifiers in current English. PhD. Thesis. University of Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S Della Pietra</author>
<author>V Della J Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--312</pages>
<contexts>
<context position="13595" citStr="Brown et al., 1993" startWordPosition="2118" endWordPosition="2121">r the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a &amp;quot;phrase&amp;quot; is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). (2) where is a string of phrases in the target language, is the source language string, Hm (e , f , a) are feature functions, and weights A. are typically optimized to maximize the scoring function (Och, 2003). 3.4 Feature design In Section 3.2 above, we have discussed our lexicon-based approach, which leverages lexiconbased sentiment consistency. Below, we describe the specific features we designed for our experiments. For a phrase pair ( 7, e ) or a sentence pair (f, e)6, we propose the following four groups of consistency features. Subjectivity The first group of features is designed to c</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. Della Pietra, V. Della J. Pietra, and R. Mercer. 1993. The mathematics of Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. of WMT.</booktitle>
<contexts>
<context position="26392" citStr="Callison-Burch et al., 2012" startWordPosition="4307" endWordPosition="4310">per took part in this experiment). Each subset contains 26 randomly selected sentences, which are 234 sentences in total. The other 16 sentences are randomly selected to serve as a common data set: they are added to each of the 9 subsets in order to observe agreements between the 9 annotators. In short, each human evaluator was presented with 42 evaluation samples. Each sample is a tuple containing the output of the baseline system, that of the system considering sentiment, and the reference translation. The two automatic translations were presented in a random order to the evaluators. As in (Callison-Burch et al., 2012), we performed a pairwise comparison of the translations produced by the systems. We asked the annotators the following two questions Q1 and Q2: • Q1(general preference): For any reason, which of the two translations do you prefer according to the provided references, otherwise mark “no preference”? • Q2 (sentiment preference): Does the reference contains sentiment? If so, in terms of the translations of the sentiment, which of the two translations do you prefer, otherwise mark “no preference”? ment, Ka ll . Then, we excluded one and only one We computed Fleiss’s Kappa (Fleiss, 1971) on the co</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, R. Soricut, and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="12733" citStr="Chiang, 2005" startWordPosition="1981" endWordPosition="1982">a et al., 2011) and (Zhang et al., 2012), for our bilingual task. We suspect that better sentiment modeling may further improve the general translation performance or the quality of sentiment in translation. We will discuss some directions we think interesting in the future work section. 3.3 Incorporating sentiment consistency into phrase-based SMT In this paper, we focus on exploring sentiment consistency for phrase-based SMT. However, the approach might be used in other translation framework. For example, consistency may be considered in the variables used in hierarchical translation rules (Chiang, 2005). 5 Note that when sentiment-annotated training data are available, merg(.) can be trained, e.g., if assuming it to be the widely-used (log-) linear form. 609 We will examine the role of sentiment consistency in two ways: designing features for the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translat</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL, 263– 270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Bilingual Sense Similarity for Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>834--843</pages>
<contexts>
<context position="6053" citStr="Chen et al. (2010)" startWordPosition="914" endWordPosition="917">e SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem &amp; Approach 3.1 Consistency of sentiment Ideally, sentiment should be properly preserved in high-quality translation. An inte</context>
</contexts>
<marker>Chen, Foster, Kuhn, 2010</marker>
<rawString>B. Chen, G. Foster, and R. Kuhn. 2010. Bilingual Sense Similarity for Statistical Machine Translation. In Proc. of ACL, 834-843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>R Kuhn</author>
<author>G Foster</author>
<author>H Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In Proc. of MT Summit.</booktitle>
<contexts>
<context position="19425" citStr="Chen et al., 2011" startWordPosition="3158" endWordPosition="3161">s for this purpose. • F15: if both sides have an odd number of negation words appearing in all sentiment units, or if both sides have an even number of such negation words. 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 2</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>B. Chen, R. Kuhn, G. Foster, and H. Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In Proc. of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>G Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="19797" citStr="Cherry and Foster, 2012" startWordPosition="3219" endWordPosition="3222">BM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evalua</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<pages>378--382</pages>
<contexts>
<context position="26982" citStr="Fleiss, 1971" startWordPosition="4406" endWordPosition="4407">son-Burch et al., 2012), we performed a pairwise comparison of the translations produced by the systems. We asked the annotators the following two questions Q1 and Q2: • Q1(general preference): For any reason, which of the two translations do you prefer according to the provided references, otherwise mark “no preference”? • Q2 (sentiment preference): Does the reference contains sentiment? If so, in terms of the translations of the sentiment, which of the two translations do you prefer, otherwise mark “no preference”? ment, Ka ll . Then, we excluded one and only one We computed Fleiss’s Kappa (Fleiss, 1971) on the common set to measure inter-annotator agreeannotator at a time to compute (Kappa score without i-th annotator, i.e., from the other eight). Finally, we removed the annotation of the two annotators whose answers were most different from the others’: i.e., annotators with the biggest aaa — xc` values. As a result, we got a Kappa score 0.432 on question Q1 and 0.415 on question Q2, which both mean moderate agreement. base win bsc win equal total Translation 58 82 42 182 (31.86%) (45.05 %) (23.09%) Sentiment 30 49 55 134 (22.39%) (36.57 %) (41.04%) Table 7: Human evaluation preference for </context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>J. L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5): 378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP:</booktitle>
<pages>848--856</pages>
<contexts>
<context position="19506" citStr="Galley and Manning, 2008" startWordPosition="3169" endWordPosition="3172">rds appearing in all sentiment units, or if both sides have an even number of such negation words. 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proc. of EMNLP: 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proc. of EACL:</booktitle>
<pages>174--181</pages>
<contexts>
<context position="1196" citStr="Hatzivassiloglou &amp; McKeown, 1997" startWordPosition="168" endWordPosition="172">petitive baseline. 1 Introduction The expression of sentiment is an interesting and integral part of human languages. In written text sentiment is conveyed by senses and in speech also via prosody. Sentiment is associated with both evaluative (positive or negative) and potency (degree of sentiment) ― involving two of the three major semantic differential categories identified by Osgood et al. (1957). Automatically analyzing the sentiment of monolingual text has attracted a large bulk of research, which includes, but is not limited to, the early exploration of (Turney, 2002; Pang et al., 2002; Hatzivassiloglou &amp; McKeown, 1997). Since then, research has involved a variety of approaches and been conducted on various type of data, e.g., product reviews, news, blogs, and the more recent social media text. As sentiment has been an important concern in monolingual settings, better translation of such information between languages could be of interest to help better cross language barriers, particularly for sentiment-abundant data. Even when we randomly sampled a subset of sentence pairs from the NIST Open MT1 training data, we found that about 48.2% pairs contain at least one sentiment word on both sides, and 22.4% pairs</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of EACL: 174-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP:</booktitle>
<pages>388--395</pages>
<contexts>
<context position="20592" citStr="Koehn, 2004" startWordPosition="3352" endWordPosition="3353">odel and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evaluation metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores on two test sets NIST06 and NIST08. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. In Table 4-6, the sign * and ** denote statistically significant gains over the baseline at the p &lt; 0.05 and p &lt; 0.01 level, respectively. 611 NIST06 NIST08 Avg. Baseline 35.1 28.4 31.7 +feat. group1 35.6** 29.0** 32.3 +feat. group2 35.3* 28.7* 32.0 +feat. group3 35.3 28.7* 32.0 +feat. group4 35.5* 28.8* 32.1 +feat. group1+2 35.8** 29.1** 32.5 +feat. group1+2+3 36.1** 29.3** 32.7 +feat. group1+2+3+4 36.2** 29.4** 32.8 Table 4: BLEU(%) scores on two original test sets for different feature combinations. The sign * and ** indicat</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP: 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="19135" citStr="Koehn et al., 2007" startWordPosition="3108" endWordPosition="3111">th sides have an even number of them; • F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 8 The Stanford POS tagger (Toutanova et al., 2003) was used to tag phrase and sentence pairs for this purpose. • F15: if both sides have an odd number of negation words appearing in all sentiment units, or if both sides have an even number of such negation words. 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The syst</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL, 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>P Resnik</author>
<author>H Daume</author>
</authors>
<title>Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based Translation.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>540--549</pages>
<contexts>
<context position="5968" citStr="Li et al., 2013" startWordPosition="900" endWordPosition="903"> ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem &amp; Approach 3.1 Consistency of sentiment</context>
</contexts>
<marker>Li, Resnik, Daume, 2013</marker>
<rawString>J. Li, P. Resnik and H. Daume III. 2013. Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based Translation. In Proc. of NAACL, 540-549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>D Gildea</author>
</authors>
<title>Semantic role features for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>716--724</pages>
<contexts>
<context position="5950" citStr="Liu and Gildea, 2010" startWordPosition="896" endWordPosition="899">cern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem &amp; Approach 3.1 Consis</context>
</contexts>
<marker>Liu, Gildea, 2010</marker>
<rawString>D. Liu and D. Gildea. 2010. Semantic role features for machine translation. In Proc. of COLING, 716–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Banea</author>
<author>J Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5134" citStr="Mihalcea et al., 2007" startWordPosition="773" endWordPosition="776">bilingual sentiment-annotated data for our study. It suits our purpose here of exploring the basic role of sentiment for translation. Also, such a method has been reported to achieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource-poorer language. This includes the idea of constructing sentiment lexicons automatically by using a translation dictionary (Mihalcea et al., 2007), as well as the idea of utilizing parallel corpora or automatically translated documents to incorporate sentiment-labeled data from different languages (Wan, 2009; Mihalcea et al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation Th</context>
<context position="6702" citStr="Mihalcea et al. (2007)" startWordPosition="1010" endWordPosition="1013">arity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem &amp; Approach 3.1 Consistency of sentiment Ideally, sentiment should be properly preserved in high-quality translation. An interesting study conducted by Mihalcea et al. (2007) suggests that in most cases the sentence-level subjectivity is preserved by human translators. In their experiments, one English and two Romanian native speakers were asked to independently annotate the sentiment of English-Romanian sentence pairs from the SemCor corpus (Miller et al., 1993), a balanced corpus covering a number of topics in sports, politics, fashion, education, and others. These human subjects were restricted to only access and annotate the sentiment of their native-language side of sentence pairs. The sentiment consistency was observed by examining the annotation on both sid</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13522" citStr="Och and Ney, 2002" startWordPosition="2105" endWordPosition="2108">ine the role of sentiment consistency in two ways: designing features for the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a &amp;quot;phrase&amp;quot; is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). (2) where is a string of phrases in the target language, is the source language string, Hm (e , f , a) are feature functions, and weights A. are typically optimized to maximize the scoring function (Och, 2003). 3.4 Feature design In Section 3.2 above, we have discussed our lexicon-based approach, which leverages lexiconbased sentiment consistency. Below, we describe the specific features we designed for our experiments. For a phrase pair ( 7, e ) or a sentence pair (f, e)6, we propose the following four groups of consis</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13806" citStr="Och, 2003" startWordPosition="2159" endWordPosition="2160">tion is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a &amp;quot;phrase&amp;quot; is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). (2) where is a string of phrases in the target language, is the source language string, Hm (e , f , a) are feature functions, and weights A. are typically optimized to maximize the scoring function (Och, 2003). 3.4 Feature design In Section 3.2 above, we have discussed our lexicon-based approach, which leverages lexiconbased sentiment consistency. Below, we describe the specific features we designed for our experiments. For a phrase pair ( 7, e ) or a sentence pair (f, e)6, we propose the following four groups of consistency features. Subjectivity The first group of features is designed to check the subjectivity of a phrase or a sentence pair (f, e). This set of features examines if the source or target side contains sentiment units. As the name suggests, these features only capture if subjectivity</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Osgood</author>
<author>G J Suci</author>
<author>P H Tannenbaum</author>
</authors>
<title>The measurement of meaning.</title>
<date>1957</date>
<publisher>University of Illinois Press.</publisher>
<contexts>
<context position="965" citStr="Osgood et al. (1957)" startWordPosition="131" endWordPosition="134">ly model the consistency of sentiment between the source and target side with a lexicon-based approach. The experiments show that the proposed model significantly improves Chinese-to-English NIST translation over a competitive baseline. 1 Introduction The expression of sentiment is an interesting and integral part of human languages. In written text sentiment is conveyed by senses and in speech also via prosody. Sentiment is associated with both evaluative (positive or negative) and potency (degree of sentiment) ― involving two of the three major semantic differential categories identified by Osgood et al. (1957). Automatically analyzing the sentiment of monolingual text has attracted a large bulk of research, which includes, but is not limited to, the early exploration of (Turney, 2002; Pang et al., 2002; Hatzivassiloglou &amp; McKeown, 1997). Since then, research has involved a variety of approaches and been conducted on various type of data, e.g., product reviews, news, blogs, and the more recent social media text. As sentiment has been an important concern in monolingual settings, better translation of such information between languages could be of interest to help better cross language barriers, part</context>
</contexts>
<marker>Osgood, Suci, Tannenbaum, 1957</marker>
<rawString>C. E. Osgood, G. J. Suci, and P. H. Tannenbaum. 1957. The measurement of meaning. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1161" citStr="Pang et al., 2002" startWordPosition="164" endWordPosition="167">nslation over a competitive baseline. 1 Introduction The expression of sentiment is an interesting and integral part of human languages. In written text sentiment is conveyed by senses and in speech also via prosody. Sentiment is associated with both evaluative (positive or negative) and potency (degree of sentiment) ― involving two of the three major semantic differential categories identified by Osgood et al. (1957). Automatically analyzing the sentiment of monolingual text has attracted a large bulk of research, which includes, but is not limited to, the early exploration of (Turney, 2002; Pang et al., 2002; Hatzivassiloglou &amp; McKeown, 1997). Since then, research has involved a variety of approaches and been conducted on various type of data, e.g., product reviews, news, blogs, and the more recent social media text. As sentiment has been an important concern in monolingual settings, better translation of such information between languages could be of interest to help better cross language barriers, particularly for sentiment-abundant data. Even when we randomly sampled a subset of sentence pairs from the NIST Open MT1 training data, we found that about 48.2% pairs contain at least one sentiment </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proc. of EMNLP, 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of ma-chine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="20461" citStr="Papineni et al., 2002" startWordPosition="3325" endWordPosition="3328">-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evaluation metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores on two test sets NIST06 and NIST08. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. In Table 4-6, the sign * and ** denote statistically significant gains over the baseline at the p &lt; 0.05 and p &lt; 0.01 level, respectively. 611 NIST06 NIST08 Avg. Baseline 35.1 28.4 31.7 +feat. group1 35.6** 29.0** 32.3 +feat. group2 35.3* 28.7* 32.0 +feat. group3 35.3 28.7* 32.0 +feat. group4 35.5* 28.8* 32.1 +feat. group1+2 35.8** 29.1** 32.5 +feat. group1+2+3 36.1** 29.3** 32.7 +feat. group1+2+3+4</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of ma-chine translation. In Proc. of ACL, 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
<author>M Tofiloski</author>
<author>J Brooke</author>
<author>K Voll</author>
<author>M Stede</author>
</authors>
<title>Lexicon-Based Methods for Sentiment Analysis. Computational Linguistics.</title>
<date>2011</date>
<volume>37</volume>
<issue>2</issue>
<pages>267--307</pages>
<contexts>
<context position="4387" citStr="Taboada et al., 2011" startWordPosition="660" endWordPosition="663">r Computational Linguistics, pages 607–615, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics The granularities of text have spanned from words and phrases to passages and documents. Sentiment analysis has been approached mainly as an unsupervised or supervised problem, although the middle ground, semi-supervised approaches, exists. In this paper, we take a lexiconbased, unsupervised approach to considering sentiment consistency for translation, although the translation system itself is supervised. The advantages of such an approach have been discussed in (Taboada et al., 2011). Briefly, it is good at capturing the basic sentiment expressions common to different domains, and certainly it requires no bilingual sentiment-annotated data for our study. It suits our purpose here of exploring the basic role of sentiment for translation. Also, such a method has been reported to achieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource</context>
<context position="8194" citStr="Taboada et al., 2011" startWordPosition="1231" endWordPosition="1234"> may help improve the translation performance. In this paper, we explore bilingual sentiment consistency for translation. 3.2 Lexicon-based bilingual sentiment analysis To capture bilingual sentiment consistency, we use a lexicon-based approach to sentiment analysis. Based on this, we design four groups of features to represent the consistency. The basic idea of the lexicon-based approach is first identifying the sentiment words, intensifiers, and negation words with lexicons, and then calculating the sentiment value using manually designed formulas. To this end, we adapted the approaches of (Taboada et al., 2011) and (Zhang et al., 2012) so as to use the same formulas to analyze the sentiment on both the source and the target side. The English and Chinese sentiment lexicons we used are from (Wilson et al. 2005) and (Xu and Lin, 2007), respectively. We further use 75 English in608 tensifiers listed in (Benzinger, 1971; page 171) and 81 Chinese intensifiers from (Zhang et al., 2012). We use 17 English and 13 Chinese negation words. Similar to (Taboada et al., 2011) and (Zhang et al., 2012), we assigned a numerical score to each sentiment word, intensifier, and negation word. More specifically, one of th</context>
<context position="10101" citStr="Taboada et al., 2011" startWordPosition="1549" endWordPosition="1552">slightly (-0.5) depressing (-0.8) Table 2: Examples of sentiment words and their sentiment strength; intensifiers and their modify rate; negation words and their negation degree. Each sentiment word and its modifiers (negation words and intensifiers) form a sentiment unit. We first found all sentiment units by identifying sentiment words with the sentiment lexicons and their modifiers with the corresponding lexicon in a 7- word window. Then, for different patterns of sentiment unit, we calculated the sentiment values using the formulas listed in Table 3, where these formulas are adapted from (Taboada et al., 2011) and (Zhang et al., 2012) so as to be applied to both languages. Sen. Sen. value Example Sen. unit formula value ws S(ws) good 0.40 wnws D(wn)S(ws) not good -0.32 wiws (1+R(wi))S(ws) very good 0.68 wnwiws (1+ D(wn)R(wi))S(ws) not very good 0.176 wiwnws D(wn)(1+R(wi))S(ws) very not good4 -0.544 Table 3: Heuristics used to compute the lexicon-based sentiment values for different types of sentiment units. 4 The expression “very not good” is ungrammatical in English. However, in Chinese, it is possible to have this kind of expression, such as “TVTXn”, whose transliteration is “very not beautiful”,</context>
<context position="12135" citStr="Taboada et al., 2011" startWordPosition="1886" endWordPosition="1889">nt units, which may take several different forms below in our feature design. For example, it can be a mean function to take the average of the sentiment units’ lsv-scores, or a logic OR function to examine if a sentence or phrase contains positive or negative units (depending on the basis function). It can also be a linear function that gives different weights to different units according to further knowledge, e.g., syntactic information. In this paper, we only leverage the basic, surface-level analysis5. In brief, our model here can be thought of as a unification and simplification of both (Taboada et al., 2011) and (Zhang et al., 2012), for our bilingual task. We suspect that better sentiment modeling may further improve the general translation performance or the quality of sentiment in translation. We will discuss some directions we think interesting in the future work section. 3.3 Incorporating sentiment consistency into phrase-based SMT In this paper, we focus on exploring sentiment consistency for phrase-based SMT. However, the approach might be used in other translation framework. For example, consistency may be considered in the variables used in hierarchical translation rules (Chiang, 2005). </context>
<context position="18171" citStr="Taboada et al., 2011" startWordPosition="2936" endWordPosition="2939">ntiment lexicons to help decide if a word is a sentiment word or not, when we extract features; i.e., a word is considered to have sentiment only if its POS tag also matches what is specified in the lexicons8. Using POS tags, however, did not improve our translation performance. Negation The fourth group of features checks the consistency of negation words on the source and target side. Note that negation words have already been considered in computing the lsv-scores of sentiment units. One motivation is that a negation word may appear far from the sentiment word it modifies, as mentioned in (Taboada et al., 2011) and may be outside the window we used to calculate the lsv-score above. The features here additionally check the counts of negation words. This group of features is binary and triggered by the following conditions. • F12: if neither side of the pair (f, e) contain negation words; • F13: if both sides have an odd number of negation words or both sides have an even number of them; • F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 8 The Stanford POS tagger (Toutanova et al., 2003) was </context>
</contexts>
<marker>Taboada, Tofiloski, Brooke, Voll, Stede, 2011</marker>
<rawString>M. Taboada, M. Tofiloski, J. Brooke, K. Voll, and M. Stede. 2011. Lexicon-Based Methods for Sentiment Analysis. Computational Linguistics. 37(2): 267-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In</title>
<date>2003</date>
<booktitle>Proc. of HLTNAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="18766" citStr="Toutanova et al., 2003" startWordPosition="3046" endWordPosition="3049">ed in (Taboada et al., 2011) and may be outside the window we used to calculate the lsv-score above. The features here additionally check the counts of negation words. This group of features is binary and triggered by the following conditions. • F12: if neither side of the pair (f, e) contain negation words; • F13: if both sides have an odd number of negation words or both sides have an even number of them; • F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 8 The Stanford POS tagger (Toutanova et al., 2003) was used to tag phrase and sentence pairs for this purpose. • F15: if both sides have an odd number of negation words appearing in all sentiment units, or if both sides have an even number of such negation words. 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in bo</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proc. of HLTNAACL, 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1142" citStr="Turney, 2002" startWordPosition="162" endWordPosition="163">glish NIST translation over a competitive baseline. 1 Introduction The expression of sentiment is an interesting and integral part of human languages. In written text sentiment is conveyed by senses and in speech also via prosody. Sentiment is associated with both evaluative (positive or negative) and potency (degree of sentiment) ― involving two of the three major semantic differential categories identified by Osgood et al. (1957). Automatically analyzing the sentiment of monolingual text has attracted a large bulk of research, which includes, but is not limited to, the early exploration of (Turney, 2002; Pang et al., 2002; Hatzivassiloglou &amp; McKeown, 1997). Since then, research has involved a variety of approaches and been conducted on various type of data, e.g., product reviews, news, blogs, and the more recent social media text. As sentiment has been an important concern in monolingual settings, better translation of such information between languages could be of interest to help better cross language barriers, particularly for sentiment-abundant data. Even when we randomly sampled a subset of sentence pairs from the NIST Open MT1 training data, we found that about 48.2% pairs contain at l</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proc. of ACL, 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based word alignment in statistical translation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-Training for Cross-Lingual Sentiment Classification.</title>
<date>2009</date>
<booktitle>In proc. of ACL,</booktitle>
<pages>235--243</pages>
<contexts>
<context position="5297" citStr="Wan, 2009" startWordPosition="798" endWordPosition="799">chieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource-poorer language. This includes the idea of constructing sentiment lexicons automatically by using a translation dictionary (Mihalcea et al., 2007), as well as the idea of utilizing parallel corpora or automatically translated documents to incorporate sentiment-labeled data from different languages (Wan, 2009; Mihalcea et al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging se</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>X. Wan. 2009. Co-Training for Cross-Lingual Sentiment Classification. In proc. of ACL, 235-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8396" citStr="Wilson et al. 2005" startWordPosition="1270" endWordPosition="1273">istency, we use a lexicon-based approach to sentiment analysis. Based on this, we design four groups of features to represent the consistency. The basic idea of the lexicon-based approach is first identifying the sentiment words, intensifiers, and negation words with lexicons, and then calculating the sentiment value using manually designed formulas. To this end, we adapted the approaches of (Taboada et al., 2011) and (Zhang et al., 2012) so as to use the same formulas to analyze the sentiment on both the source and the target side. The English and Chinese sentiment lexicons we used are from (Wilson et al. 2005) and (Xu and Lin, 2007), respectively. We further use 75 English in608 tensifiers listed in (Benzinger, 1971; page 171) and 81 Chinese intensifiers from (Zhang et al., 2012). We use 17 English and 13 Chinese negation words. Similar to (Taboada et al., 2011) and (Zhang et al., 2012), we assigned a numerical score to each sentiment word, intensifier, and negation word. More specifically, one of the five values: -0.8, -0.4, 0, 0.4, and 0.8, was assigned to each sentiment word in both the source and target sentiment lexicons, according to the strength information annotated in these lexicons. The s</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>P Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="5928" citStr="Wu and Fung, 2009" startWordPosition="892" endWordPosition="895">al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>D. Wu and P. Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In Proc. of NAACL, 13-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu</author>
<author>H Lin</author>
</authors>
<title>Ontology-Driven Affective Chinese Text Analysis and Evaluation Method.</title>
<date>2007</date>
<booktitle>In Lecture Notes in Computer Science</booktitle>
<volume>4738</volume>
<pages>723</pages>
<contexts>
<context position="8419" citStr="Xu and Lin, 2007" startWordPosition="1275" endWordPosition="1278">-based approach to sentiment analysis. Based on this, we design four groups of features to represent the consistency. The basic idea of the lexicon-based approach is first identifying the sentiment words, intensifiers, and negation words with lexicons, and then calculating the sentiment value using manually designed formulas. To this end, we adapted the approaches of (Taboada et al., 2011) and (Zhang et al., 2012) so as to use the same formulas to analyze the sentiment on both the source and the target side. The English and Chinese sentiment lexicons we used are from (Wilson et al. 2005) and (Xu and Lin, 2007), respectively. We further use 75 English in608 tensifiers listed in (Benzinger, 1971; page 171) and 81 Chinese intensifiers from (Zhang et al., 2012). We use 17 English and 13 Chinese negation words. Similar to (Taboada et al., 2011) and (Zhang et al., 2012), we assigned a numerical score to each sentiment word, intensifier, and negation word. More specifically, one of the five values: -0.8, -0.4, 0, 0.4, and 0.8, was assigned to each sentiment word in both the source and target sentiment lexicons, according to the strength information annotated in these lexicons. The scores indicate the stre</context>
</contexts>
<marker>Xu, Lin, 2007</marker>
<rawString>L. Xu and H. Lin. 2007. Ontology-Driven Affective Chinese Text Analysis and Evaluation Method. In Lecture Notes in Computer Science Vol. 4738, 723-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>