<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001068">
<title confidence="0.978468">
ITSPOKE: An Intelligent Tutoring Spoken Dialogue System
</title>
<author confidence="0.990868">
Diane J. Litman Scott Silliman
</author>
<affiliation confidence="0.995411">
University of Pittsburgh University of Pittsburgh
Department of Computer Science &amp; Learning Research and Development Center
</affiliation>
<author confidence="0.397559">
Learning Research and Development Center Pittsburgh PA, 15260, USA
</author>
<affiliation confidence="0.447648">
Pittsburgh PA, 15260, USA scotts@pitt.edu
</affiliation>
<email confidence="0.999185">
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999195818181818">
ITSPOKE is a spoken dialogue system that
uses the Why2-Atlas text-based tutoring sys-
tem as its “back-end”. A student first types a
natural language answer to a qualitative physics
problem. ITSPOKE then engages the student
in a spoken dialogue to provide feedback and
correct misconceptions, and to elicit more com-
plete explanations. We are using ITSPOKE
to generate an empirically-based understanding
of the ramifications of adding spoken language
capabilities to text-based dialogue tutors.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926916666667">
The development of computational tutorial dialogue sys-
tems has become more and more prevalent (Aleven and
Rose, 2003), as one method of attempting to close the
performance gap between human and computer tutors.
While many such systems have yielded successful evalu-
ations with students, most are currently text-based (Evens
et al., 2001; Aleven et al., 2001; Zinn et al., 2002; Van-
Lehn et al., 2002). There is reason to believe that speech-
based tutorial dialogue systems could be even more ef-
fective. Spontaneous self-explanation by students im-
proves learning gains during human-human tutoring (Chi
et al., 1994), and spontaneous self-explanation occurs
more frequently in spoken tutoring than in text-based tu-
toring (Hausmann and Chi, 2002). In human-computer
tutoring, the use of an interactive pedagogical agent that
communicates using speech rather than text output im-
proves student learning, while the visual presence or ab-
sence of the agent does not impact performance (Moreno
et al., 2001). In addition, it has been hypothesized that
the success of computer tutors could be increased by rec-
ognizing and responding to student emotion. (Aist et al.,
2002) have shown that adding emotional processing to
a dialogue-based reading tutor increases student persis-
tence. Information in the speech signal such as prosody
has been shown to be a rich source of information for
predicting emotional states in other types of dialogue in-
teractions (Ang et al., 2002; Lee et al., 2002; Batliner et
al., 2003; Devillers et al., 2003; Shafran et al., 2003).
With advances in speech technology, several projects
have begun to incorporate basic spoken language capabil-
ities into their systems (Mostow and Aist, 2001; Fry et al.,
2001; Graesser et al., 2001; Rickel and Johnson, 2000).
However, to date there has been little examination of the
ramifications of using a spoken modality for dialogue tu-
toring. To assess the impact and evaluate the utility of
adding spoken language capabilities to dialogue tutoring
systems, we have built ITSPOKE (Intelligent Tutoring
SPOKEn dialogue system), a spoken dialogue system
that uses the Why2-Atlas conceptual physics tutoringsys-
tem (VanLehn et al., 2002) as its “back-end.” We are
using ITSPOKE as a platform for examining whether
acoustic-prosodic information can be used to improve the
recognition of pedagogically useful information such as
student emotion (Forbes-Riley and Litman, 2004; Lit-
man and Forbes-Riley, 2004), and whether speech can
improve the performance evaluations of dialogue tutoring
systems (e.g., as measured by learning gains, efficiency,
usability, etc.) (Ros´e et al., 2003).
</bodyText>
<sectionHeader confidence="0.972062" genericHeader="method">
2 Application Description
</sectionHeader>
<bodyText confidence="0.9994507">
ITSPOKE is a speech-enabled version of the Why2-
Atlas (VanLehn et al., 2002) text-based dialogue tutoring
system. As in Why2-Atlas, a student first types a nat-
ural language answer to a qualitative physics problem.
In ITSPOKE, however, the system engages the student
in a spoken dialogue to correct misconceptions and elicit
more complete explanations.
Consider the screenshot shown in Figure 1. ITSPOKE
first poses conceptual physics problem 58 to the student,
as shown in the upper right of the figure. Next, the
</bodyText>
<figureCaption confidence="0.997137">
Figure 1: Screenshot during ITSPOKE Human-Computer Spoken Dialogue
</figureCaption>
<bodyText confidence="0.99794435">
user types in a natural language essay answer (as shown
in the essay box in the middle right of Figure 1), and
clicks “SUBMIT.” ITSPOKE then analyzes the essay, af-
ter which the spoken dialogue with the student begins.
During the dialogue, the system and student discuss
a solution to the problem relative to the student’s es-
say explanation, using spoken English. At the time the
screenshot was generated, the student had just said “free
fall.” After each system or student utterance, the system
prompt, or the system’s understanding of the student’s re-
sponse, respectively, are added to the dialogue history (as
shown in the dialogue box in the middle left of Figure 1).1
At some point later in the dialogue, the system will
eventually ask the student to edit the typed essay explana-
tion. The system will then either express satisfaction and
end the tutoring for the current problem, or continue with
another round of spoken dialogue interaction and typed
essay revision. Upon completing the problem, the stu-
dent is presented with an example of an ideal essay.
Another excerpt with ITSPOKE is shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.401759">
&apos;The “Tell Tutor” box is used for typed student login only.
</subsectionHeader>
<sectionHeader confidence="0.902916" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.897683375">
The architecture of ITSPOKE is shown in Figure 3. Stu-
dent speech is digitized from microphone input and sent
to the Sphinx2 automatic speech recognizer. Sphinx2’s
best hypothesis is then sent to Why2-Atlas for syntactic
and semantic analysis (Ros´e, 2000), discourse and do-
main processing (Jordan et al., 2003), and finite-state di-
alogue management (Ros´e et al., 2001). The tutor’s text
output is sent to the Cepstral text-to-speech system, and
played through a speaker or headphone.
ITSPOKE is implemented in Python and acts as a
proxy server between the Why2-Atlas server and client.
ITSPOKE monitors the XML being sent between the two
and decides what text should be spoken and when to lis-
ten. After speech recognition, ITSPOKE submits what it
thinks the student said to Why2-Atlas (as if the student
had typed it). Sphinx2’s C API is tied into Python by us-
ing SWIG to generate a Python wrapper API. Cepstral’s
speech synthesis engine is currently run out of process
using a fork/exec, but we plan to use SWIG to tie its C
API into Python in the future. For speed, we have in-
ITSPOKE: I didn’t understand. What force acts on all objects
within close proximity to earth?
STUDENT: uh the force of gravity
ITSPOKE: Fine. Besides the Earth’s gravitational force, are
there any other forces acting on the person, keys, and elevator
after the release?
STUDENT: no
ITSPOKE: Yep. Thus, keep in mind that after the release, the
only force on the person, keys, and elevator is the force of grav-
ity. If gravity is the only force acting on an object, will it be
moving or staying still?
STUDENT: it will be moving
</bodyText>
<figureCaption confidence="0.85587475">
ITSPOKE: Fine. Not only are the person, keys, and elevator
moving, they have only gravitational forces on them. When an
object is falling and has only gravitational force on it, it is said
to be in what?
STUDENT: freefall
ITSPOKE: Good. Thus, after the release the only force on
the keys and man is the downward force of earth’s gravity, so
they are in freefall. Before the release, what is the relationship
between the man’s velocity and the keys’ velocity?
STUDENT: they’re equal
Figure 2: ITSPOKE Excerpt (3.5 minutes into session)
Figure 3: The Architecture of ITSPOKE
</figureCaption>
<bodyText confidence="0.99919475">
stalled Sphinx2 and Cepstral on the ITSPOKE machine.
The dashed line in the figure reflects that Why2-Atlas can
be installed on a different machine, although we are cur-
rently running it on the same machine.
</bodyText>
<sectionHeader confidence="0.994781" genericHeader="evaluation">
4 Performance Analysis
</sectionHeader>
<bodyText confidence="0.999967361111111">
A formal evaluation comparing ITSPOKE and other tu-
toring methods began in November 2003, and is still on-
going. Subjects are University of Pittsburgh students who
have taken no college physics and are native speakers of
American English. Our experimental procedure, taking
roughly 4 hours/student, is as follows: students 1) read a
small document of background material, 2) take a pretest
measuring their physics knowledge, 3) use ITSPOKE to
work through 5 physics problems, and 4) take a post-test
similar to the pretest.
As of March 2004, we have collected 80 dialogues
from 16 students (21 total hours of speech, mean dialogue
time of 17 minutes). An average dialogue contains 21.3
student turns and 26.3 tutor turns. The mean student turn
length is 2.8 words (max=28, min=1).2
ITSPOKE uses 56 dialogue-state dependent language
models for speech recognition; 43 of these 56 models
have been used to process the data collected to date.3
These stochastic language models were initially trained
using 4551 typed student utterances from a 2002 eval-
uation of Why2-Atlas, then later enhanced with spo-
ken utterances obtained during ITSPOKE’s pilot testing.
For the 1600 student turns that we have collected, IT-
SPOKE’s current Word Error Rate is 31.2%. While this is
the traditional method of evaluating speech recognition,
semantic rather than transcription accuracy is more useful
for dialogue evaluation as it does not penalize for word
errors that are unimportant to overall utterance interpre-
tation. Semantic analysis based on speech recognition
is the same as based on perfect transcription 92% of the
time. An average dialogue contains 1.4 rejection prompts
(when ITSPOKE is not confident of the speech recogni-
tion output, it asks the user to repeat the utterance), and
.8 timeout prompts (when the student doesn’t say any-
thing within a specified time frame, ITSPOKE repeats its
previous question).
</bodyText>
<sectionHeader confidence="0.997216" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999844117647059">
The goal of ITSPOKE is to generate an empirically-based
understanding of the implications of using speech instead
of text-based dialogue tutoring, and to use these results
to build an improved version of ITSPOKE. We are cur-
rently analyzing our corpus of dialogues with ITSPOKE
to determine whether spoken dialogues yield increased
performance compared to text with respect to a variety
of evaluation metrics, and whether acoustic-prosodic fea-
tures only found in speech can be used to better predict
pedagogically useful information such as student emo-
tions. Our next step will be to modify the dialogue man-
ager inherited from Why2-Atlas to use new tutorial strate-
gies optimized for speech, and to enhance ITSPOKE to
predict and adapt to student emotion. In previous work
on adaptive (non-tutoring) dialogue systems (Litman and
Pan, 2002), adaptation to problematic dialogue situations
measurably improved system performance.
</bodyText>
<sectionHeader confidence="0.99169" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.822013">
This research has been supported by NSF Grant Nos.
9720359, 0328431 and 0325054, and by the Office of
</bodyText>
<footnote confidence="0.986307333333333">
2Word count is estimated from speech recognition output.
3The remaining language models correspond to physics
problems that are not being tested in the current evaluation.
</footnote>
<note confidence="0.525167">
Naval Research. Thanks to Kurt VanLehn and the Why2-
Atlas team for the use and modification of their system.
</note>
<sectionHeader confidence="0.615497" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.959133144329897">
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Picard.
2002. Experimentally augmenting an intelligent tutor-
ing system with human-supplied capabilities: Adding
Human-Provided Emotional Scaffolding to an Auto-
mated Reading Tutor that Listens. In Proc. of Intel-
ligent Tutoring Systems.
V. Aleven and C. P. Rose. 2003. Proc. of the AIED 2003
Workshop on Tutorial Dialogue Systems: With a View
toward the Classroom.
V. Aleven, O. Popescu, and K. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In J. D. Moore, C. L. Redfield, and W. L. John-
son, editors, Proc. of Artificial Intelligence in Educa-
tion, pages 246–255.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. of ICSLP.
A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth.
2003. How to find trouble in communication. Speech
Communication, 40:117–143.
M. Chi, N. De Leeuw, M. Chiu, and C. Lavancher.
1994. Eliciting self-explanations improves under-
standing. Cognitive Science, 18:439–477.
L. Devillers, L. Lamel, and I. Vasilescu. 2003. Emotion
detection in task-oriented spoken dialogs. In Proc. of
ICME.
M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass,
Y. Lee, L. Shim, C. Woo, Y. Zhang, Y. Zhou,
J. Michaeland, and Allen A. Rovick. 2001. Circsim-
tutor: An Intelligent Tutoring System Using Natural
Language Dialogue. In Proc. Midwest AI and Cogni-
tive Science Conference.
K. Forbes-Riley and D. Litman. 2004. Predicting
emotion in spoken dialogue from multiple knowledge
sources. In Proc. Human Language Technology Con-
ference and North American Chapter of the Associa-
tion for Computational Linguistics.
J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proc. SIGdial Workshop
on Discourse and Dialogue.
A. Graesser, N. Person, and D. Harter et al. 2001. Teach-
ing tactics and dialog in Autotutor. International Jour-
nal of Artificial Intelligence in Education.
R. Hausmann and M. Chi. 2002. Can a computer inter-
face support self-explaining? The International Jour-
nal of Cognitive Technology, 7(1).
P. Jordan, M. Makatchev, and K. VanLehn. 2003. Ab-
ductive theorem proving for analyzing student expla-
nations. In Proc. Artificial Intelligence in Education.
C.M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. of ICSLP.
D. J. Litman and K. Forbes-Riley. 2004. Annotating stu-
dent emotional states in spoken tutoring dialogues. In
Proc. SIGdial Workshop on Discourse and Dialogue.
D. J. Litman and S. Pan. 2002. Designing and evaluating
an adaptive spoken dialogue system. User Modeling
and User-Adapted Interaction, 12.
R. Moreno, R.E. Mayer, H. A. Spires, and J. C. Lester.
2001. The case for social agency in computer-based
teaching: Do students learn more deeply when they
interact with animated pedagogical agents. Cognition
and Instruction, 19(2):177–213.
J. Mostow and G. Aist. 2001. Evaluating tutors that lis-
ten: An overview of Project LISTEN. In K. Forbus and
P. Feltovich, editors, Smart Machines in Education.
J. Rickel and W. L. Johnson. 2000. Task-oriented col-
laboration with embodied agents in virtual worlds. In
J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, ed-
itors, Embodied Conversational Agents.
C. P. Ros´e, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive conceptual
tutoringin Atlas-Andes. In Proc. Artificial Intelligence
in Education.
C. P. Ros´e, D. Litman, D. Bhembe, K. Forbes, S. Silli-
man, R. Srivastava, and K. VanLehn. 2003. A com-
parison of tutor and student behavior in speech versus
text based tutoring. In Proc. HLT/NAACL Workshop:
Building Educational Applications Using NLP.
C. P. Ros´e. 2000. A framework for robust sentence level
interpretation. In Proc. North American Chapter of the
Association for Computational Lingusitics.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice signa-
tures. In Proc. of Automatic Speech Recognition and
Understanding.
K. VanLehn, P. W. Jordan, C. Ros´e, D. Bhembe,
M. B¨ottner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intelligent Tutoring Systems.
C. Zinn, J. D. Moore, and M. G. Core. 2002. A 3-tier
planning architecture for managing tutorial dialogue.
In Proc. Intelligent Tutoring Systems.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823844">
<title confidence="0.999551">ITSPOKE: An Intelligent Tutoring Spoken Dialogue System</title>
<author confidence="0.999989">Diane J Litman Scott Silliman</author>
<affiliation confidence="0.999657">University of Pittsburgh University of Department of Computer Science &amp; Learning Research and Development Center</affiliation>
<address confidence="0.916277">Learning Research and Development Center Pittsburgh PA, 15260, USA PA, 15260, USA</address>
<email confidence="0.99982">litman@cs.pitt.edu</email>
<abstract confidence="0.998742666666667">ITSPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its “back-end”. A student first types a natural language answer to a qualitative physics problem. ITSPOKE then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>B Kort</author>
<author>R Reilly</author>
<author>J Mostow</author>
<author>R Picard</author>
</authors>
<title>Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens.</title>
<date>2002</date>
<booktitle>In Proc. of Intelligent Tutoring Systems.</booktitle>
<contexts>
<context position="2014" citStr="Aist et al., 2002" startWordPosition="297" endWordPosition="300">improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (Hausmann and Chi, 2002). In human-computer tutoring, the use of an interactive pedagogical agent that communicates using speech rather than text output improves student learning, while the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Joh</context>
</contexts>
<marker>Aist, Kort, Reilly, Mostow, Picard, 2002</marker>
<rawString>G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Picard. 2002. Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens. In Proc. of Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>C P Rose</author>
</authors>
<date>2003</date>
<booktitle>Proc. of the AIED 2003 Workshop on Tutorial Dialogue Systems: With a View toward the Classroom.</booktitle>
<contexts>
<context position="972" citStr="Aleven and Rose, 2003" startWordPosition="133" endWordPosition="136">SPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its “back-end”. A student first types a natural language answer to a qualitative physics problem. ITSPOKE then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors. 1 Introduction The development of computational tutorial dialogue systems has become more and more prevalent (Aleven and Rose, 2003), as one method of attempting to close the performance gap between human and computer tutors. While many such systems have yielded successful evaluations with students, most are currently text-based (Evens et al., 2001; Aleven et al., 2001; Zinn et al., 2002; VanLehn et al., 2002). There is reason to believe that speechbased tutorial dialogue systems could be even more effective. Spontaneous self-explanation by students improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (</context>
</contexts>
<marker>Aleven, Rose, 2003</marker>
<rawString>V. Aleven and C. P. Rose. 2003. Proc. of the AIED 2003 Workshop on Tutorial Dialogue Systems: With a View toward the Classroom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>O Popescu</author>
<author>K Koedinger</author>
</authors>
<title>Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor.</title>
<date>2001</date>
<booktitle>Proc. of Artificial Intelligence in Education,</booktitle>
<pages>246--255</pages>
<editor>In J. D. Moore, C. L. Redfield, and W. L. Johnson, editors,</editor>
<contexts>
<context position="1211" citStr="Aleven et al., 2001" startWordPosition="171" endWordPosition="174">o provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors. 1 Introduction The development of computational tutorial dialogue systems has become more and more prevalent (Aleven and Rose, 2003), as one method of attempting to close the performance gap between human and computer tutors. While many such systems have yielded successful evaluations with students, most are currently text-based (Evens et al., 2001; Aleven et al., 2001; Zinn et al., 2002; VanLehn et al., 2002). There is reason to believe that speechbased tutorial dialogue systems could be even more effective. Spontaneous self-explanation by students improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (Hausmann and Chi, 2002). In human-computer tutoring, the use of an interactive pedagogical agent that communicates using speech rather than text output improves student learning, while the visual presence or absence of the agent does not i</context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2001</marker>
<rawString>V. Aleven, O. Popescu, and K. Koedinger. 2001. Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor. In J. D. Moore, C. L. Redfield, and W. L. Johnson, editors, Proc. of Artificial Intelligence in Education, pages 246–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="2312" citStr="Ang et al., 2002" startWordPosition="345" endWordPosition="348">ech rather than text output improves student learning, while the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOK</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>K Fischer</author>
<author>R Huber</author>
<author>J Spilker</author>
<author>E Noth</author>
</authors>
<title>How to find trouble in communication.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>40--117</pages>
<contexts>
<context position="2353" citStr="Batliner et al., 2003" startWordPosition="353" endWordPosition="356"> student learning, while the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue sy</context>
</contexts>
<marker>Batliner, Fischer, Huber, Spilker, Noth, 2003</marker>
<rawString>A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth. 2003. How to find trouble in communication. Speech Communication, 40:117–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chi</author>
<author>N De Leeuw</author>
<author>M Chiu</author>
<author>C Lavancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<pages>18--439</pages>
<marker>Chi, De Leeuw, Chiu, Lavancher, 1994</marker>
<rawString>M. Chi, N. De Leeuw, M. Chiu, and C. Lavancher. 1994. Eliciting self-explanations improves understanding. Cognitive Science, 18:439–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Devillers</author>
<author>L Lamel</author>
<author>I Vasilescu</author>
</authors>
<title>Emotion detection in task-oriented spoken dialogs.</title>
<date>2003</date>
<booktitle>In Proc. of ICME.</booktitle>
<contexts>
<context position="2377" citStr="Devillers et al., 2003" startWordPosition="357" endWordPosition="360">e the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-</context>
</contexts>
<marker>Devillers, Lamel, Vasilescu, 2003</marker>
<rawString>L. Devillers, L. Lamel, and I. Vasilescu. 2003. Emotion detection in task-oriented spoken dialogs. In Proc. of ICME.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evens</author>
<author>S Brandle</author>
<author>R Chang</author>
<author>R Freedman</author>
<author>M Glass</author>
<author>Y Lee</author>
<author>L Shim</author>
<author>C Woo</author>
<author>Y Zhang</author>
<author>Y Zhou</author>
<author>J Michaeland</author>
<author>Allen A Rovick</author>
</authors>
<title>Circsimtutor: An Intelligent Tutoring System Using Natural Language Dialogue.</title>
<date>2001</date>
<booktitle>In Proc. Midwest AI and Cognitive Science Conference.</booktitle>
<contexts>
<context position="1190" citStr="Evens et al., 2001" startWordPosition="167" endWordPosition="170"> a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors. 1 Introduction The development of computational tutorial dialogue systems has become more and more prevalent (Aleven and Rose, 2003), as one method of attempting to close the performance gap between human and computer tutors. While many such systems have yielded successful evaluations with students, most are currently text-based (Evens et al., 2001; Aleven et al., 2001; Zinn et al., 2002; VanLehn et al., 2002). There is reason to believe that speechbased tutorial dialogue systems could be even more effective. Spontaneous self-explanation by students improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (Hausmann and Chi, 2002). In human-computer tutoring, the use of an interactive pedagogical agent that communicates using speech rather than text output improves student learning, while the visual presence or absence of</context>
</contexts>
<marker>Evens, Brandle, Chang, Freedman, Glass, Lee, Shim, Woo, Zhang, Zhou, Michaeland, Rovick, 2001</marker>
<rawString>M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass, Y. Lee, L. Shim, C. Woo, Y. Zhang, Y. Zhou, J. Michaeland, and Allen A. Rovick. 2001. Circsimtutor: An Intelligent Tutoring System Using Natural Language Dialogue. In Proc. Midwest AI and Cognitive Science Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Predicting emotion in spoken dialogue from multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In Proc. Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3275" citStr="Forbes-Riley and Litman, 2004" startWordPosition="492" endWordPosition="495">ere has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoringsystem (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the recognition of pedagogically useful information such as student emotion (Forbes-Riley and Litman, 2004; Litman and Forbes-Riley, 2004), and whether speech can improve the performance evaluations of dialogue tutoring systems (e.g., as measured by learning gains, efficiency, usability, etc.) (Ros´e et al., 2003). 2 Application Description ITSPOKE is a speech-enabled version of the Why2- Atlas (VanLehn et al., 2002) text-based dialogue tutoring system. As in Why2-Atlas, a student first types a natural language answer to a qualitative physics problem. In ITSPOKE, however, the system engages the student in a spoken dialogue to correct misconceptions and elicit more complete explanations. Consider t</context>
</contexts>
<marker>Forbes-Riley, Litman, 2004</marker>
<rawString>K. Forbes-Riley and D. Litman. 2004. Predicting emotion in spoken dialogue from multiple knowledge sources. In Proc. Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fry</author>
<author>M Ginzton</author>
<author>S Peters</author>
<author>B Clark</author>
<author>H Pon-Barry</author>
</authors>
<title>Automated tutoring dialogues for training in shipboard damage control.</title>
<date>2001</date>
<booktitle>In Proc. SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2575" citStr="Fry et al., 2001" startWordPosition="388" endWordPosition="391"> and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoringsystem (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the re</context>
</contexts>
<marker>Fry, Ginzton, Peters, Clark, Pon-Barry, 2001</marker>
<rawString>J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry. 2001. Automated tutoring dialogues for training in shipboard damage control. In Proc. SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graesser</author>
<author>N Person</author>
<author>D Harter</author>
</authors>
<title>Teaching tactics and dialog in Autotutor.</title>
<date>2001</date>
<journal>International Journal of Artificial Intelligence in Education.</journal>
<contexts>
<context position="2598" citStr="Graesser et al., 2001" startWordPosition="392" endWordPosition="395"> student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoringsystem (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the recognition of pedagogica</context>
</contexts>
<marker>Graesser, Person, Harter, 2001</marker>
<rawString>A. Graesser, N. Person, and D. Harter et al. 2001. Teaching tactics and dialog in Autotutor. International Journal of Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hausmann</author>
<author>M Chi</author>
</authors>
<title>Can a computer interface support self-explaining?</title>
<date>2002</date>
<journal>The International Journal of Cognitive Technology,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1595" citStr="Hausmann and Chi, 2002" startWordPosition="230" endWordPosition="233">, as one method of attempting to close the performance gap between human and computer tutors. While many such systems have yielded successful evaluations with students, most are currently text-based (Evens et al., 2001; Aleven et al., 2001; Zinn et al., 2002; VanLehn et al., 2002). There is reason to believe that speechbased tutorial dialogue systems could be even more effective. Spontaneous self-explanation by students improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (Hausmann and Chi, 2002). In human-computer tutoring, the use of an interactive pedagogical agent that communicates using speech rather than text output improves student learning, while the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a</context>
</contexts>
<marker>Hausmann, Chi, 2002</marker>
<rawString>R. Hausmann and M. Chi. 2002. Can a computer interface support self-explaining? The International Journal of Cognitive Technology, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>M Makatchev</author>
<author>K VanLehn</author>
</authors>
<title>Abductive theorem proving for analyzing student explanations.</title>
<date>2003</date>
<booktitle>In Proc. Artificial Intelligence in Education.</booktitle>
<contexts>
<context position="5598" citStr="Jordan et al., 2003" startWordPosition="866" endWordPosition="869">or continue with another round of spoken dialogue interaction and typed essay revision. Upon completing the problem, the student is presented with an example of an ideal essay. Another excerpt with ITSPOKE is shown in Figure 2. &apos;The “Tell Tutor” box is used for typed student login only. 3 System Architecture The architecture of ITSPOKE is shown in Figure 3. Student speech is digitized from microphone input and sent to the Sphinx2 automatic speech recognizer. Sphinx2’s best hypothesis is then sent to Why2-Atlas for syntactic and semantic analysis (Ros´e, 2000), discourse and domain processing (Jordan et al., 2003), and finite-state dialogue management (Ros´e et al., 2001). The tutor’s text output is sent to the Cepstral text-to-speech system, and played through a speaker or headphone. ITSPOKE is implemented in Python and acts as a proxy server between the Why2-Atlas server and client. ITSPOKE monitors the XML being sent between the two and decides what text should be spoken and when to listen. After speech recognition, ITSPOKE submits what it thinks the student said to Why2-Atlas (as if the student had typed it). Sphinx2’s C API is tied into Python by using SWIG to generate a Python wrapper API. Cepstr</context>
</contexts>
<marker>Jordan, Makatchev, VanLehn, 2003</marker>
<rawString>P. Jordan, M. Makatchev, and K. VanLehn. 2003. Abductive theorem proving for analyzing student explanations. In Proc. Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
</authors>
<title>Combining acoustic and language information for emotion recognition.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="2330" citStr="Lee et al., 2002" startWordPosition="349" endWordPosition="352">xt output improves student learning, while the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system</context>
</contexts>
<marker>Lee, Narayanan, Pieraccini, 2002</marker>
<rawString>C.M. Lee, S. Narayanan, and R. Pieraccini. 2002. Combining acoustic and language information for emotion recognition. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>K Forbes-Riley</author>
</authors>
<title>Annotating student emotional states in spoken tutoring dialogues.</title>
<date>2004</date>
<booktitle>In Proc. SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="3307" citStr="Litman and Forbes-Riley, 2004" startWordPosition="496" endWordPosition="500"> of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoringsystem (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the recognition of pedagogically useful information such as student emotion (Forbes-Riley and Litman, 2004; Litman and Forbes-Riley, 2004), and whether speech can improve the performance evaluations of dialogue tutoring systems (e.g., as measured by learning gains, efficiency, usability, etc.) (Ros´e et al., 2003). 2 Application Description ITSPOKE is a speech-enabled version of the Why2- Atlas (VanLehn et al., 2002) text-based dialogue tutoring system. As in Why2-Atlas, a student first types a natural language answer to a qualitative physics problem. In ITSPOKE, however, the system engages the student in a spoken dialogue to correct misconceptions and elicit more complete explanations. Consider the screenshot shown in Figure 1.</context>
</contexts>
<marker>Litman, Forbes-Riley, 2004</marker>
<rawString>D. J. Litman and K. Forbes-Riley. 2004. Annotating student emotional states in spoken tutoring dialogues. In Proc. SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>S Pan</author>
</authors>
<title>Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction,</title>
<date>2002</date>
<pages>12</pages>
<marker>Litman, Pan, 2002</marker>
<rawString>D. J. Litman and S. Pan. 2002. Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moreno</author>
<author>R E Mayer</author>
<author>H A Spires</author>
<author>J C Lester</author>
</authors>
<title>The case for social agency in computer-based teaching: Do students learn more deeply when they interact with animated pedagogical agents. Cognition and Instruction,</title>
<date>2001</date>
<contexts>
<context position="1850" citStr="Moreno et al., 2001" startWordPosition="270" endWordPosition="273">; VanLehn et al., 2002). There is reason to believe that speechbased tutorial dialogue systems could be even more effective. Spontaneous self-explanation by students improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (Hausmann and Chi, 2002). In human-computer tutoring, the use of an interactive pedagogical agent that communicates using speech rather than text output improves student learning, while the visual presence or absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several proj</context>
</contexts>
<marker>Moreno, Mayer, Spires, Lester, 2001</marker>
<rawString>R. Moreno, R.E. Mayer, H. A. Spires, and J. C. Lester. 2001. The case for social agency in computer-based teaching: Do students learn more deeply when they interact with animated pedagogical agents. Cognition and Instruction, 19(2):177–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mostow</author>
<author>G Aist</author>
</authors>
<title>Evaluating tutors that listen: An overview of Project LISTEN.</title>
<date>2001</date>
<booktitle>Smart Machines in Education.</booktitle>
<editor>In K. Forbus and P. Feltovich, editors,</editor>
<contexts>
<context position="2557" citStr="Mostow and Aist, 2001" startWordPosition="384" endWordPosition="387">ncreased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoringsystem (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used</context>
</contexts>
<marker>Mostow, Aist, 2001</marker>
<rawString>J. Mostow and G. Aist. 2001. Evaluating tutors that listen: An overview of Project LISTEN. In K. Forbus and P. Feltovich, editors, Smart Machines in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rickel</author>
<author>W L Johnson</author>
</authors>
<title>Task-oriented collaboration with embodied agents in virtual worlds.</title>
<date>2000</date>
<journal>Embodied Conversational Agents.</journal>
<editor>In J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors,</editor>
<contexts>
<context position="2625" citStr="Rickel and Johnson, 2000" startWordPosition="396" endWordPosition="399"> et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physics tutoringsystem (VanLehn et al., 2002) as its “back-end.” We are using ITSPOKE as a platform for examining whether acoustic-prosodic information can be used to improve the recognition of pedagogically useful information such</context>
</contexts>
<marker>Rickel, Johnson, 2000</marker>
<rawString>J. Rickel and W. L. Johnson. 2000. Task-oriented collaboration with embodied agents in virtual worlds. In J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors, Embodied Conversational Agents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>P Jordan</author>
<author>M Ringenberg</author>
<author>S Siler</author>
<author>K VanLehn</author>
<author>A Weinstein</author>
</authors>
<title>Interactive conceptual tutoringin Atlas-Andes.</title>
<date>2001</date>
<booktitle>In Proc. Artificial Intelligence in Education.</booktitle>
<marker>Ros´e, Jordan, Ringenberg, Siler, VanLehn, Weinstein, 2001</marker>
<rawString>C. P. Ros´e, P. Jordan, M. Ringenberg, S. Siler, K. VanLehn, and A. Weinstein. 2001. Interactive conceptual tutoringin Atlas-Andes. In Proc. Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>D Litman</author>
<author>D Bhembe</author>
<author>K Forbes</author>
<author>S Silliman</author>
<author>R Srivastava</author>
<author>K VanLehn</author>
</authors>
<title>A comparison of tutor and student behavior in speech versus text based tutoring.</title>
<date>2003</date>
<booktitle>In Proc. HLT/NAACL Workshop: Building Educational Applications Using NLP.</booktitle>
<marker>Ros´e, Litman, Bhembe, Forbes, Silliman, Srivastava, VanLehn, 2003</marker>
<rawString>C. P. Ros´e, D. Litman, D. Bhembe, K. Forbes, S. Silliman, R. Srivastava, and K. VanLehn. 2003. A comparison of tutor and student behavior in speech versus text based tutoring. In Proc. HLT/NAACL Workshop: Building Educational Applications Using NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
</authors>
<title>A framework for robust sentence level interpretation.</title>
<date>2000</date>
<booktitle>In Proc. North American Chapter of the Association for Computational Lingusitics.</booktitle>
<marker>Ros´e, 2000</marker>
<rawString>C. P. Ros´e. 2000. A framework for robust sentence level interpretation. In Proc. North American Chapter of the Association for Computational Lingusitics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Shafran</author>
<author>M Riley</author>
<author>M Mohri</author>
</authors>
<title>Voice signatures.</title>
<date>2003</date>
<booktitle>In Proc. of Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="2400" citStr="Shafran et al., 2003" startWordPosition="361" endWordPosition="364"> absence of the agent does not impact performance (Moreno et al., 2001). In addition, it has been hypothesized that the success of computer tutors could be increased by recognizing and responding to student emotion. (Aist et al., 2002) have shown that adding emotional processing to a dialogue-based reading tutor increases student persistence. Information in the speech signal such as prosody has been shown to be a rich source of information for predicting emotional states in other types of dialogue interactions (Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003). With advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems (Mostow and Aist, 2001; Fry et al., 2001; Graesser et al., 2001; Rickel and Johnson, 2000). However, to date there has been little examination of the ramifications of using a spoken modality for dialogue tutoring. To assess the impact and evaluate the utility of adding spoken language capabilities to dialogue tutoring systems, we have built ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), a spoken dialogue system that uses the Why2-Atlas conceptual physic</context>
</contexts>
<marker>Shafran, Riley, Mohri, 2003</marker>
<rawString>I. Shafran, M. Riley, and M. Mohri. 2003. Voice signatures. In Proc. of Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>P W Jordan</author>
<author>C Ros´e</author>
<author>D Bhembe</author>
<author>M B¨ottner</author>
<author>A Gaydos</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>M Ringenberg</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>R Wilson</author>
</authors>
<title>The architecture of Why2-Atlas: A coach for qualitative physics essay writing.</title>
<date>2002</date>
<booktitle>In Proc. Intelligent Tutoring Systems.</booktitle>
<marker>VanLehn, Jordan, Ros´e, Bhembe, B¨ottner, Gaydos, Makatchev, Pappuswamy, Ringenberg, Roque, Siler, Srivastava, Wilson, 2002</marker>
<rawString>K. VanLehn, P. W. Jordan, C. Ros´e, D. Bhembe, M. B¨ottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zinn</author>
<author>J D Moore</author>
<author>M G Core</author>
</authors>
<title>A 3-tier planning architecture for managing tutorial dialogue.</title>
<date>2002</date>
<booktitle>In Proc. Intelligent Tutoring Systems.</booktitle>
<contexts>
<context position="1230" citStr="Zinn et al., 2002" startWordPosition="175" endWordPosition="178">d correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors. 1 Introduction The development of computational tutorial dialogue systems has become more and more prevalent (Aleven and Rose, 2003), as one method of attempting to close the performance gap between human and computer tutors. While many such systems have yielded successful evaluations with students, most are currently text-based (Evens et al., 2001; Aleven et al., 2001; Zinn et al., 2002; VanLehn et al., 2002). There is reason to believe that speechbased tutorial dialogue systems could be even more effective. Spontaneous self-explanation by students improves learning gains during human-human tutoring (Chi et al., 1994), and spontaneous self-explanation occurs more frequently in spoken tutoring than in text-based tutoring (Hausmann and Chi, 2002). In human-computer tutoring, the use of an interactive pedagogical agent that communicates using speech rather than text output improves student learning, while the visual presence or absence of the agent does not impact performance (</context>
</contexts>
<marker>Zinn, Moore, Core, 2002</marker>
<rawString>C. Zinn, J. D. Moore, and M. G. Core. 2002. A 3-tier planning architecture for managing tutorial dialogue. In Proc. Intelligent Tutoring Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>