<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.990654">
Handling phrase reorderings for machine translation
</title>
<author confidence="0.998835">
Yizhao Ni, Craig J. Saunders∗, Sandor Szedmak and Mahesan Niranjan
</author>
<affiliation confidence="0.991016333333333">
ISIS Group
School of Electronics and Computer Science
University of Southampton
</affiliation>
<address confidence="0.944369">
Southampton, SO17 1BJ
United Kingdom
</address>
<email confidence="0.905439">
yn05r@ecs.soton.ac.uk, craig.saunders@xrce.xerox.com,
{ss03v,mn}@ecs.soton.ac.uk
</email>
<sectionHeader confidence="0.993734" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941">
We propose a distance phrase reordering
model (DPR) for statistical machine trans-
lation (SMT), where the aim is to cap-
ture phrase reorderings using a structure
learning framework. On both the reorder-
ing classification and a Chinese-to-English
translation task, we show improved perfor-
mance over a baseline SMT system.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999310875">
Word or phrase reordering is a common prob-
lem in bilingual translations arising from dif-
ferent grammatical structures. For example,
in Chinese the expression of the date follows
“Year/Month/Date”, while when translated into
English, “Month/Date/Year” is often the correct
grammar. In general, the fluency of machine trans-
lations can be greatly improved by obtaining the
correct word order in the target language.
As the reordering problem is computation-
ally expensive, a word distance-based reordering
model is commonly used among SMT decoders
(Koehn, 2004), in which the costs of phrase move-
ments are linearly proportional to the reordering
distance. Although this model is simple and effi-
cient, the content independence makes it difficult
to capture many distant phrase reordering caused
by the grammar. To tackle the problem, (Koehn
et al., 2005) developed a lexicalized reordering
model that attempted to learn the phrase reorder-
ing based on content. The model learns the local
orientation (e.g. “monotone” order or “switching”
order) probabilities for each bilingual phrase pair
using Maximum Likelihood Estimation (MLE).
These orientation probabilities are then integrated
into an SMT decoder to help finding a Viterbi–best
local orientation sequence. Improvements by this
∗the author’s new address: Xerox Research Centre Europe
6, Chemin de Maupertuis, 38240 Meylan France.
model have been reported in (Koehn et al., 2005).
However, the amount of the training data for each
bilingual phrase is so small that the model usually
suffers from the data sparseness problem. Adopt-
ing the idea of predicting the orientation, (Zens
and Ney, 2006) started exploiting the context and
grammar which may relate to phrase reorderings.
In general, a Maximum Entropy (ME) framework
is utilized and the feature parameters are tuned
by a discriminative model. However, the training
times for ME models are usually relatively high,
especially when the output classes (i.e. phrase re-
ordering orientations) increase.
Alternative to the ME framework, we propose
using a classification scheme here for phrase re-
orderings and employs a structure learning frame-
work. Our results confirm that this distance phrase
reordering model (DPR) can lead to improved per-
formance with a reasonable time efficiency.
</bodyText>
<figureCaption confidence="0.997117">
Figure 1: The phrase reordering distance d.
</figureCaption>
<sectionHeader confidence="0.629191" genericHeader="method">
2 Distance phrase reordering (DPR)
</sectionHeader>
<bodyText confidence="0.999900333333333">
We adopt a discriminative model to capture the
frequent distant reordering which we call distance
phrase reordering. An ideal model would consider
every position as a class and predict the position of
the next phrase, although in practice we must con-
sider a limited set of classes (denoted as Q). Using
the reordering distance d (see Figure 1) as defined
by (Koehn et al., 2005), we extend the two class
model in (Xiong et al., 2006) to multiple classes
(e.g. three–class setup Q = {d &lt; 0, d = 0, d &gt;
0}; or five–class setup Q = {d ≤ −5, −5 &lt; d &lt;
0, d = 0, 0 &lt; d &lt; 5, d ≥ 5}). Note that the more
</bodyText>
<page confidence="0.974902">
241
</page>
<note confidence="0.925438">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 241–244,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997910333333333">
classes it has, the closer it is to the ideal model, but
the smaller amount of training samples it would
receive for each class.
</bodyText>
<subsectionHeader confidence="0.9512305">
2.1 Reordering Probability model and
training algorithm
</subsectionHeader>
<bodyText confidence="0.988621333333333">
Given a (source, target) phrase pair ( fj, ¯ei) with
fj = [fjl, ... , fjr] and ¯ei = [eil, ... , eir], the dis-
tance phrase reordering probability has the form
</bodyText>
<equation confidence="0.9983545">
p(o�¯fj, ei) :=hll(wTo φ(�j,((ei)) (1)
&amp;0∈Ω hll(wo0φlfj, ei))
</equation>
<bodyText confidence="0.928752444444444">
where wo = [wo,0, ... , wo,dim(φ)]T is the weight
vector measuring features’ contribution to an ori-
entation o E Q, φ is the feature vector and h is a
pre-defined monotonic function. As the reorder-
ing orientations tend to be interdependent, learn-
ing {wo}o∈Ω is more than a multi–class classifi-
cation problem. Take the five–class setup for ex-
ample, if an example in class d &lt; −5 is classified
in class −5 &lt; d &lt; 5, intuitively the loss should be
smaller than when it is classified in class d &gt; 5.
The output (orientation) domain has an inherent
structure and the model should respect it. Hence,
we utilize the structure learning framework pro-
posed in (Taskar et al., 2003) which is equivalent
to minimising the sum of the classification errors
λ
ρ(o, ¯fnj , ¯eni , w) + 2 �w�2 (2)
where λ &gt; 0 is a regularisation parameter,
</bodyText>
<equation confidence="0.968365333333333">
ρ(o, ¯fj, ¯ei, w) = max{0, maxo06=o[o(o, o0)+
wTo0φ( ¯fj, ¯ei)] − wTo φ(¯fj, ¯ei)}
is a structured margin loss function with
o(o, o0) = { 0 if o = o0
0.5 if o and o0 are close in Q
1 else
</equation>
<bodyText confidence="0.997902">
measuring the distance between pseudo orienta-
tion o0 and the true one o. Theoretically, this loss
requires that orientation o0 which are “far away”
from the true one o must be classified with a large
margin while nearby candidates are allowed to
be classified with a smaller margin. At training
time, we used a perceptron–based structure learn-
ing (PSL) algorithm to learn {wo}o∈Ω which is
shown in Table 1.
Input: The samples {o, φ( ¯fj, ¯ei)}Nn=1, step size η
</bodyText>
<equation confidence="0.733078615384615">
Initialization: k = 0; wo,k = 0 bo E Q;
Repeat
for n = 1,2,...,N do
for o0 =� o get
o∗ = arg maxo0 {o(o, o0) + wTo0,kφ(¯fj, ¯ei)}
V = maxo0 {o(o, o0) + wTo0,kφ( ¯fj, ¯ei)}
if wTo,kφ( ¯
fj, ¯ei) &lt; V then
wo,k+1 = wo,k + ηφ( fj, ¯ei)
wo∗,k+1 = wo∗,k − ηφ( fj, ¯ei)
k = k + 1
until converge
Output: wo,k+1 bo E Q
</equation>
<tableCaption confidence="0.994996">
Table 1: Perceptron-based structure learning.
</tableCaption>
<bodyText confidence="0.749284">
phrase environment (see Table 2), where given a
sequence s (e.g. s = [fjl−z, ... , fjl]), the features
</bodyText>
<equation confidence="0.559445333333333">
selected are φu(s|u|
p ) = δ(s|u|
p , u), with the
</equation>
<bodyText confidence="0.7722255">
indicator function δ(·, ·), p = {jl − z, ... , jr + z}
and string s|u|
</bodyText>
<equation confidence="0.500247">
p = [fp, ... , fp+|u|]. Hence, the
</equation>
<bodyText confidence="0.9822596">
phrase features are distinguished by both the
content u and its start position p. For exam-
ple, the left side context features for phrase
pair (xiang gang, Hong Kong) in Figure 1 are
{δ(s1 0, “zhou”), δ(s1 1, “liu”), δ(s20, “zhou liu”)}.
As required by the algorithm, we then normalise
the feature vector
To train the DPR model, the training samples
¯φt = φt
kφk.
{( ¯fnj , ¯eni )}Nn=1 are extracted following the phrase
pair extraction procedure in (Koehn et al., 2005)
and form the sample pool, where the instances
having the same source phrase ¯fj are considered
to be from the same cluster. A sub-DPR model is
then trained for each cluster using the PSL algo-
rithm. During the decoding, the DPR model finds
the corresponding sub-DPR model for a source
phrase ¯fj and generates the reordering probability
for each orientation class using equation (1).
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.996758833333333">
Experiments used the Hong Kong Laws corpus1
(Chinese-to-English), where sentences of lengths
between 1 and 100 words were extracted and the
ratio of source/target lengths was no more than
2 : 1. The training and test sizes are 50, 290 and
1, 000 respectively.
</bodyText>
<equation confidence="0.606434666666667">
1
min
W
N
N
n=1
</equation>
<subsectionHeader confidence="0.586249">
2.1.1 Feature Extraction and Application
</subsectionHeader>
<bodyText confidence="0.916732">
Following (Zens and Ney, 2006), we consider
different kinds of information extracted from the
</bodyText>
<footnote confidence="0.9976495">
1This bilingual Chinese-English corpus consists of mainly
legal and documentary texts from Hong Kong. The corpus is
aligned at the sentence level which are collected and revised
manually by the author. The full corpus will be released soon.
</footnote>
<page confidence="0.989372">
242
</page>
<table confidence="0.9991054">
Features for source phrase ¯fj Features for target phrase ¯ei
Context Source word n–grams within a window Target word n–grams
(length z) around the phrase edge [jl] and [jr] of the phrase [ei,, ... , ei,.]
Syntactic Source word class tag n-grams within a Target word class tag
window (length z) around the phrase edge [jl] and [jr] n-grams of the phrase [ei,, ... , ei,.]
</table>
<tableCaption confidence="0.999909">
Table 2: The environment for the feature extraction. The word class tags are provided by MOSES.
</tableCaption>
<subsectionHeader confidence="0.948233">
3.1 Classification Experiments
</subsectionHeader>
<figureCaption confidence="0.98127">
Figure 2: Classification results with respect to d.
</figureCaption>
<bodyText confidence="0.9999736">
We used GIZA++ to produce alignments, en-
abling us to compare using a DPR model against
a baseline lexicalized reordering model (Koehn et
al., 2005) that uses MLE orientation prediction
and a discriminative model (Zens and Ney, 2006)
that utilizes an ME framework. Two orientation
classification tasks are carried out: one with three–
class setup and one with five–class setup. We
discarded points that had long distance reorder-
ing (|d |&gt; 15) to avoid some alignment errors
cause by GIZA++ (representing less than 5% of
the data). This resulted in data sizes shown in Ta-
ble 3. The classification performance is measured
by an overall precision across all classes and the
class-specific F1 measures and the experiments
are are repeated three times to asses variance.
Table 4 depicts the classification results ob-
tained, where we observed consistent improve-
ments for the DPR model over the baseline and
the ME models. When the number of classes
(orientations) increases, the average relative im-
provements of DPR for the switching classes
(i.e. d =� 0) increase from 41.6% to 83.2% over
the baseline and from 7.8% to 14.2% over the ME
model, which implies a potential benefit of struc-
ture learning. Figure 2 further demonstrate the av-
erage accuracy for each reordering distance d. It
shows that even for long distance reordering, the
DPR model still performs well, while the MLE
baseline usually performs badly (more than half
examples are classified incorrectly). With so many
classification errors, the effect of this baseline in
an SMT system is in doubt, even with a powerful
language model. At training time, training a DPR
model is much faster than training an ME model
(both algorithms are coded in Python), especially
when the number of classes increase. This is be-
cause the generative iterative scaling algorithm of
an ME model requires going through all examples
twice at each round: one is for updating the condi-
tional distributions p(o |¯fj, ¯ei) and the other is for
updating {wo}o∈Ω. Alternatively, the PSL algo-
rithm only goes through all examples once at each
round, making it faster and more applicable for
larger data sets.
</bodyText>
<subsectionHeader confidence="0.99963">
3.2 Translation experiments
</subsectionHeader>
<bodyText confidence="0.9999927">
We now test the effect of the DPR model in an
MT system, using MOSES (Koehn et al., 2005)
as a baseline system. To keep the comparison
fair, our MT system just replaces MOSES’s re-
ordering models with DPR while sharing all other
models (i.e. phrase translation probability model,
4-gram language model (A. Stolcke, 2002) and
beam search decoder). As in classification exper-
iments the three-class setup shows better results
in switching classes, we use this setup in DPR. In
detail, all consistent phrases are extracted from the
training sentence pairs and form the sample pool.
The three-class DPR model is then trained by the
PSL algorithm and the function h(z) = exp(z) is
applied to equation (1) to transform the prediction
scores. Contrasting the direct use of the reorder-
ing probabilities used in (Zens and Ney, 2006),
we utilize the probabilities to adjust the word
distance–based reordering cost, where the reorder-
ing cost of a sentence is computed as Po(f, e) =
</bodyText>
<page confidence="0.996015">
243
</page>
<table confidence="0.99882175">
Settings three–class setup five–class setup
Classes d &lt; 0 d = 0 d &gt; 0 d ≤ −5 −5 &lt; d &lt; 0 d = 0 0 &lt; d &lt; 5 d &gt; 5
Train 181, 583 755, 854 181, 279 82, 677 98, 907 755, 854 64, 881 116, 398
Test 5, 025 21, 106 5, 075 2, 239 2, 786 21, 120 1, 447 3, 629
</table>
<tableCaption confidence="0.982635">
Table 3: Data statistics for the classification experiments.
</tableCaption>
<table confidence="0.9997703">
System three–class setup task
Precision d &lt; 0 d = 0 d &gt; 0 Training time (hours)
Lexicalized 77.1 ± 0.1 55.7 ± 0.1 86.5 ± 0.1 49.2 ± 0.3 1.0
ME 83.7 ± 0.3 67.9 ± 0.3 90.8 ± 0.3 69.2 ± 0.1 58.6
DPR 86.7 ± 0.1 73.3 ± 0.1 92.5 ± 0.2 74.6 ± 0.5 27.0
System five–class setup task
Precision d ≤ −5 −5 &lt; d &lt; 0 d = 0 0 &lt; d &lt; 5 d &gt; 5 Training Time (hours)
Lexicalized 74.3 ± 0.1 44.9 ± 0.2 32.0 ± 1.5 86.4 ± 0.1 29.2 ± 1.7 46.2 ± 0.8 1.3
ME 80.0 ± 0.2 52.1 ± 0.1 54.7 ± 0.7 90.4 ± 0.2 63.9 ± 0.1 61.8 ± 0.1 83.6
DPR 84.6 ± 0.1 60.0 ± 0.7 61.4 ± 0.1 92.6 ± 0.2 75.4 ± 0.6 68.8 ± 0.5 29.2
</table>
<tableCaption confidence="0.974114">
Table 4: Overall precision and class-specific F1 scores [%] using different number of orientation classes.
Bold numbers refer to the best results.
</tableCaption>
<figure confidence="0.752579571428571">
exp{− E
m
fjm,¯eim)} with tuning parameter β.
performance as well as its time efficiency.
dm
¯
βp(o|
</figure>
<bodyText confidence="0.999682357142857">
This distance–sensitive expression is able to fill
the deficiency of the three–class setup of DPR and
is verified to produce better results. For parameter
tuning, minimum-error-rating training (F. J. Och,
2003) is used in both systems. Note that there are
7 parameters needed tuning in MOSES’s reorder-
ing models, while only 1 requires tuning in DPR.
The translation performance is evaluated by four
MT measurements used in (Koehn et al., 2005).
Table 5 shows the translation results, where we
observe consistent improvements on most evalua-
tions. Indeed both systems produced similar word
accuracy, but our MT system does better in phrase
reordering and produces more fluent translations.
</bodyText>
<sectionHeader confidence="0.998805" genericHeader="conclusions">
4 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.99974725">
We have proposed a distance phrase reordering
model using a structure learning framework. The
classification tasks have shown that DPR is bet-
ter in capturing the phrase reorderings over the
lexicalized reordering model and the ME model.
Moreover, compared with ME DPR is much faster
and more applicable to larger data sets. Transla-
tion experiments carried out on the Chinese-to-
English task show that DPR gives more fluent
translation results, which verifies its effectiveness.
For future work, we aim at improving the predic-
tion accuracy for the five-class setup using a richer
feature set before applying it to an MT system, as
DPR can be more powerful if it is able to provide
more precise phrase position for the decoder. We
will also apply DPR on a larger data set to test its
</bodyText>
<table confidence="0.9941284">
Tasks Measure MOSES DPR
BLEU [%] 44.7 ± 1.2 47.1 ± 1.3
CH–EN word accuracy 76.5 ± 0.6 76.1 ± 1.5
NIST 8.82 ± 0.11 9.04 ± 0.26
METEOR [%] 66.1 ± 0.8 66.4 ± 1.1
</table>
<tableCaption confidence="0.946985">
Table 5: Four evaluations for the MT experiments.
Bold numbers refer to the best results.
</tableCaption>
<sectionHeader confidence="0.997162" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999723863636364">
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase–based statistical machine translation models.
In Proc. of AMTA 2004, Washington DC, October.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison–
Burch, M. Osborne and D. Talbot. 2005. Ed-
inburgh system description for the 2005 IWSLT
speech translation evaluation. In Proc. of IWSLT,
Pittsburgh, PA.
F. J. Och. 2003. SRILM - An Extensible Language
Modeling Toolkit. In Proc. Intl. Conf. Spoken Lan-
guage Processing, Colorado, September.
A. Stolcke. 2002. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, Japan.
B. Taskar, C. Guestrin, and D.Koller. 2003. Max–
margin Markov networks. In Proc. NIPS, Vancou-
ver, Canada, December.
D. Xiong, Q. Liu and S. Lin. 2006. Maximum En-
tropy Based Phrase Reordering Model for Statistical
Machine Translation. In Proc. ofACL, Sydney, July.
R. Zens and H. Ney. 2006. Discriminative Reordering
Models for Statistical Machine Translation. In Proc.
ofACL, pages 55–63, New York City, June.
</reference>
<page confidence="0.998537">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.288753">
<title confidence="0.99971">Handling phrase reorderings for machine translation</title>
<author confidence="0.997827">Craig J Sandor Szedmak Ni</author>
<author confidence="0.997827">Mahesan Niranjan</author>
<affiliation confidence="0.999182666666667">ISIS Group School of Electronics and Computer Science University of Southampton</affiliation>
<address confidence="0.6031875">Southampton, SO17 1BJ United Kingdom</address>
<email confidence="0.58095">yn05r@ecs.soton.ac.uk,craig.saunders@xrce.xerox.com,</email>
<abstract confidence="0.995545333333333">We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to capture phrase reorderings using a structure learning framework. On both the reordering classification and a Chinese-to-English translation task, we show improved performance over a baseline SMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase–based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. of AMTA 2004,</booktitle>
<location>Washington DC,</location>
<contexts>
<context position="1216" citStr="Koehn, 2004" startWordPosition="168" endWordPosition="169">performance over a baseline SMT system. 1 Introduction Word or phrase reordering is a common problem in bilingual translations arising from different grammatical structures. For example, in Chinese the expression of the date follows “Year/Month/Date”, while when translated into English, “Month/Date/Year” is often the correct grammar. In general, the fluency of machine translations can be greatly improved by obtaining the correct word order in the target language. As the reordering problem is computationally expensive, a word distance-based reordering model is commonly used among SMT decoders (Koehn, 2004), in which the costs of phrase movements are linearly proportional to the reordering distance. Although this model is simple and efficient, the content independence makes it difficult to capture many distant phrase reordering caused by the grammar. To tackle the problem, (Koehn et al., 2005) developed a lexicalized reordering model that attempted to learn the phrase reordering based on content. The model learns the local orientation (e.g. “monotone” order or “switching” order) probabilities for each bilingual phrase pair using Maximum Likelihood Estimation (MLE). These orientation probabilitie</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: a beam search decoder for phrase–based statistical machine translation models. In Proc. of AMTA 2004, Washington DC, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A B Mayne</author>
<author>C Callison– Burch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1508" citStr="Koehn et al., 2005" startWordPosition="213" endWordPosition="216">h, “Month/Date/Year” is often the correct grammar. In general, the fluency of machine translations can be greatly improved by obtaining the correct word order in the target language. As the reordering problem is computationally expensive, a word distance-based reordering model is commonly used among SMT decoders (Koehn, 2004), in which the costs of phrase movements are linearly proportional to the reordering distance. Although this model is simple and efficient, the content independence makes it difficult to capture many distant phrase reordering caused by the grammar. To tackle the problem, (Koehn et al., 2005) developed a lexicalized reordering model that attempted to learn the phrase reordering based on content. The model learns the local orientation (e.g. “monotone” order or “switching” order) probabilities for each bilingual phrase pair using Maximum Likelihood Estimation (MLE). These orientation probabilities are then integrated into an SMT decoder to help finding a Viterbi–best local orientation sequence. Improvements by this ∗the author’s new address: Xerox Research Centre Europe 6, Chemin de Maupertuis, 38240 Meylan France. model have been reported in (Koehn et al., 2005). However, the amoun</context>
<context position="3402" citStr="Koehn et al., 2005" startWordPosition="507" endWordPosition="510">ture learning framework. Our results confirm that this distance phrase reordering model (DPR) can lead to improved performance with a reasonable time efficiency. Figure 1: The phrase reordering distance d. 2 Distance phrase reordering (DPR) We adopt a discriminative model to capture the frequent distant reordering which we call distance phrase reordering. An ideal model would consider every position as a class and predict the position of the next phrase, although in practice we must consider a limited set of classes (denoted as Q). Using the reordering distance d (see Figure 1) as defined by (Koehn et al., 2005), we extend the two class model in (Xiong et al., 2006) to multiple classes (e.g. three–class setup Q = {d &lt; 0, d = 0, d &gt; 0}; or five–class setup Q = {d ≤ −5, −5 &lt; d &lt; 0, d = 0, 0 &lt; d &lt; 5, d ≥ 5}). Note that the more 241 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 241–244, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP classes it has, the closer it is to the ideal model, but the smaller amount of training samples it would receive for each class. 2.1 Reordering Probability model and training algorithm Given a (source, target) phrase pair ( fj, ¯ei) with fj = [fjl,</context>
<context position="6715" citStr="Koehn et al., 2005" startWordPosition="1143" endWordPosition="1146">d are φu(s|u| p ) = δ(s|u| p , u), with the indicator function δ(·, ·), p = {jl − z, ... , jr + z} and string s|u| p = [fp, ... , fp+|u|]. Hence, the phrase features are distinguished by both the content u and its start position p. For example, the left side context features for phrase pair (xiang gang, Hong Kong) in Figure 1 are {δ(s1 0, “zhou”), δ(s1 1, “liu”), δ(s20, “zhou liu”)}. As required by the algorithm, we then normalise the feature vector To train the DPR model, the training samples ¯φt = φt kφk. {( ¯fnj , ¯eni )}Nn=1 are extracted following the phrase pair extraction procedure in (Koehn et al., 2005) and form the sample pool, where the instances having the same source phrase ¯fj are considered to be from the same cluster. A sub-DPR model is then trained for each cluster using the PSL algorithm. During the decoding, the DPR model finds the corresponding sub-DPR model for a source phrase ¯fj and generates the reordering probability for each orientation class using equation (1). 3 Experiments Experiments used the Hong Kong Laws corpus1 (Chinese-to-English), where sentences of lengths between 1 and 100 words were extracted and the ratio of source/target lengths was no more than 2 : 1. The tra</context>
<context position="8467" citStr="Koehn et al., 2005" startWordPosition="1435" endWordPosition="1438">rd n–grams within a window Target word n–grams (length z) around the phrase edge [jl] and [jr] of the phrase [ei,, ... , ei,.] Syntactic Source word class tag n-grams within a Target word class tag window (length z) around the phrase edge [jl] and [jr] n-grams of the phrase [ei,, ... , ei,.] Table 2: The environment for the feature extraction. The word class tags are provided by MOSES. 3.1 Classification Experiments Figure 2: Classification results with respect to d. We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model (Koehn et al., 2005) that uses MLE orientation prediction and a discriminative model (Zens and Ney, 2006) that utilizes an ME framework. Two orientation classification tasks are carried out: one with three– class setup and one with five–class setup. We discarded points that had long distance reordering (|d |&gt; 15) to avoid some alignment errors cause by GIZA++ (representing less than 5% of the data). This resulted in data sizes shown in Table 3. The classification performance is measured by an overall precision across all classes and the class-specific F1 measures and the experiments are are repeated three times t</context>
<context position="10574" citStr="Koehn et al., 2005" startWordPosition="1786" endWordPosition="1789">uch faster than training an ME model (both algorithms are coded in Python), especially when the number of classes increase. This is because the generative iterative scaling algorithm of an ME model requires going through all examples twice at each round: one is for updating the conditional distributions p(o |¯fj, ¯ei) and the other is for updating {wo}o∈Ω. Alternatively, the PSL algorithm only goes through all examples once at each round, making it faster and more applicable for larger data sets. 3.2 Translation experiments We now test the effect of the DPR model in an MT system, using MOSES (Koehn et al., 2005) as a baseline system. To keep the comparison fair, our MT system just replaces MOSES’s reordering models with DPR while sharing all other models (i.e. phrase translation probability model, 4-gram language model (A. Stolcke, 2002) and beam search decoder). As in classification experiments the three-class setup shows better results in switching classes, we use this setup in DPR. In detail, all consistent phrases are extracted from the training sentence pairs and form the sample pool. The three-class DPR model is then trained by the PSL algorithm and the function h(z) = exp(z) is applied to equa</context>
<context position="13035" citStr="Koehn et al., 2005" startWordPosition="2264" endWordPosition="2267">ent number of orientation classes. Bold numbers refer to the best results. exp{− E m fjm,¯eim)} with tuning parameter β. performance as well as its time efficiency. dm ¯ βp(o| This distance–sensitive expression is able to fill the deficiency of the three–class setup of DPR and is verified to produce better results. For parameter tuning, minimum-error-rating training (F. J. Och, 2003) is used in both systems. Note that there are 7 parameters needed tuning in MOSES’s reordering models, while only 1 requires tuning in DPR. The translation performance is evaluated by four MT measurements used in (Koehn et al., 2005). Table 5 shows the translation results, where we observe consistent improvements on most evaluations. Indeed both systems produced similar word accuracy, but our MT system does better in phrase reordering and produces more fluent translations. 4 Conclusions and Future work We have proposed a distance phrase reordering model using a structure learning framework. The classification tasks have shown that DPR is better in capturing the phrase reorderings over the lexicalized reordering model and the ME model. Moreover, compared with ME DPR is much faster and more applicable to larger data sets. T</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Burch, Osborne, Talbot, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A. B. Mayne, C. Callison– Burch, M. Osborne and D. Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. of IWSLT, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2003</date>
<booktitle>In Proc. Intl. Conf. Spoken Language Processing,</booktitle>
<location>Colorado,</location>
<contexts>
<context position="12802" citStr="Och, 2003" startWordPosition="2226" endWordPosition="2227">.3 ME 80.0 ± 0.2 52.1 ± 0.1 54.7 ± 0.7 90.4 ± 0.2 63.9 ± 0.1 61.8 ± 0.1 83.6 DPR 84.6 ± 0.1 60.0 ± 0.7 61.4 ± 0.1 92.6 ± 0.2 75.4 ± 0.6 68.8 ± 0.5 29.2 Table 4: Overall precision and class-specific F1 scores [%] using different number of orientation classes. Bold numbers refer to the best results. exp{− E m fjm,¯eim)} with tuning parameter β. performance as well as its time efficiency. dm ¯ βp(o| This distance–sensitive expression is able to fill the deficiency of the three–class setup of DPR and is verified to produce better results. For parameter tuning, minimum-error-rating training (F. J. Och, 2003) is used in both systems. Note that there are 7 parameters needed tuning in MOSES’s reordering models, while only 1 requires tuning in DPR. The translation performance is evaluated by four MT measurements used in (Koehn et al., 2005). Table 5 shows the translation results, where we observe consistent improvements on most evaluations. Indeed both systems produced similar word accuracy, but our MT system does better in phrase reordering and produces more fluent translations. 4 Conclusions and Future work We have proposed a distance phrase reordering model using a structure learning framework. Th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. SRILM - An Extensible Language Modeling Toolkit. In Proc. Intl. Conf. Spoken Language Processing, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<contexts>
<context position="10804" citStr="Stolcke, 2002" startWordPosition="1824" endWordPosition="1825">wice at each round: one is for updating the conditional distributions p(o |¯fj, ¯ei) and the other is for updating {wo}o∈Ω. Alternatively, the PSL algorithm only goes through all examples once at each round, making it faster and more applicable for larger data sets. 3.2 Translation experiments We now test the effect of the DPR model in an MT system, using MOSES (Koehn et al., 2005) as a baseline system. To keep the comparison fair, our MT system just replaces MOSES’s reordering models with DPR while sharing all other models (i.e. phrase translation probability model, 4-gram language model (A. Stolcke, 2002) and beam search decoder). As in classification experiments the three-class setup shows better results in switching classes, we use this setup in DPR. In detail, all consistent phrases are extracted from the training sentence pairs and form the sample pool. The three-class DPR model is then trained by the PSL algorithm and the function h(z) = exp(z) is applied to equation (1) to transform the prediction scores. Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance–based reordering cost, where the reorde</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Minimum error rate training in statistical machine translation. In Proc. ACL, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max– margin Markov networks.</title>
<date>2003</date>
<booktitle>In Proc. NIPS,</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="4829" citStr="Taskar et al., 2003" startWordPosition="775" endWordPosition="778"> vector measuring features’ contribution to an orientation o E Q, φ is the feature vector and h is a pre-defined monotonic function. As the reordering orientations tend to be interdependent, learning {wo}o∈Ω is more than a multi–class classification problem. Take the five–class setup for example, if an example in class d &lt; −5 is classified in class −5 &lt; d &lt; 5, intuitively the loss should be smaller than when it is classified in class d &gt; 5. The output (orientation) domain has an inherent structure and the model should respect it. Hence, we utilize the structure learning framework proposed in (Taskar et al., 2003) which is equivalent to minimising the sum of the classification errors λ ρ(o, ¯fnj , ¯eni , w) + 2 �w�2 (2) where λ &gt; 0 is a regularisation parameter, ρ(o, ¯fj, ¯ei, w) = max{0, maxo06=o[o(o, o0)+ wTo0φ( ¯fj, ¯ei)] − wTo φ(¯fj, ¯ei)} is a structured margin loss function with o(o, o0) = { 0 if o = o0 0.5 if o and o0 are close in Q 1 else measuring the distance between pseudo orientation o0 and the true one o. Theoretically, this loss requires that orientation o0 which are “far away” from the true one o must be classified with a large margin while nearby candidates are allowed to be classified </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D.Koller. 2003. Max– margin Markov networks. In Proc. NIPS, Vancouver, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Xiong</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. ofACL,</booktitle>
<location>Sydney,</location>
<contexts>
<context position="3457" citStr="Xiong et al., 2006" startWordPosition="518" endWordPosition="521">distance phrase reordering model (DPR) can lead to improved performance with a reasonable time efficiency. Figure 1: The phrase reordering distance d. 2 Distance phrase reordering (DPR) We adopt a discriminative model to capture the frequent distant reordering which we call distance phrase reordering. An ideal model would consider every position as a class and predict the position of the next phrase, although in practice we must consider a limited set of classes (denoted as Q). Using the reordering distance d (see Figure 1) as defined by (Koehn et al., 2005), we extend the two class model in (Xiong et al., 2006) to multiple classes (e.g. three–class setup Q = {d &lt; 0, d = 0, d &gt; 0}; or five–class setup Q = {d ≤ −5, −5 &lt; d &lt; 0, d = 0, 0 &lt; d &lt; 5, d ≥ 5}). Note that the more 241 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 241–244, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP classes it has, the closer it is to the ideal model, but the smaller amount of training samples it would receive for each class. 2.1 Reordering Probability model and training algorithm Given a (source, target) phrase pair ( fj, ¯ei) with fj = [fjl, ... , fjr] and ¯ei = [eil, ... , eir], the distance ph</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>D. Xiong, Q. Liu and S. Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proc. ofACL, Sydney, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ofACL,</booktitle>
<pages>55--63</pages>
<location>New York City,</location>
<contexts>
<context position="2303" citStr="Zens and Ney, 2006" startWordPosition="333" endWordPosition="336">ching” order) probabilities for each bilingual phrase pair using Maximum Likelihood Estimation (MLE). These orientation probabilities are then integrated into an SMT decoder to help finding a Viterbi–best local orientation sequence. Improvements by this ∗the author’s new address: Xerox Research Centre Europe 6, Chemin de Maupertuis, 38240 Meylan France. model have been reported in (Koehn et al., 2005). However, the amount of the training data for each bilingual phrase is so small that the model usually suffers from the data sparseness problem. Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. In general, a Maximum Entropy (ME) framework is utilized and the feature parameters are tuned by a discriminative model. However, the training times for ME models are usually relatively high, especially when the output classes (i.e. phrase reordering orientations) increase. Alternative to the ME framework, we propose using a classification scheme here for phrase reorderings and employs a structure learning framework. Our results confirm that this distance phrase reordering model (DPR) can lead to improved perfo</context>
<context position="7460" citStr="Zens and Ney, 2006" startWordPosition="1269" endWordPosition="1272"> sub-DPR model is then trained for each cluster using the PSL algorithm. During the decoding, the DPR model finds the corresponding sub-DPR model for a source phrase ¯fj and generates the reordering probability for each orientation class using equation (1). 3 Experiments Experiments used the Hong Kong Laws corpus1 (Chinese-to-English), where sentences of lengths between 1 and 100 words were extracted and the ratio of source/target lengths was no more than 2 : 1. The training and test sizes are 50, 290 and 1, 000 respectively. 1 min W N N n=1 2.1.1 Feature Extraction and Application Following (Zens and Ney, 2006), we consider different kinds of information extracted from the 1This bilingual Chinese-English corpus consists of mainly legal and documentary texts from Hong Kong. The corpus is aligned at the sentence level which are collected and revised manually by the author. The full corpus will be released soon. 242 Features for source phrase ¯fj Features for target phrase ¯ei Context Source word n–grams within a window Target word n–grams (length z) around the phrase edge [jl] and [jr] of the phrase [ei,, ... , ei,.] Syntactic Source word class tag n-grams within a Target word class tag window (length</context>
<context position="11306" citStr="Zens and Ney, 2006" startWordPosition="1905" endWordPosition="1908">R while sharing all other models (i.e. phrase translation probability model, 4-gram language model (A. Stolcke, 2002) and beam search decoder). As in classification experiments the three-class setup shows better results in switching classes, we use this setup in DPR. In detail, all consistent phrases are extracted from the training sentence pairs and form the sample pool. The three-class DPR model is then trained by the PSL algorithm and the function h(z) = exp(z) is applied to equation (1) to transform the prediction scores. Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance–based reordering cost, where the reordering cost of a sentence is computed as Po(f, e) = 243 Settings three–class setup five–class setup Classes d &lt; 0 d = 0 d &gt; 0 d ≤ −5 −5 &lt; d &lt; 0 d = 0 0 &lt; d &lt; 5 d &gt; 5 Train 181, 583 755, 854 181, 279 82, 677 98, 907 755, 854 64, 881 116, 398 Test 5, 025 21, 106 5, 075 2, 239 2, 786 21, 120 1, 447 3, 629 Table 3: Data statistics for the classification experiments. System three–class setup task Precision d &lt; 0 d = 0 d &gt; 0 Training time (hours) Lexicalized 77.1 ± 0.1 55.7 ± 0.1 86.5 ± 0.1 49.2 ± 0.3 1.0</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proc. ofACL, pages 55–63, New York City, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>