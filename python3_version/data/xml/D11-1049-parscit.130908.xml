<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000218">
<title confidence="0.952028">
Random Walk Inference and Learning in A Large Scale Knowledge Base
</title>
<author confidence="0.996981">
Ni Lao Tom Mitchell William W. Cohen
</author>
<affiliation confidence="0.996327">
Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University
</affiliation>
<address confidence="0.9455895">
5000 Forbes Avenue 5000 Forbes Avenue 5000 Forbes Avenue
Pittsburgh, PA 15213 Pittsburgh, PA 15213 Pittsburgh, PA 15213
</address>
<email confidence="0.999538">
nlao@cs.cmu.edu tom.mitchell@cs.cmu.edu wcohen@cs.cmu.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906125">
We consider the problem of performing learn-
ing and inference in a large scale knowledge
base containing imperfect knowledge with
incomplete coverage. We show that a soft
inference procedure based on a combination
of constrained, weighted, random walks
through the knowledge base graph can be
used to reliably infer new beliefs for the
knowledge base. More specifically, we
show that the system can learn to infer
different target relations by tuning the weights
associated with random walks that follow
different paths through the graph, using a
version of the Path Ranking Algorithm (Lao
and Cohen, 2010b). We apply this approach to
a knowledge base of approximately 500,000
beliefs extracted imperfectly from the web
by NELL, a never-ending language learner
(Carlson et al., 2010). This new system
improves significantly over NELL’s earlier
Horn-clause learning and inference method:
it obtains nearly double the precision at rank
100, and the new learning method is also
applicable to many more inference tasks.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999769862068965">
Although there is a great deal of recent research
on extracting knowledge from text (Agichtein and
Gravano, 2000; Etzioni et al., 2005; Snow et
al., 2006; Pantel and Pennacchiotti, 2006; Banko
et al., 2007; Yates et al., 2007), much less
progress has been made on the problem of drawing
reliable inferences from this imperfectly extracted
knowledge. In particular, traditional logical
inference methods are too brittle to be used to make
complex inferences from automatically-extracted
knowledge, and probabilistic inference methods
(Richardson and Domingos, 2006) suffer from
scalability problems. This paper considers the
problem of constructing inference methods that can
scale to large knowledge bases (KB’s), and that are
robust to imperfect knowledge. The KB we consider
is a large triple store, which can be represented as a
labeled, directed graph in which each entity a is a
node, each binary relation R(a, b) is an edge labeled
R between a and b, and unary concepts C(a) are
represented as an edge labeled “isa” between the
node for the entity a and a node for the concept
C. We present a trainable inference method that
learns to infer relations by combining the results of
different random walks through this graph, and show
that the method achieves good scaling properties and
robust inference in a KB containing over 500,000
triples extracted from the web by the NELL system
(Carlson et al., 2010).
</bodyText>
<subsectionHeader confidence="0.971511">
1.1 The NELL Case Study
</subsectionHeader>
<bodyText confidence="0.999948545454545">
To evaluate our approach experimentally, we study
it in the context of the NELL (Never Ending
Language Learning) research project, which is an
effort to develop a never-ending learning system that
operates 24 hours per day, for years, to continuously
improve its ability to read (extract structured facts
from) the web (Carlson et al., 2010). NELL began
operation in January 2010. As of March 2011,
NELL had built a knowledge base containing several
million candidate beliefs which it had extracted from
the web with varying confidence. Among these,
</bodyText>
<page confidence="0.977494">
529
</page>
<note confidence="0.9582205">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529–539,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999247658536585">
NELL had fairly high confidence in approximately
half a million, which we refer to as NELL’s
(confident) beliefs. NELL had lower confidence in a
few million others, which we refer to as its candidate
beliefs.
NELL is given as input an ontology that defines
hundreds of categories (e.g., person, beverage,
athlete, sport) and two-place typed relations among
these categories (e.g., atheletePlaysSport((athlete),
(sport))), which it must learn to extract from the
web. It is also provided a set of 10 to 20 positive
seed examples of each such category and relation,
along with a downloaded collection of 500 million
web pages from the ClueWeb2009 corpus (Callan
and Hoy, 2009) as unlabeled data, and access to
100,000 queries each day to Google’s search engine.
Each day, NELL has two tasks: (1) to extract
additional beliefs from the web to populate its
growing knowledge base (KB) with instances of the
categories and relations in its ontology, and (2) to
learn to perform task 1 better today than it could
yesterday. We can measure its learning competence
by allowing it to consider the same text documents
today as it did yesterday, and recording whether it
extracts more beliefs, more accurately today.1
NELL uses a large-scale semi-supervised multi-
task learning algorithm that couples the training
of over 1500 different classifiers and extraction
methods (see (Carlson et al., 2010)). Although
many of the details of NELL’s learning method
are not central to this paper, two points should
be noted. First, NELL is a multistrategy learning
system, with components that learn from different
“views” of the data (Blum and Mitchell, 1998): for
instance, one view uses orthographic features of
a potential entity name (like “contains capitalized
words”), and another uses free-text contexts in
which the noun phrase is found (e.g., “X frequently
follows the bigram ‘mayor of’ ”). Second, NELL
is a bootstrapping system, which self-trains on its
growing collection of confident beliefs.
</bodyText>
<subsectionHeader confidence="0.94776">
1.2 Knowledge Base Inference: Horn Clauses
</subsectionHeader>
<bodyText confidence="0.6179745">
Although NELL has now grown a sizable knowl-
edge base, its ability to perform inference over this
</bodyText>
<footnote confidence="0.916849">
1NELL’s current KB is available online at
http://rtw.ml.cmu.edu.
</footnote>
<figureCaption confidence="0.99877">
Figure 1: An example subgraph.
</figureCaption>
<bodyText confidence="0.9905628">
knowledge base is currently very limited. At present
its only inference method beyond simple inheritance
involves applying first order Horn clause rules to
infer new beliefs from current beliefs. For example,
it may use a Horn clause such as
</bodyText>
<equation confidence="0.995838">
AthletePlaysForTeam(a, b) (1)
∧ TeamPlaysInLeague(b, c)
==�, AthletePlaysInLeague(a,c)
</equation>
<bodyText confidence="0.99871984375">
to infer that AthletePlaysInLeague(HinesWard,NFL),
if it has already extracted the beliefs in the
preconditions of the rule, with variables a, b and c
bound to HinesWard, PittsburghSteelers and NFL
respectively as shown in Figure 1. NELL currently
has a set of approximately 600 such rules, which
it has learned by data mining its knowledge base
of beliefs. Each learned rule carries a conditional
probability that its conclusion will hold, given that
its preconditions are satisfied.
NELL learns these Horn clause rules using
a variant of the FOIL algorithm (Quinlan and
Cameron-Jones, 1993), henceforth N-FOIL.
N-FOIL takes as input a set of positive and
negative examples of a rule’s consequent
(e.g., +AthletePlaysInLeague(HinesWard,NFL),
−AthletePlaysInLeague(HinesWard,NBA)), and
uses a “separate-and-conquer” strategy to learn a
set of Horn clauses that fit the data well. Each
Horn clause is learned by starting with a general
rule and progressively specializing it, so that it
still covers many positive examples but covers few
negative examples. After a clause is learned, the
examples covered by that clause are removed from
the training set, and the process repeats until no
positive examples remain.
Learning first-order Horn clauses is computation-
ally expensive—not only is the search space large,
but some Horn clauses can be costly to evaluate
(Cohen and Page, 1995). N-FOIL uses two tricks
to improve its scalability. First, it assumes that
the consequent predicate is functional—e.g., that
</bodyText>
<figure confidence="0.998510071428572">
AthletePlays
ForTeam
HinesWard
Eli Manning
TeamPlays
Steelers
TeamPlays
InLeague
TeamPlays
InLeague
Giants
MLB
AthletePlays
ForTeam InLeague NFL
</figure>
<page confidence="0.988912">
530
</page>
<bodyText confidence="0.994460075">
each Athlete plays in at most one League. This each performing a random walk through the graph,
means that explicit negative examples need not constrained to follow that sequence of edge types,
be provided (Zelle et al., 1995): e.g., if Ath- and ranking nodes b by their weights in the resulting
letePlaysInLeague(HinesWard,NFL) is a positive distribution. Finally, PRA combines the weights
example, then AthletePlaysInLeague(HinesWard,c0) contributed by different “experts” using logistic
for any other value of c0 is negative. In general, regression to predict the probability that the relation
this constraint guides the search algorithm toward R(a, b) is satisfied.
Horn clauses that have fewer possible instantiations, As an example, consider a path from a to b via
and hence are less expensive to match. Second, the sequence of edge types isa, isa−1 (the inverse of
N-FOIL uses “relational pathfinding” (Richards isa), and AthletePlaysInLeague, which corresponds
and Mooney, 1992) to produce general rules—i.e., to the Horn clause
the starting point for a predicate R is found isa(a, c) ∧ isa−1(c, a&apos;) (3)
by looking at positive instances R(a, b) of the ∧ AthletePlaysInLeague(a&apos;, b)
consequent, and finding a clause that corresponds ⇒ AthletePlaysInLeague(a, b)
to a bounded-length path of binary relations that
link a to b. In the example above, a start clause
might be the clause (1). As in FOIL, the clause
is then (potentially) specialized by greedily adding
additional conditions (like ProfessionalAthlete(a))
or by replacing variables with constants (eg,
replacing c with NFL).
For each N-FOIL rule, an estimated conditional
probability Pˆ(conclusion|preconditions) is calcu-
lated using a Dirichlet prior according to
Pˆ = (N+ + m ∗ prior)/(N+ + N− + m) (2)
where N+ is the number of positive instances
matched by this rule in the FOIL training data,
N− is the number of negative instances matched,
m = 5 and prior = 0.5. As the results below
show, N-FOIL generally learns a small number of
high-precision inference rules. One important role
of these inference rules is that they contribute to
the bootstrapping procedure, as inferences made by
N-FOIL increase either the number of candidate
beliefs, or (if the inference is already a candidate)
improve NELL’s confidence in candidate beliefs.
1.3 Knowledge Base Inference: Graph
Random Walks
In this paper, we consider an alternative approach,
based on the Path Ranking Algorithm (PRA) of Lao
and Cohen (2010b), described in detail below. PRA
learns to rank graph nodes b relative to a query
node a. PRA begins by enumerating a large set of
bounded-length edge-labeled path types, similar to
the initial clauses used in NELL’s variant of FOIL.
These path types are treated as ranking “experts”,
531
Suppose a random walk starts at a query node a
(say a=HinesWard). If HinesWard is linked to the
single concept node ProfessionalAthlete via isa, the
walk will reach that node with probability 1 after
one step. If A is the set of ProfessionalAthlete’s
in the KB, then after two steps, the walk will have
probability 1/|A |of being at any a0 ∈ A. If L is
the set of athletic leagues and E ∈ L, let At be the
set of athletes in league E: after three steps, the walk
will have probability |At|/|A |of being at any point
b ∈ L. In short, the ranking associated with this
path gives the prior probability of a value b being an
athletic league for a—which is useful as a feature in
a combined ranking method, although not by itself a
high-precision inference rule.
Note that the rankings produced by this “expert”
will change as the knowledge base evolves—for
instance, if the system learns about proportionally
more soccer players than hockey players over time,
then the league rankings for the path of clause (3)
will change. Also, the ranking is specific to the
query node a. For instance, suppose the KB contains
facts which reflect the ambiguity of the team name
“Giants”2 as in Figure 1. Then the path for clause (1)
above will give lower weight to b = NFL for a =
EliManning than to b = NFL for a = HinesWard.
The main contribution of this paper is to introduce
and evaluate PRA as an algorithm for making
probabilistic inference in large KBs. Compared to
Horn clause inference, the key characteristics of this
new inference method are as follows:
2San Francisco’s Major-League Baseball and New York’s
National Football League teams are both called the “Giants”.
</bodyText>
<listItem confidence="0.905480117647059">
• The evidence in support of inferring a relation
instance R(a, b) is based on many existing
paths between a and b in the current KB,
combined using a learned logistic function.
• The confidence in an inference is sensitive to
the current state of the knowledge base, and the
specific entities being queried (since the paths
used in the inference have these properties).
• Experimentally, the inference method yields
many more moderately-confident inferences
than the Horn clauses learned by N-FOIL.
• The learning and inference are more efficient
than N-FOIL, in part because we can exploit
efficient approximation schemes for random
walks (Lao and Cohen, 2010a). The resulting
inference is as fast as 10 milliseconds per query
on average.
</listItem>
<bodyText confidence="0.999681">
The Path Ranking Algorithm (PRA) we use is
similar to that described elsewhere (Lao and Cohen,
2010b), except that to achieve efficient model
learning, the paths between a and b are determined
by the statistics from a population of training
queries rather than enumerated completely. PRA
uses random walks to generate relational features
on graph data, and combine them with a logistic
regression model. Compared to other relational
models (e.g. FOIL, Markov Logic Networks), PRA
is extremely efficient at link prediction or retrieval
tasks, in which we are interested in identifying top
links from a large number of candidates, instead of
focusing on a particular node pair or joint inferences.
</bodyText>
<sectionHeader confidence="0.586804" genericHeader="related work">
1.4 Related Work
</sectionHeader>
<bodyText confidence="0.999986428571428">
The TextRunner system (Cafarella et al., 2006)
answers list queries on a large knowledge base
produced by open domain information extrac-
tion. Spreading activation is used to measure
the closeness of any node to the query term
nodes. This approach is similar to the random
walk with restart approach which is used as a
baseline in our experiment. The FactRank system
(Jain and Pantel, 2010) compares different ways of
constructing random walks, and combining them
with extraction scores. However, the shortcoming
of both approaches is that they ignore edge type
information, which is important for achieving high
accuracy predictions.
The HOLMES system (Schoenmackers et al.,
2008) derives new assertions using a few manually
written inference rules. A Markov network
corresponding to the grounding of these rules to
the knowledge base is constructed for each query,
and then belief propagation is used for inference.
In comparison, our proposed approach discovers
inference rules automatically from training data.
Similarly, the Markov Logic Networks (Richard-
son and Domingos, 2006) are Markov networks
constructed corresponding to the grounding of rules
to knowledge bases. In comparison, our proposed
approach is much more efficient by avoiding the
harder problem of joint inferences and by leveraging
efficient random walk schemes (Lao and Cohen,
2010a).
Below we describe our approach in greater detail,
provide experimental evidence of its value for
performing inference in NELL’s knowledge base,
and discuss implications of this work and directions
for future research.
</bodyText>
<sectionHeader confidence="0.970515" genericHeader="method">
2 Approach
</sectionHeader>
<bodyText confidence="0.9998725">
In this section, we first describe how we formulate
link (relation) prediction on a knowledge base as
a ranking task. Then we review the Path Ranking
Algorithm (PRA) introduced by Lao and Cohen
(2010b; 2010a). After that, we describe two
improvements to the PRA method to make it more
suitable for the task of link prediction in knowledge
bases. The first improvement helps PRA deal
with the large number of relations typical of large
knowledge bases. The second improvement aims at
improving the quality of inference by applying low
variance sampling.
</bodyText>
<subsectionHeader confidence="0.999126">
2.1 Learning with NELL’s Knowledge Base
</subsectionHeader>
<bodyText confidence="0.999899125">
For each relation R in the knowledge base we train a
model for the link prediction task: given a concept a,
find all other concepts b which potentially have the
relation R(a, b). This prediction is made based on an
existing knowledge base extracted imperfectly from
the web. Although a model can potentially benefit
from predicting multiple relations jointly, such joint
inference is beyond the scope of this work.
</bodyText>
<page confidence="0.885721">
532
</page>
<bodyText confidence="0.954764">
To ensure a reasonable number of training walk with edge type R`. R(e0, e) indicates whether
instances, we generate labeled training example there exists an edge with type R that connect e0 to e.
queries from 48 relations which have more than More generally, given a set of paths P1, ... , Pn,
100 instances in the knowledge base. We create one could treat each hs,Pi(e) as a path feature for
two tasks for each relation—i.e., predicting b given the node e, and rank nodes by a linear model
a and predicting a given b— yielding 96 tasks in 01hs,P1(e) + 02hs,P2(e) + ... 0nhs,Pn(e)
all. Each node a which has relation R in the where 0i are appropriate weights for the paths. This
knowledge base with any other node is treated as a gives a ranking of nodes e related to the query node
training query, the actual nodes b in the knowledge s by the following scoring function
base known to satisfy R(a, b) are treated as labeled Escore(e; s) = hs,P(e)0P, (6)
positive examples, and any other nodes are treated P∈P`
as negative examples. where P` is the set of relation paths with length ≤ E.
</bodyText>
<table confidence="0.937682333333333">
2.2 Path Ranking Algorithm Review Given a relation R and a set of node pairs
We now review the Path Ranking Algorithm {(si, ti)} for which we know whether R(si, ti) is
introduced by Lao and Cohen (2010b). A relation true or not, we can construct a training dataset
path P is defined as a sequence of relations D = {(xi, yi)}, where xi is a vector of all the
R1 ... R`, and in order to emphasize the types path features for the pair (si, ti)—i.e., the j-th
associated with each step, P can also be written as component of xi is hsi,Pj(ti), and where yi is a
T0 −−→ ... R` boolean variable indicating whether R(si, ti) is true.
R1 −→ T`, where Ti = range(Ri) = We then train a logistic function to predict the
domain(Ri+1), and we also define domain(P) ≡ conditional probability P(y|x; 0). The parameter
T0, range(P) ≡ T`. In the experiments in this vector 0 is estimated by maximizing a regularized
paper, there is only one type of node which we call form of the conditional likelihood of y given x. In
a concept, which can be connected through different particular, we maximize the objective function
types of relations. In this notation, relations like “the O(0) = E oi(0) − A1|0|1 − A2|0|2, (7)
team a certain player plays for”, and “the league a i
certain player’s team is in” can be expressed by the where A1 controls L1-regularization to help struc-
paths below (respectively): ture selection, and A2 controls L2-regularization
AtheletePlayesForTeam to prevent overfitting. oi(0) is the per-instance
P1 : concept−−−−−−−−−−−−−−→ concept weighted log conditional likelihood given by
AtheletePlayesForTeam oi(0) = wi[yi lnpi + (1 − yi) ln(1 − pi)], (8)
P2 : concept−−−−−−−−−−−−−−→ concept where pi is the predicted probability p(yi =
TeamPlaysInLeagure 1|xi; 0) = exp(θT Txi) i)
−−−−−−−−−−−−−→ concept 1+exp(θ and wi is an importance
For any relation path P = R1 ... R` and a weight to each example. A biased sampling
seed node s ∈ domain(P), a path constrained procedure selects only a small subset of negative
random walk defines a distribution hs,P recursively samples to be included in the objective (see (Lao and
as follows. If P is the empty path, then define Cohen, 2010b) for detail).
� 1, if e = s 2.3 Data-Driven Path Finding
hs,P (e) = (4) In prior work with PRA, P` was defined as all
0, otherwise relation paths of length at most E. When the number
If P = R1 ... R` is nonempty, then let P0 = of edge types is small, one can generate P` by
R1 ... R`−1, and define
Ehs,P(e) = hs,P,(e0) · P(e|e0; R`), (5)
e,∈range(P,)
where P(e|e0; R`) = |R`(e,;e) |is the probability of
reaching node e from node e&apos; with a one step random
533
</table>
<tableCaption confidence="0.998016333333333">
Table 2: Comparing PRA with RWR models. MRRs and
training times are averaged over 96 tasks.
Table 1: Number of paths in PRA models of maximum
</tableCaption>
<table confidence="0.965126166666667">
path length 3 and 4. Averaged over 96 tasks.
B=3 B=4
all paths up to length L 15,376 1,906,624
+query support≥ α = 0.01 522 5016
+ever reach a target entity 136 792
+Ll regularization 63 271
</table>
<bodyText confidence="0.999238948717949">
enumeration; however, for domains with a large
number of edge types (e.g., a knowledge base), it is
impractical to enumerate all possible relation paths
even for small B. For instance, if the number of
edge types related to each node type is 100, even
the number of length three paths types easily reaches
millions. For other domains like parsed natural
language sentences, useful relation paths can be as
long as ten relations (Minkov and Cohen, 2008). In
this case, even with smaller number of possible edge
types, the total number of relation paths is still too
large for systematic enumeration.
In order to apply PRA to these domains, we
modify the path generation procedure in PRA to
produce only relation paths which are potentially
useful for the task. Define a query s to be supporting
a path P if hs,P(e) =� 0 for any entity e. We require
that any path node created during path finding needs
to be supported by at least a fraction α of the training
queries si, as well as being of length no more than
B (In the experiments, we set α = 0.01) We also
require that in order for a relation path to be included
in the PRA model, it must retrieve at least one target
entity ti in the training set. As we can see from
Table 1, together these two constraints dramatically
reduce the number of relation paths that need to be
considered, relative to systematically enumerating
all possible relation paths. L1 regularization reduces
the size of the model even more.
The idea of finding paths that connects nodes in a
graph is not new. It has been embodied previously in
first-order learning systems (Richards and Mooney,
1992) as well as N-FOIL, and relational database
searching systems (Bhalotia et al., 2002). These
approaches consider a single query during path
finding. In comparison, the data-driven path finding
method we described here uses statistics from a
population of queries, and therefore can potentially
determine the importance of a path more reliably.
</bodyText>
<table confidence="0.9152632">
B=2 B=3
MRR Time MRR Time
RWR(no train) 0.271 0.456
RWR 0.280 3.7s 0.471 9.2s
PRA 0.307 5.7s 0.516 15.4s
</table>
<subsectionHeader confidence="0.981669">
2.4 Low-Variance Sampling
</subsectionHeader>
<bodyText confidence="0.9999853">
Lao and Cohen (2010a) previously showed that
sampling techniques like finger printing and particle
filtering can significantly speedup random walk
without sacrificing retrieval quality. However, the
sampling procedures can induce a loss of diversity
in the particle population. For example, consider a
node in the graph with just two out links with equal
weights, and suppose we are required to generate
two walkers starting from this node. A disappointing
result is that with 50 percent chance both walkers
will follow the same branch, and leave the other
branch with no probability mass.
To overcome this problem, we apply a technique
called Low-Variance Sampling (LVS) (Thrun et
al., 2005), which is commonly used in robotics
to improve the quality of sampling. Instead of
generating independent samples from a distribution,
LVS uses a single random number to generate all
samples, which are evenly distributed across the
whole distribution. Note that given a distribution
</bodyText>
<equation confidence="0.696534">
P(x), any number r in [0, 1] points to exactly one
�
x value, namely x = arg minj m=1..j P(m) ≤
</equation>
<bodyText confidence="0.9952546">
r. Suppose we want to generate M samples from
P(x). LVS first generates a random number r in
the interval [0, M−1]. Then LVS repeatedly adds
the fixed amount M−1 to r and chooses x values
corresponding to the resulting numbers.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999770714285714">
This section reports empirical results of applying
random walk inference to NELL’s knowledge base
after the 165th iteration of its learning process. We
first investigate PRA’s behavior by cross validation
on the training queries. Then we compare PRA and
N-FOIL’s ability to reliably infer new beliefs, by
leveraging the Amazon Mechanical Turk service.
</bodyText>
<page confidence="0.995034">
534
</page>
<subsectionHeader confidence="0.960865">
3.1 Cross Validation on the Training Queries
</subsectionHeader>
<bodyText confidence="0.99923443902439">
Random Walk with Restart (RWR) (also called
personalized PageRank (Haveliwala, 2002)) is a
general-purpose graph proximity measure which
has been shown to be fairly successful for many
types of tasks. We compare PRA to two versions
of RWR on the 96 tasks of link prediction with
NELL’s knowledge base. The two baseline methods
are an untrained RWR model and a trained RWR
model as described by Lao and Cohen (2010b). (In
brief, in the trained RWR model, the walker will
probabilistically prefer to follow edges associated
with different labels, where the weight for each edge
label is chosen to minimize a loss function, such as
Equation 7. In the untrained model, edge weights
are uniform.) We explored a range of values for
the regularization parameters L1 and L2 using cross
validation on the training data, and we fix both
L1 and L2 parameters to 0.001 for all tasks. The
maximum path length is fixed to 3.3
Table 2 compares the three methods using
5-fold cross validation and the Mean Reciprocal
Rank (MRR)4 measure, which is defined as the
inverse rank of the highest ranked relevant result
in a set of results. If the the first returned
result is relevant, then MRR is 1.0, otherwise,
it is smaller than 1.0. Supervised training can
significantly improve retrieval quality (p-value=9 ×
10−8 comparing untrained and trained RWR), and
leveraging path information can produce further
improvement (p-value=4 × 10−4 comparing trained
RWR with PRA). The average training time for a
predicate is only a few seconds.
We also investigate the effect of low-variance
sampling on the quality of prediction. Figure 2 com-
pares independent and low variance sampling when
applied to finger printing and particle filtering (Lao
and Cohen, 2010a). The horizontal axis corresponds
to the speedup of random walk compared with
exact inference, and the vertical axis measures the
quality of prediction by MRR with three fold cross
validation on the training query set. Low-variance
</bodyText>
<footnote confidence="0.9962985">
3Results with maximum length 4 are not reported here.
Generally models with length 4 paths produce slightly better
results, but are 4-5 times slower to train
4For a set of queries Q,
</footnote>
<figure confidence="0.404567666666667">
MRR =  |Q  |Eq∈Q rank of the first correct answer for q
0 1 2 3 4 5
Random Walk Speedup
</figure>
<figureCaption confidence="0.987366666666667">
Figure 2: Compare inference speed and quality over 96
tasks. The speedup is relative to exact inference, which is
on average 23ms per query.
</figureCaption>
<bodyText confidence="0.999717916666667">
sampling can improve prediction for both finger
printing and particle filtering. The numbers on the
curves indicate the number of particles (or walkers).
When using a large number of particles, the particle
filtering methods converge to the exact inference.
Interestingly, when using a large number of walkers,
the finger printing methods produce even better
prediction quality than exact inference. Lao and
Cohen noticed a similar improvement on retrieval
tasks, and conjectured that it is because the sampling
inference imposes a regularization penalty on longer
relation paths (2010a).
</bodyText>
<subsectionHeader confidence="0.999172">
3.2 Evaluation by Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.9999954">
The cross-validation result above assumes that the
knowledge base is complete and correct, which
we know to be untrue. To accurately compare
PRA and N-FOIL’s ability to reliably infer new
beliefs from an imperfect knowledge base, we
use human assessments obtained from Amazon
Mechanical Turk. To limit labeling costs, and
since our goal is to improve the performance of
NELL, we do not include RWR-based approaches
in this comparison. Among all the 24 functional
predicates, N-FOIL discovers confident rules for
8 of them (it produces no result for the other 16
predicates). Therefore, we compare the quality
of PRA to N-FOIL on these 8 predicates only.
Among all the 72 non-functional predicates—which
</bodyText>
<figure confidence="0.997042857142857">
0.5
0.4
100k
10k
Exact
Independent Fingerprinting
Low Variance Fingerprinting
Independent Filtering
Low Variance Filtering
10k
1k
100
1k
-lq
</figure>
<page confidence="0.996102">
535
</page>
<tableCaption confidence="0.999538">
Table 3: The top two weighted PRA paths for tasks on which N-FOIL discovers confident rules. c stands for concept.
</tableCaption>
<figure confidence="0.995144">
ID PRA Path (Comment)
athletePlaysForTeam
athletePlaysInLeague leaguePlayers athletePlaysForTeam
1 c c � c � c (teams with many players in the athlete’s league)
athletePlaysInLeague leagueTeams teamAgainstTeam
2 c c � c � c (teams that play against many teams in the athlete’s league)
athletePlaysInLeague
athletePlaysSport players athletePlaysInLeague
3 c c � c c (the league that players of a certain sport belong to)
4 c isa
� c isa−1
� c athletePlaysInLeague �c (popular leagues with many players)
athletePlaysSport
5 c isa
� c isa−1
� c athletePlaysSport �c (popular sports of all the athletes)
athletePlaysInLeague superpartOfOrganization mteamPlaysSport
6 c c c c (popular sports of a certain league)
stadiumLocatedInCity
7 cstadiumHomeTeam
� cteamHomeStadium
� cstadiumLocatedInCity
� c (city of the stadium with the same team)
latitudeLongitude latitudeLongitudeOf stadiumLocatedInCity
8 c c c � c (city of the stadium with the same location)
teamHomeStadium
teamPlaysInCity cityStadiums
9 c c � c (stadiums located in the same city with the query team)
teamMember athletePlaysForTeam teamHomeStadium
10 c � c c � c (home stadium of teams which share players with the query)
teamPlaysInCity
11 cteamHomeStadium
� cstadiumLocatedInCity
� c (city of the team’s home stadium)
12 cteamHomeStadium
� cstadiumHomeTeam
� cteamPlaysInCity
� c (city of teams with the same home stadium as the query)
teamPlaysInLeague
teamPlaysSport players athletePlaysInLe
13 c � c � c aguec (the league that the query team’s members belong to)
14 c �c teamPlaysInLeague
teamPlaysAgainstTeam � c (the league that the query team’s competing team belongs to)
teamPlaysSport
isa isa−1 teamPlaysSport
15 c � c ) c � c (sports played by many teams)
teamPlaysInLeague leagueTeams teamPlaysSport
16 c c � c � c (the sport played by other teams in the league)
</figure>
<tableCaption confidence="0.9946105">
Table 4: Amazon Mechanical Turk evaluation for the promoted knowledge. Using paired t-test at task level, PRA is
not statistically different from N-FOIL for p@10 (p-value=0.3), but is significantly better for p@100 (p-value=0.003)
</tableCaption>
<table confidence="0.99986765">
PRA N-FOIL
Task Pmajority #Paths p@10 p@100 p@1000 #Rules #Query p@10 p@100 p@1000
athletePlaysForTeam 0.07 125 0.4 0.46 0.66 1(+1) 7 0.6 0.08 0.01
athletePlaysInLeague 0.60 15 1.0 0.84 0.80 3(+30) 332 0.9 0.80 0.24
athletePlaysSport 0.73 34 1.0 0.78 0.70 2(+30) 224 1.0 0.82 0.18
stadiumLocatedInCity 0.05 18 0.9 0.62 0.54 1(+0) 25 0.7 0.16 0.00
teamHomeStadium 0.02 66 0.3 0.48 0.34 1(+0) 2 0.2 0.02 0.00
teamPlaysInCity 0.10 29 1.0 0.86 0.62 1(+0) 60 0.9 0.56 0.06
teamPlaysInLeague 0.26 36 1.0 0.70 0.64 4(+151) 30 0.9 0.18 0.02
teamPlaysSport 0.42 21 0.7 0.60 0.62 4(+86) 48 0.9 0.42 0.02
average 0.28 43 0.79 0.668 0.615 91 0.76 0.38 0.07
teamMember 0.01 203 0.8 0.64 0.48
companiesHeadquarteredIn 0.05 42 0.6 0.54 0.60
publicationJournalist 0.02 25 0.7 0.70 0.64
producedBy 0.19 13 0.5 0.58 0.68 N-FOIL does not produce results
competesWith 0.19 74 0.6 0.56 0.72 for non-functional predicates
hasOfficeInCity 0.03 262 0.9 0.84 0.60
teamWonTrophy 0.24 56 0.5 0.50 0.46
worksFor 0.13 62 0.6 0.60 0.74
average 0.11 92 0.650 0.620 0.615
</table>
<page confidence="0.985079">
536
</page>
<bodyText confidence="0.978067114285714">
N-FOIL cannot be applied to—PRA exhibits a wide Table 5: Comparing Mechanical Turk workers’ voted
range of performance in cross-validation. The are 43 assessments with our gold standard labels based on 100
tasks for which PRA obtains MRR higher than 0.4 samples.
and builds a model with more than 10 path features.
We randomly sampled 8 of these predicates to be
evaluated by Amazon Mechanical Turk.
Table 3 shows the top two weighted PRA features
for each task on which N-FOIL can successfully
learn rules. These PRA rules can be categorized into
broad coverage rules which behave like priors over
correct answers (e.g. 1-2, 4-6, 15), accurate rules
which leverage specific relation sequences (e.g. 9,
11, 14), rules which leverage information about the
synonyms of the query node (e.g. 7-8, 10, 12),
and rules which leverage information from a local
neighborhood of the query node (e.g. 3, 12-13, 16).
The synonym paths are useful, because an entity
may have multiple names on the web. We find
that all 17 general rules (no specialization) learned
by N-FOIL can be expressed as length two relation
paths such as path 11. In comparison, PRA explores
a feature space with many length three paths.
For each relation R to be evaluated, we generate
test queries s which belong to domain(R). Queries
which appear in the training set are excluded. For
each query node s, we applied a trained model
(either PRA or N-FOIL) to generate a ranked list
of candidate t nodes. For PRA, the candidates
are sorted by their scores as in Eq. (6). For
N-FOIL, the candidates are sorted by the estimated
accuracies of the rules as in Eq. (2) (which generate
the candidates). Since there are about 7 thousand
(and 13 thousand) test queries s for each functional
(and non-functional) predicate R, and there are
(potentially) thousands of candidates t returned for
</bodyText>
<table confidence="0.943724703703704">
each query s, we cannot evaluate all candidates of
all queries. Therefore, we first sort the queries s for
each predicate R by the scores of their top ranked
candidate t in descending order, and then calculate
precisions at top 10, 100 and 1000 positions for the
list of result R(sR,1, tR,1
1 ), R(sR,2, tR,2
1 ), ..., where
sR,1 is the first query for predicate R, tR,1
1 is its first
candidate, sR,2 is the second query for predicate R,
tR,2
1 is its first candidate, so on and so forth. To
reduce the labeling load, we judge all top 10 queries
for each predicate, but randomly sample 50 out of
the top 100, and randomly sample 50 out of the
537
AMT=F AMT=T
Gold=F 25% 15%
Gold=T 11% 49%
top 1000. Each belief is evaluated by 5 workers
at Mechanical Turk, who are given assertions like
“Hines Ward plays for the team Steelers”, as well
as Google search links for each entity, and the
combination of both entities. Statistics shows
that the workers spend on average 25 seconds to
judge each belief. We also remove some workers’
</table>
<bodyText confidence="0.998645106060606">
judgments which are obviously incorrect5. We
sampled 100 beliefs, and compared their voted result
to gold-standard labels produced by one author of
this paper. Table 5 shows that 74% of the time the
workers’ voted result agrees with our judgement.
Table 4 shows the evaluation result. The
Pmajority column shows for each predicate the
accuracy achieved by the majority prediction: given
a query R(a, ?), predict the b that most often
satisfies R over all possible a in the knowledge
base. Thus, the higher Pmajority is, the simpler
the task. Predicting the functional predicates
is generally easier predicting the non-functional
predicates. The #Query column shows the number
of queries on which N-FOIL is able to match any
of its rules, and hence produce a candidate belief.
For most predicates, N-FOIL is only able to produce
results for at most a few hundred queries. In
comparison, PRA is able to produce results for 6,599
queries on average for each functional predicate, and
12,519 queries on average for each non-functional
predicate. Although the precision at 10 (p@10) of
N-FOIL is comparable to that of PRA, precision
at 10 and at 1000 (p@100 and p@1000) are much
lower6.
The #Path column shows the number of paths
learned by PRA, and the #Rule column shows the
number of rules learned by N-FOIL, with the num-
bers before brackets correspond to unspecialized
rules, and the numbers in brackets correspond to
5Certain workers label all the questions with the same
answer
6If a method makes k predictions, and k &lt; n, then p@n is
the number correct out of the k predictions, divided by n
specialized rules. Generally, specialized rules have
much smaller recall than unspecialized rules. There-
fore, the PRA approach achieves high recall partially
by combining a large number of unspecialized paths,
which correspond to unspecialized rules. However,
learning more accurate specialized paths is part of
our future work.
A significant advantage of PRA over N-FOIL is
that it can be applied to non-functional predicates.
The last eight rows of Table 4 show PRA’s
performance on eight of these predicates. Compared
to the result on functional predicates, precisions
at 10 and at 100 of non-functional predicates
are slightly lower, but precisions at 1000 are
comparable. We note that for some predicates
precision at 1000 is better than at 100. After
some investigation we found that for many relations,
the top portion of the result list is more diverse:
i.e. showing products produced by different com-
panies, journalist working at different publications.
While the lower half of the result list is more
homogeneous: i.e. showing relations concentrated
on one or two companies/publications. On the
other hand, through the process of labeling the
Mechanical Turk workers seem to build up a prior
about which company/publication is likely to have
correct beliefs, and their judgments are positively
biased towards these companies/publications. These
two factors combined together result in positive bias
towards the lower portion of the result list. In future
work we hope to design a labeling strategy which
avoids this bias.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999995148148148">
We have shown that a soft inference procedure based
on a combination of constrained, weighted, random
walks through the knowledge base graph can be
used to reliably infer new beliefs for the knowledge
base. We applied this approach to a knowledge
base of approximately 500,000 beliefs extracted
imperfectly from the web by NELL. This new
system improves significantly over NELL’s earlier
Horn-clause learning and inference method: it
obtains nearly double the precision at rank 100. The
inference and learning are both very efficient—our
experiment shows that the inference time is as fast
as 10 milliseconds per query on average, and the
training for a predicate takes only a few seconds.
There are several prominent directions for future
work. First, inference starting from both the query
nodes and target nodes (Richards and Mooney,
1992) can be much more efficient in discovering
long paths than just inference from the query nodes.
Second, inference starting from the target nodes
of training queries is a potential way to discover
specialized paths (with grounded nodes). Third,
generalizing inference paths to inference trees or
graphs can produce more expressive random walk
inference models. Overall, we believe that random
walk is a promising way to scale up relational
learning to domains with very large data sets.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999910875">
This work was supported by NIH under grant
R01GM081293, by NSF under grant IIS0811562,
by DARPA under awards FA8750-08-1-0009 and
AF8750-09-C-0179, and by a gift from Google.
We thank Geoffrey J. Gordon for the suggestion
of applying low variance sampling to random walk
inference. We also thank Bryan Kisiel for help with
the NELL system.
</bodyText>
<sectionHeader confidence="0.998942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998577791666666">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries - DL ’00, pages 85–94, New York, New York,
USA. ACM Press.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
Information Extraction from the Web. In IJCAI, pages
2670–2676.
Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe,
Soumen Chakrabarti, and S. Sudarshan. 2002.
Keyword searching and browsing in databases using
banks. ICDE, pages 431–440.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the eleventh annual conference on
Computational learning theory - COLT’ 98, pages
92–100, New York, New York, USA. ACM Press.
MJ Cafarella, M Banko, and O Etzioni. 2006. Relational
Web Search. In WWW.
Jamie Callan and Mark Hoy. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
</reference>
<page confidence="0.979178">
538
</page>
<reference confidence="0.995260563636363">
on Inductive Logic Programming (ILP-95), pages
403–416, Leuven, Belgium.
Mitchell. 2010. Toward an Architecture for
Never-Ending Language Learning. In AAAI.
William W. Cohen and David Page. 1995. Polyno-
mial learnability and inductive logic programming:
Methods and results. New Generation Comput.,
13(3&amp;4):369–409.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91–134.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517–526.
Alpa Jain and Patrick Pantel. 2010. Factrank: Random
walks on a web of facts. In COLING, pages 501–509.
Ni Lao and William W. Cohen. 2010a. Fast query exe-
cution for retrieval models based on path-constrained
random walks. KDD.
Ni Lao and William W. Cohen. 2010b. Relational
retrieval using a combination of path-constrained
random walks. Machine Learning.
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
EMNLP, pages 907–916.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In ACL.
J. Ross Quinlan and R. Mike Cameron-Jones. 1993.
FOIL: A Midterm Report. In ECML, pages 3–20.
Bradley L. Richards and Raymond J. Mooney. 1992.
Learning relations by pathfinding. In Proceedings
of the Tenth National Conference on Artificial Intel-
ligence (AAAI-92), pages 50–55, San Jose, CA, July.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling Textual Inference to the Web.
In EMNLP, pages 79–88.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic Taxonomy Induction from Heterogenous
Evidence. In ACL.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox.
2005. Probabilistic Robotics (Intelligent Robotics and
Autonomous Agents). The MIT Press.
Alexander Yates, Michele Banko, Matthew Broadhead,
Michael J. Cafarella, Oren Etzioni, and Stephen
Soderland. 2007. TextRunner: Open Information
Extraction on the Web. In HLT-NAACL (Demonstra-
tions), pages 25–26.
John M. Zelle, Cynthia A. Thompson, Mary Elaine
Califf, and Raymond J. Mooney. 1995. Inducing
logic programs without explicit negative examples.
In Proceedings of the Fifth International Workshop
</reference>
<page confidence="0.998525">
539
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953624">
<title confidence="0.969275">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
<author confidence="0.999987">Ni Lao Tom Mitchell William W Cohen</author>
<affiliation confidence="0.99991">Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University</affiliation>
<address confidence="0.999746">5000 Forbes Avenue 5000 Forbes Avenue 5000 Forbes Avenue Pittsburgh, PA 15213 Pittsburgh, PA 15213 Pittsburgh, PA 15213</address>
<email confidence="0.994381">nlao@cs.cmu.edutom.mitchell@cs.cmu.eduwcohen@cs.cmu.edu</email>
<abstract confidence="0.99955824">We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the fifth ACM conference on Digital libraries - DL ’00,</booktitle>
<pages>85--94</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="1514" citStr="Agichtein and Gravano, 2000" startWordPosition="223" endWordPosition="226">ths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s)</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proceedings of the fifth ACM conference on Digital libraries - DL ’00, pages 85–94, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open Information Extraction from the Web. In</title>
<date>2007</date>
<booktitle>IJCAI,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="1607" citStr="Banko et al., 2007" startWordPosition="239" endWordPosition="242">this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a large triple store, whi</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open Information Extraction from the Web. In IJCAI, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaurav Bhalotia</author>
<author>Arvind Hulgeri</author>
<author>Charuta Nakhe</author>
<author>Soumen Chakrabarti</author>
<author>S Sudarshan</author>
</authors>
<title>Keyword searching and browsing in databases using banks. ICDE,</title>
<date>2002</date>
<pages>431--440</pages>
<contexts>
<context position="22042" citStr="Bhalotia et al., 2002" startWordPosition="3607" endWordPosition="3610">elation path to be included in the PRA model, it must retrieve at least one target entity ti in the training set. As we can see from Table 1, together these two constraints dramatically reduce the number of relation paths that need to be considered, relative to systematically enumerating all possible relation paths. L1 regularization reduces the size of the model even more. The idea of finding paths that connects nodes in a graph is not new. It has been embodied previously in first-order learning systems (Richards and Mooney, 1992) as well as N-FOIL, and relational database searching systems (Bhalotia et al., 2002). These approaches consider a single query during path finding. In comparison, the data-driven path finding method we described here uses statistics from a population of queries, and therefore can potentially determine the importance of a path more reliably. B=2 B=3 MRR Time MRR Time RWR(no train) 0.271 0.456 RWR 0.280 3.7s 0.471 9.2s PRA 0.307 5.7s 0.516 15.4s 2.4 Low-Variance Sampling Lao and Cohen (2010a) previously showed that sampling techniques like finger printing and particle filtering can significantly speedup random walk without sacrificing retrieval quality. However, the sampling pr</context>
</contexts>
<marker>Bhalotia, Hulgeri, Nakhe, Chakrabarti, Sudarshan, 2002</marker>
<rawString>Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe, Soumen Chakrabarti, and S. Sudarshan. 2002. Keyword searching and browsing in databases using banks. ICDE, pages 431–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the eleventh annual conference on Computational learning theory - COLT’ 98,</booktitle>
<pages>92--100</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="5227" citStr="Blum and Mitchell, 1998" startWordPosition="817" endWordPosition="820">y. We can measure its learning competence by allowing it to consider the same text documents today as it did yesterday, and recording whether it extracts more beliefs, more accurately today.1 NELL uses a large-scale semi-supervised multitask learning algorithm that couples the training of over 1500 different classifiers and extraction methods (see (Carlson et al., 2010)). Although many of the details of NELL’s learning method are not central to this paper, two points should be noted. First, NELL is a multistrategy learning system, with components that learn from different “views” of the data (Blum and Mitchell, 1998): for instance, one view uses orthographic features of a potential entity name (like “contains capitalized words”), and another uses free-text contexts in which the noun phrase is found (e.g., “X frequently follows the bigram ‘mayor of’ ”). Second, NELL is a bootstrapping system, which self-trains on its growing collection of confident beliefs. 1.2 Knowledge Base Inference: Horn Clauses Although NELL has now grown a sizable knowledge base, its ability to perform inference over this 1NELL’s current KB is available online at http://rtw.ml.cmu.edu. Figure 1: An example subgraph. knowledge base is</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory - COLT’ 98, pages 92–100, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MJ Cafarella</author>
<author>M Banko</author>
<author>O Etzioni</author>
</authors>
<title>Relational Web Search.</title>
<date>2006</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="13721" citStr="Cafarella et al., 2006" startWordPosition="2172" endWordPosition="2175">model learning, the paths between a and b are determined by the statistics from a population of training queries rather than enumerated completely. PRA uses random walks to generate relational features on graph data, and combine them with a logistic regression model. Compared to other relational models (e.g. FOIL, Markov Logic Networks), PRA is extremely efficient at link prediction or retrieval tasks, in which we are interested in identifying top links from a large number of candidates, instead of focusing on a particular node pair or joint inferences. 1.4 Related Work The TextRunner system (Cafarella et al., 2006) answers list queries on a large knowledge base produced by open domain information extraction. Spreading activation is used to measure the closeness of any node to the query term nodes. This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system (Jain and Pantel, 2010) compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type information, which is important for achieving high accuracy predictions. The HOLMES s</context>
</contexts>
<marker>Cafarella, Banko, Etzioni, 2006</marker>
<rawString>MJ Cafarella, M Banko, and O Etzioni. 2006. Relational Web Search. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Callan</author>
<author>Mark Hoy</author>
</authors>
<date>2009</date>
<note>Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/.</note>
<contexts>
<context position="4262" citStr="Callan and Hoy, 2009" startWordPosition="661" endWordPosition="664"> a million, which we refer to as NELL’s (confident) beliefs. NELL had lower confidence in a few million others, which we refer to as its candidate beliefs. NELL is given as input an ontology that defines hundreds of categories (e.g., person, beverage, athlete, sport) and two-place typed relations among these categories (e.g., atheletePlaysSport((athlete), (sport))), which it must learn to extract from the web. It is also provided a set of 10 to 20 positive seed examples of each such category and relation, along with a downloaded collection of 500 million web pages from the ClueWeb2009 corpus (Callan and Hoy, 2009) as unlabeled data, and access to 100,000 queries each day to Google’s search engine. Each day, NELL has two tasks: (1) to extract additional beliefs from the web to populate its growing knowledge base (KB) with instances of the categories and relations in its ontology, and (2) to learn to perform task 1 better today than it could yesterday. We can measure its learning competence by allowing it to consider the same text documents today as it did yesterday, and recording whether it extracts more beliefs, more accurately today.1 NELL uses a large-scale semi-supervised multitask learning algorith</context>
</contexts>
<marker>Callan, Hoy, 2009</marker>
<rawString>Jamie Callan and Mark Hoy. 2009. Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>M Tom</author>
</authors>
<booktitle>on Inductive Logic Programming (ILP-95),</booktitle>
<pages>403--416</pages>
<location>Leuven, Belgium.</location>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Tom, </marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. on Inductive Logic Programming (ILP-95), pages 403–416, Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell</author>
</authors>
<title>Toward an Architecture for Never-Ending Language Learning.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<marker>Mitchell, 2010</marker>
<rawString>Mitchell. 2010. Toward an Architecture for Never-Ending Language Learning. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>David Page</author>
</authors>
<title>Polynomial learnability and inductive logic programming: Methods and results.</title>
<date>1995</date>
<journal>New Generation Comput.,</journal>
<pages>13--3</pages>
<contexts>
<context position="7521" citStr="Cohen and Page, 1995" startWordPosition="1162" endWordPosition="1165">gue(HinesWard,NBA)), and uses a “separate-and-conquer” strategy to learn a set of Horn clauses that fit the data well. Each Horn clause is learned by starting with a general rule and progressively specializing it, so that it still covers many positive examples but covers few negative examples. After a clause is learned, the examples covered by that clause are removed from the training set, and the process repeats until no positive examples remain. Learning first-order Horn clauses is computationally expensive—not only is the search space large, but some Horn clauses can be costly to evaluate (Cohen and Page, 1995). N-FOIL uses two tricks to improve its scalability. First, it assumes that the consequent predicate is functional—e.g., that AthletePlays ForTeam HinesWard Eli Manning TeamPlays Steelers TeamPlays InLeague TeamPlays InLeague Giants MLB AthletePlays ForTeam InLeague NFL 530 each Athlete plays in at most one League. This each performing a random walk through the graph, means that explicit negative examples need not constrained to follow that sequence of edge types, be provided (Zelle et al., 1995): e.g., if Ath- and ranking nodes b by their weights in the resulting letePlaysInLeague(HinesWard,N</context>
</contexts>
<marker>Cohen, Page, 1995</marker>
<rawString>William W. Cohen and David Page. 1995. Polynomial learnability and inductive logic programming: Methods and results. New Generation Comput., 13(3&amp;4):369–409.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Anamaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderl</author>
<author>S Daniel</author>
</authors>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderl, Daniel, </marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Anamaria Popescu, Tal Shaked, Stephen Soderl, Daniel S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weld</author>
<author>Er Yates</author>
</authors>
<title>Unsupervised namedentity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>165--91</pages>
<marker>Weld, Yates, 2005</marker>
<rawString>Weld, and Er Yates. 2005. Unsupervised namedentity extraction from the web: An experimental study. Artificial Intelligence, 165:91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In WWW,</booktitle>
<pages>517--526</pages>
<contexts>
<context position="24227" citStr="Haveliwala, 2002" startWordPosition="3958" endWordPosition="3959">[0, M−1]. Then LVS repeatedly adds the fixed amount M−1 to r and chooses x values corresponding to the resulting numbers. 3 Results This section reports empirical results of applying random walk inference to NELL’s knowledge base after the 165th iteration of its learning process. We first investigate PRA’s behavior by cross validation on the training queries. Then we compare PRA and N-FOIL’s ability to reliably infer new beliefs, by leveraging the Amazon Mechanical Turk service. 534 3.1 Cross Validation on the Training Queries Random Walk with Restart (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a general-purpose graph proximity measure which has been shown to be fairly successful for many types of tasks. We compare PRA to two versions of RWR on the 96 tasks of link prediction with NELL’s knowledge base. The two baseline methods are an untrained RWR model and a trained RWR model as described by Lao and Cohen (2010b). (In brief, in the trained RWR model, the walker will probabilistically prefer to follow edges associated with different labels, where the weight for each edge label is chosen to minimize a loss function, such as Equation 7. In the untrained model, edge weights are un</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alpa Jain</author>
<author>Patrick Pantel</author>
</authors>
<title>Factrank: Random walks on a web of facts.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>501--509</pages>
<contexts>
<context position="14064" citStr="Jain and Pantel, 2010" startWordPosition="2230" endWordPosition="2233">is extremely efficient at link prediction or retrieval tasks, in which we are interested in identifying top links from a large number of candidates, instead of focusing on a particular node pair or joint inferences. 1.4 Related Work The TextRunner system (Cafarella et al., 2006) answers list queries on a large knowledge base produced by open domain information extraction. Spreading activation is used to measure the closeness of any node to the query term nodes. This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system (Jain and Pantel, 2010) compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type information, which is important for achieving high accuracy predictions. The HOLMES system (Schoenmackers et al., 2008) derives new assertions using a few manually written inference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatic</context>
</contexts>
<marker>Jain, Pantel, 2010</marker>
<rawString>Alpa Jain and Patrick Pantel. 2010. Factrank: Random walks on a web of facts. In COLING, pages 501–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Fast query execution for retrieval models based on path-constrained random walks.</title>
<date>2010</date>
<publisher>KDD.</publisher>
<contexts>
<context position="976" citStr="Lao and Cohen, 2010" startWordPosition="141" endWordPosition="144">u.edu Abstract We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacch</context>
<context position="10267" citStr="Lao and Cohen (2010" startWordPosition="1587" endWordPosition="1590">ning data, N− is the number of negative instances matched, m = 5 and prior = 0.5. As the results below show, N-FOIL generally learns a small number of high-precision inference rules. One important role of these inference rules is that they contribute to the bootstrapping procedure, as inferences made by N-FOIL increase either the number of candidate beliefs, or (if the inference is already a candidate) improve NELL’s confidence in candidate beliefs. 1.3 Knowledge Base Inference: Graph Random Walks In this paper, we consider an alternative approach, based on the Path Ranking Algorithm (PRA) of Lao and Cohen (2010b), described in detail below. PRA learns to rank graph nodes b relative to a query node a. PRA begins by enumerating a large set of bounded-length edge-labeled path types, similar to the initial clauses used in NELL’s variant of FOIL. These path types are treated as ranking “experts”, 531 Suppose a random walk starts at a query node a (say a=HinesWard). If HinesWard is linked to the single concept node ProfessionalAthlete via isa, the walk will reach that node with probability 1 after one step. If A is the set of ProfessionalAthlete’s in the KB, then after two steps, the walk will have probab</context>
<context position="12882" citStr="Lao and Cohen, 2010" startWordPosition="2039" endWordPosition="2042">elation instance R(a, b) is based on many existing paths between a and b in the current KB, combined using a learned logistic function. • The confidence in an inference is sensitive to the current state of the knowledge base, and the specific entities being queried (since the paths used in the inference have these properties). • Experimentally, the inference method yields many more moderately-confident inferences than the Horn clauses learned by N-FOIL. • The learning and inference are more efficient than N-FOIL, in part because we can exploit efficient approximation schemes for random walks (Lao and Cohen, 2010a). The resulting inference is as fast as 10 milliseconds per query on average. The Path Ranking Algorithm (PRA) we use is similar to that described elsewhere (Lao and Cohen, 2010b), except that to achieve efficient model learning, the paths between a and b are determined by the statistics from a population of training queries rather than enumerated completely. PRA uses random walks to generate relational features on graph data, and combine them with a logistic regression model. Compared to other relational models (e.g. FOIL, Markov Logic Networks), PRA is extremely efficient at link predictio</context>
<context position="15029" citStr="Lao and Cohen, 2010" startWordPosition="2371" endWordPosition="2374">ference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatically from training data. Similarly, the Markov Logic Networks (Richardson and Domingos, 2006) are Markov networks constructed corresponding to the grounding of rules to knowledge bases. In comparison, our proposed approach is much more efficient by avoiding the harder problem of joint inferences and by leveraging efficient random walk schemes (Lao and Cohen, 2010a). Below we describe our approach in greater detail, provide experimental evidence of its value for performing inference in NELL’s knowledge base, and discuss implications of this work and directions for future research. 2 Approach In this section, we first describe how we formulate link (relation) prediction on a knowledge base as a ranking task. Then we review the Path Ranking Algorithm (PRA) introduced by Lao and Cohen (2010b; 2010a). After that, we describe two improvements to the PRA method to make it more suitable for the task of link prediction in knowledge bases. The first improvement</context>
<context position="17562" citStr="Lao and Cohen (2010" startWordPosition="2809" endWordPosition="2812">ths. This knowledge base with any other node is treated as a gives a ranking of nodes e related to the query node training query, the actual nodes b in the knowledge s by the following scoring function base known to satisfy R(a, b) are treated as labeled Escore(e; s) = hs,P(e)0P, (6) positive examples, and any other nodes are treated P∈P` as negative examples. where P` is the set of relation paths with length ≤ E. 2.2 Path Ranking Algorithm Review Given a relation R and a set of node pairs We now review the Path Ranking Algorithm {(si, ti)} for which we know whether R(si, ti) is introduced by Lao and Cohen (2010b). A relation true or not, we can construct a training dataset path P is defined as a sequence of relations D = {(xi, yi)}, where xi is a vector of all the R1 ... R`, and in order to emphasize the types path features for the pair (si, ti)—i.e., the j-th associated with each step, P can also be written as component of xi is hsi,Pj(ti), and where yi is a T0 −−→ ... R` boolean variable indicating whether R(si, ti) is true. R1 −→ T`, where Ti = range(Ri) = We then train a logistic function to predict the domain(Ri+1), and we also define domain(P) ≡ conditional probability P(y|x; 0). The parameter</context>
<context position="22451" citStr="Lao and Cohen (2010" startWordPosition="3672" endWordPosition="3675">ects nodes in a graph is not new. It has been embodied previously in first-order learning systems (Richards and Mooney, 1992) as well as N-FOIL, and relational database searching systems (Bhalotia et al., 2002). These approaches consider a single query during path finding. In comparison, the data-driven path finding method we described here uses statistics from a population of queries, and therefore can potentially determine the importance of a path more reliably. B=2 B=3 MRR Time MRR Time RWR(no train) 0.271 0.456 RWR 0.280 3.7s 0.471 9.2s PRA 0.307 5.7s 0.516 15.4s 2.4 Low-Variance Sampling Lao and Cohen (2010a) previously showed that sampling techniques like finger printing and particle filtering can significantly speedup random walk without sacrificing retrieval quality. However, the sampling procedures can induce a loss of diversity in the particle population. For example, consider a node in the graph with just two out links with equal weights, and suppose we are required to generate two walkers starting from this node. A disappointing result is that with 50 percent chance both walkers will follow the same branch, and leave the other branch with no probability mass. To overcome this problem, we </context>
<context position="24556" citStr="Lao and Cohen (2010" startWordPosition="4015" endWordPosition="4018">alidation on the training queries. Then we compare PRA and N-FOIL’s ability to reliably infer new beliefs, by leveraging the Amazon Mechanical Turk service. 534 3.1 Cross Validation on the Training Queries Random Walk with Restart (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a general-purpose graph proximity measure which has been shown to be fairly successful for many types of tasks. We compare PRA to two versions of RWR on the 96 tasks of link prediction with NELL’s knowledge base. The two baseline methods are an untrained RWR model and a trained RWR model as described by Lao and Cohen (2010b). (In brief, in the trained RWR model, the walker will probabilistically prefer to follow edges associated with different labels, where the weight for each edge label is chosen to minimize a loss function, such as Equation 7. In the untrained model, edge weights are uniform.) We explored a range of values for the regularization parameters L1 and L2 using cross validation on the training data, and we fix both L1 and L2 parameters to 0.001 for all tasks. The maximum path length is fixed to 3.3 Table 2 compares the three methods using 5-fold cross validation and the Mean Reciprocal Rank (MRR)4 </context>
<context position="25876" citStr="Lao and Cohen, 2010" startWordPosition="4231" endWordPosition="4234">ts. If the the first returned result is relevant, then MRR is 1.0, otherwise, it is smaller than 1.0. Supervised training can significantly improve retrieval quality (p-value=9 × 10−8 comparing untrained and trained RWR), and leveraging path information can produce further improvement (p-value=4 × 10−4 comparing trained RWR with PRA). The average training time for a predicate is only a few seconds. We also investigate the effect of low-variance sampling on the quality of prediction. Figure 2 compares independent and low variance sampling when applied to finger printing and particle filtering (Lao and Cohen, 2010a). The horizontal axis corresponds to the speedup of random walk compared with exact inference, and the vertical axis measures the quality of prediction by MRR with three fold cross validation on the training query set. Low-variance 3Results with maximum length 4 are not reported here. Generally models with length 4 paths produce slightly better results, but are 4-5 times slower to train 4For a set of queries Q, MRR = |Q |Eq∈Q rank of the first correct answer for q 0 1 2 3 4 5 Random Walk Speedup Figure 2: Compare inference speed and quality over 96 tasks. The speedup is relative to exact inf</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010a. Fast query execution for retrieval models based on path-constrained random walks. KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="976" citStr="Lao and Cohen, 2010" startWordPosition="141" endWordPosition="144">u.edu Abstract We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacch</context>
<context position="10267" citStr="Lao and Cohen (2010" startWordPosition="1587" endWordPosition="1590">ning data, N− is the number of negative instances matched, m = 5 and prior = 0.5. As the results below show, N-FOIL generally learns a small number of high-precision inference rules. One important role of these inference rules is that they contribute to the bootstrapping procedure, as inferences made by N-FOIL increase either the number of candidate beliefs, or (if the inference is already a candidate) improve NELL’s confidence in candidate beliefs. 1.3 Knowledge Base Inference: Graph Random Walks In this paper, we consider an alternative approach, based on the Path Ranking Algorithm (PRA) of Lao and Cohen (2010b), described in detail below. PRA learns to rank graph nodes b relative to a query node a. PRA begins by enumerating a large set of bounded-length edge-labeled path types, similar to the initial clauses used in NELL’s variant of FOIL. These path types are treated as ranking “experts”, 531 Suppose a random walk starts at a query node a (say a=HinesWard). If HinesWard is linked to the single concept node ProfessionalAthlete via isa, the walk will reach that node with probability 1 after one step. If A is the set of ProfessionalAthlete’s in the KB, then after two steps, the walk will have probab</context>
<context position="12882" citStr="Lao and Cohen, 2010" startWordPosition="2039" endWordPosition="2042">elation instance R(a, b) is based on many existing paths between a and b in the current KB, combined using a learned logistic function. • The confidence in an inference is sensitive to the current state of the knowledge base, and the specific entities being queried (since the paths used in the inference have these properties). • Experimentally, the inference method yields many more moderately-confident inferences than the Horn clauses learned by N-FOIL. • The learning and inference are more efficient than N-FOIL, in part because we can exploit efficient approximation schemes for random walks (Lao and Cohen, 2010a). The resulting inference is as fast as 10 milliseconds per query on average. The Path Ranking Algorithm (PRA) we use is similar to that described elsewhere (Lao and Cohen, 2010b), except that to achieve efficient model learning, the paths between a and b are determined by the statistics from a population of training queries rather than enumerated completely. PRA uses random walks to generate relational features on graph data, and combine them with a logistic regression model. Compared to other relational models (e.g. FOIL, Markov Logic Networks), PRA is extremely efficient at link predictio</context>
<context position="15029" citStr="Lao and Cohen, 2010" startWordPosition="2371" endWordPosition="2374">ference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatically from training data. Similarly, the Markov Logic Networks (Richardson and Domingos, 2006) are Markov networks constructed corresponding to the grounding of rules to knowledge bases. In comparison, our proposed approach is much more efficient by avoiding the harder problem of joint inferences and by leveraging efficient random walk schemes (Lao and Cohen, 2010a). Below we describe our approach in greater detail, provide experimental evidence of its value for performing inference in NELL’s knowledge base, and discuss implications of this work and directions for future research. 2 Approach In this section, we first describe how we formulate link (relation) prediction on a knowledge base as a ranking task. Then we review the Path Ranking Algorithm (PRA) introduced by Lao and Cohen (2010b; 2010a). After that, we describe two improvements to the PRA method to make it more suitable for the task of link prediction in knowledge bases. The first improvement</context>
<context position="17562" citStr="Lao and Cohen (2010" startWordPosition="2809" endWordPosition="2812">ths. This knowledge base with any other node is treated as a gives a ranking of nodes e related to the query node training query, the actual nodes b in the knowledge s by the following scoring function base known to satisfy R(a, b) are treated as labeled Escore(e; s) = hs,P(e)0P, (6) positive examples, and any other nodes are treated P∈P` as negative examples. where P` is the set of relation paths with length ≤ E. 2.2 Path Ranking Algorithm Review Given a relation R and a set of node pairs We now review the Path Ranking Algorithm {(si, ti)} for which we know whether R(si, ti) is introduced by Lao and Cohen (2010b). A relation true or not, we can construct a training dataset path P is defined as a sequence of relations D = {(xi, yi)}, where xi is a vector of all the R1 ... R`, and in order to emphasize the types path features for the pair (si, ti)—i.e., the j-th associated with each step, P can also be written as component of xi is hsi,Pj(ti), and where yi is a T0 −−→ ... R` boolean variable indicating whether R(si, ti) is true. R1 −→ T`, where Ti = range(Ri) = We then train a logistic function to predict the domain(Ri+1), and we also define domain(P) ≡ conditional probability P(y|x; 0). The parameter</context>
<context position="22451" citStr="Lao and Cohen (2010" startWordPosition="3672" endWordPosition="3675">ects nodes in a graph is not new. It has been embodied previously in first-order learning systems (Richards and Mooney, 1992) as well as N-FOIL, and relational database searching systems (Bhalotia et al., 2002). These approaches consider a single query during path finding. In comparison, the data-driven path finding method we described here uses statistics from a population of queries, and therefore can potentially determine the importance of a path more reliably. B=2 B=3 MRR Time MRR Time RWR(no train) 0.271 0.456 RWR 0.280 3.7s 0.471 9.2s PRA 0.307 5.7s 0.516 15.4s 2.4 Low-Variance Sampling Lao and Cohen (2010a) previously showed that sampling techniques like finger printing and particle filtering can significantly speedup random walk without sacrificing retrieval quality. However, the sampling procedures can induce a loss of diversity in the particle population. For example, consider a node in the graph with just two out links with equal weights, and suppose we are required to generate two walkers starting from this node. A disappointing result is that with 50 percent chance both walkers will follow the same branch, and leave the other branch with no probability mass. To overcome this problem, we </context>
<context position="24556" citStr="Lao and Cohen (2010" startWordPosition="4015" endWordPosition="4018">alidation on the training queries. Then we compare PRA and N-FOIL’s ability to reliably infer new beliefs, by leveraging the Amazon Mechanical Turk service. 534 3.1 Cross Validation on the Training Queries Random Walk with Restart (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a general-purpose graph proximity measure which has been shown to be fairly successful for many types of tasks. We compare PRA to two versions of RWR on the 96 tasks of link prediction with NELL’s knowledge base. The two baseline methods are an untrained RWR model and a trained RWR model as described by Lao and Cohen (2010b). (In brief, in the trained RWR model, the walker will probabilistically prefer to follow edges associated with different labels, where the weight for each edge label is chosen to minimize a loss function, such as Equation 7. In the untrained model, edge weights are uniform.) We explored a range of values for the regularization parameters L1 and L2 using cross validation on the training data, and we fix both L1 and L2 parameters to 0.001 for all tasks. The maximum path length is fixed to 3.3 Table 2 compares the three methods using 5-fold cross validation and the Mean Reciprocal Rank (MRR)4 </context>
<context position="25876" citStr="Lao and Cohen, 2010" startWordPosition="4231" endWordPosition="4234">ts. If the the first returned result is relevant, then MRR is 1.0, otherwise, it is smaller than 1.0. Supervised training can significantly improve retrieval quality (p-value=9 × 10−8 comparing untrained and trained RWR), and leveraging path information can produce further improvement (p-value=4 × 10−4 comparing trained RWR with PRA). The average training time for a predicate is only a few seconds. We also investigate the effect of low-variance sampling on the quality of prediction. Figure 2 compares independent and low variance sampling when applied to finger printing and particle filtering (Lao and Cohen, 2010a). The horizontal axis corresponds to the speedup of random walk compared with exact inference, and the vertical axis measures the quality of prediction by MRR with three fold cross validation on the training query set. Low-variance 3Results with maximum length 4 are not reported here. Generally models with length 4 paths produce slightly better results, but are 4-5 times slower to train 4For a set of queries Q, MRR = |Q |Eq∈Q rank of the first correct answer for q 0 1 2 3 4 5 Random Walk Speedup Figure 2: Compare inference speed and quality over 96 tasks. The speedup is relative to exact inf</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010b. Relational retrieval using a combination of path-constrained random walks. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Learning graph walk based similarity measures for parsed text. EMNLP,</title>
<date>2008</date>
<pages>907--916</pages>
<contexts>
<context position="20785" citStr="Minkov and Cohen, 2008" startWordPosition="3383" endWordPosition="3386"> Averaged over 96 tasks. B=3 B=4 all paths up to length L 15,376 1,906,624 +query support≥ α = 0.01 522 5016 +ever reach a target entity 136 792 +Ll regularization 63 271 enumeration; however, for domains with a large number of edge types (e.g., a knowledge base), it is impractical to enumerate all possible relation paths even for small B. For instance, if the number of edge types related to each node type is 100, even the number of length three paths types easily reaches millions. For other domains like parsed natural language sentences, useful relation paths can be as long as ten relations (Minkov and Cohen, 2008). In this case, even with smaller number of possible edge types, the total number of relation paths is still too large for systematic enumeration. In order to apply PRA to these domains, we modify the path generation procedure in PRA to produce only relation paths which are potentially useful for the task. Define a query s to be supporting a path P if hs,P(e) =� 0 for any entity e. We require that any path node created during path finding needs to be supported by at least a fraction α of the training queries si, as well as being of length no more than B (In the experiments, we set α = 0.01) We</context>
</contexts>
<marker>Minkov, Cohen, 2008</marker>
<rawString>Einat Minkov and William W. Cohen. 2008. Learning graph walk based similarity measures for parsed text. EMNLP, pages 907–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1587" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="235" endWordPosition="238">Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a lar</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
<author>R Mike Cameron-Jones</author>
</authors>
<title>FOIL: A Midterm Report.</title>
<date>1993</date>
<booktitle>In ECML,</booktitle>
<pages>3--20</pages>
<contexts>
<context position="6731" citStr="Quinlan and Cameron-Jones, 1993" startWordPosition="1044" endWordPosition="1047">ue(b, c) ==�, AthletePlaysInLeague(a,c) to infer that AthletePlaysInLeague(HinesWard,NFL), if it has already extracted the beliefs in the preconditions of the rule, with variables a, b and c bound to HinesWard, PittsburghSteelers and NFL respectively as shown in Figure 1. NELL currently has a set of approximately 600 such rules, which it has learned by data mining its knowledge base of beliefs. Each learned rule carries a conditional probability that its conclusion will hold, given that its preconditions are satisfied. NELL learns these Horn clause rules using a variant of the FOIL algorithm (Quinlan and Cameron-Jones, 1993), henceforth N-FOIL. N-FOIL takes as input a set of positive and negative examples of a rule’s consequent (e.g., +AthletePlaysInLeague(HinesWard,NFL), −AthletePlaysInLeague(HinesWard,NBA)), and uses a “separate-and-conquer” strategy to learn a set of Horn clauses that fit the data well. Each Horn clause is learned by starting with a general rule and progressively specializing it, so that it still covers many positive examples but covers few negative examples. After a clause is learned, the examples covered by that clause are removed from the training set, and the process repeats until no posit</context>
</contexts>
<marker>Quinlan, Cameron-Jones, 1993</marker>
<rawString>J. Ross Quinlan and R. Mike Cameron-Jones. 1993. FOIL: A Midterm Report. In ECML, pages 3–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley L Richards</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning relations by pathfinding.</title>
<date>1992</date>
<booktitle>In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92),</booktitle>
<pages>50--55</pages>
<location>San Jose, CA,</location>
<contexts>
<context position="21957" citStr="Richards and Mooney, 1992" startWordPosition="3594" endWordPosition="3597">o more than B (In the experiments, we set α = 0.01) We also require that in order for a relation path to be included in the PRA model, it must retrieve at least one target entity ti in the training set. As we can see from Table 1, together these two constraints dramatically reduce the number of relation paths that need to be considered, relative to systematically enumerating all possible relation paths. L1 regularization reduces the size of the model even more. The idea of finding paths that connects nodes in a graph is not new. It has been embodied previously in first-order learning systems (Richards and Mooney, 1992) as well as N-FOIL, and relational database searching systems (Bhalotia et al., 2002). These approaches consider a single query during path finding. In comparison, the data-driven path finding method we described here uses statistics from a population of queries, and therefore can potentially determine the importance of a path more reliably. B=2 B=3 MRR Time MRR Time RWR(no train) 0.271 0.456 RWR 0.280 3.7s 0.471 9.2s PRA 0.307 5.7s 0.516 15.4s 2.4 Low-Variance Sampling Lao and Cohen (2010a) previously showed that sampling techniques like finger printing and particle filtering can significantl</context>
<context position="38088" citStr="Richards and Mooney, 1992" startWordPosition="6226" endWordPosition="6229">d this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL. This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100. The inference and learning are both very efficient—our experiment shows that the inference time is as fast as 10 milliseconds per query on average, and the training for a predicate takes only a few seconds. There are several prominent directions for future work. First, inference starting from both the query nodes and target nodes (Richards and Mooney, 1992) can be much more efficient in discovering long paths than just inference from the query nodes. Second, inference starting from the target nodes of training queries is a potential way to discover specialized paths (with grounded nodes). Third, generalizing inference paths to inference trees or graphs can produce more expressive random walk inference models. Overall, we believe that random walk is a promising way to scale up relational learning to domains with very large data sets. Acknowledgments This work was supported by NIH under grant R01GM081293, by NSF under grant IIS0811562, by DARPA un</context>
</contexts>
<marker>Richards, Mooney, 1992</marker>
<rawString>Bradley L. Richards and Raymond J. Mooney. 1992. Learning relations by pathfinding. In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), pages 50–55, San Jose, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="1966" citStr="Richardson and Domingos, 2006" startWordPosition="288" endWordPosition="291">d is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a large triple store, which can be represented as a labeled, directed graph in which each entity a is a node, each binary relation R(a, b) is an edge labeled R between a and b, and unary concepts C(a) are represented as an edge labeled “isa” between the node for the entity a and a node for the concept C. We present a trainable inference method that learns to infer relations by comb</context>
<context position="14757" citStr="Richardson and Domingos, 2006" startWordPosition="2329" endWordPosition="2333">ining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type information, which is important for achieving high accuracy predictions. The HOLMES system (Schoenmackers et al., 2008) derives new assertions using a few manually written inference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatically from training data. Similarly, the Markov Logic Networks (Richardson and Domingos, 2006) are Markov networks constructed corresponding to the grounding of rules to knowledge bases. In comparison, our proposed approach is much more efficient by avoiding the harder problem of joint inferences and by leveraging efficient random walk schemes (Lao and Cohen, 2010a). Below we describe our approach in greater detail, provide experimental evidence of its value for performing inference in NELL’s knowledge base, and discuss implications of this work and directions for future research. 2 Approach In this section, we first describe how we formulate link (relation) prediction on a knowledge b</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling Textual Inference to the Web. In</title>
<date>2008</date>
<booktitle>EMNLP,</booktitle>
<pages>79--88</pages>
<contexts>
<context position="14355" citStr="Schoenmackers et al., 2008" startWordPosition="2271" endWordPosition="2274">s list queries on a large knowledge base produced by open domain information extraction. Spreading activation is used to measure the closeness of any node to the query term nodes. This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system (Jain and Pantel, 2010) compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type information, which is important for achieving high accuracy predictions. The HOLMES system (Schoenmackers et al., 2008) derives new assertions using a few manually written inference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatically from training data. Similarly, the Markov Logic Networks (Richardson and Domingos, 2006) are Markov networks constructed corresponding to the grounding of rules to knowledge bases. In comparison, our proposed approach is much more efficient by avoiding the harder problem of joint infer</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld. 2008. Scaling Textual Inference to the Web. In EMNLP, pages 79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Taxonomy Induction from Heterogenous Evidence.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1555" citStr="Snow et al., 2006" startWordPosition="231" endWordPosition="234">Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowle</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic Taxonomy Induction from Heterogenous Evidence. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Thrun</author>
<author>Wolfram Burgard</author>
<author>Dieter Fox</author>
</authors>
<title>Probabilistic Robotics (Intelligent Robotics and Autonomous Agents).</title>
<date>2005</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="23124" citStr="Thrun et al., 2005" startWordPosition="3776" endWordPosition="3779">ger printing and particle filtering can significantly speedup random walk without sacrificing retrieval quality. However, the sampling procedures can induce a loss of diversity in the particle population. For example, consider a node in the graph with just two out links with equal weights, and suppose we are required to generate two walkers starting from this node. A disappointing result is that with 50 percent chance both walkers will follow the same branch, and leave the other branch with no probability mass. To overcome this problem, we apply a technique called Low-Variance Sampling (LVS) (Thrun et al., 2005), which is commonly used in robotics to improve the quality of sampling. Instead of generating independent samples from a distribution, LVS uses a single random number to generate all samples, which are evenly distributed across the whole distribution. Note that given a distribution P(x), any number r in [0, 1] points to exactly one � x value, namely x = arg minj m=1..j P(m) ≤ r. Suppose we want to generate M samples from P(x). LVS first generates a random number r in the interval [0, M−1]. Then LVS repeatedly adds the fixed amount M−1 to r and chooses x values corresponding to the resulting n</context>
</contexts>
<marker>Thrun, Burgard, Fox, 2005</marker>
<rawString>Sebastian Thrun, Wolfram Burgard, and Dieter Fox. 2005. Probabilistic Robotics (Intelligent Robotics and Autonomous Agents). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Michele Banko</author>
<author>Matthew Broadhead</author>
<author>Michael J Cafarella</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<date>2007</date>
<booktitle>TextRunner: Open Information Extraction on the Web. In HLT-NAACL (Demonstrations),</booktitle>
<pages>25--26</pages>
<contexts>
<context position="1628" citStr="Yates et al., 2007" startWordPosition="243" endWordPosition="246">nowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a large triple store, which can be represented</context>
</contexts>
<marker>Yates, Banko, Broadhead, Cafarella, Etzioni, Soderland, 2007</marker>
<rawString>Alexander Yates, Michele Banko, Matthew Broadhead, Michael J. Cafarella, Oren Etzioni, and Stephen Soderland. 2007. TextRunner: Open Information Extraction on the Web. In HLT-NAACL (Demonstrations), pages 25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Inducing logic programs without explicit negative examples.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth International Workshop</booktitle>
<contexts>
<context position="8022" citStr="Zelle et al., 1995" startWordPosition="1236" endWordPosition="1239">y expensive—not only is the search space large, but some Horn clauses can be costly to evaluate (Cohen and Page, 1995). N-FOIL uses two tricks to improve its scalability. First, it assumes that the consequent predicate is functional—e.g., that AthletePlays ForTeam HinesWard Eli Manning TeamPlays Steelers TeamPlays InLeague TeamPlays InLeague Giants MLB AthletePlays ForTeam InLeague NFL 530 each Athlete plays in at most one League. This each performing a random walk through the graph, means that explicit negative examples need not constrained to follow that sequence of edge types, be provided (Zelle et al., 1995): e.g., if Ath- and ranking nodes b by their weights in the resulting letePlaysInLeague(HinesWard,NFL) is a positive distribution. Finally, PRA combines the weights example, then AthletePlaysInLeague(HinesWard,c0) contributed by different “experts” using logistic for any other value of c0 is negative. In general, regression to predict the probability that the relation this constraint guides the search algorithm toward R(a, b) is satisfied. Horn clauses that have fewer possible instantiations, As an example, consider a path from a to b via and hence are less expensive to match. Second, the sequ</context>
</contexts>
<marker>Zelle, Thompson, Califf, Mooney, 1995</marker>
<rawString>John M. Zelle, Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1995. Inducing logic programs without explicit negative examples. In Proceedings of the Fifth International Workshop</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>