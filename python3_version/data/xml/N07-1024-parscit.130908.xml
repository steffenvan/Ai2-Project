<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002877">
<title confidence="0.999487">
Hybrid Models for Semantic Classification of
Chinese Unknown Words
</title>
<author confidence="0.998779">
Xiaofei Lu
</author>
<affiliation confidence="0.9988465">
Department of Linguistics and Applied Language Studies
Pennsylvania State University
</affiliation>
<address confidence="0.832469">
University Park, PA 16802, USA
</address>
<email confidence="0.999473">
xxl13@psu.edu
</email>
<sectionHeader confidence="0.993911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961058823529">
This paper addresses the problem of clas-
sifying Chinese unknown words into
fine-grained semantic categories defined
in a Chinese thesaurus. We describe
three novel knowledge-based models that
capture the relationship between the se-
mantic categories of an unknown word
and those of its component characters in
three different ways. We then combine
two of the knowledge-based models with
a corpus-based model which classifies
unknown words using contextual infor-
mation. Experiments show that the
knowledge-based models outperform
previous methods on the same task, but
the use of contextual information does
not further improve performance.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936540983607">
Research on semantic annotation has focused
primarily on word sense disambiguation (WSD),
i.e., the task of determining the appropriate sense
for each instance of a polysemous word out of a
set of senses defined for the word in some lexi-
con. Much less work has been done on semantic
classification of unknown words, i.e., words that
are not listed in the lexicon. However, real texts
typically contain a large number of unknown
words. Successful classification of unknown
words is not only useful for lexical acquisition,
but also necessary for natural language process-
ing (NLP) tasks that require semantic annotation.
This paper addresses the problem of classify-
ing Chinese unknown words into fine-grained
semantic categories defined in a Chinese thesau-
rus, Cilin (Mei et al., 1984). This thesaurus clas-
sifies over 70,000 words into 12 major catego-
ries, including human (A), concrete object (B),
time and space (C), abstract object (D), attributes
(E), actions (F), mental activities (G), activities
(H), physical states (I), relations (J), auxiliaries
(K), and honorifics (L). The 12 major categories
are further divided into 94 medium categories,
which in turn are subdivided into 1428 small
categories. Each small category contains syno-
nyms that are close in meaning. For example,
under the major category D, the medium cate-
gory Dm groups all words that refer to institu-
tions, and the small category Dm05 groups all
words that refer to educational institutions, e.g.,
学校 xuéxiào ‘school’. Unknown word classifi-
cation involves a much larger search space than
WSD. In classifying words into small categories
in Cilin, the search space for a polysemous
known word consists of all the categories the
word belongs to, but that for an unknown word
consists of all the 1428 small categories.
Research on WSD has concentrated on using
contextual information, which may be limited
for infrequent unknown words. On the other
hand, Chinese characters carry semantic infor-
mation that is useful for predicting the semantic
properties of the words containing them. We pre-
sent three novel knowledge-based models that
capture the relationship between the semantic
categories of an unknown word and those of its
component characters in three different ways,
and combine two of them with a corpus-based
model that uses contextual information to clas-
sify unknown words. Experiments show that the
combined knowledge-based model achieves an
accuracy of 61.6% for classifying unknown
words into small categories in Cilin, but the use
of contextual information does not further im-
prove performance.
The rest of the paper is organized as follows.
Section 2 details the three novel knowledge-
based models proposed for this task. Section 3
describes a corpus-based model. Section 4 re-
ports the experiment results of the proposed
</bodyText>
<page confidence="0.99037">
188
</page>
<note confidence="0.797783">
Proceedings of NAACL HLT 2007, pages 188–195,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.995316666666667">
In each variant, the category with the maxi-
mum score for a target word is proposed as the
category of the word.
models. Section 5 compares these results with
previous results. Section 6 concludes the paper
and points to avenues for further research.
</bodyText>
<sectionHeader confidence="0.996505" genericHeader="method">
2 Knowledge-based Models
</sectionHeader>
<bodyText confidence="0.9999265">
This section describes three novel knowledge-
based models for semantic classification of Chi-
nese unknown words, including an overlapping-
character model, a character-category association
model, and a rule-based model. These models
model the relationship between the semantic
category of an unknown word and those of its
component characters in three different ways.
</bodyText>
<subsectionHeader confidence="0.941443">
2.1 The Baseline Model
</subsectionHeader>
<bodyText confidence="0.99999">
The baseline model predicts the category of an
unknown word by counting the number of over-
lapping characters between the unknown word
and the member words in each category. As
words in the same category are similar in mean-
ing and the meaning of a Chinese word is gener-
ally the composition of the meanings of its char-
acters, it is common for words in the same cate-
gory to share one or more character. This model
tests the hypothesis that speakers draw upon the
repertoire of characters that relate to a concept
when creating new words to realize it.
For each semantic category in Cilin, the set of
unique characters in its member words are ex-
tracted, and the number of times each character
occurs in word-initial, word-middle, and word-
final positions is recorded. With this informa-
tion, we develop two variants of the baseline
model, which differ from each other in terms of
whether it takes into consideration the positions
in which the characters occur in words.
In variant 1, the score of a category is the sum
of the number of occurrences of each character
of the target word in the category, as in (1),
where tj denotes a category, w denotes the target
word, ci denotes the ith character in w, n is the
length of w, and f(ci) is the frequency of ci in tj.
</bodyText>
<equation confidence="0.990885">
n
(1) Score tj w
( , ) = E
i=1
</equation>
<bodyText confidence="0.895106625">
In variant 2, the score of a category is the sum
of the number of occurrences of each character
of the unknown word in the category in its corre-
sponding position, as in (2), where pi denotes the
position of ci in w, which could be word-initial,
word-middle, or word-final, and f(ci,pi) denotes
the frequency of ci in position pi in tj.
n
</bodyText>
<page confidence="0.8350755">
(2) Score(tj , w) = E
i=1
</page>
<subsectionHeader confidence="0.978414">
2.2 Character-Category Associations
</subsectionHeader>
<bodyText confidence="0.999877176470588">
The relationship between the semantic category
of an unknown word and those of its component
characters can also be captured in a more sophis-
ticated way using information-theoretical models.
We use two statistical measures, mutual infor-
mation and χ2, to compute character-category
associations and word-category associations.
Chen (2004) used the χ2 measure to compute
character-character and word-word associations,
but not word-category associations. We use
word-category associations to directly predict
the semantic categories of unknown words.
The mutual information and χ2 measures are
calculated as in (3) and (4), where Asso(c,tj) de-
notes the association between a character c and a
semantic category tj, and P(X) and f(X) denote
the probability and frequency of X respectively.
</bodyText>
<equation confidence="0.988128">
P(c, tj )
[Ac, tj)] 2
f(c)+ f(tj)
</equation>
<bodyText confidence="0.9996033">
Once the character-category associations are
calculated, the association between a word w and
a category tj, Asso(w,tj), can be calculated as the
sum of the weighted associations between each
of the word’s characters and the category, as in
(6), where ci denotes the ith character of w, |w|
denotes the length of w, and λi denotes the weight
of Asso(ci,tj). The λ’s add up to 1. The weights
are determined empirically based on the posi-
tions of the characters in the word.
</bodyText>
<equation confidence="0.999393">
w |
(6) Asso(w,tj) = E
i=1
</equation>
<bodyText confidence="0.999965333333333">
As in variant 2 of the baseline model, the
character-category association model can also be
made sensitive to the positions in which the
characters occur in the words. To this end, we
first need to compute the position-sensitive asso-
ciations between a category and a character in
the word-initial, word-middle, and word-final
positions separately. The position-sensitive asso-
ciation between an unknown word and a cate-
gory can then be computed as the sum of the
weighted position-sensitive associations between
each of its characters and the category.
</bodyText>
<equation confidence="0.995721333333333">
i
f c
(
)
f c i p i
( , )
(3) Asso MI (c, tj ) = log
P c P t
( ) ( )
j
(4) Assoχ2 (c, tj) = α(c, tj )
max k α(c,tk)
(5) α(c, tj) =
|
)
Ai Asso ci t
( ,
j
</equation>
<page confidence="0.99342">
189
</page>
<bodyText confidence="0.9998885">
Once the word-category associations are com-
puted, we can propose the highest ranked cate-
gory or a ranked list of categories for each un-
known word.
</bodyText>
<subsectionHeader confidence="0.999025">
2.3 A Rule-Based Model
</subsectionHeader>
<bodyText confidence="0.999802022222222">
The third knowledge-based model uses linguistic
rules to classify unknown words based on the
syntactic and semantic categories of their com-
ponent characters. Rule-based models have not
been used for this task before. However, there
are some regularities in the relationship between
the semantic categories of unknown words and
those of their component characters that can be
captured in a more direct and effective way by
linguistic rules than by statistical models.
A separate set of rules are developed for
words of different lengths. Rules are initially
developed based on knowledge about Chinese
word formation, and are then refined by examin-
ing the development data. In general, the com-
plete rule set takes a few hours to develop.
The rule in (7) is developed for bisyllabic un-
known words. This rule proposes the common
category of a bisyllabic word’s two characters as
its category. It is especially useful for words
with a parallel structure, i.e., words whose two
characters have the same meaning and syntactic
category, e.g., 坍塌 tāntā ‘collapse’, where 坍
tān and 塌 tā both mean ‘collapse’ and share the
category Id05. The thresholds for fA and fB are
determined empirically and are both set to 1 if
AB is a noun and to 0 and 3 respectively other-
wise.
(7) For a bisyllabic word AB, if A and B share a cate-
gory c1, let fA and fB denote the number of times
A and B occur in word-initial and word-final po-
sitions in c respectively. If fA and fB both surpass
the predetermined thresholds, propose c for AB.
A number of rules are developed for trisyl-
labic words. While most rules in the model are
general, the first rule in this set is rather specific,
as it handles words with three specific prefixes,
大 dà ‘big’, 小 xiăo ‘little’, and 老 lăo ‘old’,
which usually do not change the category of the
root word. The other four rules again utilize the
categories of the unknown word’s component
characters. The rules in (8b) and (8c) are similar
to the rule in (7). The ones in (8d) and (8e)
search for neighbor words with a similar struc-
ture as the target word. Eligible neighbors have a
</bodyText>
<page confidence="0.572722">
1 A and B may each belong to more than one category.
</page>
<bodyText confidence="0.999970090909091">
common morpheme with the target word in the
same position and a second morpheme that
shares a category with the second morpheme of
the target word. For example, an eligible
neighbor for 推销商 tuīxiāo-shāng ‘sales-man’
is 销售商 xiāoshòu-shāng ‘distribut-or’. These
two words share the morpheme 商 shāng ‘busi-
nessman’ in the word-final position, and the
morphemes 推销 tuīxiāo ‘to market’ and 销售
xiāoshòu ‘distribute’ share the category He03.
The rule in (8d) therefore applies in this case.
</bodyText>
<listItem confidence="0.940010266666667">
(8) For a trisyllabic word ABC:
a. If A equals 大 dà ‘big’, 小 xiăo ‘little’, or 老
lăo ‘old’, propose the category of AB for
ABC if C is the diminutive suffix 儿 er or the
category of BC for ABC otherwise.
b. If A and BC share a category c, propose c for
ABC.
c. If AB and C share a category c, propose c for
ABC.
d. If there is a word XYC such that XY and AB
share a category, propose the category of
XYC for ABC.
e. If there is a word XBC such that X and A
share a category, propose the category of
XBC for ABC.
</listItem>
<bodyText confidence="0.998667666666667">
The rules for four-character words are given
in (9). Like the rules in (8d) and (8e), these rules
also search for neighbors of the target word.
</bodyText>
<listItem confidence="0.941807583333333">
(9) For a four-character word ABCD:
a. If there is a word XYZD/YZD such that
XYZ/YZ and ABC share a category, propose
the category of XYZ/YZ for ABCD.
b. If there is a word ABCX such that X and D
share a category, propose the category of
ABCX for ABCD.
c. If there is a word XYCD such that XY and AB
share a category, propose the category of
XYCD for ABCD.
d. If there is a word XBCD/BCD, propose the
category of XBCD/BCD for ABCD.
</listItem>
<sectionHeader confidence="0.989138" genericHeader="method">
3 A Corpus-Based Model
</sectionHeader>
<bodyText confidence="0.9998624">
The knowledge-based models described above
classify unknown words using information about
the syntactic and semantic categories of their
component characters. Another useful source of
information is the context in which unknown
words occur. While contextual information is the
primary source of information used in WSD re-
search and has been used for acquiring semantic
lexicons and classifying unknown words in other
languages (e.g., Roark and Charniak 1998; Ci-
</bodyText>
<page confidence="0.983838">
190
</page>
<bodyText confidence="0.998850130434783">
aramita 2003; Curran 2005), it has been used in
only one previous study on semantic classifica-
tion of Chinese unknown words (Chen and Lin,
2000). Part of the goal of this study is to investi-
gate whether and how these two different
sources of information can be combined to im-
prove performance on semantic classification of
Chinese unknown words.
To this end, we first use the knowledge-based
models to propose a list of five candidate catego-
ries for the target word, then extract a general-
ized context for each category in Cilin from a
corpus, and finally compute the similarity be-
tween the context of the target word and the gen-
eralized context of each of its candidate catego-
ries. Comparing the context of the target word
with generalized contexts of categories instead
of contexts of individual words alleviates the
data-sparseness problem, as infrequent words
have limited contextual information. Limiting
the search space for each target word to the top
five candidate categories reduces the computa-
tional cost that comes with the full search space.
</bodyText>
<subsectionHeader confidence="0.999801">
3.1 Context Extraction and Representation
</subsectionHeader>
<bodyText confidence="0.999895363636364">
A generalized context for each semantic cate-
gory is built from the contexts of its member
words. This is done based on the assumption that
as the words in the same category have the same
or similar meaning, they tend to occur in similar
contexts. In terms of context extraction and rep-
resentation, we need to consider four factors.
Member Words The issue here is whether to
include the contexts of polysemous member
words in building the generalized context of a
category. Including these contexts without dis-
crimination introduces noise. To measure the
effect of such noise, we build two versions of
generalized context for each category, one using
contexts of unambiguous member words only,
and the other using contexts of all member
words.
Context Words There are two issues in select-
ing words for context representation. First,
words that contribute little information to the
discrimination of meaning of other words, in-
cluding conjunctions, numerals, auxiliaries, and
non-Chinese sequences, are excluded. Second, to
model the effect of frequency on the context
words’ contribution to meaning discrimination,
we use two sets of context words: one consists of
the 1000 most frequent words in the corpus; the
other consists of all words in the corpus.
Window Size For WSD, both topical context
and microcontext have been used (Ide and
Véronis 1998). Topical context includes substan-
tive words that co-occur with the target word
within a larger window, whereas microcontext
includes words in a small window around the
target word. We experiment with topical context
and microcontext with window sizes of 100 and
6 respectively (i.e., 50 and 3 words to the left
and right of the target word respectively).
Context Representation We represent the con-
text of a category as a vector &lt;w1, w2, ..., wn&gt;,
where n is the total number of context words,
and wi is the weight of the ith context word. To
arrive at this representation, we first record the
number of times each context word occurs
within a specified window of each member word
of a category in the corpus as a vector &lt;f1, f2, ...,
fn&gt;, where fi is the number of times the ith con-
text word co-occurs with a member word of the
category. We then compute the weight of a con-
text word w in context c, W(w, c), using mutual
information and t-test, which were reported by
Weeds and Weir (2005) to perform the best on a
pseudo-disambiguation task. These weight func-
tions are computed as in (10) and (11), where N
denotes the size of the corpus.
</bodyText>
<equation confidence="0.9906255">
P w c
( , )
</equation>
<listItem confidence="0.536283">
(10) WPMI w c =
( , ) log
</listItem>
<subsectionHeader confidence="0.996258">
3.2 Contextual Similarity Measurement
</subsectionHeader>
<bodyText confidence="0.999940666666667">
We compute the similarity between the context
vectors of the unknown word and its candidate
categories using cosine. The cosine of two n-
dimensional vectors x� and y� , cos( x� , y� ), is com-
puted as in (12), where xi and yi denote the
weight of the ith context word in x� and y� .
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.997034">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.9996898">
The models are developed and tested using the
Contemporary Chinese Corpus from Peking
University (Yu et al. 2002) and the extended
Cilin released by the Information Retrieval Lab
at Harbin Institute of Technology. The corpus
</bodyText>
<equation confidence="0.9900105">
∑ n
i 1
=
x y
i i
2
yi
(12)
�
cos(x,
�
y)
=
∑
i
n
1
x
i
2
n
∑
i
1
P(w)P(c)
(11) Wt (w, c) = P(w, c) − P(w)P(c)
P w c N
( , )
</equation>
<page confidence="0.988001">
191
</page>
<bodyText confidence="0.918970571428571">
contains all the articles published in January,
1999 in People’s Daily, a major newspaper in
China. It contains over 1.12 million tokens and is
word-segmented and POS-tagged. Table 1 sum-
marizes the distribution of words in Cilin. Of the
76,029 words in Cilin, 35,151 are found in the
Contemporary Chinese Corpus.
</bodyText>
<table confidence="0.999760285714286">
Length Unambiguous Polysemous Total
1 2,674 2,068 4,742
2 39,057 5,403 44,460
3 15,112 752 15,864
4 9,397 942 10,338
≥5 590 34 624
Total 66,830 9,199 76,029
</table>
<tableCaption confidence="0.999864">
Table 1: Word distribution in the extended Cilin
</tableCaption>
<bodyText confidence="0.999996366666667">
We classify words into the third-level catego-
ries in the extended Cilin, which are equivalent
to the small categories in the original Cilin. The
development and test sets consist of 3,000 words
each, which are randomly selected from the sub-
set of words in Cilin that are two to four charac-
ters long, that have occurred in the Contempo-
rary Chinese Corpus, and that are tagged as
nouns, verbs, or adjectives in the corpus. The
words in the development and test sets are also
controlled for frequency, with 1/3 of them occur-
ring 1-3 times, 3-6 times, and 7 or more times in
the corpus respectively.
As Chen (2004) noted, excluding all the
words in the development and test data in the
testing stage worsens the data-sparseness prob-
lem for knowledge-based models, as some cate-
gories have few member words, and some char-
acters appear in few words in some categories.
To alleviate this problem, the remove-one
method is used for testing the knowledge-based
models. In other words, the models are re-trained
for each test word using information about all
the words in Cilin except the test word. The cor-
pus-based model is trained once using the train-
ing data only, as the data-sparseness problem is
alleviated by using generalized contexts of cate-
gories. Finally, if a word is polysemous, it is
considered to have been correctly classified if
the proposed category is one of its categories.
</bodyText>
<subsectionHeader confidence="0.965871">
4.2 Results of the Baseline Model
</subsectionHeader>
<bodyText confidence="0.999854333333334">
Tables 2 and 3 summarize the results of the
baseline model in terms of the accuracy of its
best guess and best five guesses respectively.
The columns labeled “Non-filtered” report re-
sults where all categories are considered for each
unknown word, and the ones labeled “POS-
filtered” report results where only the categories
that agree with the POS category of the unknown
word are considered. In the latter case, if the tar-
get word is a noun, only the small categories un-
der major categories A-D are considered; other-
wise, only those under major categories E-L are
considered. The results show that using POS in-
formation about the unknown word to filter cate-
gories improves performance. Variant 2 per-
forms better when only the best guess is consid-
ered, indicating that it is useful to model the ef-
fect of position on a character’s contribution to
word meaning in this case. However, it is not
helpful to be sensitive to character position when
the best five guesses are considered.
</bodyText>
<table confidence="0.9971232">
Model Non-filtered POS-filtered
variant
Dev Test Dev Test
1 0.391 0.398 0.450 0.464
2 0.471 0.469 0.514 0.517
</table>
<tableCaption confidence="0.9919">
Table 2: Results of the baseline model: best guess
</tableCaption>
<table confidence="0.9994212">
Model Non-filtered POS-filtered
variant
Dev Test Dev Test
1 0.757 0.760 0.813 0.817
2 0.764 0.762 0.809 0.805
</table>
<tableCaption confidence="0.999896">
Table 3: Results of the baseline model: best 5 guesses
</tableCaption>
<subsectionHeader confidence="0.9815965">
4.3 Results of the Character-Category As-
sociation Model
</subsectionHeader>
<bodyText confidence="0.9999158">
In this model, only categories that agree with the
POS category of the unknown word and that
share at least one character with the unknown
word are considered. These filtering steps sig-
nificantly reduce the search space for this model.
We discussed three parameters of the model in
Section 2.2, including the statistical measure, the
sensitivity to character position in computing
character-category associations, and the weights
of the associations between categories and char-
acters in different positions. In addition, the
computation of the character-category associa-
tions can be sensitive or insensitive to the POS
categories of the words containing the characters.
In the POS-sensitive way, associations are com-
puted among nouns (words in categories A-D)
and non-nouns (words in categories E-L) sepa-
rately, whereas in the POS-insensitive way, they
are computed using all the words.
Tables 4 and 5 summarize the results of the
character-category association model in terms of
the accuracy of its best guess and best five
guesses respectively. In all cases, the weights
assigned to word-initial, word-middle, and word-
final characters are 0.49, 0, and 0.51 respectively.
</bodyText>
<page confidence="0.996138">
192
</page>
<bodyText confidence="0.999944">
In terms of the best guess, the model achieves
a best accuracy of 58.2%, a 6.5% improvement
over the baseline result. The results show that χ2
consistently performs better than mutual infor-
mation, and computing position-sensitive char-
acter-category associations consistently im-
proves performance. However, computing POS-
sensitive associations gives mixed results.
In terms of the best five guesses, the model
achieves a best accuracy of 83.8% on the test
data, a 2.1% improvement over the best baseline
result. Using χ2 again achieves better results.
However, in this case, the best results are
achieved when the character-category associa-
tions are insensitive to both character position
and the POS categories of words.
</bodyText>
<table confidence="0.999006333333333">
Sensitivity Development Test
POS Position MI χ2 MI χ2
Yes Yes 0.482 0.586 0.507 0.582
Yes No 0.440 0.578 0.458 0.573
No Yes 0.487 0.565 0.511 0.567
No No 0.457 0.555 0.459 0.559
</table>
<tableCaption confidence="0.9867005">
Table 4: Results of the character-category association
model: best guess
</tableCaption>
<table confidence="0.999879166666667">
Sensitivity Development Test
POS Position MI χ2 MI χ2
Yes Yes 0.735 0.805 0.720 0.810
Yes No 0.743 0.828 0.754 0.821
No Yes 0.702 0.813 0.718 0.812
No No 0.735 0.830 0.746 0.838
</table>
<tableCaption confidence="0.9179955">
Table 5: Results of the character-category association
model: best 5 guesses
</tableCaption>
<table confidence="0.9998855">
Word Development Test
Len R P F R P F
2 0.159 0.796 0.265 0.158 0.772 0.262
3 0.368 0.838 0.511 0.351 0.830 0.493
4 0.582 0.852 0.692 0.540 0.900 0.675
All 0.218 0.816 0.344 0.216 0.803 0.340
</table>
<tableCaption confidence="0.998776">
Table 6: Results of the rule-based model: best guess
</tableCaption>
<subsectionHeader confidence="0.990458">
4.4 Results of the Rule-Based Model
</subsectionHeader>
<bodyText confidence="0.999964941176471">
Table 6 summarizes the results of the rule-
based model in terms of recall, precision and F-
score. The model returns multiple categories for
some words, and it is considered to have cor-
rectly classified a word only when it returns a
single, correct category for the word. Precision
of the model is computed over all the cases
where the model returns a single guess, and re-
call is computed over all cases. The model
achieves an overall precision of 80.3% on the
test data, much higher than the accuracy of the
other two knowledge-based models. However,
recall of the model is only 21.6%. The compara-
ble results on the development and test sets indi-
cate that the encoded rules are general. The
model generally performs better on longer words
than on shorter words.
</bodyText>
<subsectionHeader confidence="0.998956">
4.5 Combining the Character-Category
Association and Rule-Based Models
</subsectionHeader>
<bodyText confidence="0.999857333333333">
Given that the rule-based model achieves a
higher precision but a lower recall than the char-
acter-category association model, the two mod-
els can be combined to improve the overall per-
formance. In general, if the rule-based model
returns one or more categories, these categories
are first ranked among themselves by their asso-
ciations with the unknown word. They are then
followed by the other categories returned by the
character-category association model. Tables 7
and 8 summarize the results of combining the
two models.
</bodyText>
<tableCaption confidence="0.9979475">
Table 7: Results of combining the character-category
association and rule-based models: best guess
</tableCaption>
<table confidence="0.999851666666667">
Sensitivity Development Test
POS Position MI χ2 MI χ2
Yes Yes 0.834 0.846 0.845 0.843
Yes No 0.791 0.860 0.801 0.851
No Yes 0.760 0.848 0.742 0.845
No No 0.773 0.859 0.782 0.856
</table>
<tableCaption confidence="0.997402">
Table 8: Results of combining the character-category
association and rule-based models: best 5 guesses
</tableCaption>
<bodyText confidence="0.997804647058823">
In terms of the best guess, the combined
model achieves an accuracy of 61.6%, a 3.4%
improvement over the best result of the charac-
ter-category association model alone. This is
achieved using χ2 with POS-sensitive and posi-
tion-sensitive computation of character-category
associations. In terms of the best five guesses,
the model achieves an accuracy of 85.6%, a
1.8% improvement over the best result of the
character-category association model alone.
To facilitate comparison with previous studies,
the results of the combined model in terms of its
best guess in classifying unknown words into
major and medium categories are summarized in
Table 9. As χ2 consistently outperforms mutual
information, results are reported for χ2 only.
With POS-sensitive and position-sensitive com-
</bodyText>
<table confidence="0.999330833333333">
Sensitivity Development Test
POS Position MI χ2 MI χ2
Yes Yes 0.561 0.623 0.572 0.616
Yes No 0.536 0.622 0.542 0.615
No Yes 0.562 0.610 0.575 0.608
No No 0.530 0.601 0.532 0.606
</table>
<page confidence="0.998303">
193
</page>
<bodyText confidence="0.99908475">
putation of character-category associations, the
combined model achieves an accuracy of 83.0%
and 69.9% for classifying unknown words into
major and medium categories respectively.
</bodyText>
<table confidence="0.998928">
Sensitivity Development Test
POS Position Major Med Major Med
Yes Yes 0.840 0.705 0.830 0.699
Yes No 0.831 0.698 0.828 0.698
No Yes 0.832 0.692 0.825 0.692
No No 0.821 0.687 0.821 0.689
</table>
<tableCaption confidence="0.991097">
Table 9: Results of the combined model for classify-
ing unknown words into major and medium catego-
ries: best guess
</tableCaption>
<subsectionHeader confidence="0.932846">
4.6 Results of the Corpus-Based Model
</subsectionHeader>
<bodyText confidence="0.9999228">
The corpus-based model re-ranks the five high-
est ranked categories proposed by the combined
knowledge-based model. Table 10 enumerates
the parameters of the model and lists the labels
used to denote the various settings in Table 11.
</bodyText>
<table confidence="0.996349694444444">
Parameter Label Setting Label
Member MW All members words all
words Unambiguous members un
Context CW All words all
words 1000 most frequent 1000
Window WS 100 100
size 6 6
Weight WF Mutual information mi
function t-test t
on words with higher frequency, suggesting that
it may benefit from a larger corpus.
Run Parameter Setting Accuracy
ID
MW CW WS WF Dev Test
1 un 1000 100 mi 0.326 0.303
2 un 1000 100 t 0.317 0.288
3 un 1000 6 mi 0.304 0.301
4 un 1000 6 t 0.299 0.301
5 un all 100 mi 0.359 0.371
6 un all 100 t 0.292 0.296
7 un all 6 mi 0.370 0.365
8 un all 6 t 0.322 0.297
9 all 1000 100 mi 0.302 0.294
10 all 1000 100 t 0.314 0.304
11 all 1000 6 mi 0.313 0.314
12 all 1000 6 t 0.308 0.308
13 all all 100 mi 0.336 0.333
14 all all 100 t 0.287 0.300
15 all all 6 mi 0.356 0.356
16 all all 6 t 0.308 0.308
Table 11: Results of the corpus-based model
Run Development Test
ID 1-2 3-6 &gt;7 1-2 3-6 &gt;7
5 0.331 0.360 0.385 0.323 0.389 0.402
7 0.323 0.363 0.423 0.335 0.357 0.402
15 0.328 0.346 0.395 0.334 0.355 0.379
</table>
<tableCaption confidence="0.9832995">
Table 12: Results of the corpus-based model on
words with different frequency
</tableCaption>
<sectionHeader confidence="0.993735" genericHeader="related work">
5 Related Work
</sectionHeader>
<tableCaption confidence="0.7988025">
Table 10: Parameter settings of the corpus-based
model
</tableCaption>
<bodyText confidence="0.998683130434783">
Table 11 summarizes the results of 16 runs of
the model with different parameter settings. The
best accuracy on the test data is 37.1%, achieved
in run 5 with the following parameter settings:
using unambiguous member words for building
contexts of categories, using all words in the
corpus for context representation, using a win-
dow size of 100, and using mutual information
as the weight function. As the combined knowl-
edge-based model gives an accuracy of 85.6%
for its best five guesses, the expected accuracy
of a naive model that randomly picks a candidate
for each word as its best guess is 17.1%. Com-
pared with this baseline, the corpus-based model
achieves a 13.0% improvement, but it performs
much worse than the knowledge-based models.
Table 12 summarizes the accuracy of the top
three runs of the model on words with different
frequency in the corpus. Each of the three groups
consists of 1,000 words that have occurred 1-2,
3-6, and 7 or more times in the corpus respec-
tively. The model consistently performs better
The few previous studies on semantic classifica-
tion of Chinese unknown word have primarily
adopted knowledge-based models. Chen (2004)
proposed a model that retrieves the word with
the greatest association with the target word.
This model is computationally more expensive
than our character-category association model,
as it entails computing associations between
every character-category, category-character,
character-character, and word-word pair. He re-
ported an accuracy of 61.6% on bisyllabic V-V
compounds. However, he included all the test
words in training the model. If we also include
the test words in computing character-category
associations, the computationally cheaper model
achieves an overall accuracy of 75.6%, with an
accuracy of 75.1% on verbs.
Chen and Chen (2000) adopted similar exem-
plar-based models. Chen and Chen used a mor-
phological analyzer to identify the head of the
target word and the semantic categories of its
modifier. They then retrieved examples with the
same head as the target word. Finally, they com-
puted the similarity between two words as the
</bodyText>
<page confidence="0.995699">
194
</page>
<bodyText confidence="0.999983289473684">
similarity between their modifiers, using the
concept of information load (IC) of the least
common ancestor (LCA) of the modifiers’ se-
mantic categories. They reported an accuracy of
81% for classifying 200 unknown nouns. Given
the small test set of their study, it is hard to di-
rectly compare their results with ours.
Tseng used a morphological analyzer in the
same way, but she also derived the morpho-
syntactic relationship between the morphemes.
She retrieved examples that share a morpheme
with the target word in the same position and
filtered those with a different morpho-syntactic
relationship. Finally, she computed the similarity
between two words as the similarity between
their non-shared morphemes, using a similar
concept of IC of the LCA of two categories. She
classified unknown words into the 12 major
categories only, and reported accuracies 65.8%
on adjectives, 71.4% on nouns, and 52.8% on
verbs. These results are not as good as the 83.0%
overall accuracy our combined knowledge-based
model achieved for classifying unknown words
into major categories.
Chen and Lin (2000) is the only study that
used contextual information for the same task.
To generate candidate categories for a word,
they looked up its translations in a Chinese-
English dictionary and the synsets of the transla-
tions in WordNet, and mapped the synsets to the
categories in Cilin. They used a corpus-based
model similar to ours to rank the candidates.
They reported an accuracy of 34.4%, which is
close to the 37.1% accuracy of our corpus-based
model, but lower than the 61.6% accuracy of our
combined knowledge-based model. In addition,
they could only classify the unknown words
listed in the Chinese-English dictionary.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99999475">
We presented three knowledge-based models
and a corpus-based model for classifying Chi-
nese unknown words into fine-grained categories
in the Chinese thesaurus Cilin, a task important
for lexical acquisition and NLP applications that
require semantic annotation. The knowledge-
based models use information about the catego-
ries of the unknown words’ component charac-
ters, while the corpus-based model uses contex-
tual information. By combining the character-
category association and rule-based models, we
achieved an accuracy of 61.6%. The corpus-
based model did not improve performance.
Several avenues can be taken for further re-
search. First, additional resources, such as bilin-
gual dictionaries, morphological analyzers, par-
allel corpora, and larger corpora with richer lin-
guistic annotation may prove useful for improv-
ing both the knowledge-based and corpus-based
models. Second, we only explored one way to
combine the knowledge-based and corpus-based
models. Future work may explore alternative
ways to combine these models to make better
use of contextual information.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904564102564">
C.-J. Chen. 2004. Character-sense association and
compounding template similarity: Automatic se-
mantic classification of Chinese compounds. In
Proceedings of the 3rd SIGHAN Workshop on Chi-
nese Language Processing, pages 33–40.
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in WordNet. In Proceed-
ings of EMNLP-2003, pages 594-602.
K.-J. Chen and C.-J. Chen. 2000. Automatic semantic
classification for Chinese unknown compound
nouns. In Proceedings of COLING-2000, pages
173-179.
H.-H. Chen and C.-C. Lin. 2000. Sense-tagging Chi-
nese corpus. In Proceedings of the 2nd Chinese
Language Processing Workshop, pages 7-14.
J. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
ACL-2006, pages 26-33.
N. Ide and J. Véronis. 1998. Introduction on the spe-
cial issue on word sense disambiguation: The state
of the art. Computational Linguistics 24(1):2–40.
J. Mei, Y. Zhu, Y. Gao, and H. Yin. (eds.) 1984.
Tongyici Cilin [A Thesaurus of Chinese Words].
Commercial Press, Hong Kong.
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic
lexicon construction. In Proceedings of COL-
ING/ACL-1998, pages 1110-1116.
H. Tseng. 2003. Semantic classification of Chinese
unknown words. In Proceedings of ACL-2003 Stu-
dent Research Workshop, pages 72-79.
J. Weeds and D. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional
similarity. Computational Linguistics 31(4):439–
475.
S. Yu, H. Duan, X. Zhu, and B. Sun. 2002. The basic
processing of Contemporary Chinese Corpus at
Peking University. Journal of Chinese Information
Processing 16(5):49–64.
</reference>
<page confidence="0.99894">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.807337">
<title confidence="0.9977485">Hybrid Models for Semantic Classification of Chinese Unknown Words</title>
<author confidence="0.942407">Xiaofei</author>
<affiliation confidence="0.9389205">Department of Linguistics and Applied Language Pennsylvania State</affiliation>
<address confidence="0.988247">University Park, PA 16802, USA</address>
<email confidence="0.997452">xxl13@psu.edu</email>
<abstract confidence="0.999454">This paper addresses the problem of classifying Chinese unknown words into fine-grained semantic categories defined in a Chinese thesaurus. We describe three novel knowledge-based models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways. We then combine two of the knowledge-based models with a corpus-based model which classifies unknown words using contextual information. Experiments show that the knowledge-based models outperform previous methods on the same task, but the use of contextual information does not further improve performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C-J Chen</author>
</authors>
<title>Character-sense association and compounding template similarity: Automatic semantic classification of Chinese compounds.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="6493" citStr="Chen (2004)" startWordPosition="1047" endWordPosition="1048">egory in its corresponding position, as in (2), where pi denotes the position of ci in w, which could be word-initial, word-middle, or word-final, and f(ci,pi) denotes the frequency of ci in position pi in tj. n (2) Score(tj , w) = E i=1 2.2 Character-Category Associations The relationship between the semantic category of an unknown word and those of its component characters can also be captured in a more sophisticated way using information-theoretical models. We use two statistical measures, mutual information and χ2, to compute character-category associations and word-category associations. Chen (2004) used the χ2 measure to compute character-character and word-word associations, but not word-category associations. We use word-category associations to directly predict the semantic categories of unknown words. The mutual information and χ2 measures are calculated as in (3) and (4), where Asso(c,tj) denotes the association between a character c and a semantic category tj, and P(X) and f(X) denote the probability and frequency of X respectively. P(c, tj ) [Ac, tj)] 2 f(c)+ f(tj) Once the character-category associations are calculated, the association between a word w and a category tj, Asso(w,</context>
<context position="18013" citStr="Chen (2004)" startWordPosition="3101" endWordPosition="3102"> We classify words into the third-level categories in the extended Cilin, which are equivalent to the small categories in the original Cilin. The development and test sets consist of 3,000 words each, which are randomly selected from the subset of words in Cilin that are two to four characters long, that have occurred in the Contemporary Chinese Corpus, and that are tagged as nouns, verbs, or adjectives in the corpus. The words in the development and test sets are also controlled for frequency, with 1/3 of them occurring 1-3 times, 3-6 times, and 7 or more times in the corpus respectively. As Chen (2004) noted, excluding all the words in the development and test data in the testing stage worsens the data-sparseness problem for knowledge-based models, as some categories have few member words, and some characters appear in few words in some categories. To alleviate this problem, the remove-one method is used for testing the knowledge-based models. In other words, the models are re-trained for each test word using information about all the words in Cilin except the test word. The corpus-based model is trained once using the training data only, as the data-sparseness problem is alleviated by usin</context>
<context position="28643" citStr="Chen (2004)" startWordPosition="4886" endWordPosition="4887"> candidate for each word as its best guess is 17.1%. Compared with this baseline, the corpus-based model achieves a 13.0% improvement, but it performs much worse than the knowledge-based models. Table 12 summarizes the accuracy of the top three runs of the model on words with different frequency in the corpus. Each of the three groups consists of 1,000 words that have occurred 1-2, 3-6, and 7 or more times in the corpus respectively. The model consistently performs better The few previous studies on semantic classification of Chinese unknown word have primarily adopted knowledge-based models. Chen (2004) proposed a model that retrieves the word with the greatest association with the target word. This model is computationally more expensive than our character-category association model, as it entails computing associations between every character-category, category-character, character-character, and word-word pair. He reported an accuracy of 61.6% on bisyllabic V-V compounds. However, he included all the test words in training the model. If we also include the test words in computing character-category associations, the computationally cheaper model achieves an overall accuracy of 75.6%, with</context>
</contexts>
<marker>Chen, 2004</marker>
<rawString>C.-J. Chen. 2004. Character-sense association and compounding template similarity: Automatic semantic classification of Chinese compounds. In Proceedings of the 3rd SIGHAN Workshop on Chinese Language Processing, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>M Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in WordNet.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<pages>594--602</pages>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>M. Ciaramita and M. Johnson. 2003. Supersense tagging of unknown nouns in WordNet. In Proceedings of EMNLP-2003, pages 594-602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-J Chen</author>
<author>C-J Chen</author>
</authors>
<title>Automatic semantic classification for Chinese unknown compound nouns.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="29295" citStr="Chen and Chen (2000)" startWordPosition="4977" endWordPosition="4980"> the word with the greatest association with the target word. This model is computationally more expensive than our character-category association model, as it entails computing associations between every character-category, category-character, character-character, and word-word pair. He reported an accuracy of 61.6% on bisyllabic V-V compounds. However, he included all the test words in training the model. If we also include the test words in computing character-category associations, the computationally cheaper model achieves an overall accuracy of 75.6%, with an accuracy of 75.1% on verbs. Chen and Chen (2000) adopted similar exemplar-based models. Chen and Chen used a morphological analyzer to identify the head of the target word and the semantic categories of its modifier. They then retrieved examples with the same head as the target word. Finally, they computed the similarity between two words as the 194 similarity between their modifiers, using the concept of information load (IC) of the least common ancestor (LCA) of the modifiers’ semantic categories. They reported an accuracy of 81% for classifying 200 unknown nouns. Given the small test set of their study, it is hard to directly compare the</context>
</contexts>
<marker>Chen, Chen, 2000</marker>
<rawString>K.-J. Chen and C.-J. Chen. 2000. Automatic semantic classification for Chinese unknown compound nouns. In Proceedings of COLING-2000, pages 173-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-H Chen</author>
<author>C-C Lin</author>
</authors>
<title>Sense-tagging Chinese corpus.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Chinese Language Processing Workshop,</booktitle>
<pages>7--14</pages>
<contexts>
<context position="12697" citStr="Chen and Lin, 2000" startWordPosition="2160" endWordPosition="2163">sed Model The knowledge-based models described above classify unknown words using information about the syntactic and semantic categories of their component characters. Another useful source of information is the context in which unknown words occur. While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci190 aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin, 2000). Part of the goal of this study is to investigate whether and how these two different sources of information can be combined to improve performance on semantic classification of Chinese unknown words. To this end, we first use the knowledge-based models to propose a list of five candidate categories for the target word, then extract a generalized context for each category in Cilin from a corpus, and finally compute the similarity between the context of the target word and the generalized context of each of its candidate categories. Comparing the context of the target word with generalized con</context>
<context position="30691" citStr="Chen and Lin (2000)" startWordPosition="5202" endWordPosition="5205">at share a morpheme with the target word in the same position and filtered those with a different morpho-syntactic relationship. Finally, she computed the similarity between two words as the similarity between their non-shared morphemes, using a similar concept of IC of the LCA of two categories. She classified unknown words into the 12 major categories only, and reported accuracies 65.8% on adjectives, 71.4% on nouns, and 52.8% on verbs. These results are not as good as the 83.0% overall accuracy our combined knowledge-based model achieved for classifying unknown words into major categories. Chen and Lin (2000) is the only study that used contextual information for the same task. To generate candidate categories for a word, they looked up its translations in a ChineseEnglish dictionary and the synsets of the translations in WordNet, and mapped the synsets to the categories in Cilin. They used a corpus-based model similar to ours to rank the candidates. They reported an accuracy of 34.4%, which is close to the 37.1% accuracy of our corpus-based model, but lower than the 61.6% accuracy of our combined knowledge-based model. In addition, they could only classify the unknown words listed in the Chinese-</context>
</contexts>
<marker>Chen, Lin, 2000</marker>
<rawString>H.-H. Chen and C.-C. Lin. 2000. Sense-tagging Chinese corpus. In Proceedings of the 2nd Chinese Language Processing Workshop, pages 7-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Supersense tagging of unknown nouns using semantic similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2006,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="12579" citStr="Curran 2005" startWordPosition="2141" endWordPosition="2142">gory of XYCD for ABCD. d. If there is a word XBCD/BCD, propose the category of XBCD/BCD for ABCD. 3 A Corpus-Based Model The knowledge-based models described above classify unknown words using information about the syntactic and semantic categories of their component characters. Another useful source of information is the context in which unknown words occur. While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci190 aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin, 2000). Part of the goal of this study is to investigate whether and how these two different sources of information can be combined to improve performance on semantic classification of Chinese unknown words. To this end, we first use the knowledge-based models to propose a list of five candidate categories for the target word, then extract a generalized context for each category in Cilin from a corpus, and finally compute the similarity between the context of the target word and the </context>
</contexts>
<marker>Curran, 2005</marker>
<rawString>J. Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of ACL-2006, pages 26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Véronis</author>
</authors>
<title>Introduction on the special issue on word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="15000" citStr="Ide and Véronis 1998" startWordPosition="2534" endWordPosition="2537">member words. Context Words There are two issues in selecting words for context representation. First, words that contribute little information to the discrimination of meaning of other words, including conjunctions, numerals, auxiliaries, and non-Chinese sequences, are excluded. Second, to model the effect of frequency on the context words’ contribution to meaning discrimination, we use two sets of context words: one consists of the 1000 most frequent words in the corpus; the other consists of all words in the corpus. Window Size For WSD, both topical context and microcontext have been used (Ide and Véronis 1998). Topical context includes substantive words that co-occur with the target word within a larger window, whereas microcontext includes words in a small window around the target word. We experiment with topical context and microcontext with window sizes of 100 and 6 respectively (i.e., 50 and 3 words to the left and right of the target word respectively). Context Representation We represent the context of a category as a vector &lt;w1, w2, ..., wn&gt;, where n is the total number of context words, and wi is the weight of the ith context word. To arrive at this representation, we first record the numbe</context>
</contexts>
<marker>Ide, Véronis, 1998</marker>
<rawString>N. Ide and J. Véronis. 1998. Introduction on the special issue on word sense disambiguation: The state of the art. Computational Linguistics 24(1):2–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mei</author>
<author>Y Zhu</author>
<author>Y Gao</author>
<author>H Yin</author>
</authors>
<date>1984</date>
<booktitle>Tongyici Cilin [A Thesaurus of Chinese Words].</booktitle>
<editor>(eds.)</editor>
<publisher>Commercial Press,</publisher>
<location>Hong Kong.</location>
<contexts>
<context position="1654" citStr="Mei et al., 1984" startWordPosition="244" endWordPosition="247">olysemous word out of a set of senses defined for the word in some lexicon. Much less work has been done on semantic classification of unknown words, i.e., words that are not listed in the lexicon. However, real texts typically contain a large number of unknown words. Successful classification of unknown words is not only useful for lexical acquisition, but also necessary for natural language processing (NLP) tasks that require semantic annotation. This paper addresses the problem of classifying Chinese unknown words into fine-grained semantic categories defined in a Chinese thesaurus, Cilin (Mei et al., 1984). This thesaurus classifies over 70,000 words into 12 major categories, including human (A), concrete object (B), time and space (C), abstract object (D), attributes (E), actions (F), mental activities (G), activities (H), physical states (I), relations (J), auxiliaries (K), and honorifics (L). The 12 major categories are further divided into 94 medium categories, which in turn are subdivided into 1428 small categories. Each small category contains synonyms that are close in meaning. For example, under the major category D, the medium category Dm groups all words that refer to institutions, an</context>
</contexts>
<marker>Mei, Zhu, Gao, Yin, 1984</marker>
<rawString>J. Mei, Y. Zhu, Y. Gao, and H. Yin. (eds.) 1984. Tongyici Cilin [A Thesaurus of Chinese Words]. Commercial Press, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase cooccurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-1998,</booktitle>
<pages>1110--1116</pages>
<contexts>
<context position="12545" citStr="Roark and Charniak 1998" startWordPosition="2133" endWordPosition="2136"> XY and AB share a category, propose the category of XYCD for ABCD. d. If there is a word XBCD/BCD, propose the category of XBCD/BCD for ABCD. 3 A Corpus-Based Model The knowledge-based models described above classify unknown words using information about the syntactic and semantic categories of their component characters. Another useful source of information is the context in which unknown words occur. While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci190 aramita 2003; Curran 2005), it has been used in only one previous study on semantic classification of Chinese unknown words (Chen and Lin, 2000). Part of the goal of this study is to investigate whether and how these two different sources of information can be combined to improve performance on semantic classification of Chinese unknown words. To this end, we first use the knowledge-based models to propose a list of five candidate categories for the target word, then extract a generalized context for each category in Cilin from a corpus, and finally compute the similarity between the c</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase cooccurrence statistics for semi-automatic semantic lexicon construction. In Proceedings of COLING/ACL-1998, pages 1110-1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
</authors>
<title>Semantic classification of Chinese unknown words.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-2003 Student Research Workshop,</booktitle>
<pages>72--79</pages>
<marker>Tseng, 2003</marker>
<rawString>H. Tseng. 2003. Semantic classification of Chinese unknown words. In Proceedings of ACL-2003 Student Research Workshop, pages 72-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>4</issue>
<pages>475</pages>
<contexts>
<context position="15991" citStr="Weeds and Weir (2005)" startWordPosition="2714" endWordPosition="2717">entation We represent the context of a category as a vector &lt;w1, w2, ..., wn&gt;, where n is the total number of context words, and wi is the weight of the ith context word. To arrive at this representation, we first record the number of times each context word occurs within a specified window of each member word of a category in the corpus as a vector &lt;f1, f2, ..., fn&gt;, where fi is the number of times the ith context word co-occurs with a member word of the category. We then compute the weight of a context word w in context c, W(w, c), using mutual information and t-test, which were reported by Weeds and Weir (2005) to perform the best on a pseudo-disambiguation task. These weight functions are computed as in (10) and (11), where N denotes the size of the corpus. P w c ( , ) (10) WPMI w c = ( , ) log 3.2 Contextual Similarity Measurement We compute the similarity between the context vectors of the unknown word and its candidate categories using cosine. The cosine of two ndimensional vectors x� and y� , cos( x� , y� ), is computed as in (12), where xi and yi denote the weight of the ith context word in x� and y� . 4 Results 4.1 Experiment Setup The models are developed and tested using the Contemporary Ch</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>J. Weeds and D. Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics 31(4):439– 475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yu</author>
<author>H Duan</author>
<author>X Zhu</author>
<author>B Sun</author>
</authors>
<title>The basic processing of Contemporary Chinese Corpus at Peking University.</title>
<date>2002</date>
<journal>Journal of Chinese Information Processing</journal>
<volume>16</volume>
<issue>5</issue>
<contexts>
<context position="16643" citStr="Yu et al. 2002" startWordPosition="2840" endWordPosition="2843">mbiguation task. These weight functions are computed as in (10) and (11), where N denotes the size of the corpus. P w c ( , ) (10) WPMI w c = ( , ) log 3.2 Contextual Similarity Measurement We compute the similarity between the context vectors of the unknown word and its candidate categories using cosine. The cosine of two ndimensional vectors x� and y� , cos( x� , y� ), is computed as in (12), where xi and yi denote the weight of the ith context word in x� and y� . 4 Results 4.1 Experiment Setup The models are developed and tested using the Contemporary Chinese Corpus from Peking University (Yu et al. 2002) and the extended Cilin released by the Information Retrieval Lab at Harbin Institute of Technology. The corpus ∑ n i 1 = x y i i 2 yi (12) � cos(x, � y) = ∑ i n 1 x i 2 n ∑ i 1 P(w)P(c) (11) Wt (w, c) = P(w, c) − P(w)P(c) P w c N ( , ) 191 contains all the articles published in January, 1999 in People’s Daily, a major newspaper in China. It contains over 1.12 million tokens and is word-segmented and POS-tagged. Table 1 summarizes the distribution of words in Cilin. Of the 76,029 words in Cilin, 35,151 are found in the Contemporary Chinese Corpus. Length Unambiguous Polysemous Total 1 2,674 2,</context>
</contexts>
<marker>Yu, Duan, Zhu, Sun, 2002</marker>
<rawString>S. Yu, H. Duan, X. Zhu, and B. Sun. 2002. The basic processing of Contemporary Chinese Corpus at Peking University. Journal of Chinese Information Processing 16(5):49–64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>