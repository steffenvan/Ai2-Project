<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.577452">
Probabilistic models of similarity in syntactic context
</title>
<author confidence="0.684836">
Diarmuid O´ S´eaghdha
</author>
<affiliation confidence="0.7753655">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.466934">
United Kingdom
</address>
<email confidence="0.760075">
do242@cl.cam.ac.uk
</email>
<author confidence="0.977114">
Anna Korhonen
</author>
<affiliation confidence="0.98339">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.619074">
United Kingdom
</address>
<email confidence="0.970393">
Anna.Korhonen@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.983625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997715">
This paper investigates novel methods for in-
corporating syntactic information in proba-
bilistic latent variable models of lexical choice
and contextual similarity. The resulting mod-
els capture the effects of context on the inter-
pretation of a word and in particular its effect
on the appropriateness of replacing that word
with a potentially related one. Evaluating our
techniques on two datasets, we report perfor-
mance above the prior state of the art for esti-
mating sentence similarity and ranking lexical
substitutes.
</bodyText>
<sectionHeader confidence="0.992425" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992261533333334">
Distributional models of lexical semantics, which
assume that aspects of a word’s meaning can be re-
lated to the contexts in which that word is typically
used, have a long history in Natural Language Pro-
cessing (Sp¨arck Jones, 1964; Harper, 1965). Such
models still constitute one of the most popular ap-
proaches to lexical semantics, with many proven ap-
plications. Much work in distributional semantics
treats words as non-contextualised units; the models
that are constructed can answer questions such as
“how similar are the words body and corpse?” but
do not capture the way the syntactic context in which
a word appears can affect its interpretation. Re-
cent developments (Mitchell and Lapata, 2008; Erk
and Pad´o, 2008; Thater et al., 2010; Grefenstette et
al., 2011) have aimed to address compositionality of
meaning in terms of distributional semantics, lead-
ing to new kinds of questions such as “how similar
are the usages of the words body and corpse in the
1047
phrase the body/corpse deliberated the motion... ?”
and “how similar are the phrases the body deliber-
ated the motion and the corpse rotted?”. In this pa-
per we focus on answering questions of the former
type and investigate models that describe the effect
of syntactic context on the meaning of a single word.
The work described in this paper uses probabilis-
tic latent variable models to describe patterns of syn-
tactic interaction, building on the selectional prefer-
ence models of O´ S´eaghdha (2010) and Ritter et al.
(2010) and the lexical substitution models of Dinu
and Lapata (2010). We propose novel methods for
incorporating information about syntactic context in
models of lexical choice, yielding a probabilistic
analogue to dependency-based models of contextual
similarity. Our models attain state-of-the-art per-
formance on two evaluation datasets: a set of sen-
tence similarity judgements collected by Mitchell
and Lapata (2008) and the dataset of the English
Lexical Substitution Task (McCarthy and Navigli,
2009). In view of the well-established effectiveness
of dependency-based distributional semantics and of
probabilistic frameworks for semantic inference, we
expect that our approach will prove to be of value in
a wide range of application settings.
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99653675">
The literature on distributional semantics is vast; in
this section we focus on outlining the research that is
most directly related to capturing effects of context
and compositionality.1 Mitchell and Lapata (2008)
</bodyText>
<note confidence="0.88596275">
1The interested reader is referred to Pad´o and Lapata (2007)
and Turney and Pantel (2010) for a general overview.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1047–1057,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999807438596491">
follow Kintsch (2001) in observing that most dis-
tributional approaches to meaning at the phrase or
sentence level assume that the contribution of syn-
tactic structure can be ignored and the meaning of a
phrase is simply the commutative sum of the mean-
ings of its constituent words. As Mitchell and Lap-
ata argue, this assumption clearly leads to an impov-
erished model of semantics. Mitchell and Lapata in-
vestigate a number of simple methods for combining
distributional word vectors, concluding that point-
wise multiplication best corresponds to the effects
of syntactic interaction.
Erk and Pad´o (2008) introduce the concept of a
structured vector space in which each word is as-
sociated with a set of selectional preference vec-
tors corresponding to different syntactic dependen-
cies. Thater et al. (2010) develop this geometric ap-
proach further using a space of second-order distri-
butional vectors that represent the words typically
co-occurring with the contexts in which a word typi-
cally appears. The primary concern of these authors
is to model the effect of context on word meaning;
the work we present in this paper uses similar intu-
itions in a probabilistic modelling framework.
A parallel strand of research seeks to represent
the meaning of larger compositional structures us-
ing matrix and tensor algebra (Smolensky, 1990;
Rudolph and Giesbrecht, 2010; Baroni and Zampar-
elli, 2010; Grefenstette et al., 2011). This nascent
approach holds the promise of providing a much
richer notion of context than is currently exploited
in semantic applications.
Probabilistic latent variable frameworks for gen-
eralising about contextual behaviour (in the form
of verb-noun selectional preferences) were proposed
by Pereira et al. (1993) and Rooth et al. (1999). La-
tent variable models are also conceptually similar
to non-probabilistic dimensionality reduction tech-
niques such as Latent Semantic Analysis (Landauer
and Dumais, 1997). More recently, O´ S´eaghdha
(2010) and Ritter et al. (2010) reformulated Rooth et
al.’s approach in a Bayesian framework using mod-
els related to Latent Dirichlet Allocation (Blei et al.,
2003), demonstrating that this “topic modelling” ar-
chitecture is a very good fit for capturing selectional
preferences. Reisinger and Mooney (2010) inves-
tigate nonparametric Bayesian models for teasing
apart the context distributions of polysemous words.
As described in Section 3 below, Dinu and Lapata
(2010) propose an LDA-based model for lexical sub-
stitution; the techniques presented in this paper can
be viewed as a generalisation of theirs. Topic models
have also been applied to other classes of semantic
task, for example word sense disambiguation (Li et
al., 2010), word sense induction (Brody and Lapata,
2009) and modelling human judgements of semantic
association (Griffiths et al., 2007).
</bodyText>
<sectionHeader confidence="0.997993" genericHeader="method">
3 Models
</sectionHeader>
<subsectionHeader confidence="0.999986">
3.1 Latent variable context models
</subsectionHeader>
<bodyText confidence="0.999786444444444">
In this paper we consider generative models of lex-
ical choice that assign a probability to a particular
word appearing in a given linguistic context. In par-
ticular, we follow recent work (Dinu and Lapata,
2010; O´ S´eaghdha, 2010; Ritter et al., 2010) in as-
suming a latent variable model that associates con-
texts with distributions over a shared set of variables
and associates each variable with a distribution over
the vocabulary of word types:
</bodyText>
<equation confidence="0.998025">
P(w|c) = � P(w|z)P(z|c) (1)
zEZ
</equation>
<bodyText confidence="0.999422666666667">
The set of latent variables Z is typically much
smaller than the vocabulary size; this induces a (soft)
clustering of the vocabulary. Latent Dirichlet Allo-
cation (Blei et al., 2003) is a powerful method for
learning such models from a text corpus in an unsu-
pervised way; LDA was originally applied to doc-
ument modelling, but it has recently been shown to
be very effective at inducing models for a variety of
semantic tasks (see Section 2).
Given the latent variable framework in (1) we can
develop a generative model of paraphrasing a word
o with another word n in a particular context c:
</bodyText>
<equation confidence="0.9998438">
�PC→T (n|o, c) = P(n|z)P(z|o, c) (2)
z
P
(o|z)P(z|c)
P(z|o, c) = �z, P(o|z,)P(z,|c) (3)
</equation>
<bodyText confidence="0.9992728">
In words, the probability P(n|o, c) is the probability
that n would be generated given the latent variable
distribution associated with seeing o in context c;
this latter distribution P(z|o, c) can be derived using
Bayes’ rule and the assumption P(o|z, c) = P(o|z).
</bodyText>
<page confidence="0.571635">
1048
</page>
<bodyText confidence="0.999232666666667">
Given a set of contexts C in which an instance o ap-
pears (e.g., it may be both the subject of a verb and
modified by an adjective), (2) and (3) become:
</bodyText>
<equation confidence="0.9935">
XPC-+T (n|o, C) = P(n|z)P(z|o, C) (4)
z
P(z|o, C)P(o|z)P(z|( |(5)
Pz, P(o|z&apos;)Pz&apos;C)
P zC QcEC P(z|c)
(  |) = P QcEC P(z&apos;|c) (6)
z,
</equation>
<bodyText confidence="0.866026">
Equation (6) can be viewed as defining a “product
of experts” model (Hinton, 2002). Dinu and Lapata
(2010) also use a similar formulation to (5), except
that P(z|o, C) is factorised over P(z|o, C) rather
than just P(z|C):
</bodyText>
<equation confidence="0.999703666666667">
YPDL10(z|o, C) =
cEC Pz, P(o|z&apos;)P(z&apos;|c) (7)
P(o|z)P(z|c)
</equation>
<bodyText confidence="0.998755285714286">
In Section 5 below, we find that using (5) rather than
(7) gives better results.
The model described above (henceforth C -+ T)
models the dependence of a target word on its con-
text. An alternative perspective is to model the de-
pendence of a set of contexts on a target word, i.e.,
we induce a model
</bodyText>
<equation confidence="0.987723">
P (c|w) = X P(c|z)P(z|w) (8)
z
</equation>
<bodyText confidence="0.9034615">
Making certain assumptions, a formula for P(n|o, c)
can be derived from (8):
</bodyText>
<equation confidence="0.999879666666667">
PT-+C(n |o, c) = P(c |, � |o)(n |o) (9)
P(c|o, n) = X P(c|z)P(z|o, n)
z
P(z|o,n) = P( z|o) )(z|n ) |) (10)
P
, P(z&apos;|oPz&apos;n
P (c|o) = X P(c|z)P(z|o) (11)
z
P(n|o) = 1/V (12)
</equation>
<bodyText confidence="0.999378142857143">
The assumption of a uniform prior P(n|o) on the
choice of a paraphrase n for o is clearly not appro-
priate from a language modelling perspective (one
could imagine an alternative P(n) based on corpus
frequency), but in the context of measuring semantic
similarity it serves well. The T -+ C model for a set
of contexts C is:
</bodyText>
<equation confidence="0.996361125">
PT-+C(n |o, C) = P(Cp(C |n)Po)(n |o) (13)
P (C|o, n) = X YP(z|o, n) P(c|z) (14)
z cEC
P (C|o) = X P(z|o) Y P(c|z) (15)
z cEC
P(z|o,C) = P(z|o) )C )|) (16)
P
, P(z&apos;OPCO
</equation>
<bodyText confidence="0.999970695652174">
With appropriate priors chosen for the distribu-
tions over words and latent variables, P(n|o, C) is
a fully generative model of lexical substitution. A
non-generative alternative is one that estimates the
similarity of the latent variable distributions associ-
ated with seeing n and o in context C. The princi-
ple that similarity between topic distributions corre-
sponds to semantic similarity is well-known in doc-
ument modelling and was proposed in the context
of lexical substitution by Dinu and Lapata (2010).
In terms of the equations presented above, we could
compare the distributions P(z|o, C) with P(z|n, C)
using equations (5) or (16). However, Thater et
al. (2010) and Dinu and Lapata (2010) both ob-
serve that contextualising both o and n can degrade
performance; in view of this we actually compare
P(z|o, C) with P(z|n) and make the further simpli-
fying assumption that P(z|n) a P(n|z). The sim-
ilarity measure we adopt is the Bhattacharyya coef-
ficient, which is a natural measure of similarity be-
tween probability distributions and is closely related
to the Hellinger distance used in previous work on
topic modelling (Blei and Lafferty, 2007):
</bodyText>
<equation confidence="0.9698135">
Xsimbhatt(Px(z), Py(z)) = qPx(z)Py(z) (17)
z
</equation>
<bodyText confidence="0.999641">
This measure takes values between 0 and 1.
In this paper we train LDA models of P(w|c) and
P(c|w). In the former case, the analogy to document
modelling is that each context type plays the role of
a “document” consisting of all the words observed
in that context in a corpus; for P(c|w) the roles are
reversed. The models are trained by Gibbs sampling
using the efficient procedure of Yao et al. (2009).
The empirical estimates for distributions over words
and latent variables are derived from the assignment
</bodyText>
<page confidence="0.439981">
1049
</page>
<bodyText confidence="0.998248333333333">
of topics over the training corpus in a single sam-
pling state. For example, to model P(w|c) we cal-
culate:
</bodyText>
<equation confidence="0.997984">
P(w|z) = fzw + β (18)
fz· + Nβ
P(z|c) = fzc + αz (19)
f·c + Ez, αz,
</equation>
<bodyText confidence="0.999991533333333">
where fzw is the number of words of type w as-
signed topic z, fzc is the number of times z is associ-
ated with context c, fz· and f·c are the marginal topic
and context counts respectively, N is the number of
word types and α and β parameterise the Dirichlet
prior distributions over P(z|c) and P(w|z). Follow-
ing the recommendations of Wallach et al. (2009)
we use asymmetric α and symmetric β; rather than
using fixed values for these hyperparameters we es-
timate them from data in the course of LDA train-
ing using an EM-like method.2 We use standard set-
tings for the number of training iterations (1000), the
length of the burnin period before hyperparameter
estimation begins (200 iterations) and the frequency
of hyperparameter estimation (50 iterations).
</bodyText>
<subsectionHeader confidence="0.999871">
3.2 Context types
</subsectionHeader>
<bodyText confidence="0.9841138">
We have not yet defined what the contexts c look
like. In vector space models of semantics it is
common to distinguish between window-based and
dependency-based models (Pad´o and Lapata, 2007);
one can make the same distinction for probabilis-
tic context models. A broad generalisation is that
window-based models capture semantic association
(e.g. referee is associated with football), while
dependency models capture a finer-grained notion
of similarity (referee is similar to umpire but not
to football). Dinu and Lapata (2010) propose a
window-based model of lexical substitution; the set
of contexts in which a word appears is the set of
surrounding words within a prespecified “window
size”. In this paper we also investigate dependency-
based context sets derived from syntactic structure.
Given a sentence such as
2We use the estimation methods provided by the MAL-
LET toolkit, available from http://mallet.cs.umass.
edu/.
</bodyText>
<equation confidence="0.82793725">
v:ncsubj:n
V
The:d executive:j body:n decided:v . . .
n:ncmod:j
</equation>
<bodyText confidence="0.999837666666667">
the set C of dependency contexts for the noun body
is {executive:j:ncmod−1:n, decide:v:ncsubj:n},
where ncmod−1 denotes that body stands in an in-
verse non-clausal modifier relation to executive (we
assume that nouns are the heads of their adjectival
modifiers).
</bodyText>
<sectionHeader confidence="0.937215" genericHeader="method">
4 Experiment 1: Similarity in context
</sectionHeader>
<subsectionHeader confidence="0.988285">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999974954545455">
Mitchell and Lapata (2008) collected human judge-
ments of semantic similarity for pairs of short sen-
tences, where the sentences in a pair share the same
subject but different verbs. For example, the sales
slumped and the sales declined should be judged as
very similar while the shoulders slumped and the
shoulders declined should be judged as less similar.
The resulting dataset (henceforth ML08) consists of
120 such pairs using 15 verbs, balanced across high
and low expected similarity. 60 subjects rated the
data using a scale of 1–7; Mitchell and Lapata cal-
culate average interannotator correlation to be 0.40
(using Spearman’s ρ). Both Mitchell and Lapata
and Erk and Pad´o (2008) split the data into a devel-
opment portion and a test portion, the development
portion consisting of the judgements of six annota-
tors; in order to compare our results with previous
research we use the same data split. To evaluate per-
formance, the predictions made by a model are com-
pared to the judgements of each annotator in turn
(using ρ) and the resulting per-annotator ρ values are
averaged.
</bodyText>
<subsectionHeader confidence="0.996723">
4.2 Models
</subsectionHeader>
<bodyText confidence="0.996607111111111">
All models were trained on the written section of the
British National Corpus (around 90 million words),
parsed with RASP (Briscoe et al., 2006). The BNC
was also used by Mitchell and Lapata (2008) and
Erk and Pad´o (2008); as the ML08 dataset was com-
piled using words appearing more than 50 times in
the BNC, there are no coverage problems caused
by data sparsity. We trained LDA models for the
grammatical relations v:ncsubj:n and n:ncsubj−1:v
</bodyText>
<table confidence="0.9807857">
1050
Model PARA SIM
C → T 0.24 0.34
No optimisation T → C 0.36 0.39
T ↔ C 0.33 0.39
C → T 0.24 0.35
Optimised on dev T → C 0.41 0.41
T ↔ C 0.37 0.41
Erk and Pad´o (2008) Mult 0.24
SVS 0.27
</table>
<tableCaption confidence="0.8345885">
Table 1: Performance (average p) on the ML08 test
set
</tableCaption>
<bodyText confidence="0.999938214285714">
and used these to create predictors of type C → T
and T → C, respectively. For each predictor, we
trained five runs with 100 topics for 1000 iterations
and averaged the predictions produced from their fi-
nal states. We investigate both the generative para-
phrasing model (PARA) and the method of compar-
ing topic distributions (SIM). For both PARA and
SIM we present results using each predictor type on
its own as well as a combination of both types (T ↔
C); for PARA the contributions of the types are mul-
tiplied and for SIM they are averaged.3 One poten-
tial complication is that the PARA model is trained
to predict P(n|c, o), which might not be comparable
across different combinations of subject c and verb
o. Using P(n|c, o) as a proxy for the desired joint
distribution P(n, c, o) is tantamount to assuming a
uniform distribution P(c, o), which can be defended
on the basis that the choice of subject noun and ref-
erence verb is not directly relevant to the task. As
shown by the results below, this assumption seems
to work reasonably well in practice.
As well as reporting correlations for straightfor-
ward averages of each set of five runs, we also inves-
tigate whether the development data can be used to
select an optimal subset of runs. This is done by sim-
ply evaluating every possible subset of 1–5 runs on
the development data and picking the best-scoring
subset.
</bodyText>
<subsectionHeader confidence="0.884702">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.998091896551724">
Table 1 presents the results of the PARA and SIM
predictors on the ML08 dataset. The best results
3This configuration seems the most intuitive; averaging
PARA predictors and multiplying SIM also give good results.
previously reported for this dataset were given by
Erk and Pad´o (2008), who measured average p val-
ues of 0.24 for a vector multiplication method and
0.27 for their structured vector space (SVS) syn-
tactic disambiguation method. Even without using
the development set to select models, performance is
well above the previous state of the art for all predic-
tors except PARAC→T. Model selection on the de-
velopment data brings average p up to 0.41, which is
comparable to the human “ceiling” of 0.40 measured
by Mitchell and Lapata. In all cases the T → C pre-
dictors outperform C → T: models that associate
target words with distributions over context clusters
are superior to those that associate contexts with dis-
tributions over target words.
Figure 1 plots the beneficial effect of averaging
over multiple runs; as the number of runs n is in-
creased, the average performance over all combi-
nations of n predictors chosen from the set of five
T → C and five C → T runs is observed to in-
crease monotonically. Figure 1 also shows that the
model selection procedure is very effective at se-
lecting the optimal combination of models; develop-
ment set performance is a reliable indicator of test
set performance.
</bodyText>
<sectionHeader confidence="0.976847" genericHeader="evaluation">
5 Experiment 2: Lexical substitution
</sectionHeader>
<subsectionHeader confidence="0.94947">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999962947368421">
The English Lexical Substitution task, run as part
of the SemEval-1 competition, required participants
to propose good substitutes for a set of target words
in various sentential contexts (McCarthy and Nav-
igli, 2009). Table 2 shows two example sentences
and the substitutes appearing in the gold standard,
ranked by the number of human annotators who pro-
posed each substitute. The dataset contains a total of
2,010 annotated sentences with 205 distinct target
words across four parts of speech (noun, verb, ad-
jective, adverb). In line with previous work on con-
textual disambiguation, we focus here on the subtask
of ranking attested substitutes rather than proposing
them from an unrestricted vocabulary. To this end,
a candidate set is constructed for each target word
from all the substitutes proposed for that word in all
sentences in the dataset.
The data contains a number of multiword para-
phrases such as rush at; as our models (like most
</bodyText>
<page confidence="0.4843">
1051
</page>
<figureCaption confidence="0.838084666666667">
Figure 1: Performance on the ML08 test set with different predictor types and different numbers of LDA
runs per predictor type; the solid line tracks the average performance, the dashed line shows the performance
of the predictor combination that scores best on the development set.
</figureCaption>
<table confidence="0.806541">
Realizing immediately that strangers have come, attack (5), rush at (1)
the animals charge them and the horses began to fight.
Commission is the amount charged to execute a trade. levy (2), impose (1), take (1), demand (1)
</table>
<tableCaption confidence="0.848183">
Table 2: Examples for the verb charge from the English Lexical Substitution Task
</tableCaption>
<figure confidence="0.998075833333333">
1 2 3 4 5
No. of predictors
(a) PARA: Target → Context
1 2 3 4 5
No. of predictors
(d) SIM: Target → Context
P
1 2 3 4 5
0.5
0.4
0.1
0.3
0.2
No. of predictors
(b) PARA: Context → Target
1 2 3 4 5
No. of predictors
(e) SIM: Context → Target
2 3 4 5 6 7 8 9 10
0.5
0.4
0.1
0.3
P
0.2
No. of predictors
(c) PARA: Target ↔ Context
2 3 4 5 6 7 8 9 10
No. of predictors
(f) SIM: Target ↔ Context
0.5
0.4
0.3
P
0.2
0.1
0.5
0.4
0.3
P
0.2
0.1
0.5
0.4
0.3
P
0.2
0.1
0.5
0.4
0.3
P
0.2
0.1
</figure>
<bodyText confidence="0.985299379310345">
current models of distributional semantics) do not
represent multiword expressions, we remove such
paraphrases and discard the 17 sentences which have
only multiword substitutes in the gold standard.4
There are also 7 sentences for which the gold stan-
dard contains no substitutes. This leaves a total of
1986 sentences. These sentences were lemmatised
and parsed with RASP.
Previous authors have partitioned the dataset in
various ways. Erk and Pad´o (2008) use only a sub-
set of the data where the target is a noun headed
by a verb or a verb heading a noun. Thater et al.
4Thater et al. (2010) and Dinu and Lapata (2010) similarly
remove multiword paraphrases (Georgiana Dinu, p.c.).
(2010) discard sentences which their parser cannot
parse and paraphrases absent from their training cor-
pus and then optimise the parameters of their model
through four-fold cross-validation. Here we aim for
complete coverage on the dataset and do not perform
any parameter tuning. We use two measures to eval-
uate performance: Generalised Averaged Precision
(Kishida, 2005) and Kendall’s τb rank correlation
coefficient, which were used for this task by Thater
et al. (2010) and Dinu and Lapata (2010), respec-
tively. Generalised Averaged Precision (GAP) is
a precision-like measure for evaluating ranked pre-
dictions against a gold standard. τb is a variant of
Kendall’s τ that is appropriate for data containing
tied ranks. We do not use the “precision out of ten”
</bodyText>
<figure confidence="0.868822">
1052
COORDINATION:
n:and:n
PREPOSITIONS:
n:prep in:n
V
the hat ⇒ The cat in the hat
i:dobj:n
</figure>
<tableCaption confidence="0.985255">
Table 3: Dependency graph preprocessing
</tableCaption>
<bodyText confidence="0.999928555555555">
measure that was used in the original Lexical Substi-
tution Task; this measure assigns credit for the pro-
portion of the first 10 proposed paraphrases that are
present in the gold standard and in the context of
ranking attested substitutes it is unclear how to ob-
tain non-trivial results for target words with 10 or
fewer possible substitutes. We calculate statistical
significance of performance differences using strati-
fied shuffling (Yeh, 2000).5
</bodyText>
<subsectionHeader confidence="0.9972">
5.2 Models
</subsectionHeader>
<bodyText confidence="0.992104492307692">
We apply the models developed in Section 3.1 to the
Lexical Substitution Task dataset using dependency-
and window-based context information. Here we
only use the SIM predictor type. PARA did not give
satisfactory results; in particular, it tended to rank
common words highly in most contexts.6
As before we compiled training data by extracting
target-context cooccurrences from a text corpus. In
addition to the parsed BNC described above we used
a corpus of Wikipedia text consisting of over 45 mil-
lion sentences (almost 1 billion words) parsed using
the fast Combinatory Categorial Grammar (CCG)
parser described by Clark et al. (2009). The depen-
5We use the software package available at http://www.
nlpado.de/˜sebastian/sigf.html.
6Favouring more general words may indeed make sense in
some paraphrasing tasks (Nulty and Costello, 2010).
dency representation produced by this parser is inter-
operable with the RASP dependency format. In or-
der to focus our models on semantically discrimina-
tive information and make inference more tractable
we ignored all parts of speech other than nouns,
verbs, adjectives, prepositions and adverbs. Stop-
words and words of fewer than three characters were
removed. We also removed the very frequent but se-
mantically weak lemmas be and have.
We compare two classes of context models: mod-
els learned from window-based contexts and models
learned from syntactic dependency contexts. For the
syntactic models we extracted all dependencies and
inverse dependencies between lemmas of the afore-
mentioned POS types; in order to maximise the ex-
traction yield, the dependency graph for each sen-
tence was preprocessed using the transformations
shown in Table 3. For the window-based context
model we follow Dinu and Lapata (2010) in treating
each word within five words of a target as a member
of its context set.
It proved necessary to subsample the corpora in
order to make LDA training tractable, especially for
the window-based model where the training set of
context-target counts is extremely dense (each in-
stance of a word in the corpus contributes up to
10 context instances). For the window-based data,
we divided each context-target count by a factor of
5 and a factor of 70 for the BNC and Wikipedia
corpora respectively, rounding fractional counts to
the closest integer. The choice of 70 for scaling
Wikipedia counts is adopted from Dinu and Lap-
ata (2010), who used the same factor for the com-
parably sized English Gigaword corpus. As the de-
pendency data is an order of magnitude smaller we
downsampled the Wikipedia counts by 5 and left the
BNC counts untouched. Finally, we created a larger
corpus by combining the counts from the BNC and
Wikipedia datasets. Type and token counts for the
BNC and combined corpora are given in Table 4.
We trained three LDA predictors for each corpus:
a window-based predictor (W5), a Context → Tar-
get predictor (C → T) and a Target → Context
predictor (T → C). For W5 the sets of types and
contexts should be symmetrical (in practice there
is some discrepancy due to preprocessing artefacts).
For C → T, individual models were trained for each
of the four target parts of speech; in each case the set
</bodyText>
<figure confidence="0.985371620689655">
OO
v:ncsubj:n
⇒ Cats and dogs
OO
run
V
V
OO
v:ncsubj:n
V
dogs run
Cats and
OO
OO
c:conj:n
c:conj:n
PREDICATION:
v:ncsubj:n
OO
n:ncmod:j
V
fierce ⇒ The cat is fierce
The cat is
v:xcomp:j
V
The cat
n:ncmod:i
y
in
</figure>
<table confidence="0.942653125">
1053
BNC BNC+Wikipedia
Tokens Types Contexts Tokens Types Contexts
Nouns 18723082 122999 316237 54145216 106448 514257
Verbs 7893462 18494 57528 20082658 16673 82580
Adjectives 4385788 73684 37163 11536424 88488 57531
Adverbs 1976837 7124 14867 3017936 4056 18510
Window5 28329238 88265 102792 42828094 139640 143443
</table>
<tableCaption confidence="0.984761">
Table 4: Type and token counts for the BNC and downsampled BNC+Wikipedia corpora
</tableCaption>
<table confidence="0.999820555555556">
BNC BNC + Wikipedia
GAP rb Coverage GAP rb Coverage
W5 44.5 0.17 100.0 44.8 0.17 100.0
C → T 43.2 0.16 86.4 48.7 0.21 86.5
T → C 47.2 0.21 86.4 49.3 0.22 86.5
T ↔ C 45.7 0.20 86.4 49.1 0.23 86.5
W5 + C → T 46.0 0.18 100.0 48.7 0.21 100.0
W5 + T → C 48.6 0.21 100.0 49.3 0.22 100.0
W5 + T ↔ C 48.1 0.20 100.0 49.5 0.23 100.0
</table>
<tableCaption confidence="0.9057445">
Table 5: Results on the English Lexical Substitution Task dataset; boldface denotes best performance at full
coverage for each corpus
</tableCaption>
<bodyText confidence="0.999921333333333">
of types is the vocabulary for that part of speech and
the set of contexts is the set of dependencies taking
those types as dependents. For T → C we again
train four models; the sets of types and contexts are
reversed. For the both corpora we trained models
with Z = {600, 800,1000,1200} topics; for each
setting of Z we ran five estimation runs. Each in-
dividual prediction of similarity between P(z|C, o)
and P(z|n) is made by averaging over the predic-
tions of all runs and over all settings of Z. Choosing
a single setting of Z does not degrade performance
significantly; however, averaging over settings is a
convenient way to avoid having to pick a specific
value.
We also investigate combinations of predictor
types, once again produced by averaging: we com-
bine C → T with C ↔ T (T ↔ C) and combine
each of these three models with W5.
</bodyText>
<subsectionHeader confidence="0.934482">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999927137931035">
Table 5 presents the results attained by our mod-
els on the Lexical Substitution Task data. The
dependency-based models have imperfect coverage
(86% of the data); they can make no prediction when
no syntactic context is provided for a target, per-
haps as a result of parsing error. The window-based
models have perfect coverage, but score noticeably
lower. By combining dependency- and window-
based models we can reach high performance with
perfect coverage. All combinations outperform the
corresponding W5 results to a statistically signifi-
cant degree (p &lt; 0.01). Performance at full cov-
erage is already very good (GAP= 48.6, rb = 0.21)
on the BNC corpus, but the best results are attained
by W5 + T ↔ C trained on the combined corpus
(GAP= 49.5, rb = 0.23). The results for the W5
model trained on BNC data is comparable to that
trained on the combined corpus; however the syntac-
tic models show a clear benefit from the less sparse
dependency data in the combined training corpus.
As remarked in Section 3.1, Dinu and Lap-
ata (2010) use a slightly different formulation of
P(z|C, o). Using the window-based context model
our formulation (5) outperforms (7) for both training
corpora; the Dinu and Lapata (2010) version scores
GAP = 41.5, rb = 0.15 for the BNC corpus and
GAP = 42.0, rb = 0.15 for the combined corpus.
The advantage of our formulation is statistically sig-
nificant for all evaluation measures.
</bodyText>
<table confidence="0.979441111111111">
1054
Nouns Verbs Adjectives Adverbs Overall
GAP τb GAP τb GAP τb GAP τb GAP τb
W5 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17
W5 + T ↔ C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23
Thater et al. (2010) (Model 1) 46.4 – 45.9 – 39.4 – 48.2 – 44.6 –
Thater et al. (2010) (Model 2) 42.5 – – – 43.2 – 51.4 – – –
Dinu and Lapata (2010) (LDA) – 0.16 – 0.14 – 0.17 – 0.21 – 0.16
Dinu and Lapata (2010) (NMF) – 0.15 – 0.14 – 0.16 – 0.26 – 0.16
</table>
<tableCaption confidence="0.999194">
Table 6: Performance by part of speech
</tableCaption>
<bodyText confidence="0.997875772727273">
Table 6 gives a breakdown of performance by tar-
get part of speech for the BNC+Wikipedia-trained
W5 and W5 + T ↔ C models, as well as figures
provided by previous researchers.7 W5 + T ↔ C
outperforms W5 on all parts of speech using both
evaluation metrics. As remarked above, previous re-
searchers have used the corpus in slightly different
ways; we believe that the results of Dinu and Lapata
(2010) are fully comparable, while those of Thater et
al. (2010) were attained on a slightly smaller dataset
with parameters set through cross-validation. The
results for W5 + T ↔ C outperform all of Dinu
and Lapata’s per-POS and overall results except for
a slightly superior score on adverbs attained by their
NMF model (τb = 0.26 compared to 0.24). Turn-
ing to Thater et al., we report higher scores for ev-
ery POS with the exception of the verbs where their
Model 1 achieves 45.9 GAP compared to 45.1; the
overall average for W5 + T ↔ C is substantially
higher at 49.5 compared to 44.6. On balance, we
suggest that our models do have an advantage over
the current state of the art for lexical substitution.
</bodyText>
<sectionHeader confidence="0.998213" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.995182772727273">
In this paper we have proposed novel methods for
modelling the effect of context on lexical mean-
ing, demonstrating that information about syntactic
context and textual proximity can fruitfully be inte-
grated to produce state-of-the-art models of lexical
choice. We have demonstrated the effectiveness of
our techniques on two datasets but they are poten-
tially applicable to a range of applications where se-
mantic disambiguation is required. In future work,
7The overall average GAP for Thater et al. (2010) does not
appear in their paper but can be calculated from the score and
number of instances listed for each POS.
we intend to adapt our approach for word sense dis-
ambiguation as well as related domain-specific tasks
such as gene name normalisation (Morgan et al.,
2008). A further, more speculative direction for fu-
ture research is to investigate more richly structured
models of context, for example capturing correla-
tions between words in a text within a framework
similar to the Correlated Topic Model of Blei and
Lafferty (2007) or more explicitly modelling poly-
semy effects as in Reisinger and Mooney (2010).
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999618">
We are grateful to the EMNLP reviewers for their
helpful comments. This research was supported by
EPSRC grant EP/G051070/1.
</bodyText>
<sectionHeader confidence="0.966394" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995874657718121">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
10), Cambridge, MA.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. The Annals of Applied Statis-
tics, 1(1):17–35.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions,
Sydney, Australia.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL-09, Athens,
Greece.
1055
Stephen Clark, Ann Copestake, James R. Curran, Yue
Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu
Ahn, Curt Van Wyk, Jessika Roesner, Jonathan Kum-
merfeld, and Tim Dawborn. 2009. Large-scale syn-
tactic processing: Parsing the web. Technical report,
Final Report of the 2009 JHU CLSP Workshop.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP-10), Cambridge,MA.
Katrin Erk and Sebastian Pad´o. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-08),
Honolulu, HI.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. In Proceedings of the 9th In-
ternational Conference on Computational Semantics
(IWCS-11), Oxford, UK.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211–244.
Kenneth E. Harper. 1965. Measurement of similarity be-
tween nouns. In Proceedings of the 1965 International
Conference on Computational Linguistics (COLING-
65), New York, NY.
Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771–1800.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173–202.
Kazuaki Kishida. 2005. Property of average precision
and its generalisation: An examination of evaluation
indicator for information retrieval experiments. Tech-
nical Report NII-2005-014E, National Institute of In-
formatics, Tokyo, Japan.
Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211–240.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL-08), Columbus, OH.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M Cohen, Juliane Fluck, Patrick Ruch, Anna
Divoli, Katrin Fundel, Robert Leaman, J¨org Haken-
berg, Chengjie Sun, Heng hui Liu, Rafael Torres,
Michael Krauthammer, William W Lau, Hongfang
Liu, Chun-Nan Hsu, Martijn Schuemie, K. Bretonnel
Cohen, and Lynette Hirschman. 2008. Overview of
BioCreative II gene normalization. Genome Biology,
9(Suppl 2).
Paul Nulty and Fintan Costello. 2010. UCD-PN: Select-
ing general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval-2), Uppsala, Sweden.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL-10), Uppsala, Sweden.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of the Association
for Computational Linguistics, Columbus, OH.
Joseph Reisinger and Raymond Mooney. 2010. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-10),
Cambridge,MA.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet allocation method for selectional prefer-
ences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
10), Uppsala, Sweden.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-99), College
Park, MD.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-10), Uppsala,
Sweden.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1–
2):159–216.
Karen Sp¨arck Jones. 1964. Synonymy and Semantic
Classification. Ph.D. thesis, University of Cambridge.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
1056
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL-10), Uppsala, Swe-
den.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Proceedings
of the 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-09),
Paris, France.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics
(COLING-00), Saarbr¨ucken, Germany.
</reference>
<page confidence="0.798787">
1057
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153652">
<title confidence="0.994816">Probabilistic models of similarity in syntactic context</title>
<author confidence="0.913513">Diarmuid O´</author>
<affiliation confidence="0.961082">Computer University of</affiliation>
<note confidence="0.409325285714286">United do242@cl.cam.ac.uk Anna Computer University of United Anna.Korhonen@cl.cam.ac.uk</note>
<abstract confidence="0.997894923076923">This paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity. The resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP10),</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="4965" citStr="Baroni and Zamparelli, 2010" startWordPosition="763" endWordPosition="767">yntactic dependencies. Thater et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010)</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP10), Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>A correlated topic model of science.</title>
<date>2007</date>
<journal>The Annals of Applied Statistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="10649" citStr="Blei and Lafferty, 2007" startWordPosition="1733" endWordPosition="1736">quations presented above, we could compare the distributions P(z|o, C) with P(z|n, C) using equations (5) or (16). However, Thater et al. (2010) and Dinu and Lapata (2010) both observe that contextualising both o and n can degrade performance; in view of this we actually compare P(z|o, C) with P(z|n) and make the further simplifying assumption that P(z|n) a P(n|z). The similarity measure we adopt is the Bhattacharyya coefficient, which is a natural measure of similarity between probability distributions and is closely related to the Hellinger distance used in previous work on topic modelling (Blei and Lafferty, 2007): Xsimbhatt(Px(z), Py(z)) = qPx(z)Py(z) (17) z This measure takes values between 0 and 1. In this paper we train LDA models of P(w|c) and P(c|w). In the former case, the analogy to document modelling is that each context type plays the role of a “document” consisting of all the words observed in that context in a corpus; for P(c|w) the roles are reversed. The models are trained by Gibbs sampling using the efficient procedure of Yao et al. (2009). The empirical estimates for distributions over words and latent variables are derived from the assignment 1049 of topics over the training corpus in </context>
</contexts>
<marker>Blei, Lafferty, 2007</marker>
<rawString>David M. Blei and John D. Lafferty. 2007. A correlated topic model of science. The Annals of Applied Statistics, 1(1):17–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5698" citStr="Blei et al., 2003" startWordPosition="872" endWordPosition="875">than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody a</context>
<context position="7100" citStr="Blei et al., 2003" startWordPosition="1095" endWordPosition="1098">s of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work (Dinu and Lapata, 2010; O´ S´eaghdha, 2010; Ritter et al., 2010) in assuming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabulary of word types: P(w|c) = � P(w|z)P(z|c) (1) zEZ The set of latent variables Z is typically much smaller than the vocabulary size; this induces a (soft) clustering of the vocabulary. Latent Dirichlet Allocation (Blei et al., 2003) is a powerful method for learning such models from a text corpus in an unsupervised way; LDA was originally applied to document modelling, but it has recently been shown to be very effective at inducing models for a variety of semantic tasks (see Section 2). Given the latent variable framework in (1) we can develop a generative model of paraphrasing a word o with another word n in a particular context c: �PC→T (n|o, c) = P(n|z)P(z|o, c) (2) z P (o|z)P(z|c) P(z|o, c) = �z, P(o|z,)P(z,|c) (3) In words, the probability P(n|o, c) is the probability that n would be generated given the latent varia</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Interactive Presentation Sessions,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="14698" citStr="Briscoe et al., 2006" startWordPosition="2406" endWordPosition="2409">sing Spearman’s ρ). Both Mitchell and Lapata and Erk and Pad´o (2008) split the data into a development portion and a test portion, the development portion consisting of the judgements of six annotators; in order to compare our results with previous research we use the same data split. To evaluate performance, the predictions made by a model are compared to the judgements of each annotator in turn (using ρ) and the resulting per-annotator ρ values are averaged. 4.2 Models All models were trained on the written section of the British National Corpus (around 90 million words), parsed with RASP (Briscoe et al., 2006). The BNC was also used by Mitchell and Lapata (2008) and Erk and Pad´o (2008); as the ML08 dataset was compiled using words appearing more than 50 times in the BNC, there are no coverage problems caused by data sparsity. We trained LDA models for the grammatical relations v:ncsubj:n and n:ncsubj−1:v 1050 Model PARA SIM C → T 0.24 0.34 No optimisation T → C 0.36 0.39 T ↔ C 0.33 0.39 C → T 0.24 0.35 Optimised on dev T → C 0.41 0.41 T ↔ C 0.37 0.41 Erk and Pad´o (2008) Mult 0.24 SVS 0.27 Table 1: Performance (average p) on the ML08 test set and used these to create predictors of type C → T and T</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the ACL-06 Interactive Presentation Sessions, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="6314" citStr="Brody and Lapata, 2009" startWordPosition="966" endWordPosition="969">, 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 Models 3.1 Latent variable context models In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work (Dinu and Lapata, 2010; O´ S´eaghdha, 2010; Ritter et al., 2010) in assuming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabulary of word types: P(w|c) = � P(w|z)P(z|c) (1)</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of EACL-09, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Ann Copestake</author>
<author>James R Curran</author>
<author>Yue Zhang</author>
<author>Aurelie Herbelot</author>
<author>James Haggerty</author>
<author>Byung-Gyu Ahn</author>
<author>Curt Van Wyk</author>
<author>Jessika Roesner</author>
<author>Jonathan Kummerfeld</author>
<author>Tim Dawborn</author>
</authors>
<title>Large-scale syntactic processing: Parsing the web. Technical report,</title>
<date>2009</date>
<journal>Final Report of the 2009 JHU CLSP Workshop.</journal>
<marker>Clark, Copestake, Curran, Zhang, Herbelot, Haggerty, Ahn, Van Wyk, Roesner, Kummerfeld, Dawborn, 2009</marker>
<rawString>Stephen Clark, Ann Copestake, James R. Curran, Yue Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu Ahn, Curt Van Wyk, Jessika Roesner, Jonathan Kummerfeld, and Tim Dawborn. 2009. Large-scale syntactic processing: Parsing the web. Technical report, Final Report of the 2009 JHU CLSP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-10),</booktitle>
<location>Cambridge,MA.</location>
<contexts>
<context position="2356" citStr="Dinu and Lapata (2010)" startWordPosition="366" endWordPosition="369">e words body and corpse in the 1047 phrase the body/corpse deliberated the motion... ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models of O´ S´eaghdha (2010) and Ritter et al. (2010) and the lexical substitution models of Dinu and Lapata (2010). We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity. Our models attain state-of-the-art performance on two evaluation datasets: a set of sentence similarity judgements collected by Mitchell and Lapata (2008) and the dataset of the English Lexical Substitution Task (McCarthy and Navigli, 2009). In view of the well-established effectiveness of dependency-based distributional semantics and of probabilistic frameworks for semantic inference, we expect that</context>
<context position="6003" citStr="Dinu and Lapata (2010)" startWordPosition="916" endWordPosition="919">milar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 Models 3.1 Latent variable context models In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular</context>
<context position="8255" citStr="Dinu and Lapata (2010)" startWordPosition="1305" endWordPosition="1308">is the probability that n would be generated given the latent variable distribution associated with seeing o in context c; this latter distribution P(z|o, c) can be derived using Bayes’ rule and the assumption P(o|z, c) = P(o|z). 1048 Given a set of contexts C in which an instance o appears (e.g., it may be both the subject of a verb and modified by an adjective), (2) and (3) become: XPC-+T (n|o, C) = P(n|z)P(z|o, C) (4) z P(z|o, C)P(o|z)P(z|( |(5) Pz, P(o|z&apos;)Pz&apos;C) P zC QcEC P(z|c) ( |) = P QcEC P(z&apos;|c) (6) z, Equation (6) can be viewed as defining a “product of experts” model (Hinton, 2002). Dinu and Lapata (2010) also use a similar formulation to (5), except that P(z|o, C) is factorised over P(z|o, C) rather than just P(z|C): YPDL10(z|o, C) = cEC Pz, P(o|z&apos;)P(z&apos;|c) (7) P(o|z)P(z|c) In Section 5 below, we find that using (5) rather than (7) gives better results. The model described above (henceforth C -+ T) models the dependence of a target word on its context. An alternative perspective is to model the dependence of a set of contexts on a target word, i.e., we induce a model P (c|w) = X P(c|z)P(z|w) (8) z Making certain assumptions, a formula for P(n|o, c) can be derived from (8): PT-+C(n |o, c) = P(c</context>
<context position="10006" citStr="Dinu and Lapata (2010)" startWordPosition="1626" endWordPosition="1629">= X YP(z|o, n) P(c|z) (14) z cEC P (C|o) = X P(z|o) Y P(c|z) (15) z cEC P(z|o,C) = P(z|o) )C )|) (16) P , P(z&apos;OPCO With appropriate priors chosen for the distributions over words and latent variables, P(n|o, C) is a fully generative model of lexical substitution. A non-generative alternative is one that estimates the similarity of the latent variable distributions associated with seeing n and o in context C. The principle that similarity between topic distributions corresponds to semantic similarity is well-known in document modelling and was proposed in the context of lexical substitution by Dinu and Lapata (2010). In terms of the equations presented above, we could compare the distributions P(z|o, C) with P(z|n, C) using equations (5) or (16). However, Thater et al. (2010) and Dinu and Lapata (2010) both observe that contextualising both o and n can degrade performance; in view of this we actually compare P(z|o, C) with P(z|n) and make the further simplifying assumption that P(z|n) a P(n|z). The similarity measure we adopt is the Bhattacharyya coefficient, which is a natural measure of similarity between probability distributions and is closely related to the Hellinger distance used in previous work o</context>
<context position="12689" citStr="Dinu and Lapata (2010)" startWordPosition="2081" endWordPosition="2084">terations) and the frequency of hyperparameter estimation (50 iterations). 3.2 Context types We have not yet defined what the contexts c look like. In vector space models of semantics it is common to distinguish between window-based and dependency-based models (Pad´o and Lapata, 2007); one can make the same distinction for probabilistic context models. A broad generalisation is that window-based models capture semantic association (e.g. referee is associated with football), while dependency models capture a finer-grained notion of similarity (referee is similar to umpire but not to football). Dinu and Lapata (2010) propose a window-based model of lexical substitution; the set of contexts in which a word appears is the set of surrounding words within a prespecified “window size”. In this paper we also investigate dependencybased context sets derived from syntactic structure. Given a sentence such as 2We use the estimation methods provided by the MALLET toolkit, available from http://mallet.cs.umass. edu/. v:ncsubj:n V The:d executive:j body:n decided:v . . . n:ncmod:j the set C of dependency contexts for the noun body is {executive:j:ncmod−1:n, decide:v:ncsubj:n}, where ncmod−1 denotes that body stands i</context>
<context position="20724" citStr="Dinu and Lapata (2010)" startWordPosition="3490" endWordPosition="3493">1 current models of distributional semantics) do not represent multiword expressions, we remove such paraphrases and discard the 17 sentences which have only multiword substitutes in the gold standard.4 There are also 7 sentences for which the gold standard contains no substitutes. This leaves a total of 1986 sentences. These sentences were lemmatised and parsed with RASP. Previous authors have partitioned the dataset in various ways. Erk and Pad´o (2008) use only a subset of the data where the target is a noun headed by a verb or a verb heading a noun. Thater et al. 4Thater et al. (2010) and Dinu and Lapata (2010) similarly remove multiword paraphrases (Georgiana Dinu, p.c.). (2010) discard sentences which their parser cannot parse and paraphrases absent from their training corpus and then optimise the parameters of their model through four-fold cross-validation. Here we aim for complete coverage on the dataset and do not perform any parameter tuning. We use two measures to evaluate performance: Generalised Averaged Precision (Kishida, 2005) and Kendall’s τb rank correlation coefficient, which were used for this task by Thater et al. (2010) and Dinu and Lapata (2010), respectively. Generalised Averaged</context>
<context position="23897" citStr="Dinu and Lapata (2010)" startWordPosition="3984" endWordPosition="3987">bs. Stopwords and words of fewer than three characters were removed. We also removed the very frequent but semantically weak lemmas be and have. We compare two classes of context models: models learned from window-based contexts and models learned from syntactic dependency contexts. For the syntactic models we extracted all dependencies and inverse dependencies between lemmas of the aforementioned POS types; in order to maximise the extraction yield, the dependency graph for each sentence was preprocessed using the transformations shown in Table 3. For the window-based context model we follow Dinu and Lapata (2010) in treating each word within five words of a target as a member of its context set. It proved necessary to subsample the corpora in order to make LDA training tractable, especially for the window-based model where the training set of context-target counts is extremely dense (each instance of a word in the corpus contributes up to 10 context instances). For the window-based data, we divided each context-target count by a factor of 5 and a factor of 70 for the BNC and Wikipedia corpora respectively, rounding fractional counts to the closest integer. The choice of 70 for scaling Wikipedia counts</context>
<context position="28268" citStr="Dinu and Lapata (2010)" startWordPosition="4764" endWordPosition="4768"> reach high performance with perfect coverage. All combinations outperform the corresponding W5 results to a statistically significant degree (p &lt; 0.01). Performance at full coverage is already very good (GAP= 48.6, rb = 0.21) on the BNC corpus, but the best results are attained by W5 + T ↔ C trained on the combined corpus (GAP= 49.5, rb = 0.23). The results for the W5 model trained on BNC data is comparable to that trained on the combined corpus; however the syntactic models show a clear benefit from the less sparse dependency data in the combined training corpus. As remarked in Section 3.1, Dinu and Lapata (2010) use a slightly different formulation of P(z|C, o). Using the window-based context model our formulation (5) outperforms (7) for both training corpora; the Dinu and Lapata (2010) version scores GAP = 41.5, rb = 0.15 for the BNC corpus and GAP = 42.0, rb = 0.15 for the combined corpus. The advantage of our formulation is statistically significant for all evaluation measures. 1054 Nouns Verbs Adjectives Adverbs Overall GAP τb GAP τb GAP τb GAP τb GAP τb W5 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17 W5 + T ↔ C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23 Thater et al. (2010) (Model 1)</context>
<context position="29529" citStr="Dinu and Lapata (2010)" startWordPosition="5011" endWordPosition="5014">ter et al. (2010) (Model 2) 42.5 – – – 43.2 – 51.4 – – – Dinu and Lapata (2010) (LDA) – 0.16 – 0.14 – 0.17 – 0.21 – 0.16 Dinu and Lapata (2010) (NMF) – 0.15 – 0.14 – 0.16 – 0.26 – 0.16 Table 6: Performance by part of speech Table 6 gives a breakdown of performance by target part of speech for the BNC+Wikipedia-trained W5 and W5 + T ↔ C models, as well as figures provided by previous researchers.7 W5 + T ↔ C outperforms W5 on all parts of speech using both evaluation metrics. As remarked above, previous researchers have used the corpus in slightly different ways; we believe that the results of Dinu and Lapata (2010) are fully comparable, while those of Thater et al. (2010) were attained on a slightly smaller dataset with parameters set through cross-validation. The results for W5 + T ↔ C outperform all of Dinu and Lapata’s per-POS and overall results except for a slightly superior score on adverbs attained by their NMF model (τb = 0.26 compared to 0.24). Turning to Thater et al., we report higher scores for every POS with the exception of the verbs where their Model 1 achieves 45.9 GAP compared to 45.1; the overall average for W5 + T ↔ C is substantially higher at 49.5 compared to 44.6. On balance, we su</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-10), Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<location>Honolulu, HI.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Stephen Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics (IWCS-11),</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="1570" citStr="Grefenstette et al., 2011" startWordPosition="233" endWordPosition="236">ypically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms of distributional semantics, leading to new kinds of questions such as “how similar are the usages of the words body and corpse in the 1047 phrase the body/corpse deliberated the motion... ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patt</context>
<context position="4993" citStr="Grefenstette et al., 2011" startWordPosition="768" endWordPosition="771"> et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2011</marker>
<rawString>Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. In Proceedings of the 9th International Conference on Computational Semantics (IWCS-11), Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="6394" citStr="Griffiths et al., 2007" startWordPosition="977" endWordPosition="980">it for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 Models 3.1 Latent variable context models In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work (Dinu and Lapata, 2010; O´ S´eaghdha, 2010; Ritter et al., 2010) in assuming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabulary of word types: P(w|c) = � P(w|z)P(z|c) (1) zEZ The set of latent variables Z is typically much smaller than the vocabulary</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth E Harper</author>
</authors>
<title>Measurement of similarity between nouns.</title>
<date>1965</date>
<booktitle>In Proceedings of the 1965 International Conference on Computational Linguistics (COLING65),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="1045" citStr="Harper, 1965" startWordPosition="151" endWordPosition="152">resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes. 1 Introduction Distributional models of lexical semantics, which assume that aspects of a word’s meaning can be related to the contexts in which that word is typically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms of distribution</context>
</contexts>
<marker>Harper, 1965</marker>
<rawString>Kenneth E. Harper. 1965. Measurement of similarity between nouns. In Proceedings of the 1965 International Conference on Computational Linguistics (COLING65), New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="8231" citStr="Hinton, 2002" startWordPosition="1303" endWordPosition="1304">lity P(n|o, c) is the probability that n would be generated given the latent variable distribution associated with seeing o in context c; this latter distribution P(z|o, c) can be derived using Bayes’ rule and the assumption P(o|z, c) = P(o|z). 1048 Given a set of contexts C in which an instance o appears (e.g., it may be both the subject of a verb and modified by an adjective), (2) and (3) become: XPC-+T (n|o, C) = P(n|z)P(z|o, C) (4) z P(z|o, C)P(o|z)P(z|( |(5) Pz, P(o|z&apos;)Pz&apos;C) P zC QcEC P(z|c) ( |) = P QcEC P(z&apos;|c) (6) z, Equation (6) can be viewed as defining a “product of experts” model (Hinton, 2002). Dinu and Lapata (2010) also use a similar formulation to (5), except that P(z|o, C) is factorised over P(z|o, C) rather than just P(z|C): YPDL10(z|o, C) = cEC Pz, P(o|z&apos;)P(z&apos;|c) (7) P(o|z)P(z|c) In Section 5 below, we find that using (5) rather than (7) gives better results. The model described above (henceforth C -+ T) models the dependence of a target word on its context. An alternative perspective is to model the dependence of a set of contexts on a target word, i.e., we induce a model P (c|w) = X P(c|z)P(z|w) (8) z Making certain assumptions, a formula for P(n|o, c) can be derived from (</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3600" citStr="Kintsch (2001)" startWordPosition="546" endWordPosition="547"> of value in a wide range of application settings. 2 Related work The literature on distributional semantics is vast; in this section we focus on outlining the research that is most directly related to capturing effects of context and compositionality.1 Mitchell and Lapata (2008) 1The interested reader is referred to Pad´o and Lapata (2007) and Turney and Pantel (2010) for a general overview. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1047–1057, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics follow Kintsch (2001) in observing that most distributional approaches to meaning at the phrase or sentence level assume that the contribution of syntactic structure can be ignored and the meaning of a phrase is simply the commutative sum of the meanings of its constituent words. As Mitchell and Lapata argue, this assumption clearly leads to an impoverished model of semantics. Mitchell and Lapata investigate a number of simple methods for combining distributional word vectors, concluding that pointwise multiplication best corresponds to the effects of syntactic interaction. Erk and Pad´o (2008) introduce the conce</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
</authors>
<title>Property of average precision and its generalisation: An examination of evaluation indicator for information retrieval experiments.</title>
<date>2005</date>
<tech>Technical Report NII-2005-014E,</tech>
<institution>National Institute of Informatics,</institution>
<location>Tokyo, Japan.</location>
<contexts>
<context position="21160" citStr="Kishida, 2005" startWordPosition="3555" endWordPosition="3556">nd Pad´o (2008) use only a subset of the data where the target is a noun headed by a verb or a verb heading a noun. Thater et al. 4Thater et al. (2010) and Dinu and Lapata (2010) similarly remove multiword paraphrases (Georgiana Dinu, p.c.). (2010) discard sentences which their parser cannot parse and paraphrases absent from their training corpus and then optimise the parameters of their model through four-fold cross-validation. Here we aim for complete coverage on the dataset and do not perform any parameter tuning. We use two measures to evaluate performance: Generalised Averaged Precision (Kishida, 2005) and Kendall’s τb rank correlation coefficient, which were used for this task by Thater et al. (2010) and Dinu and Lapata (2010), respectively. Generalised Averaged Precision (GAP) is a precision-like measure for evaluating ranked predictions against a gold standard. τb is a variant of Kendall’s τ that is appropriate for data containing tied ranks. We do not use the “precision out of ten” 1052 COORDINATION: n:and:n PREPOSITIONS: n:prep in:n V the hat ⇒ The cat in the hat i:dobj:n Table 3: Dependency graph preprocessing measure that was used in the original Lexical Substitution Task; this measu</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>Kazuaki Kishida. 2005. Property of average precision and its generalisation: An examination of evaluation indicator for information retrieval experiments. Technical Report NII-2005-014E, National Institute of Informatics, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="5504" citStr="Landauer and Dumais, 1997" startWordPosition="841" endWordPosition="844">sor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be v</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="6267" citStr="Li et al., 2010" startWordPosition="959" endWordPosition="962">Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 Models 3.1 Latent variable context models In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work (Dinu and Lapata, 2010; O´ S´eaghdha, 2010; Ritter et al., 2010) in assuming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabu</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="2793" citStr="McCarthy and Navigli, 2009" startWordPosition="427" endWordPosition="430">patterns of syntactic interaction, building on the selectional preference models of O´ S´eaghdha (2010) and Ritter et al. (2010) and the lexical substitution models of Dinu and Lapata (2010). We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity. Our models attain state-of-the-art performance on two evaluation datasets: a set of sentence similarity judgements collected by Mitchell and Lapata (2008) and the dataset of the English Lexical Substitution Task (McCarthy and Navigli, 2009). In view of the well-established effectiveness of dependency-based distributional semantics and of probabilistic frameworks for semantic inference, we expect that our approach will prove to be of value in a wide range of application settings. 2 Related work The literature on distributional semantics is vast; in this section we focus on outlining the research that is most directly related to capturing effects of context and compositionality.1 Mitchell and Lapata (2008) 1The interested reader is referred to Pad´o and Lapata (2007) and Turney and Pantel (2010) for a general overview. Proceedings</context>
<context position="18306" citStr="McCarthy and Navigli, 2009" startWordPosition="3043" endWordPosition="3047"> increased, the average performance over all combinations of n predictors chosen from the set of five T → C and five C → T runs is observed to increase monotonically. Figure 1 also shows that the model selection procedure is very effective at selecting the optimal combination of models; development set performance is a reliable indicator of test set performance. 5 Experiment 2: Lexical substitution 5.1 Data The English Lexical Substitution task, run as part of the SemEval-1 competition, required participants to propose good substitutes for a set of target words in various sentential contexts (McCarthy and Navigli, 2009). Table 2 shows two example sentences and the substitutes appearing in the gold standard, ranked by the number of human annotators who proposed each substitute. The dataset contains a total of 2,010 annotated sentences with 205 distinct target words across four parts of speech (noun, verb, adjective, adverb). In line with previous work on contextual disambiguation, we focus here on the subtask of ranking attested substitutes rather than proposing them from an unrestricted vocabulary. To this end, a candidate set is constructed for each target word from all the substitutes proposed for that wor</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="1500" citStr="Mitchell and Lapata, 2008" startWordPosition="221" endWordPosition="224">word’s meaning can be related to the contexts in which that word is typically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms of distributional semantics, leading to new kinds of questions such as “how similar are the usages of the words body and corpse in the 1047 phrase the body/corpse deliberated the motion... ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in</context>
<context position="3266" citStr="Mitchell and Lapata (2008)" startWordPosition="497" endWordPosition="500">tence similarity judgements collected by Mitchell and Lapata (2008) and the dataset of the English Lexical Substitution Task (McCarthy and Navigli, 2009). In view of the well-established effectiveness of dependency-based distributional semantics and of probabilistic frameworks for semantic inference, we expect that our approach will prove to be of value in a wide range of application settings. 2 Related work The literature on distributional semantics is vast; in this section we focus on outlining the research that is most directly related to capturing effects of context and compositionality.1 Mitchell and Lapata (2008) 1The interested reader is referred to Pad´o and Lapata (2007) and Turney and Pantel (2010) for a general overview. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1047–1057, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics follow Kintsch (2001) in observing that most distributional approaches to meaning at the phrase or sentence level assume that the contribution of syntactic structure can be ignored and the meaning of a phrase is simply the commutative sum of the meanings of its constituent words. As Mit</context>
<context position="13486" citStr="Mitchell and Lapata (2008)" startWordPosition="2203" endWordPosition="2206">this paper we also investigate dependencybased context sets derived from syntactic structure. Given a sentence such as 2We use the estimation methods provided by the MALLET toolkit, available from http://mallet.cs.umass. edu/. v:ncsubj:n V The:d executive:j body:n decided:v . . . n:ncmod:j the set C of dependency contexts for the noun body is {executive:j:ncmod−1:n, decide:v:ncsubj:n}, where ncmod−1 denotes that body stands in an inverse non-clausal modifier relation to executive (we assume that nouns are the heads of their adjectival modifiers). 4 Experiment 1: Similarity in context 4.1 Data Mitchell and Lapata (2008) collected human judgements of semantic similarity for pairs of short sentences, where the sentences in a pair share the same subject but different verbs. For example, the sales slumped and the sales declined should be judged as very similar while the shoulders slumped and the shoulders declined should be judged as less similar. The resulting dataset (henceforth ML08) consists of 120 such pairs using 15 verbs, balanced across high and low expected similarity. 60 subjects rated the data using a scale of 1–7; Mitchell and Lapata calculate average interannotator correlation to be 0.40 (using Spea</context>
<context position="14751" citStr="Mitchell and Lapata (2008)" startWordPosition="2416" endWordPosition="2419"> Erk and Pad´o (2008) split the data into a development portion and a test portion, the development portion consisting of the judgements of six annotators; in order to compare our results with previous research we use the same data split. To evaluate performance, the predictions made by a model are compared to the judgements of each annotator in turn (using ρ) and the resulting per-annotator ρ values are averaged. 4.2 Models All models were trained on the written section of the British National Corpus (around 90 million words), parsed with RASP (Briscoe et al., 2006). The BNC was also used by Mitchell and Lapata (2008) and Erk and Pad´o (2008); as the ML08 dataset was compiled using words appearing more than 50 times in the BNC, there are no coverage problems caused by data sparsity. We trained LDA models for the grammatical relations v:ncsubj:n and n:ncsubj−1:v 1050 Model PARA SIM C → T 0.24 0.34 No optimisation T → C 0.36 0.39 T ↔ C 0.33 0.39 C → T 0.24 0.35 Optimised on dev T → C 0.41 0.41 T ↔ C 0.37 0.41 Erk and Pad´o (2008) Mult 0.24 SVS 0.27 Table 1: Performance (average p) on the ML08 test set and used these to create predictors of type C → T and T → C, respectively. For each predictor, we trained fi</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander A Morgan</author>
<author>Zhiyong Lu</author>
<author>Xinglong Wang</author>
<author>Aaron M Cohen</author>
<author>Juliane Fluck</author>
<author>Patrick Ruch</author>
<author>Anna Divoli</author>
<author>Katrin Fundel</author>
<author>Robert Leaman</author>
</authors>
<title>J¨org Hakenberg, Chengjie Sun, Heng hui Liu, Rafael Torres,</title>
<date>2008</date>
<institution>Michael Krauthammer, William W Lau, Hongfang Liu, Chun-Nan Hsu, Martijn</institution>
<marker>Morgan, Lu, Wang, Cohen, Fluck, Ruch, Divoli, Fundel, Leaman, 2008</marker>
<rawString>Alexander A. Morgan, Zhiyong Lu, Xinglong Wang, Aaron M Cohen, Juliane Fluck, Patrick Ruch, Anna Divoli, Katrin Fundel, Robert Leaman, J¨org Hakenberg, Chengjie Sun, Heng hui Liu, Rafael Torres, Michael Krauthammer, William W Lau, Hongfang Liu, Chun-Nan Hsu, Martijn Schuemie, K. Bretonnel Cohen, and Lynette Hirschman. 2008. Overview of BioCreative II gene normalization. Genome Biology, 9(Suppl 2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Nulty</author>
<author>Fintan Costello</author>
</authors>
<title>UCD-PN: Selecting general paraphrases using conditional probability.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="22982" citStr="Nulty and Costello, 2010" startWordPosition="3840" endWordPosition="3843">esults; in particular, it tended to rank common words highly in most contexts.6 As before we compiled training data by extracting target-context cooccurrences from a text corpus. In addition to the parsed BNC described above we used a corpus of Wikipedia text consisting of over 45 million sentences (almost 1 billion words) parsed using the fast Combinatory Categorial Grammar (CCG) parser described by Clark et al. (2009). The depen5We use the software package available at http://www. nlpado.de/˜sebastian/sigf.html. 6Favouring more general words may indeed make sense in some paraphrasing tasks (Nulty and Costello, 2010). dency representation produced by this parser is interoperable with the RASP dependency format. In order to focus our models on semantically discriminative information and make inference more tractable we ignored all parts of speech other than nouns, verbs, adjectives, prepositions and adverbs. Stopwords and words of fewer than three characters were removed. We also removed the very frequent but semantically weak lemmas be and have. We compare two classes of context models: models learned from window-based contexts and models learned from syntactic dependency contexts. For the syntactic model</context>
</contexts>
<marker>Nulty, Costello, 2010</marker>
<rawString>Paul Nulty and Fintan Costello. 2010. UCD-PN: Selecting general paraphrases using conditional probability. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval-2), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<location>Uppsala,</location>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="5308" citStr="Pereira et al. (1993)" startWordPosition="813" endWordPosition="816">this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the con</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>A mixture model with sharing for lexical semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-10),</booktitle>
<location>Cambridge,MA.</location>
<contexts>
<context position="5840" citStr="Reisinger and Mooney (2010)" startWordPosition="892" endWordPosition="895">viour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 Models 3.1 Latent variable context models </context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond Mooney. 2010. A mixture model with sharing for lexical semantics. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-10), Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent Dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL10),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2294" citStr="Ritter et al. (2010)" startWordPosition="356" endWordPosition="359">kinds of questions such as “how similar are the usages of the words body and corpse in the 1047 phrase the body/corpse deliberated the motion... ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models of O´ S´eaghdha (2010) and Ritter et al. (2010) and the lexical substitution models of Dinu and Lapata (2010). We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity. Our models attain state-of-the-art performance on two evaluation datasets: a set of sentence similarity judgements collected by Mitchell and Lapata (2008) and the dataset of the English Lexical Substitution Task (McCarthy and Navigli, 2009). In view of the well-established effectiveness of dependency-based distributional semantics and of p</context>
<context position="5565" citStr="Ritter et al. (2010)" startWordPosition="851" endWordPosition="854">and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also b</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent Dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="5332" citStr="Rooth et al. (1999)" startWordPosition="818" endWordPosition="821">tuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of po</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="4936" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="759" endWordPosition="762">s corresponding to different syntactic dependencies. Thater et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<pages>2--159</pages>
<contexts>
<context position="4906" citStr="Smolensky, 1990" startWordPosition="757" endWordPosition="758">preference vectors corresponding to different syntactic dependencies. Thater et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). </context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1– 2):159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>Synonymy and Semantic Classification.</title>
<date>1964</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge. Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.</institution>
<contexts>
<context position="1030" citStr="Jones, 1964" startWordPosition="149" endWordPosition="150">ilarity. The resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes. 1 Introduction Distributional models of lexical semantics, which assume that aspects of a word’s meaning can be related to the contexts in which that word is typically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms </context>
</contexts>
<marker>Jones, 1964</marker>
<rawString>Karen Sp¨arck Jones. 1964. Synonymy and Semantic Classification. Ph.D. thesis, University of Cambridge. Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.</rawString>
</citation>
<citation valid="true">
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2269" citStr="(2010)" startWordPosition="354" endWordPosition="354">ing to new kinds of questions such as “how similar are the usages of the words body and corpse in the 1047 phrase the body/corpse deliberated the motion... ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models of O´ S´eaghdha (2010) and Ritter et al. (2010) and the lexical substitution models of Dinu and Lapata (2010). We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity. Our models attain state-of-the-art performance on two evaluation datasets: a set of sentence similarity judgements collected by Mitchell and Lapata (2008) and the dataset of the English Lexical Substitution Task (McCarthy and Navigli, 2009). In view of the well-established effectiveness of dependency-based distribu</context>
<context position="4381" citStr="(2010)" startWordPosition="674" endWordPosition="674"> of a phrase is simply the commutative sum of the meanings of its constituent words. As Mitchell and Lapata argue, this assumption clearly leads to an impoverished model of semantics. Mitchell and Lapata investigate a number of simple methods for combining distributional word vectors, concluding that pointwise multiplication best corresponds to the effects of syntactic interaction. Erk and Pad´o (2008) introduce the concept of a structured vector space in which each word is associated with a set of selectional preference vectors corresponding to different syntactic dependencies. Thater et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette e</context>
<context position="5840" citStr="(2010)" startWordPosition="895" endWordPosition="895"> verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer and Dumais, 1997). More recently, O´ S´eaghdha (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 Models 3.1 Latent variable context models </context>
<context position="8255" citStr="(2010)" startWordPosition="1308" endWordPosition="1308">ty that n would be generated given the latent variable distribution associated with seeing o in context c; this latter distribution P(z|o, c) can be derived using Bayes’ rule and the assumption P(o|z, c) = P(o|z). 1048 Given a set of contexts C in which an instance o appears (e.g., it may be both the subject of a verb and modified by an adjective), (2) and (3) become: XPC-+T (n|o, C) = P(n|z)P(z|o, C) (4) z P(z|o, C)P(o|z)P(z|( |(5) Pz, P(o|z&apos;)Pz&apos;C) P zC QcEC P(z|c) ( |) = P QcEC P(z&apos;|c) (6) z, Equation (6) can be viewed as defining a “product of experts” model (Hinton, 2002). Dinu and Lapata (2010) also use a similar formulation to (5), except that P(z|o, C) is factorised over P(z|o, C) rather than just P(z|C): YPDL10(z|o, C) = cEC Pz, P(o|z&apos;)P(z&apos;|c) (7) P(o|z)P(z|c) In Section 5 below, we find that using (5) rather than (7) gives better results. The model described above (henceforth C -+ T) models the dependence of a target word on its context. An alternative perspective is to model the dependence of a set of contexts on a target word, i.e., we induce a model P (c|w) = X P(c|z)P(z|w) (8) z Making certain assumptions, a formula for P(n|o, c) can be derived from (8): PT-+C(n |o, c) = P(c</context>
<context position="10006" citStr="(2010)" startWordPosition="1629" endWordPosition="1629">(c|z) (14) z cEC P (C|o) = X P(z|o) Y P(c|z) (15) z cEC P(z|o,C) = P(z|o) )C )|) (16) P , P(z&apos;OPCO With appropriate priors chosen for the distributions over words and latent variables, P(n|o, C) is a fully generative model of lexical substitution. A non-generative alternative is one that estimates the similarity of the latent variable distributions associated with seeing n and o in context C. The principle that similarity between topic distributions corresponds to semantic similarity is well-known in document modelling and was proposed in the context of lexical substitution by Dinu and Lapata (2010). In terms of the equations presented above, we could compare the distributions P(z|o, C) with P(z|n, C) using equations (5) or (16). However, Thater et al. (2010) and Dinu and Lapata (2010) both observe that contextualising both o and n can degrade performance; in view of this we actually compare P(z|o, C) with P(z|n) and make the further simplifying assumption that P(z|n) a P(n|z). The similarity measure we adopt is the Bhattacharyya coefficient, which is a natural measure of similarity between probability distributions and is closely related to the Hellinger distance used in previous work o</context>
<context position="12689" citStr="(2010)" startWordPosition="2084" endWordPosition="2084">he frequency of hyperparameter estimation (50 iterations). 3.2 Context types We have not yet defined what the contexts c look like. In vector space models of semantics it is common to distinguish between window-based and dependency-based models (Pad´o and Lapata, 2007); one can make the same distinction for probabilistic context models. A broad generalisation is that window-based models capture semantic association (e.g. referee is associated with football), while dependency models capture a finer-grained notion of similarity (referee is similar to umpire but not to football). Dinu and Lapata (2010) propose a window-based model of lexical substitution; the set of contexts in which a word appears is the set of surrounding words within a prespecified “window size”. In this paper we also investigate dependencybased context sets derived from syntactic structure. Given a sentence such as 2We use the estimation methods provided by the MALLET toolkit, available from http://mallet.cs.umass. edu/. v:ncsubj:n V The:d executive:j body:n decided:v . . . n:ncmod:j the set C of dependency contexts for the noun body is {executive:j:ncmod−1:n, decide:v:ncsubj:n}, where ncmod−1 denotes that body stands i</context>
<context position="20697" citStr="(2010)" startWordPosition="3488" endWordPosition="3488">.3 P 0.2 0.1 current models of distributional semantics) do not represent multiword expressions, we remove such paraphrases and discard the 17 sentences which have only multiword substitutes in the gold standard.4 There are also 7 sentences for which the gold standard contains no substitutes. This leaves a total of 1986 sentences. These sentences were lemmatised and parsed with RASP. Previous authors have partitioned the dataset in various ways. Erk and Pad´o (2008) use only a subset of the data where the target is a noun headed by a verb or a verb heading a noun. Thater et al. 4Thater et al. (2010) and Dinu and Lapata (2010) similarly remove multiword paraphrases (Georgiana Dinu, p.c.). (2010) discard sentences which their parser cannot parse and paraphrases absent from their training corpus and then optimise the parameters of their model through four-fold cross-validation. Here we aim for complete coverage on the dataset and do not perform any parameter tuning. We use two measures to evaluate performance: Generalised Averaged Precision (Kishida, 2005) and Kendall’s τb rank correlation coefficient, which were used for this task by Thater et al. (2010) and Dinu and Lapata (2010), respect</context>
<context position="23897" citStr="(2010)" startWordPosition="3987" endWordPosition="3987">d words of fewer than three characters were removed. We also removed the very frequent but semantically weak lemmas be and have. We compare two classes of context models: models learned from window-based contexts and models learned from syntactic dependency contexts. For the syntactic models we extracted all dependencies and inverse dependencies between lemmas of the aforementioned POS types; in order to maximise the extraction yield, the dependency graph for each sentence was preprocessed using the transformations shown in Table 3. For the window-based context model we follow Dinu and Lapata (2010) in treating each word within five words of a target as a member of its context set. It proved necessary to subsample the corpora in order to make LDA training tractable, especially for the window-based model where the training set of context-target counts is extremely dense (each instance of a word in the corpus contributes up to 10 context instances). For the window-based data, we divided each context-target count by a factor of 5 and a factor of 70 for the BNC and Wikipedia corpora respectively, rounding fractional counts to the closest integer. The choice of 70 for scaling Wikipedia counts</context>
<context position="28268" citStr="(2010)" startWordPosition="4768" endWordPosition="4768">ormance with perfect coverage. All combinations outperform the corresponding W5 results to a statistically significant degree (p &lt; 0.01). Performance at full coverage is already very good (GAP= 48.6, rb = 0.21) on the BNC corpus, but the best results are attained by W5 + T ↔ C trained on the combined corpus (GAP= 49.5, rb = 0.23). The results for the W5 model trained on BNC data is comparable to that trained on the combined corpus; however the syntactic models show a clear benefit from the less sparse dependency data in the combined training corpus. As remarked in Section 3.1, Dinu and Lapata (2010) use a slightly different formulation of P(z|C, o). Using the window-based context model our formulation (5) outperforms (7) for both training corpora; the Dinu and Lapata (2010) version scores GAP = 41.5, rb = 0.15 for the BNC corpus and GAP = 42.0, rb = 0.15 for the combined corpus. The advantage of our formulation is statistically significant for all evaluation measures. 1054 Nouns Verbs Adjectives Adverbs Overall GAP τb GAP τb GAP τb GAP τb GAP τb W5 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17 W5 + T ↔ C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23 Thater et al. (2010) (Model 1)</context>
<context position="29529" citStr="(2010)" startWordPosition="5014" endWordPosition="5014">) (Model 2) 42.5 – – – 43.2 – 51.4 – – – Dinu and Lapata (2010) (LDA) – 0.16 – 0.14 – 0.17 – 0.21 – 0.16 Dinu and Lapata (2010) (NMF) – 0.15 – 0.14 – 0.16 – 0.26 – 0.16 Table 6: Performance by part of speech Table 6 gives a breakdown of performance by target part of speech for the BNC+Wikipedia-trained W5 and W5 + T ↔ C models, as well as figures provided by previous researchers.7 W5 + T ↔ C outperforms W5 on all parts of speech using both evaluation metrics. As remarked above, previous researchers have used the corpus in slightly different ways; we believe that the results of Dinu and Lapata (2010) are fully comparable, while those of Thater et al. (2010) were attained on a slightly smaller dataset with parameters set through cross-validation. The results for W5 + T ↔ C outperform all of Dinu and Lapata’s per-POS and overall results except for a slightly superior score on adverbs attained by their NMF model (τb = 0.26 compared to 0.24). Turning to Thater et al., we report higher scores for every POS with the exception of the verbs where their Model 1 achieves 45.9 GAP compared to 45.1; the overall average for W5 + T ↔ C is substantially higher at 49.5 compared to 44.6. On balance, we su</context>
</contexts>
<marker>2010</marker>
<rawString>2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<pages>188</pages>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS-09,</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="11740" citStr="Wallach et al. (2009)" startWordPosition="1934" endWordPosition="1937">ates for distributions over words and latent variables are derived from the assignment 1049 of topics over the training corpus in a single sampling state. For example, to model P(w|c) we calculate: P(w|z) = fzw + β (18) fz· + Nβ P(z|c) = fzc + αz (19) f·c + Ez, αz, where fzw is the number of words of type w assigned topic z, fzc is the number of times z is associated with context c, fz· and f·c are the marginal topic and context counts respectively, N is the number of word types and α and β parameterise the Dirichlet prior distributions over P(z|c) and P(w|z). Following the recommendations of Wallach et al. (2009) we use asymmetric α and symmetric β; rather than using fixed values for these hyperparameters we estimate them from data in the course of LDA training using an EM-like method.2 We use standard settings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations). 3.2 Context types We have not yet defined what the contexts c look like. In vector space models of semantics it is common to distinguish between window-based and dependency-based models (Pad´o and L</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. In Proceedings of NIPS-09, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-09),</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="11098" citStr="Yao et al. (2009)" startWordPosition="1812" endWordPosition="1815">sure of similarity between probability distributions and is closely related to the Hellinger distance used in previous work on topic modelling (Blei and Lafferty, 2007): Xsimbhatt(Px(z), Py(z)) = qPx(z)Py(z) (17) z This measure takes values between 0 and 1. In this paper we train LDA models of P(w|c) and P(c|w). In the former case, the analogy to document modelling is that each context type plays the role of a “document” consisting of all the words observed in that context in a corpus; for P(c|w) the roles are reversed. The models are trained by Gibbs sampling using the efficient procedure of Yao et al. (2009). The empirical estimates for distributions over words and latent variables are derived from the assignment 1049 of topics over the training corpus in a single sampling state. For example, to model P(w|c) we calculate: P(w|z) = fzw + β (18) fz· + Nβ P(z|c) = fzc + αz (19) f·c + Ez, αz, where fzw is the number of words of type w assigned topic z, fzc is the number of times z is associated with context c, fz· and f·c are the marginal topic and context counts respectively, N is the number of word types and α and β parameterise the Dirichlet prior distributions over P(z|c) and P(w|z). Following th</context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-09), Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics (COLING-00),</booktitle>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="22130" citStr="Yeh, 2000" startWordPosition="3712" endWordPosition="3713"> use the “precision out of ten” 1052 COORDINATION: n:and:n PREPOSITIONS: n:prep in:n V the hat ⇒ The cat in the hat i:dobj:n Table 3: Dependency graph preprocessing measure that was used in the original Lexical Substitution Task; this measure assigns credit for the proportion of the first 10 proposed paraphrases that are present in the gold standard and in the context of ranking attested substitutes it is unclear how to obtain non-trivial results for target words with 10 or fewer possible substitutes. We calculate statistical significance of performance differences using stratified shuffling (Yeh, 2000).5 5.2 Models We apply the models developed in Section 3.1 to the Lexical Substitution Task dataset using dependencyand window-based context information. Here we only use the SIM predictor type. PARA did not give satisfactory results; in particular, it tended to rank common words highly in most contexts.6 As before we compiled training data by extracting target-context cooccurrences from a text corpus. In addition to the parsed BNC described above we used a corpus of Wikipedia text consisting of over 45 million sentences (almost 1 billion words) parsed using the fast Combinatory Categorial Gra</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th Conference on Computational Linguistics (COLING-00), Saarbr¨ucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>