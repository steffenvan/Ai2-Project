<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.85121">
Community Answer Summarization for Multi-Sentence Question with
Group Ll Regularization
</title>
<author confidence="0.970463">
Wen Chan†, Xiangdong Zhou†, Wei Wang†, Tat-Seng Chua$
</author>
<affiliation confidence="0.983249">
†School of Computer Science, Fudan University, Shanghai, 200433, China
</affiliation>
<email confidence="0.912697">
{11110240007,xdzhou,weiwang1}@fudan.edu.cn
</email>
<affiliation confidence="0.897941">
$School of Computing, National University of Singapore
</affiliation>
<email confidence="0.962634">
chuats@nus.edu.sg
</email>
<sectionHeader confidence="0.983155" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851115384615">
We present a novel answer summarization
method for community Question Answering
services (cQAs) to address the problem of “in-
complete answer”, i.e., the “best answer” of a
complex multi-sentence question misses valu-
able information that is contained in other an-
swers. In order to automatically generate a
novel and non-redundant community answer
summary, we segment the complex original
multi-sentence question into several sub ques-
tions and then propose a general Conditional
Random Field (CRF) based answer summary
method with group L, regularization. Vari-
ous textual and non-textual QA features are
explored. Specifically, we explore four differ-
ent types of contextual factors, namely, the in-
formation novelty and non-redundancy mod-
eling for local and non-local sentence inter-
actions under question segmentation. To fur-
ther unleash the potential of the abundant cQA
features, we introduce the group L, regu-
larization for feature learning. Experimental
results on a Yahoo! Answers dataset show
that our proposed method significantly outper-
forms state-of-the-art methods on cQA sum-
marization task.
</bodyText>
<sectionHeader confidence="0.992339" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999773205128205">
Community Question and Answering services
(cQAs) have become valuable resources for users
to pose questions of their interests and share their
knowledge by providing answers to questions. They
perform much better than the traditional frequently
asked questions (FAQ) systems (Jijkoun and Rijke
, 2005; Riezler et al., 2007) which are just based
on natural language processing and information re-
trieving technologies due to the need for human in-
telligence in user generated contents(Gyongyi et al.,
2007). In cQAs such as Yahoo! Answers, a resolved
question often gets more than one answers and a
“best answer” will be chosen by the asker or voted
by other community participants. This {question,
best answer} pair is then stored and indexed for fur-
ther uses such as question retrieval. It performs very
well in simple factoid QA settings, where the an-
swers to factoid questions often relate to a single
named entity like a person, time or location. How-
ever, when it comes to the more sophisticated multi-
sentence questions, it would suffer from the problem
of “incomplete answer”. That is, such question often
comprises several sub questions in specific contexts
and the asker wishes to get elaborated answers for as
many aspects of the question as possible. In which
case, the single best answer that covers just one or
few aspects may not be a good choice (Liu et al.,
2008; Takechi et al., 2007). Since “everyone knows
something” (Adamic et al., 2008), the use of a single
best answer often misses valuable human generated
information contained in other answers.
In an early literature, Liu et al.(2008) reported that
no more than 48% of the 400 best answers were in-
deed the unique best answers in 4 most popular Ya-
hoo! Answers categories. Table 1 shows an example
of the “incomplete answer” problem from Yahoo!
Answers&apos;. The asker wishes to know why his teeth
bloods and how to prevent it. However, the best an-
swer only gives information on the reason of teeth
</bodyText>
<note confidence="0.9209356">
lhttp://answers.yahoo.com/question/index?qid=
20100610161858AAmAGrV
582
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 582–591,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.991682333333333">
blooding. It is clear that some valuable information
about the reasons of gums blooding and some solu-
tions are presented in other answers.
</bodyText>
<subsectionHeader confidence="0.912195">
Question
</subsectionHeader>
<bodyText confidence="0.9941252">
Why do teeth bleed at night and how do you prevent/stop it? This
morning I woke up with blood caked between my two front teeth.
This is the third morning in a row that it has happened. I brush and
floss regularly, and I also eat a balanced, healthy diet. Why is this
happening and how do I stop it?
</bodyText>
<subsectionHeader confidence="0.921951">
Best Answer - Chosen by Asker
</subsectionHeader>
<bodyText confidence="0.9861015">
Periodontal disease is a possibility, gingivitis, or some gum infec-
tion. Teeth don’t bleed; gums bleed.
</bodyText>
<subsectionHeader confidence="0.800761">
Other Answers
Vitamin C deficiency!
</subsectionHeader>
<bodyText confidence="0.976580363636364">
Ever heard of a dentist? Not all the problems in life are solved on
the Internet.
You could be brushing or flossing too hard. Try a brush with softer
bristles or brushing/flossing lighter and slower. If this doesn’t solve
your problem, try seeing a dentist or doctor. Gums that bleed could
be a sign of a more serious issue like leukemia, an infection, gum
disease, a blood disorder, or a vitamin deficiency.
wash your mouth with warm water and salt, it will help to strengthen
your gum and teeth, also salt avoid infection. You probably have
weak gums, so just try to follow the advice, it works in many cases
of oral problems.
</bodyText>
<tableCaption confidence="0.70161375">
Table 1: An example of question with incomplete answer
problem from Yahoo! Answers. The “best answer” seems to
miss valuable information and will not be ideal for re-use when
similar question is asked again.
</tableCaption>
<bodyText confidence="0.9998710625">
In general, as noted in (Jurafsky and Martin ,
2009), most interesting questions are not factoid
questions. User’s needs require longer, more infor-
mative answers than a single phrase. In fact, it is
often the case, that a complex multi-sentence ques-
tion could be answered from multiple aspects by dif-
ferent people focusing on different sub questions.
Therefore we address the incomplete answer prob-
lem by developing a novel summarization technique
taking different sub questions and contexts into con-
sideration. Specifically we want to learn a concise
summary from a set of corresponding answers as
supplement or replacement to the “best answer”.
We tackle the answer summary task as a sequen-
tial labeling process under the general Conditional
Random Fields (CRF) framework: every answer
sentence in the question thread is labeled as a sum-
mary sentence or non-summary sentence, and we
concatenate the sentences with summary label to
form the final summarized answer. The contribution
of this paper is two-fold:
First, we present a general CRF based framework
and incorporate four different contextual factors
based on question segmentation to model the local
and non-local semantic sentence interactions to ad-
dress the problem of redundancy and information
novelty. Various textual and non-textual question
answering features are exploited in the work.
Second, we propose a group LI-regularization ap-
proach in the CRF model for automatic optimal fea-
ture learning to unleash the potential of the features
and enhance the performance of answer summariza-
tion.
We conduct experiments on a Yahoo! Answers
dataset. The experimental results show that the
proposed model improve the performance signifi-
cantly(in terms of precision, recall and F1 measures)
as well as the ROUGE-1, ROUGE-2 and ROUGE-L
measures as compared to the state-of-the-art meth-
ods, such as Support Vector Machines (SVM), Lo-
gistic Regression (LR) and Linear CRF (LCRF)
(Shen et al., 2007).
The rest of the paper is arranged as follows: Sec-
tion 2 presents some definitions and a brief review
of related research. In Section 3, we propose the
summarization framework and then in Section 4 and
5 we detail the experimental setups and results re-
spectively. We conclude the paper in Section 6.
</bodyText>
<sectionHeader confidence="0.911915" genericHeader="introduction">
2 Definitions and Related Work
</sectionHeader>
<subsectionHeader confidence="0.93674">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.961895611111111">
In this subsection we define some concepts that
would be helpful to clarify our problems. First we
define a complex multi-sentence question as a ques-
tion with the following properties:
Definition: A complex multi-sentence question
is one that contains multiple sub-questions.
In the cQAs scenario a question often consists of
one or more main question sentences accompany by
some context sentences described by askers. We
treat the original question and context as a whole
single complex multi-sentence question and obtain
the sub questions by question segmentation. We
then define the incomplete answer problem as:
Definition: The incomplete answer problem is
one where the best answer of a complex multi-
sentence question is voted to be below certain star
ratings or the average similarity between the best an-
swer and all the sub questions is below some thresh-
</bodyText>
<page confidence="0.879085">
583
</page>
<bodyText confidence="0.995927">
olds.
We study the issues of similarity threshold and the
minimal number of stars empirically in the experi-
mental section and show that they are useful in iden-
tifying questions with the incomplete answer prob-
lem.
</bodyText>
<subsectionHeader confidence="0.757152">
2.2 Related Work
</subsectionHeader>
<bodyText confidence="0.999825520833334">
There exist several attempts to alleviate the answer
completeness problem in cQA. One of them is to
segment the multi-sentence question into a set of
sub-questions along with their contexts, then se-
quentially retrieve the sub questions one by one,
and return similar questions and their best answers
(Wang et al., 2010). This strategy works well in gen-
eral, however, as the automatic question segmenta-
tion is imperfect and the matched similar questions
are likely to be generated in different contextual sit-
uations, this strategy often could not combine multi-
ple independent best answers of sub questions seam-
lessly and may introduce redundancy in final answer.
On general problem of cQA answer summariza-
tion, Liu et al.(2008) manually classified both ques-
tions and answers into different taxonomies and ap-
plied clustering algorithms for answer summariza-
tion.They utilized textual features for open and opin-
ion type questions. Through exploiting metadata,
Tomasoni and Huang(2010) introduced four char-
acteristics (constraints) of summarized answer and
combined them in an additional model as well as
a multiplicative model. In order to leverage con-
text, Yang et al.(2011) employed a dual wing fac-
tor graph to mutually enhance the performance of
social document summarization with user generated
content like tweets. Wang et al. (2011) learned on-
line discussion structures such as the replying rela-
tionship by using the general CRFs and presented a
detailed description of their feature designs for sites
and edges embedded in discussion thread structures.
However there is no previous work that explores the
complex multi-sentence question segmentation and
its contextual modeling for community answer sum-
marization.
Some other works examined the evaluation of the
quality of features for answers extracted from cQA
services (Jeon et al., 2006; Hong and Davison ,
2009; Shah et al., 2010). In the work of Shah et
al.(2010), a large number of features extracted for
predicting asker-rated quality of answers was evalu-
ated by using a logistic regression model. However,
to the best of our knowledge, there is no work in
evaluating the quality of features for community an-
swer summarization. In our work we model the fea-
ture learning and evaluation problem as a group L1
regularization problem (Schmidt, 2010) on different
feature groups.
</bodyText>
<sectionHeader confidence="0.967335" genericHeader="method">
3 The Summarization Framework
</sectionHeader>
<subsectionHeader confidence="0.998607">
3.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.999994">
We utilize the probabilistic graphical model to solve
the answer summarization task, Figure 1 gives some
illustrations, in which the sites correspond to the
sentences and the edges are utilized to model the
interactions between sentences. Specifically, let x
be the sentence sequence to all answers within a
question thread, and y be the corresponding label se-
quence. Every component yi of y has a binary value,
with +1 for the summary sentence and -1 otherwise.
Then under CRF (Lafferty et al., 2001), the condi-
tional probability of y given x obeys the following
distribution:
where Z(x) is the normalization constant called
partition function, gl denotes the cQA feature func-
tion of site l, fk denotes the function of edge k( mod-
eling the interactions between sentences), µ and A
are respectively the weights of function of sites and
edges, and y1t denotes the components of y related
to site (edge) t.
</bodyText>
<subsectionHeader confidence="0.999779">
3.2 cQA Features and Contextual Modeling
</subsectionHeader>
<bodyText confidence="0.945034555555555">
In this section, we give a detailed description of
the different sentence-level cQA features and the
contextual modeling between sentences used in our
model for answer summarization.
Sentence-level Features
Different from the conventional multi-document
summarization in which only the textual features are
utilized, we also explore a number of non-textual
author related features (Shah et al., 2010) in cQAs.
</bodyText>
<equation confidence="0.964582">
p(y|x) = 1 exp( vE
Z
�
+
eEE,k
Akfk(e, y|e, x)),
µlgl(v, y|v, x)
(1)
584
</equation>
<bodyText confidence="0.970862">
The textual features used are:
</bodyText>
<listItem confidence="0.964698434782609">
1. Sentence Length: The length of the sentence in the
answers with the stop words removed. It seems that a
long sentence may contain more information.
2. Position: The sentence’s position within the answer. If
a sentence is at the beginning or at the end of one answer,
it might be a generation or viewpoint sentence and will
be given higher weight in the summarization task.
3. Answer Length: The length of the answer to which the
sentence belonged, again with the stop words removed.
4. Stopwords Rate: The rate of stop words in the
sentence. If a sentence contains too many stop words, it
is more likely a spam or chitchat sentence rather than an
informative one.
5. Uppercase Rate: The rate of uppercase words.
Uppercase words are often people’s name, address or
other name entities interested by askers.
6. Has Link Whether the sentence contains a hyperlink
or not. The link often points to a more detailed informa-
tion source.
7. Similarity to Question: Semantic similarity to the
question and question context. It imports the semantic
information relevance to the question and question
context.
</listItem>
<bodyText confidence="0.682334">
The non-textual features used include:
</bodyText>
<listItem confidence="0.921920421052631">
8. Best Answer Star: The stars of the best answer
received by the askers or voters.
9. Thumbs Up: The number of thumbs-ups the answer
which contains the sentence receives. Users are often
used to support one answer by giving a thumbs up after
reading some relevant or interesting information for their
intentions.
10. Author Level: The level of stars the author who gives
the answer sentence acquires. The higher the star level,
the more authoritative the asker is.
11. Best Answer Rate: Rate of answers annotated as the
best answer the author who gives the answer sentence
receives.
12. Total Answer Number: The number of total answers
by the author who gives the answer sentence. The
more answers one gives, the more experience he or she
acquires.
13. Total Points: The total points that the author who
gives the answer sentence receives.
</listItem>
<bodyText confidence="0.999968666666667">
The previous literature (Shah et al., 2010) hinted
that some cQA features, such as Sentence Length,
Has Link and Best Answer Star, may be more im-
portant than others. We also expect that some fea-
ture may be redundant when their most related fea-
tures are given, e.g., the Author Level feature is pos-
itively related with the Total Points received by an-
swerers, and Stopwords Rate is of little help when
both Sentence Length (not including stop words) and
Uppercase Rate are given. Therefore, to explore the
optimal combination of these features, we propose
a group L1 regularization term in the general CRF
model (Section 3.3) for feature learning.
All features presented here can be extracted au-
tomatically from the Yahoo! Answers website. We
normalize all these feature values to real numbers
between 0 and 1 by dividing them by the corre-
sponding maximal value of these features. These
sentence-level features can be easily utilized in the
CRF framework. For instance, if the rate of upper-
case words is prominent or the position is close to
the beginning or end of the answer, then the proba-
bility of the label +1 (summary sentence) should be
boosted by assigning it with a large value.
</bodyText>
<subsectionHeader confidence="0.891063">
Contextual Modeling Under Question
Segmentation
</subsectionHeader>
<bodyText confidence="0.998477375">
For cQAs summarization, the semantic interac-
tions between different sentence sites are crucial,
that is, some context co-occurrences should be en-
couraged and others should be penalized for require-
ments of information novelty and non-redundancy
in the generated summary. Here we consider both
local (sentences from the same answer) and global
(sentences from different answers) settings. This
give rise to four contextual factors that we will ex-
plore for modeling the pairwise semantic interac-
tions based on question segmentation. In this paper,
we utilize a simple but effective lightweight ques-
tion segmentation method (Ding et al., 2008; Wang
et al., 2010). It mainly involves the following two
steps:
Step 1. Question sentence detection: every sen-
tence in the original multi-sentence question is clas-
sified into question sentence and non-question (con-
text) sentence. The question mark and 5W1H fea-
tures are applied.
Step 2. Context assignment: every context sen-
tence is assigned to the most relevant question sen-
tence. We compute the semantic similarity(Simpson
and Crowe, 2005) between sentences or sub ques-
</bodyText>
<page confidence="0.687563">
585
</page>
<figureCaption confidence="0.9475575">
Figure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based
models.
</figureCaption>
<equation confidence="0.900199333333333">
tions as:
sim(w1, w2)
|x |+ |y |(2)
</equation>
<bodyText confidence="0.999955166666667">
where M(x, y) denotes synset pairs matched in sen-
tences x and y; and the similarity between the two
synsets w1 and w2 is computed to be inversely pro-
portional to the length of the path in Wordnet.
One answer sentence may related to more than
one sub questions to some extent. Thus, we de-
fine the replied question Qri as the sub question
with the maximal similarity to sentence xi: Qri =
argmaxQjsim(xi, Qj). It is intuitive that different
summary sentences aim at answering different sub
questions. Therefore, we design the following two
contextual factors based on the similarity of replied
questions.
Dissimilar Replied Question Factor: Given two
answer sentences xi , xj and their corresponding
replied questions Qri, Qrj. If the similarity2 of Qri
and Qrj is below some threshold Tlq, it means that
xi and xj will present different viewpoints to answer
different sub questions. In this case, it is likely that
xi and xj are both summary sentences; we ensure
this by setting the contextual factor cf1 with a large
value of exp v, where v is a positive real constant
often assigned to value 1; otherwise we set cf1 to
exp − v for penalization.
</bodyText>
<equation confidence="0.951735666666667">
�
exp v, yi = yj = 1
cf1 =
</equation>
<bodyText confidence="0.986795617647059">
exp − v, otherwise
Similar Replied Question Factor: Given two an-
2We use the semantic similarity of Equation 2 for all our
similarity measurement in this paper.
swer sentences xi , xj and their corresponding
replied questions Qri, Qrj. If the similarity of Qri
and Qrj is above some upper threshold Tuq, this
means that xi and xj are very similar and likely to
provide similar viewpoint to answer similar ques-
tions. In this case, we want to select either xi or
xj as answer. This is done by setting the contextual
factor cf2 such that xi and xj have opposite labels,
�
exp v, yi ∗ yj = −1
exp − v, otherwise
Assuming that sentence xi is selected as a sum-
mary sentence, and its next local neighborhood sen-
tence xi+1 by the same author is dissimilar to it but
it is relevant to the original multi-sentence question,
then it is reasonable to also pick xi+1 as a summary
sentence because it may offer new viewpoints by
the author. Meanwhile, other local and non-local
sentences which are similar to it at above the up-
per threshold will probably not be selected as sum-
mary sentences as they offer similar viewpoint as
discussed above. Therefore, we propose the follow-
ing two kinds of contextual factors for selecting the
answer sentences in the CRF model.
Local Novelty Factor: If the similarity of answer
sentence xi and xi+1 given by the same author is
below a lower threshold Tls, but their respective sim-
ilarities to the sub questions both exceed an upper
threshold Tus, then we will boost the probability of
selecting both as summary sentences by setting:
</bodyText>
<equation confidence="0.8930633">
�
cf3 exp v, yi = yi+1 = 1
=
exp − v, otherwise
Redundance Factor: If the similarity of answer
�
sim(x, y) = 2 ×
(w1,w2)EM(x,y)
cf2 =
586
</equation>
<bodyText confidence="0.9298315">
sentence xi and xj is greater than the upper thresh-
old τus, then they are likely to be redundant and
hence should be given opposite labels. This is done
by setting:
</bodyText>
<equation confidence="0.963719333333333">
{
=
cf4 exp ν, yi ∗ yj = −1
</equation>
<bodyText confidence="0.9913346">
exp − ν, otherwise
Figure 1 gives an illustration of these four con-
textual factors in our proposed general CRF based
model. The parameter estimation and model infer-
ence are discussed in the following subsection.
</bodyText>
<subsectionHeader confidence="0.9917285">
3.3 Group L1 Regularization for Feature
Learning
</subsectionHeader>
<bodyText confidence="0.999997142857143">
In the context of cQA summarization task, some fea-
tures are intuitively to be more important than oth-
ers. As a result, we group the parameters in our CRF
model with their related features3 and introduce a
group L1-regularization term for selecting the most
useful features from the least important ones, where
the regularization term becomes,
</bodyText>
<equation confidence="0.996690333333333">
G
R(θ) = C ∑ ∥−→ θg∥2, (3)
g=1
</equation>
<bodyText confidence="0.999432">
where C controls the penalty magnitude of the pa-→−θg
denotes the parameters corresponding to the partic-
ular group g. Notice that this penalty term is indeed
a L(1, 2) regularization because in every particu-
lar group we normalize the parameters in L2 norm
while the weight of a whole group is summed in L1
form.
Given a set of training data D = (x(i), y(i)), i =
1, ..., N, the parameters θ = (µl, λk) of the general
CRF with the group L1-regularization are estimated
in using a maximum log likelihood function L as:
</bodyText>
<equation confidence="0.991675">
L= ∑N log(pθ(y(i)|x(i))) − C G ∥−→θg∥2, (4)
i=1 ∑
g=1
</equation>
<bodyText confidence="0.931043071428571">
3We note that every sentence-level feature discussed in Sec-
tion 3.2 presents a variety of instances (e.g., the sentence with
longer or shorter length is the different instance), and we may
call it sub-feature of the original sentence-level feature in the
micro view. Every sub-feature has its corresponding weight in
our CRF model. Whereas in a macro view, those related sub-
features can be considered as a group.
where N denotes the total number of training sam-
ples. we compute the log-likelihood gradient com-
ponent of θ in the first term of Equation 4 as in
usual CRFs. However, the second term of Equation
−→
comes exactly zero. To tackle this problem, an ad-
ditional variable is added for each group (Schmidt,
</bodyText>
<equation confidence="0.9648802">
−→ θg∥2 with
the variable αg, subject to the constraint αg ≥
∥−→ θg∥2, i.e.,
log(pθ(y(i)|x(i))) − C
subject to αg ≥ ∥−→ θg∥2, ∀g.
</equation>
<bodyText confidence="0.999515090909091">
This formulation transforms the non-differentiable
regularizer to a simple linear function and maximiz-
ing Equation 5 will lead to a solution to Equation 4
because it is a lower bound of the latter. Then, we
add a sufficient small positive constant ε when com-
puting the L2 norm (Lee et al., 2006), i.e.,  |−→θg∥2 =
√∑|g |j=1 θ2 gj + ε, where |g |denotes the number of
features in group g. To obtain the optimal value of
parameter θ from the training data, we use an effi-
cient L-BFGS solver to solve the problem, and the
first derivative of every feature j in group g is,
</bodyText>
<equation confidence="0.999173571428571">
δL ∑N Cgj(y(i), x(i)) − ∑N ∑
= i=1 i=1 y
δθgj
θgj
p(y|x(i))Cgj(y,x(i)) − 2C
√∑|g|
l=1 θ2gl + ε
</equation>
<bodyText confidence="0.999566111111111">
where Cgj(y, x) denotes the count of feature j in
group g of observation-label pair (x, y). The first
two terms of Equation 6 measure the difference be-
tween the empirical and the model expected values
of feature j in group g, while the third term is the
derivative of group L1 priors.
For inference, the labeling sequence can be ob-
tained by maximizing the probability of y condi-
tioned on x,
</bodyText>
<equation confidence="0.998319">
y* = argmaxypθ(y|x). (7)
</equation>
<bodyText confidence="0.9997735">
We use a modification of the Viterbi algorithm to
perform inference of the CRF with non-local edges
</bodyText>
<figure confidence="0.802672571428572">
rameters, G is the number of feature groups and
4 is non-differentiable when some special ∥
θg∥2 be-
2010); that is, by replacing each norm ∥
∑N
i=1
L=
(5)
αg,
G
∑
g=1
(6)
587
</figure>
<bodyText confidence="0.962632">
previously used in (Galley , 2006). That is , we
replace the edge connection zt = (yt−2, yt−1, yt)
of order-2 Markov model by zt = (yNt, yt−1, yt),
where yNt represents the label at the source of the
non-local edge. Although it is an approximation of
the exact inference, we will see that it works well for
our answer summarization task in the experiments.
</bodyText>
<sectionHeader confidence="0.992654" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.902186">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999992852941176">
To evaluate the performance of our CRF based an-
swer summarization model, we conduct experiments
on the Yahoo! Answers archives dataset. The Ya-
hoo! WebscopeTM Program4 opens up a number of
Yahoo! Answers datasets for interested academics
in different categories. Our original dataset con-
tains 1,300,559 questions and 2,770,896 answers in
ten taxonomies from Yahoo! Answers. After fil-
tering the questions which have less than 5 answers
and some trivial factoid questions using the features
by (Tomasoni and Huang, 2010) , we reduce the
dataset to 55,132 questions. From this sub-set, we
next select the questions with incomplete answers
as defined in Section 2.1. Specifically, we select the
questions where the average similarity between the
best answer and all sub questions is less than 0.6 or
when the star rating of the best answer is less than 4.
We obtain 7,784 questions after this step. To eval-
uate the effectiveness of this method, we randomly
choose 400 questions in the filtered dataset and in-
vite 10 graduate candidate students (not in NLP re-
search field) to verify whether a question suffers
from the incomplete answer problem. We divide the
students into five groups of two each. We consider
the questions as the “incomplete answer questions”
only when they are judged by both members in a
group to be the case. As a result, we find that 360
(90%) of these questions indeed suffer from the in-
complete answer problem, which indicates that our
automatic detection method is efficient. This ran-
domly selected 400 questions along with their 2559
answers are then further manually summarized for
evaluation of automatically generated answer sum-
maries by our model in experiments.
</bodyText>
<footnote confidence="0.764219">
4http://sandbox.yahoo.com/
</footnote>
<subsectionHeader confidence="0.985717">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999944875">
When taking the summarization as a sequential bi-
classification problem, we can make use of the usual
precision, recall and F1 measures (Shen et al., 2007)
for classification accuracy evaluation.
In our experiments, we also compare the preci-
sion, recall and F1 score in the ROUGE-1, ROUGE-
2 and ROUGE-L measures (Lin , 2004) for answer
summarization performance.
</bodyText>
<sectionHeader confidence="0.96694" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.998901">
5.1 Summarization Results
</subsectionHeader>
<bodyText confidence="0.999727457142857">
We adapt the Support Vector Machine (SVM) and
Logistic Regression (LR) which have been reported
to be effective for classification and the Linear CRF
(LCRF) which is used to summarize ordinary text
documents in (Shen et al., 2007) as baselines for
comparison. To better illustrate the effectiveness of
question segmentation based contextual factors and
the group L1 regularization term, we carry the tests
in the following sequence: (a) we use only the con-
textual factors cf3 and cf4 with default L2 regular-
ization (gCRF); (b) we add the reply question based
factors cf1 and cf2 to the model (gCRF-QS); and (c)
we replace default L2 regularization with our pro-
posed group L1 regularization term (gCRF-QS-l1).
For linear CRF system, we use all our textual and
non-textual features as well as the local (exact pre-
vious and next) neighborhood contextual factors in-
stead of the features of (Shen et al., 2007) for fair-
ness. For the thresholds used in the contextual fac-
tors, we enforce Tl4 to be equal to Tls and Tu4 equal
to Tus for the purpose of simplifying the parameters
setting (Tl4 = Tls = 0.4, Tu4 = Tus = 0.8 in our ex-
periments). We randomly divide the dataset into ten
subsets (every subset with 40 questions and the as-
sociated answers), and conduct a ten-fold cross val-
idation and for each round where the nine subsets
are used to train the model and the remaining one
for testing. The precision, recall and F1 measures of
these models are presented in Table 2.
Table 2 shows that our general CRF model based
on question segmentation with group L1 regulariza-
tion out-performs the baselines significantly in all
three measures (gCRF-QS-l1 is 13.99% better than
SVM in precision, 9.77% better in recall and 11.72%
better in F1 score). We note that both SVM and LR,
</bodyText>
<page confidence="0.578959">
588
</page>
<table confidence="0.999676142857143">
Model R1 P R1 R R1 F1 R2 P R2 R R2 F1 RL P RL R RL F1
SVM 79.2% 52.5% 63.1% 71.9% 41.3% 52.4% 67.1% 36.7% 47.4%
LR 75.2%t 57.4%T 65.1%T 66.1%t 48.5%T 56.0%T 61.6%t 43.2%T 50.8%T
LCRF 78.7%- 61.8%T 69.3%- 71.4%- 54.1%T 61.6%T 67.1%- 49.6%T 57.0%T
gCRF 81.9%T 65.2%T 72.6%T 76.8%T 57.3%T 65.7%T 73.9%T 53.5%T 62.1%T
gCRF-QS 81.4%- 70.0%T 75.3%T 76.2%- 62.4%T 68.6%T 73.3%- 58.6%T 65.1%T
gCRF-QS-l1 86.6%T 68.3%- 76.4%T 82.6%T 61.5%- 70.5%T 80.4%T 58.2%- 67.5%T
</table>
<tableCaption confidence="0.974965">
Table 3: The Precision, Recall and F1 of ROUGE-1, ROUGE-2, ROUGE-L in the baselines SVM,LR, LCRF and our
general CRF based models (gCRF, gCRF-QS, gCRF-QS-l1). The down-arrow means performance degradation with
statistical significance.
</tableCaption>
<table confidence="0.999912714285714">
Model Precision Recall F1
SVM 65.93% 61.96% 63.88%
LR 66.92%- 61.31%- 63.99%-
LCRF 69.80% T 63.91%- 66.73%T
gCRF 73.77%T 69.43%T 71.53%T
gCRF-QS 74.78%T 72.51%T 73.63%T
gCRF-QS-l1 79.92%T 71.73%- 75.60%T
</table>
<tableCaption confidence="0.741924714285714">
Table 2: The Precision, Recall and F1 measures of the
baselines SVM,LR, LCRF and our general CRF based
models (gCRF, gCRF-QS, gCRF-QS-l1). The up-arrow
denotes the performance improvement compared to the
precious method (above) with statistical significance un-
der p value of 0.05, the short line ’-’ denotes there is no
difference in statistical significance.
</tableCaption>
<bodyText confidence="0.999986236363637">
which just utilize the independent sentence-level
features, behave not vary well here, and there is no
statistically significant performance difference be-
tween them. We also find that LCRF which utilizes
the local context information between sentences per-
form better than the LR method in precision and F1
with statistical significance. While we consider the
general local and non-local contextual factor cf3 and
cf4 for novelty and non-redundancy constraints, the
gCRF performs much better than LCRF in all three
measures; and we obtain further performance im-
provement by adding the contextual factors based
on QS, especially in the recall measurement. This
is mainly because we have divided the question into
several sub questions, and the system is able to se-
lect more novel sentences than just treating the origi-
nal multi-sentence as a whole. In addition, when we
replace the default L2 regularization by the group
L1 regularization for more efficient feature weight
learning, we obtain a much better performance in
precision while not sacrificing the recall measure-
ment statistically.
We also compute the Precision, Recall and F1
in ROUGE-1, ROUGE-2 and ROUGE-L measure-
ments, which are widely used to measure the quality
of automatic text summarization. The experimental
results are listed in Table 3. All results in the Ta-
ble are the average of the ten-fold cross validation
experiments on our dataset.
It is observed that our gCRF-QS-l1 model im-
proves the performance in terms of precision, recall
and F1 score on all three measurements of ROUGE-
1, ROUGE-2 and ROUGE-L by a significant mar-
gin compared to other baselines due to the use of
local and non-local contextual factors and factors
based on QS with group L1 regularization. Since
the ROUGE measures care more about the recall and
precision of N-grams as well as common substrings
to the reference summary rather than the whole sen-
tence, they offer a better measurement in modeling
the user’s information needs. Therefore, the im-
provements in these measures are more encouraging
than those of the average classification accuracy for
answer summarization.
From the viewpoint of ROUGE measures we ob-
serve that our question segmentation method can en-
hance the recall of the summaries significantly due
to the more fine-grained modeling of sub questions.
We also find that the precision of the group L1 reg-
ularization is much better than that of the default
L2 regularization while not hurting the recall signifi-
cantly. In general, the experimental results show that
our proposed method is more effective than other
baselines in answer summarization for addressing
the incomplete answer problem in cQAs.
</bodyText>
<page confidence="0.752519">
589
</page>
<figureCaption confidence="0.9943225">
Figure 2: The accumulated weight of each site feature
group in the group Ll-regularization to our Yahoo! An-
swer dataset. The horizonal axis corresponds to the name
of each feature group.
</figureCaption>
<subsectionHeader confidence="0.997302">
5.2 Evaluation of Feature Learning
</subsectionHeader>
<bodyText confidence="0.994007166666667">
For group L1 regularization term, we set the e =
10−4 in Equation 6. To see how much the dif-
ferent textual and non-textual features contribute to
community answer summarization, the accumulated
weight of each group of sentence-level features5 is
presented in Figure 2. It shows that the textual fea-
tures such as 1 (Sentence Length), 2 (Position) 3 (An-
swer Length), 6 (Has Link) and non-textual features
such as 8 (Best Answer Star) , 12 (Total Answer
Number) as well as 13 (Total Points) have larger
weights, which play a significant role in the sum-
marization task as we intuitively considered; fea-
tures 4 (Stopwords Rate), 5 (Uppercase Rate) and 9
(Thumbs Up) have medium weights relatively; and
the other features like 7 (Similarity to Question), 10
(Author Level) and 11 (Best Answer Rate) have the
smallest accumulated weights. The main reasons
that the feature 7 (Similarity to Question) has low
contribution is that we have utilized the similarity
to question in the contextual factors, and this simi-
larity feature in the single site becomes redundant.
Similarly, the features Author Level and Best An-
swer Number are likely to be redundant when other
non-textual features(Total Answer Number and To-
tal Points) are presented together. The experimental
results demonstrate that with the use of group L1-
regularization we have learnt better combination of
these features.
5Note that we have already evaluated the contribution of the
contextual factors in Section 5.1.
</bodyText>
<subsectionHeader confidence="0.994211">
5.3 An Example of Summarized Answer
</subsectionHeader>
<bodyText confidence="0.999971461538461">
To demonstrate the effectiveness of our proposed
method, Table 4 shows the generated summary of
the example question which is previously illustrated
in Table 1 in the introduction section. The best an-
swer available in the system and the summarized an-
swer generated by our model are compared in Table
4. It is found that the summarized answer contains
more valuable information about the original multi-
sentence question, as it better answers the reason of
teeth blooding and offers some solution for it. Stor-
ing and indexing this summarized answer in ques-
tion archives should provide a better choice for an-
swer reuse in question retrieval of cQAs.
</bodyText>
<subsectionHeader confidence="0.91991">
Question
</subsectionHeader>
<bodyText confidence="0.983518">
Why do teeth bleed at night and how do you prevent/stop it? This
morning I woke up with blood caked between my two front teeth.[...]
</bodyText>
<subsectionHeader confidence="0.537244">
Best Answer - Chosen by Asker
</subsectionHeader>
<bodyText confidence="0.992263">
Periodontal disease is a possibility, gingivitis, or some gum infec-
tion. Teeth don’t bleed; gums bleed.
</bodyText>
<subsectionHeader confidence="0.895435">
Summarized Answer Generated by Our Method
</subsectionHeader>
<bodyText confidence="0.9986605">
Periodontal disease is a possibility, gingivitis, or some gum infec-
tion. Teeth don’t bleed; gums bleed. Gums that bleed could be a
sign of a more serious issue like leukemia, an infection, gum dis-
ease, a blood disorder, or a vitamin deficiency. wash your mouth
with warm water and salt, it will help to strengthen your gum and
teeth, also salt avoid infection.
</bodyText>
<tableCaption confidence="0.9442315">
Table 4: Summarized answer by our general CRF based model
for the question in Table 1.
</tableCaption>
<sectionHeader confidence="0.997197" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999908125">
We proposed a general CRF based community an-
swer summarization method to deal with the in-
complete answer problem for deep understanding of
complex multi-sentence questions. Our main con-
tributions are that we proposed a systematic way
for modeling semantic contextual interactions be-
tween the answer sentences based on question seg-
mentation and we explored both the textual and non-
textual answer features learned via a group L1 reg-
ularization. We showed that our method is able to
achieve significant improvements in performance of
answer summarization compared to other baselines
and previous methods on Yahoo! Answers dataset.
We planed to extend our proposed model with more
advanced feature learning as well as enriching our
summarized answer with more available Web re-
</bodyText>
<page confidence="0.500261">
590
</page>
<bodyText confidence="0.922732">
sources.
</bodyText>
<sectionHeader confidence="0.992888" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.966554">
This work was supported by the NSFC under Grant
No.61073002 and No.60773077.
</bodyText>
<sectionHeader confidence="0.991943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999019373737374">
L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman.
2008. Knowledge sharing and yahoo answers: every-
one knows something. Proceedings of WWW 2008.
Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan Zhu.
2008. Rouge: Using Conditional Random Fields to
Extract Contexts and Answers of Questions from On-
line Forums. Proceedings of ACL-08: HLT, pages
710–718.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Impor-
tance. Proceedings of EMNLP 2006.
Z. Gyongyi, G. Koutrika, J. Pedersen, and H. Garcia-
Molina. 2007. Questioning yahoo! answers. Tech-
nical report. Stanford InfoLab.
F. Maxwell Harper, Daphne Raban, Sheizaf Rafaeli, and
Joseph A. Konstan. 2008. Predictors of Answer Qual-
ity in Online Q&amp;A Sites. Proceedings of CHI 2008.
Liangjie Hong and Brian D. Davison. 2009. A
Classification-based Approach to Question Answering
in Discussion Boards. Proceedings of the 32th ACM
SIGIR Conference, pages 171–178.
Eduard Hovy, Chin Y. Lin, and Liang Zhou. 2005. A
BE-based Multi-document Summarization with Sen-
tence Compression. Proceedings of Multilingual Sum-
marization Evaluation (ACL 2005 workshop).
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee and Soyeon
Park 2006. A Framework to Predict the Quality of
Answers with NonTextual Features. Proceedings of
the 29th ACM SIGIR Conference, pages 228–235.
V. Jijkoun and M. de Rijke. 2005. Retrieving answers
from frequently asked questions pages on the web. In
CIKM.
Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing: An introduction to natural
language processing, computational linguistics, and
speech recognition. Published by Pearson Education.
John D. Lafferty, Andrew McCallum, and Fernando C.
N. Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. Proceedings of the 18th ICML, pages 282–289.
S. Lee, H. Lee, P. Abbeel, and A. Ng. 2006. Efficient L1
Regularized Logistic Regression. In AAAI.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. Proceedings of ACL Work-
shop, pages 74–81.
Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008.
Predicting Information Seeker Satisfaction in Commu-
nity Question Answering. Proceedings of the 31th
ACM SIGIR Conference.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understanding and
summarizing answers in community-based question
answering services. Proceedings of the 22nd ICCL,
pages 497–504.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. Proceedings of the 45th
Annual Meeting of ACL.
Mark Schmidt. 2010. Graphical Model Structure Learn-
ing with L1-Regularization. Doctoral Thesis.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and Predicting Answer Quality in Community QA.
Proceedings of the 33th ACM SIGIR Conference.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang and Zheng
Chen. 2007. Document Summarization using Condi-
tional Random Fields. Proceedings of the 20th IJCAI.
Troy Simpson and Malcolm Crowe 2005. WordNet.Net
http://opensource.ebswift.com/WordNet.Net
Mineki Takechi, Takenobu Tokunaga, and Yuji Mat-
sumoto. 2007. Chunking-based Question Type Iden-
tification for Multi-Sentence Queries. Proceedings of
SIGIR 2007 Workshop.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-
Aware Measures for Answer Summarization in Com-
munity Question Answering. Proceedings of the 48th
Annual Meeting of ACL, pages 760–769.
Hongning Wang, Chi Wang, ChengXiang Zhai, Jiawei
Han 2011. Learning Online Discussion Structures by
Conditional Random Fields. Proceedings of the 34th
ACM SIGIR Conference.
Kai Wang, Zhao-Yan Ming and Tat-Seng Chua. 2009.
A Syntactic Tree Matching Approach to Finding Simi-
lar Questions in Community-based QA Services. Pro-
ceedings of the 32th ACM SIGIR Conference.
Kai Wang, Zhao-Yan Ming, Xia Hu and Tat-Seng Chua.
2010. Segmentation of Multi-Sentence Questions:
Towards Effective Question Retrieval in cQA Ser-
vices. Proceedings of the 33th ACM SIGIR Confer-
ence, pages 387–394.
X. Xue, J.Jeon, and W.B.Croft. 2008. Retrieval models
for question and answers archives. Proceedings of the
31th ACM SIGIR Conference.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhou Su, and
Juanzi Li. 2011. Social Context Summarization. Pro-
ceedings of the 34th ACM SIGIR Conference.
Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006. Sum-
marizing answers for complicated questions. Proceed-
ings of the 5th International Conference on LREC,
Genoa, Italy.
</reference>
<page confidence="0.922208">
591
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548996">
<title confidence="0.997672">Community Answer Summarization for Multi-Sentence Question</title>
<author confidence="0.982553">Xiangdong Wei Tat-Seng</author>
<address confidence="0.72659">of Computer Science, Fudan University, Shanghai, 200433, of Computing, National University of</address>
<email confidence="0.845676">chuats@nus.edu.sg</email>
<abstract confidence="0.998483333333333">We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of “incomplete answer”, i.e., the “best answer” of a complex multi-sentence question misses valuable information that is contained in other answers. In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary with group Various textual and non-textual QA features are explored. Specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. To further unleash the potential of the abundant cQA we introduce the group regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L A Adamic</author>
<author>J Zhang</author>
<author>E Bakshy</author>
<author>M S Ackerman</author>
</authors>
<title>Knowledge sharing and yahoo answers: everyone knows something.</title>
<date>2008</date>
<booktitle>Proceedings of WWW</booktitle>
<contexts>
<context position="2905" citStr="Adamic et al., 2008" startWordPosition="438" endWordPosition="441">ings, where the answers to factoid questions often relate to a single named entity like a person, time or location. However, when it comes to the more sophisticated multisentence questions, it would suffer from the problem of “incomplete answer”. That is, such question often comprises several sub questions in specific contexts and the asker wishes to get elaborated answers for as many aspects of the question as possible. In which case, the single best answer that covers just one or few aspects may not be a good choice (Liu et al., 2008; Takechi et al., 2007). Since “everyone knows something” (Adamic et al., 2008), the use of a single best answer often misses valuable human generated information contained in other answers. In an early literature, Liu et al.(2008) reported that no more than 48% of the 400 best answers were indeed the unique best answers in 4 most popular Yahoo! Answers categories. Table 1 shows an example of the “incomplete answer” problem from Yahoo! Answers&apos;. The asker wishes to know why his teeth bloods and how to prevent it. However, the best answer only gives information on the reason of teeth lhttp://answers.yahoo.com/question/index?qid= 20100610161858AAmAGrV 582 Proceedings of th</context>
</contexts>
<marker>Adamic, Zhang, Bakshy, Ackerman, 2008</marker>
<rawString>L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. 2008. Knowledge sharing and yahoo answers: everyone knows something. Proceedings of WWW 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shilin Ding</author>
<author>Gao Cong</author>
<author>Chin-Yew Lin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Rouge: Using Conditional Random Fields to Extract Contexts and Answers of Questions from Online Forums.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="16239" citStr="Ding et al., 2008" startWordPosition="2611" endWordPosition="2614">n, the semantic interactions between different sentence sites are crucial, that is, some context co-occurrences should be encouraged and others should be penalized for requirements of information novelty and non-redundancy in the generated summary. Here we consider both local (sentences from the same answer) and global (sentences from different answers) settings. This give rise to four contextual factors that we will explore for modeling the pairwise semantic interactions based on question segmentation. In this paper, we utilize a simple but effective lightweight question segmentation method (Ding et al., 2008; Wang et al., 2010). It mainly involves the following two steps: Step 1. Question sentence detection: every sentence in the original multi-sentence question is classified into question sentence and non-question (context) sentence. The question mark and 5W1H features are applied. Step 2. Context assignment: every context sentence is assigned to the most relevant question sentence. We compute the semantic similarity(Simpson and Crowe, 2005) between sentences or sub ques585 Figure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based models. tio</context>
</contexts>
<marker>Ding, Cong, Lin, Zhu, 2008</marker>
<rawString>Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan Zhu. 2008. Rouge: Using Conditional Random Fields to Extract Contexts and Answers of Questions from Online Forums. Proceedings of ACL-08: HLT, pages 710–718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance.</title>
<date>2006</date>
<booktitle>Proceedings of EMNLP</booktitle>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. Proceedings of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Gyongyi</author>
<author>G Koutrika</author>
<author>J Pedersen</author>
<author>H GarciaMolina</author>
</authors>
<title>Questioning yahoo! answers.</title>
<date>2007</date>
<tech>Technical report. Stanford InfoLab.</tech>
<contexts>
<context position="1955" citStr="Gyongyi et al., 2007" startWordPosition="274" endWordPosition="277">that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task. 1 Introduction Community Question and Answering services (cQAs) have become valuable resources for users to pose questions of their interests and share their knowledge by providing answers to questions. They perform much better than the traditional frequently asked questions (FAQ) systems (Jijkoun and Rijke , 2005; Riezler et al., 2007) which are just based on natural language processing and information retrieving technologies due to the need for human intelligence in user generated contents(Gyongyi et al., 2007). In cQAs such as Yahoo! Answers, a resolved question often gets more than one answers and a “best answer” will be chosen by the asker or voted by other community participants. This {question, best answer} pair is then stored and indexed for further uses such as question retrieval. It performs very well in simple factoid QA settings, where the answers to factoid questions often relate to a single named entity like a person, time or location. However, when it comes to the more sophisticated multisentence questions, it would suffer from the problem of “incomplete answer”. That is, such question </context>
</contexts>
<marker>Gyongyi, Koutrika, Pedersen, GarciaMolina, 2007</marker>
<rawString>Z. Gyongyi, G. Koutrika, J. Pedersen, and H. GarciaMolina. 2007. Questioning yahoo! answers. Technical report. Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Maxwell Harper</author>
<author>Daphne Raban</author>
<author>Sheizaf Rafaeli</author>
<author>Joseph A Konstan</author>
</authors>
<date>2008</date>
<booktitle>Predictors of Answer Quality in Online Q&amp;A Sites. Proceedings of CHI</booktitle>
<marker>Harper, Raban, Rafaeli, Konstan, 2008</marker>
<rawString>F. Maxwell Harper, Daphne Raban, Sheizaf Rafaeli, and Joseph A. Konstan. 2008. Predictors of Answer Quality in Online Q&amp;A Sites. Proceedings of CHI 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Brian D Davison</author>
</authors>
<title>A Classification-based Approach to Question Answering in Discussion Boards.</title>
<date>2009</date>
<booktitle>Proceedings of the 32th ACM SIGIR Conference,</booktitle>
<pages>171--178</pages>
<marker>Hong, Davison, 2009</marker>
<rawString>Liangjie Hong and Brian D. Davison. 2009. A Classification-based Approach to Question Answering in Discussion Boards. Proceedings of the 32th ACM SIGIR Conference, pages 171–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin Y Lin</author>
<author>Liang Zhou</author>
</authors>
<title>A BE-based Multi-document Summarization with Sentence Compression.</title>
<date>2005</date>
<booktitle>Proceedings of Multilingual Summarization Evaluation (ACL</booktitle>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>Eduard Hovy, Chin Y. Lin, and Liang Zhou. 2005. A BE-based Multi-document Summarization with Sentence Compression. Proceedings of Multilingual Summarization Evaluation (ACL 2005 workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Joon Ho Lee and Soyeon Park</title>
<date>2006</date>
<booktitle>Proceedings of the 29th ACM SIGIR Conference,</booktitle>
<pages>228--235</pages>
<marker>Jeon, Croft, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee and Soyeon Park 2006. A Framework to Predict the Quality of Answers with NonTextual Features. Proceedings of the 29th ACM SIGIR Conference, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the web. In CIKM.</title>
<date>2005</date>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>V. Jijkoun and M. de Rijke. 2005. Retrieving answers from frequently asked questions pages on the web. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: An introduction to natural language processing, computational linguistics, and speech recognition. Published by Pearson Education.</title>
<date>2009</date>
<marker>Jurafsky, Martin, 2009</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing: An introduction to natural language processing, computational linguistics, and speech recognition. Published by Pearson Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>Proceedings of the 18th ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="11427" citStr="Lafferty et al., 2001" startWordPosition="1815" endWordPosition="1818">ization problem (Schmidt, 2010) on different feature groups. 3 The Summarization Framework 3.1 Conditional Random Fields We utilize the probabilistic graphical model to solve the answer summarization task, Figure 1 gives some illustrations, in which the sites correspond to the sentences and the edges are utilized to model the interactions between sentences. Specifically, let x be the sentence sequence to all answers within a question thread, and y be the corresponding label sequence. Every component yi of y has a binary value, with +1 for the summary sentence and -1 otherwise. Then under CRF (Lafferty et al., 2001), the conditional probability of y given x obeys the following distribution: where Z(x) is the normalization constant called partition function, gl denotes the cQA feature function of site l, fk denotes the function of edge k( modeling the interactions between sentences), µ and A are respectively the weights of function of sites and edges, and y1t denotes the components of y related to site (edge) t. 3.2 cQA Features and Contextual Modeling In this section, we give a detailed description of the different sentence-level cQA features and the contextual modeling between sentences used in our mode</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proceedings of the 18th ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lee</author>
<author>H Lee</author>
<author>P Abbeel</author>
<author>A Ng</author>
</authors>
<title>Efficient L1 Regularized Logistic Regression.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="22274" citStr="Lee et al., 2006" startWordPosition="3684" endWordPosition="3687"> in the first term of Equation 4 as in usual CRFs. However, the second term of Equation −→ comes exactly zero. To tackle this problem, an additional variable is added for each group (Schmidt, −→ θg∥2 with the variable αg, subject to the constraint αg ≥ ∥−→ θg∥2, i.e., log(pθ(y(i)|x(i))) − C subject to αg ≥ ∥−→ θg∥2, ∀g. This formulation transforms the non-differentiable regularizer to a simple linear function and maximizing Equation 5 will lead to a solution to Equation 4 because it is a lower bound of the latter. Then, we add a sufficient small positive constant ε when computing the L2 norm (Lee et al., 2006), i.e., |−→θg∥2 = √∑|g |j=1 θ2 gj + ε, where |g |denotes the number of features in group g. To obtain the optimal value of parameter θ from the training data, we use an efficient L-BFGS solver to solve the problem, and the first derivative of every feature j in group g is, δL ∑N Cgj(y(i), x(i)) − ∑N ∑ = i=1 i=1 y δθgj θgj p(y|x(i))Cgj(y,x(i)) − 2C √∑|g| l=1 θ2gl + ε where Cgj(y, x) denotes the count of feature j in group g of observation-label pair (x, y). The first two terms of Equation 6 measure the difference between the empirical and the model expected values of feature j in group g, while</context>
</contexts>
<marker>Lee, Lee, Abbeel, Ng, 2006</marker>
<rawString>S. Lee, H. Lee, P. Abbeel, and A. Ng. 2006. Efficient L1 Regularized Logistic Regression. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>Proceedings of ACL Workshop,</booktitle>
<pages>74--81</pages>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Proceedings of ACL Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yandong Liu</author>
<author>Jiang Bian</author>
<author>Eugene Agichtein</author>
</authors>
<title>Predicting Information Seeker Satisfaction in Community Question Answering.</title>
<date>2008</date>
<booktitle>Proceedings of the 31th ACM SIGIR Conference.</booktitle>
<contexts>
<context position="2826" citStr="Liu et al., 2008" startWordPosition="426" endWordPosition="429">such as question retrieval. It performs very well in simple factoid QA settings, where the answers to factoid questions often relate to a single named entity like a person, time or location. However, when it comes to the more sophisticated multisentence questions, it would suffer from the problem of “incomplete answer”. That is, such question often comprises several sub questions in specific contexts and the asker wishes to get elaborated answers for as many aspects of the question as possible. In which case, the single best answer that covers just one or few aspects may not be a good choice (Liu et al., 2008; Takechi et al., 2007). Since “everyone knows something” (Adamic et al., 2008), the use of a single best answer often misses valuable human generated information contained in other answers. In an early literature, Liu et al.(2008) reported that no more than 48% of the 400 best answers were indeed the unique best answers in 4 most popular Yahoo! Answers categories. Table 1 shows an example of the “incomplete answer” problem from Yahoo! Answers&apos;. The asker wishes to know why his teeth bloods and how to prevent it. However, the best answer only gives information on the reason of teeth lhttp://an</context>
</contexts>
<marker>Liu, Bian, Agichtein, 2008</marker>
<rawString>Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008. Predicting Information Seeker Satisfaction in Community Question Answering. Proceedings of the 31th ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanjie Liu</author>
<author>Shasha Li</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Dingyi Han</author>
<author>Yong Yu</author>
</authors>
<title>Understanding and summarizing answers in community-based question answering services.</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd ICCL,</booktitle>
<pages>497--504</pages>
<contexts>
<context position="2826" citStr="Liu et al., 2008" startWordPosition="426" endWordPosition="429">such as question retrieval. It performs very well in simple factoid QA settings, where the answers to factoid questions often relate to a single named entity like a person, time or location. However, when it comes to the more sophisticated multisentence questions, it would suffer from the problem of “incomplete answer”. That is, such question often comprises several sub questions in specific contexts and the asker wishes to get elaborated answers for as many aspects of the question as possible. In which case, the single best answer that covers just one or few aspects may not be a good choice (Liu et al., 2008; Takechi et al., 2007). Since “everyone knows something” (Adamic et al., 2008), the use of a single best answer often misses valuable human generated information contained in other answers. In an early literature, Liu et al.(2008) reported that no more than 48% of the 400 best answers were indeed the unique best answers in 4 most popular Yahoo! Answers categories. Table 1 shows an example of the “incomplete answer” problem from Yahoo! Answers&apos;. The asker wishes to know why his teeth bloods and how to prevent it. However, the best answer only gives information on the reason of teeth lhttp://an</context>
</contexts>
<marker>Liu, Li, Cao, Lin, Han, Yu, 2008</marker>
<rawString>Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and summarizing answers in community-based question answering services. Proceedings of the 22nd ICCL, pages 497–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>A Vasserman</author>
<author>I Tsochantaridis</author>
<author>V Mittal</author>
<author>Y Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>Proceedings of the 45th Annual Meeting of ACL.</booktitle>
<contexts>
<context position="1775" citStr="Riezler et al., 2007" startWordPosition="246" endWordPosition="249">To further unleash the potential of the abundant cQA features, we introduce the group L, regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task. 1 Introduction Community Question and Answering services (cQAs) have become valuable resources for users to pose questions of their interests and share their knowledge by providing answers to questions. They perform much better than the traditional frequently asked questions (FAQ) systems (Jijkoun and Rijke , 2005; Riezler et al., 2007) which are just based on natural language processing and information retrieving technologies due to the need for human intelligence in user generated contents(Gyongyi et al., 2007). In cQAs such as Yahoo! Answers, a resolved question often gets more than one answers and a “best answer” will be chosen by the asker or voted by other community participants. This {question, best answer} pair is then stored and indexed for further uses such as question retrieval. It performs very well in simple factoid QA settings, where the answers to factoid questions often relate to a single named entity like a </context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. 2007. Statistical machine translation for query expansion in answer retrieval. Proceedings of the 45th Annual Meeting of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Schmidt</author>
</authors>
<title>Graphical Model Structure Learning with L1-Regularization. Doctoral Thesis.</title>
<date>2010</date>
<contexts>
<context position="10836" citStr="Schmidt, 2010" startWordPosition="1722" endWordPosition="1723">ty answer summarization. Some other works examined the evaluation of the quality of features for answers extracted from cQA services (Jeon et al., 2006; Hong and Davison , 2009; Shah et al., 2010). In the work of Shah et al.(2010), a large number of features extracted for predicting asker-rated quality of answers was evaluated by using a logistic regression model. However, to the best of our knowledge, there is no work in evaluating the quality of features for community answer summarization. In our work we model the feature learning and evaluation problem as a group L1 regularization problem (Schmidt, 2010) on different feature groups. 3 The Summarization Framework 3.1 Conditional Random Fields We utilize the probabilistic graphical model to solve the answer summarization task, Figure 1 gives some illustrations, in which the sites correspond to the sentences and the edges are utilized to model the interactions between sentences. Specifically, let x be the sentence sequence to all answers within a question thread, and y be the corresponding label sequence. Every component yi of y has a binary value, with +1 for the summary sentence and -1 otherwise. Then under CRF (Lafferty et al., 2001), the con</context>
</contexts>
<marker>Schmidt, 2010</marker>
<rawString>Mark Schmidt. 2010. Graphical Model Structure Learning with L1-Regularization. Doctoral Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chirag Shah</author>
<author>Jefferey Pomerantz</author>
</authors>
<title>Evaluating and Predicting Answer Quality</title>
<date>2010</date>
<booktitle>in Community QA. Proceedings of the 33th ACM SIGIR Conference.</booktitle>
<marker>Shah, Pomerantz, 2010</marker>
<rawString>Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and Predicting Answer Quality in Community QA. Proceedings of the 33th ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document Summarization using Conditional Random Fields.</title>
<date>2007</date>
<booktitle>Proceedings of the 20th IJCAI.</booktitle>
<contexts>
<context position="7087" citStr="Shen et al., 2007" startWordPosition="1119" endWordPosition="1122">loited in the work. Second, we propose a group LI-regularization approach in the CRF model for automatic optimal feature learning to unleash the potential of the features and enhance the performance of answer summarization. We conduct experiments on a Yahoo! Answers dataset. The experimental results show that the proposed model improve the performance significantly(in terms of precision, recall and F1 measures) as well as the ROUGE-1, ROUGE-2 and ROUGE-L measures as compared to the state-of-the-art methods, such as Support Vector Machines (SVM), Logistic Regression (LR) and Linear CRF (LCRF) (Shen et al., 2007). The rest of the paper is arranged as follows: Section 2 presents some definitions and a brief review of related research. In Section 3, we propose the summarization framework and then in Section 4 and 5 we detail the experimental setups and results respectively. We conclude the paper in Section 6. 2 Definitions and Related Work 2.1 Definitions In this subsection we define some concepts that would be helpful to clarify our problems. First we define a complex multi-sentence question as a question with the following properties: Definition: A complex multi-sentence question is one that contains </context>
<context position="25618" citStr="Shen et al., 2007" startWordPosition="4265" endWordPosition="4268">judged by both members in a group to be the case. As a result, we find that 360 (90%) of these questions indeed suffer from the incomplete answer problem, which indicates that our automatic detection method is efficient. This randomly selected 400 questions along with their 2559 answers are then further manually summarized for evaluation of automatically generated answer summaries by our model in experiments. 4http://sandbox.yahoo.com/ 4.2 Evaluation Measures When taking the summarization as a sequential biclassification problem, we can make use of the usual precision, recall and F1 measures (Shen et al., 2007) for classification accuracy evaluation. In our experiments, we also compare the precision, recall and F1 score in the ROUGE-1, ROUGE2 and ROUGE-L measures (Lin , 2004) for answer summarization performance. 5 Experimental Results 5.1 Summarization Results We adapt the Support Vector Machine (SVM) and Logistic Regression (LR) which have been reported to be effective for classification and the Linear CRF (LCRF) which is used to summarize ordinary text documents in (Shen et al., 2007) as baselines for comparison. To better illustrate the effectiveness of question segmentation based contextual fac</context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang and Zheng Chen. 2007. Document Summarization using Conditional Random Fields. Proceedings of the 20th IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Troy Simpson</author>
<author>Malcolm Crowe</author>
</authors>
<date>2005</date>
<note>WordNet.Net http://opensource.ebswift.com/WordNet.Net</note>
<contexts>
<context position="16682" citStr="Simpson and Crowe, 2005" startWordPosition="2680" endWordPosition="2683">eling the pairwise semantic interactions based on question segmentation. In this paper, we utilize a simple but effective lightweight question segmentation method (Ding et al., 2008; Wang et al., 2010). It mainly involves the following two steps: Step 1. Question sentence detection: every sentence in the original multi-sentence question is classified into question sentence and non-question (context) sentence. The question mark and 5W1H features are applied. Step 2. Context assignment: every context sentence is assigned to the most relevant question sentence. We compute the semantic similarity(Simpson and Crowe, 2005) between sentences or sub ques585 Figure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based models. tions as: sim(w1, w2) |x |+ |y |(2) where M(x, y) denotes synset pairs matched in sentences x and y; and the similarity between the two synsets w1 and w2 is computed to be inversely proportional to the length of the path in Wordnet. One answer sentence may related to more than one sub questions to some extent. Thus, we define the replied question Qri as the sub question with the maximal similarity to sentence xi: Qri = argmaxQjsim(xi, Qj). It</context>
</contexts>
<marker>Simpson, Crowe, 2005</marker>
<rawString>Troy Simpson and Malcolm Crowe 2005. WordNet.Net http://opensource.ebswift.com/WordNet.Net</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mineki Takechi</author>
<author>Takenobu Tokunaga</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking-based Question Type Identification for Multi-Sentence Queries.</title>
<date>2007</date>
<booktitle>Proceedings of SIGIR</booktitle>
<note>Workshop.</note>
<contexts>
<context position="2849" citStr="Takechi et al., 2007" startWordPosition="430" endWordPosition="433">etrieval. It performs very well in simple factoid QA settings, where the answers to factoid questions often relate to a single named entity like a person, time or location. However, when it comes to the more sophisticated multisentence questions, it would suffer from the problem of “incomplete answer”. That is, such question often comprises several sub questions in specific contexts and the asker wishes to get elaborated answers for as many aspects of the question as possible. In which case, the single best answer that covers just one or few aspects may not be a good choice (Liu et al., 2008; Takechi et al., 2007). Since “everyone knows something” (Adamic et al., 2008), the use of a single best answer often misses valuable human generated information contained in other answers. In an early literature, Liu et al.(2008) reported that no more than 48% of the 400 best answers were indeed the unique best answers in 4 most popular Yahoo! Answers categories. Table 1 shows an example of the “incomplete answer” problem from Yahoo! Answers&apos;. The asker wishes to know why his teeth bloods and how to prevent it. However, the best answer only gives information on the reason of teeth lhttp://answers.yahoo.com/questio</context>
</contexts>
<marker>Takechi, Tokunaga, Matsumoto, 2007</marker>
<rawString>Mineki Takechi, Takenobu Tokunaga, and Yuji Matsumoto. 2007. Chunking-based Question Type Identification for Multi-Sentence Queries. Proceedings of SIGIR 2007 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mattia Tomasoni</author>
<author>Minlie Huang</author>
</authors>
<title>MetadataAware Measures for Answer Summarization in Community Question Answering.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of ACL,</booktitle>
<pages>760--769</pages>
<contexts>
<context position="24242" citStr="Tomasoni and Huang, 2010" startWordPosition="4038" endWordPosition="4041">that it works well for our answer summarization task in the experiments. 4 Experimental Setting 4.1 Dataset To evaluate the performance of our CRF based answer summarization model, we conduct experiments on the Yahoo! Answers archives dataset. The Yahoo! WebscopeTM Program4 opens up a number of Yahoo! Answers datasets for interested academics in different categories. Our original dataset contains 1,300,559 questions and 2,770,896 answers in ten taxonomies from Yahoo! Answers. After filtering the questions which have less than 5 answers and some trivial factoid questions using the features by (Tomasoni and Huang, 2010) , we reduce the dataset to 55,132 questions. From this sub-set, we next select the questions with incomplete answers as defined in Section 2.1. Specifically, we select the questions where the average similarity between the best answer and all sub questions is less than 0.6 or when the star rating of the best answer is less than 4. We obtain 7,784 questions after this step. To evaluate the effectiveness of this method, we randomly choose 400 questions in the filtered dataset and invite 10 graduate candidate students (not in NLP research field) to verify whether a question suffers from the inco</context>
</contexts>
<marker>Tomasoni, Huang, 2010</marker>
<rawString>Mattia Tomasoni and Minlie Huang. 2010. MetadataAware Measures for Answer Summarization in Community Question Answering. Proceedings of the 48th Annual Meeting of ACL, pages 760–769.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hongning Wang</author>
<author>Chi Wang</author>
</authors>
<title>ChengXiang Zhai, Jiawei Han 2011. Learning Online Discussion Structures by Conditional Random Fields.</title>
<booktitle>Proceedings of the 34th ACM SIGIR Conference.</booktitle>
<marker>Wang, Wang, </marker>
<rawString>Hongning Wang, Chi Wang, ChengXiang Zhai, Jiawei Han 2011. Learning Online Discussion Structures by Conditional Random Fields. Proceedings of the 34th ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wang</author>
</authors>
<title>Zhao-Yan Ming and Tat-Seng Chua.</title>
<date>2009</date>
<booktitle>Proceedings of the 32th ACM SIGIR Conference.</booktitle>
<marker>Wang, 2009</marker>
<rawString>Kai Wang, Zhao-Yan Ming and Tat-Seng Chua. 2009. A Syntactic Tree Matching Approach to Finding Similar Questions in Community-based QA Services. Proceedings of the 32th ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wang</author>
<author>Zhao-Yan Ming</author>
<author>Xia Hu</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Segmentation of Multi-Sentence Questions: Towards Effective Question Retrieval in cQA Services.</title>
<date>2010</date>
<booktitle>Proceedings of the 33th ACM SIGIR Conference,</booktitle>
<pages>387--394</pages>
<contexts>
<context position="8849" citStr="Wang et al., 2010" startWordPosition="1408" endWordPosition="1411">en the best answer and all the sub questions is below some thresh583 olds. We study the issues of similarity threshold and the minimal number of stars empirically in the experimental section and show that they are useful in identifying questions with the incomplete answer problem. 2.2 Related Work There exist several attempts to alleviate the answer completeness problem in cQA. One of them is to segment the multi-sentence question into a set of sub-questions along with their contexts, then sequentially retrieve the sub questions one by one, and return similar questions and their best answers (Wang et al., 2010). This strategy works well in general, however, as the automatic question segmentation is imperfect and the matched similar questions are likely to be generated in different contextual situations, this strategy often could not combine multiple independent best answers of sub questions seamlessly and may introduce redundancy in final answer. On general problem of cQA answer summarization, Liu et al.(2008) manually classified both questions and answers into different taxonomies and applied clustering algorithms for answer summarization.They utilized textual features for open and opinion type que</context>
<context position="16259" citStr="Wang et al., 2010" startWordPosition="2615" endWordPosition="2618">eractions between different sentence sites are crucial, that is, some context co-occurrences should be encouraged and others should be penalized for requirements of information novelty and non-redundancy in the generated summary. Here we consider both local (sentences from the same answer) and global (sentences from different answers) settings. This give rise to four contextual factors that we will explore for modeling the pairwise semantic interactions based on question segmentation. In this paper, we utilize a simple but effective lightweight question segmentation method (Ding et al., 2008; Wang et al., 2010). It mainly involves the following two steps: Step 1. Question sentence detection: every sentence in the original multi-sentence question is classified into question sentence and non-question (context) sentence. The question mark and 5W1H features are applied. Step 2. Context assignment: every context sentence is assigned to the most relevant question sentence. We compute the semantic similarity(Simpson and Crowe, 2005) between sentences or sub ques585 Figure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based models. tions as: sim(w1, w2) |</context>
</contexts>
<marker>Wang, Ming, Hu, Chua, 2010</marker>
<rawString>Kai Wang, Zhao-Yan Ming, Xia Hu and Tat-Seng Chua. 2010. Segmentation of Multi-Sentence Questions: Towards Effective Question Retrieval in cQA Services. Proceedings of the 33th ACM SIGIR Conference, pages 387–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xue</author>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Retrieval models for question and answers archives.</title>
<date>2008</date>
<booktitle>Proceedings of the 31th ACM SIGIR Conference.</booktitle>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>X. Xue, J.Jeon, and W.B.Croft. 2008. Retrieval models for question and answers archives. Proceedings of the 31th ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zi Yang</author>
<author>Keke Cai</author>
<author>Jie Tang</author>
<author>Li Zhang</author>
<author>Zhou Su</author>
<author>Juanzi Li</author>
</authors>
<title>Social Context Summarization.</title>
<date>2011</date>
<booktitle>Proceedings of the 34th ACM SIGIR Conference.</booktitle>
<marker>Yang, Cai, Tang, Zhang, Su, Li, 2011</marker>
<rawString>Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhou Su, and Juanzi Li. 2011. Social Context Summarization. Proceedings of the 34th ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin Y Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Summarizing answers for complicated questions.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th International Conference on LREC,</booktitle>
<location>Genoa, Italy.</location>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006. Summarizing answers for complicated questions. Proceedings of the 5th International Conference on LREC, Genoa, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>