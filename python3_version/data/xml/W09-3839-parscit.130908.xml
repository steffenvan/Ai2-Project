<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004623">
<title confidence="0.996332">
A generative re-ranking model for dependency parsing
</title>
<author confidence="0.998626">
Federico Sangati, Willem Zuidema and Rens Bod
</author>
<affiliation confidence="0.9955615">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.831882">
Science Park 904, 1098 XH Amsterdam, The Netherlands
</address>
<email confidence="0.997976">
{f.sangati,zuidema,rens.bod}@uva.nl
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960538461538">
We propose a framework for dependency
parsing based on a combination of dis-
criminative and generative models. We
use a discriminative model to obtain a k-
best list of candidate parses, and subse-
quently rerank those candidates using a
generative model. We show how this ap-
proach allows us to evaluate a variety of
generative models, without needing differ-
ent parser implementations. Moreover, we
present empirical results that show a small
improvement over state-of-the-art depen-
dency parsing of English sentences.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795902439024">
Probabilistic generative dependency models de-
fine probability distributions over all valid depen-
dency structures, and thus provide a useful inter-
mediate representation that can be used for many
NLP tasks including parsing and language mod-
eling. In recent evaluations of supervised de-
pendency parsing, however, generative approaches
are consistently outperformed by discriminative
models (Buchholz et al., 2006; Nivre et al.,
2007), which treat the task of assigning the cor-
rect structure to a given sentence as a classifica-
tion task. In this category we include both transi-
tion based (Nivre and Hall, 2005) and graph based
parsers (McDonald, 2006).
In this paper, we explore a reranking approach
that combines a generative and a discrimative
model and tries to retain the strengths of both.
The idea of combining these two types of models
through re-ranking is not new, although it has been
mostly explored in constituency parsing (Collins
et al., 2002). This earlier work, however, used the
generative model in the first step, and trained the
discriminative model over its k-best candidates. In
this paper we reverse the usual order of the two
models, by employing a generative model to re-
score the k-best candidates provided by a discrim-
inative model. Moreover, the generative model of
the second phase uses frequency counts from the
training set but is not trained on the k-best parses
of the discriminative model.
The main motivation for our approach is that
it allows for efficiently evaluating many gener-
ative models, differing from one another on (i)
the choice of the linguistic units that are gener-
ated (words, pairs of words, word graphs), (ii) the
generation process (Markov process, top-down,
bottom-up), and (iii) the features that are consid-
ered to build the event space (postags/words, dis-
tance). Although efficient algorithms exist to cal-
culate parse forests (Eisner, 1996a), each choice
gives rise to different parser instantiations.
</bodyText>
<subsectionHeader confidence="0.996991">
1.1 A generative model for re-ranking
</subsectionHeader>
<bodyText confidence="0.999913227272727">
In our re-ranking perspective, all the generative
model has to do is to compute the probability of
k pre-generated structures, and select the one with
maximum probability. In a generative model, ev-
ery structure can be decomposed into a series of
independent events, each mapped to a correspond-
ing conditioning event. As an example, if a gener-
ative model chooses D as the right dependent of a
certain word H, conditioned uniquely on their rel-
ative position, we can define the event as D is the
right dependent of H, and the conditioning event
as H has a right dependent.
As a preprocessing step, every sentence struc-
ture in the training corpus is decomposed into a se-
ries of independent events, with their correspond-
ing conditioning events. During this process, our
model updates two tables containing the frequency
of events and their conditioning counterparts.
In the re-ranking phase, a given candidate struc-
ture can be decomposed into independent events
(e1, e2, ... , en) and corresponding conditioning
events (c1, c2, ... , cn) as in the training phase.
</bodyText>
<page confidence="0.959888">
238
</page>
<bodyText confidence="0.980790291666667">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 238–241,
Paris, October 2009. c�2009 Association for Computational Linguistics
The probability of the structure can then be cal-
culated as
where f(x) returns the frequency of x previously
stored in the tables.
It is important to stress the point that the only
specificity each generative model introduces is in
the way sentence structures are decomposed into
events; provided a generic representation for the
(conditioning) event space, both training phase
and probability calculation of candidate structures
can be implemented independently from the spe-
cific generative model, through the implementa-
tion of generic tables of (conditioning) events.
In this way the probabilities of candidate struc-
tures are exact probabilities, and do not suf-
fer from possible approximation techniques that
parsers often utilize (i.e., pruning). On the other
hand the most probable parse is selected from the
set of the k candidates generated by the discrimi-
native model, and it will equal with the most prob-
able parse among all possible structures, only for
sufficiently high k.
</bodyText>
<sectionHeader confidence="0.508599" genericHeader="method">
2 MST discriminative model
</sectionHeader>
<bodyText confidence="0.999993766666667">
In order to generate a set of k-candidate struc-
tures for every test sentence, we use a state-of-
the-art discriminative model (McDonald, 2006).
This model treats every dependency structure as
a set of word-dependent relations, each described
by a high dimensional feature representation. For
instance, if in a certain sentence word i is the
head of word j, v(i, j) is the vector describing
all the features of such relation (i.e., labels of the
two words, their postag, and other information
including words in between them, and ancestral
nodes). During the training phase the model learns
a weight vector w which is then used to find the
best dependency structure y for a given test sen-
tence x. The score that needs to be maximized is
defined as E(i j) ∈y w · v(i, j), and the best candi-
date is called the maximum spanning tree (MST).
Assuming we have the weight vector and we
only consider projective dependency structures,
the search space can be efficiently computed by
using a dynamic algorithm on a compact repre-
sentation of the parse forest (Eisner, 1996a). The
training phase is more complex; for details we re-
fer to (McDonald, 2006). Roughly, the model em-
ploys a large-margin classifier which iterates over
the structures of the training corpus, and updates
the weight vector w trying to keep the score of the
correct structure above the scores of the incorrect
ones by an amount which is proportional to how
much they differ in accuracy.
</bodyText>
<sectionHeader confidence="0.999223" genericHeader="method">
3 Generative model
</sectionHeader>
<subsectionHeader confidence="0.98682">
3.1 Eisner model
</subsectionHeader>
<bodyText confidence="0.999972352941177">
As a generative framework we have chosen to use
a variation of model C in (Eisner, 1996a). In
this approach nodes are generated recursively in
a top-down manner starting from the special sym-
bol EOS (end of sentence). At any given node, left
and right children are generated as two separate
Markov sequences of nodes1, each conditioned on
ancestral and sibling information (which, for now,
we will simply refer to as context).
One of the relevant variations with respect to
the original model is that in our version the direc-
tion of the Markov chain sequence is strictly left
to right, instead of the usual inside outwards.
More formally, given a dependency structure T,
and any of its node N, the probability of generat-
ing the fragment T (N) of the dependency struc-
ture rooted in N is defined as:
</bodyText>
<equation confidence="0.9983965">
L
P(T(N)) = P(Nal)|context) · P(T(Nal))
l=1
R
x H P(N,r)|context) · P(T(N�r)) (2)
r=1
</equation>
<bodyText confidence="0.999942277777778">
where L and R are the number of left and right
children of N in T (L, R &gt; 0), Nal is the left
daughter of N at position l in T (analogously for
right daughters). The probability of the entire de-
pendency structure T is computed as P(T(EOS)).
In order to illustrate how a dependency struc-
ture can be decomposed into events, we present
in table 1 the list of events and the correspond-
ing conditioning events extracted from the depen-
dency structure illustrated in figure 1. In this sim-
ple example, each node is identified with its word,
and the context is composed of the direction with
respect to the head node, the head node, and the
previously chosen daughter (or NONE if it is the
first). While during the training phase the event
tables are updated with these events, in the test
phase they are looked-up to compute the structure
probability, as in equation 1.
</bodyText>
<footnote confidence="0.479079">
1Every sequence ends with the special symbol EOC.
</footnote>
<equation confidence="0.97956425">
f(ei)
f(ci) (1)
n
i=1
</equation>
<page confidence="0.981336">
239
</page>
<figure confidence="0.953394857142857">
EOS
V
won
N N
Obama election
D J
the presidential
</figure>
<figureCaption confidence="0.9373305">
Figure 1: Dependency tree of the sentence
“Obama won the presidential election”.
</figureCaption>
<subsectionHeader confidence="0.999717">
3.2 Model extension
</subsectionHeader>
<bodyText confidence="0.9999893125">
In equation 2 we have generically defined the
probability of choosing a daughter D based on
specific features associated with D and the con-
text in which it occurs. In our implementation,
this probability is instantiated as in equation 3.
The specific features associated with D are: the
distance2 dist(H, D) between D and its head H,
the flag term(D) which specifies whether D has
more dependents, and the lexical and postag repre-
sentation of D. The context in which D occurs is
defined by features of the head node H, the previ-
ously chosen sister S, the grandparent G, and the
direction dir (left or right).
Equation 3 is factorized in four terms, each em-
ploying an appropriate backoff reduction list re-
ported in descending priority3.
</bodyText>
<equation confidence="0.973273">
P(D|context) = (3)
P(dist(H, D), term(D), word(D), tag(D)|H, S, G, dir) =
P(tag(D)|H, S, G, dir)
</equation>
<bodyText confidence="0.681229666666667">
wt(H), wt(S), wt(G), dir
wt(H), wt(S), t(G), dir
t(S), t(G), dir
�wt(H),
t(H), wt(S), t(G), dir
t(H), t(S), t(G), dir
</bodyText>
<equation confidence="0.641457222222222">
× P(word(D)|tag(D), H, S, G, dir)
reduction list: wt(H), t(S), dir
t(H), t(S), dir
× P(term(D)|word(D), tag(D), H, S, G, dir)
reduction list: tag(D), wt(H), t(S), dir
tag(D), t(H), t(S), dir
× P(dist(P, D)|term(D), word(D), tag(D), H, S, G, dir)
word(D), tag(D), t(H), t(S), dir
tag(D), t(H), t(S), dir
</equation>
<footnote confidence="0.8084497">
2In our implementation distance values are grouped in 4
categories: 1, 2, 3 − 6, 7 − ∞.
3In the reduction lists, wt(N) stands for the string in-
corporating both the postag and the word of N, and t(N)
stands for its postag. This second reduction is never applied
to closed class words. All the notation and backoff parame-
ters are identical to (Eisner, 1996b), and are not reported here
for reasons of space.
4The counts are extracted from a two-sentence corpus
which also includes “Obama lost the election.”
</footnote>
<table confidence="0.998588833333333">
Events Freq. Conditioning Events Freq.
won L EOS NONE 1 L EOS NONE 2
EOC L EOS won 1 L EOS won 1
EOC R EOS NONE 2 R EOS NONE 2
Obama L won NONE 1 L won NONE 1
EOC L won Obama 1 L won Obama 1
election R won NONE 1 R won NONE 1
EOC R won election 1 R won election 1
EOC L Obama NONE 2 L Obama NONE 2
EOC R Obama NONE 2 R Obama NONE 2
the L election NONE 2 L election NONE 2
presidential L election the 1 L election the 2
EOC L election presidential 1 L election presidential 1
EOC R election NONE 2 R election NONE 2
EOC L the NONE 2 L the NONE 2
EOC R the NONE 2 R the NONE 2
EOC L presidential NONE 1 L presidential NONE 1
EOC R presidential NONE 1 R presidential NONE 1
</table>
<tableCaption confidence="0.99916">
Table 1: Events occurring when generating the de-
</tableCaption>
<bodyText confidence="0.66184925">
pendency structure in figure 1, for the event space
(dependent  |direction, head, sister). According to
the reported frequency counts4, the structure has a
associated probability of 1/4.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999805821428571">
In our investigation, we have tested our model
on the Wall Street Journal corpus (Marcus et al.,
1993) with sentences up to 40 words in length,
converted to dependency structures. Although
several algorithms exist to perform such a conver-
sion (Sangati and Zuidema, 2008), we have fol-
lowed the scheme in (Collins, 1999). Section 2-21
was used as training, and section 22 as test set.
The MST discriminative parser was provided with
the correct postags of the words in the test set, and
it was run in second-order5 and projective mode.
Results are reported in table 2, as unlabeled attach-
ment score (UAS). The MST dependency parser
obtains very high results when employed alone
(92.58%), and generates a list of k-best-candidates
which can potentially achieve much better results
(an oracle would score above 95% when selecting
from the first 5-best, and above 99% from the first
1000-best). The decrease in performance of the
generative model, as the number of the candidate
increases, suggests that its performance would be
lower than a discriminative model if used alone.
On the other hand, our generative model is able to
select better candidates than the MST parser, when
their number is limited to a few dozens, yielding a
maximum accuracy for k = 7 where it improves
accuracy on the discriminative model by a 0.51%
(around 7% error reduction).
</bodyText>
<footnote confidence="0.969846">
5The features of every dependency relation include infor-
mation about the previously chosen sister of the dependent.
</footnote>
<table confidence="0.8253748125">
reduction list:
reduction list:
240
k-best Oracle best Oracle worst Reranked 100.00%
1 92.58 92.58 92.58 99.00%
2 94.22 88.66 92.89
3 95.05 87.04 93.02 98.00%
4 95.51 85.82 93.02 97.00%
5 95.78 84.96 93.02 96.00%
6 96.02 84.20 93.06 95.00%
7 96.23 83.62 93.09
8 96.40 83.06 93.02 94.00%
9 96.54 82.57 92.97 93.00%
10 96.64 82.21 92.96 92.00%
100 98.48 73.30 92.32
1000 99.34 64.86 91.47 91.00%
</table>
<figure confidence="0.69471975">
Oracle-Best
Reranked
MST
1 2 3 4 5 6 7 8 9 10 100 1000
</figure>
<figureCaption confidence="0.983957">
Figure 2: UAS accuracy of the MST discriminative and re-ranking parser on section 22 of the WSJ.
Oracle best: always choosing the best result in the k-best, Oracle worst: always choosing the worst,
Reranked: choosing the most probable candidate according to the generative model.
</figureCaption>
<sectionHeader confidence="0.999292" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999943636363636">
We have presented a general framework for depen-
dency parsing based on a combination of discrim-
inative and generative models. We have used this
framework to evaluate and compare several gener-
ative models, including those of Eisner (1996) and
some of their variations. Consistently with earlier
results, none of these models performs better than
the discriminative baseline when used alone. We
have presented an instantiation of this framework
in which our newly defined generative model leads
to an improvement of the state-of-the-art parsing
results, when provided with a limited number of
best candidates. This result suggests that discrim-
inative and generative model are complementary:
the discriminative model is very accurate to filter
out “bad” candidates, while the generative model
is able to further refine the selection among the
few best candidates. In our set-up it is now pos-
sible to efficiently evaluate many other generative
models and identify the most promising ones for
further investigation. And even though we cur-
rently still need the input from a discriminative
model, our promising results show that pessimism
about the prospects of probabilistic generative de-
pendency models is premature.
Acknowledgments We gratefully acknowledge
funding by the Netherlands Organization for
Scientific Research (NWO): FS and RB are
funded through a Vici-grant “Integrating Cogni-
tion” (277.70.006) to RB, and WZ through a Veni-
grant “Discovering Grammar” (639.021.612) of
NWO. We also thank 3 anonymous reviewers for
useful comments.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912444444445">
S. Buchholz, and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of the 10th CoNLL Conference, pp. 149–164.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins, N. Duffy, and F. Park. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In In
Proceedings of the ACL 2002, pp. 263–270.
J. Eisner. 1996a. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
the 16th International Conference on Computational
Linguistics (COLING-96), pp. 340–345.
J. Eisner. 1996b. An Empirical Comparison of Proba-
bility Models for Dependency Grammar. Technical
Report number IRCS-96-11, Univ. of Pennsylvania.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. In Computational Linguistics,
19(2), pp. 313–330.
R. McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and J. Hall. 2005. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Proc. of the Fourth Workshop on Tree-
banks and Linguistic Theories, pp. 137–148.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nils-
son,S. Riedel, and D. Yuret. 2007. The CONLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task Session, pp. 915–
932.
F. Sangati and W. Zuidema. 2008. Unsupervised
Methods for Head Assignments. In Proc. of the
EACL 2009 Conference, pp. 701–709.
</reference>
<page confidence="0.998101">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771946">
<title confidence="0.999701">A generative re-ranking model for dependency parsing</title>
<author confidence="0.98988">Federico Sangati</author>
<author confidence="0.98988">Willem Zuidema</author>
<author confidence="0.98988">Rens</author>
<affiliation confidence="0.9943725">Institute for Logic, Language and University of</affiliation>
<address confidence="0.821794">Science Park 904, 1098 XH Amsterdam, The</address>
<email confidence="0.979457">f.sangati@uva.nl</email>
<email confidence="0.979457">zuidema@uva.nl</email>
<email confidence="0.979457">rens.bod@uva.nl</email>
<abstract confidence="0.996908214285714">We propose a framework for dependency parsing based on a combination of discriminative and generative models. We a discriminative model to obtain a best list of candidate parses, and subsequently rerank those candidates using a generative model. We show how this approach allows us to evaluate a variety of generative models, without needing different parser implementations. Moreover, we present empirical results that show a small improvement over state-of-the-art dependency parsing of English sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the 10th CoNLL Conference,</booktitle>
<pages>149--164</pages>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz, and E. Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proc. of the 10th CoNLL Conference, pp. 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="11470" citStr="Collins, 1999" startWordPosition="1943" endWordPosition="1944">E 1 EOC R presidential NONE 1 R presidential NONE 1 Table 1: Events occurring when generating the dependency structure in figure 1, for the event space (dependent |direction, head, sister). According to the reported frequency counts4, the structure has a associated probability of 1/4. 4 Results In our investigation, we have tested our model on the Wall Street Journal corpus (Marcus et al., 1993) with sentences up to 40 words in length, converted to dependency structures. Although several algorithms exist to perform such a conversion (Sangati and Zuidema, 2008), we have followed the scheme in (Collins, 1999). Section 2-21 was used as training, and section 22 as test set. The MST discriminative parser was provided with the correct postags of the words in the test set, and it was run in second-order5 and projective mode. Results are reported in table 2, as unlabeled attachment score (UAS). The MST dependency parser obtains very high results when employed alone (92.58%), and generates a list of k-best-candidates which can potentially achieve much better results (an oracle would score above 95% when selecting from the first 5-best, and above 99% from the first 1000-best). The decrease in performance </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
<author>F Park</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1748" citStr="Collins et al., 2002" startWordPosition="261" endWordPosition="264">are consistently outperformed by discriminative models (Buchholz et al., 2006; Nivre et al., 2007), which treat the task of assigning the correct structure to a given sentence as a classification task. In this category we include both transition based (Nivre and Hall, 2005) and graph based parsers (McDonald, 2006). In this paper, we explore a reranking approach that combines a generative and a discrimative model and tries to retain the strengths of both. The idea of combining these two types of models through re-ranking is not new, although it has been mostly explored in constituency parsing (Collins et al., 2002). This earlier work, however, used the generative model in the first step, and trained the discriminative model over its k-best candidates. In this paper we reverse the usual order of the two models, by employing a generative model to rescore the k-best candidates provided by a discriminative model. Moreover, the generative model of the second phase uses frequency counts from the training set but is not trained on the k-best parses of the discriminative model. The main motivation for our approach is that it allows for efficiently evaluating many generative models, differing from one another on</context>
</contexts>
<marker>Collins, Duffy, Park, 2002</marker>
<rawString>M. Collins, N. Duffy, and F. Park. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In In Proceedings of the ACL 2002, pp. 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="2683" citStr="Eisner, 1996" startWordPosition="414" endWordPosition="415">ve model of the second phase uses frequency counts from the training set but is not trained on the k-best parses of the discriminative model. The main motivation for our approach is that it allows for efficiently evaluating many generative models, differing from one another on (i) the choice of the linguistic units that are generated (words, pairs of words, word graphs), (ii) the generation process (Markov process, top-down, bottom-up), and (iii) the features that are considered to build the event space (postags/words, distance). Although efficient algorithms exist to calculate parse forests (Eisner, 1996a), each choice gives rise to different parser instantiations. 1.1 A generative model for re-ranking In our re-ranking perspective, all the generative model has to do is to compute the probability of k pre-generated structures, and select the one with maximum probability. In a generative model, every structure can be decomposed into a series of independent events, each mapped to a corresponding conditioning event. As an example, if a generative model chooses D as the right dependent of a certain word H, conditioned uniquely on their relative position, we can define the event as D is the right </context>
<context position="6082" citStr="Eisner, 1996" startWordPosition="969" endWordPosition="970">words, their postag, and other information including words in between them, and ancestral nodes). During the training phase the model learns a weight vector w which is then used to find the best dependency structure y for a given test sentence x. The score that needs to be maximized is defined as E(i j) ∈y w · v(i, j), and the best candidate is called the maximum spanning tree (MST). Assuming we have the weight vector and we only consider projective dependency structures, the search space can be efficiently computed by using a dynamic algorithm on a compact representation of the parse forest (Eisner, 1996a). The training phase is more complex; for details we refer to (McDonald, 2006). Roughly, the model employs a large-margin classifier which iterates over the structures of the training corpus, and updates the weight vector w trying to keep the score of the correct structure above the scores of the incorrect ones by an amount which is proportional to how much they differ in accuracy. 3 Generative model 3.1 Eisner model As a generative framework we have chosen to use a variation of model C in (Eisner, 1996a). In this approach nodes are generated recursively in a top-down manner starting from th</context>
<context position="10085" citStr="Eisner, 1996" startWordPosition="1671" endWordPosition="1672">ist: wt(H), t(S), dir t(H), t(S), dir × P(term(D)|word(D), tag(D), H, S, G, dir) reduction list: tag(D), wt(H), t(S), dir tag(D), t(H), t(S), dir × P(dist(P, D)|term(D), word(D), tag(D), H, S, G, dir) word(D), tag(D), t(H), t(S), dir tag(D), t(H), t(S), dir 2In our implementation distance values are grouped in 4 categories: 1, 2, 3 − 6, 7 − ∞. 3In the reduction lists, wt(N) stands for the string incorporating both the postag and the word of N, and t(N) stands for its postag. This second reduction is never applied to closed class words. All the notation and backoff parameters are identical to (Eisner, 1996b), and are not reported here for reasons of space. 4The counts are extracted from a two-sentence corpus which also includes “Obama lost the election.” Events Freq. Conditioning Events Freq. won L EOS NONE 1 L EOS NONE 2 EOC L EOS won 1 L EOS won 1 EOC R EOS NONE 2 R EOS NONE 2 Obama L won NONE 1 L won NONE 1 EOC L won Obama 1 L won Obama 1 election R won NONE 1 R won NONE 1 EOC R won election 1 R won election 1 EOC L Obama NONE 2 L Obama NONE 2 EOC R Obama NONE 2 R Obama NONE 2 the L election NONE 2 L election NONE 2 presidential L election the 1 L election the 2 EOC L election presidential 1</context>
<context position="13596" citStr="Eisner (1996)" startWordPosition="2300" endWordPosition="2301">00 99.34 64.86 91.47 91.00% Oracle-Best Reranked MST 1 2 3 4 5 6 7 8 9 10 100 1000 Figure 2: UAS accuracy of the MST discriminative and re-ranking parser on section 22 of the WSJ. Oracle best: always choosing the best result in the k-best, Oracle worst: always choosing the worst, Reranked: choosing the most probable candidate according to the generative model. 5 Conclusions We have presented a general framework for dependency parsing based on a combination of discriminative and generative models. We have used this framework to evaluate and compare several generative models, including those of Eisner (1996) and some of their variations. Consistently with earlier results, none of these models performs better than the discriminative baseline when used alone. We have presented an instantiation of this framework in which our newly defined generative model leads to an improvement of the state-of-the-art parsing results, when provided with a limited number of best candidates. This result suggests that discriminative and generative model are complementary: the discriminative model is very accurate to filter out “bad” candidates, while the generative model is able to further refine the selection among t</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996a. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proc. of the 16th International Conference on Computational Linguistics (COLING-96), pp. 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>An Empirical Comparison of Probability Models for Dependency Grammar.</title>
<date>1996</date>
<tech>Technical Report number IRCS-96-11,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="2683" citStr="Eisner, 1996" startWordPosition="414" endWordPosition="415">ve model of the second phase uses frequency counts from the training set but is not trained on the k-best parses of the discriminative model. The main motivation for our approach is that it allows for efficiently evaluating many generative models, differing from one another on (i) the choice of the linguistic units that are generated (words, pairs of words, word graphs), (ii) the generation process (Markov process, top-down, bottom-up), and (iii) the features that are considered to build the event space (postags/words, distance). Although efficient algorithms exist to calculate parse forests (Eisner, 1996a), each choice gives rise to different parser instantiations. 1.1 A generative model for re-ranking In our re-ranking perspective, all the generative model has to do is to compute the probability of k pre-generated structures, and select the one with maximum probability. In a generative model, every structure can be decomposed into a series of independent events, each mapped to a corresponding conditioning event. As an example, if a generative model chooses D as the right dependent of a certain word H, conditioned uniquely on their relative position, we can define the event as D is the right </context>
<context position="6082" citStr="Eisner, 1996" startWordPosition="969" endWordPosition="970">words, their postag, and other information including words in between them, and ancestral nodes). During the training phase the model learns a weight vector w which is then used to find the best dependency structure y for a given test sentence x. The score that needs to be maximized is defined as E(i j) ∈y w · v(i, j), and the best candidate is called the maximum spanning tree (MST). Assuming we have the weight vector and we only consider projective dependency structures, the search space can be efficiently computed by using a dynamic algorithm on a compact representation of the parse forest (Eisner, 1996a). The training phase is more complex; for details we refer to (McDonald, 2006). Roughly, the model employs a large-margin classifier which iterates over the structures of the training corpus, and updates the weight vector w trying to keep the score of the correct structure above the scores of the incorrect ones by an amount which is proportional to how much they differ in accuracy. 3 Generative model 3.1 Eisner model As a generative framework we have chosen to use a variation of model C in (Eisner, 1996a). In this approach nodes are generated recursively in a top-down manner starting from th</context>
<context position="10085" citStr="Eisner, 1996" startWordPosition="1671" endWordPosition="1672">ist: wt(H), t(S), dir t(H), t(S), dir × P(term(D)|word(D), tag(D), H, S, G, dir) reduction list: tag(D), wt(H), t(S), dir tag(D), t(H), t(S), dir × P(dist(P, D)|term(D), word(D), tag(D), H, S, G, dir) word(D), tag(D), t(H), t(S), dir tag(D), t(H), t(S), dir 2In our implementation distance values are grouped in 4 categories: 1, 2, 3 − 6, 7 − ∞. 3In the reduction lists, wt(N) stands for the string incorporating both the postag and the word of N, and t(N) stands for its postag. This second reduction is never applied to closed class words. All the notation and backoff parameters are identical to (Eisner, 1996b), and are not reported here for reasons of space. 4The counts are extracted from a two-sentence corpus which also includes “Obama lost the election.” Events Freq. Conditioning Events Freq. won L EOS NONE 1 L EOS NONE 2 EOC L EOS won 1 L EOS won 1 EOC R EOS NONE 2 R EOS NONE 2 Obama L won NONE 1 L won NONE 1 EOC L won Obama 1 L won Obama 1 election R won NONE 1 R won NONE 1 EOC R won election 1 R won election 1 EOC L Obama NONE 2 L Obama NONE 2 EOC R Obama NONE 2 R Obama NONE 2 the L election NONE 2 L election NONE 2 presidential L election the 1 L election the 2 EOC L election presidential 1</context>
<context position="13596" citStr="Eisner (1996)" startWordPosition="2300" endWordPosition="2301">00 99.34 64.86 91.47 91.00% Oracle-Best Reranked MST 1 2 3 4 5 6 7 8 9 10 100 1000 Figure 2: UAS accuracy of the MST discriminative and re-ranking parser on section 22 of the WSJ. Oracle best: always choosing the best result in the k-best, Oracle worst: always choosing the worst, Reranked: choosing the most probable candidate according to the generative model. 5 Conclusions We have presented a general framework for dependency parsing based on a combination of discriminative and generative models. We have used this framework to evaluate and compare several generative models, including those of Eisner (1996) and some of their variations. Consistently with earlier results, none of these models performs better than the discriminative baseline when used alone. We have presented an instantiation of this framework in which our newly defined generative model leads to an improvement of the state-of-the-art parsing results, when provided with a limited number of best candidates. This result suggests that discriminative and generative model are complementary: the discriminative model is very accurate to filter out “bad” candidates, while the generative model is able to further refine the selection among t</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996b. An Empirical Comparison of Probability Models for Dependency Grammar. Technical Report number IRCS-96-11, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>In Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="11254" citStr="Marcus et al., 1993" startWordPosition="1906" endWordPosition="1909">he 1 L election the 2 EOC L election presidential 1 L election presidential 1 EOC R election NONE 2 R election NONE 2 EOC L the NONE 2 L the NONE 2 EOC R the NONE 2 R the NONE 2 EOC L presidential NONE 1 L presidential NONE 1 EOC R presidential NONE 1 R presidential NONE 1 Table 1: Events occurring when generating the dependency structure in figure 1, for the event space (dependent |direction, head, sister). According to the reported frequency counts4, the structure has a associated probability of 1/4. 4 Results In our investigation, we have tested our model on the Wall Street Journal corpus (Marcus et al., 1993) with sentences up to 40 words in length, converted to dependency structures. Although several algorithms exist to perform such a conversion (Sangati and Zuidema, 2008), we have followed the scheme in (Collins, 1999). Section 2-21 was used as training, and section 22 as test set. The MST discriminative parser was provided with the correct postags of the words in the test set, and it was run in second-order5 and projective mode. Results are reported in table 2, as unlabeled attachment score (UAS). The MST dependency parser obtains very high results when employed alone (92.58%), and generates a </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics, 19(2), pp. 313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1442" citStr="McDonald, 2006" startWordPosition="212" endWordPosition="213">ncy models define probability distributions over all valid dependency structures, and thus provide a useful intermediate representation that can be used for many NLP tasks including parsing and language modeling. In recent evaluations of supervised dependency parsing, however, generative approaches are consistently outperformed by discriminative models (Buchholz et al., 2006; Nivre et al., 2007), which treat the task of assigning the correct structure to a given sentence as a classification task. In this category we include both transition based (Nivre and Hall, 2005) and graph based parsers (McDonald, 2006). In this paper, we explore a reranking approach that combines a generative and a discrimative model and tries to retain the strengths of both. The idea of combining these two types of models through re-ranking is not new, although it has been mostly explored in constituency parsing (Collins et al., 2002). This earlier work, however, used the generative model in the first step, and trained the discriminative model over its k-best candidates. In this paper we reverse the usual order of the two models, by employing a generative model to rescore the k-best candidates provided by a discriminative </context>
<context position="5163" citStr="McDonald, 2006" startWordPosition="809" endWordPosition="810"> tables of (conditioning) events. In this way the probabilities of candidate structures are exact probabilities, and do not suffer from possible approximation techniques that parsers often utilize (i.e., pruning). On the other hand the most probable parse is selected from the set of the k candidates generated by the discriminative model, and it will equal with the most probable parse among all possible structures, only for sufficiently high k. 2 MST discriminative model In order to generate a set of k-candidate structures for every test sentence, we use a state-ofthe-art discriminative model (McDonald, 2006). This model treats every dependency structure as a set of word-dependent relations, each described by a high dimensional feature representation. For instance, if in a certain sentence word i is the head of word j, v(i, j) is the vector describing all the features of such relation (i.e., labels of the two words, their postag, and other information including words in between them, and ancestral nodes). During the training phase the model learns a weight vector w which is then used to find the best dependency structure y for a given test sentence x. The score that needs to be maximized is define</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
</authors>
<title>MaltParser: A LanguageIndependent System for Data-Driven Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proc. of the Fourth Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>137--148</pages>
<contexts>
<context position="1401" citStr="Nivre and Hall, 2005" startWordPosition="204" endWordPosition="207">1 Introduction Probabilistic generative dependency models define probability distributions over all valid dependency structures, and thus provide a useful intermediate representation that can be used for many NLP tasks including parsing and language modeling. In recent evaluations of supervised dependency parsing, however, generative approaches are consistently outperformed by discriminative models (Buchholz et al., 2006; Nivre et al., 2007), which treat the task of assigning the correct structure to a given sentence as a classification task. In this category we include both transition based (Nivre and Hall, 2005) and graph based parsers (McDonald, 2006). In this paper, we explore a reranking approach that combines a generative and a discrimative model and tries to retain the strengths of both. The idea of combining these two types of models through re-ranking is not new, although it has been mostly explored in constituency parsing (Collins et al., 2002). This earlier work, however, used the generative model in the first step, and trained the discriminative model over its k-best candidates. In this paper we reverse the usual order of the two models, by employing a generative model to rescore the k-best</context>
</contexts>
<marker>Nivre, Hall, 2005</marker>
<rawString>J. Nivre and J. Hall. 2005. MaltParser: A LanguageIndependent System for Data-Driven Dependency Parsing. In Proc. of the Fourth Workshop on Treebanks and Linguistic Theories, pp. 137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<booktitle>The CONLL</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,S. Riedel, and D. Yuret. 2007. The CONLL 2007 shared task on dependency parsing. In Proc. of the CoNLL 2007 Shared Task Session, pp. 915– 932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sangati</author>
<author>W Zuidema</author>
</authors>
<title>Unsupervised Methods for Head Assignments.</title>
<date>2008</date>
<booktitle>In Proc. of the EACL 2009 Conference,</booktitle>
<pages>701--709</pages>
<contexts>
<context position="11422" citStr="Sangati and Zuidema, 2008" startWordPosition="1932" endWordPosition="1935"> 2 R the NONE 2 EOC L presidential NONE 1 L presidential NONE 1 EOC R presidential NONE 1 R presidential NONE 1 Table 1: Events occurring when generating the dependency structure in figure 1, for the event space (dependent |direction, head, sister). According to the reported frequency counts4, the structure has a associated probability of 1/4. 4 Results In our investigation, we have tested our model on the Wall Street Journal corpus (Marcus et al., 1993) with sentences up to 40 words in length, converted to dependency structures. Although several algorithms exist to perform such a conversion (Sangati and Zuidema, 2008), we have followed the scheme in (Collins, 1999). Section 2-21 was used as training, and section 22 as test set. The MST discriminative parser was provided with the correct postags of the words in the test set, and it was run in second-order5 and projective mode. Results are reported in table 2, as unlabeled attachment score (UAS). The MST dependency parser obtains very high results when employed alone (92.58%), and generates a list of k-best-candidates which can potentially achieve much better results (an oracle would score above 95% when selecting from the first 5-best, and above 99% from th</context>
</contexts>
<marker>Sangati, Zuidema, 2008</marker>
<rawString>F. Sangati and W. Zuidema. 2008. Unsupervised Methods for Head Assignments. In Proc. of the EACL 2009 Conference, pp. 701–709.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>