<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.586187">
Lnerican Journal of Computational Linguistics Miarnfiche 80
</note>
<sectionHeader confidence="0.663913" genericHeader="abstract">
INFERENCE AND THEORY
</sectionHeader>
<author confidence="0.802689">
DAVID L. WALTZ, EDITOR
</author>
<affiliation confidence="0.591803333333333">
Coordinated Scierce Laboratory
University of Illinois
Urbana 61801
</affiliation>
<title confidence="0.786834611111111">
Papers presented in two sessions of TINLAP-2, the 1978 Meeting
of the Association for Computational Linguistics, held with
joint sponsorship by the Association for Computing Machinery
and its Special Interest Group in Artificial Intelligence.
Copyright 1978, 1979
Association for Computing Machinery
Association for Computational Linguistics
TABLE OF CONTENTS
PAPER r ci
Session Inference.Mechaniws in Ndtural Language
A Note on Partial Match of Descriptions. Can One Simultaneougly Question (Retrieve) 184 3
and Inform (Update)?
Aravind K. Joshi •
With Spoon in Hand this must be the Eating Frame I. 187 6
Eugene Charniak
Fragments of a Theory of Human Plausible Reasoning
Allan Collins .. 194 13
Indirect Responses to Loaded Questions
</title>
<author confidence="0.4901">
S. Jerrold Kaplan . 202 21
</author>
<note confidence="0.788467875">
On Reasoning by Default
Raymond Reiter 210 29
Path-Based and Node-Based Inference in Semantic Networks
Stuart C. Shapiro 219 38
The %presentation of Derivable Information in Memory When What Might Have Been Left
Unsaid is Said
Rand J. Spiro Joseph Esposito, and Richard J. Vondruska 226 46
Session 6 Computational Models as a Vehicle tor Theoretical Linguistics
</note>
<title confidence="0.790397">
A Heuristic for Paradigms
Joseph E. Grimes 232 51
A &apos;Computational Account of Some Constraints on Language
Mitchell Marcus 236 55
Remarks on Processing, Constraints, and the Lexicon
</title>
<author confidence="0.924713">
Thomas Wasow . 247 66
</author>
<affiliation confidence="0.810367">
List of Questions Suggested for Consideration in Each Session 252 71
</affiliation>
<note confidence="0.759324666666667">
iii
A NOTE ON PARTIAL MATCH OF DESCRIPTIONS: CAN ONE ,
SIMULTANEOUSLY QUESTION (RETRIEVE) AND INroRm (UPDATE)?&apos;
</note>
<author confidence="0.548833">
Aravind K. Joshi
</author>
<affiliation confidence="0.7040805">
Department of Computer and Information Science
The Moore School, University of Pennsylvania
</affiliation>
<address confidence="0.382503">
Philadelphia, Pa. 19104
</address>
<bodyText confidence="0.993857084210526">
Summary: In data base query systems there is an
implicit assumption that descriptions in queries
must match exactly, i.e., queries are for retrieval
only, and not for retrieval and updating simul-
taneously. A related assumption (or constraint)
that in questions descriptions are used refer-
entially only (i.e., a question cannot be used
simultaneously for questioning and informing) seems
to hold in ordinary conversations also, with some
qualifications. Some issues related to the
yalidity of such a conztraint.and its relation
to partial matching of.aescriptions are briefly
discubsed in this note.
1. In a question-answer syAtem each description
In a query is used referentially i.e., for each
description one expects to find an entity in the
Oata base which serves as the unique referent for
that description. For simplicity, hereafter we
will consider only definite descriptions (in
particular, definite noun phrases consisting of a
definite article, an adjective, and a noun). Thus
in (1)
(1) Is the red book on the table?
the description the red book will serve to identify
an entity, say, el in the data base2 and the
description the table, an entity, say, e2. The
question can be answered after verifying the
appropriate relation between el and e2. For the
purpose of making the definiteness transpareAt and
also for simplifying the discussiOn in this note,
let hs assume that there is exactly eeP hr,ek and
one table in the data base.
2. The match for the red book can succeed if el
las a color attribute with the value red. The
match can fail either due to a mismatch or a
partial match. A, mismatch will occur if el has a
color value other than red, say green. A partial
match will occur if el has an unspecified value
for the color attribute or iftthe possession of
the color attribute itself has not been specified
for el.
In the rest of the discussion, we will not be
concerned with failure due to mismatch, although
many of the issues raised below are quite relevant
to this case also. We will be concerned with
partial matches only. A partial match really is
a-partially successful match, where a part of the
Ieseription has matched exactly, and the
namaindee has failed to match due to the lack of
some information, and not due to a mismatch.
3. Let us consider the case of a partial match
where the part of the description that matched is
sufficient to identify the referent uniquely. In
(2) this is trivially arcomplished because of our
assumption that there is exactly one book and one
table in the data base.3 Although we have a
partial match (due to the lack of the color value
or the color attribute itself for e1), it will be
possible to answer the questicn either by yes, or
no depending on whether el is on e2 or not, since
the referents el and e2 have been uniquely
identified. How should we proceed in this case?
1. if we insist that each description th
the question must match exactly, then clearly, wi
have failed to establish a refPree and thc-
question cannct be answered.
2. On the other hand, we may ,sume that
whenever we have a partial match and the referents
are uniquely identified somehow, we should answer
the question, and treat that part of the
description which was not accounted for as new
information. This new information can then be
used to update the data base. Thus for the
question (2), if the partial match is due to the
fact that in the data base the value for the color
attribute for el is not specified, then we can now
specify it to be red. If, on the other hand, the
partial match %la:, due to the fact that the
possession of the color attribute itself is not
specified for el, then tle updating would involve
adding a new attribute called color for el, and
then 9pecifyine, a value for it, which in is this
case is red. The first type of update can be
called cent update ahd the second type,
structure update; in the first ease we have made
a local mcdification of assigning a value to an
attribute, while in the second case a new
structural item has been added .4
4. There are a number of issues involved in
adopting a strategy for updating upon a partial
match when the matched part uniquely identifies
the referent. We will state only two of these
issues here and pursue the second in some detail.
a) The part of the description that was
missing in the data base (and which led to a
</bodyText>
<page confidence="0.983006">
184
</page>
<sectionHeader confidence="0.394941" genericHeader="keywords">
4
</sectionHeader>
<bodyText confidence="0.97015464516129">
partial match) is accepted as new information and
used for updating. The strategy followed is that.
if an exact match fails due to the lack of some
information then the missing information is treated
as new and updating is done accordingly. This is
a kind of default reasoning.5 Howeveriiit is not
clear ahether we can allow such unconstrained
updatea. In data base query syatemS there is an
implici assumption that the descriptions ,in
queries must match exactly, i.e., queries are for
retrieval only8 and not for retrieval and updating
simultaneously. Can we relax this requirement
somewhat? We an get some ideas by looking at
queetions in ordinary conversations, which is what
we will do briefly in b) below.
b) The hypothesis (or constraint) that in a
question construct7 definite descriptions are
used referentially only (i.e., a question cannot
be used simultaneously for asking a question ond
conveying some additional information) seems to
bold in ordinary conversations also, with sortie
qualificatiens. The three examples below brief 15
describe some of the problems involved.
1) Suppose that l) tnere ia only one
individual in the context, 2) the speaker believes
that he is d plumber, 3) the hearer is unaware
of his being a plumber, and 4) the speaker
believes that the hearer is unaware of his being
a plumber. Under such circuipstances it would be
inappropriate to use (3) to aske the duestion (4),
and simultanepusly inform the hearer that (5).
</bodyText>
<listItem confidence="0.812552333333333">
(3) wnen uid the plumber leave?
(4) When did the person leave?
(5) He is a plumber.
</listItem>
<bodyText confidence="0.970289567164179">
If (3) is used by the speaker (possibly au° 0 a
mistaken belief that the bearer I &apos;mire that the
person is a plumber), it is unlikely that the
hearer will update his model without some
clarification or some response such as Oh&apos; I
didn&apos;t know that he was a plumber, i.e.711-i- hearer
will not update without any interrupt in responses.
This example&apos; illustrates that the question
coostruct cannot be used for questioning and
informing simultaneously, and if it appears to
havP been 9D used (due to the speaker&apos;s ignorance
of the hearer&apos;s lack of some information), the
updating by hearer is not without an interrupting
response, thu5 indirectly c&apos;onfirtniitp the hypothesis.
2) Again suppose that 1) there is only one
individual In the context, 2) the speaker regards
him as d grouch, 3) the hearer has no such
specific evaluation of him, dnd 4) the speaker
believes that the hearer has no such evaluation.
In this case, it seems not completely inappropri-
ate for the speaker to use (6), in order to ask
the question (7), and simultane4us1y inform the
user that the ,,peaker regards (8) to be the case.
(6) When did the grouch leave?
(7) When did the person leave?
(8) He is a grouch.
With evaluative information; simultanedusly
questioning and-informing appears to be a bit
more convenient. If (6) is used by the speaker,
it appears that the hearer can update-his mx:60.1
without any intetrupting responses, with the
attribute grouchy attached to the entity, as
speaker&apos;s evaluation&apos;(and the fiearer&apos;s too if he
agrees with the speaker). &apos;Even if the hearer
asks for clarifications it is likely to be of the
form Oh! I didn&apos;t know thatyoulhoulght&apos;he was a
grouch rather than Oh! 1 didn&apos;t now that he was
a grouch (compare this to the respewnse in the
previous example).
3) Finally, there is an apparent viol* ion
of the hypothesis in examples such as (9).
(9) Who is sitting to the right of your lovely,.
alfe?
(9) can be used by the speaker to ask the question
and pay a compliment (a side effect) rather than
to convey new information. Thus the hypothesis
does not appear to be violated in these cases.
5. Some of the issues which merit further
-Investigation are as followS. 1) To what extent
the hypothesis can be violated and what are the
side effects. If the constraint is mutually
understood by the speaker and the hearer, then
any apparent violation of it will be recognized
and may be accompanied by a side effect
(implicature?) in addition to the updating. 2) To
what -extent updating without interrupting
responses depends on the shape of the deserietion,
the syntactic construct in which it appears
(e.g., questions, it-clefts, declaratives, etc,)8,
the role it plays in the construct (e.g„ subject,
topic, etc. ), the discourse model (for the
speaker and for the hearer) created so far, 9 etc.
3) To what extent tha &apos;new&apos; information used
for updating has to be somehow relevant to the
&apos;old&apos; information, either by being Werrable
from it or by being able to fit it into the
disecurse structure created so far, etc.1°
</bodyText>
<note confidence="0.782789666666667">
Notes:
1. This work is partially supported 01
NS! Grant MCS75-19486. I wish to thank
</note>
<author confidence="0.8450755">
Jerry Kaplan, Lorrie Levin, Stan Rosenschein,
Ivan Sag, and Bonnie Webber for valuable
</author>
<bodyText confidence="0.91505875">
dineussions.
Some of the issues raised here will be
cliscussed in detail in a forthcoming paper by
Joshi and Rosenschcin (Strategies for reference
and ascription in object centered representations).
2. We 1411 assume a rather simple-
minded structure for the data base. It will
consist of entities and attributes, and relations
among entities.
3. However, in general, unique reference
may be established due to the context, and the
structure and content of the data base.
</bodyText>
<listItem confidence="0.3304965">
4. In the data base context, updates are
u5wity content updates. Structure updates are nol
</listItem>
<page confidence="0.976513">
185
</page>
<note confidence="0.989379125">
permitted. In a conversational context and
discourse understanding, clearly, both types of
updates are possible. In these contexts it is not
clear whether we can always tell which type of
update has taken place: Structure updates should
be hardcm than context todates, cognitively
speaking, but this is on ix a conjecture at .this
dine.
</note>
<author confidence="0.271264">
S. See &amp;quot;On reasoning by detault&amp;quot; by Raymond
</author>
<bodyText confidence="0.575804923076923">
Reiter (this volume). The closed world assumption
discussed in this paper is also re:levant to
our discussion.. See also &amp;quot;Fragments Qf a theory
of Xmaniadausible reasoning&amp;quot; by Allan Collins
(this volume), and &amp;quot;InZerencing on partial
information&amp;quot; by Arkvind K. Josh!, an Pattern
Directed Inference (ed. F. Hays-Roth and D.
Waterman), Academic Presn. 1978.
6. See &amp;quot;Cooperative responses from a natural
language data base query system: Preliminary
report&amp;quot;, by S. Jerrold, Kaplan, Technical Report,
De.; t of Com&apos;titer and Information Science,
University of Pennsylvania vember 77.
7. We will limit outselves (Illy to wh
questions and yes/no questions.
8. Lorrie Levin has made a perliminary
investigation of the update potential of gam of
these constructs (unpublished).
a. En1ity-oriented discourse models have
been considered for problems of reference
(see &amp;quot;Alormal approach to discourse anaphora&amp;quot;
by Bonnie Webber, Ph.D. Dissertation, Harvard
University, 1978).
10. A detailed discussion of some of these
issues will be included in a forthcoming paper
by Joshi and Rosenschein (see note 1).
</bodyText>
<page confidence="0.950524">
186
</page>
<note confidence="0.782601333333333">
WITH AISPOON IN HAND THIS MUST lig THE EATING FRAME The lawyer 400k a cab to the restaurant near
the uniOersity.
Eugene Charniak Here we have &amp;quot;lawyer&amp;quot;, &amp;quot;cab&amp;quot;, &amp;quot;restaurant&amp;quot; and
Department of cnmputer Science &amp;quot;university&amp;quot; all of which are calling for our
Yale University attention. Somehow on the basis of latet lines we
must weed out those which our only incidental.
</note>
<sectionHeader confidence="0.938736" genericHeader="introduction">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9999062">
A language comprehension program using
&amp;quot;frames&amp;quot; &amp;quot;scripts&amp;quot;, etc. must be able to decide
which frames are appropriate to the text. Often
there will be explicit indication (&amp;quot;Fred was
playing tennis&amp;quot; suggests the TENNIS frame) but it
is not alusys so easy.(&amp;quot;Tt4 woman waged while.the
man on the stage sawed her in half&amp;quot; suggests
MAOICIAN but how?) This paper will examine how a
program might go about determining the appropriate
frame in such cases. At a sufficiently vague
level the model presented here will resemble that
of Minsky (1975) in it&apos;s assumption that one
usually has available one or more context frames.
Hence one only needs worry if information comes in
which does not lit them. As opposed to Minsky
however the suggestions 63r new context frames
will not come from the old ones, but rather from
the conflicting information. The problem them
becomes how potential frames are indexed under the
information which &amp;quot;suggests&amp;quot; them.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="method">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.955838433333333">
Understanding every day discourse requires
making inferences from a very large base of common
sense knowledge. To avoid death by combinatorial
explosion our computer must be sable to access the
knowledge it needs without irrelevant knowledge
getting in its way. A plausible constraint on the
knowledge we might use at a given point in a story
or conversation (I shall henceforth simply assume
we are dealing with a story) is to restrict
consideration to that portion of our knowledge
which is &amp;quot;about&amp;quot; things which have been mentioned
in the discourse. So if we have a story which
mentions trains and train stations, we will not
use our knowledge of, say, circuses. This
requires, of course, that given a topic, such as
trains, or eating, we must be able to actess its
knowledge without going through everything we
know. Hence we are lead in a natural way to
something approaching a notion of &amp;quot;frame&amp;quot; (Minsky
1975): a collection of knowledge about a single
stereotyped situation.
In the above discussion however I have made a
rather important slight of hand. Given a. story we
only want to consider those frames &amp;quot;about&amp;quot; things
in the story. How is it that we decide which
frames qualify? I was able to gloss over this
because in most situations the prablen, at least
at a surface level, does not appear 01 that
difficult. If the story is about trains, it will
surely, mention trains. So we see the word
&amp;quot;train&amp;quot;, and we assume that trains are relevant.
What could be easier.
Unfortunately, this ease is deceptive for the
story may mention many topics of which only a few
are truely important to the story. For example.
But a more immediate difficulpy are those
situations where et story dealt; with a well defined
topic, yet never explicitly mentions it. So
consider:
The wmdan waved as the man on the stage sawed
her ig.half.
Here we lave no difficulty in guessing that this
a a magic trick, although nothing of the sort his
beet mentioned. We are able to take &amp;quot;low level&amp;quot;
facts concerning sawing, stages, etc and put them
together in, a higher level &amp;quot;magician&amp;quot; hypothesis.
As such, the phenomena illustrated here &apos;is
essentialy bottom up.
Of course, any time we try to infer
relatively global propeFties from more local
evidence we may make mistakes. That this creates
problems N in frame determinatiOn is dhawn by the
nice example of Collins et. al. (forthcomming).
(To aet the full import of the example, try
pausing briefly after each sentence.)
He plunked down $5 at he window. She tried
to give him $2.50 but he tefused fo taike it.
So when they got inside she bought him a large
bag of popcorn.
The first line is uniformly interpreted as a
buying .act (most even going fortheg and assuming
somethinA like a. bet at a racetrack l. The second
line is then seen as a return or change, but the
refusal is problematic. The third, line resolves
all of this by suggesting a date at the movies -
considerable revision of the initial hypothesis.
To sometime. the last few paragraphs, the
problem of frame determination, in language
comprehension involves three sub-problems.
1) Stories will typically elude to many higher
fram&apos;es, any of which might serve as the
context for the incoming lines. HOW do we
choose between them?
2) The words used in a story may not directly
indicate the proper higher frame. Haw do we&apos;
do the bottom up processing to find it?
3) If we are lead astray in the course of (2),
how do we correct ourselves on the basis of
further evidence.
In the paper which follows I will be primarily
concentrate an (2) with (3) being mentioned
occasionally. In essence my position on (1) is
that it will not be too much of a problem,
provided that the cost of setting up a context
like &amp;quot;restaurant&amp;quot; is small. If it is never used
then as the story goes on it will receeded into
the background. How this &amp;quot;receeding&amp;quot; takes place
I shall not say, since for one thing it is a
problem in many areas, and for another, I don&apos;t
know.
</bodyText>
<page confidence="0.988917">
187
</page>
<note confidence="0.99323525">
Concerning (2) and (3), we will be lead to a
position similar to that of Minsky (1975) and
Collins et. al (forthcomming) in that a frame
will be selected on the basis of local evidence,
and corrections will be made if it proves
ffecepsary. We will see however, that there ate
still a lot of problems with this, poSition which
do not at first glance meet the eye.
</note>
<sectionHeader confidence="0.94187" genericHeader="method">
/ THE CLUE INTERSECTION METHOD
</sectionHeader>
<bodyText confidence="0.997812285714286">
Rather than immediately presenting my scheme,
let me start by showing the problems with an
alternative possibility, which I will call the
&amp;quot;clue intersection&amp;quot; method. This alternative is
by no means a straw man as one researcher has in
fact explicitly suggested it (Fahlman 1977) and I
for one find it a very natural way of thinking
about the problem.
The idea behind this method is that we are
given certain clues in tho story about the nature
of the correct frame, and to find the frame we
simply intersect the passible frames associated
with each clue. TO set how this might work let us
take a close laak at the following example
As Jack walked down the aisle he put a can of
tunafish in his basket.
The clues here are things like &amp;quot;aisle&amp;quot;, &amp;quot;tunafish&apos;
etc. Of course, I do not mean to say that it is
the English words which are the clues, but rather
the concepts which underlie the words. I will
assume that we go from one to the other via an
independent&apos; parsing algorithm. (However this
assumes that there Is no vicious interaction
between frdme docermination and disambiguation.
Given that dfsadbiguation depends vu prior frame
determinatiop (see (Hayes 1977) for numerous
examplea) this may be incorrect.) So the input to
the frame determinPr will be something like
</bodyText>
<note confidence="0.568953">
ST-1 (WALK JACK-1 AISLE-1)
ST-2 (PERSON JACK-i)
ST-3 (EQUAL (NAME JACK-1) &amp;quot;JACK&amp;quot;)
</note>
<sectionHeader confidence="0.5681855" genericHeader="method">
ST-4 (EQUAL (SEX JACK-1) MALE)
ST-5 (AISLE AISLE-1)
</sectionHeader>
<subsectionHeader confidence="0.166541">
ST-6 (PUT JACK-1 TUNA -FISH-CM -1 BASKET-1)
</subsectionHeader>
<bodyText confidence="0.127245">
ST-7 (BASKET BASICET-1)
</bodyText>
<listItem confidence="0.492553">
• • •
</listItem>
<bodyText confidence="0.991623768292683">
The details of the representation do not figure in
the paper, and those which do are fairly
uncontroversial. An exception here is the use of
specific predicates like BASKET or AISLE. We will
return to this point in the conclusion.
Given this representation we can imagine one
method of finding the appropriate frame. Our
clues are the various predicates in the input,
such as as AISLE, BASKET, etc. Index under each
of them will be pointers to those places where it
comes up. Under AISLE we might find CHURCH,
THEATER, and SUPERMARKET, while BASKET will have
LITTLE-RED-RIDING-HOOD, and SUPERMARKET. The
point is that none of these clues will be
unambiguous, but when we take the intersection the
gnly thing which 4111 be left is SUPERMARKET.
There are, however, problems with this view
of things. For one thing it ignores what I will
call the &amp;quot;clue selection&amp;quot; problem. Put in the
plainest fashiqn the diffictdtiy here is deciding
exactly what clues we will hand over to the clue
resolution component, and in what order. So in
the last eXample I selected some of the coetent of
the sentence to hand over to the clue resolver,
in particular AISLE, ana BASKET. This seemed
reasonable given that they do tend to suggest
supermarket&amp;quot;, as desired. But there is more
information in the sentence. It was Jack who did
all of this. Why not intersect what we know about
Jack with all of the rest, or WALK? Or again-
suppose something ever so slightly odd happam,
such as the basket hitting a acreudriver which is
on the floor. SCREWDRIVER will have various
things indexed under it, but more likely than not
the intersection with the rest of the items
mentioned above will give us the null set. For
that matter, is there any reason to only intersect
things in the same sentence? The answer here is
clearly ne, since there are many examples which
require just the opposite.
Jack was walking down an aisle. He was
pushing his basket.
But if we do not stop a sentence bow:dries where
do we stop&apos; It is ridiculous to go through the
entire story coDleoting clues and then do a grand
intersection at tbe end.
A reasonably natural solution to the clue
selection problem tvotdd start with the observation
that usually we already have a general frame.
When new clues come in we see if they are
compatible with what we already believe. If se,
fine. If not, we see if the clue suggests a
different context frame. If not (as with, gay,
WALK which occures so often as to be unsuggestive)
then nothing more need be done. If there are
newly suggested context frames they shOsid be
investigated. This will be done for every
predicate. Now the clue intersection method is
compatible with this idea, but in Its broad
outline we are moving closer to what I have been
characterizing as the Minsky proposal.
Furthermore, there are some problems with the
clue intersection method which go beyond the mere
suggestive. Consider the fallowing example
Jack took a can of tunafish from the shelf.
Then he turned on a light.
After the first line the intersection method
shou/ leave us undecided between KITCHEN and
SUPERMARKET. The next line should resolve the
Issue, but how is it that it does so? It must
have something to do with the fact that normally a
sbopper at a store would nof be the person to turn
lights on or off, while it would be perfectly
normal for Jack to do it in what presumably is his
own kitchen. But this sort of reasoning is not
easily modeled by clue intersection because it
would seem to depend on making inferences which
are themselves dependent on having the context
frames available. That is to say, before we can
rule out SUPERMARKET, we need some piece of
information from the SUPERMARKET frame which will
enable us to say that Jack should not be turning
</bodyText>
<page confidence="0.996998">
188
</page>
<bodyText confidence="0.990549473684211">
on a light, given that he is cast in the role of
SHOPPER in that frame.
Interestingly enough, Fahlman (who I earlier
noted is a proponent of the clue intersection
method) had a major role in the evolution of the
Minsky proposal whic4 I advocate. As such it
behoves us to consider why he then rejected the
idea in (Fahlman 1977). His primary reason is his
observatiob that frequently in vision one does not
Pave eloy ;tingle clue which could serve as the
basis for the first guess at the appropriate
frame. Rather it would seem that one has a
multitute of very vague features, each one of
which could belong to a wide variety of objects or
scenes. TO select one of them for a first guess
would be quite arbitrary and would involve one in
an incredible amount of backtrack. It would seem
much more plausible to simply do an intersection
on the clues and in this way weed out the obvious
implausibilites.
While this analysis of the situation in
vision is quite plausibile. I estimate that high
level vision is still in a sufficiently
rudimentary state that these conclusions need not
be taken as anything near the final word.
Furthermore, even if it were proved that vision
does need an intersection type process, I can
easily believe that the process which goes on in
vision is not the &apos;same as that which goes on in
language. For one thing in vision there is a
natural cut-off for clue selection - the single
scene. For another, within the scene there is a
natural metric on the likelyness of two features
belonging to the same frame - distance. Weither
or not these in fact work in vision, they do
suggest why someone primarily worried about the
vision problem would not see clue selection as the
problem-it appears to be in language.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="method">
3 DIFFERENT KINDS OF INDICES
</sectionHeader>
<bodyText confidence="0.996065842105263">
As I have already said, the scheme I believe
:an surmount the difficulities presented in the
Last section is a variant on one proposed by
qinsky, and elaborated by Fahlman (1974) and
Kuipers (1915). The basic idea is that one major
feature or clue is used to select an initial
frame. Other facts are then interpreted in light
this frame. If they fit, fine. If ngt then
another frame must, be found which either
complements or replaces the original frame. In
the previous propolsals the original frame
contained information about alternate frames to be
tried in case of certain types of
incompatabilities. This may or may not work in
vision (which was the primary concern of those
mentioned earlier) however I shall drop this part
of the theory. In discourse there are simply too
many ways a frame can be inappropriate to make
this feasible. For example, it stretches
credibility to believe that SUMRMARKET would
suggest looking at KITCHFN in the case the shopper
turns on the lights.
So let us consider a very simple example.
Jack walked over to the phone. He had t? talk
to Bill.
It seems reasonable to assume that we guess even
before the second sentence that Jack will make a
call. To anticipate this we must have TELEPHONING
indexed under TELEPHONE. When we see the first
line we first try to &apos;integrate it into What we
already know. Since there will be nothing there
to integrate it ineo, we try to construct
something. TO do this we look to see what we have
Indexed under TELEPHONE, find TELEPHONING, and try
that out. Indeed it will work quite well, since
onq, of the things under.TELEPHONING is that the
AGENT must be in the preximity of the phone, and
Jack just accomplished that. Hence we are able to
integrate (AT JACK-I TELEPHONE-I) into the
TELEPHONING frame, and everything is fine.
Nothing is ever really this simple however,
and even in this example, which has been selected
for its comparative simplicity, there are
complications. I suspect most people have assumed
In the course of this example that Jack is in a
room, and perhaps have even gone so far as to
assume he is at home. Nothing in the story says
so of course, and if the next line went on to say
that Jack put a dime into the phone we would
quickly revise our theory.
To account for our tendencydto place Jack in
room, we must have a second index under
TELEPHONE which points to places where phones are
tyWally found. (An possible alternative is to
have this stated under TELEPHONING but this would
make it difficult to use the Information in cases
whore no call is actually being made, so
TELEPHONING, even if hypothesized, would not stay
around long.) So we will hypothesize two kinds of
indices, an ACTION index and a LOCATION index.
This .distinction should mirror the intuitive
difference between placing and object in a typical
local and placing an action in a typical sequenc*.
Other distinctions of this sort exist and may well
lead to the introduction of other such index
types locating objects and actions in time ftlr
example. However I would anticipate that the
total number is small (under 10, say).
TO illustrate how these index types might
hook up to TELEPHONE I will use a slightly
extended version of the frame representation
introduced in (Charniak 1977) and (Charniak
forthcomming). From the point of view of this
paper nothing is dependent on this choice. It is
simply to give us a sepecific notattnfl with Which
to work.
</bodyText>
<page confidence="0.99846">
189
</page>
<bodyText confidence="0.969709333333333">
(TELEPHONE (OBJECT) ;The frame describes an OBJECT
;(and not, say, an event).
VARS:(THING) ,I only introduce one variable
;THING which is bound to the
;token in the story repre-
,senting the phone
</bodyText>
<sectionHeader confidence="0.70105" genericHeader="method">
LOCATION: ((ROOM (HOME-PHONE . THING))
(PUBLIC-LOC (PAY-PHONE . THING)))
</sectionHeader>
<bodyText confidence="0.9951745">
;If we instantiate the ROOM frame then the
;HOME-PHONE variable in it should be bound
;to the token which is bound to THING.
;Similarly for PUBLIC-LOC and PAY-PHONE.
</bodyText>
<sectionHeader confidence="0.577015" genericHeader="method">
ACTION:UTELEPHONING (PHONE . THING)))
</sectionHeader>
<bodyText confidence="0.997920363636364">
...) ,Other portions of the frame would
;describe its appearance, etc.
We will not be able to integrate the first
line of our story into any other frame, so we will
hypothesize the TELEPHONING frame and either the
room frame or the public place frame. Given my
subject data on what people assume, the room frame
is placed, and hence tried, first. This will
cause the creation of two new statements which
serve to specify the frames now active, and their
bindings
</bodyText>
<equation confidence="0.676713666666667">
(TELEPHONING (PHONE . TELEPHONE-1))
tROOM (ROOM . ROOM-1)
(HOMELPHONE . TELEPHONE-1))
</equation>
<bodyText confidence="0.999014">
The syntax here is the name of the frame followed
by dotted pairs (VARIABLE . BINDING). Earlier I
uaed a place notation for simplicity, e.g.,
</bodyText>
<subsectionHeader confidence="0.46485">
(TELEPHONE TELEPHONE-1)
</subsectionHeader>
<bodyText confidence="0.970665285714286">
In fact this would be converted internally to the
dotted pair format:
(TELEPHONE (THING . TELEPHONE-1))
I might note that my variables are what Minsky
(1975) calles &amp;quot;slots&amp;quot;. They are also equivalent
(to a first approximation) to KRL slots such as-
HOME-PHONE in.
</bodyText>
<equation confidence="0.778026666666667">
[ROOM-1 (UNIT)
&lt;SELF (a ROOM with
HOME-PHONE = TELEPHONE-i)&gt;)
</equation>
<bodyText confidence="0.999865928571429">
So we are hypothesizing 1) an instance of
telephoning, where the only thing we know about it
is the telephone involved, and 2) a room (ROOM-1)
which at the moment is only furnished with a
telephone. Note &amp;hat this assumes that in our
room frame we have an explicit slot for a
telephone. This is equivalent to assuming that
rooms typically have phones in them.
We can now integrate the fact that Jack is at
the phone into the telephoning frame, assuming
that this state is explicitly mentioned there
(i.e. we know- that as part of telephoning the
AGENT must be AT the TELEPHONE). With this added
our TELEPHONING statement will now be:
</bodyText>
<sectionHeader confidence="0.386368" genericHeader="method">
(TELEPHONINO (AGENT . JACK-1)
</sectionHeader>
<subsectionHeader confidence="0.311004">
(TELEPHONE . TELEPHONE-1))
</subsectionHeader>
<bodyText confidence="0.99983325">
wnen tne secona Ilne comes lu we must see how this
fits into the TELEPHONING frame, but this is a
problem of integration. The frame determination
problem is over for this examnle.
</bodyText>
<sectionHeader confidence="0.986621" genericHeader="method">
CONSTRAINTS ON THE HYPOTHESIS OF NEW FRAMES
</sectionHeader>
<bodyText confidence="0.997561">
Early on we noted that it was only necessary
to worry about a new frame if we received
information which did not fit in the old ones.
Then when we introduced the two kinds of indecfes
we noted that we wanted to place events in a
sequence of events, and objects in their Typical
local. This immediately suggests that when we ,et
an unintegratable action we use the ACTION index
on the predicate, while for objects we would use
the LOCATION index.. However, this is not general
enough in at least two ways.
For one thing, often we will have a
non-integratable action where it is not the action
frame, but rather the objects involved if&apos; the
action which suggest the appropriate frame. Our
example of someone going over to a phone is a case
in point. Here CO tells us nothing, but TELEPHONE
is quite suggestive. To handle this the search
for ACTION indices must include those which are on
OBJECT frames describing the tokens involved in
the action. So since Jack is going to something
which is a telephone, we look on the ACTION index
of TELEPHONE.
We must also extend out analysis to handle
states. If we are told that Jack is in a
restaurant we must activate RESTAURANTING. In our
current analysis (RESTAURANT (THING .
RESTAURANT-1)) will not do this since it is an
OBJECT frame and hence will only be looking for
LOCATIONS in which the restaurant will fit. Hence
in this case the IN frame must act like the GO
frame in looking for ACTION indecies in which it
might fit. More generally, any state which is
typically modified by an action should cause us to
look for ACTION indicies. So IN or7STICKY -ON
would do so, SOLID or AGE would not. (But if
the case at hand we are told that something did
change the SOLID status then we would treat it
like an action, as in &amp;quot;In the morning the water in
the pond was solid&amp;quot;.
Up to this point then the frame selection
process looks like this:
1) When a statement comes in try to integrate
it into the frames which are already active.
In general this can require inference and a
major open problem is blow much inference one
performs before giving up. If the
integration ts successful, then go on to the
next statemeua..
</bodyText>
<listItem confidence="0.6527668">
2) If-the statement is a description of an
objeet. (i.e. an OBJECT frame) then use the
LOCATION index on the frame to find a&amp;quot;frame
which incorporates the statement. Keep
track of yet untried suggested LOCATION
frames.
3) If the statemept is an action or changable
state, then look for an ACTION frame into
which the action (or state) can be
integrated. First look on the frame for the
</listItem>
<page confidence="0.644626">
190
/0
</page>
<bodyText confidence="0.998836642857143">
action (or state) and then on ihe object
frames describing the arguments of the
action (or state). Again, keep track of any
remaining ones.
4) There must be a complicated process by which
we test frames for consistency with what we
know about the story already. If it is not
consistent we must involve an even more
complicated process of deciding which is
more believable, previous hypothesis about
the story, or the current frame. I have
nothing to say on this aspect of the
problem.
There is however one type of example which
raises some doubts about the above algorithm.
These mention some object With associated ACTION
frames, but only in connection with states which
dm not demand an ACTION frame for their
integration. For example:
The car was green. Jack had to be home by
three.
In this example the above algorithm will not
consider DRIVING because GREEN will not demand
that we look at the action index assoicated with
its arguments (the car). (Even if it did nothing
would happen because the fact that the car is
green would not integrate into DRIVING.) However,
much to my surprise, when I gave this example to
pdbple they did not get the DRIVING frame either.
However, with a modified example they do.
The steering wheel was green. Jack had to be
home by three.
This is most mysterious. One suggestion (Lehnert
personal communication) is that to &amp;quot;see&amp;quot; the
steering wheel the &amp;quot;viewer&amp;quot; must be in the car,
which inturn suggests driving (since IN would
demand action integration). This may indeed be
correct, but we must then explain Why in the first
example the fact that the viewer must be NEAR the
car does not cause the same thing. In any case
however, these examples are sufficiently odd that
it seems inadvisable to mold a theory around them,
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="method">
5 MORE COMPLEX INDICES
</sectionHeader>
<bodyText confidence="0.990670791666667">
There is one way in which the telephone
example makes the problem look simpler than it is.
In the case of TELEPHONE it seems reasonable to
have a direct link between the object TELEPHONE
and the context frame TELEPHONING. In other cases
this is not so clear. For example, we earlier
consider the example:
The woman waved &apos;s the man on the stage sawed
her in half.
Here it would seem that the notion of sawing a
person in half is the crutial concept which leads
us to magic, although the fact that the woman does
not seem concerned, and the entire thing is
happening on a stage certainly help re-enforce
this idea. But presulably the output of our
parser will simply state that we have here an
incident of SAWING. Does this mean that we have.
under SAWING a pointer to MAGIC-PERFORMANCE9 At
first glance this seems odd at best. Some other
examples where the same problem arise ere:
The ground shook.
(EARTHQUAKE) (Example due to J. DeJong)
There were tin cans and streamers tied to the
car. (WEDDING)
There were piedes of the fusilage scattered
on the ground. (AIRPLANE ACCIDENT)
In the anal analysis the real problem here is one
of efficiency. If, for example we attach
EARTHQUAKE to EARTH, then we will be looking at it
in many circumstances when it is not applicable.
(The alternative of attaching it to SHAKE is
little better, and possibly worse since it would
not handle &amp;quot;Jack felt the earth MOVE beneath him&amp;quot;
assuming the average person gets EARTHQUAKE out
of this also.)
One way to cut down the number of false
suggestions is to complicate the indices we have
on each frame. So far they have simply been lists
of possibilities. Suppose we make them
discrimination nets. So, under SAWING we would
have various tests. On one branch would appear
MAGIC-PERFORMANCE, but we would only get to it
after many tests, one of which, would see if the
thing saWed was a person. In much the same way
the discrimination net for EARTH could enquire
about the action or state which caused us to
access it. If it were a MOVE with the EARTH as
the thing moved then EARTHQUAKE.
Note however that if there were few enough
things attached to SAWING our net would not save
significant time. Even if we were to access the
MAGIC-PERFORMANCE frame the first thing we would
do is check that the thing proposed for the
SAWED-PERSON variable was indeed a person. The
net only saves time when a single test in the net
rules out a number of frames. At the present time
I have not thought of enough frames associated
with SAWING to make this worth while. But as.&apos;I
suspect this is primarily do to leek of work on my
part. I will assume that discrimination nets will
be required.
If we allow a discrimination net to ask
arbitrary questions there will be the problem that
it may ask questions which are not yet answered in
the story. However a reasonable restriction which
would prevent this would go as follows. Suppose
statement A causes us to look at frames on an
index of B. The discrimination net may only
enquire about the predicate of A (EARTH looks to
see if A was a MOVE), and what object frames
describe the arguments of A or B. (SAW looks to see
if the thing sawed was a PERSON).
</bodyText>
<sectionHeader confidence="0.992578" genericHeader="method">
6 OTHER USES OF FRAME DETERMINATION
</sectionHeader>
<bodyText confidence="0.9979672">
Earlier I noted that integrating a statement
into a frame requires inference. Here I would
like to point out that 4 modification of the above
ideas would be helpful in this process as well.
Consider the following:
</bodyText>
<page confidence="0.999294">
191
</page>
<tableCaption confidence="0.9319132">
Jack went to a restaurant. The menu was in
Chinese. &amp;quot;What will I do now&amp;quot;, thought Jack.
Our rules here will get us to RgSTAURANTING after
the first line. But if we are to understand the
significance of the last line we must realize the
import of line two; Jack can&apos;t read the menu. It
would seem unlikely that RESTAURANTING would ask
about the language of the menu, hence sentence two
cannot be immediately integrated into
RESTAURANTING. More reasonable would be to know
that if something is in a foreign language it
cannot be read, and one normally reads the menu so
one can order. Only the second of these can
plausibly be included in RESTAURANTING.
Given our algorithm the following will occur.
The second line will become something like
(IN-LANGUAGE MENU-1 CHINESE). Since the statement
is not integrated we look to see if there is an
ACTION pointer on IN-LANGUAGE. Indeed there is,
and it will be to the following rule.
</tableCaption>
<figure confidence="0.718182333333333">
(READ (MOTIVATIONAL-ACTIVITY)
VARS:
EVENT:
(AND
(SEE READER READING-MATERIAL)
(IN-LANGUAGE READING-MATERIAL LANGUAGE)
</figure>
<sectionHeader confidence="0.848180666666667" genericHeader="method">
(KNOW READER LANGUAGE))
ENABLES
(KNOW-CONTENTS READER READING-MATERIAL))
</sectionHeader>
<bodyText confidence="0.999798444444445">
In effect we are saying here that the typical
signficance of something being in ,a certain
language is whether a person can read it or not.
This will,, cause us to activatve the READ frame.
Initially there is little else we can do since at
this point the we do not even know who is trying
to read. However When we try to integrate READ we
will be successful, and we will have further bound
READER to JACK-1. At this point (and this is the
modification required) we should return to READ
and note that we can assume he does not know
Chineese and hence will not be able to read ,the
menu.
Early on I commented that the only
controversial aspect of my representation was the
use of very specific predicates (BASKET, AISLE,
TELEPHONE, etc) rather than a break down into more
primitive concepts. We might, for example, define
AISEL as a path which is bounded on each tide by
things which are considered pieces of furniture
(e.g., sgelves or chairs). The problem with using
a primitive representation here is that vane it
is somewhat plausible having SUPERMARKET and
CHURCH indexd under AISLE, indexing them under
PATH or some other component of the primitive
definition is much less plausible. However, we
can circumvent this problem by the use of
discrimination nets, just as we did to get
EARTHQUAKE from MOVE and EARTH. But, It should be
noted that by using this method we are eliminating
one of the benefits of a primitive analysis - we
can no longer assume that we can get our
information in a piecemeal fashion and come out
with the same analysis. In partieular we must get
&amp;quot;aisle&amp;quot;, or else we must get all of its components
at the same time. If we do not then the
discrimination net will fail to notice that we do
not have any old path, we have an AISLE. Given
this restriction the primitive and non primitive
analyses come out pretty much the same. A
primitive decomposition just becomes a long name
for a higher level concept. Or to turn this
around, the use of high level discriptions is not
so controversial after all - it is simply a short
name for a primitive decomposition.
</bodyText>
<sectionHeader confidence="0.979575" genericHeader="method">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9945276">
I have benefited from conversations with J.
Carbonelle, J. DeJong W. Lehnert, D. McDermott,
and R. Wilensky, allAf Whom have been thinking
about these problems for a long time. Many of
their ideas have gone into this paper. This
research was done at the Yale A.I.4 Project which
is funded in part by the Advanced Research
Projects Agency of the Department of Defense and
monitored under the Office of Naval Research under
contract N00014-75-C-1111.
</bodyText>
<sectionHeader confidence="0.993086" genericHeader="conclusions">
7 COWLUSION
</sectionHeader>
<bodyText confidence="0.998458294117647">
There is, or course, Bloch I have not covered.
The most glaring ommision is the lack of any
discussion of how one detects a discrepency
between a suggested frame and what we already know
-45f the story. The problem is that a frame cannot
afford to mention everything which is incompatable
with it - there is simply too much. And the same
is true for everything Which is compatable.
Furthermore, what would be enough to switch to a
new frame under some circumstances would not be
sufficient at other times. So &amp;quot;Jack walked down
the isle and picked up a can of tunafish&amp;quot; takes us
from CHURCH to SUPERMARKET. But if we added &amp;quot;from
a pew&amp;quot; things are different. These are malor
problems and aside from (McDermott 72) and
(Collins et. al. forthcomming) they have hardly
been confronted, much less solved.
</bodyText>
<sectionHeader confidence="0.998036" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9544852">
Charniak, E., A framed PAINTING. on the
representation of a common sense knowledge
fragment. Journal of Cognitive Science, 1,
4, August 1977.
Charniak, E., On the use of framed knowledge in
language comprehension, forthcomming.
Collins, A, Bromn, J. S., and Larkin, IC M.,
Inference in text understanding, in: R. J.
Spiro, B. C. Bruce, and W. F. Brewer
(Eds.) Theoretical issues in reading
comprehension. Hillsdale, N. J., Lawrence
Erlbaum Associates, forthcomming.
Fahlman, S. E., A hypothesis-frame system for
recognition problems, Working Paper 57,
M.I.T. Artificial Intelligence Lab, 1974.
Fahlman, S. E., A system for representing and
using real-world knowledge. Unpublished
Ph.D. thesis, M.I.T., September 1977.
Hayes, P. J., Some association-based techniques
for lexical disambiguation by machine.
</reference>
<page confidence="0.997128">
192
</page>
<note confidence="0.735103">
TR25, University of Rochester Lomputer
Science Department, June 1977.
</note>
<figure confidence="0.903733666666667">
Kuipers, B., A frame for frames, In D. Bobrow and
A. Collins (Eds.) Representation and
understanding, New York, Academic Press,
1975
McDermott, D., Assimilation of new information by
a natural language understanding system, TR
291, M.I.T Artificial Intelligence Lab,
1972.
Minsky, M., A framework
psychology oIn clher
McGraw-Hill, 1975, pp.
for representing
Winston (Ed.), The
vision, New York,
211-277.
</figure>
<page confidence="0.802937">
193
</page>
<figure confidence="0.559677">
Fragments of a Theory
F Human Plausible Reasoning
Allan Collins
Bolt Beranek and Newman In..
</figure>
<page confidence="0.788078">
13
</page>
<sectionHeader confidence="0.971287" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999915904761905">
The paper outlines a computational theory
of human plausible reasoning constructed from
analysis of people&apos;s answers to everyday
questions. Like logic, the theory is
expressed in a content-independent forbalism.
Unlike logic, the theory specifies how
diffel.ent information in memory affects the
certainty of the conclusions drawn The
theory consists of a dimensionalized space of
different inference types and their certainty
conditions, including a variety of
meta-inference types where the inference
depends on the person&apos;s knowledge about his
own knowledge. The protocols from people&apos;s
answers to questions are analyzed in terms of
the different inference types. The paper also
discusses how memory is structured in multiple
ways to support the different inference types,
and how the information found in memory
determines which inference types are
triggered.
</bodyText>
<sectionHeader confidence="0.996125" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9791923125">
The goal of this paper is to briefly
describe a theory of human plausible reasoning
I am currently developing (Collins, 1978)
The theory is a procedural theory and hence
one, which can be implemented in a computer, as
parts of it have been in the SCHOLAR and
MAP-SCHOLAR systems (Carbonell &amp; Collins,
1973; Collins &amp; Warnock, 1974, Collins,
Warnock, Aiello &amp; Miller, 105). The theory
is expressed in the production-rule formalism
of Newell (1973)., Unlike logic, the theory
specifies how different configurations of
information affect the certainty of the
conclusions drawn. These certainty conditions
are in fact the major contribution of the
theory.
</bodyText>
<subsectionHeader confidence="0.839222">
Methodology of Constructing.pe Theory
</subsectionHeader>
<bodyText confidence="0.9998865">
To construct a theory of human plausible
reasoning, I collected about 60 answers to
exeryday questions from 4 different subjects.
The questions ranged from whether there are
black princess phones to when the respondent
first drank beer
The analysis of the prot000ls attempts to
account for the reasoning and the conclusions
drawn in the protocols in terms of 1) a
taxonomy of plausible inferenoe types, 2) a
taxonomy of default assumptions, and 3) what
the subject must have known a priori As will
be evident, this is an inferential analysid
I am trying to donstruct a deep structure
theory from the surface structure traces of
the reasoning process.
The protocols have the following
characteristics.
</bodyText>
<listItem confidence="0.98372775">
1) There are usually several different
inrerence types used to answer any
question.
2) The same inference types recur in many
different answers.
3) People weigh alL, the evidence they find
that bears on a question.
4) People are more or less certain depending
</listItem>
<bodyText confidence="0.896307037037037">
on the certainty of the information, the
certainty of the inferences, and on whether
different inferences lead to the *same or
opposite conclusions
can illustrate some of these
characteristics of the protocols as well as
several of the inference types in the theory
with a protocol taken from a tutorial session
on South American geography (Carbonell &amp;
Collins, 1973)*
(T) There is some jungle in here (points to
Venezuela) but this breaks into a savanna
around the Orinoco (points to, the Llanos
in Venezuela and Colombia).
(8) Oh right, that is where they grow the
coffee up there&amp;quot;
(T) I don&apos;t think that the savanna is used for
growing coffee. The trouble is the
savanna has a rainy season and you can&apos;t
count on rain in general. But I don&apos;t
know. This area around Sao Paulo (in
Brazil) is coffee region, and it is sort
of getting into the savanna region there.
In the protocol the tUtor went through
the following reasoning on the question of
whether coffee is grown in the Llanos.
Initially, the tutor made a hedged &amp;quot;no&amp;quot;
</bodyText>
<page confidence="0.998109">
194
</page>
<bodyText confidence="0.999971537037037">
response for two reasqns. First,&apos; the tutor
did not have Stored that the Llanos was used
for growing coffee. Second, the tutor knew
that coffee growing depends on a number of
factors&apos; (e.g,, rainfall, temperature, soil,
aneterrain), and that savannas do not have
the correct value for growing coffee on at
least one of those factors (i.e., reliable
rainfall). However, the tutor later hedged.
his initial negative response, because he
found some positive evidence. In partibular,
he thought the Brazilian savanna, might overlap
the coffee growing region in Brazil around Sao
Paulo and that the Brazilian savanna might
produce coffee. Thus by analogy the Llanos
might also produce coffee. Hence, the tutor
ended up saying &amp;quot;I don&apos;t know.&amp;quot;
The answer exhibj.ts a number of the
important aspects of the protocols. In
general, a number of inferences are used to
derive&apos; an answer. Some of these are inference
chains where the premise of one inference
depends on the conclusion of another
fnference. In other cases the inferences are
independent sources of evidence. When there
are different sources of evidence, the subject,
weighs them together to determine his
conclusion.
It is also apparent in. this protocol how
different pieces of information are found over
time. What appears to happer is that- the
subject launches a search for relevant
information, (Collins &amp; 4oftus, 1975). As
relevant pieces of information are found (or
are found to be missing), they trigger
particular inferences. The type of inference
applied is determined by the relation between
the information found and the queshion asked.
For example, if the subject knew that savannas
are in general good for growing coffee, that
would trigger a ceduction. If the subject
knew of one savanna somewhere that produced
coffee, that Would trigger an analogy. The
search for information ts such that the most
relevant information is found first. In the
protocol, the more relevant information about
the unreliable rainfall in savannas was found
before the more far fetched information about
the coffee growing region in Brazil and its
relation to the Brazilian savanna. Thus,
information seems to be found at different
times by an autonomous search process, and the
particular information found determines
inferences that are triggeted.
</bodyText>
<sectionHeader confidence="0.661805" genericHeader="method">
THE THEORY
</sectionHeader>
<bodyText confidence="0.995810285714286">
The theory specifies a large numper ox
different inference types, together with the
conditions that affect the certainty of each
inference type. In the theory the different
types of inference are arrayed in a five
dimensional space.
The dimensions of the inference space
</bodyText>
<listItem confidence="0.608307666666667">
are:
(1) Inferences on.gnowledge vs Inferences on
Meta-gnowledge
</listItem>
<bodyText confidence="0.999928230769231">
There are inference patterns based on
people&apos;s knowledge, such as deduction and
induction, and inference patterns based on
people&apos;s knowledge about their own- or other&apos;s
knowledge (i.e. meta-knowledge) (Brown, 1977),
such as lack-of-knowledge and confusability
inferences. I refer to these latter as
meta-inferences They are ubiquitous in the
protocols, and yet they fall outside the scope
of most theories of logic. The other four
dimensions refer to the space of inferences
but may also partially apply to the space of
meta-inferences.
</bodyText>
<listItem confidence="0.847397">
(2) Functional. vs Set Inferences
</listItem>
<bodyText confidence="0.99990503030303">
For ehch type of inference, there is a
functional variation and a set variation. The
set variation involves mapping the property of
one set (which may be a single-member set or
instance) onto another set. The functional
variation has an additional premise that the
property to be mapped (the dependent variable)
depends on other properties the independent
variables). The mapping of the property from
Dne set to ancther makes use of this
functional dependency. The set variation, in
fact, is a degenerate ford of the functional
variation, which is used when people have
little or no knowledge of the functional
dependencies involved:
People&apos;s knowledge about functtonal
dependencies consists of a kind of directional
correlation. A judgment about whether a place
can grow coffee might depend on factors that
are causal precursors for coffee growing
(e.g., temperature), correlated factors (e.g.,
other types of vegetation)., or factors
causally subsequent to coffee growing (e.g.,
export trade) For example, one might decide
a place does not produce coffee, because it
produces apples which seem incompatible with
coffee, or because there is little export
trade from the region. The directional nature
of the correlation shows up in the last
example. A region easily could have export
trade without producing coffee, but it would
be unlikely that a region would produce coffee
without having export trade.
</bodyText>
<listItem confidence="0.968354">
(3) Semantic. Spatial_, vs Temporal Inferences
</listItem>
<bodyText confidence="0.999618764705882">
For each type of inference, there is a
semantic, spatial, or temporal variation of
the inference. Semantic inferences involve
mapping properties across semantic space,
spatial inferences across Euclidean space, and
temporal inferences across time. These are
treated as different types of inferences in
the theory because the proftedures for
cod)outing them are somewhat different.
Semantic inferences are based on information
structured in a semantic or conceptual memory
(Quillian, 1968; Schenk, 1972). Spatial
inferences are based on information (or
images) derived from a spatial structure
(Collins &amp; Warnock, 1975; Kosslyn &amp; Schwartz,
1977). Temporal ihferences are based On
information derived from an event (or
</bodyText>
<page confidence="0.998402">
195
</page>
<bodyText confidence="0.476159333333333">
miasmic) structure (Tulving, 1972).
Wrrelates of each of these types of memory
itructures are found in Winograd&apos;s SHRDLU
(1972).
tA) Stwerordinate sets, similar sets. vs.
pnberdinate sets
Inferences can involve mapping properties
Moe gunerordinate sets, similar sets, or
OMPOrdinate sets The property can be mapped
grow one set or from many sets (either
exhaustively or not). The different kinds of
Mappings delineated in the theory are:
</bodyText>
<figure confidence="0.866823416666667">
(a) Peduction (Superordinate Inferences) maps
properties of the set onto subsets.
() Analogy (SimiLarity Inferences) maps
properties from one set to a siwilar set.
te) Inductlon maps properties of subsets of a
set onto other subsets.
(4) Gensralization (proof-by-cases) maps
properties of subsets of a set onto the
set.
(0) AbdUctIon maps a subset with the same
property as some set into the set
(151 Positive vs. Negative Inferences
</figure>
<bodyText confidence="0.9850885">
Each type of inference has both a
positive and negative version, depending on
gmether the mapping involves the presence or
gmsence of a property
</bodyText>
<subsectionHeader confidence="0.960582">
Osumntions of the Theory
</subsectionHeader>
<bodyText confidence="0.981831125">
The theory rests on a number of
heeumptions about the way information is
&amp;presented and processed by people. I will
Oesoribe briefly what these assumptions are.
&amp; Harris, 1978). Such an event memory was
constructed by Winograd&apos;s SHRDLU (1972) to
record the movements of blocks and the goals
they accomplished, in order to answer &amp;quot;why&amp;quot;
and &amp;quot;how&amp;quot; questions about events in the Blocks
World.
Batt:lit/al I assume there are autonomous
Searchprocesses that find relevant
information with respect to any query (Collins
&amp; Loftus, 1975). The search process has
access to semantic, spatial and temporal
infotmation in parallel, and whenever relevant
information of any kind is found, it triggers
an inference (Collins &amp; Quillian, 1972,
Kosslyn, Murphy, Bemesderfer &amp; Feinstein,
1977.) The information found by the search
processes determines what inference patterns
are applied.
Matching Processes. I assume there are
decision processes for determining whether any
two concepts can be identified as the same.
The semantic matching process could he that
proposed by Collins &amp; Loftus (1979) or by
Smith, Shoben &amp; Rips (1974). The spatial
matching process compares places or objects to
decide their spatial relation. Similarly,
there must be a temporal matching process that
determines the relation between two events.
Imoortance and Certainty. I assume that
for each concept and relation a person has a
notion of its relative importance (i.e. its
criteriality), and his degree of certainty
about its truth. In a computer, these could
be stored as tags on Vile concepts and
relations (Carbonelf &amp; Collins, 1973).
Ic
</bodyText>
<sectionHeader confidence="0.94779" genericHeader="method">
EXAMPLES OF INFERENCE RULES AND PROTOCOLS
</sectionHeader>
<subsectionHeader confidence="0.976954">
Semantic Information assume
</subsectionHeader>
<bodyText confidence="0.97493475">
Nnformation about differeat concepts is
&amp;presented in a cross-referenced, semantic
Mtructure (Quillian, 1968; Schenk, 1972). The
nodes in the network are\schemas, which are
the kind of structured objects implied by the
nation of frames (Minsky, 1975) or scripts
Lachank &amp; Abelson, 1977). The links between
MCOOS represent different relations between
the poncepts. The correlate of this kind of
memantio structure in Winograd&apos;s SHRDLU (1972)
lime the cross-referenced information structure
Donstructed by MICROPLANNER.
</bodyText>
<subsectionHeader confidence="0.642849">
Spatial Information. I assume spatial
</subsectionHeader>
<bodyText confidence="0.996323139534884">
Enformation about concepts, such as the size,
Mape, color, or location of objects and
manes, is represented in a spatial structure,
sport frog] but connected to the semantic
mmructure (Collins &amp; Warnock, 1974). The
Eprrelate of such a spatial representation in
mmUMgrad&apos;s SHRDLU (1972) was the Cartesian
representation of the blocks, on the table top.
gvent information. SimilarAy event
rmation is assumed to be stored in a form
p;,eserves its temporal, causal, and goal
lappucture. This requires a hierarchical
!Structure of events and subevents nested
mocording to the goals and subgoals of the
sisters involved in the events (Brown, Collins,
Because it is impossible to present the
entire theory&apos; here, I will give the
formulations., for three types of inference and
show three protocols which illustrate these
three types, as well as others. The three
types are the lack-of-knowledge inference, the
functional analogy, and the spatial superpart
inference. They are all common inferences and
serve to illustrate the different kinds of
inferences in the theory.
The formal analysis of the protocols
attempts to specify all the underlying
Inferences that the subject was using in his
response. For the inferences that bear
directly on the question, I have marked
whether they are evidence for a negative or
positive answer. Where a premise was not
directly stored, but derived from another
inference, I have indicated the inference from
which it is derived. I have indicated the
approximate degree of certainty by marking the
conclusion with &amp;quot;Maybe&amp;quot;, &amp;quot;Probably&amp;quot;, or
leaving it unmarked. Where a subject may be
making a .particular inference which the
protocol does not clearly indicate, I have
marked the inference &amp;quot;possible&amp;quot;. Separating
inferences in this manner is oversimplified,
but has the virtue of being understandable.
</bodyText>
<page confidence="0.994152">
196
</page>
<bodyText confidence="0.9289452">
Larg1k=9&amp;quot;:&amp;quot;Anglaregin--IagrAnSa.
The lack-of-knowledge inference is the
most common of all the meta-inferences. The
protocol I selected to show the
lack-of-knowledge inference shows the subject
using a variety of meta-inferences to reach an
initial conclusion which he then backs off a
bit.
Q. Is the Nile longer than the Mekong River?
JB. I think so.
</bodyText>
<subsectionHeader confidence="0.898084">
Q Why?
</subsectionHeader>
<bodyText confidence="0.999456">
JB Beeauee (pause) in%junior high I read a
book on rivers and I kmpt looking for the
Hudson River because thut. was the river I
knew about and it never appeared, and the
Amazon was in there and the Nile was in
there and all these rivers were in there,
and they were big, and long, and
</bodyText>
<figure confidence="0.532955741935484">
important. The Mekong wasn&apos;t in there
(pause) It could be just
Q. So therefore, it is not important.
JB. That&apos;s right It could be just an
American view. At that time the Mekong
wasn&apos;t so important
Underlving Inferences
1) Functional Abduction on Importance Level
(Possible)
The importance of a .4= depends in part
on how long it is
The 411e is very important
Probably the Nile is extremely long
2) Meta-Induction From Cases
I know the Amazon is extremely long
I know the Nile is extremely long (from
1)
I would know the Mekong is extreeely long
if it were
3) Lack-of-Knowledge Inference
I don&apos;t know the Mekong is extremely long
I would know the Mekong is extremely ionic
kt_i Vete (from 2)
Probably the Mekong is not extremely long
4) Functional Abduction on Importance Level
(Possible)
The importance of a river depends in part
on length
Ihe_MakesA is not verv_imoortant
Probably the Mekong is not extremely long
5) Simple Comparison (Positive Evidence)
</figure>
<subsectionHeader confidence="0.902587">
The Mekong is not extremely long (from 3
</subsectionHeader>
<bodyText confidence="0.887096333333333">
and 4)
The Nile is eXtremelv lona (from 1)
The Nile is longer than the Mekong
</bodyText>
<listItem confidence="0.7610485">
6) Functional. Attribution on Importance Level
(Possible)
</listItem>
<bodyText confidence="0.850209428571429">
The importance of something depends on how
remote it is
The Nile is very important
The Nile is less remote than the
Mekong
Maybe the Nile is more important than the
Mekong because it&apos;s less remote
</bodyText>
<subsectionHeader confidence="0.550845">
7) Functional Alternative on Importance Level
(Negative Evidence) (Possible)
</subsectionHeader>
<bodyText confidence="0.980335083333334">
The importance of a river depends on how
close it is and how long it is
The Nile is more important than the Mekong
teesuse it&apos;s closet (from 6)
Maybe the Nile is not longer than the
Mekong
Contributing to the certainty of Oese
inferences are several mete-ir,feronces working
on importance level. The functional
abductions (1 and 4) are ,suggested by the
subject&apos;s tying length to importance. He
seems to know that importance depends in part
on length, and since he assigns different
meerees of importance to the Nile and the
Mekong, he must be using that in part to infer
that the Mekong is not as long as the Nile.
There also i5 a meta-induction he is making:
that since he knows the Amazon and the Nile
are very long, he would know the Mekong is
long if it were. This meta-induction is
acting on one of the certainty conditions for
the lack-of-Anowledge inference* the more
similar cases stored with the given property,
the more certain the inference Taken
together, these inferences make the
lack-of-knowledge inference very certain.
However at the end the subject backs off
his con?lusion becausd he finds another chain
of reasoning that makes him less certain
(inferences 6 and 7). The idea of
&amp;quot;remoteness&amp;quot; only represents the underlying
argument when interpreted in terms of
conceptual distance What the subject is
really doing is evaluating how remote
Southeast Asia was at the time he was in
junior high (before the Vietnam War). This
notion of remoteness is the outcome of
matching processes The Mekong was remote
because it was far away culturally,
historically, physically, etc. from America.
Based on this the subject realizes that the
Mekong&apos;s lack of importance may be due to this
remoteness rather than its shortness in
length 1.4s reasoning then depends on his
notion of what alternative factors importance
depends on, and how it might mislead him in
this ease. So this chain of reasoning is also
acting on the certainty conditions affecting
the lack-of-knowledge inference, but in the
opposite direction from the other
meta-inferences
The rule for a lack-of-knowledge
inference is shown in the table below. It
generally has the form. If it were true, I
would know about its. I don&apos;t, so it must not
be true. It is computed by comparing the
Importance level of the proposition in
question against the depth of knowledge about
the concepts involved (Collins et al, 1975;
Gentner &amp; Collins, 1978).
</bodyText>
<page confidence="0.8378455">
197
17
</page>
<bodyText confidence="0.477993">
Lack-of-Knowledge Inference
</bodyText>
<listItem confidence="0.807090714285714">
1) If a*person would know about a property for
a given set if it were in a given range,
and
2) if the person does not know about that
property,
3) then infer that the property is not in the
given range for thatset.
</listItem>
<subsectionHeader confidence="0.335546">
Example
</subsectionHeader>
<bodyText confidence="0.991097547619047">
If Kissinger were 6&apos;6&amp;quot; tall, I would know
he is very tall. I don&apos;t, so he must not be
that tall.
Conditions that incpase certaintv:
1) The more important the particular set.
2) The less likely the property is in the
given range.
3) The more information stored about the given
set.
4) The more similar properties stored about
*he given set.
5) The more important the given property.
6) The more information stored about the given
property.
7) The more similar sets stored that have the
given property.
The conditions affecting the certainty of
a lack-of-knowledge inference can be
illustrated by the example in the table:
1) Condition 1 refers to the importance of the
given set. In the example Kissinger is
quite important, so one is more likely to
know whether he is 6&apos;6&amp;quot; than whether
Senator John Stennis is 6&apos;6&amp;quot; for example.
2) Condition 2 refers to the likelihood that
the property is in the given range.
Likelihood affects the inference in two
ways: low likelihood, makes a negative
inference more certain a priori, and low
likelihood also makes a property more
unusual and therefore more likely to come
to a person&apos;s attention. For example, it
is less likely that Kissinger is if 2&amp;quot; than
6&apos; 6&amp;quot;, because 7&apos; 2&amp;quot; is more unusual. If
Kissinger were a basketball player, on the
other hand, his being 6&apos; 6&amp;quot; would not be
unusual at all.
3) Condition 3 relates to the
depth-of-knowledge about the given set.
The more one knows about Kissinger, the
more certainly one would know that he is 6&apos;
6&amp;quot;, if he is.
</bodyText>
<listItem confidence="0.573567846153846">
4) Condition 4 relates to the number of
similar properties stored ,about the set
(i.e. the relatedness of the information
known about the set). If one knows a lot
about Kissipger&apos;s physical appearance, one
feels more certain one would know he is
extremely tall, if he is.
5) Condition 5 relates to the importance of
the particular property. Being extremely
tall isn&apos;t as important as missing a leg
say, so people are more likely to know if
Kissinger is missing a leg.
6) Condition 6 relates to the
</listItem>
<bodyText confidence="0.899243272727273">
depth-of-knowledge about the particular
property. For example, a person who has
particular expertise about the physical
stature of people is more likely to know
that Kissinger is extremely tall, if he is.
7) Condition 7 relates to the number of
similar sets known to have the given
property. For example, if one knows that
Ed Muskie and Tip O&apos;Neil are unusually
tall, then one ought to know that Kissinger
is unusually tall, if in fact he is 6&apos; 6&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.98303">
Functional Analogy
</subsectionHeader>
<bodyText confidence="0.979196733333333">
The initial protocol on coffee growing in
the Llanos illustrated two functional
inferences: a functional calculation
concerning rainfall, and a funct4onal analogy
betwedn the Brazilian- savanna and the Llanos.
One of the more common functional inferences
is the functional analogy. The protocol I
selected to illustrate it contrasts the use of
a simple analogy and a functional analogy.
Q. Can a goose quack?
BF. No, a goose - Well,. its like a duck, but
its not a duck. It can honk, but to say
it can quack. No, I think its vocal cords
are built differently. They have a beak
and everything, but no, it can&apos;t quack.
</bodyText>
<subsectionHeader confidence="0.416019">
Underlying Inferences
</subsectionHeader>
<bodyText confidence="0.752411444444444">
1) Simple Analogy (Positive Evidence)
A goose is similar to a duck
A duck Quacks
Maybe a goose quacks
2) Importance-Level Inequality (Possible)
I know a goose honks
Quacking is as important as honking
Probably I would know about a goose
quacking if it did
</bodyText>
<listItem confidence="0.9815155">
3) Lack-of-Knowledge Inference (Negative
Exidence) (Possible)
</listItem>
<bodyText confidence="0.989910666666667">
L.don&apos;t know that a goose quacks
I would know about a goose quacking if
did (from 2)
</bodyText>
<subsectionHeader confidence="0.337771">
Probably a goose doesn&apos;t quack
</subsectionHeader>
<bodyText confidence="0.875066333333333">
4) Negative Functional knalogy (Negative
Evidence)
The sound a bird makes depends on its vocal
cords
A goose, is different from a duck in its
vocal cords
</bodyText>
<sectionHeader confidence="0.63919" genericHeader="method">
A duck Quacks
</sectionHeader>
<subsectionHeader confidence="0.818757">
Probably a goose doesn&apos;t quack
</subsectionHeader>
<bodyText confidence="0.9999538">
The simple analogy, which is based on a
match Of all the properties of ducks and
geese, leads to the possible conclusion that a
goose can quack, because a duck quacks. This
inrerence shows up in the reference to &amp;quot;its
like a duck&amp;quot; and in the uncertainty of the
negative conclusion the student is drawing.
It is positixe evidence and only shows up to
the, degree it argues against the general
negative conclusion.
</bodyText>
<page confidence="0.965237">
1.98
</page>
<bodyText confidence="0.978627434782609">
I?
The importance-level inequality and
lack-of-knowledge inTerence are suggested by
the sentence &amp;quot;It can honk, but to say it can
quack.&amp;quot; Here knowledge about honking stems to
imply that a goose doesn&apos;t quack. I would
argue that arch an inference has to involve
the lack-of-knowledge inference, since it is
possible that a goose might sometimes honk and
sometimes quack.
The functional analogy is apparent in the
concern about vocal cords, which the subject
thinks are the functional determinants of the
sounds made. I think the sound is determined
by the length of the neck, which is probably
what the subject was thinking of. Honking may
just be quacking resonated through a longer
tube. But in any case, the mismatch the
subject finds on the relevant factor leads to
a negative conclusion which supports the
lack-of-knowledge inference.
The table shows the rule for a functional
analogy.
</bodyText>
<subsectionHeader confidence="0.703037">
Fuoctional Analogy
</subsectionHeader>
<listItem confidence="0.963916777777778">
1) If a dependentivariable depends on a number
of independent variables, and
2) if one set matches another set on the
independent variables, and
3) if the value of the dependent variable for
one set is in a given range,
4) then-tnfer that the value of the dependent
variable for the other set is in the given
range.
</listItem>
<subsectionHeader confidence="0.550003">
Fxample
</subsectionHeader>
<bodyText confidence="0.953616178217822">
The Brazilian savanna is like Llanos in
its temperature, rainfall, soil, and
vegetation. Thus, if the Brazilian savanna
produces coffee, then the Llanos ought to
also.
Conditions that increve certainty:
1) The more independent variables on which the
two sets match, and the fewer on which they
mismatch.
2) The greater the dependency on any
independent variables on which the two sets
match, dnd the less the dependency on any
independent variables that mismatch.
3) The better the match on any independent
variable.
4) The greeter the dependency on those
independent variables that match best.
5) The more certain the dependent variable is
in the given range for the one set.
6) The more likely the value of the dependent
variable is in the. given range a priori.
7) The more certain the independent variables
are in the given ranges for both sets.
I can illustrate the different certainty
conditions for a functional analogy in terms
of the example in the table.
1) Condition 1 refers to the number of
factors on which the two sets match. If
the two regions match only in dtimate
and vegetation, that would be less
strong evidence that they produce the
same products than if they match on all
four variables.
2) Condition 2 refers to the degree the
dependent variable depends on diffdrent
factors that match or mismatch. Coffee
groVing depends more on temperature and
rainfall than on soil or vegetation.
Thus a match on these first two factors
makes the inference more certain than a
match on the latter two factors.
3) Condition 3 relates to the quality 0
the match on any factor. The better the
match with respect to temperature,
rainfall, etc. the more certain the
inference.
4) Condition 4 refers to the degree of
dependency on those factors that match
best. A good match with respect to the
rainfall pattern leads to more certainty
than a good match with respect to the
vegetation.
5) Condition 5 relates to the certainty
that the property is in the given range
for the first set. The more certain one
is that the Brazilian savanna produces
coffee, the more certain the inference.
6) Condition 6 relates to the a priori
likelih9od that the property will be in
the given range. The more likely that
any region grows coffee, the more
certain the inference.
7) Condition 7 relates to the certainty
that the factors are in the given ranges
for both sets. For example, the more
certain that bpth savannas have the same
temperature, etc., the more certain the
inference.
Spatial Sunernart Inference
The theory assumes that spatial
inferences mire made by constructing an image
of the concepts involved, and making various
computations on that image (eollins &amp; Warnock,
1974; Kosslyn &amp; Schwartz, 1977). An example
of a spatial inference occurred in the earlier
protocol about coffee growing, when the
respondent concluded that a savanna might be
used for growing coffee because he thought the
coffee growing region around Sao Paulo might
overlap the Brazilian savanna. This spatial
matching process, which occurs in a variety of
protocols, involves constructing a spatial
image with both concepts in it, and finding
their spatial relationship (e.g., degree of
overlap, relative size or direction) from the
constructed image.
The protocol I selected illustrates a
spatial subpart inference, together with
several other spatial and meta-inferences..
Q. Is Texas east of Seattle?
JB. Texas is south and east of Seattle.
Q. How did you get that?
JB. I essentially looked at a visual image of
the U.S. where I remembered that Seattle
was in Washington and know that its up in
the left corner and I know that Texas is
in the middle on the bottom. Sometimes
you get fooled by things like that, like
for exammle Las Vegas being further west
than San Diego. This case I think we&apos;re
O.K.
</bodyText>
<page confidence="0.994252">
199
</page>
<bodyText confidence="0.981985166666667">
Underlying inferiknwirs
I) Spatial line slope inference
Washington is in upper left corner of the
U.S.
Texas is n the middle bnttom of U.S.
Line from Washington to Texas slopes east.
</bodyText>
<listItem confidence="0.5273965">
?) Spatial subpart inference (Positive
evidence)
</listItem>
<bodyText confidence="0.893522">
Line from Washington to Texas slopes east.
eattle is nart_of WAShington.,.
Line from Seattle to Texas slopes east
</bodyText>
<listItem confidence="0.687814">
3) Meta Analogy (Negative evidence)
</listItem>
<bodyText confidence="0.982505111111111">
People are often mistaken in thinking that
Las Vegas is east of San Diego, because
Las Vegas is inland and San Diego is on
ale Pacific Coast.
Seattle, like San Diego, is on the Pacific
coact.
Texas. like Las Vegas. is inland.
Maybe I am mistaken in thinking that Texas
is east of Seattle.
</bodyText>
<listItem confidence="0.898136">
4) Functional Modus Tollens (Positive
evidence) (possible)
</listItem>
<bodyText confidence="0.999033142857143">
The Pacific coast misconception depends on
the inland planP being north of the
coastal place.
Seattle is on the coast.
Texas is inland.
122Eallat-Etab-41--.&amp;&amp;111..e-u
The Pacific coast misconception does not
apply to Texas and Seattle.
In the protocol the subject constructs a
line from Washington to Texas for the purpose
of evaluating its slope. The constructed line
does slope east, so he answers yes. Implicit
in this protocol is a spatial subpart
inference or spatial deduction, that Seattle
is part of Washington and the slope of the
line found earlier applies to Seattle. This
kind of subpart inference was found to show up
in response time by Stevens (1976).
The sub-ject briefly reconsidered his
conclusion because he thought of the &amp;quot;Pacific
Coast Misconception,&amp;quot; that people mistakenly
think that places inland are always east of
places on the coast. By the meta-analogy in
3, he inferred that maybe Seattle-Texas was
like San Diego-Las Vegas in that the inland
location was west of the coastal location.
Put the subject ruled out the analogy by some
inference such as that shown in 4. Actually,
the functional modus tollens in 4 hides the
spatial processing that the &apos;subject 4trobably
used to rule out the analogy in 3. Probably,
he knew that the reason for the &amp;quot;Pacific Coast
Misconception&amp;quot; has to do with the
southeasterly slant of the Pacific coast. By
knowing that, you can figure out that the
misconception depends on the inland location
being north of the coastal location. I have
finessed the spatial reasoning process by
stating that conclusion as a premise in 4.
The next table shows the rule Cr a
spatial superpart inference (or spatial
deduction).
</bodyText>
<subsectionHeader confidence="0.499469">
Spatial Sunerpart Inference
</subsectionHeader>
<listItem confidence="0.8400102">
1) If a property is in a given range for some
set, and
2) if another set is a subpart of that set,
3) then infer that the property is in that
range for the subpart.
</listItem>
<bodyText confidence="0.953353913043478">
failiaraa.
It is raining in hew England and Boston
is in New England. Therefore it may be
raining in Boston.
Comdkktons that increase certainty:
1) The more central the subpart is to the set.
2) The greater the average spatial extent of
the property.
3) The greater the distance of the nearest set
with a contradictory property_
4) The greater the extent of the saipart
within the set.
5) Th 9 more likely a priori that the property
is in the given range for the subpart.
6) The more certain the property is In the
given range for the set.
The certainty conditions can be illustrated in
terms of the example in the table:
1) Condition 1 relates to the centrality of
the subpart. For example, if it&apos;s raining
in New England it is more likely to be
raining in Massachusetts than Maine because
Massachusetts is more central.
</bodyText>
<listItem confidence="0.595003842105263">
2) Condition 2 relates to whether the property
tends to be spatially distributed dr not.
For example, rain tends to be distributed
over smaller areas than electric service,
SQ it is a less certain inference that it
is raining in Maine than that there is
electric service in Maine, giver that the
property applies to New England.
3) Condition 3 relates to the distance to the
nearest concept with a contradictory
property. For example, if you know it&apos;s
not raining in New grunswick, that is
stronger evidence against it&apos;s raining in
Maine than if it&apos;s not raining in Montreal.
4) Condition 4 relates to the extent of the
subpart. For example, if it&apos;s raining in
New England it is more likely to be raining
in Rhode Maud than in Boston, because
Rhode Island is larger.
</listItem>
<bodyText confidence="0.775218666666667">
5) Condition 5 relates to the a priori
likelihood of the property. for example,
If&apos; it&apos;s raining in Washington State, it&apos;s
more likely to be raining in Seattle than
in Spokane because Seattle gets more rain
on the average.
6) Condition 6 relates to the person&apos;s
certainty that the property holds for the
concept. For example, the more certain the
person is that it is raining in New
England, the more certain that it&apos;s raining
in Boston.
</bodyText>
<sectionHeader confidence="0.994042" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999876333333333">
The theory I am developing is based on
these and similar analyses of a large number
of human protocols. Because the same
inference types recur in many different
answers, it is possible to abstract the
systematio patterns in the inferences
</bodyText>
<page confidence="0.952647">
200
</page>
<bodyText confidence="0.9041195">
ow
themselves, and many of. the different
conditions that affect people&apos;s certainty in
using different inference typesc
</bodyText>
<sectionHeader confidence="0.950799" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999879777777778">
I want to thank my colleagues who have
influenced my views about inference over the
years namely Marilyn Adams, Nelleke Aiello,
John Seely Brown, Jaime Carbonell, Dedre
Gentner, Mark Miller, Ross Quillian, Albert
Stevens, and Eleanor Warnock. I particularly
would like to thank Marilyn Adams for
encouraging me to fit the inference types into
a dimensionalized space, and John Seely Brown
for bullying me into stating the rules and
protocol Analyses in a form understandable to
readers.
This research was supported in part by
the Advanced Research Projects Agency of the
Department of Defense under Contract No. MDA
903-77-C-0025, and in part by a fellowship
from the John Simon Guggenheim Memorial
Foundation.
</bodyText>
<sectionHeader confidence="0.995881" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.5429538">
Brown, A. L. Knowing when, where &amp; how to
remember. In R. Glaser (Ed/), Advances in
instructional psychology. Hillsdale, NJ•
Lawrence Erlbaum Associates, 1977, in
press.
Brown, J.S., Collins, A., &amp; &apos;Harris, G.
Artificial intelligence and learning
strategies. To appear in H.F. O&apos;Neil
(Ed.), Learning strategies. New York:
Academic Press, 1978, in press.
</bodyText>
<reference confidence="0.850844457142857">
Carbonell, J.R. &amp; Collins, A. Natural
Semantics in Artificial Intelligence.
Proceedings of Third International Joint
Conference on Artificial Intelligence,
1973, pp. 344-351. (Reprinted in the
American Journal of Computational
Linguistics, 1974, 1, Mfc. 3).
Gentner, D., &amp; Collins, A. Knowing about
knowing: Effects of meta-knowledge on
inference. Submitted to Cognitive
Psychology.
Kosslyn, S.M. &amp; Schwartz, S.P. A simulation
of visual imagery. Cognitive_ Science,
1977, J., 265-295.
Kosslyn, S.M., Murphy, G.L., Bemesderfer,
M.E., &amp; Feinstein, K.J. Category and
continuum in mental comparisons. journal
gxperimental PsvchOloglt: General, 1977,
UAL 341-375.
Minsky, M. A framework for representing
knOwledge. In P. H. Winston (Ed.), Mt
psychology oCcomouter vision. New York:
McGraw-Hill, 1975.
Quillian, M. R. Semantic memory. In M. Minsky
(Ed.), Semantic informatlon processing.
Cambridge, Mass.: MIT Press, 1968.
Schenk, R. Conceptual Dependency: A Theory of
Natural Language Understanding, Cognitive
Psychology, 1972, 552-631.
Schenk, R. &amp; Abelson, R. Scripts. plans.,
goals. and understanding. Hillsdale, N.J.:
Lawrence Erlbaum Associates, 1977.
Smith, BE., Shoben, E.J., &amp; Rips, L.J.
Comparison processes in semantic memory.
Psychological Review, 1974, 15.,, 214-241.
</reference>
<bodyText confidence="0.805660769230769">
Stevens, A.L.. The. role of inference and
internal structure in the representatidn
of spatial information. Doctoral
dissertation. University of California at
San Diego, 1976.
Tulving, E. Episodic &amp; semantic memory. In E.
Tulving &amp; W. Donaldson (Eds.),
OrWliAeJon &amp; memory. New York: Academic
Press, 1972.
Collins, A. &amp; Warnock, E:R. Semantic Winograd, T. Understanding_natural language.
networks. BBN Report No. 3E83, Bolt New York: Academic Press, 1972.
Beranek and Newman Inc., Cambridge, Mass.,
1974.
</bodyText>
<reference confidence="0.933567882352941">
Collins, A. M. &amp; Loftus, E. F. A spreading
activation theory of semantic processing.
PsYqhological Revtew, 1975, la, 407-428.
Collins, A., Warnock, E.H., hello, N. &amp;
Miller, M.L. Reasoning from Incomplete
Khowledge, in D. Bobrow &amp; A. Collins
(eda.). Reoresentation &amp; understanding.
New York: Academic Press, 1975.
Collins, A.ma, &amp; Quillian, M.R. Experiments
on semantic memory and language
comprehension. In L.W. Gregg (Ed*:),
Cognition in learning and memorv. New
York: Wiley, 1972.
Collins, A.M., Adams, M.J. &amp; Pew, R.W. The
Effectiveness of an interactive map
display in tutoring geography. journal of
Educational Psychology, 19781 ja, 1-7.
</reference>
<page confidence="0.99683">
201
</page>
<table confidence="0.3213108">
INDIRECT RESPONSES-TO LOADED QUESTIONS*
S. Jerrold Kaplan
Department of Computer and Information Science
University of Pensylvania
Philadelphia, Pa. 19104
</table>
<page confidence="0.578883">
621
</page>
<bodyText confidence="0.995166810810811">
Casual users of Natural Language (NL)
computer systems are typically ihexpert not
only with regard to the techniCl details
of the underlying ptograms, but often with
regard to the structure and/or content of
the domain of discourse: Consequently, NL
systems must be designed bib, respond
appropriately when they can letect\.a
misconception on the part oi tne user.
Several conventions exist in cooperative
conversation that allow a speaker to
indirectly encode their intentions and
beliefs about the domain into their
utterances, (&amp;quot;loading&amp;quot; the utterances), and
allow (in fact, often require) a
cooperative respondent to address thos
intentions and beliefs beyond a literal,
direct response. To be effective, NL
computer systems must do thegsame. The
problem, then, is to provide practical
computational tools which will determine
both when an indirect response is required,
and WHWE— that response should be, without
recitianig that large amounts of domain
4ppendent world knowledge be encoded in
special formalisms.
This paper will take the position thpt
distinguishing language driven inferences
from domain driven inferences provides a
framework for-i-raution to this problem in
the Data Base (DB) query domain. An
implemented query system (CO-OP) is
described that uses this distinction to
provide cooperative responses to DB
queries, using only a standard (CODASYL) DB
and a lexicon as sources of world
knowledge.
</bodyText>
<sectionHeader confidence="0.910962" genericHeader="method">
WHAT IS A LOADED.QUESTION?
</sectionHeader>
<bodyText confidence="0.893692571428571">
loaded question is one that
indicates that the questioner presumes
something to be true aboyt the domain of
discourse that is actually false. Question
lA presumes 1B. A cooperative speaker must
* This work partially supported by NSF
grant mcs 76-19466
find IB assumable (i.e. not believe it to
be false) in order to appropriately utter
lA in a cooperative conversation, intend it
literally, and expect a correct, direct
response.
1A. What day odes John go to his
weekly piano lessdn?
</bodyText>
<sectionHeader confidence="0.3393335" genericHeader="method">
IB. John takes weekly piano lessons.
1C. Tuesday.
</sectionHeader>
<bodyText confidence="0.592672">
Similarly, 2A presumes 2B.
</bodyText>
<listItem confidence="0.7518676">
2A. How many Bloody Marys did Bill
down at the banquet?
2B. Hard liquor was available at the
banquet.
2C. Zero.
</listItem>
<bodyText confidence="0.999173444444444">
If the questioner believed 2B to be false,
there would be no point in asking 2A - s/he
would already know that the correct answer
had to be &amp;quot;Zero.&amp;quot; (2C).
Both examples 1 and 2 can be explained
by a conventiQn of conversational
cooperation: that a questioner should leave
the respondent a choice of direct answers.
That is, from the questioner&apos;s viewpoint
upon asking a question, more than one
direct answer must be possible.
It follows, then, that if a question
presupposes something about the domain of
discourse, as lA does, that a questioner
cannot felicitously utter the question and
believe the presupposition to be false.
This is a result of the fact that each
direct answer to a question entails the
question&apos;s presuppositions. (More
formally, if question Q presupposes
proposition P, then each question-direct
answer pair (Q, Ai) entails P*.) Therefore,
* This entailment condition is a necessary
but not sufficient condition for
presupposition. The concept of
presupposition normally includes a
condition that the negation of a
</bodyText>
<page confidence="0.990976">
202
</page>
<bodyText confidence="0.988597755102041">
if a- questioner believes a presupposition
to be false, s/he leaves no options for a
correct, diregt response - violating the
convention. Conversely, a respondent can
infer in a cooperative conversation from
the fact that a question has been asked,
that the questioner finds it&apos;s
presuppositions assumable. (In the terms
of [Keenan 71], the logical presupposition
is pragmatically presupposed.)
Surprisingly, a more general semantic
relationship exists that still allows a
respondent to infer a questioner&apos;s beliefs.
Consider the situation where a proposition
is entailed by all but one of a question&apos;s
direct answers. (Such a proposition will
be called a-presumption of the question.)
By a similar argument, it follows that if a
questioner believes that proposition to be
false, s/he can infer the direct, correct
answer to the question - it is the answer
that does not entail the proposition. Once
again, to ask such a question leaves the
respondent no choice of (potentially)
correct answers, violating the
conversational convention. More
importantly/ upon being asked such a
question, tfie respondent can infer what the
questioner presumes about the context.
Question 2A above presumes 2B, but
does not presuppose it: 2B is not entailed
by the direct answer C. Nonetheless, a
qutIptioner4 must find, 2B assumable to
felicitously ask 2A in a cooperative
conversation - tt do otherwise would
violate the cooperative convention.
Similarly, 3B below is a presuMption but
not a presupposition of 3A (it is not
entailed by 3C).
;4.2,
the failure of a presupposition renders a
questThr-riTiferrcitous because it &apos;51-0Wi-ria
options for a direct .response; the failui7
of a presumption- renders a qiiiiITER
TiifeirCitous because it-n-a-Ws at most one
option for a direct response. (Note tEWE
the defTETtion-U--gresumption subsumes the
definition of presupposition in this
context.)
</bodyText>
<sectionHeader confidence="0.980956" genericHeader="method">
CORRECTIVE INDIRECT RESPONSES
</sectionHeader>
<bodyText confidence="0.996885666666667">
In a cooperative conversation, if a
respondent detects that a questioner
incorrectly presumes something about the
domain of discourse, s/he is required to
correct that misimpression. A failure to
do so will impricitly confirm the
questioner&apos;s presumption. Consequently, it
is not always the case that a correct,
direct answer is the most cooperative
response. When an incorrect presumption is
detected, it is more cooperative to &apos;correct
the presumption than to give a direct
response. Such a response can be called a
Corrective Indirect Resporse. For example,
imagine qUWEET3F- 4A tittered in a
cooperative conversation when the
respondent knows that no departments sell
knives.
</bodyText>
<listItem confidence="0.979099">
4A. Which departments that sell
knives also sell blade sharpeners?
4B. None.
4C. No deparments sell knives.
3A. Did Sandy pass the prelims?
3B. Sandy took the prelims.
3C. No.
</listItem>
<bodyText confidence="0.999720142857143">
if a que#tioner believes 4.41 the falsehood
of a presupposition of a question, the
question is inappropriate because s/he must
believe that no direct answer can be
correct; similarly, if a questioner
believes in the falsehood of a presumption,
the question is inappropriate because the
questioner must know the answer to the
question - it is the direct answer that
does not entail the presumption. In short,
proposition (in this case, the negation of
the proposition expressed by a
question-direct answer pair) should also
entail its presuppositions. Consequently,
the truth of a presupposition of a question
is normally considered a prerequisite for
an answer to be either true or false (for a
more detailed discussion see [Keenan 73]).
These subtleties of the concept of
presupposition are irrelevant to this
discussion, because false responses to
questions are considered a-priori to be
uncooperative.
Although 4B is a direct, correct response
in this context, it is less cooperative
than 4C. This effect is explained by the
fact that 4A presumes that some departments
sell knives. To be cooperative, the
respondent should correct the questioner&apos;s
misimpression with an indirect response,
informing the questioner that no
departments sell knives (4C). (The direct,
correct response 4B will reinforce the
questioner&apos;s mistaken presumption in a
cooperative conversation through it&apos;s
failure to state otherwise.) A failure to
produce corrective indirect responses is
highly inappropriate in a cooperative
conversation, and leads to &amp;quot;stonewalling&amp;quot; -
the giving of very limited and precise
responses that fail to address the larger
goals and beliefs of the questioner.
</bodyText>
<sectionHeader confidence="0.959051" genericHeader="method">
RELEVANCE TO DB QUERIES
</sectionHeader>
<bodyText confidence="0.910307">
Most NL computer systems stonewall,
because their designs erroneously assume
that simply producing the correct, direct
response to a query insures a cooperative
response. (To a great extent, this
assumption results from the view that NL
</bodyText>
<page confidence="0.723299">
2031
23
</page>
<bodyText confidence="0.887922615384615">
functions in this domain simply as a
high-level query language.) Unfortunately,
the domain of most realistic DB&apos;s are
sufficiently complex that the, user of a NL
query facility (most likely a naive user)
will frequently make incorrect presumptions
in his or her queries. A NL system that is
only capable of a direct- respons-i—gar
necessarily pr7duce ailaTingless responses
to failed presuppositions„ and stonewall on
!Wiled presumptions. Consider tri
following hypothetical exchange with a
typical NL query system:
</bodyText>
<table confidence="0.942111166666667">
Q: Which students got a grade of F in
CIS500 in Spring, &apos;77?
R: Nil. [the empty set]
Q: Did anyone fail CI8500 in Spring,
77?
R: No.
Q: How many people passed CIS500 in
Spring, &apos;77?
R: Zero.
Q: Was CIS500 given in Spring &apos;77?
Rr No.
A cooperative NL query system should
</table>
<bodyText confidence="0.998921387096774">
be able to detect that the initial query in
the dialog incorrectly presumed that CIS500
was offered in Spring, &apos;77, and respond
appropriately. This ability is essential
to a NL system that will function in a
practical environment, because the fact
that NL is used in the interaction will
imply to the users that the normal
coopekative conventions followed in a human
dialog will be observed by the machine.
The CO-OP query system, descrIbed below,
obeys a number of conversational
conventions.
While the definition of presumption
given above may be of interest from a
linguistic standpoint, it leaves much to be
desired as g computational theory.
Although it provides a descriptive model of
certain aspects of conversational behavior,
it does not provide an adequate basis for
computing the presumptions of a givep
question in a reasonable way. By limiting
the comain of application to the area of
data retrieval, it is possible to show that
the linguiqtic structure of questions
encodes considerable information about the
presumptions that the questioner has made.
This structure can be exploited to compute
a significant class of presumptions, and
provide appropriate corrective indirect
responses.
</bodyText>
<sectionHeader confidence="0.921707" genericHeader="method">
LANGUAGE DRIVEN VS. DOMAIN DRIVEN INFERENCE
</sectionHeader>
<bodyText confidence="0.968357957746479">
A long standing observation in Al
research is that knowledge about the world
- both procedural and declarative - is
required in order to understand NL.*
Consequently, a great deal of study has
gone into determining just what type of
knowledge is required, and how that
knowledge is to be organized, accessed, and
utilized. One practical difficulty with
systems adopting this approach is that they
require the encoding of large amounts of
world knowledge to be properly tested, or
even to function at all. It is not easy to
determine if a particular failure of a
system is due to an inadequacy in the
formalism or simply an insufficient base of
knowledge. Frequently, the tollection and
encoding of the appropriate knowledge is a
painstaking and time consuming task,
further hindering an effective evaluation.
Most NL systems that follow this paradigm
have a common property: they decompose the
input into a suitable &amp;quot;meaning&amp;quot;
representation, and rely on various
deduction and/or rea&amp;oning mechanisms to
provide the tintelligence&amp;quot; required to draw
the necessary inferentes. Inferences made
in fhis way can be called domain** driven
inferences, because they are motivated by
the domain itself***.
While domain driven inferences are
surely essential to an understanding of NL
(and will be a required part of any
comprehensive cognitive model of human
intelligence), they alone are not
sufficient to produce a reasonable
understanding of NL. Consider the
following story:
John is pretty crazy, and sometimes
does strange things. Yesterday he went
to Sardi&apos;s for dinner. He sat down,
examined the menu, ordered a steak, and
got up and left.
For a NL system to infer that something
unusual has happened in the story, it must
distinguish the story from the events the
story describes. A question answering&apos;
system that would respond to &amp;quot;What did John
eat?&amp;quot; with &amp;quot;A steak.&amp;quot; cannot be said to
understand the story. As 4 sequence of
events, the passage contains nothing
unusual - it simply omits details that can
be filled in on the basis of common
knowledge about restaurants. As aistory,
* F4r example, to understand the statement
&amp;quot;I bought a briefcase yesterday, and today
the handle broke off.&amp;quot; it is necessary to
know that briefcases typically have
handles.
** &amp;quot;Domain&amp;quot; here is meant to include
general world knowledge, knowledge about
the specific context, and inferential rules
of a general and/or specific nature about
that knowledge.
*** Of course, these inferences are
actually made on the basis of descriptions
of the domain (the internal meaning
representation) and not the domain itself.
What is to be evaluated in such systems is
the sufficiency of that description in
representing the domain.
</bodyText>
<page confidence="0.993398">
204
</page>
<bodyText confidence="0.999015067307692">
however, it raises expectations that the
events do not. Drawing the inference &amp;quot;John
didn&apos;t eat the steak he ordered.&amp;quot; requires
knowledge about the language in addition to
knowledge about the domain. Inferences
that require language related knowledge can
be called language\drfven inferendes.
Language driven inferencescan be
characterized as follows: they are based on
the fact that a story, dialog, utterance,
etc. ia a description, and that the
descriltion itself may exhibit useful
Broperties not associated with the =Fig
being deiFFTbed.* These WWitional
properties are used by speakers to encode
essential information - a knowledge of
language related conventions is required to
understand NL.
Language driven inferences have
several useful properties in a
computational framework. First, being
based on general knowledge about the
language, they do not require a large
infusion of knowledge to operate in
differing domains. As a result, they are
somewhat more amenable to encoding in
computer systems (requiring less
programming effort), and tend to be more
transportable to new domains. Second, they
do not appear to be as subject to runaway
inferencing, i.e. the inferencing is
driven (and hence controlled) by the
phranng of the input. Third, they can
often achieve results approximating that of
domain driven inference techniques with
substantially less computational machinery
and exechtion time.
As a simple example, considet the case
of factive verbs. The sentence &amp;quot;John
doesn&apos;t know that the Beatles broke up.&amp;quot;
carries the inference that the Beatles
broke up. Treated as a domain driven
inference, this result might typically be
achieved as follows. The sentence could be
parsed into a representation indicating
John&apos;s lack of knowledge of the Beatles&apos;
breakup. Either immediately or at some
suitable later time, a procedure might be
invoked that encodes the knowledge &amp;quot;For
someone to not know something, that
something has to be the case.&amp;quot; The
inferencial procedures can then update the
knowleOge base accordirigly. As a language
driven inference, this inference can be
regarded as a lexical property, i.e. that
factive verbs presuppose their complements,
and the complement immediately asserted,
namely, that the Beatles broke up. (Note
that this process cannot be reasonably said
to &amp;quot;understand&amp;quot; the utterance, but achieves
the same results.) Effectively, certain
* In the story example, assumptidns about
the connectedness of the story and the
uniformity of bhe*level of description give
rise to the inference tihat John didn&apos;t eat
what he ordered. These assumpttons are
conventions in the language, and not
properties of the situation being
described.
inference rules have been encoded directly
into the lexical and syntactic structure of
the language - facilitating the drawing of
the inference without resorting to general
reasoning processes.
Another (simpler) type of language
driven inferences are those that relate
specifically to the structure of the
discourse, and not to it&apos;s meaning.
Consider the interpretation of anaphoric
references such as &amp;quot;former&amp;quot;, &amp;quot;latter&amp;quot;,
&amp;quot;vice versa&amp;quot;, &amp;quot;respectively&amp;quot;, etc. These
words exploit the linear nature of language
to convey their meaning. To infer the
appropriate referents, a NL system must
retain a sufficient amount of the structure
of the text to determine the relative
positions of potential referents. If the
system &amp;quot;digests&amp;quot; a text into a non-linear
representation (a common procedure), it is
likely to lose the information required for
understanding.
The CO-OP system, described below,
demonstrates that a language driven
inference approach to computdtional Systems
can to a considerable extent produce
appropriate NL behavior in practical
domains without the overhead of a detailed
and comprehensive world model. By limitihg
the domain of discourse to DB queries, the
lexical and syntactic structure of the
questions encodes sufficient information
about the user&apos;s beliefs that a significant
class of presumptions can be computed on a
purely language driven basis.
</bodyText>
<sectionHeader confidence="0.973507" genericHeader="method">
CO-OP: A COOPERATIVE QUERY SYSTEM
</sectionHeader>
<bodyText confidence="0.863485307692308">
The design and a pilot implementation
of a NL query system (CO-OP) that provides
cooperative responses and operates with a
standard (CODASYL) DB system has been
completed. In addition to producing direct
answers, CO-OP is capable of producing a
variety of indirect responses, including
corrective indirect responses. The design
methodology of the system is based on two
observations:
1) To a large extent, the inferencing
required to detect the need for an
indirect response and to select the
appropriate one can be driven ditectly
from the le-xical and syntactic
structure of the input question, and
2) the information already encoded in
standard ways in DB systems complements
the language related knowledge
sufficiently to produce appropriate
conversational behavior without the
need for separate &amp;quot;world knowledge&amp;quot; or
&amp;quot;domain specific knowledge&amp;quot; modules.
Consequently, the inferencing mechanisms
required to produce. the cooperative
responses are domain transparent, In the
</bodyText>
<page confidence="0.993892">
205
</page>
<bodyText confidence="0.941684317647059">
sense that they will produce appropriate
behavior without modification from any
suitable D8 system. These mechanisms can
therefore be transported to new DB&apos;s
without modification.
To illustrate this claim, a detailed
description of the method by which
corrective indirect responses are produced
follows.
THE META QUERY LANGUAGE
Most DB queries can be viewed as
requesting the selection of a subset (the
response set) from a presented set of
entities, (this analysis follows [Belnap
76]). Normally, the presented set is put
through a series of restrictions, each of
which produces a subset, until the response
set is found. This view is formalized in
the Procedures that manipulate an
intermediate representation of the query,
called the Meta Query Langua9e (MQL).
The MQL isa graph structure, where
the nodes &apos;represent sets (in the the
mathematical, not the DB sense) &amp;quot;presented&amp;quot;
by the user, and the edges represent binary
relations defined on those sets, derived
from the lexical and syntactic structure of
the input query. Conceptually, the direct
response to a query is an N-place relation
realized by obtaining the referent of the
sets in the DB, and composing them
according to the binary relations. Each
composition will have the effect of
selecting a subset of the current sets.
The subsets will contain the elements that
survive (participate) in ehe relation.
(Actually, the responses are realized in a
much more efficient fashion - this is
simply a convenient view.)
As an example, consider the query
&amp;quot;Which students got Fs in Linguistics
courses?&amp;quot; as diagrammed ir FIGURE 1.
Meta Query Language representation of
&amp;quot;Which students got Fs in Linguistics
courses?&amp;quot;
FIGURE 1
This query would be parsed as presenting 4
sets&amp;quot;: &amp;quot;students&amp;quot;, &amp;quot;Fs&amp;quot;, &amp;quot;Linguistics&amp;quot;, and
&amp;quot;coprses&amp;quot;. (The sets &amp;quot;Linguistics&amp;quot; and
&amp;quot;Fs&amp;quot; may appear counterintuitive, but
6/5&amp;quot;
should be viewed as singleton entities
assumed by the user to -exist somewhere in
the DB.) The direct answer to the query
woul0 be a 4 place relation consisting of a
column of students, grades (all Fs),
departments (all Linguistics), and courses.
For convenience, the columns containing
singleton sets (grades and departments)
would be removed, and the remaining list of
students and associated courses presented
to the user.
Executing the query consists of
passing the MQL representation of theoquery
to an interpretive component that produces
a query suitable for execution on a CODASYL
OB using information associated for this
)urpose with the lexical items in the MQL.
(The specific knowledge required to perform
-his translation is encoded purely at the
Leiçal level: the only additional domain
dependent knowledge required is access to
the DB schenia:)
The MQL, by encoding some of the
syntactic relationships present in the NL
query, can hardly be paid to capture the
meaning of the question: it is merely a
convenient representation formalizing
certain linguistic characteristics of the
query. The procedures that mainipulate
this representation to generate inferences
are based on observations of a general
nature regarding these syntactic
relationships. Consequently, these
inferences are language driven inferences.
</bodyText>
<sectionHeader confidence="0.992786" genericHeader="method">
COMPUTING CORRECTIVE INDIRECT RESPONSES
</sectionHeader>
<bodyText confidence="0.987202086956522">
The crucial observation required to
produce a reasonabld set of cotrective
indirect responses is that the MQL query
presumes the non-emptiness of it&apos;s
connected subgraphs. Each connected
subgraph corresponds to a presumption the
User has made about the domain of
discourse. Consequently, should the
initial qUery return a null response, the
control structure can check the users
presumptions by passing each connected
subgraph to the interpretive component to
check it&apos;s non-emptiness (notice that each
subgraph itself constitutes a well formed
query). Should a presumption prove false,
an appropriate indirect response can be
generated, rather than a meaningless or
misleading dieect response of &amp;quot;None.&amp;quot;
For example, in the query of FIGURE 1,
the subgraphs and their correspopding
corrective indirect respopses are (the
numbers represent the seta the subgraphs
consist of):
</bodyText>
<listItem confidence="0.974109571428571">
1) &amp;quot;I don&apos;t know of any students.&amp;quot;
2) &amp;quot;I don&apos;t know of any Fs.&amp;quot;
3) mI don&apos;t Know of any courses.&amp;quot;
4) &amp;quot;I don&apos;t know of any Linguistics.&amp;quot;
1,2) &amp;quot;I don&apos;t know of any students
that got Fs.&amp;quot;
2,3) &amp;quot;I don&apos;t know of any Fs in
</listItem>
<page confidence="0.987509">
206
</page>
<table confidence="0.937931625">
courses.&amp;quot; representations and associated algorithms, c2.6
3,4) &amp;quot;I don&apos;t know of any Linguistics are produced in. a domain transparent
dburses.&amp;quot; fashion with minimal system overhead using
1,2,3) &amp;quot;I don&apos;t know of any students knowlefte alreAdy available in the DB.
that got Fs in courses.&amp;quot;
2,3,4) &amp;quot;I don&apos;t know of any Fs ir
linguistics courses.&amp;quot;
A SHORT SAMPLE SESSION
</table>
<bodyText confidence="0.972493952380952">
SUppose that there are no linguistics
courses in the DB. Rather than presenting
the direct, correct answer of &amp;quot;None.&amp;quot; the
control structure will pass each connected
subgraph in turn to be executed against the
DB. It will discover that no linguistics
courses extst in the DB, and so will
respond with &amp;quot;I dorOt know of any
linguistics courses.&amp;quot; This corrective
indirect response (and all responses
generated through this method) will entail
the direct answer, since they will entail
the emptiness of the direct response set.
Several aspects of this procedure are
worthy of note. First, although the
selection of thb response is dependent on
knowledge ot the domain (as encoded in a
very general sense in the DB system - not
as separate theorems, structures, or
programs)&apos;, the computation of the
presumptions -rg totally independent -a
domain specific knowledge. Because the
inferences are driven solely by the parser
output (MQL representation), the procedures
that determine the presumptions (by
computing subgraphs) require no knowledge
of the DB. Consequently, producing
corrective indirect responses from another
DB, or even another DB system, requires no
changes to the inferencing procedures.
Secondly, the mechanism for selecting the
indirect response is identical to the
procedure for executing a query. No
additional computational machinery need Fe-
invoked to select the appropriate indirect
response. Thirdly i the coal:man=
overhead&amp;quot; involved in checking and
correcting the users presumptioils is not
incurred unless it has been determined that
an indirect response may be required.
Should the query succeed initially, no
penalty in execution time will be paid for
the abrfity to produce theindirect
responses. In adOltion, the only increase
in &apos;space overhead is a small control
program to produde the appropriate
subgraphs (the linguistic generation of the
indirect response is essentially free - it
is a trivial addition to the paraphrase
component already used in the parsing
phase).
Corrective indirect responses:
produced in this fashion, dare language
driyen inferences, because they are derived
directly from the structure of the query as
represented by the MQL. If the query were
phrased differently, a different set of
presumptions would be computed. _(This is
not a drawback, as it might seem at first -
it insured-that the respogse will be in
terms that the user understands.) For these
&apos;reasons, correctipe indirect responses,
made possible by a &apos;careful choice of
</bodyText>
<subsectionHeader confidence="0.742697">
This section contains an example
</subsectionHeader>
<bodyText confidence="0.999104425531915">
interaction with the preliminary
implementation of the CO-OP system as it
row stands. An actual session has been
abstracted and commented here.
CO-OP i4 currently implemented on a
DEC KI-10 under TOPS-10 mainly in
UCI-RUTGERS-LISP, and runs in 9n( words,
including the SEED DB system. SEED is a
commercially available CODASYL DB system,
marketed by International Data Base
Systems, Inc. (Gerritsen 78]. The DB is a
live ohe, used by the National Center For
Atmospheric Research (NCAR) in Boulder Co.
to keep track of their computer users,
projects, research sponsors, etc. The
primary purpose of the DB is to track the
use of NCAR facilities, for accounting and
other uses.
The user in the example is a
hypothetical Computer Science department
chairperson who has heard that the NCAR DB
uuntains information about who is
sponsoring research in various areas, and
is interested in securing some research
funds. S/he believes (incorrectly, it
turns out) that funds are available to
study mass storage. S/he begins by asking:
Q Who sponsors active projects in mass
storage?
R: I don&apos;t know of any projects in mass
storage
The system is unaware of any projectb in
mass atorage, and so explkeitly corrects
the users presumption that there are some.
The user tries a more general question.
Q: Who sponsors research in computer
science?
R: I don&apos;t know of anyone that sponsors
research in computer science.
Again, a corredtive indirect response - but
this time stating that no sponsors arf
known, rather than that no suitablf
projects are known. In addition to being
able to generate corrective indirect
responses, the system can choose the most
appropriate one when more than one is
applicable. This is done by observing that
</bodyText>
<page confidence="0.895817">
20,
</page>
<bodyText confidence="0.994617866666667">
the subgraphs of an MQL (very are partially
ordered according to a subgraph relation,
and it is most appropriate to correct the
&amp;quot;least&amp;quot; failing subgraphs with respect to
this ordering.
Getting nowhere, the chairperson now
asks:
list (sorted by contact), along with the 27
projects they sponsor. Although the user
has presumed that there is only one contact
at Nasa &amp;quot;Headquarters, the system provides
the entire list, without objecting. This
and other forms of sloppy reference are
tolerated by the system.
Q: What are the areas?
</bodyText>
<sectionHeader confidence="0.994552" genericHeader="method">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.995557835616438">
and recelves a list of the areas of
interest (s/he knew that the protects at
NCAR are grouped according to a
predetermined list of areas of interest),
identified by the area number and name.
S/he observes that area #6 (data
processing) and area #12 (numerical
methods) look interetting, and follows up
with:
Q: Who sponsors projects in area 6?
The response is a list of sponsor names
with a supportive indirect component of the
projects they sponsor in area 6, the name
of the area (because only the number was
supplied - the system doesn&apos;t currently
remember that it just provided the area
name to the user), and the project numbers
ofethe sponsor.)d projects. The user now
decides that Nasa *Headquarters looks the
most promising, and so asks:
Q: What is sponsored in numerical methods
by Vasa Headquarters?
After checking the DB, the system discovers
that Nasa Headquarters doesn&apos;t sponsor
anything in numerical methods.
Additionally, it is unable to detect any
failed presumptions on the part of the
user. It therefore provides a negative
response followed by a suggestive indirect
resp9nse listing the projects that Nasa
Headquarters sponsors in any area, in the
hope that this will be helpful to the user.
R: I don&apos;t know of anything in numerical
methods that Nasa Headquarters sponsors.
But you might be interested in anything
that Nasa Headquarters sponsors...
After perusing this list, the chairperson
concludes that although the projects don&apos;t
look very promising, s/he will get in touch
with Nasa Headquarters. S/he asks:
Q: Who is the contact at Nasa Headquarters?
The problem of producing apparently
intelligent behavior from a NL system has
traditionally been viewed in Artificial,
Intelligence as a problem of modelling
human cognitive processes, or modelling
knowledge about the real world. It has
been dembnstrated here that such approaches
must include a pragmatic theory of the
conventions and properties of the use of
language, to function effectively. Domain
driven inferences must be complemented by
language driven inferences to appropriately
process NL. Further, it has been argued
that language driven inference mechanisms
help to control the inference process, and
can provide a more general and
computationally attractive sq.utkons to
many problems previously thought to require
domain driven inference.
A descriptive theory of one type of
cooperative indirect response to
inappropriate questions has been presented,
and extended to a prescriptive
(computational) theory by restricting the
domain of application to DB query systems.
This theory has been implemented using
language driven mechanisms in the design of
CO-OP, a cooperative query system. the
result is the generation of appropriate
corrective indirect responses in a
computationally efficient and domain
transparent fashion.
</bodyText>
<sectionHeader confidence="0.996632" genericHeader="method">
REFERENCES
</sectionHeader>
<bodyText confidence="0.9800032">
Austin, J.L., How To Do Things with Words,
J.O. Urmson, Tr, Oxford University Press,
N.Y. 1965.
Belnap, N. D., and T. B. Steel, The
Locpc of Questions and Answers, YWIW
Universny Press, New Haven, Conn., 1976.
Gerritsen, Rob, SEED Reference Manual,
Version COO - B04 draft, International Data
Base Systems, Inc., Philadelphia, Pa.,
19104, 1978.
It turns out that there is a contact at
Nasa Headquarters for each project.
sponsored, and so the system prints out the
Grice, H. P., &amp;quot;Logic and Conversation&amp;quot;, in
Syntax and Semantics: Speech Acts, Vol. 3,
</bodyText>
<note confidence="0.645463">
(ID. CM- and 3. Mnrnan. pa_l
</note>
<page confidence="0.993519">
208
</page>
<table confidence="0.4912481875">
Academic Press, N.Y., 1975.
Harris, L. R., &amp;quot;Natural Language Data Base
Query: Using the Data Base Itself as the
Definition of World Knowledge and as an
Extension of the Dictionary&amp;quot;, Technical
Report #TR 77-2, Mathematics Dept.,
Dartmouth College, Hanover, N.H., 1977.
Joshi, A. K., S. J. Kaplan, and R. M.
Lee, &amp;quot;Approximate Responses from a Data
Base Query System: An Application of
Inferencing in Natural Language&amp;quot;, in
Proceedings of the 5th IJCAI, Vol. 11
1977.
Kaplan, S. Jerrold, &amp;quot;Cooperative Responses
from a Natural Language Data Base Query
System: Preliminary Report&amp;quot;, Technical
</table>
<affiliation confidence="0.3446">
Report, Dept. of Computer *and Information
Science, Moore School, University of
Pennsylvania, Philadelphia, Pa., 1977.
</affiliation>
<note confidence="0.788983142857143">
Kaplan, S. J., and Joshi, A. K.,
&amp;quot;Cooperative Responses: An Application of
Discourse Inference to Data Base Query
Systems&amp;quot;, to appear in proceedings of the
Second Annual Conference of the Canadian
Society for Computational Studies of
Intelligence, Toronto, Ontario, July, 1978.
</note>
<page confidence="0.326674">
Zoshi, A. K., Kaplan, S. J., and Sag, I.
</page>
<bodyText confidence="0.812394444444444">
A., &amp;quot;Cooperative Responses: Why Query
Systems Stonewall&amp;quot;, to appear in
proceedings of the 7th International
Conference on Computational Linguistics,
Bergen, Norway, August, 1978.
Keenan, E. L., &amp;quot;Two kinds of
Presupposition in Natural Language&amp;quot;, in
Studies in Linguistic Semantics, (C. J.
Fillmore and D. T. Langendoen, Ed.),
Holt, Rinehart, and Winston, N.Y., 1971.
Keenan, E. L., and Hull, R. D., &amp;quot;The
Logical Presuppositions of Questions and
Answers&amp;quot;, in • Prasuuositionen in
Philosophie und Linguistik, (Petofi and
Frank, Ed.), Atheneum Verlag, Frankfurt,
1973.
Lee, Ronald M. &amp;quot;Informative Failure in
Database Queries&amp;quot;, Working Paper #77-11-05,
</bodyText>
<affiliation confidence="0.7559705">
Dept. of Decision Sciences, Wharton
School, University of Pennsylvania, 1977.
</affiliation>
<note confidence="0.942935166666667">
Lehnert, W., &amp;quot;Human and Computational
Question,Answering&amp;quot;„ in Cognitive Science,
Vol. 1, #1, 1977.
Searle, J. R., Speech Acts, an Essay in
the Philosoex of Langua9er- Cambridge
University Press, London, 1969.
Weischedel, R. M., Computation of a Unique
Class of Inferences: Presuppoiition and
EMTiment, Ph.D. dissertation, Dept. .&amp;quot;-c7
Computer and Information Science,
University of Pennsylvania, Philadelphia,
Pa. 1975.
</note>
<page confidence="0.995558">
209
</page>
<figure confidence="0.292165333333333">
2 1
ON REASONING •BY DEFAULT
Raymond Reiter
</figure>
<affiliation confidence="0.254296">
Department of Computer Science
University of British Columbia
Vancouver, B.C., Canada
</affiliation>
<sectionHeader confidence="0.73018" genericHeader="method">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.997411818181818">
This paper surveys a number of kinds of
default reasoning in Artificial Intelligence, spec-
ifically, default assignments to variables, the
closed world assumption, the frame default for
causal worlds, exceptions as defaults: and negation
in Artificial Intelligence programming languages.
Some of these defaults provide clear representa-
tional and computational advantanges over their
corresponding first order theories. Finally, the
paper discusses various difficulties associated
with default theories.
</bodyText>
<figure confidence="0.838749">
If I don&apos;t know I don&apos;t know
I think I know
If I don&apos;t know I know
I think I don&apos;t know
R.D. Laing, Knots.
</figure>
<sectionHeader confidence="0.82018" genericHeader="method">
I. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.997316916666667">
Default reasoning is commonly used in natural
language understanding systems and in Artificial
Intelligence in general. We use the term &amp;quot;default
reasoning&amp;quot; to denote the process of arriving at
conclusions based upon patterns of inference of
the form &amp;quot;In the absence of any information to the
contrary, assume...&amp;quot; In this paper, we take this
pattern to have the more formal meaning &amp;quot;If certain
information cannot be deduced from the given know-
ledge base, then conclude...&amp;quot; Such reasoning rep-
resents a form of plausible inference and is
typically required whenever conclusions must be
drawn despite the absence of total knowledge about
a world.
In order to fix some of these ideas, we begin
by surveying a nuMber of instances of default
reasoning as they are commonly invoked in A.I.
Specifically, we discuss default assignments to
variables, the closed world assumption, the frame
default for causal worlds, exceptions as defaults,
and negation in A.I. programming languages. We shall
see that these may all be formillized by introducing
a single default operator i4 where ii.vw is taken
to mean &amp;quot;W is not deducible from the given know-
ledge base&amp;quot;.
In addition, we shall discover that the closed
world and frame defaults provide clear representa-
tional and computational advantages over their cor-
responding first order theories. The former elim-
inates the need for an explicit representation of
negative knowledge about a world, while the latter
eliminates the so-called frame axioms for dynamic
worlds.
Finally, we discuss various oroblems which
arise as a result of augmenting first order logic
with a default operator.
</bodyText>
<sectionHeader confidence="0.491465" genericHeader="method">
2. SOME INSTANES OF DEFAULT REASONING IN A.I.
</sectionHeader>
<bodyText confidence="0.992587">
The use of default reasoning in A.I. is far
more widespread than is commonly realized. The
purpose of this section is to point out a variety
of seemingly different situations in which such
reasoning arises, to accent common patterns which
emerge when defaults are formalized, and to indi-
cate certain representational and computational
advantages of default reasoning.
</bodyText>
<subsectionHeader confidence="0.971146">
2.1 Default Assignments to Variables
</subsectionHeader>
<bodyText confidence="0.913710285714285">
A number of knowledge representation schemes,
e.g. FRL [Roberts and Goldstein 19773, KRL LBobrow
and Winograd 1977], explicitly provide for the
assignment of default values to variables (slots,
terminals). For example, in KRL the unit far a
person in an airline travel system has the form:
&apos;in
</bodyText>
<page confidence="0.386309">
30
</page>
<bodyText confidence="0.981684716666667">
(Person UNIT Basic
hometown((a City) PaloAlto; DEFAULT)
We can view this declaration as an instruction to
the KRL interpreter to carry out the foLlowing:
If x is a person, then in the Jabsence of any infor-
mation to the contrary, assume hometown(x)=PaloAltg,
or phrased in a way which makes explicit the fact
that a default assignment is being made to a
variable:
_
If x is a person and no value can be determined for
the variable y such that hometown(x)=y, then assume
y=PaloAlto.
Notice that in assigning a default value to a var-
iable, it is not sufficient Lo &apos;id], to find an ex-
plicit match for the variable in the data base.
For example, the non existence in the data base of
a fact of the form hometown(JohnDoe)=y for some
city y does not necessarily permit the default
assignment y=PaloAlto. It might be the case that
the following information is aVtilable:
(x/EMPLOYER)(y/PERSON)(z/CITY)EMPLOYS(x,y)
A location(x)=z D hometown(y)=z1
i.e. a person&apos;s hometoWn is the same as his or her
employer. In this case the default assignment
y=PaloAlto can be made only if we fail to deduce the
existence of an employer x and city z such that
EMPLOYS(x,JohnDoe) A location(x)=z
in general then, default assignments to variables
:rtetepmep:idttdeedd9::iyona.s Wer:saunitfoor=u:eg:Ine::17
inference pattern for the default assignment of
values to variables:
For all xl,...,xn in classbs T11...,Tn respectively,
if we fail to deduce (Ey/o)P(xl,...,xn,y) then in-
fer the default statement
•
I Throughout this paper we shall use a typeci logical
representation language. &apos;Noes, e.§. EMPOYER,
PERSON, CITY correspona to tneimpal categories
of IS-A hierarchies. A typed universal quantifier
like (x/EMPLOYER) is read &amp;quot;for all x whillebelong
to the class EMPLOYER&amp;quot; or simply &amp;quot;for all employ-
ers x&amp;quot;. A typed existential quantifier like
(Ex/CITY) is read &amp;quot;44-ere is a dty x. The oota-
tion derives from that used by Woods in his &amp;quot;MP
function&amp;quot; [Woods 1968].
P(xl,...,xn,,default value for y&gt;)
or more sticcinctly,
(x / 1):..cxn/Tn)
q(Ey/o)P(xl,...,xn,Y)
Ii51,...•xn,&lt;default value for y&gt;)
Here i4 is to be read &amp;quot;fail to deduce&amp;quot;, a and the
I&apos;s are types, and P(xl,...,xn,y) is any statement
about the variables xl,...„xn,y% There are some
serious difficulties associated with just what ex-
actly is meant by &amp;quot; 1,1&amp;quot; but we shall defer these
issues for the moment and rely instead on the
reader&apos;s intuition. The default rule for home
toWns can now be seen as an instance of the above
pattern:
</bodyText>
<equation confidence="0.246732">
111124C 117
(x/PERSON) PlralV
</equation>
<subsectionHeader confidence="0.815843">
2.2 THE CLOSED WORLD ASSUMPTION
</subsectionHeader>
<bodyText confidence="0.972964368421053">
It seems not generally recognized that the
reasoning components of many natural language
understanding systems have default assumptions
built into thgm. The representation of knOwledge
upon which the reasoner computes does not explic-
itly indicate certain default assumptions. Rather
these defaults are realized as part of the code of
the reasoner, or, as we shall say, following
[Hayes 19771, as part of the reasoner&apos;s process
structure. The most common such default corresponds
to what has elsewhere been referred to as the
closed world assumption [Reiter 19781. In this
section we describe two commonly used closed world
defaults.
2.2.1 Hierarchies
As an illustration of the class of closed
world defaults, consider standard taxonomies
(IS-A hierarchies) as they are usually represented
in the A.I. literature, for example the following:
</bodyText>
<sectionHeader confidence="0.916277666666667" genericHeader="method">
THING
ANIMATE INANIMATE
MAMMAL REPTILE
</sectionHeader>
<bodyText confidence="0.745112">
d(\&apos;■.,
DOG CAT
This has, as its first order logical representation,
the following:
</bodyText>
<equation confidence="0.31932">
(DI)
</equation>
<page confidence="0.868511">
211
</page>
<figure confidence="0.7857104">
(x)DOG(x) MAMMAL(x)
(X)CAT(X) D MAMMAL(x) (2.1)
(x)MAMMAL(x) D ANIMATE(x)
etc.
Now if Fido is known to be a dog we can conclude
</figure>
<listItem confidence="0.753003857142857">
that Fido is animate in either of two essentially
isomorphic ways:
1. If the hierarchy is implemented as some sort of
network, then we infer ANIMATE(fido) if the class
ANIMATE lies &amp;quot;above DOG i.e. there is some pointer
chain leading from node DOG to node ANIMATE in the
network.
2. If the hierarchy is implemented as a set of first
order formulae, then we conclude ANIMATE(fido) if
we can forward chain (modus ponens) with DOC(fido)
to derive ANIMATE(fido). This forward chaining
from DOG(fide) to ANIMATE(fido) corresponds exactly
to following pointers from node DOG to node ANIMATE
Th the network.
</listItem>
<bodyText confidence="0.989393725806452">
Thus far, there is no essential difference be-
tween a network representation of a hierarchy with
its pointer-chasing interpreter and a first order
representation with its forward chaining theorem
proving interpreter. A fundamental distinction
arises with respect to negation. As an example,
consider how one deduces that Fido is not a reptile.
A network interpFeter will determine that the node
REPTILE does not lie &amp;quot;above&amp;quot; DOG and will thereby
conclude that DOGs are not REPTILEs so that
,REPTILE(fido) is deduced. On the other hand,
theorem prover will try to prove ,REPTILE(fido).
Given the above first order representation, no such
proof exists. The reason is dee - nothing in
the reprebentation (2.1) states that the categories
MAMMAL and REPTILE are disjoint. For the theorem
prover to deal with negative information, the
knowledge base (2.1) must be augmented by the
following facts stating that the categories of
the hierarchy are disjoint:
(x)ANIMATE(x) 1INANIMATE(x)
(x)MAMMAL(x) = iREPTILE(xl (2.2)
(x)DOG(x) D-1CAT(x)
It is now clear that a first order theorem proving
interpreter can establish -&apos;REPTILE(fido) by a pure
forward chaining proof procedure from DOG(fido)
using (2.1) and (2.2). However, unlike the earlier
proof of ANIMATE(fido) this proof of REPTILE(fido)
is not isomorphic to that generated, by the network
interpreter. (Recall that the network interpreter
deduces -,REPTILE(fido) by failing to find a pointer
chain linking DOG and REPTILE). Moreover, while
the network interpreter must conteno only with &apos;a
representation equivalent to that of (2.11, the
theorem prover must additionally utilize the nega-
tive information (2.2). Somehow, then, the process
structure of the network interpreter implicitly
represents the negative knowledge (2.2), while
computing only on declarative knowledge equivalent
to (2.1).
We can best distinguish the two approaches by
observing that two different logics are involved.
To see this, consider modifying the theorem prover
so as to simulate the network process structure.
Since the network interpreter tries, and fails, to
establish a pointer chain from DOG to REPTILE using
a declarative knowledge base equivalent to (2.1)
the theorem prover can likewise attempt to prove
REPTILE(fido) using only (2.1). As for the net-
work interpreter, this attempt will fail. If we
now endow the theorem prover with the additional
inference rule:
&amp;quot;If you fail to deduce REPI1LE(fido) then conclude
REPTILE(fido)&amp;quot;
the deduction of REPTILE(fido) will be isomorphic
to that of the network interpreter. More generally,
we require an inference schema, applicable to any
of the monadic predicates MAMMAL, DOG, CAT, etc. of
the hierarchy:
&amp;quot;If x is an individual and P(x) cannot be deduced,
then infer 1P(x)&amp;quot;
or in the notation of the previous section
</bodyText>
<equation confidence="0.9950045">
(x) -V-114.4
X (D2)
</equation>
<bodyText confidence="0.999957363636364">
What we have argued then is that the process
structure of a network interpreter is formally
equivalent to that of a first order theorem prover
augmented by the ability to use the inference
schema (D2). In a sense, a network interpreter is
the compiled form of such an augmented theorem
prover.
There are several potnts worth noting:
1. The schema (D2) is not a first order rule of
inference since the operator 4 is not a first
order notion. (It is a meta notion.) Thus a theorem
</bodyText>
<page confidence="0.996051">
212
</page>
<bodyText confidence="0.986458230769231">
prover which evokes (D2) in order to egtablish
negative conclusions by failure i$ not performing
first order deductions.
2. The schema (02) has a similar pattern to the
default schema (Dl)
3. In the presence of the default schema (02),
the negative knowledge (2.2), which would be
necessary in the absence of (D2), is not required.
As we shall see in the next section, this property
is a general charactertstic of the closed world
default, and leads to a significant reduction in
the complexity of both the representation and
processing of knowledge.
</bodyText>
<subsubsectionHeader confidence="0.923185">
2.2.2 The Closed World Default
</subsubsectionHeader>
<bodyText confidence="0.999831">
The schema (D2) is actually a special case of
the following more general default schema:
</bodyText>
<equation confidence="0.8418945">
(xI /T1 )...(xn/Tn)
, x )
</equation>
<bodyText confidence="0.999001083333333">
If (D3) is in force for all predicates P of some
domain, then reasoning is being done under the
closed world assumption Reiter 1978], In most
A.I. representation schemes, hierarchies are
treated as closed worlri domains. The use of the
closed worid assumption in A.I. and in ordinary
human reasoning extends beyond such hierarchies,
however. As a simple example, consider an airline
schedule for a direct Air Canada flight from
Vancouver to New York. If none is found, one
assumes that no such flight exists. Formally, we
can view the schedule as a data base, and the query
as an attempt to establish DIRECTLY-CONNECTS(AC,
Van,NY). This fails, whence one concludes
-1DIRECTLY-CONNECTS(AC,Van,NY) by an application of
schema (03). Such schedules are designed to be
used under the closed world assumption. They con-
tain only positive information; negative inform-
ation is inferred by default. There is one very
good reason for making the closed world assumption
in this-setting. The number of negative facts
vastly exceeds the number of positive ones. For
example, Air Canada does not directly connec
Vancouver and Moscoweor Toronto and Bombay, or
Moscovand Bombay, etc. etc. It is totally un-
feasible to explicitly represent all such negative
information in the data base, as would be required
under a first order theorem prover. It is
important to notice, however, that the closed
world assumption presumes perfect knowledge about
the domain being modeled. If it were not known,
for example, whether Air Canada directly connects
Vancouver and Chicago, we would no longer be just-
ified in making the closed world assumption wtth
respect to the flight schedule. For by the absenct.
Of this fact from the data base, we would conclude
that Air.Ctnada does not directly connect
Vancouver and Chicago, violating our assumed state
of ignorance about this fact.
The flight schedule illustrates a very common
use of the closed world default rule for purely
extensional data bases. In particular, it illus-
trates how this default factors out the need for
any explicit representation of negative facts.
This result holds for more general data bases. As
an example, consider the ubiquitous blocks world,
under the following decomposition hierarchy of
objects in that world:
</bodyText>
<sectionHeader confidence="0.4836245" genericHeader="method">
OBJECT
BLOCK TABLE
</sectionHeader>
<equation confidence="0.9730905">
CU71N
E r PYRAMID
</equation>
<bodyText confidence="0.99884575">
Let SUPPORTS(x,y) denote &amp;quot;x directly supports y&amp;quot;
and FREE(x) denote &amp;quot;x is free&amp;quot; i.e. objects may be
placed upon x. Then the following general facts
hold:
</bodyText>
<equation confidence="0.973508866666667">
(x/OBJECT)(y/TABLE),SUPPORTS(x,y) (1)
(x/OBJECT),SUPPORTS(x,x) (2)
(x/PYRAMID)(y/BLOCK),SUPPORTS(x,y) (3)
(x y/BLOCK)SUPPORTS(x,y)
,SUPPORTS(y,x) (4)
(x/PYRAMID),FREE(x) (5)
(x y/ BLOCK )(z/TABLE)SUPPORTS(x,y)
nStIPPORTS(z,y) (6)
(x/CUBE)FREE(x) D
(Y/BLOCOISUPPORTS(x,y) (7)
(x/CUBE)(y/BLOCK),SUPPORTS(x,y) D
FREE(x) (8)
(x/TABLE)FREE(x) (9)
Consider the following scene
(D3)
</equation>
<page confidence="0.91855">
213
</page>
<bodyText confidence="0.787652">
This is representable by
</bodyText>
<equation confidence="0.912291875">
SUPPORTS(T,C1) SUPPORTS(T,C2) (10)
SUPPORTS(C1,P1) SUPPORTS(C2,C3)
SUPPORTS(T,P2)
together with the following negative facts
,SUPPORTS(C1,C2) ,SUPPORTS(C2,C1)
,SUPPORTS(C3,C1) ,SUPPORTS(C1,P2)
,SUPPORTS(C3,P1) ,SUPPORA(C3,P2)
,SUPPORTS(C1,C3) ,SUATORTS(C2,P1)
</equation>
<bodyText confidence="0.991984102040816">
Notice that virtually all of the knowledge about the
blocks domatp is negative, namely the fiegative
specific Facts (11), together with the negative
fact1)-(7)1 This is not an accidental feature.
Mot of what we know about any world is negative.
Now I ,first order theorem prover must have
access to all of the facts (1)-(11). For example,
in protringa,SUPP0RTS(C3,C2) it must use (4). Con-
sider instead such a theorem prover endowed with
the additional ability to interpret tht closed
world default schema (D3). Then, in-attempting a
proof of ,SUPPORTS(C3,C2) it tries to show that
SUPPORTS(C3,C2) is not provable. Since
SUPPORTS(C3,C2) cannot be proved, it concludes
,SUPPORTS(C3,C2), as required.
It should be clear intuitively that in the
presence of the closed world default schema (03),
none of the negative facts (1)-(7), (11) need be
represented explicitly nor used in reasohing. This
can be proved, under fairly general conditionc
[Reiter 1978]. One function, then, of the closed
world default is to &amp;quot;factor out&amp;quot; of the represen-
tation all negative knowledge about the domain. It
is of some interest to compare the blocks world
representation (1)-(11) with those commonly used in
blocks world problem-solvers (e.g.(Winograd 1972,
Warren 1974]). These systems do not represent explic-
itly the negative knowledge (1)-(7), (11) but in-
stead use the closed world default for reasoning
about negation. (See Section 3 below for a dis-
cussion of negatiol in A.I. programming languages.)
Although the closed world default factors out
negative knowledge for answering questions about a
domain, this knowledge must nevertheless be avail-
1 The notion of a negative fact has a precise defin-
ition. A fact is negative iff all of the literals
in its clausal form are negative.
able. To see why, consider an attempted update of 33
the example blocks world scene with the new &amp;quot;fact&amp;quot;
SUPPORTS(C3,C2). To detect the resulting inconsis-
tency requires the negative fact (4). In general
then, negative knowledge is necessary for maintain-
in y the integrity of a data base. A consequence of
the closed world assumption is a decomposition of
knowledge into positive and negative facts. Only
positive knowledge is required for querying the
data base. Both positive and negative knowledge
are required for maintaining the integrity of the
data base.
</bodyText>
<subsectionHeader confidence="0.670642">
2.3 DEFAULTS AND THE FRAME PRovm
</subsectionHeader>
<bodyText confidence="0.973885870967742">
The frame problem [Raphael 19711 arises in the
representation of dynamic worlds. Roughly speaking,
the problem stems from the need to represent those
aspects of the world which remain invariant under
certain state changes. For example, moving a par-
ticular object or switching on a light will not
change the colours of any objects in the world.
Painting an object will not affect the locations of
the objects. In a first order representation of
such worlds, it is necessary to represent explicitly
all of the invariants under all state changes.
These are referred to at the frame axioms for the
world being modeled. For example, to represent the
fact that painting an object does not alter the
locations of objects would require, in the situa-
tional calculus of [McCarthy and Hayes 19691 a
frame axiom something like
(x VOBJECT)(y/POSITION)(s/STATE)(C/COLOUR)
LOCATION(x,y,$) LOCATION(x,y,paint(z,C,$))
The problem is that in general we will require a
vast number of such axioms e.g. object locations
also remain invariant when lights are switched on.
when it thunders, when someone speaks etc. so there
is a major difficulty in even articulatinI a de-
ductively adequate set of frame axioms for a given
world.
A solution to the frame problem is a represen-
tation of the world coupled with appropriate rules
of inference such that the frame axioms are neither
represented explicitly nor used explicitly in
reasoning about the world. We will focus on a
</bodyText>
<page confidence="0.993888">
214
</page>
<note confidence="0.386127">
3zt
</note>
<bodyText confidence="0.999952">
proposed solution by LSandewa-11 1972-41. A- related
approach is described in [Hayes 1973]. Sandewall
proposes a new operator, UNLESS, which takes form-
ula W as argument. The intended interpretation of
UNLESS(W) is &amp;quot;W can not be proved&amp;quot; i.e. it is
identical to the operator of this paper.
Sandewall proposes a single &amp;quot;frame inference rule&amp;quot;
which, in the notation of this paper, can be para-
phrased as follows:
For all predicates P which take a state variable
as an argument
</bodyText>
<equation confidence="0.677414666666667">
(x /1-1)...(xn/0)(s/STATE)(f/ACTION-FUNCTION)
(D4)
P(xl,...,xn,f(xl,...,xn,$))
</equation>
<bodyText confidence="0.996719185185185">
Intuitively, (D4) fohnalizes the so-called &amp;quot;STRIPS
assumption&amp;quot; Naldinger 19751: Every action (state
change) is assumed to leave every relation un-
affected unless it is possible to deduce otherwise.
This schema can be used in the following way. say
in order to establish that cube33 is at location A
after box7 has been painted blue:
To establish LOCATION(cube33,A,paint(box7,blue,$))
fail to provegLOCATION(cube33,A,paint(box7,bluets))
There are several.observations that can be
made*
1. The frame inference schema (04) has a pattern
similar to the default schemata (D2) and (03) of
earlier sections of this paper. It too is a
defautt schema.
2. The frame schema (04) is in some sense a dual
of the closed world schema (D3). The former per-
mits the deduction of a positive fact from failure
to establish its negation. The latter provides
for the deduction of a negative fact from failure
to derive its positive counterpart. This duality
is preserved with respect to the knowledge
&amp;quot;factored out&amp;quot; of the representation. Whereas the
frame default elfin-hates the need for certain kinds
of positive knowledge (the frame axioms), the
closed world default factors out the explicit rep-
resentation of negative knowledge.
</bodyText>
<subsectionHeader confidence="0.965286">
2.4 DEFAULTS AND EXCEPTIONS
</subsectionHeader>
<bodyText confidence="0.982301526315789">
A good deal of what we know about the world is
1 [Kramosil 1975] claims to have proved that
Sandewall&apos;s approach is either meaningless or
equivalent to a first order approach. See Section
4 for a discussion of this issue.
nalmogt-alwayr-true, witn a few exceptions. For
example, all birds fly except for penguins,
ostriches, fledglings, etc. Given a particular
bird, we will conclude that it flies unless we
happen to know that is satisfies one of these excep-
tions. Nevertheless, we want it true of birds &amp;quot;in
general&amp;quot; that they fly. How can we reconcile these
apparently conflicting points of view? The natural
first order representation is inconsistent:
(x/BIRD)FLY(x) &amp;quot;In general, birds fly&amp;quot;
(x)PENGUIN(x) BIRD(x) &amp;quot;Penguins are birds
(x/PENGUIN),FLY(x) which don&apos;t fly.&amp;quot;
An alternative first order representation explic-
itly lists the exceptions to flying
</bodyText>
<listItem confidence="0.6214095">
(x/BIRD),PENGUIN(x) &amp;quot;OSTRICH(x) A ... D
FLY(x)
</listItem>
<bodyText confidence="0.9992064">
But with this representation, we cannot conclude of
a &amp;quot;general&amp;quot; bird, that it can fly. To see why,
consider an attempt to prove FLY(tweety) where all
we know of tweety is that she is a bird. Then we
must establish the subgaal
</bodyText>
<subsectionHeader confidence="0.683635">
PENGUIN(tweety) A -,OSTRICH(tweety) A...
</subsectionHeader>
<bodyText confidence="0.9999376">
which is impossible given that we have no further
information about tweety. We are blocked from con-
cluding that tweety can fly even though, intui-
tively we want to deduce just that. In effect, we
need a default rule of the form
</bodyText>
<equation confidence="0.704415">
(x/BIRD) li(PENGUIN(x) V OSTRICH(x) V ...
FLY(x)
</equation>
<bodyText confidence="0.999727875">
With this rule of inference we can deduce
FLY(tweety), as required. Notice however, that
whenever there are exceptions to a &amp;quot;,general&amp;quot; fact
in some domain of knowledge we are no longer free
to arbitrarily structure that knowledge. For ex-
ample, the following hierarchy would be unaccept-
able, where the dotted link indicates-the existence
of an exception
</bodyText>
<sectionHeader confidence="0.90390875" genericHeader="method">
ANIMAL
FLY CRAWL
BAT BIRD
PENGUIN ROBIN
</sectionHeader>
<bodyText confidence="0.99506">
Clearly there is no way in this hierarchy of estab-
lishing that penguins are animals. For hierarchies
the constraint imposed by exceptions is easily
</bodyText>
<page confidence="0.997461">
215
</page>
<bodyText confidence="0.999709714285714">
imitiebiated: If P and Q are nodes with P below Q
and if (x)P(x) n Q(x) is true without exception,
then there must be a sequence of solid links con-
necting P and Q. For more General kinds of know-
ledge the situation is more problematic. One must
be carefiil to ensure that chains of implications do
not unwittingly inherit unintended exceptions.
</bodyText>
<sectionHeader confidence="0.90652" genericHeader="method">
3. DEFAULTS AND &amp;quot;NEGATION&amp;quot; IN A.I.
</sectionHeader>
<subsectionHeader confidence="0.837827">
PROGRAMMING LANGUAGES
</subsectionHeader>
<bodyText confidence="0.972176303030303">
It has been observed by several authors [Haye...
1973, Sandewall 1972, Reiter 1978] that the basic de-
fault operator 4 has,as its &amp;quot;procedural equivalent&amp;quot;
the negation operator in a number of A.I. programming
languages e.g. THNOT in MICROPLANNER [Hewitt 1972,
Sussman et al. 1970], NOT in PROLOG [Roussel 19751.
For example, in MICROPLANNER, the command
(THGOAL &lt;pattern&gt;) can be viewed as an attempt to
prove &lt;pattern&gt; given a data base of facts and
theorems. (THNOT(THGOAL &lt;pattern&gt;)) then succeeds
iff (THGOAL &lt;pattern&gt;) fails i.e. iff &lt;pattern&gt; is
not provable, and this of course is precisely the
interpretation of the default operator 4.
Given that &amp;quot;negation&amp;quot; in A.I. procedural&apos;
languages corresponds to the defiult operator and
not to logical negation, it-would seem that some of
the criticism often directed at theorem proving
from within the A.I. community is misdirected,. For
the so-called procedural approach, often proposed
as an alternative to theorem proving as a represen-
tation and reasoning component in A.I. systems, is
a realization of a default logic, whereas theorem
provers are usually realizations of a first order
logic, and as we have seen, these are different
logics.
In a sense, the so-called procedural vs.
declarative issue in A.I. might better be phrased
as the default vs. first order logic issue. Many
of the advantages of the procedural approach can
be interpreted as representational and computa-
tional advantages of the default operator. There
is a fair amount of empirical evidence in support
of this point of view, primarily based upon the
successful use of PROLOG [Roussel 1975] - a pure
theorem prover augmented with a &amp;quot;THNOT&amp;quot; operator
for such diverse A.I. tasks as problem solving
(Warren 1974), symbolic mathematics [Kanoui 1976],
and natural language question-answering [Colmeraurer
1973].
On the theuretical level, we are just begin-
ning to understand the advantages of a first order
logic augmented with the default operator:
1. Default logic&apos; provides a repretentation language
which more faithfully reflects a good deal of
common sense knowledge than do traditional logics.
Similarly, for many situations, default reasoning
corresponds to what is usually viewed as common
sense reasoning.
2. For many settings, the appropriate default
theories lead to a significant reduction in both
representational and computational complexity with
respect to the corresponding first order theory.
Thus, under the closed world default, negative
knowledge about a domain need not explicitly be
represented nor reasoned with in querying a data
base. Similarly under the frame default, the usual
frame axioms are not required.
There are, of course, other advantages of the
procedural approach - specifically, explicit con-
trol over reasoning - which are not account*d for
by the above logical analysis. We have distin-
guished the purely logical structure of such rep-
resentational languages from their process structure,
and have argued that at least some of their success
derives from the nature of the logic which they
realize.
</bodyText>
<sectionHeader confidence="0.980123" genericHeader="method">
4. SOME PROBLEMS WITH DEFAULT THEORIES
</sectionHeader>
<bodyText confidence="0.942147823529412">
Given that default reasoning has such wide-
splead applications in A.I. it is natural to define
a default theory as a first order theory augmented
by one or more inference schemata like (D1), (D2)
etc. and to investigate the properties of such
theories. Unfortunately, some such theories display
peculiar dnd intuitively unacceptable behaviours.
One dltticulty is the ease with which incon-
sistent theories can be defined, tor example
coupled with a knowledge base with the
single fact -1B. Another, pointed out by [Sandewall
1972] is that the theorems of certain default
theories will depend upon the ordtr in which they
are derived As an example, consider the theory
siB
A
Since A is not provable, we can infer B. Since B
</bodyText>
<page confidence="0.997139">
216
</page>
<bodyText confidence="0.999962106382979">
Is now proved, we cannot infer A, so this theory
has the single theorem B. I instead, we had
started by observing that B is not provable, then
the theory would have the single theorem A. De-
fault theories exhibiting such behaviour are clearly
unacceptable. At the very least, we must demand of
a default theory that it satisfy a kind of
Church-Rosser property: No matter what the order
in which the theorems of the theory are derived,
the resulting set of theorems will be unique.
Another difficulty arises in modeling dynam-
ically changing worlds e.g. in causal worlds or in
text understanding where the model of the text
being bu4lt up changes as mbre of the text is assim
hated. Under these circumstances, inferences
which have been made as a result of a default
assumption may subsequently be falsified by new in-
formation which now violates that default assump-
tion. As a simple example, consider a travel con-
sultant which has made the default assumption that
the traveller&apos;s starting point is Palo Alto and has,
on the basis of this, planned all of the details of
a trip. If the consultant subsequently learns that
the starting point is Los Angeles, it must undo at
least part of the planned trip, specifically the
first (and possibly last) leg of the plan. But how
is the consultant to know to focus just on these
changes? Somehow, whenever a ndw fact is deduced
and stored in the data base, all of the facts which
rely upon a default assumption and which supported
this deduction must be associated with this new
fact. These supporting facts must themselves have
their default supports associated with them, and
so on. Now, should the data base be updated with
new information which renders an instance of some
default rule inapplicable, delete all facts which
had been previously deduced whose support sets
rejied upon this instance of the default rule.
There are obviously some-technical and implementa-
tion details that require articulation, but the
basic idea should be clear. A related proposal for
dealing with beliefs and real world observations is
described in [Hayes 1973].
One way of viewing the role of a default theory
is as a way of implicitly further completing an
undttlying incomplete first order theory. Recall
that a first order theory is said to be complete
iff for all closed formulae W, wither W or 41 is
provable. Most interesting mathematical theories
turn out to be incomplete - a celebrated result
due to Godel. Most of what we know about the world,
when formalized, will yield an incomplete theory
precisely because we cannot know everything - there
are gaps in our knowledge. The effect of a default
rule is to implicitly fill in some of those gaps by
a form of plausible reasoning. In particular, the
effect of the closed world default is to fully com-
plete an underlying incomplete first order Jleory.
However, it is well known that there are insurmount-
able problems associateo with completing an incom-
plete theory like arithmetic. Although it is a
trivial matter conceptually to augment the axioms
of arithmetic with a default rule 41-.4 where W is
any closed formula, we will be no further ahead
because the non theorems of arithmetic are not re-
cursively enumerable. What this means is that
there is no way in general that, given a W, we
can establish that W is not a theorem even if W
happens not to be a theorem. This in turn means
that we are not even guaranteed that an arbitrary
default rule of inference is effective-i.e. there
may be no algorithm which will inform us whether or
not a given default rule of inference is applicable!
From this we can conclude that the theories of a
default theory may not be recursively enumerable.
This situation is in marked contrast to what norm-
ally passes for a logic where, at the very least,
the.rulAs of inference must be effective and the
theorems recursively enumerable.
Finally, it is not hard to see that default
theories fail to satisfy the extension property
[Hayes 1973] which all &amp;quot;respectable&amp;quot; logics do sat-
isfy. (A logical calculus has the extension prop-
erty iff whenever a formula is provable from a set
P of premises, it is also provable from any set P&apos;
such that P&apos;c P&apos;.)
EKramosil 19751 attempts to establish some
general results on default theories. Kramosil
&amp;quot;proves&amp;quot; that for any such theory, the default
rules are irrelevant in the sense that either the
theory will be meaningless or the theorems of the
theory will be precisely the same as those obtain-
able by ignoring the default rules of inference.
Kramosil&apos;s result, if correct, would invalidate the
</bodyText>
<page confidence="0.994123">
217
</page>
<bodyText confidence="0.979339512195122">
main pofht of this paper, namely that defau4t theor-
ies play a prominent role in reasoning about the
world. Fortunately, hi&apos;s &amp;quot;proof&amp;quot; relies on an incor-
rectAefinition of theoremhood 0 that the problem
of characterizing the theorems of a default theory
remain open.
CONCLUSIONS
Default reasoning may well be the rule, rather
than the exception, in reasoning about the world
since normally we must act in the presence of incom-
plete knowledge. Moreover, aside from mathematics
and the physical sciences, most of what we know
about the world has associated exceptions and
caveats. Conventional logics, such as first order
logic, lack the expressive power to adequately rep-
resent the knowledge required for reasoning by de-
fault. We gain this expressive power by introducing
the default operator.
In order to provide an adequate formal (as
opposed to heuristic) foundation for default reason-
ing we need a well articulated theory of default
logic. This requires, in part, a theory of the
semantics of default logic, a suitable notion of
theoremhood and deduction, and conditions under which
the default inference rules are effective and the
set of theorems unique. Since in any realistic do-
main, all of the default schemata of Section 2 will
be in force (together, no doubt, with Qthers we have
not considered) we require a deeper understanding of
how these different schemata interact: Finally,
there is an intriguihg relationship between certain
defaults and the complexity of the underlying repre-
sentation. Both the closed world and frame defaults
implicitly represent whole classes of first order
axioms. Is this an accidental phenomemon or is some
general principal involved?
ACKNOWLEDGEMENTS
This paper was written with the financial sup-
port of NRC grant A 7642. I am indebted to Brian
Funt, Randy Goebel and Richard Rosenberg for their
criticisms of an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.983837" genericHeader="method">
REFERENCES
</sectionHeader>
<reference confidence="0.99024268852459">
Bobrow, D.G. and Winograd, T., (1977). &amp;quot;An Overview
of KRL-0, a Knowledge Representation Language,&amp;quot;
Cognitive Science, Vol.1, No.1, Jan. 1977.
Colmeraurer, A., (1973). Un System de Communication
Home-Machine en Francais, Rapport interne, UER de
37
Luminy, Universite d&apos;Aix-Marseille, 1973.
Hayes, P.J., (1973). &amp;quot;The Frame Problem and Related
Problems in Artificial Intelligence*&amp;quot; in Artificial
and Human Thinking, A. Elithorn and D. Jones (Eds.),
Jossey-Bass Inc., San Francisco, 1973, pp.45-49.
Hayes, P.J., (1977). &amp;quot;In Defence of Logic,&amp;quot; Proc.
IJCAI-5, M.I.T., Cambridge, Mass., August 227237
077, pp. 559-565.
Hewitt, C., (1972). Description and TheoreVcal
Analysir§, (Using Schemata) of PLANNER: AvLanguage for
Proving4Theorems and Matdpulating Models in a Robot,
A.I.Memo No. 251, M.I.T. Plvject MAC, Cambridge,
Mass., April 1972.
Kanoui, h., (1976). &amp;quot;Sortie Aspects of Symbolic
Integrattart via Predicate Logic Programming,&amp;quot;SIGSAM
Bulletin, 10, Nov. 1976, pp. 29-42.
Kramosil, I., (1975). &amp;quot;A Note on Deduction Rules
with Negative Premises,&amp;quot; Proc. IJCAI-4, Tbilisi,
USSR, Sept. 3-8, 1975, pp. 53-56.
McCarthy J. and Hayes, P.J., (1969). &amp;quot;Some
Philosophic Problems from the Standpoint of
Artificial Intelligence,&amp;quot; in Machine Intelligence 4,
B. Meltzer and D. Michie (Eds.&apos;), Edinburgh
University Press, Edinburgh, 1969, pp. 463-502.
Raphael, B., (1971). &amp;quot;The Frame Problem in Problem-
Solving Systems,&amp;quot; in Artificial Intelligence and
Heuristic Programming, N.V. Findler and B. Meltzer
(Eds.), Edinburgh University Press, Edinburgh.
Reiter, R., (1978). &amp;quot;On Closed World Data Bases,&amp;quot;
in Logic and Data Bases, H. 411aire and J. Minker
(Eds.), Plenum Press, New York, to appear.
Roberts, R.B. and Goldstein, I., (1977). The FRL
Manual, A.I. Memo No. 409, M.I.T., Sept. 1977.
Rouslel, P., (1975). PROLOG, Manuel de Reference et
d&apos;Utilisation, Group d&apos;Intelligence Artificielle.
U.E.R. de Marseille, France, 1975.
Sandewall, E., (1972). &amp;quot;An Approach to the Frame
Problem, and its Implementation,&amp;quot; in Machine
InWligence 7, B. Meltzer and D. Michie (Eds.),
Edinpurgh UniverNty Press, Edinburgh, pp. 195-204.
Sussman, G., Winograd, T., and Charniak,E., (1970).
MICRO-PLANNER Reference Manual, A.I. MEMO No.
M.I.T., Cambridge, Mass., 1970.
Waldinger, R., (1975). Achieving Several Goals
Simultaneously, Artificial Intelligence Center
Technical Note 107, Stanford Research Institute,
Menlo Park, Calif., July 1975.
Warren, D., (1974). WARPLAN: A System for Generattng
Plans, Memo No, 76, Dept. of Computational Logic,
University of Edinburgh, June 1974.
Winograd, T., (1972). Understanding Natural
Language, Academic Press, New York, 1972.
Woods, W.A., (1968). &amp;quot;Procedural Semantics for a
Question-Answering Machine,&amp;quot; AFIPS Conference
Proceedings, Vol. 3, Part I, 1968, pp. 457-471.
</reference>
<page confidence="0.792384">
218
38
</page>
<sectionHeader confidence="0.641128" genericHeader="method">
FAIH-BASLE AND NUDL-BASED INI-LKLNLL IN SLMANIIC NETWORKS
</sectionHeader>
<subsectionHeader confidence="0.6497438">
Stuart C. Shsapiro
Department of Comptiter Science
State University of New York at Buffalo
Amherst, New York 14226
Abstract
</subsectionHeader>
<bodyText confidence="0.99942972">
Two styles of performing inferema: in
semantic networks are presented and com-
pared. Path-based inference allows an arc
or a path of arcs between two given nodes
to be inferred from the existence of an-
other specified path between the same two
nodes. Path-based inference rules may be
written using a binary relational calculus
notation. Node-based inference allows a
structure of nodes to be inferred from the
existence of an instaece of a pattern of
node structures. Node-based inference
rules can be constructed in a semantic
network using a variant of&apos;a predicate
calculus notation. Path-based inference
is more efficient, while node-based infer-
ence is more general. A method is de-
scribed of combining the two styles in a
single system in order to taka advantage
of the strengths of each. Applications of
path-based inference rules to the repre-
sentation of he extgpsional equivalence
of intensional concepts, and to the expli-
cation of inheritance in hierarchies are
sketched.
</bodyText>
<sectionHeader confidence="0.990238" genericHeader="method">
1. Introduction
</sectionHeader>
<bodyText confidence="0.996539695652174">
Semantic networks have developed
since the mid sixties [10;11] as a formal-
ism for the representation of knowlEdge.
Methods have also been developing for per-
forming deductive inference on the knowl-
edge represented in the network. In this
paper, we will compare two styles of in-
ference that are used in semantic networks,
path-based inference and node-based infer-
ence. In sections 2 and 3, these terms
will be explained and references to sys-
tems tfiitt use them will be provided. In
sections 4 and 5, the advantages and dis-
advantages of each will be discussed.
Section&apos;s bf 7 and 8 will show how they can
be used to complement each other in a sin-
gle setantic network system, how path-
based Inference can talp represent the ex-
tensional equivalence of intensional con-
cepts, and :low a formalism for writing
path-based inference rules can be used to
explicate the notion of &amp;quot;inheritance&amp;quot; in a
semantic network.
</bodyText>
<sectionHeader confidence="0.987687" genericHeader="method">
2. Path-Based Inference
</sectionHeader>
<bodyText confidence="0.999579857142857">
Let us refer to a relation (perforce
binary) the&apos; is represented by an arc in a
network as an arc-relation. If R is an
arc-relation, an arc labelled R from node
a to mile b represents that the relation-
ship aRb holds. It may be that this arc
is not present in the network, but aRb may
be inferred from other information present
in the network and one or more inference
rules. If the other information in the
network is a specified path of arcs from a
to b, we will refer to the inference as
path-based. The ways in which such paths
may be specified will be developed as this
paper proceeds.
The two clearest examples of the gen-
eral use of path-based inference are in
SAMENLAQ II [18] and Protosynthex III (131.
Both these systems use what might be call-
ed &amp;quot;relational&amp;quot; networks rather than
&amp;quot;semantic&amp;quot; networks since arc-relations
include conceptual relations as well as
structural relations (see [14] for a dis-
cussion of the difference). For example,
in Protosynthex III there is an arc label-
led COMMANDED from the node representing
Napoleon to the node representing the
French army, and in SAMENLAQ II an arc la-
belled EAST.OF goes from the node for
Albany to the node&apos; for Buffalo. Both Bye
tems use relational calculus expresiions
to form path-based &apos;inference rules. The
following relational operators are employ-
ed (we here use a variaht of -the earlier
notations):
</bodyText>
<listItem confidence="0.994868">
1. Relational Converse -- If R is a
relation, RC is its converse.
So, Wx„y(xRCy &lt;&gt; yRx).
2. Relational Composition -- If R
and S are relations, R/$ is R
composed with S. So,
Vx,y(xR/Sy &lt;-&gt; ax(xRs 6 aSy)).
3. Domain Restriction -- If R and S
</listItem>
<bodyText confidence="0.7212414">
are relations, ;OR e the re-
lation R with its domain re-
stricted to those objects that
bear the relation 3 to z. So,
Vx,y,x(x(S ORy &lt;-4 (xSag C xRy))
</bodyText>
<listItem confidence="0.623587">
4. Range Restriction -- If R and S
</listItem>
<page confidence="0.997253">
219
</page>
<note confidence="0.6478">
361
</note>
<construct confidence="0.77169475">
are relations, R(S s) is the re-
lation R with its range restrict-
to those objects that bear the
relation S to s. So,
</construct>
<table confidence="0.885211538461538">
Vx,y,s(xfl(S s)y &lt;-› (xRy 6 ySx)).
4
5. Relational Intersection -- If R
and S are relations, AS is the
intersection of R and S. So,
lizty(x/Wy (:Ry xSy)).
Notice that VQ,R,S,x,y,x(xR(Qrs)/Sy &lt;-4
xRiN 4704) so we can use the notation
R(Q x)S unambiguously:
In SAMENLAQ II, path-based inference
rules&apos; are entered, by using the relational
operators to give alternate definitions of
simple arc labels. For example (again in
</table>
<figure confidence="0.8149065">
a variant notation):
-EAST.OF + EAST.OF/EAST.Of
declares that EAST.OF is transitive
SOUTH.OF + NORTH.OFC
declares that
Vx,y(yNORTH.OFx xSOUTH.OFy)
</figure>
<figureCaption confidence="0.665119">
FATHER.OF 4- (GENDER MALE)PARENT.OF
declares that a father is a male
parent.
</figureCaption>
<bodyText confidence="0.947355352941176">
SIR [11] is another relational net-
work system thak uses path-based inference.
Although the original expressed inference
rules in the form of general LISP func-
tions, the reproduction in [16, Chap. 73
uses the notion of path grammars. The re-q.
lation operators listed above are augment-
ed with Rs, meaning zero or more occur-
rences of R composed with itself, R+,
meaning one or more occuKrences of R com-
posed with itself, and RS, meaning the
union of R and S. The following relations
are used:
x EQUIV y
x SUBSET y
z MEMBER y
x POSSESS y
</bodyText>
<note confidence="0.68898425">
chies have been abused as well as used.
In Section 8, we will propose a method
authOrS can use to describe their hier-
chies precisely.
</note>
<figureCaption confidence="0.82076">
FIGURE 1: ISA hierarchy based on that of Collins
and Quillian
</figureCaption>
<sectionHeader confidence="0.98015" genericHeader="method">
3. Node-Based Inference
</sectionHeader>
<bodyText confidence="0.986734375">
Several semantic network systems in-
corporate methods of representing general
rules in a semantic network version of
predicate calculus. Among these systems
are those of Shapiro [14;15;17], Kay [7],
Hendrix [6], Schubert [12], and Pikes and
Hendrix [3]. Figure 2 shows such a. net-
work deduction rule representing
</bodyText>
<figure confidence="0.89091825">
IdzEzeMAN Ry[yeWOMAN g xLOVES01.,
Figure 3 shows a rule for
W[reTRANSITIVE
Vx,y,x(xry 6 yrs + xrz)].
</figure>
<figureCaption confidence="0.649095">
FIGURE 2: A semantic network deduction rule for
</figureCaption>
<bodyText confidence="0.934444">
Next seMAN 4. ay (yEWOMAN &amp; sLOVESy)
The network formalism employed is that of
Shapiro [15;17]. These deduction rules
employ pattern nodes (P1, P2, P3, P4, PS
P6, P7), each one of which represents a
pattern of nodes that might occur in the
network. We will therefore call this kind
of inference rule a node-hased inference
rulp. Pattern node are related to each
other by ruie nodes, each of which repre-
sent a propositional operator, or, equiva-
lently, an inference mechanism. For exam-
ple, R2 represents the rule that if an in-
stance of P1 occurs in the network, an in-
stance of R1 with the same substitution
means x and y are
the same individual
means x is a subset
of y
means x is a member
of the set y
means x owns a mem-
ber of the set y
x POSSESS-BY-EACH y means every member
of the set x owns a
member of the set y.
To determine if x POSSESS y, the Abtwork
is searched using the following rule:
POSSESS 4- EQUIV*
/(POSSESS
v(MEMBER/SUBSET./POSSESS-BY-EACH))
/SUBSET*
The widest use of path-based rider-
ence is in ISA hierarchies, Fig. 1 is
based on probably the most famous ISA
hierarchy, that of Collins and Quillian
[2]. The two important rules here are
</bodyText>
<footnote confidence="0.534361666666667">
ISA 4- ISA*
and PROP + ISA*/PROP
As McDermott [8] points out, ISA hierar-
</footnote>
<page confidence="0.944263">
220
</page>
<bodyText confidence="0.992611">
for x may be deduced. Quantification is
represented in this notation by an arc-re-
lation between a rule node and the vari-
able nodes bound in the rule. For example,
</bodyText>
<note confidence="0.460716666666667">
X is bound by a universal quantifier in R2
and y is bound by an-existential quanti-
fier in R1.
</note>
<figureCaption confidence="0.642224">
FIGURE 3: A semantic network deduction rule for
---WITETRANSITIVE Vx,y,x(xry yrs 4 xrs))
</figureCaption>
<bodyText confidence="0.978884764705882">
To see how a node-based inference
proceeds, consider the network of Figure 4
in conjunction with the rule of Figure 3,
and say. that we wish to decide if
4A SUPPORTS C. The network that would rep-
resent that A SUPPORTS C matches P7 with
the variable binding (x/A, r/SUPPORTS,
s/CI. P4 in the binding (r/SUPPORTSJ is
matched against the network and is found
to successfully match M1. P5lx/A,
r/SUPPORTS, y/y] and P6(Y/Y, r/SUPPORTS,
s/C) are then both matched against the
network and each succeeds with a consis-
tent binding of y to B. The rule thus
succeeds and A SUPPORTS C is deduced.
(Details of the bindings and the match
routine are given in [151.)
</bodyText>
<figureCaption confidence="0.488013">
FIGURE 4: A network data base asserting that
</figureCaption>
<sectionHeader confidence="0.9541705" genericHeader="method">
A SUPPORTS B, B SUPPORTS C and
SUPPOR1S C TRANSITIVE
</sectionHeader>
<bodyText confidence="0.979810090909091">
It should be noted that set inclusion
was represented by an arc (ISA) in Section
2, but set membership is being represented
by a node (with a MEMBER, CLASS &amp;quot;case
frame&amp;quot;) in this section. The nodal repre-
sentation is required by node-based infer-
ence rules and is consistent with the no-
tion that everything that the Aetwork
&amp;quot;knows&amp;quot;, and every concept to which the
network can refer is represented by a
node.
</bodyText>
<sectionHeader confidence="0.750876" genericHeader="method">
4. Advantages of Node-Based Inference
</sectionHeader>
<bodyText confidence="0.99189605">
The adVahtiges of-node-based infer-
ence stem from the generality of the syn-
tax of node-based inference rules. Path-
based rules are limited to binary rela-
tions, have a restricted quantification
structure and required thatan arc between
two nodes be implied by a path between the
same two nodes. Rule R2 of tigure.2 could
not be written as a path-based rule, and,
although the transitivity of SUPPORTS
could be expressed by a path-based rule
(SUPPORTS* SUPPORTS+), the &amp;quot;second order&amp;quot;
rule R4 of Figure 3 could not.
Let us briefly consider how rule R4
is constructed, whether it really is or is
not a second otder rule, and why it could
not be expressed as a path-based rule.
Rule R4 supplies a rule for use with
transitive relations. In order to assert
that a relation is transitive (e.g. asser-
tion node M1 of Figure 4), the relation
must be represented as a node, rather than
as an arc. This also allows quantifica-
tion over such relations, since in all
node-based inference rule formalisms vari-
ables may only be substituted for nodes,
not for arcs. Since the relation is a
node, another node must be used to show
the relationship of the relation to its
arguments (e.g. nodes M2 and M3 in Figure
4): Thus, R4 is really a first order rule
derived from the second order rule
ir[reTRANSITIVE • wafx(xry 6 yrs 4. xrx))
by reducing r to an individual variable
and introducing a higher order relation,
AVO, whose second argument is a conceptual
relation and whose other arguments are
conceptual individuals. So R4 is more
accurately seen as the first order rule
vrfreTRANSITIVE
</bodyText>
<subsubsectionHeader confidence="0.329705">
Vx,y,s(AVO(x,r,y) L AVO(yIrds)
</subsubsectionHeader>
<bodyText confidence="0.9940891875">
AVO(xir,$))].
In this view, the predicates of semantic
networks are not the nodes representing
conceptual relations, but the different
case frames. Rule R4 cannot be represent-
ed as a path-based rule because it is a
rule ;bout the relation AVO, and AVO is a
trinary, rather than a binary relation.
Although some node-based inference
rules cannot be expressed by path-based
inference rules, it is easy to see that
any path-based inference rule can be ex-
pressed by a node-based inference rule, as
long as we are willing to replace some
arc-relations by nodes and higher order
predicates.
</bodyText>
<sectionHeader confidence="0.924634" genericHeader="method">
5. Advantages of Path-Based Inference
</sectionHeader>
<bodyText confidence="0.9996388">
The major advantage of path-based in-
ference is efficiency. Carrying out a
Okth-based inference involves merely
checking that a specified path exists in
the network between two given nodes (plus,
</bodyText>
<figure confidence="0.614865">
B R CLAS
TRANSITIVE
</figure>
<page confidence="0.993107">
221
</page>
<bodyText confidence="0.999008904761905">
perhaps, some side paths to specified
nodes- required by domain and range restric-
tions). Thls is a well understood and re-
latively efficient operation, especially
compared to the backtracking, intersection,
or unification operations required to
chmk the consistency of variable substi-
tutions iii nodi=B-add&apos; Thference rules.
Moreover, path following seems to
many people to be what semantic networks
were originally designed for. The major
search algorithm of Quillian&apos;s Semantic
Memory is a bi-directional search for a
path connecting two nodes [10, p. 249].
Also, the ability to do path tracing is a
motivation underlying ISA hierarchies, and
is why the Collins and Quillian results
[2) gained such attention. These effi-
ciencies are lost by replacing path-based
inference rules by node-based inference
rules.
</bodyText>
<subsectionHeader confidence="0.854236">
6t Combining Path-Based and
Node-Based Inference
</subsectionHeader>
<bodyText confidence="0.99962593457944">
We begin the task of unifying path-
based and node-based inferences by noting
the formal equivalence between an arc-re-
lation and a two case case frame., Figure
5 illustrates this using ISA vs. SUB-SUP.
Figure 5a shows the use of the ISA arc-re-
lation to represent that canaries are -
birds-. Figure 5b represents the same re-
lationship by a SUB-SUP case frame, and
has the advantage that the relationship is
represented by a node, M4. Figure Sc is a
redrawing of 5b, using the arc label SUB-
to represent the relation SUBC. (It is
generally understood in semantic network
formalisms that whenever,an arc represent-
ing a relation R goes front some node n to
some node m, there is also an arc repre-
senting RC going from in to n). Figure Sc
clarifies the notion that we may think of
ap indtance of a two case case frame (such
as 114) as both an arc and a node if we are
willing to recalibrate the measurement of
time it takes to follow one arc-relation
to be the time it takes to folloW two
arcs. We can replace all instances of ISA
in the path-based inference rules of
Section 2 by the composition SUB-/SUP and
still have valid rules except that we now
have paths on the left of the &amp;quot;+&amp;quot; symbol.
FIGUPE 5: An illustration of the equi-
valence of an arc-relation to a two case
case frame. a) Representing set member-
ship as the ISA arc-relation. b) Repre-
senting set membership as a SUB-SUP case
frame. c) Redrawing (b) so it looks like
(a).
Let us, therefore, extend our syntax
of path-based inference rules to allow a
path of arc compositions on the left of
the &amp;quot;*&amp;quot; symbol. The rule ISA + ISA*
states that whenever there is a path of
ISA arcs from node n to mode in, we can in-
fer a &amp;quot;virtual&amp;quot; ISA arc directly from n to
in which we may, if we-wish, actually add
to the network. SiMtkar1V, let the rule
SUB-/SUP + (SUB-/SUP). state tnat whenever
a path of alternating SUB- and SUP arcs
goes from node n to node in, we can infer a
&amp;quot;virtual&amp;quot; node with SUB to n and SUP to m
which we may, if we Wish, actually add to
the network.
We now have a formalism for specify-
ing path-based inference rules in a neta
work formalism that represents binary con-
ceptual relations by two case case frames.
This would allow, for example, for a more
unified representation in the SNIFFER
system (3), in which node-based inference
rules are implemented and built-in path
based inference rules are used for set
membership and set inclusion, both of
which are represented only by arc-rela-
tions. The formalism presented here
would allow set membership and set inclu-
sion assertions to be represented by
nodes, permitting other assertions to
reference them, without giving up the
efficiency of built-in routines to imple-
ment the set inclusion hierarchies.
We desire, however, a more general
unification of path-based and node-based
inferences. There are two basic routines
used to implement node-based inferences
(although specific implementations may
differ). One is the match routine that is
given a pattern node and,finds instances
of it in the network, and the other is the
routine that interprets the quantifiers
and connectives to carry out the actual
deduction. The match routine can be en-
hanced to make use of path-based inference
rules. Consider a typical match routine
used in the dedubtion in Section 3 of
A SUPPORTS C from the network of Figure 4
and the rule of Figure 3, and let us in-
troduce the notation that if P is a path
of arcs and n is a node, P[n) represents
the set of nodes found by following the
path P from the node n. In the example,
the match routine was given the pattern
P4 to match in the binding Er/SUPPORTS).
It was leble to find MI by intersecting
CLASSC(TRANSITIVE) with MEMBERC[SUPPORTS].
Now, let us suppose that the path-based
inference rule CLASS .4- CLASS/ (SUB-AUP )*
has been declared in such a way that the
match routine could use it. Vie match
routine would intersect MEMBER [SUPPORTS)
with (CLASS/(SUB-/SUP)*)CITRANSITIVE] and
be able to find a virtual node asserting
that SUPPORTS is TRANSITIVE even if a long
chain of set inclusions separated them.
The proposal, therefore, is this: any
are-relation in a semantic network may be
defined in terms of a path-based inference
rule which the match routine is capable of
using when finding instances of pattern
</bodyText>
<figure confidence="0.998346428571429">
(a)
(b)
(c)
BIRD
SUP
SUB-
CANAnY
</figure>
<page confidence="0.993782">
222
</page>
<bodyText confidence="0.995877555555556">
nodes. This completes the general unifica-
tion of path-barsa and node-based infer-
euce we desired. Since path-based infer-
ence is embedded in the match routine,
while node-based inference requires the
quantifier and connective interpreter, the
difference is reminiscent of the differ-
ence between subconscious inference and
conscious reasoning.
</bodyText>
<sectionHeader confidence="0.51966" genericHeader="method">
7. Application to Extensional
</sectionHeader>
<subsectionHeader confidence="0.7974515">
Eguivaience of
Intensional Concepts
</subsectionHeader>
<bodyText confidence="0.991897942622951">
A basic assumption of semanticenet-
works is that each concept is represented
by a single node and that all information
about a concept is reachable from its node.
Yet, since Woods&apos; discussion [20], most
Semantic network authors have agreed that
a node represents an intensional, rather
than an extensional concept. How should
we handle the information that two differ-
ent intensional concepts are extensionally
equivalent?
Let us illustrate this by a story
(entirely fictional). For the last year
we have heard of a renowned surgeon in
town, Dr. Smith, known for his brilliance
and dexterity, who saved the life of the
famous actress Maureen Gelt by a difficult
heart transplant operation. Meanwhile, at
several social gatherings, we have met
someone by the name of John Smith, about
five feet, six inches tall, black hair and
beard, generally 44sheveled and clumsy.
We now discover, much to our amazement
that John Smith and Dr. Smith are one and
the samel In our semantic netwokk, we
have one node for Dr. Smith connected to
his attributes, and another for John Smith
connected to his attributes. What are we
to do? Although we now know that John
Smith saved the life of Maureen Gelt and
that Dr. Smith has black hair, surely we
cannot retrieve that information as fast
as that Dr. Smith is a surgeon and that
John Smith is 5&apos;6&amp;quot; tall, If we were to
combine the two nodes by taking all the
arcs from one node, tying them to the
other and throwing away the firist, we
would lose this distinction. We pust in-
troduce an assertion, say an EQUIII&apos;EQUIV
case frame, that represents the fact that
Dr. Smith and John Smith, different inten-
sional concepts, are extensionalay the
same.1 How are we to use this assertion?
Ignoring for the moment referentially
opaque contexts (&amp;quot;We didn&apos;t know that John
Smith was Dr. Smith.&amp;quot;), how can we express
the rulea.that if n EQUIV&apos;/EQUIV m, than
anything true of n is true of m? Our node
based inference rules cannot express this
rule because expressing &amp;quot;anything-true of
n&amp;quot; requires quantifying over those higher
order case frame predicates such as
1The psychological plausibility of this
discussion is supported by the experiments
of Anderson and Hastie [1] and of McNabb
191.
and MEMBER-TLASS. One possibility is to
use lambda abstraction as Schubert does
[12]. Each n-ary higher order predicate
involving some node becomes a unary predi-
cate by replacing that node by a lambda
variabIe. Thus, &amp;quot;Dr. Smith saved Maureen
Gelt&apos;s life&amp;quot; becoMes an instance of the
unary predicate X(x)(x saved Maureen
Gelt&apos;s life] applied to Dr. Smith. Using
a PRED-ARG case frame, it is easy to rep-
resent the rule
Vx,yos [EQUIV-EQUIV(x4,0 6 PRED-AROxpx)
PRED-ARG(YOM-
The trouble with this solution is, how&apos; are
we to retrieve this information as a fact
about Maureen Galt? Must we also store
X(x)(Dt. Smith saved x&apos;s life]
(Maureen Gelt)?
This duplication is unsatisfying. An al-
ternative is to include in the path-based
inference rule defining each arc-relation
the path (EQUIV-/EQUIV)*. For example,
AGENT 4- AGENT/(EQUIV-/EQUIV)*, and CLASS
CLASSP(EQUIV-/EQUIV)*/(SUB-/SUP)*)*.
Although this solution requires more rules
than the lambda abstraction solution, and
the rules look complicated, it avoids the
duplication of the same assertion in dif-
ferent forms and the postulation of con-
ceptual predicates such as A(x)(x. saved
Maureen Gelt&apos;s life].
Hays&apos; cognitive networks [4;5] In-
clude a scheme similar to the one proposed
here. Each assertion about Dr. Smith
would refer to a different node, each with
an MST (manifestation) arc to a common
node. This node would represent the in
tension of Dr. Smith, while the others
represent Dr. Smith as surgeon, Dr. Smith
as saviour of Maureen Gelt, etc. Presum-
ably, when Hays&apos; network learns of the
identity of Dr. Smith with John Smith, a
new node is introduced with msT arcs from
both Dr. Smith and John Smith.I Dr. Smith
and John Smith are then seen as two mani-
festations of the newly integrated Dr.
John Smith. Hays presumably uses an
MST*/(MSTC)* path where we propose an
(EQUO-/EQUIV)* path.
Blocking referentially opaque con-
texts seems to require introducing rela-
tional °implement. For any path. P and
nodes xlIkand y, let xPy hold just in case
a path P from x to y does not exist in the
network.* We might block referentially
opaque contexts by including the domain or
range restriction (013J-/VERB/MEMBER-/CLASS
OPAQUE) in the arc definitions.
8. Applicatir to the Exlication
o Inheritance
As was mentioned in section 2, many
2Actually, Hays&apos; networks have not yet
been implemented, and I have been warned
[R. Fritzson, personal communication] that
the implementation may differ from what I
have supposed.
</bodyText>
<sectionHeader confidence="0.367406" genericHeader="method">
AVO
</sectionHeader>
<page confidence="0.996668">
223
</page>
<bodyText confidence="0.997902555555556">
Semantic networks include inheritance
(ISA) hierarchies. Often these are at
lest vague and at worst inconsistent. We
propose that the inheritance properties of
these hierarchies be clearly defined by
path-based inference rules using the syn-
tax we are ptesenting here or some other
well defined syntax. We do not say that
all systems should be able to input and
interpret such rules, but only that auth-
ors use such rules to explain clearly to
their readers how their hierarchies work.
Before this proposal is feasible, we
must be able to handle two more situations.
The first is the exception principle,
first expressed by Raphael [11, p.85] and
succinctly stated by Winograd as, &amp;quot;Any
property true of a concept in the hier-
archy is implicitly true of anything link-
ed below it, unless explicitly contradict-
ed at the lower level&amp;quot; [19, p.197]. To
allow for this, let us introduce an excep-
tion operator. If P and Q are paths and
x and y are nodes, let xP\Qy hold just in
case there is a pall described by P from x
toy and no path of equal or shorter
length described by Q from x to y. To see
that this suffices to handle the exception
principle, consider the hierarchy of
pigure 6, where, to make things more in-
teresting, we have postulated a variety of
flying penguins. We have also taken the
liberty of explicitly representing that
CAN-FLY and CAN-NOT-FLY are negations of
each other. &apos;The rule for inheritance
in this hieraray is
</bodyText>
<figure confidence="0.497534">
PROP * (ISA*/PROP) \ (ISWPROP/NOT).
ISA
(MALE-FLYING-PENGUIN
FIGURE 6 An ISA hierarchy illustrating the exception
principle.
</figure>
<construct confidence="0.944479769230769">
The other situation we must discuss
is &amp;quot;almost transitive&amp;quot; relations such as
SIBLING. SIBLING is certainly symmetric,
but it cannot be transitive since it is
irreflexive. Yet your sibling&apos;s sibling
is your sibling as long as he/she is not
yourself. This is what we mean by &amp;quot;almost
transitive.&amp;quot; Note that for any relation;
R, R*g(e) is the identity relation. Lel
us call it I. Then for any relation P,
let PR be PET. PR is the irreflexive
restriction of P We can use this to de-
fine SIBLING as
</construct>
<sectionHeader confidence="0.509723" genericHeader="method">
SIBLING * (STBLINGvSIBLINGC)*R.
</sectionHeader>
<bodyText confidence="0.9982686">
We suggest that the syntax for path-
based inference rules is now complete
enough to explicate the inheritance rules
of various hierarchies. The complete syn-
tax will be summarized in the next section
</bodyText>
<sectionHeader confidence="0.727787" genericHeader="method">
9. Summary
</sectionHeader>
<bodyText confidence="0.993302620689655">
We have presented and compared two
styles of inference in semantic networks,
path-band inference and node-based infer-
ence. The former is more efficient, while
the latter is more general. We showed the
equivalence of an arc-relation to a two
case case frame, and described how path-
based inference could be incorporated into
the match routine of a node-based infer-
ence mechanism, thereby combining ttle
strengths of the two inference styles. We
discussed the use of equivalence paths to
represent the extensional equivalence of
intensional concepts. Finally, we urged
authors of inheritance hierarchies to ex-
plicate their hierarchies by displaying
the path-based inference rules that govern
inheritance in them.
We also presented a syntax for path-
based inference rules which can be summar-
ized as follows:
1. A path is either an arc-relation or a
path as described in part 2 enclosed
in parentheses. Parentheses may be
omitted whenever an ambiguity does not
result.
2. If P and Q are paths and x, y, and a
are nodes, paths may be formed as
follows:
</bodyText>
<listItem confidence="0.747340428571428">
a. Converse: if P is a path from x
to y, PU.is a path from y to x.
b. Composition: if P is a path from
x to z and Q is a path from a to
y, P/Q is a path from x to y.
c. Composition zero or more times:
If P composed with itself zero or
more times describes a path from x
to y, P* is a path from x to y.
d. Composition one or more times: If
P composed wrip Itself one or more
times is .a path from x to y, P+ is
a path from x.to y.
e. Union: If P is a path from x to y
or Q is a path from x to y, PvQ is
a path from x to y.
f. Intersection: If P is a path from
x to y and Q is a path from x to
y, PSQ is a path from x to y.
g. Complement:
from x to y,
</listItem>
<sectionHeader confidence="0.429905" genericHeader="method">
Y.
</sectionHeader>
<reference confidence="0.467611555555556">
h. trreflexive
a path fiom
a path from
1. Exception: If P is a path from x
to y and there is no path Q of
length equal to or less than the
length of P, P\Q is a path from x
to y.
3. Domain restriction: If P is a
</reference>
<figure confidence="0.997973142857143">
CIOAARIES
PROP
CAN-NOT-FLY
PENGUIN
ISA
eve
EMPEROR-
ENGUINS
PROP
BIRDS
ISA
A
FLYING-PENGUINS
q 3
</figure>
<bodyText confidence="0.758875">
If there is no path P
P is a path from x to
restriction: If P is
x to y and xiay, PR is
x to y.
</bodyText>
<page confidence="0.992703">
224
</page>
<bodyText confidence="0.640198454545455">
path from x to y and&apos;Q
from x to a, (Q OP is
x to y.
k. Range restriction: If
from x toy and- Q is a
to a, P(Q is a path
is a path
a Oath from
P is a path
path from y
from x to
</bodyText>
<note confidence="0.928432090909091">
Psychology Program, Department et Psy-
chology, Indiana University,
Bloomiqgton, IN. August, 1977..
10. Quillian, M.R.
M.14inksy, ed.
Processin9, MIT
1968, /27-270.
qie
Semantic memory. In
Semantic Information
Press, Cambridge, MA,
</note>
<sectionHeader confidence="0.711326" genericHeader="method">
3. A path-based inference rule is of the
</sectionHeader>
<bodyText confidence="0.981713533333333">
form &lt;defined path&gt; + &lt;defining path&gt;
where &lt;defining path&gt; is any path de-
scribed by parts 1 or 2 above, and
&lt;defined path&gt; is either a) a single
arc-relation, or b) a composition of n
arc relations for are fixed n, i.e.
using only &amp;quot;/&amp;quot;, not &amp;quot;*&amp;quot; or &amp;quot;+&amp;quot;. The
rule is interpreted to mean that if
the &lt;defining path&gt; goes from some
node x to some node y then: a) the arc
that id the &lt;defined path&gt; is inferred
to exist from x to y; b) the n arcs
that are the &lt;defined path&gt; and n-1
new intermediate nodes are inferred to
exist from x to y.
</bodyText>
<sectionHeader confidence="0.998364" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99848945054945">
1. Anderson, J. and Hastier R. Individu-
ation and reference&apos; in mewary: proper
names and definite descriptions. Cog-
nitive Psychology 6, 4 (October, 1974),
495-5)4.
2. Collins, A.M. and Quillian, R. Re-
trieval time from semantic memory. J.
ofVerbalLearniirgand Verbal avior
3. Fikes, R. and Hendrix, G. A network-
based knowledge representation and its
natural deduction system. Proc. Fifth
Int. Jt. Conf. on Artificial Intelli-
gence, Dept. of-Computer Science,
Carnegie-Mellon University, Pittsburgh,
1977, 235-246.
4. Hays, D.G. Cognitive Structures.&apos; un-
published ms. Dept. of LinguIstict,
SUNY at Buffalo, Amherst, Ni.
5. Hays, D.G. Types of processes on cog-
nitive networks. In L.S. Olschki,
Mathematical Linguistics, Frienze,
Pisa, 1977, 523-53.
6. Hendrix, G.G. Expanding the utility
of semantic networks through partition
ing. AdvareFourth
Int. .7t:-tOnf.--bn AitifIdLil-tiftkIli-
geR2e, MIT Al Laboratory, Cambrigge,
MA, 1975, 115-121.
1. Kay, M. The MIND system. In R.
Rustin, ed. Natural Language Process-
ing, Algorithmics Press, New Yoik,
1973, 155-188.
8. McDermott, D. Artificial intelligence
meets natural stupidity. SIGART
Newsletter, 57 (April, 1976), 4-9.
9. McNabb, S.D. The effects of encoding
strategies and age on the memory re-
presentation for sentences containing
proper names and dtlAnite descriptions
Report No. 77-3, Ind-iana Mathematical
11. Raphael, B. SIR: semantic informa-
tion retrieval. In M. Minsky, ed.
Semantic Information Pkocessing, MIT
Press, Cambridge, MA., 1901, 33-145.
12. Schubert, LAC. Extending the exp4es-
sive power of semantic networks.
Artificial Intelligence 7, 2 (Summer,
1976), 10498.
13. Schwarcz, RM., Burger, J.F., and
Simmons, R.F. A deductive question-
answerer for natural language infer-
ence. CACM 13, 3 (March, 1970), 167-
183.
14. Shapiro, S.C. A net structure for
semantic information storage, deduc-
tion and retrieval. Proc. Second Int.
Artificial
Th-e-Btitlii-CompuW•on,
1971, 512-523.
15. Shapiro, S.C. Representing and locat-
ing deduction eules in a semantic
network. Proc. Workshop on Pattern-
Directed INTWFWErfirstems. In siaRT
Newsletter, C3 (June, 1977), 14-TE----
16. Shapiro, S.C. Teciii•elifitrUfi-ir
cial intelligence. D. Van NoStrañd
New York, i97§.
17. Shapiro, S.C. The SNePS semantic net-
Work processing system. In N. Findler,
ed. Associative Networks -- The
.....21r3tzlItn-Use of Knowledge in
Computers,
mic Press, New York,
in press.
18. Shapiro, S.C. and Woodmansee, G.H. A
net structure based relational ques-
tion answerer: description and exam-
ples. Proc. Int. Jt. Conf. on
Artificial Intelligence, The MITRE
Corp., Bedferd, MA., 1969, 325-346.
19. Winograd, T. Frame representations
and the declarative/procedural Contro-
versy. In D.G. Bobrow and A. Collins,
eds. Representation and Understandings
Academic Prelim- Inc., New York, TrAFTT-
185-210.
20. Woods, W.A. What&apos;s in a link: Fowl-
dations for semantic networks. In D.
G. Bobrow and A. Collins, ens. Repre-
sentation and Understanding, Academic
Press, Inc., New York, 1975, 35-82.
</reference>
<figure confidence="0.779888">
•
</figure>
<page confidence="0.964658">
225
</page>
<subsectionHeader confidence="0.4748575">
The&apos;Representation of Dernable Information in Memory:
When What Might Have Been Left Unsaid Is Said
</subsectionHeader>
<bodyText confidence="0.945809605263158">
Rand J. Spiro, Joseph Esposito, and Richard J. Vondruska
Center for the Study of Reading
University of Illinois at Urbana-Champaign
It is now widely accepted that natural lan-
guage comprehension is a constructive process.
Information in. kmcourse interacts with a variety
of impinging FOntextual factors (inclading, most
prominently, the comprehender&apos;s pre-existing,
knowledge) in an active, creative process that
results in understandings not derivable by any
solely linguistic or logical analysis (c.f.,
Bransford &amp; Mrrarrell, 1975; Spiro, 1977, in
press). Acceptance of the constructive view of
comprehension entails a concomitant delimitation
of the range of possible theories of mental
representation. Knowledge structures must possess
some capability for detecting the pragmatic, as
well as logical,: implkations of the incomplete
data contained in discourse (c.f., Charniak,
1974; Minsky, 1975; Rumeihart &amp; Ortony, 1977,
Schenk &amp; Abelson, 1977). In other words, know-
ledge structures must contain considerable
informatian about the way the world usually works.
This characteristic of representation is useful
and efficient because natural and social contexts
do produce sufficient constraints on worldly
events and ideas as to make them, to a limited
extent, orderly and predictable.
However,a point often overlooked is that
these same knowledge structures, with their
information about the world&apos;s orAerlinessi, may
allow for more efficient processing and memorial
representation of explicit information in dis-
course, in addition to their role in deriving
implicit information. This paper will be con-
cerned with the psychological processing of
(imperfectly) predictable or derivable informa-
tion that is nevertheless explicit in discourse.
</bodyText>
<subsectionHeader confidence="0.851584">
Predictable Information in Discourse
</subsectionHeader>
<bodyText confidence="0.999913985074627">
Despite the fact that most research on
inferential firocesses in comprehension has been
concerned with generation of implicit informa-
tion, much inferentially related information
is embodged explicitly tri discourse. We are
referring here primarily to pragmatic inferences,
i.e., implications that are usually but not
necessarily true. Language is infrequently
characterized by absolute redundancy; semantic
content is rarely &amp;quot;repeated,&amp;quot; except for special
purposes such as emphasis. However, pragmatic
inferences are only imperfectly predictable. If
you read that a karate champion hit a block,
uncertainty is reduced by also reading that the
block broke, despite the fact that that outcome
is usually to be expected. Similarly, it would
not be considered unusual when relating the
events at a birthday party to mention that there
was a take with candles blown out by the
celebrant. Many things go in stereotyped ways
but require explicit mention because thS
stereotype does not describe all possible cases.
Throughout this paper, &amp;quot;predictable&amp;quot; is used
as a shorthand for &amp;quot;imperfectly predictable, or
characterized by significantly less than per-
fect uncertainty.&amp;quot;
How is explicit but predictable information
processed? As was mentioned above, attention
has been primarily devoted to the processing
of implicit predictable information, leaving
little guidance on the present issue. However,
in a variety of t6oretical orientations, there
is a common implication about howi predictable
information would be dealt with: simply out,
explicit information, whether predictable or
not, receives sufficient processing to be
encoded in long-term memory. For example,
Kintsch (1974) assumes &amp;quot;that subjects process
and store tan inference] whether or not it is
presented explicitly&amp;quot; (p. 154). It is difficult
to imagine discourse representation theorists,
who argue for-the explicit representation in
memory of implicit inferences (e.g.,
Frederiksen, 1975, Meyer, 1974), arguing that
explicit inferences are not represented. In
schema theories (e.g., Rumelhart &amp; Ortony,
1977), explicit discourse information is used
to bind schema variables, again suggesting
that predictable information would receive
explicit mental representation. If anything,
one would expect existing t*ories to predict
that explicit. inferences would receive a
stronger memorial representation than un-
predictable information, given their greater
contextual support. For example, in their
associative network model, HAM, Anderson and
Bower (1973) argued that the greater the number
of interconnections between information, the
,greater the likelihood that information within
the interconnected network would be recalled.
This view will be referred to as the &amp;quot;storage
of explicit inferences&amp;quot; (SEI) hypothesis.
An alternative hypothesis is that predictable
information, however central to a discourse,
is taken for granted, processed only super-
ficially and receives an attenuated cognitive
representation or no enduring representation
</bodyText>
<page confidence="0.992987">
226
</page>
<bodyText confidence="0.997712732824428">
at all. if needed subsequently, if can be de-
rived. This view will be referred to as the
&amp;quot;superficial processing of explicit inferences&amp;quot;
(SPEI) hypothesis. Processing explicit inferences
in such a manner has the advantage of a cognitive
economy of representation (besides a fikely reduc-
tion in processing time), Most information that
is acquired, will never be used again. It would
then seem to be More efficient to devote extra
processing effort to the occasions when the
information is needed (i.e., by deriving it when
remembering) rather than exerting effort toward
stable encoding at the time of comprehension.
Experiments on the Representation of
Explicit Inferences
There arc considerable problems in designing
an empirical test of the hypothesis that explicit
pragmatic inference 5 in discourse are not repre-
sented in long-term memory. If one merely tests
memory for the inference, failure to remember
could be attributed to not storing the informa-
tion or to storing and then forgetting it, if the
inference is remembereo, it could be because it
was stored and then retrieved, or it may have
been generated at the time of test without having
been stored.
Spiro and Esposito (1977) developed a para-
digm not subject to the ambiguities of interpre-
tation of the more simple design discussed above.
The primary manipulation 9f interest involved
subsequentlyrvitiating the force of an earlier
explicit inference. If the inference is not
stored, certain predictable errors in recalling
it should be made.
In the first experiment, subjects were pre-
sented stories&apos;which contained information A, B,
and C such that B was strongly implied by A
except in the presence of C. For example, the
A, B, and C elements in one story (about a demon-
stration by a karate champion) could be para-
phrased as follows:,
A: The karate champion hit the block.
B: The block broke.
C: He had had a fight with his wife
earlier. It was impairing his
concentration.
C was either presented prior to A and B (C-Before),
after A and B,(C-After), or not at all (No-C).
When C was not included in the story, if SPEI
is correct, the B element should be taken for
granted, processed only superfi-c1-dlly, and not
stably represented. It would be derivable if
needed. However, if C is presented,after A and
B, memory for B should be-Impaired since B was
not stored and C will block its derivation from
A at the time of test. On the other hand, if C
occurs in the text prior to A and B, then B is
not strongly implied by-A. B cannot be taken
for granted with the assumption that it can be
generated later if needed. Here B should be
stably represented and memory for B should not
be impaired.
However. if SEI is correct, memory for B
should not be affected by whether C is before or
after A and B, since B is .stored whether it Is
implied by A (C-After) or not implied by A
(C-Before). Two objections to this argument
can be made. The information might be stored,
but remembering C might lead to a decision
that the memory for B must be mistaken (a
kind of output interference). However, C is
present whether it occurs before or after A and
B, so such an explanation would not account
for differential effects of C-placement. The
other possibility is that B is represented in
C-After, but the representation is altered
of corrected when the C information-is encoun-
tered. This possibility was investigated in
the second experiment.
In the first experiment, the following
,pr&amp;dictions of the SPE1 hypothesis&apos;were tested.
More errors in response t9 questions about the
presented predictable information (B) should
be made in the C-After than in the C-Before
conditions. Errors can be erroneous judgments
that nothing about the implied information
was presented, called 6-Mention errors (e.g.,
the story did not mention whether the block
was broken), or, when the subject believes
that something about B was mentioned,,re-
membering incorrectly what was specifically
said in the direction of conforming with the
C information, called B-ineorrect errors
(e.g., it said in the story that the block
did not break when he hit it). Confidence in
errors or the latter kind were also analyzed.
If subjects are as confident about these
errors as they are about their accurate
responses, It would be even more difficult IO
maintain the hypothesis that the explicit in-
ferences were represented.
In the No-C condition, B-Mention errors may
occur since B would not be represented ccording
to the SPEI hypothesis. The more important
prediction regarding the No-C condition is
that B-Incorrect errors should not occur more
often than in the C-Before condition. Other-
wise, the differences between C-Before and
C-After might be attributable to heightened
accuracy due to greater salience of the implied
information in the former condition rather
thdn greater inaccuracy due to a failure to
store the implied information in the letter
condition.
College subjects read eight target
vignettes each containing A and B Information,
and C information included or not and placed
as a function of which of the three conditions
subjects were randomly assigned to. C informa-
tion was always on a separate page from the A
and ,13 information, and subjects were instructed
to not look back after reading a page. After
reading all the vignettes, the subjects. were
tested for their memory for the vignettes.
Of particular interest were the two types of
questions, mentioned above, concerning the B
information (remember, B was always explicit
in the stories).
— The results supported the hypothesis that
pragmatic inferences presented in text are
superficially processed and do not receive a
</bodyText>
<page confidence="0.986638">
227
</page>
<bodyText confidence="0.999726519230769">
stable and enduring representation in memory. in
the C-After condition, subjects tended either to
repOrt that the inference was not presented in the
text Or that the opposite of the inference was
presented. Furthermore, confidence in these
errors was as high as confidence in correct mem-
ories. It is difficult to retain the notion that
inferences are deeply processed and stably encoded
4hen the C-After manipulation can produce errors
like remembering the block was not broken when
the karate champion hit it. The results cannot
)e attributed to interference produced by the
inference-vitiating C information at output,
since the M.-Before subjects would also be subject
to such interference. Neither can the results be
sttributed to differential availability of C at
Dutput, perhaps due to primacy/recency effects
related to the position of t ill the text, since
the information was almost always recaLled. Also,
unimportance of the 8 informtion Is not a viable
alternative since B tended to be central to the
story (e.g., in a story about a karate champion&apos;s
performance, information about his success in the
demonstratioif is ceFtainly important).
One alternative interpretation that remains
is that suosects do deeply process and stably
encode the presented inierence, but &amp;quot;correct&amp;quot;
their representation when the inference-vitiatjng
Information is presented. If subjects are stofing
B and then changing or correcting it at the time
C is preseated, errors ob b should occur in the
C-After condition no matter how soon the test is
admjnistered after reading. However, if the SPEI
hypothesis is correct, when delay intervals are
brief enough some surface memory for the super-
ficially processed 8 information may remain,.
reducing the numb a of B errors. Accordingly,
in the second experiment subjects were tested
either immediately. after reading each story
(interspersed Questions condition) W, as in the
first experiment, after the entire set of stories
had been read (Questions-After condition). Again,
the C-Before and C-After manipulations were
employed.
The results of the second experiment repli-
cated those of the first one in the Questions-
After Condition. Furthermore, the C-after
effect was largely absent in the Interspersed
Questions condition, demonstrating that the
effect is not due to storing and then changing
the representation, of the 8 information (the
explicit inference).
</bodyText>
<subsectionHeader confidence="0.70725">
Related Issues
</subsectionHeader>
<bodyText confidence="0.991958443037975">
The discussion of implications of the super-
ficial processing effect will at times ,pe limited
to reading rather than listening. Most of the
following is of a speculative nature.
Representation and Underlying Mechanisms
Assuming some compatible representation
system, what characterizes the processes that
produce the superficial processing effect? At
this time, only speculations about alternative
possibilities can be offered. There are three
potentially beneficial aspects of superficial
processing of explicit predictable information:
cognitive etOnomy (the information need not be
specifically stored in. long-term memory), speed
of processing (you can prooess and understand
such information rapidly), and automaticity of
processing (less consaious effort and wriAing
memory &apos;space are required).
Two simple, preliminary accounts of the
first factor, cognitive economy, can be offered.
The superficial processing phenomenon appears
most compatible with a schema-theoretic mode
of representation. Perhaps variable bindings
that are default (or at least high probability)
values are not explicitly instantiated when
they are explicit in discourse (but see the
discussion of Determinants of Performance
Variability below). However, one should not
be overly persuaded by the simplicity of such
an account. Other types of representation
systems could also account for the phenomenon.
For example, a spreading activation model
(e.g., Collins &amp; Loftus, 1975) might predict
that explicit information is not tagged in
memory when it has been recently activated with
some greater than criterion strength. This
issue will receive further discussion in the
next section.
Regarding speed of processing, several
possibilities may be offPted: the information
Is actually predicted, perhaps followed by a
selective scanning for partial clues of con-
firmation (e.g., the word &amp;quot;broke&amp;quot; in the karate
champion example; perhaps such checks coulid be
made in the visual periphery and, when posi-
tive, result in saccades that skip the predicted
information), or the expectation may be formed
after beginning to read the predictable informa-
tion followed by skipping ahead to the next
linguistic unit (&amp;quot;Oh. They&apos;re talking about
this now. Well there&apos;s no doubt how it will
turn out. I can pass this by.&amp;quot;); or temporary
binding of a schema variable (essentially a
verification of fit) may be more rapid than
more durable instantiation, or less metacognitive
activity (pondering, studying, rehearsing,
etc.) may be devoted to predictable information,
given its derivability (this also relates to
automaticity, obviously). Regarding auto-
maticity, it seems likely that the amount of
conscious processing required would be nega-
tively correlated with the goodness of fit to
prior knowledge. Thus conscious attempts to
make sense of predictable information would be
expected less often. Also, related to the
suggestions above regarding expectations and
rapidity of processing, the operation of some
preattentive process (in the sense of Neisser,
1967) is.a possibility. Naturally, it may be
the case that all of these factors are con-
tributing. However, some of Oft factors may
be mutually exclusive. For example, if default
values are processed automatically, an expecta-
tion and confirmation process may be redundant.
Determinants of Performance Variability
Occurrence of superficial processing and
failure to store information probably depends on
more than predictability or derivability con-
sidered in isolation. For. one thing, the
</bodyText>
<page confidence="0.612333">
Lf7
228
</page>
<bodyText confidence="0.9724335">
dealvability of other information in the dis-
course will have an effect. The greater the
proportion of fit to one&apos;s schemata for the dis-
course as a whole, the more likely Lt is that
conforming information will be left to be de-
rived. if a story takes place in a restauraqt,
and all the restaurant-related information is
typical then that aspect of the story can be
stored with the abstract schema node &amp;quot;typical
restaurant activities.&amp;quot; However, when the pro-
portion of fit is poor, i.e., some atypical
events occur, even typical, predictable events
may have to be stored.
Occurrence of superficialprocessing is
also likely to be affected by the extent to
which the system is taxed. When the system 15
overloaded, as when there is a large amount ot
information to be acquired or the time to
acquire the information is limited, more super-
ficial processing and leaving of information
to be derived probably goes on. Perhaps the
system has flexible criteria for derivability,
reducing criteria under overload conditions and
increasing them when processing load is light
(and when demands for recall accuracy are high
or when subsequent availability of the informa-
tion is limited). Briefly digressing, l_here may
be a temptation to confuse superficial pro-
cessing.of derivable information with skimming.
However, skimming is a selective seeking and
then deep processing of situationally important
information (see FRUMP, in Schank &amp; Abelson,
1975) whereas superficial processing involves
selectively not processing deeply information
perceived as derivable, however important it
might be. In other words, the same information
that might receive more attention while skimming
may receive less attention in normal situations
if the information is derivable. This will
happen to the extent that skimming results in
shallow processing of earlier information that
is the basis for the derivabiLity of the later
information.
Besides context-based variability in
derivability criteria, research in the psychology
of prediction indicates the potential operation
of a general bias in determining the criterion
for derivability and superficial processing.
For example, Fischoff (1975, 1977) hat found
that when people are told that some event has
occurred, they increase their subjective
probability estimate of the likelihood that
the event was going to occur. Similarly, estima-
tion of how much was known before being given a
correct answer increases when the answer is
provided. In the case of superficial processing
of information in discourse, it is possible that
the derivability of information is overestimated
after it is explicitly encountered. It seems
to 6e a fairly common experience, for example,
to not write down an idea that you are sure
will be derivable agaie later, only to find
subsequent derivation impossible. What is being
suggested here is a source of forgetting not
usually .discussed in memory theorits: super-
ficial processing of information whose deriv-
ability has &amp;ten overestimated.
The Form of Expression of Derivable Information
Semantic content, prior knowledge, awl task
contexts are not the only determinants of per-
ceived derivability. The lingaistic form in
which information is expressed will sometimes
</bodyText>
<reference confidence="0.842578580645161">
provide signals of whet information is already
knownier can be taken for granted, as when
information is expressed near the beginning of
a sentence (c.f., Clark &amp; Haviland, 1977, on
the given-new strategy). Taking an example from
Morgan and Green (in press), compare sentences
(1) and (2).
(1) The government has not yet acknowledged
that distilled water causes cancer.
(2) That disvilled water causes cancer has
not yet been acknowledged by the
government.
In (2) there is a stronger implied presumption
of the truth of the proposition regarding dis-
tilled water and cancer than there is in (1).
In general, it seems that placing i.nforma-
tion in a sentence-initial subordinate clause
lowers the-superficial processing criterion.
Consider continuations (3) and (4) of &amp;quot;The
karate champion hit the block.&amp;quot;
(3) The block broke, and then he bowed.
(4) After the block broke, he bowed.
The block&apos;s breaking would appear to be more
taken for granted in (4) than in (3).
‘Linguistic signals of predictability or
derivability need not be implicit. Consider
continuations (5), (6), and (7) of the same
sentence as above.
(5) Obviously, the block broke.
(6) As you would expect, the block broke.
(7) Naturally, the block broke.
</reference>
<bodyText confidence="0.98903828">
Words like &amp;quot;clearly&amp;quot; and phrases like &amp;quot;of
course&amp;quot; are explicit lifiguistic signals that
information to follow,is prediorell4e and can
be superficialty processed. However, one would
expect that such signals could have their effect
only for information within an acceptgble range
of plausibility. That is, a plausible but not
predictable continuation may be more likely to
be taken (erroneously) as predictable when
preceded by a linguistic signal. However, if
the information contains salient implausible
aspects or something clearly irrelevant, a
signalling phrase such as &amp;quot;as you would expect&amp;quot;
might result in more attention being devoted
to the continuationinforAtion.
Implications for the Nature of Discourse Memory
To the extent that discourse is super-
ficially processed, m6mory must be reconstructive
rather than reproductive. Rather than re-
trieving traces or instantiations of past
experieve, the past must be inferred or derived.
Just as a paleontologist reconstructs a dinosaur
From bone fragments, the past must be recon-
5tructed from the incomplete data explicitly
stored. Evidence for such reconstructive
</bodyText>
<page confidence="0.993398">
229
</page>
<bodyText confidence="0.986402753623188">
processes has been provided by Spiro (1977), who
found a pervasive tendency for subjects to pro-
duce predictable meaning.-changing distortions
and importations in text recall under certain
conditions. In general, when subsequently en-
countered information contradicted continuation
expectations derived from a target story, the
story frequently was reconstructed in such a way
as to reconcile or cohere with the continuation
information. This process of inferring the past
!Used on the present was termed accommodative re-
construction. After a long retention interval,
subjects tended to be more confident that their
accommodative recall errors had actually been-
included in the story than they were confident
about the accurate aspects of their recall. Why
should such gross errors occur and then be
assigned such high confidence? Part of the answer
surely involves their function in preducing co-
herence. Still, It is somewhat surprising that
subjects should be so sure they read information
that bore not even a distant inferential relation-
ship to what they actually did read.
Spiro suggested that the basis for such an
effect may be in the way information is treated
at the time of comprehension; namely, it is
superficially processed and not stored in long-
term memory. Then, when remembering, individuals
should know (at least tacitly) that considerable
amounts of predictable or derivable information
they have encountered will not be available in
memory, In that case, recall would typically
involve deriving a lot of missing information.
Accordingly, it would not be surprising that
subjects faced with Memories that lack coherence
would assume that missing reconciling information
was presented but only superficially processed
at comprehension. The information could then
be derived at recall with high confidence. Hence
the capacity for restructuring the past based on
the present.
Individual Differences
A final caveat should be offered regarding
the superficial processing effect, but also
applicable to al.] research on schema-based pro-
cesses in comprehension and memory. The assump-
tion is usually made that there are no qualita-
tive differences between individuals in the
manner in which discourse is processed. How-
ever, Spiro and his colleagues have recently
found that reliable style differences can be
predicted in children (Spiro &amp; Smith, 1978) and
in college students (Spiro&apos; &amp; Tirre, in prepara-
tion). Some individuals appear to be more dis-
course bound, tending toward over-reliance-on
bottom-up processes. Others are more prior
knowledge bound, tending toward over-reliance
on top-down processes. For the adult bottom-
up readers, prior knowledge obviously must be
used to a certain extent in comprehension. How-
ever, where use of prior knowledge is more
optional, e.g., in providing a scaffolding for
remembering information (Anderson, Spiro, &amp;
Anderson, 1978), the bottom-up readers capitalize
less. Whether the latter type of individual will
evince less knowledge-based superficial pro-
cessing (again an optional use of prior know-
ledge) Is a question currently under investiga-
tion.
</bodyText>
<sectionHeader confidence="0.978547" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873833333333">
Anderson, R., g Bower, G. H. Human associa-
tive memory.. New York* Wiley, 1973.
Anderson, R. C., Spiro, R. J., &amp; Anderson, M. C.
Schemata as scaffolding for information
in text. American Educational Research
Journal, 1978, in press.
Bransford, J. D., &amp; McCarrell, N. S. A sketch
of a cognitive approach to comprehension.
In W. B. Weimer and D. S. Pallrmo (Eds.),
Cognition and the symbolic prod*sses.
Hillsdale, N.J. Erlbaum, 1975.
Charniak, E. Organization and inference in
a frame-like system of common sense
knowledge. In proceedings of Theoretical
issues in natural language processing.
Cambridge, Mass. Bolt Beranek &amp; Newman
Inc., 1975.
Clark, H. H., &amp; Haviland, S. E. Comprehension
and the given-new contract. In R.
&apos;reedle (Ed.), Discourse processing.
Hillsdale, N.J. Ertbaum, 1978.
Collins, A. M., &amp; Loftus, E. F. A spreading
activation theory of semantic processing.
Psychological Review, 1975, 82, 407-428.
Fischoff, B. Hindsight 0 foresight: The
effect of outcome knowledge on judgMent
under uncertainty. JoUrna1 of Experi-
mental Psychology• Human Perception
and Performance, 1975, 1, 288-299.
Kintsch, W. The representation of meaning in
memory. Hillsdale, N.J.: Erlbaum, 1974.
Minsky, M. A framework for representing know-
ledge. In P. H. Winston (td.), The
psychology of computer vision. New York:
McGraw-Hill, 1975.
Morgan, J. L., &amp; Green, G. M. &apos;Pragmatics and
reading comprehension. In R. J. Spiro,
B. C. Bruce, and W. F. Brewer (Eds.),
Theoreticaj issues in reading comprehen-
sion: Perspectives fromhcognitivp
psychology, linguistics, artificial
intelligence, and education. Hillsdale,
N.J.: Erlbaum, in press.
Neisser, U. Cognitive psychology. New York
Appleton-Century-Crofts, 1967.
Rumelhart, D. E., &amp; Ortony, A. The repretenta-
tion of knowledge in memory. in R. C.
Anderson, R. J. Spiro, and W. E. Montague
(Eds.), Schooling anti the acquisition
of knowledge. Hillsdale, N.J.: Erlbaum,
1977.
Schenk, R. C., &amp; Abelson, R. P. Scripts,
plans, goals, and understanding.
Hillsdale, N.J.: tTlbaum, 1977.
</reference>
<page confidence="0.997089">
230
</page>
<bodyText confidence="0.875884333333333">
SO
Spiro, R. J. Remembering information from text:
The &amp;quot;State of Schema&amp;quot; approach. In k. C.
Anderson, R. J. Spiro, and W. E. Montague
(Eds.), Schooling and the acquisition of
knowledge. Hillsdale, N.J.: Erlbaum, 1977.
</bodyText>
<reference confidence="0.992409133333333">
Spiro, R. J. Constructive processes ib text
comprehension and recall. In R. J. Spiro,
B. C. Bruce, and W. F. Brewer (Eds.),
Theoretical issues in reading comprehen-
sion: Perspectives from cognitive psy-
chology, linguistics, artificial intelli-
Rence, and education. Hillsdale, N.J.:
Erlbaum, in press.
Spiro, R. J., &amp; Esposito, J. Superficial pro-
cessing of explicic inferences in text
(Tech. Rep. No. 60). Urbana, Ill.: Center
for the Study of Reading, University of
Illinois, 1977.
Spiro, R. J., &amp; Smith, D. Distinguishing sub-
types of poor comprehenders: Patterns of
over-ra4ance on conceptual- vs. data-
driven processes (Tech. Rep. No. 61).
Urbana, Ill.. Center fOr the Study of
Reading, University of Illinois, 1978.
Frederiksen, C. H. Representing logical and
semantic structure of knowledge acquired
from discourse. Cognitive Psychology,
1975, 7, 371-458.
Meyer, B. J. F. The organization of prose and
Its effects on memory. Amsterdam: North
Holland, 1975.
Footnote
This research was supported by the National
Institute of Education under Contract No. US-NIE-
C-400-76-0116.
</reference>
<page confidence="0.997767">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.757732">Journal of Computational Linguistics 80</note>
<affiliation confidence="0.77538025">INFERENCE AND THEORY WALTZ, EDITOR Coordinated Scierce Laboratory University of Illinois</affiliation>
<address confidence="0.983992">Urbana 61801</address>
<abstract confidence="0.8749495">Papers presented in two sessions of TINLAP-2, the 1978 Meeting of the Association for Computational Linguistics, held with joint sponsorship by the Association for Computing Machinery and its Special Interest Group in Artificial Intelligence.</abstract>
<note confidence="0.996385">Copyright 1978, 1979</note>
<title confidence="0.731840166666667">Association for Computing Machinery Association for Computational Linguistics TABLE OF CONTENTS ci Session Inference.Mechaniws in Ndtural Language A Note on Partial Match of Descriptions. Can One Simultaneougly Question (Retrieve) and Inform (Update)? 184 3</title>
<author confidence="0.874843">Spoon in must be Frame Eugene Charniak</author>
<title confidence="0.595094">of a Theory of Plausible Reasoning</title>
<note confidence="0.545587">Collins .. 194</note>
<title confidence="0.78635">Indirect Responses to Loaded Questions</title>
<author confidence="0.687815">Jerrold Kaplan</author>
<title confidence="0.622731333333333">On Reasoning by Default Reiter 210 and Node-Based Inference</title>
<author confidence="0.678321">C Shapiro</author>
<title confidence="0.549558">The %presentation of Derivable Information in Memory When What Might Have Been Left Unsaid is Said</title>
<author confidence="0.485604">J Spiro Joseph Esposito</author>
<author confidence="0.485604">J</author>
<title confidence="0.8240605">6 Computational Models as a tor Theoretical Linguistics A Heuristic for Paradigms</title>
<author confidence="0.434002">Joseph E Grimes</author>
<title confidence="0.848445">Account of Some Constraints on Language Marcus Remarks on Processing, Constraints, and the Lexicon</title>
<author confidence="0.333711">Wasow</author>
<phone confidence="0.363882">List of Questions Suggested for Consideration in Each Session 252 71</phone>
<email confidence="0.644425">iii</email>
<author confidence="0.374334">NOTE ON PARTIAL DESCRIPTIONS CAN ONE</author>
<email confidence="0.235826">QUESTION(RETRIEVE)ANDINroRm</email>
<author confidence="0.936066">K Aravind</author>
<affiliation confidence="0.986736">Department of Computer and Information The Moore School, University of</affiliation>
<address confidence="0.984467">Philadelphia, Pa. 19104</address>
<abstract confidence="0.999600191919192">Summary:In data base query systems there is an implicit assumption that descriptions in queries must match exactly, i.e., queries are for retrieval only, and not for retrieval and updating simul- A related assumption (or questions descriptions are referentially only (i.e., a question cannot be used simultaneously for questioning and informing) seems to hold in ordinary conversations also, with some qualifications. Some issues related to the of such a its relation to partial matching of.aescriptions are briefly discubsed in this note. 1. In a question-answer syAtem each description query is used referentially i.e., for each description one expects to find an entity in the Oata base which serves as the unique referent for that description. For simplicity, hereafter we will consider only definite descriptions (in particular, definite noun phrases consisting of a definite article, an adjective, and a noun). Thus in (1) Is the red book on description red bookwill serve to identify entity, say, el in the data and the table,an entity, say, e2. The question can be answered after verifying the relation between and e2. For the purpose of making the definiteness transpareAt and also for simplifying the discussiOn in this note, hs assume that there is exactly eeP and one table in the data base. The match for red bookcan succeed if color attribute with the value red. The can fail either due to a mismatchor a match.A, mismatch occur if el has a color value other than red, say green. A partial will occur if has an unspecified value for the color attribute or iftthe possession of the color attribute itself has not been specified for el. In the rest of the discussion, we will not be with failure due to mismatch,although many of the issues raised below are quite relevant to this case also. We will be concerned with matchesonly. match really is a-partially successful match, where a part of the has matched exactly, namaindee has failed to match due to the lack of some information, and not due to a mismatch. 3. Let us consider the case of a partial match where the part of the description that matched is sufficient to identify the referent uniquely. In (2) this is trivially arcomplished because of our assumption that there is exactly one book and one in the data Although we have a partial match (due to the lack of the color value the color attribute itself for it will be to answer the questicn yes, or depending on whether is or not, since referents el and have been uniquely identified. How should we proceed in this case? if we insist that each description th question must match exactly, then clearly, have failed to establish a refPree and thcquestion cannct be answered. 2. On the other hand, we may ,sume that whenever we have a partial match and the referents are uniquely identified somehow, we should answer the question, and treat that part of the description which was not accounted for as new information. This new information can then be used to update the data base. Thus for the question (2), if the partial match is due to the fact that in the data base the value for the color attribute for el is not specified, then we can now specify it to be red. If, on the other hand, the match due that the possession of the color attribute itself is not specified for el, then tle updating would involve a new attribute called color for then 9pecifyine, a value for it, which in is this is red. The first update can be updateahd the second type, update;in the first ease we have made a local mcdification of assigning a value to an attribute, while in the second case a new item added There are of issues involved in adopting a strategy for updating upon a partial match when the matched part uniquely identifies the referent. We will state only two of these issues here and pursue the second in some detail. a) The part of the description that was missing in the data base (and which led to a 184 4 partial match) is accepted as new information and used for updating. The strategy followed is that. if an exact match fails due to the lack of some information then the missing information is treated as new and updating is done accordingly. This is kind of default is not clear ahether we can allow such unconstrained updatea. In data base query syatemS there is an implici assumption that the descriptions ,in queries must match exactly, i.e., queries are for and not for retrieval and updating simultaneously. Can we relax this requirement somewhat? We an get some ideas by looking at queetions in ordinary conversations, which is what we will do briefly in b) below. b) The hypothesis (or constraint) that in a definite descriptions are used referentially only (i.e., a question cannot be used simultaneously for asking a question ond conveying some additional information) seems to bold in ordinary conversations also, with sortie qualificatiens. The three examples below brief 15 describe some of the problems involved. 1) Suppose that l) tnere ia only one individual in the context, 2) the speaker believes he is 3) the hearer is unaware of his being a plumber, and 4) the speaker believes that the hearer is unaware of his being a plumber. Under such circuipstances it would be inappropriate to use (3) to aske the duestion (4), and simultanepusly inform the hearer that (5). (3) wnen uid the plumber leave? (4) When did the person leave? (5) He is a plumber. If (3) is used by the speaker (possibly au° 0 a mistaken belief that the bearer I &apos;mire that the person is a plumber), it is unlikely that the hearer will update his model without some clarification or some response such as Oh&apos; I know that he was a plumber,hearer not without any interrupt in responses. This example&apos; illustrates that the question coostruct cannot be used for questioning and informing simultaneously, and if it appears to (due to the speaker&apos;s ignorance of the hearer&apos;s lack of some information), the updating by hearer is not without an interrupting thu5 c&apos;onfirtniitp the suppose that 1) there is only one In the context, 2) the regards him as d grouch, 3) the hearer has no such specific evaluation of him, dnd 4) the speaker believes that the hearer has no such evaluation. In this case, it seems not completely inappropriate for the speaker to use (6), in order to ask the question (7), and simultane4us1y inform the that the regards (8) to the case. (6) When did the grouch leave? (7) When did the person leave? (8) He is a grouch. With evaluative information; simultanedusly and-informing appears to be bit more convenient. If (6) is used by the speaker, appears that the hearer can without any intetrupting responses, with the grouchyattached to the entity, as speaker&apos;s evaluation&apos;(and the fiearer&apos;s too if he with the speaker). &apos;Even if hearer for it is likely to be of the I didn&apos;t know thatyoulhoulght&apos;he was a grouchrather 1 didn&apos;t now that he was grouch(compare this to the respewnse in the previous example). 3) Finally, there is an apparent viol* ion of the hypothesis in examples such as (9). Who is sitting to the right of your lovely,. alfe? (9) can be used by the speaker to ask the question and pay a compliment (a side effect) rather than to convey new information. Thus the hypothesis does not appear to be violated in these cases. 5. Some of the issues which merit further are as followS. 1) To what extent the hypothesis can be violated and what are the side effects. If the constraint is mutually understood by the speaker and the hearer, then any apparent violation of it will be recognized and may be accompanied by a side effect (implicature?) in addition to the updating. 2) To what -extent updating without interrupting responses depends on the shape of the deserietion, the syntactic construct in which it appears questions, it-clefts, declaratives, the role it plays in the construct (e.g„ subject, topic, etc. ), the discourse model (for the and for the hearer) created so far, 9etc. 3) To what extent tha &apos;new&apos; information used for updating has to be somehow relevant to the &apos;old&apos; information, either by being Werrable from it or by being able to fit it into the structure created so far,</abstract>
<note confidence="0.704182666666667">Notes: 1. This work is partially supported 01 NS! Grant MCS75-19486. I wish to thank</note>
<author confidence="0.6926655">Jerry Kaplan</author>
<author confidence="0.6926655">Lorrie Levin</author>
<author confidence="0.6926655">Stan Rosenschein</author>
<author confidence="0.6926655">Ivan Sag</author>
<author confidence="0.6926655">Bonnie Webber for valuable</author>
<abstract confidence="0.913011943396226">dineussions. Some of the issues raised here will be cliscussed in detail in a forthcoming paper by Joshi and Rosenschcin (Strategies for reference and ascription in object centered representations). We 1411 assume a rather simpleminded structure for the data base. It will consist of entities and attributes, and relations among entities. 3. However, in general, unique reference may be established due to the context, and the structure and content of the data base. 4. In the data base context, updates are u5wity content updates. Structure updates are nol 185 permitted. In a conversational context and discourse understanding, clearly, both types of updates are possible. In these contexts it is not clear whether we can always tell which type of update has taken place: Structure updates should hardcm than todates, cognitively speaking, but this is on ix a conjecture at .this dine. S. See &amp;quot;On reasoning by detault&amp;quot; by Raymond volume).The closed world assumption discussed in this paper is also re:levant to our discussion.. See also &amp;quot;Fragments Qf a theory of Xmaniadausible reasoning&amp;quot; by Allan Collins volume),and &amp;quot;InZerencing on partial by Arkvind K. Josh!, Inference(ed. F. Hays-Roth and D. Academic 1978. 6. See &amp;quot;Cooperative responses from a natural language data base query system: Preliminary by S. Jerrold, Kaplan, Report, De.; t of Com&apos;titer and Information Science, of Pennsylvaniavember 77. We will limit outselves to wh and yes/noquestions. 8. Lorrie Levin has made a perliminary of potential of gam (unpublished). discourse models have been considered for problems of reference (see &amp;quot;Alormal approach to discourse anaphora&amp;quot; Bonnie Webber, Dissertation, Harvard University,1978). 10. A detailed discussion of some of these issues will be included in a forthcoming paper Joshi and Rosenschein 1). 186 IN HAND THIS MUST lig THE EATING FRAME The lawyer 400k a cab to the restaurant near the uniOersity.</abstract>
<author confidence="0.828787">Eugene Charniak Here we have lawyer</author>
<author confidence="0.828787">restaurant cab</author>
<author confidence="0.828787">university all of which are calling for our lines we weed out those which our only</author>
<affiliation confidence="0.98062">Department of cnmputer Science Yale University</affiliation>
<abstract confidence="0.999260095238095">A language comprehension program using &amp;quot;frames&amp;quot; &amp;quot;scripts&amp;quot;, etc. must be able to decide which frames are appropriate to the text. Often there will be explicit indication (&amp;quot;Fred was playing tennis&amp;quot; suggests the TENNIS frame) but it is not alusys so easy.(&amp;quot;Tt4 woman waged while.the man on the stage sawed her in half&amp;quot; suggests MAOICIAN but how?) This paper will examine how a program might go about determining the appropriate frame in such cases. At a sufficiently vague level the model presented here will resemble that of Minsky (1975) in it&apos;s assumption that one usually has available one or more context frames. Hence one only needs worry if information comes in which does not lit them. As opposed to Minsky the suggestions context frames will not come from the old ones, but rather from the conflicting information. The problem them becomes how potential frames are indexed under the information which &amp;quot;suggests&amp;quot; them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A framed PAINTING. on the representation of a common sense knowledge fragment.</title>
<date>1977</date>
<journal>Journal of Cognitive Science,</journal>
<volume>1</volume>
<contexts>
<context position="28854" citStr="Charniak 1977" startWordPosition="4960" endWordPosition="4961">e will hypothesize two kinds of indices, an ACTION index and a LOCATION index. This .distinction should mirror the intuitive difference between placing and object in a typical local and placing an action in a typical sequenc*. Other distinctions of this sort exist and may well lead to the introduction of other such index types locating objects and actions in time ftlr example. However I would anticipate that the total number is small (under 10, say). TO illustrate how these index types might hook up to TELEPHONE I will use a slightly extended version of the frame representation introduced in (Charniak 1977) and (Charniak forthcomming). From the point of view of this paper nothing is dependent on this choice. It is simply to give us a sepecific notattnfl with Which to work. 189 (TELEPHONE (OBJECT) ;The frame describes an OBJECT ;(and not, say, an event). VARS:(THING) ,I only introduce one variable ;THING which is bound to the ;token in the story repre,senting the phone LOCATION: ((ROOM (HOME-PHONE . THING)) (PUBLIC-LOC (PAY-PHONE . THING))) ;If we instantiate the ROOM frame then the ;HOME-PHONE variable in it should be bound ;to the token which is bound to THING. ;Similarly for PUBLIC-LOC and PAY</context>
</contexts>
<marker>Charniak, 1977</marker>
<rawString>Charniak, E., A framed PAINTING. on the representation of a common sense knowledge fragment. Journal of Cognitive Science, 1, 4, August 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Charniak</author>
</authors>
<title>On the use of framed knowledge in language comprehension,</title>
<pages>forthcomming.</pages>
<marker>Charniak, </marker>
<rawString>Charniak, E., On the use of framed knowledge in language comprehension, forthcomming.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Collins</author>
<author>J S Bromn</author>
<author>IC M Larkin</author>
</authors>
<title>Inference in text understanding, in:</title>
<journal>R. J.</journal>
<marker>Collins, Bromn, Larkin, </marker>
<rawString>Collins, A, Bromn, J. S., and Larkin, IC M., Inference in text understanding, in: R. J.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B C Bruce Spiro</author>
<author>W F Brewer</author>
</authors>
<title>(Eds.) Theoretical issues in reading comprehension.</title>
<journal>Hillsdale, N. J., Lawrence Erlbaum Associates, forthcomming.</journal>
<marker>Spiro, Brewer, </marker>
<rawString>Spiro, B. C. Bruce, and W. F. Brewer (Eds.) Theoretical issues in reading comprehension. Hillsdale, N. J., Lawrence Erlbaum Associates, forthcomming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Fahlman</author>
</authors>
<title>A hypothesis-frame system for recognition problems,</title>
<date>1974</date>
<journal>Working Paper 57, M.I.T. Artificial Intelligence Lab,</journal>
<contexts>
<context position="25781" citStr="Fahlman (1974)" startWordPosition="4423" endWordPosition="4424"> one thing in vision there is a natural cut-off for clue selection - the single scene. For another, within the scene there is a natural metric on the likelyness of two features belonging to the same frame - distance. Weither or not these in fact work in vision, they do suggest why someone primarily worried about the vision problem would not see clue selection as the problem-it appears to be in language. 3 DIFFERENT KINDS OF INDICES As I have already said, the scheme I believe :an surmount the difficulities presented in the Last section is a variant on one proposed by qinsky, and elaborated by Fahlman (1974) and Kuipers (1915). The basic idea is that one major feature or clue is used to select an initial frame. Other facts are then interpreted in light this frame. If they fit, fine. If ngt then another frame must, be found which either complements or replaces the original frame. In the previous propolsals the original frame contained information about alternate frames to be tried in case of certain types of incompatabilities. This may or may not work in vision (which was the primary concern of those mentioned earlier) however I shall drop this part of the theory. In discourse there are simply too</context>
</contexts>
<marker>Fahlman, 1974</marker>
<rawString>Fahlman, S. E., A hypothesis-frame system for recognition problems, Working Paper 57, M.I.T. Artificial Intelligence Lab, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Fahlman</author>
</authors>
<title>A system for representing and using real-world knowledge. Unpublished Ph.D.</title>
<date>1977</date>
<location>thesis, M.I.T.,</location>
<contexts>
<context position="18889" citStr="Fahlman 1977" startWordPosition="3203" endWordPosition="3204">ar to that of Minsky (1975) and Collins et. al (forthcomming) in that a frame will be selected on the basis of local evidence, and corrections will be made if it proves ffecepsary. We will see however, that there ate still a lot of problems with this, poSition which do not at first glance meet the eye. / THE CLUE INTERSECTION METHOD Rather than immediately presenting my scheme, let me start by showing the problems with an alternative possibility, which I will call the &amp;quot;clue intersection&amp;quot; method. This alternative is by no means a straw man as one researcher has in fact explicitly suggested it (Fahlman 1977) and I for one find it a very natural way of thinking about the problem. The idea behind this method is that we are given certain clues in tho story about the nature of the correct frame, and to find the frame we simply intersect the passible frames associated with each clue. TO set how this might work let us take a close laak at the following example As Jack walked down the aisle he put a can of tunafish in his basket. The clues here are things like &amp;quot;aisle&amp;quot;, &amp;quot;tunafish&apos; etc. Of course, I do not mean to say that it is the English words which are the clues, but rather the concepts which underlie</context>
<context position="24166" citStr="Fahlman 1977" startWordPosition="4136" endWordPosition="4137">d seem to depend on making inferences which are themselves dependent on having the context frames available. That is to say, before we can rule out SUPERMARKET, we need some piece of information from the SUPERMARKET frame which will enable us to say that Jack should not be turning 188 on a light, given that he is cast in the role of SHOPPER in that frame. Interestingly enough, Fahlman (who I earlier noted is a proponent of the clue intersection method) had a major role in the evolution of the Minsky proposal whic4 I advocate. As such it behoves us to consider why he then rejected the idea in (Fahlman 1977). His primary reason is his observatiob that frequently in vision one does not Pave eloy ;tingle clue which could serve as the basis for the first guess at the appropriate frame. Rather it would seem that one has a multitute of very vague features, each one of which could belong to a wide variety of objects or scenes. TO select one of them for a first guess would be quite arbitrary and would involve one in an incredible amount of backtrack. It would seem much more plausible to simply do an intersection on the clues and in this way weed out the obvious implausibilites. While this analysis of th</context>
</contexts>
<marker>Fahlman, 1977</marker>
<rawString>Fahlman, S. E., A system for representing and using real-world knowledge. Unpublished Ph.D. thesis, M.I.T., September 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P J Hayes</author>
</authors>
<title>Some association-based techniques for lexical disambiguation by machine.</title>
<marker>Hayes, </marker>
<rawString>Hayes, P. J., Some association-based techniques for lexical disambiguation by machine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Carbonell</author>
<author>A Collins</author>
</authors>
<title>Natural Semantics in Artificial Intelligence.</title>
<date>1973</date>
<journal>the American Journal of Computational Linguistics,</journal>
<booktitle>Proceedings of Third International Joint Conference on Artificial Intelligence,</booktitle>
<volume>1974</volume>
<pages>344--351</pages>
<note>Reprinted in</note>
<marker>Carbonell, Collins, 1973</marker>
<rawString>Carbonell, J.R. &amp; Collins, A. Natural Semantics in Artificial Intelligence. Proceedings of Third International Joint Conference on Artificial Intelligence, 1973, pp. 344-351. (Reprinted in the American Journal of Computational Linguistics, 1974, 1, Mfc. 3).</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Gentner</author>
<author>A Collins</author>
</authors>
<title>Knowing about knowing: Effects of meta-knowledge on inference.</title>
<note>Submitted to Cognitive Psychology.</note>
<marker>Gentner, Collins, </marker>
<rawString>Gentner, D., &amp; Collins, A. Knowing about knowing: Effects of meta-knowledge on inference. Submitted to Cognitive Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kosslyn</author>
<author>S P Schwartz</author>
</authors>
<title>A simulation of visual imagery.</title>
<date>1977</date>
<journal>Cognitive_ Science,</journal>
<pages>265--295</pages>
<marker>Kosslyn, Schwartz, 1977</marker>
<rawString>Kosslyn, S.M. &amp; Schwartz, S.P. A simulation of visual imagery. Cognitive_ Science, 1977, J., 265-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kosslyn</author>
<author>G L Murphy</author>
<author>M E Bemesderfer</author>
<author>K J Feinstein</author>
</authors>
<title>Category and continuum in mental comparisons. journal gxperimental PsvchOloglt: General,</title>
<date>1977</date>
<pages>341--375</pages>
<marker>Kosslyn, Murphy, Bemesderfer, Feinstein, 1977</marker>
<rawString>Kosslyn, S.M., Murphy, G.L., Bemesderfer, M.E., &amp; Feinstein, K.J. Category and continuum in mental comparisons. journal gxperimental PsvchOloglt: General, 1977, UAL 341-375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
</authors>
<title>A framework for representing knOwledge. In</title>
<date>1975</date>
<publisher>McGraw-Hill,</publisher>
<location>New York:</location>
<contexts>
<context position="13859" citStr="Minsky (1975)" startWordPosition="2320" endWordPosition="2321">is of latet lines we must weed out those which our only incidental. ABSTRACT A language comprehension program using &amp;quot;frames&amp;quot; &amp;quot;scripts&amp;quot;, etc. must be able to decide which frames are appropriate to the text. Often there will be explicit indication (&amp;quot;Fred was playing tennis&amp;quot; suggests the TENNIS frame) but it is not alusys so easy.(&amp;quot;Tt4 woman waged while.the man on the stage sawed her in half&amp;quot; suggests MAOICIAN but how?) This paper will examine how a program might go about determining the appropriate frame in such cases. At a sufficiently vague level the model presented here will resemble that of Minsky (1975) in it&apos;s assumption that one usually has available one or more context frames. Hence one only needs worry if information comes in which does not lit them. As opposed to Minsky however the suggestions 63r new context frames will not come from the old ones, but rather from the conflicting information. The problem them becomes how potential frames are indexed under the information which &amp;quot;suggests&amp;quot; them. 1 INTRODUCTION Understanding every day discourse requires making inferences from a very large base of common sense knowledge. To avoid death by combinatorial explosion our computer must be sable t</context>
<context position="15191" citStr="Minsky 1975" startWordPosition="2543" endWordPosition="2544"> we might use at a given point in a story or conversation (I shall henceforth simply assume we are dealing with a story) is to restrict consideration to that portion of our knowledge which is &amp;quot;about&amp;quot; things which have been mentioned in the discourse. So if we have a story which mentions trains and train stations, we will not use our knowledge of, say, circuses. This requires, of course, that given a topic, such as trains, or eating, we must be able to actess its knowledge without going through everything we know. Hence we are lead in a natural way to something approaching a notion of &amp;quot;frame&amp;quot; (Minsky 1975): a collection of knowledge about a single stereotyped situation. In the above discussion however I have made a rather important slight of hand. Given a. story we only want to consider those frames &amp;quot;about&amp;quot; things in the story. How is it that we decide which frames qualify? I was able to gloss over this because in most situations the prablen, at least at a surface level, does not appear 01 that difficult. If the story is about trains, it will surely, mention trains. So we see the word &amp;quot;train&amp;quot;, and we assume that trains are relevant. What could be easier. Unfortunately, this ease is deceptive fo</context>
<context position="18303" citStr="Minsky (1975)" startWordPosition="3102" endWordPosition="3103">orrect ourselves on the basis of further evidence. In the paper which follows I will be primarily concentrate an (2) with (3) being mentioned occasionally. In essence my position on (1) is that it will not be too much of a problem, provided that the cost of setting up a context like &amp;quot;restaurant&amp;quot; is small. If it is never used then as the story goes on it will receeded into the background. How this &amp;quot;receeding&amp;quot; takes place I shall not say, since for one thing it is a problem in many areas, and for another, I don&apos;t know. 187 Concerning (2) and (3), we will be lead to a position similar to that of Minsky (1975) and Collins et. al (forthcomming) in that a frame will be selected on the basis of local evidence, and corrections will be made if it proves ffecepsary. We will see however, that there ate still a lot of problems with this, poSition which do not at first glance meet the eye. / THE CLUE INTERSECTION METHOD Rather than immediately presenting my scheme, let me start by showing the problems with an alternative possibility, which I will call the &amp;quot;clue intersection&amp;quot; method. This alternative is by no means a straw man as one researcher has in fact explicitly suggested it (Fahlman 1977) and I for one</context>
<context position="30372" citStr="Minsky (1975)" startWordPosition="5210" endWordPosition="5211">y subject data on what people assume, the room frame is placed, and hence tried, first. This will cause the creation of two new statements which serve to specify the frames now active, and their bindings (TELEPHONING (PHONE . TELEPHONE-1)) tROOM (ROOM . ROOM-1) (HOMELPHONE . TELEPHONE-1)) The syntax here is the name of the frame followed by dotted pairs (VARIABLE . BINDING). Earlier I uaed a place notation for simplicity, e.g., (TELEPHONE TELEPHONE-1) In fact this would be converted internally to the dotted pair format: (TELEPHONE (THING . TELEPHONE-1)) I might note that my variables are what Minsky (1975) calles &amp;quot;slots&amp;quot;. They are also equivalent (to a first approximation) to KRL slots such asHOME-PHONE in. [ROOM-1 (UNIT) &lt;SELF (a ROOM with HOME-PHONE = TELEPHONE-i)&gt;) So we are hypothesizing 1) an instance of telephoning, where the only thing we know about it is the telephone involved, and 2) a room (ROOM-1) which at the moment is only furnished with a telephone. Note &amp;hat this assumes that in our room frame we have an explicit slot for a telephone. This is equivalent to assuming that rooms typically have phones in them. We can now integrate the fact that Jack is at the phone into the telephoni</context>
</contexts>
<marker>Minsky, 1975</marker>
<rawString>Minsky, M. A framework for representing knOwledge. In P. H. Winston (Ed.), Mt psychology oCcomouter vision. New York: McGraw-Hill, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In M Minsky</author>
</authors>
<title>Semantic informatlon processing.</title>
<date>1968</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.:</location>
<marker>Minsky, 1968</marker>
<rawString>Quillian, M. R. Semantic memory. In M. Minsky (Ed.), Semantic informatlon processing. Cambridge, Mass.: MIT Press, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schenk</author>
</authors>
<title>Conceptual Dependency: A Theory of Natural Language Understanding, Cognitive Psychology,</title>
<date>1972</date>
<pages>552--631</pages>
<marker>Schenk, 1972</marker>
<rawString>Schenk, R. Conceptual Dependency: A Theory of Natural Language Understanding, Cognitive Psychology, 1972, 552-631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>goals plans</author>
<author>understanding Hillsdale</author>
</authors>
<title>N.J.: Lawrence Erlbaum Associates,</title>
<date>1977</date>
<marker>plans, Hillsdale, 1977</marker>
<rawString>Schenk, R. &amp; Abelson, R. Scripts. plans., goals. and understanding. Hillsdale, N.J.: Lawrence Erlbaum Associates, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BE Smith</author>
<author>E J Shoben</author>
<author>L J Rips</author>
</authors>
<title>Comparison processes in semantic memory. Psychological Review,</title>
<date>1974</date>
<volume>15</volume>
<pages>214--241</pages>
<marker>Smith, Shoben, Rips, 1974</marker>
<rawString>Smith, BE., Shoben, E.J., &amp; Rips, L.J. Comparison processes in semantic memory. Psychological Review, 1974, 15.,, 214-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>E F Loftus</author>
</authors>
<title>A spreading activation theory of semantic processing. PsYqhological Revtew,</title>
<date>1975</date>
<pages>407--428</pages>
<marker>Collins, Loftus, 1975</marker>
<rawString>Collins, A. M. &amp; Loftus, E. F. A spreading activation theory of semantic processing. PsYqhological Revtew, 1975, la, 407-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Collins</author>
<author>E H Warnock</author>
<author>N hello</author>
<author>M L Miller</author>
</authors>
<title>Reasoning from Incomplete Khowledge,</title>
<date>1975</date>
<booktitle>in D. Bobrow &amp; A. Collins (eda.). Reoresentation &amp; understanding.</booktitle>
<publisher>Academic Press,</publisher>
<location>New York:</location>
<marker>Collins, Warnock, hello, Miller, 1975</marker>
<rawString>Collins, A., Warnock, E.H., hello, N. &amp; Miller, M.L. Reasoning from Incomplete Khowledge, in D. Bobrow &amp; A. Collins (eda.). Reoresentation &amp; understanding. New York: Academic Press, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A ma Collins</author>
<author>M R Quillian</author>
</authors>
<title>Experiments on semantic memory and language comprehension.</title>
<date>1972</date>
<booktitle>In L.W. Gregg (Ed*:), Cognition in learning and memorv.</booktitle>
<publisher>Wiley,</publisher>
<location>New York:</location>
<marker>Collins, Quillian, 1972</marker>
<rawString>Collins, A.ma, &amp; Quillian, M.R. Experiments on semantic memory and language comprehension. In L.W. Gregg (Ed*:), Cognition in learning and memorv. New York: Wiley, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>M J Adams</author>
<author>R W Pew</author>
</authors>
<title>The Effectiveness of an interactive map display in tutoring geography. journal of Educational Psychology,</title>
<date>1978</date>
<pages>1--7</pages>
<marker>Collins, Adams, Pew, 1978</marker>
<rawString>Collins, A.M., Adams, M.J. &amp; Pew, R.W. The Effectiveness of an interactive map display in tutoring geography. journal of Educational Psychology, 19781 ja, 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
<author>T Winograd</author>
</authors>
<title>An Overview of KRL-0, a Knowledge Representation Language,&amp;quot; Cognitive Science,</title>
<date>1977</date>
<location>Vol.1, No.1,</location>
<marker>Bobrow, Winograd, 1977</marker>
<rawString>Bobrow, D.G. and Winograd, T., (1977). &amp;quot;An Overview of KRL-0, a Knowledge Representation Language,&amp;quot; Cognitive Science, Vol.1, No.1, Jan. 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Colmeraurer</author>
</authors>
<title>Un System de Communication Home-Machine en Francais, Rapport interne, UER de Luminy, Universite d&apos;Aix-Marseille,</title>
<date>1973</date>
<marker>Colmeraurer, 1973</marker>
<rawString>Colmeraurer, A., (1973). Un System de Communication Home-Machine en Francais, Rapport interne, UER de Luminy, Universite d&apos;Aix-Marseille, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
</authors>
<title>The Frame Problem and Related Problems in Artificial Intelligence*&amp;quot;</title>
<date>1973</date>
<booktitle>in Artificial and Human Thinking, A. Elithorn and D. Jones (Eds.), Jossey-Bass Inc.,</booktitle>
<pages>45--49</pages>
<location>San Francisco,</location>
<marker>Hayes, 1973</marker>
<rawString>Hayes, P.J., (1973). &amp;quot;The Frame Problem and Related Problems in Artificial Intelligence*&amp;quot; in Artificial and Human Thinking, A. Elithorn and D. Jones (Eds.), Jossey-Bass Inc., San Francisco, 1973, pp.45-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
</authors>
<title>In Defence of Logic,&amp;quot;</title>
<date>1977</date>
<booktitle>Proc. IJCAI-5, M.I.T.,</booktitle>
<volume>227237</volume>
<pages>559--565</pages>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="19774" citStr="Hayes 1977" startWordPosition="3362" endWordPosition="3363">e. TO set how this might work let us take a close laak at the following example As Jack walked down the aisle he put a can of tunafish in his basket. The clues here are things like &amp;quot;aisle&amp;quot;, &amp;quot;tunafish&apos; etc. Of course, I do not mean to say that it is the English words which are the clues, but rather the concepts which underlie the words. I will assume that we go from one to the other via an independent&apos; parsing algorithm. (However this assumes that there Is no vicious interaction between frdme docermination and disambiguation. Given that dfsadbiguation depends vu prior frame determinatiop (see (Hayes 1977) for numerous examplea) this may be incorrect.) So the input to the frame determinPr will be something like ST-1 (WALK JACK-1 AISLE-1) ST-2 (PERSON JACK-i) ST-3 (EQUAL (NAME JACK-1) &amp;quot;JACK&amp;quot;) ST-4 (EQUAL (SEX JACK-1) MALE) ST-5 (AISLE AISLE-1) ST-6 (PUT JACK-1 TUNA -FISH-CM -1 BASKET-1) ST-7 (BASKET BASICET-1) • • • The details of the representation do not figure in the paper, and those which do are fairly uncontroversial. An exception here is the use of specific predicates like BASKET or AISLE. We will return to this point in the conclusion. Given this representation we can imagine one method o</context>
</contexts>
<marker>Hayes, 1977</marker>
<rawString>Hayes, P.J., (1977). &amp;quot;In Defence of Logic,&amp;quot; Proc. IJCAI-5, M.I.T., Cambridge, Mass., August 227237 077, pp. 559-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hewitt</author>
</authors>
<title>Description and TheoreVcal Analysir§, (Using Schemata) of PLANNER: AvLanguage for Proving4Theorems and Matdpulating Models in a Robot,</title>
<date>1972</date>
<booktitle>A.I.Memo No. 251, M.I.T. Plvject MAC,</booktitle>
<location>Cambridge, Mass.,</location>
<marker>Hewitt, 1972</marker>
<rawString>Hewitt, C., (1972). Description and TheoreVcal Analysir§, (Using Schemata) of PLANNER: AvLanguage for Proving4Theorems and Matdpulating Models in a Robot, A.I.Memo No. 251, M.I.T. Plvject MAC, Cambridge, Mass., April 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>h Kanoui</author>
</authors>
<title>Sortie Aspects of Symbolic Integrattart via Predicate Logic</title>
<date>1976</date>
<journal>Programming,&amp;quot;SIGSAM Bulletin,</journal>
<volume>10</volume>
<pages>29--42</pages>
<marker>Kanoui, 1976</marker>
<rawString>Kanoui, h., (1976). &amp;quot;Sortie Aspects of Symbolic Integrattart via Predicate Logic Programming,&amp;quot;SIGSAM Bulletin, 10, Nov. 1976, pp. 29-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kramosil</author>
</authors>
<title>A Note on Deduction Rules with Negative Premises,&amp;quot;</title>
<date>1975</date>
<booktitle>Proc. IJCAI-4,</booktitle>
<pages>53--56</pages>
<location>Tbilisi, USSR,</location>
<marker>Kramosil, 1975</marker>
<rawString>Kramosil, I., (1975). &amp;quot;A Note on Deduction Rules with Negative Premises,&amp;quot; Proc. IJCAI-4, Tbilisi, USSR, Sept. 3-8, 1975, pp. 53-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>P J Hayes</author>
</authors>
<title>Some Philosophic Problems from the Standpoint of Artificial Intelligence,&amp;quot;</title>
<date>1969</date>
<journal>in Machine Intelligence</journal>
<volume>4</volume>
<marker>McCarthy, Hayes, 1969</marker>
<rawString>McCarthy J. and Hayes, P.J., (1969). &amp;quot;Some Philosophic Problems from the Standpoint of Artificial Intelligence,&amp;quot; in Machine Intelligence 4,</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Meltzer</author>
<author>D Michie</author>
</authors>
<date>1969</date>
<pages>463--502</pages>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh,</location>
<marker>Meltzer, Michie, 1969</marker>
<rawString>B. Meltzer and D. Michie (Eds.&apos;), Edinburgh University Press, Edinburgh, 1969, pp. 463-502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Raphael</author>
</authors>
<title>The Frame Problem in ProblemSolving Systems,&amp;quot;</title>
<date>1971</date>
<booktitle>in Artificial Intelligence and Heuristic Programming, N.V. Findler</booktitle>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh.</location>
<marker>Raphael, 1971</marker>
<rawString>Raphael, B., (1971). &amp;quot;The Frame Problem in ProblemSolving Systems,&amp;quot; in Artificial Intelligence and Heuristic Programming, N.V. Findler and B. Meltzer (Eds.), Edinburgh University Press, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reiter</author>
</authors>
<title>On Closed World Data Bases,&amp;quot;</title>
<date>1978</date>
<booktitle>in Logic and Data Bases, H. 411aire</booktitle>
<editor>and J. Minker (Eds.),</editor>
<publisher>Plenum Press,</publisher>
<location>New York,</location>
<note>to appear.</note>
<marker>Reiter, 1978</marker>
<rawString>Reiter, R., (1978). &amp;quot;On Closed World Data Bases,&amp;quot; in Logic and Data Bases, H. 411aire and J. Minker (Eds.), Plenum Press, New York, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Roberts</author>
<author>I Goldstein</author>
</authors>
<date>1977</date>
<booktitle>The FRL Manual, A.I. Memo No. 409, M.I.T.,</booktitle>
<marker>Roberts, Goldstein, 1977</marker>
<rawString>Roberts, R.B. and Goldstein, I., (1977). The FRL Manual, A.I. Memo No. 409, M.I.T., Sept. 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rouslel</author>
</authors>
<title>PROLOG, Manuel de Reference et d&apos;Utilisation,</title>
<date>1975</date>
<booktitle>Group d&apos;Intelligence Artificielle. U.E.R. de</booktitle>
<location>Marseille, France,</location>
<marker>Rouslel, 1975</marker>
<rawString>Rouslel, P., (1975). PROLOG, Manuel de Reference et d&apos;Utilisation, Group d&apos;Intelligence Artificielle. U.E.R. de Marseille, France, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sandewall</author>
</authors>
<title>An Approach to the Frame Problem, and its Implementation,&amp;quot;</title>
<date>1972</date>
<booktitle>in Machine InWligence 7, B. Meltzer and D. Michie (Eds.), Edinpurgh</booktitle>
<pages>195--204</pages>
<publisher>UniverNty Press,</publisher>
<location>Edinburgh,</location>
<marker>Sandewall, 1972</marker>
<rawString>Sandewall, E., (1972). &amp;quot;An Approach to the Frame Problem, and its Implementation,&amp;quot; in Machine InWligence 7, B. Meltzer and D. Michie (Eds.), Edinpurgh UniverNty Press, Edinburgh, pp. 195-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sussman</author>
<author>T Winograd</author>
<author>E Charniak</author>
</authors>
<date>1970</date>
<journal>MICRO-PLANNER Reference Manual, A.I. MEMO No. M.I.T.,</journal>
<location>Cambridge, Mass.,</location>
<marker>Sussman, Winograd, Charniak, 1970</marker>
<rawString>Sussman, G., Winograd, T., and Charniak,E., (1970). MICRO-PLANNER Reference Manual, A.I. MEMO No. M.I.T., Cambridge, Mass., 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Waldinger</author>
</authors>
<title>Achieving Several Goals Simultaneously,</title>
<date>1975</date>
<journal>Artificial Intelligence Center Technical Note</journal>
<volume>107</volume>
<institution>Stanford Research Institute, Menlo Park, Calif.,</institution>
<marker>Waldinger, 1975</marker>
<rawString>Waldinger, R., (1975). Achieving Several Goals Simultaneously, Artificial Intelligence Center Technical Note 107, Stanford Research Institute, Menlo Park, Calif., July 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Warren</author>
</authors>
<title>WARPLAN: A System for Generattng Plans,</title>
<date>1974</date>
<journal>Memo No,</journal>
<volume>76</volume>
<institution>Dept. of Computational Logic, University of Edinburgh,</institution>
<marker>Warren, 1974</marker>
<rawString>Warren, D., (1974). WARPLAN: A System for Generattng Plans, Memo No, 76, Dept. of Computational Logic, University of Edinburgh, June 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language,</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T., (1972). Understanding Natural Language, Academic Press, New York, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Procedural Semantics for a Question-Answering Machine,&amp;quot;</title>
<date>1968</date>
<booktitle>AFIPS Conference Proceedings,</booktitle>
<volume>3</volume>
<pages>457--471</pages>
<location>Part I,</location>
<marker>Woods, 1968</marker>
<rawString>Woods, W.A., (1968). &amp;quot;Procedural Semantics for a Question-Answering Machine,&amp;quot; AFIPS Conference Proceedings, Vol. 3, Part I, 1968, pp. 457-471.</rawString>
</citation>
<citation valid="false">
<authors>
<author>h</author>
</authors>
<title>trreflexive a path fiom a path from</title>
<marker>h, </marker>
<rawString>h. trreflexive a path fiom a path from</rawString>
</citation>
<citation valid="false">
<authors>
<author>Exception</author>
</authors>
<title>If P is a path from x to y and there is no path Q of length equal to or less than the length of P, P\Q is a path from x to y. 3. Domain restriction: If P is a</title>
<marker>Exception, </marker>
<rawString>1. Exception: If P is a path from x to y and there is no path Q of length equal to or less than the length of P, P\Q is a path from x to y. 3. Domain restriction: If P is a</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Anderson</author>
<author>R Hastier</author>
</authors>
<title>Individuation and reference&apos; in mewary: proper names and definite descriptions.</title>
<date>1974</date>
<journal>Cognitive Psychology</journal>
<volume>6</volume>
<pages>495--5</pages>
<marker>Anderson, Hastier, 1974</marker>
<rawString>1. Anderson, J. and Hastier R. Individuation and reference&apos; in mewary: proper names and definite descriptions. Cognitive Psychology 6, 4 (October, 1974), 495-5)4.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A M Collins</author>
<author>R Quillian</author>
</authors>
<title>Retrieval time from semantic memory.</title>
<journal>J. ofVerbalLearniirgand Verbal avior</journal>
<marker>Collins, Quillian, </marker>
<rawString>2. Collins, A.M. and Quillian, R. Retrieval time from semantic memory. J. ofVerbalLearniirgand Verbal avior</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fikes</author>
<author>G Hendrix</author>
</authors>
<title>A networkbased knowledge representation and its natural deduction system.</title>
<date>1977</date>
<booktitle>Proc. Fifth Int. Jt. Conf. on Artificial Intelligence, Dept. of-Computer Science,</booktitle>
<pages>235--246</pages>
<institution>Carnegie-Mellon University,</institution>
<location>Pittsburgh,</location>
<marker>Fikes, Hendrix, 1977</marker>
<rawString>3. Fikes, R. and Hendrix, G. A networkbased knowledge representation and its natural deduction system. Proc. Fifth Int. Jt. Conf. on Artificial Intelligence, Dept. of-Computer Science, Carnegie-Mellon University, Pittsburgh, 1977, 235-246.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D G Hays</author>
</authors>
<title>Cognitive Structures.&apos; unpublished ms.</title>
<booktitle>Dept. of LinguIstict, SUNY at</booktitle>
<location>Buffalo, Amherst, Ni.</location>
<marker>Hays, </marker>
<rawString>4. Hays, D.G. Cognitive Structures.&apos; unpublished ms. Dept. of LinguIstict, SUNY at Buffalo, Amherst, Ni.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Types of processes on cognitive networks.</title>
<date>1977</date>
<booktitle>In L.S. Olschki, Mathematical Linguistics,</booktitle>
<pages>523--53</pages>
<location>Frienze, Pisa,</location>
<marker>Hays, 1977</marker>
<rawString>5. Hays, D.G. Types of processes on cognitive networks. In L.S. Olschki, Mathematical Linguistics, Frienze, Pisa, 1977, 523-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Hendrix</author>
</authors>
<title>Expanding the utility of semantic networks through partition ing.</title>
<date>1975</date>
<booktitle>AdvareFourth Int. .7t:-tOnf.--bn AitifIdLil-tiftkIligeR2e, MIT Al Laboratory,</booktitle>
<pages>115--121</pages>
<location>Cambrigge, MA,</location>
<marker>Hendrix, 1975</marker>
<rawString>6. Hendrix, G.G. Expanding the utility of semantic networks through partition ing. AdvareFourth Int. .7t:-tOnf.--bn AitifIdLil-tiftkIligeR2e, MIT Al Laboratory, Cambrigge, MA, 1975, 115-121.</rawString>
</citation>
<citation valid="true">
<date>1973</date>
<booktitle>Natural Language Processing,</booktitle>
<pages>155--188</pages>
<editor>1. Kay, M. The MIND system. In R. Rustin, ed.</editor>
<publisher>Algorithmics Press,</publisher>
<location>New Yoik,</location>
<marker>1973</marker>
<rawString>1. Kay, M. The MIND system. In R. Rustin, ed. Natural Language Processing, Algorithmics Press, New Yoik, 1973, 155-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDermott</author>
</authors>
<title>Artificial intelligence meets natural stupidity.</title>
<date>1976</date>
<journal>SIGART Newsletter,</journal>
<volume>57</volume>
<pages>4--9</pages>
<marker>McDermott, 1976</marker>
<rawString>8. McDermott, D. Artificial intelligence meets natural stupidity. SIGART Newsletter, 57 (April, 1976), 4-9.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S D McNabb</author>
</authors>
<title>The effects of encoding strategies and age on the memory representation for sentences containing proper names and dtlAnite descriptions</title>
<tech>Report No. 77-3,</tech>
<institution>Ind-iana Mathematical</institution>
<marker>McNabb, </marker>
<rawString>9. McNabb, S.D. The effects of encoding strategies and age on the memory representation for sentences containing proper names and dtlAnite descriptions Report No. 77-3, Ind-iana Mathematical</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Raphael</author>
</authors>
<title>SIR: semantic information retrieval.</title>
<date>1901</date>
<booktitle>Semantic Information Pkocessing,</booktitle>
<pages>33--145</pages>
<editor>In M. Minsky, ed.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.,</location>
<marker>Raphael, 1901</marker>
<rawString>11. Raphael, B. SIR: semantic information retrieval. In M. Minsky, ed. Semantic Information Pkocessing, MIT Press, Cambridge, MA., 1901, 33-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LAC Schubert</author>
</authors>
<title>Extending the exp4essive power of semantic networks.</title>
<date>1976</date>
<journal>Artificial Intelligence</journal>
<volume>7</volume>
<pages>10498</pages>
<location>Summer,</location>
<marker>Schubert, 1976</marker>
<rawString>12. Schubert, LAC. Extending the exp4essive power of semantic networks. Artificial Intelligence 7, 2 (Summer, 1976), 10498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RM Schwarcz</author>
<author>J F Burger</author>
<author>R F Simmons</author>
</authors>
<title>A deductive questionanswerer for natural language inference.</title>
<date>1970</date>
<journal>CACM</journal>
<volume>13</volume>
<pages>167--183</pages>
<marker>Schwarcz, Burger, Simmons, 1970</marker>
<rawString>13. Schwarcz, RM., Burger, J.F., and Simmons, R.F. A deductive questionanswerer for natural language inference. CACM 13, 3 (March, 1970), 167-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Shapiro</author>
</authors>
<title>A net structure for semantic information storage, deduction and retrieval.</title>
<date>1971</date>
<booktitle>Proc. Second Int. Artificial Th-e-Btitlii-CompuW•on,</booktitle>
<pages>512--523</pages>
<marker>Shapiro, 1971</marker>
<rawString>14. Shapiro, S.C. A net structure for semantic information storage, deduction and retrieval. Proc. Second Int. Artificial Th-e-Btitlii-CompuW•on, 1971, 512-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Shapiro</author>
</authors>
<title>Representing and locating deduction eules in a semantic network.</title>
<date>1977</date>
<booktitle>Proc. Workshop on PatternDirected INTWFWErfirstems. In siaRT Newsletter, C3</booktitle>
<pages>14</pages>
<marker>Shapiro, 1977</marker>
<rawString>15. Shapiro, S.C. Representing and locating deduction eules in a semantic network. Proc. Workshop on PatternDirected INTWFWErfirstems. In siaRT Newsletter, C3 (June, 1977), 14-TE----</rawString>
</citation>
<citation valid="false">
<authors>
<author>S C Shapiro</author>
</authors>
<title>Teciii•elifitrUfi-ir cial intelligence.</title>
<location>D. Van NoStrañd New York, i97§.</location>
<marker>Shapiro, </marker>
<rawString>16. Shapiro, S.C. Teciii•elifitrUfi-ir cial intelligence. D. Van NoStrañd New York, i97§.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S C Shapiro</author>
</authors>
<title>The SNePS semantic netWork processing system.</title>
<booktitle>Associative Networks -- The .....21r3tzlItn-Use of Knowledge in Computers,</booktitle>
<editor>In N. Findler, ed.</editor>
<marker>Shapiro, </marker>
<rawString>17. Shapiro, S.C. The SNePS semantic netWork processing system. In N. Findler, ed. Associative Networks -- The .....21r3tzlItn-Use of Knowledge in Computers,</rawString>
</citation>
<citation valid="false">
<authors>
<author>mic Press</author>
</authors>
<location>New York,</location>
<note>in press.</note>
<marker>Press, </marker>
<rawString>mic Press, New York, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Shapiro</author>
<author>G H Woodmansee</author>
</authors>
<title>A net structure based relational question answerer: description and examples.</title>
<date>1969</date>
<booktitle>Proc. Int. Jt. Conf. on Artificial Intelligence, The MITRE Corp.,</booktitle>
<pages>325--346</pages>
<location>Bedferd, MA.,</location>
<marker>Shapiro, Woodmansee, 1969</marker>
<rawString>18. Shapiro, S.C. and Woodmansee, G.H. A net structure based relational question answerer: description and examples. Proc. Int. Jt. Conf. on Artificial Intelligence, The MITRE Corp., Bedferd, MA., 1969, 325-346.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Winograd</author>
</authors>
<title>Frame representations and the declarative/procedural Controversy.</title>
<booktitle>Representation and Understandings Academic Prelim-</booktitle>
<pages>185--210</pages>
<editor>In D.G. Bobrow and A. Collins, eds.</editor>
<publisher>Inc.,</publisher>
<location>New York,</location>
<marker>Winograd, </marker>
<rawString>19. Winograd, T. Frame representations and the declarative/procedural Controversy. In D.G. Bobrow and A. Collins, eds. Representation and Understandings Academic Prelim- Inc., New York, TrAFTT185-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>What&apos;s in a link: Fowldations for semantic networks. In</title>
<date>1975</date>
<pages>35--82</pages>
<publisher>Academic Press, Inc.,</publisher>
<location>New York,</location>
<marker>Woods, 1975</marker>
<rawString>20. Woods, W.A. What&apos;s in a link: Fowldations for semantic networks. In D. G. Bobrow and A. Collins, ens. Representation and Understanding, Academic Press, Inc., New York, 1975, 35-82.</rawString>
</citation>
<citation valid="true">
<title>provide signals of whet information is already knownier can be taken for granted, as when information is expressed near the beginning of a sentence (c.f.,</title>
<date>1977</date>
<location>Clark &amp; Haviland,</location>
<marker>1977</marker>
<rawString>provide signals of whet information is already knownier can be taken for granted, as when information is expressed near the beginning of a sentence (c.f., Clark &amp; Haviland, 1977, on the given-new strategy). Taking an example from Morgan and Green (in press), compare sentences (1) and (2).</rawString>
</citation>
<citation valid="true">
<title>The government has not yet acknowledged that distilled water causes cancer.</title>
<date></date>
<marker></marker>
<rawString>(1) The government has not yet acknowledged that distilled water causes cancer.</rawString>
</citation>
<citation valid="false">
<title>(2) That disvilled water causes cancer has not yet been acknowledged by the government.</title>
<marker></marker>
<rawString>(2) That disvilled water causes cancer has not yet been acknowledged by the government.</rawString>
</citation>
<citation valid="false">
<authors>
<author>In</author>
</authors>
<title>there is a stronger implied presumption of the truth of the proposition regarding distilled water and cancer than there is</title>
<note>in (1).</note>
<marker>In, </marker>
<rawString>In (2) there is a stronger implied presumption of the truth of the proposition regarding distilled water and cancer than there is in (1).</rawString>
</citation>
<citation valid="false">
<authors>
<author>In general</author>
</authors>
<title>it seems that placing i.nformation in a sentence-initial subordinate clause lowers the-superficial processing criterion. Consider continuations (3) and (4) of &amp;quot;The karate champion hit the block.&amp;quot; (3) The block broke, and then he bowed. (4) After the block broke, he bowed.</title>
<marker>general, </marker>
<rawString>In general, it seems that placing i.nformation in a sentence-initial subordinate clause lowers the-superficial processing criterion. Consider continuations (3) and (4) of &amp;quot;The karate champion hit the block.&amp;quot; (3) The block broke, and then he bowed. (4) After the block broke, he bowed.</rawString>
</citation>
<citation valid="false">
<title>The block&apos;s breaking would appear to be more taken for granted in (4) than</title>
<note>in (3).</note>
<marker></marker>
<rawString>The block&apos;s breaking would appear to be more taken for granted in (4) than in (3).</rawString>
</citation>
<citation valid="false">
<title>Linguistic signals of predictability or derivability need not be implicit.</title>
<journal>Consider</journal>
<volume>5</volume>
<marker></marker>
<rawString>‘Linguistic signals of predictability or derivability need not be implicit. Consider continuations (5), (6), and (7) of the same sentence as above.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Obviously</author>
</authors>
<title>the block broke. (6) As you would expect, the block broke. (7) Naturally, the block broke.</title>
<marker>Obviously, </marker>
<rawString>(5) Obviously, the block broke. (6) As you would expect, the block broke. (7) Naturally, the block broke.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Anderson</author>
<author>g Bower</author>
<author>G H</author>
</authors>
<title>Human associative memory..</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York*</location>
<marker>Anderson, Bower, H, 1973</marker>
<rawString>Anderson, R., g Bower, G. H. Human associative memory.. New York* Wiley, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Anderson</author>
<author>R J Spiro</author>
<author>M C Anderson</author>
</authors>
<title>Schemata as scaffolding for information in text.</title>
<date>1978</date>
<journal>American Educational Research Journal,</journal>
<note>in press.</note>
<marker>Anderson, Spiro, Anderson, 1978</marker>
<rawString>Anderson, R. C., Spiro, R. J., &amp; Anderson, M. C. Schemata as scaffolding for information in text. American Educational Research Journal, 1978, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Bransford</author>
<author>N S McCarrell</author>
</authors>
<title>A sketch of a cognitive approach to comprehension. In</title>
<date>1975</date>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, N.J.</location>
<marker>Bransford, McCarrell, 1975</marker>
<rawString>Bransford, J. D., &amp; McCarrell, N. S. A sketch of a cognitive approach to comprehension. In W. B. Weimer and D. S. Pallrmo (Eds.), Cognition and the symbolic prod*sses. Hillsdale, N.J. Erlbaum, 1975.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Charniak</author>
</authors>
<title>Organization and inference in a frame-like system of common sense knowledge.</title>
<booktitle>In proceedings of Theoretical issues in natural language processing.</booktitle>
<marker>Charniak, </marker>
<rawString>Charniak, E. Organization and inference in a frame-like system of common sense knowledge. In proceedings of Theoretical issues in natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mass Cambridge</author>
</authors>
<title>Bolt Beranek &amp;</title>
<date>1975</date>
<publisher>Newman Inc.,</publisher>
<marker>Cambridge, 1975</marker>
<rawString>Cambridge, Mass. Bolt Beranek &amp; Newman Inc., 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>S E Haviland</author>
</authors>
<title>Comprehension and the given-new contract.</title>
<date>1978</date>
<booktitle>In R. &apos;reedle (Ed.), Discourse processing.</booktitle>
<location>Hillsdale, N.J. Ertbaum,</location>
<marker>Clark, Haviland, 1978</marker>
<rawString>Clark, H. H., &amp; Haviland, S. E. Comprehension and the given-new contract. In R. &apos;reedle (Ed.), Discourse processing. Hillsdale, N.J. Ertbaum, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>E F Loftus</author>
</authors>
<title>A spreading activation theory of semantic processing. Psychological Review,</title>
<date>1975</date>
<volume>82</volume>
<pages>407--428</pages>
<marker>Collins, Loftus, 1975</marker>
<rawString>Collins, A. M., &amp; Loftus, E. F. A spreading activation theory of semantic processing. Psychological Review, 1975, 82, 407-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fischoff</author>
</authors>
<title>Hindsight 0 foresight: The effect of outcome knowledge on judgMent under uncertainty.</title>
<date>1975</date>
<booktitle>JoUrna1 of Experimental Psychology• Human Perception and Performance,</booktitle>
<volume>1</volume>
<pages>288--299</pages>
<marker>Fischoff, 1975</marker>
<rawString>Fischoff, B. Hindsight 0 foresight: The effect of outcome knowledge on judgMent under uncertainty. JoUrna1 of Experimental Psychology• Human Perception and Performance, 1975, 1, 288-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<title>The representation of meaning in memory.</title>
<date>1974</date>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, N.J.:</location>
<marker>Kintsch, 1974</marker>
<rawString>Kintsch, W. The representation of meaning in memory. Hillsdale, N.J.: Erlbaum, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
</authors>
<title>A framework for representing knowledge. In</title>
<date>1975</date>
<publisher>McGraw-Hill,</publisher>
<location>New York:</location>
<contexts>
<context position="13859" citStr="Minsky (1975)" startWordPosition="2320" endWordPosition="2321">is of latet lines we must weed out those which our only incidental. ABSTRACT A language comprehension program using &amp;quot;frames&amp;quot; &amp;quot;scripts&amp;quot;, etc. must be able to decide which frames are appropriate to the text. Often there will be explicit indication (&amp;quot;Fred was playing tennis&amp;quot; suggests the TENNIS frame) but it is not alusys so easy.(&amp;quot;Tt4 woman waged while.the man on the stage sawed her in half&amp;quot; suggests MAOICIAN but how?) This paper will examine how a program might go about determining the appropriate frame in such cases. At a sufficiently vague level the model presented here will resemble that of Minsky (1975) in it&apos;s assumption that one usually has available one or more context frames. Hence one only needs worry if information comes in which does not lit them. As opposed to Minsky however the suggestions 63r new context frames will not come from the old ones, but rather from the conflicting information. The problem them becomes how potential frames are indexed under the information which &amp;quot;suggests&amp;quot; them. 1 INTRODUCTION Understanding every day discourse requires making inferences from a very large base of common sense knowledge. To avoid death by combinatorial explosion our computer must be sable t</context>
<context position="15191" citStr="Minsky 1975" startWordPosition="2543" endWordPosition="2544"> we might use at a given point in a story or conversation (I shall henceforth simply assume we are dealing with a story) is to restrict consideration to that portion of our knowledge which is &amp;quot;about&amp;quot; things which have been mentioned in the discourse. So if we have a story which mentions trains and train stations, we will not use our knowledge of, say, circuses. This requires, of course, that given a topic, such as trains, or eating, we must be able to actess its knowledge without going through everything we know. Hence we are lead in a natural way to something approaching a notion of &amp;quot;frame&amp;quot; (Minsky 1975): a collection of knowledge about a single stereotyped situation. In the above discussion however I have made a rather important slight of hand. Given a. story we only want to consider those frames &amp;quot;about&amp;quot; things in the story. How is it that we decide which frames qualify? I was able to gloss over this because in most situations the prablen, at least at a surface level, does not appear 01 that difficult. If the story is about trains, it will surely, mention trains. So we see the word &amp;quot;train&amp;quot;, and we assume that trains are relevant. What could be easier. Unfortunately, this ease is deceptive fo</context>
<context position="18303" citStr="Minsky (1975)" startWordPosition="3102" endWordPosition="3103">orrect ourselves on the basis of further evidence. In the paper which follows I will be primarily concentrate an (2) with (3) being mentioned occasionally. In essence my position on (1) is that it will not be too much of a problem, provided that the cost of setting up a context like &amp;quot;restaurant&amp;quot; is small. If it is never used then as the story goes on it will receeded into the background. How this &amp;quot;receeding&amp;quot; takes place I shall not say, since for one thing it is a problem in many areas, and for another, I don&apos;t know. 187 Concerning (2) and (3), we will be lead to a position similar to that of Minsky (1975) and Collins et. al (forthcomming) in that a frame will be selected on the basis of local evidence, and corrections will be made if it proves ffecepsary. We will see however, that there ate still a lot of problems with this, poSition which do not at first glance meet the eye. / THE CLUE INTERSECTION METHOD Rather than immediately presenting my scheme, let me start by showing the problems with an alternative possibility, which I will call the &amp;quot;clue intersection&amp;quot; method. This alternative is by no means a straw man as one researcher has in fact explicitly suggested it (Fahlman 1977) and I for one</context>
<context position="30372" citStr="Minsky (1975)" startWordPosition="5210" endWordPosition="5211">y subject data on what people assume, the room frame is placed, and hence tried, first. This will cause the creation of two new statements which serve to specify the frames now active, and their bindings (TELEPHONING (PHONE . TELEPHONE-1)) tROOM (ROOM . ROOM-1) (HOMELPHONE . TELEPHONE-1)) The syntax here is the name of the frame followed by dotted pairs (VARIABLE . BINDING). Earlier I uaed a place notation for simplicity, e.g., (TELEPHONE TELEPHONE-1) In fact this would be converted internally to the dotted pair format: (TELEPHONE (THING . TELEPHONE-1)) I might note that my variables are what Minsky (1975) calles &amp;quot;slots&amp;quot;. They are also equivalent (to a first approximation) to KRL slots such asHOME-PHONE in. [ROOM-1 (UNIT) &lt;SELF (a ROOM with HOME-PHONE = TELEPHONE-i)&gt;) So we are hypothesizing 1) an instance of telephoning, where the only thing we know about it is the telephone involved, and 2) a room (ROOM-1) which at the moment is only furnished with a telephone. Note &amp;hat this assumes that in our room frame we have an explicit slot for a telephone. This is equivalent to assuming that rooms typically have phones in them. We can now integrate the fact that Jack is at the phone into the telephoni</context>
</contexts>
<marker>Minsky, 1975</marker>
<rawString>Minsky, M. A framework for representing knowledge. In P. H. Winston (td.), The psychology of computer vision. New York: McGraw-Hill, 1975.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J L Morgan</author>
<author>G M Green</author>
</authors>
<title>Pragmatics and reading comprehension. In</title>
<location>Hillsdale, N.J.:</location>
<note>Erlbaum, in press.</note>
<marker>Morgan, Green, </marker>
<rawString>Morgan, J. L., &amp; Green, G. M. &apos;Pragmatics and reading comprehension. In R. J. Spiro, B. C. Bruce, and W. F. Brewer (Eds.), Theoreticaj issues in reading comprehension: Perspectives fromhcognitivp psychology, linguistics, artificial intelligence, and education. Hillsdale, N.J.: Erlbaum, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Neisser</author>
</authors>
<title>Cognitive psychology.</title>
<date>1967</date>
<location>New York Appleton-Century-Crofts,</location>
<marker>Neisser, 1967</marker>
<rawString>Neisser, U. Cognitive psychology. New York Appleton-Century-Crofts, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>A Ortony</author>
</authors>
<title>The repretentation of knowledge in memory. in</title>
<date>1977</date>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, N.J.:</location>
<marker>Rumelhart, Ortony, 1977</marker>
<rawString>Rumelhart, D. E., &amp; Ortony, A. The repretentation of knowledge in memory. in R. C. Anderson, R. J. Spiro, and W. E. Montague (Eds.), Schooling anti the acquisition of knowledge. Hillsdale, N.J.: Erlbaum, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schenk</author>
<author>R P Scripts Abelson</author>
</authors>
<title>plans, goals, and understanding.</title>
<date>1977</date>
<location>Hillsdale, N.J.: tTlbaum,</location>
<marker>Schenk, Abelson, 1977</marker>
<rawString>Schenk, R. C., &amp; Abelson, R. P. Scripts, plans, goals, and understanding. Hillsdale, N.J.: tTlbaum, 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R J Spiro</author>
</authors>
<title>Constructive processes ib text comprehension and recall. In</title>
<location>Hillsdale, N.J.:</location>
<note>Erlbaum, in press.</note>
<marker>Spiro, </marker>
<rawString>Spiro, R. J. Constructive processes ib text comprehension and recall. In R. J. Spiro, B. C. Bruce, and W. F. Brewer (Eds.), Theoretical issues in reading comprehension: Perspectives from cognitive psychology, linguistics, artificial intelliRence, and education. Hillsdale, N.J.: Erlbaum, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Spiro</author>
<author>J Esposito</author>
</authors>
<title>Superficial processing of explicic inferences in text (Tech.</title>
<date>1977</date>
<journal>Rep. No.</journal>
<volume>60</volume>
<institution>Center for the Study of Reading, University of Illinois,</institution>
<location>Urbana, Ill.:</location>
<marker>Spiro, Esposito, 1977</marker>
<rawString>Spiro, R. J., &amp; Esposito, J. Superficial processing of explicic inferences in text (Tech. Rep. No. 60). Urbana, Ill.: Center for the Study of Reading, University of Illinois, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Spiro</author>
<author>D Smith</author>
</authors>
<title>Distinguishing subtypes of poor comprehenders: Patterns of over-ra4ance on conceptual- vs. datadriven processes (Tech.</title>
<date>1978</date>
<journal>Rep. No.</journal>
<volume>61</volume>
<institution>Urbana, Ill.. Center fOr the Study of Reading, University of Illinois,</institution>
<marker>Spiro, Smith, 1978</marker>
<rawString>Spiro, R. J., &amp; Smith, D. Distinguishing subtypes of poor comprehenders: Patterns of over-ra4ance on conceptual- vs. datadriven processes (Tech. Rep. No. 61). Urbana, Ill.. Center fOr the Study of Reading, University of Illinois, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Frederiksen</author>
</authors>
<title>Representing logical and semantic structure of knowledge acquired from discourse. Cognitive Psychology,</title>
<date>1975</date>
<volume>7</volume>
<pages>371--458</pages>
<marker>Frederiksen, 1975</marker>
<rawString>Frederiksen, C. H. Representing logical and semantic structure of knowledge acquired from discourse. Cognitive Psychology, 1975, 7, 371-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J F Meyer</author>
</authors>
<title>The organization of prose and Its effects on memory.</title>
<date>1975</date>
<location>Amsterdam: North Holland,</location>
<marker>Meyer, 1975</marker>
<rawString>Meyer, B. J. F. The organization of prose and Its effects on memory. Amsterdam: North Holland, 1975.</rawString>
</citation>
<citation valid="false">
<title>Footnote This research was supported by the</title>
<booktitle>National Institute of Education under Contract No.</booktitle>
<pages>400--76</pages>
<marker></marker>
<rawString>Footnote This research was supported by the National Institute of Education under Contract No. US-NIEC-400-76-0116.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>