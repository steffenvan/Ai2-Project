<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9985405">
Default Representation in
Constraint-based Frameworks
</title>
<author confidence="0.999558">
Alex Lascarides* Ann Copestaket
</author>
<affiliation confidence="0.992244">
University of Edinburgh CSLI, Stanford University
</affiliation>
<bodyText confidence="0.9831632">
Default unification has been used in several linguistic applications. Most of them have utilized
defaults at a metalevel, as part of an extended description language. We propose that allowing
default unification to be a fully integrated part of a typed feature structure system requires
default unification to be a binary, order independent function, so that it acquires the perspicuity
and declarativity familiar from normal unification-based frameworks. Furthermore, in order to
respect the behavior of defaults, default unification should allow default reentrancies and values
on more general types to be overridden by conflicting default information on more specific types.
We define what we believe is the first version of default unification to fully satisfy these criteria,
and argue that it can improve the representation of a range of phenomena in syntax, semantics
and the lexico-pragmatic interface.
</bodyText>
<sectionHeader confidence="0.981341" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99991565">
The utility of defaults in linguistic representation has been widely discussed (for an
overview, see Daelemans, de Smedt, and Gazdar [1992]). The most common linguistic
application for default inheritance is to encode lexical generalizations (e.g., Boguraev
and Pustejovsky 1990; Briscoe, Copestake, and Boguraev 1990; Vossen and Copestake
1993; Daelemans 1987; Evans and Gazdar 1989a, 1989b, 1996; Flickinger, Pollard, and
Wasow 1985; Flickinger 1987; Flickinger and Nerbonne 1992; Kilgarriff 1993; Krieger
and Nerbonne 1993; Sanfilippo 1993; Shieber 1986a), but defaults have also been used
for specification in syntactic theory (e.g., Gazdar 1987; Shieber 1986b), and for the
analysis of gapping constructions (Kaplan 1987) and ellipsis (Grover et al. 1994). In
Lascarides et al. (1996), we argued for the role of defaults both in descriptions of
the lexicon and grammar and to allow the linguistic component to make defeasible
proposals to discourse processing/pragmatics. Most current constraint-based systems
either do not support defaults or only allow them at a metalevel, as part of an extended
description language. Our aim is to allow defaults as a fully integrated part of a typed
feature structure system.&apos;
In general, although there have been several approaches to formalizing default
inheritance within feature structure languages by defining an operation of default uni-
fication (some examples are cited in Section 1.2), these have failed to achieve the com-
bination of perspicuity, declarativity, and expressibility familiar from unification-based
approaches to nondefault inheritance. The version of default unification described in
</bodyText>
<footnote confidence="0.989716">
* Centre for Cognitive Science and Human Communication Research Centre, University of Edinburgh, 2,
Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK. E-mail: alexOcogsci . ed. ac .uk
t Center for the Study of Language and Information, Stanford University, Ventura Hall, Stanford,
CA 94305, USA. E-mail: aaacsli. stanf ord. edu
1 We will assume a notion of types very similar to that of Carpenter (1992), although we use the
opposite polarity in hierarchies (i.e., for us the most general type is T, top)—details are given in
Copestake (1992).
</footnote>
<note confidence="0.827432">
© 1999 Association for Computational Linguistics
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.9808156">
Lascarides et al. (1996) was more satisfactory in this respect, but not without its prob-
lems (see Section 1.2). Our aim here is to present an alternative version of default
unification, which overcomes those flaws, and also to give detailed examples of vari-
ous ways in which constraint-based systems can utilize defaults, and the requirements
that these impose on the formalism.
</bodyText>
<subsectionHeader confidence="0.948649">
1.1 Criteria for Default Unification
</subsectionHeader>
<bodyText confidence="0.987871">
Lascarides et al. (1996) list a number of desiderata for default unification, which we
repeat here (with some amendments to the justifications):
</bodyText>
<listItem confidence="0.9452215">
1. Nondefault information can be distinguished from default information
and is always preserved.
</listItem>
<bodyText confidence="0.999369285714286">
We intend our representation language to supplement rather than
supplant existing work using monotonic inheritance within frameworks
such as HPSG. Nondefault processing is crucial to parsing and
generation in a unification-based framework, because unification failure
is required to prevent ungrammatical structures. From this perspective, it
seems reasonable to expect grammars and lexicons to have a monotonic
backbone, which encodes the main architectural properties of the feature
structures. Since construction of grammars and lexicons is error-prone,
we believe that grammar writers will want to prevent accidental
overriding of such structural information by ensuring that it is
indefeasible. We therefore believe that it is desirable that defaults be
explicitly marked, and, as shown by Young and Rounds (1993), this is a
necessary condition for the order-independence of default unification
(criterion 5, below).
</bodyText>
<listItem confidence="0.7200505">
2. Default unification never fails unless there is conflict in nondefault
information.
</listItem>
<bodyText confidence="0.99936025">
The usual assumption about conflicting defaults is that they do not result
in an inconsistent knowledge state (e.g., Reiter 1980). So, it is clearly
desirable that unification failure does not occur as a side effect of the
way default information is defined.
</bodyText>
<listItem confidence="0.993332">
3. Default unification behaves like monotonic unification in the cases where
monotonic unification would succeed.
</listItem>
<bodyText confidence="0.98468625">
We want to use the same notions of unification, subsumption, etc. with
respect to both default and nondefault feature structures. This will
enable us to use default unification to extend existing monotonic
approaches to linguistic analysis, rather than replacing them.
</bodyText>
<listItem confidence="0.851474">
4. Default unification returns a single result, deterministically.
</listItem>
<bodyText confidence="0.998800166666667">
As we will describe in Section 3, there are definitions of default
unification in which a disjunction of feature structures may be produced.
While there is nothing wrong with such definitions formally, they are
practically inconvenient, since they can result in a multiplication of
structures, and they are only suitable for implementations that allow
disjunction.
</bodyText>
<listItem confidence="0.9664005">
5. Default unification can be described using a binary, order-independent
(i.e., commutative and associative) operation.
</listItem>
<bodyText confidence="0.9833195">
The main failing of most definitions of default unification is that they are
not order-independent. Having an order-dependent operation in the
</bodyText>
<page confidence="0.994236">
56
</page>
<note confidence="0.895217">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.993773">
description language is inelegant, but it has been regarded as acceptable,
because all the structures to be unified are in a fixed hierarchy and an
inheritance order can therefore be imposed (there will be a difference
between top-down vs. bottom-up inheritance, for instance). However,
order dependence undermines many of the arguments in favor of
constraint-based grammar formalisms: that is, that processing can be
seen as a uniform accumulation of constraints and is independent of the
algorithm that controls evaluation order. For instance, an
order-dependent operation causes problems for strategies such as lazy
evaluation. More fundamentally, we have argued (e.g., in Lascarides and
Copestake, in press) that it is necessary for the lexicon to propose default
information that may be overridden by pragmatics. In a discourse
situation, however, it is impossible to predict which pieces of
information are to be unified in advance of starting the discourse parsing
process, so the interface between discourse processing and
order-dependent lexical processing would have to take into account the
order in which the unification operations are done, which is impractical.
</bodyText>
<listItem confidence="0.445059">
6. Defaults can be given a precedence ordering such that more specific
</listItem>
<bodyText confidence="0.998984714285714">
information overrides less specific information.
This is the usual assumption made about inheritance hierarchies, and is
necessary if exceptions to a generalization themselves have exceptions.
The approach to default unification that we will describe in this paper
allows any defaults to be overridden by defaults which are associated
with more specific types: thus priority ordering reflects the type
hierarchy ordering. (In Section 6.2, we will mention other possibilities for
imposing a priority order on defaults.)
Barring criterion 6, all of the above properties are necessary for making default
unification behave as much like normal unification as possible, save that (default) infor-
mation can be overridden. These criteria ensure that the default unification operation
has properties familiar from monotonic unification, such as determinacy, the way infor-
mation is accumulated, the conditions when unification fails, and order independence.
Since this guarantees that default unification shares many of the properties of normal
unification, a &amp;quot;seamless transition&amp;quot; is possible between the monotonic approach to
linguistic analysis supplied by normal unification, and the extension to these analy-
ses provided by supplying default constraints and default unification operating over
them. We will justify these assumptions with respect to particular linguistic examples
in Section 4. In this paper, we define an order-independent typed default unification
operation called YADU (Yet Another Default Unification), which we believe is the first
definition of default unification that fulfills all of the above criteria.
</bodyText>
<subsectionHeader confidence="0.993586">
1.2 Previous Definitions of Default Operations on Feature Structures
</subsectionHeader>
<bodyText confidence="0.99998375">
There have been a number of previous definitions of default unification, including
those given by: van den Berg and Priist (1991), Bouma (1990, 1992), Calder (1991),
Carpenter (1993), Copestake (1992, 1993), Russell, Carroll, and Warwick-Armstrong
(1991), and Russell et al. (1993). These definitions were all based on Kaplan&apos;s sketch
of priority union (Kaplan 1987) and are asymmetric since one feature structure (FS) is
taken to be indefeasible while the other is defeasible. This operation is not commuta-
tive or associative (Carpenter 1993). Although there are some applications for which
an asymmetric operation is useful (see Section 6.5), the order dependency makes it un-
</bodyText>
<page confidence="0.997248">
57
</page>
<note confidence="0.881349">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.999954333333333">
desirable as a basis for inheritance, as we discussed above. Furthermore, since these
definitions do not allow for statements of precedence order in defaults, the inheritance
hierarchy has to be stipulated separately.
Young and Rounds (1993) define an order-independent version of default unifica-
tion using Reiter&apos;s default logic (Reiter 1980) to model the operation. But it does not
allow for precedence between defaults based on specificity (criterion 6, above). The
most straightforward way of extending the definition to meet criterion 6 would be to
extend Reiter&apos;s default logic so that it validates specificity, but as Asher and Morreau
(1991), Lascarides and Asher (1993), and Lascarides et al. (1996) argue, all such exten-
sions to Reiter&apos;s default logic either impose ordering constraints on the application
of logical axioms in proofs (e.g., Konolige 1988), or they impose a context-sensitive
translation of the premises into the formal language (e.g,. Brewka 1991). So extending
Young and Rounds&apos; definition in this way comes at the cost of an underlying for-
mal semantics that has separately defined order constraints on the logical axioms, or
context-sensitive translation.
Lascarides et al. (1996) use conditional logic to extend Young and Rounds&apos; def-
inition to typed feature structures, describing an operation called Persistent Default
Unification (PDU). They use a conditional logic precisely because these logics are able
to validate nonmonotonic patterns of inference involving specificity without impos-
ing ordering constraints on the application of axioms or imposing context-sensitive
translation on the premises. In order to allow default inheritance in type hierarchies,
precedence of default values in PDU is determined by the specificity of the types at
which the default is introduced. PDU thus meets criterion 6 for nonreentrant informa-
tion, but it can&apos;t validate the overriding of default reentrancy by conflicting default
values introduced by a more specific type. This is partly because the logic underlying
PDU demands that the default values on paths are worked out independently of one
another. Since PDU can&apos;t compare values on paths that, by default, share nodes, we
were forced to make the design decision that default reentrancies always survive, and
if the default values on the shared node that are introduced by a more specific type
conflict, then the value on the shared node in the result is 1, indicating unification
failure.
This failure to fully meet criterion 6 restricts PDU&apos;s linguistic application, as we
will show in Section 4. There are also problems in interpreting FSs containing I. Such
structures cannot be treated as normal FSs and the operation, DefFill, which converts
a partially defeasible FS to a monotonic one (necessary, for instance, when defaults are
being used to allow concise description of classes of lexical entry, as we will discuss
in Sections 2 and 4) has a complex definition because of the need to allow for I.
Furthermore, as we discuss briefly in Section 3.6.3, PDU can result in FSs that are
not well-formed with respect to the distinction between simple values and FS values.
We also found that the complexity of the definition of PDU made it difficult to use.
These problems led us to develop the alternative definition, YADU, presented here.
Like PDU, YADU can be formalized in a conditional logic, but in this paper we will
give a definition in terms of an algebraic operation on FSs. These algebraic definitions
are easier to follow, and provide much simpler proofs of theorems than the conditional
logic (cf. the theorems for PDU in Lascarides et al. [19961).
In the next section, we give an informal overview of YADU, by means of a worked
example. This is followed in Section 3 by the formal definitions, some illustrative ex-
amples, and an explicit comparison with PDU. In Section 4, we describe some linguistic
examples in detail and discuss the requirements they impose on the default unification
operation. Section 6 covers some alternative and extended definitions, including one
that makes use of Carpenter&apos;s (1992) inequalities.
</bodyText>
<page confidence="0.99663">
58
</page>
<figure confidence="0.908479444444445">
Lascarides and Copestake Default Representation
verb
PAST :
PASTP
PASSP
I regverb 1
[PAST./+ed
[PAST
Tsts-t-vbm
</figure>
<figureCaption confidence="0.879863">
Figure 1
</figureCaption>
<bodyText confidence="0.705672">
Constraint descriptions for a type hierarchy for inflections: informal notation.
</bodyText>
<sectionHeader confidence="0.731677" genericHeader="method">
2. An Informal Overview of YADU
</sectionHeader>
<bodyText confidence="0.9779094">
YADU is based on the intuitively simple idea of incorporating the maximal amount of
default information, according to its priority. We will use a simple example in order
to illustrate the operation and to contrast it with the PDU operation discussed in
Lascarides et al. (1996). Suppose we wish to encode the following information about
the suffixes of English verbs:
</bodyText>
<listItem confidence="0.999861666666667">
1. Past participle and passive participle suffixes are always the same.
2. Past tense suffixes are usually the same as participle suffixes.
3. Most verbs have the past suffix +ed.
</listItem>
<bodyText confidence="0.99993895">
We assume a paradigm style of encoding with separate features for each suffix: here
the past tense suffix slot is indicated by the feature PAST, past participle by PASTP, and
passive participle by PASSP. Figure 1 shows a fragment of a type hierarchy, which is
intended to show informally how the generalizations above could be encoded in this
way. The figure uses the conventional AVM notation for FSs, but with the addition of
a slash, which indicates that material to its right is to be treated as default. All features
have both nondefault and default values, but where the nondefault value is T (i.e.,
the most general type in the bounded complete partial order) we omit it: e.g., /+ed is
equivalent to T/+ed. We omit both the slash and the default value if the nondefault
value is equal to the default value. Default reentrancy between nodes is indicated by
slashes preceding the labels: e.g., in the constraint for verb in Figure 1, PASTP and
PASSP are necessarily coindexed, while PAST is defeasibly coindexed with both of
them. The intention of describing such a hierarchy is that verbs such as walk can be
defined as regverbs, while sleep, for example, would be a pst-t-verb. We assume that
inflectional rules are responsible for ensuring that the correct affix is realized, but we
will not give details of such rules here.
However the slashed notation used in Figure 1 cannot in general contain sufficient
information to ensure order independence, as was shown in Lascarides et al. (1996).
As we will show in more detail in Section 3, additional information is required, which
encodes part of the history of a series of default unifications. Lascarides et al. referred
</bodyText>
<page confidence="0.996645">
59
</page>
<figure confidence="0.4923679">
Computational Linguistics Volume 25, Number 1
verb verb
PAST .T / PAST in.
PASTP &apos; PASTP
PASSP PASSP
1 regverb 1/ regverb
[ PAST PAST +ed /{( [PAST.+ed],regverb)}
[ pst-t-verb 1 [ pst-t-verb r r
/ \ L PAST
PAST T &apos; PAST +t +t , pst-t-verb)}
</figure>
<figureCaption confidence="0.962135">
Figure 2
</figureCaption>
<bodyText confidence="0.995299441176471">
Type hierarchy using PDU.
to the structure that encoded this information as a tail. In that paper, a tail was nota-
tionally equivalent to a set of atomic feature structures labeled with a type. An atomic
FS is defined as in Carpenter (1993), i.e., it cannot be decomposed into simpler FS.
Intuitively, an atomic FS in the tail represents a piece of default information that was
introduced at some point in the series of default unifications that resulted in the cur-
rent structure. The type that labels an atomic FS in a tail serves to prioritize the default
information given by the atomic FS—the priority being determined by the position of
this type in the type hierarchy (more specific types have higher priority).
In Lascarides et al. (1996), we used a tripartite structure consisting of an indefeasi-
ble typed feature structure, a defeasible TFS and the tail, written in a slashed notation:
Indefeasible/Defeasible/Tail. The full PDU-style representation corresponding to Fig-
ure 1 is shown in Figure 2 (the atomic FSs shown here are notationally equivalent to
the path:value pairs used in Lascarides et al.).
Note that in Figure 2 the tails do not contain all the default information; in partic-
ular, they contain only path value structures, and do not contain equalities on paths
(i.e., reentrancies). This contributed to PDU failing to achieve all the criteria mentioned
in Section 1.1. YADU is different from PDU, in that it will achieve these criteria. In
contrast, the tails in YADU contain information about reentrancy. This means that,
unlike in PDU, it is not necessary to maintain the defeasible TFS as a substructure in
the representation, since it can be calculated directly from the indefeasible structure
and the tail. Thus for YADU we use bipartite structures, which we write Indefeasi-
ble/Tail. We will refer to these as typed default feature structures (TDFSs). The YADU
representations corresponding to Figure 2 are shown in Figure 3.
There are two operations in YADU: one involves combining two TDFSs to form
another TDFS, which we will notate Ii, and another takes a TDFS and returns a TFS, as
discussed below. n corresponds to unifying the indefeasible TFSs of the input TDFSs
using the normal definition of typed unification, and taking the set union of their
tails, with removal of elements that are inconsistent with the combined indefeasible
structure. The full TDFSs corresponding to the constraints in Figure 3 after inheritance
are shown in the left-hand column of Figure 4.
Note that the tails may contain conflicting information. In general, for any opera-
tion that makes use of default information, we want to know which elements in the
tail &amp;quot;win.&amp;quot; We therefore also define an operation on a TDFS, DefF&apos;S, which returns a
</bodyText>
<page confidence="0.984526">
60
</page>
<figure confidence="0.96599308">
Lascarides and Copestake Default Representation
verb {(F PAST
O
[PASTP E ]verb).
PAST T
PASTP PAST E verb)}
PASSP PASSP E
[regverb
[ PAST T /I( [ PAST +ed ], regverb)}
[ Vs-t,r-ve /I( [ PAST pst-t-verb)}
Figure 3
Type hierarchy using YADU.
1.
PAST T
verb
PASTP PAST M
PASSP [ Y 0 &apos;
f/ r PAST _ PAST m
(L PASSP &apos;
verb)}
verb), [verb
PAST
PASTP E
PASSP n
El
</figure>
<table confidence="0.985101157894737">
itr PAST m ]verb). regverb
regverb 1 \ [ PASTP m PAST +ed 1
PAST T / / i PAST I PASTP III
[ PASSP ElI
PASTP I &apos; \ 1. PASSP I] pst-t-verb
[PASSP g 1, verb), PAST 0 +t
([PAST.+ed 1, regverb)} PASTP
I/I-PAST M PASSP
1` ]verb).
[PASTP I &apos;
pst-t-verb
PAST T
PASTP m
E 1
[PASSP / ,r PAST
\ [PASSP Dm]
, verb),
( [ PAST +ed 1, regverb),
([ PAST +t ], pst-t-vb)}
</table>
<figureCaption confidence="0.865242">
Figure 4
</figureCaption>
<bodyText confidence="0.976009909090909">
TDFSs and default structures after inheritance using YADU.
single default TFS (corresponding to the default structure in PDU). The default struc-
tures corresponding to the TDFSs are shown in the right-hand column in Figure 4. A
default TFS is calculated from a TDFS by unifying in the maximal set of compatible
elements of the union of the tails to the nondefault TFS in priority order. In this case,
the priority order is given by the ordering in the type hierarchy, that is pst-t-verb is
strictly more specific than regverb, which is strictly more specific than verb. In the
case where there are no conflicts in the tail, this straightforwardly amounts to the uni-
fication of the atomic FSs in the tail, as in the structures shown for verb and regverb
in Figure 4. For pst-t-verb, the intermediate results of successively unifying in the tail
elements according to their specificity ordering are shown below.
</bodyText>
<equation confidence="0.992791571428572">
PAST =_
pst-t-vb pst-t-vb
T
[ PAST +t n PAST +t
PASTP m PASTP I
[
PASSP g] [PASSP g
</equation>
<page confidence="0.996704">
61
</page>
<table confidence="0.95095625">
Computational Linguistics pst-t-vb Volume 25, Number 1
2. [ PAST +ed] is incompatible with PAST : +t
PASTP E
PASSP
pst-t-vb pst-t-vb
3. iF PAST E 1 F PAST El 1 in PAST +t — PAST E +t
[PASSP ft J&apos; L PASTP &apos; PASTP E PASTP M
PASSP M PASSP
</table>
<bodyText confidence="0.999015578947368">
Thus the conflict between the information on regverb and pst-t-verb is resolved in
the favor of the latter, since it is more specific. This is the reason for separating verb
and reg-verb in the example above, since we want the +t value to override the +ed
information on regverb while leaving intact the default reentrancy which was specified
on verb. If we had not done this, there would have been a conflict between the defaults
that was not resolved by priority. In such cases, we take the generalization of the
competing defaults, but for simplicity we leave the details to Section 3, where the
definition of YADU is given.
Note that in examples such as this one, defaults are being used to capture lexical
generalizations, but the suffixes seen by the morphological analyzer must be nonde-
feasible, otherwise incorrect strings such as sleeped would be accepted, because the
default +t could be overridden. We refer to such defaults as nonpersistent since the
tails of the TDFSs must be incorporated into the indefeasible structure at the interface
between the lexicon and the rest of the system. This is the analogue of the operation
called DefFill in Lascarides et al. (1996), but, in contrast to PDU, YADU DefF&apos;ill simply
amounts to taking the defeasible structure, constructed as described above, since this
is guaranteed to be a valid TFS that is more specific than the indefeasible structure.
For our purposes, it is convenient to define DefF&apos;ill as returning a TDFS with an empty
tail (see Section 3.3 and Section 4). For instance:
</bodyText>
<equation confidence="0.9971843">
I / 1 PAST M
` [ PASTP M ], verb),
DefF&apos;ill( RI, T E
[ pst-t-vb 1
( [ PAST +ed ], regverb), pst-t-vb
PAST E +t / { }
PASTP ID
PASSP E ( MsFp DE ] , verb), ) _
PASSP E
( [ PAST +t ], pst-t-vb)}
</equation>
<bodyText confidence="0.999916166666667">
It will be apparent that the tail notation for TDFSs can become quite cumber-
some. For cases where there are no conflicts in the tails and where the priority order
corresponds to the type of the indefeasible TFS, we can use the slash notation that
we initially introduced in Figure 1. This notation can be used as a shorthand for the
description of type constraints, and in fact it is generally convenient to use it in the
description language, rather than to specify tails individually. We will formalize the
relationship between the abbreviatory notation and the full TDFSs in Section 3, and
use it when discussing examples where possible.
Before concluding this section, we should note that there is a contrast in behavior
between the PDU and YADU operation, in that with YADU a default value could
override a default reentrancy, which was not possible in the earlier work. The reason
for making the reentrancy between PAST and PASSP default on verb was that these
forms differ in some classes of irregular verbs (e.g., speak, spoke, spoken). In PDU, the
default reentrancy could only have been overridden by a nondefault value. Arguably
however, the specification of the vowel change and suffixation for this class ought also
to be defeasible, because of a subclass of these verbs, such as weave, which are also
found with the regular suffixation pattern (wove/ weaved, woven/weaved) (cf. Russell
et al. 1993). Since such a treatment of dual forms rests on a number of somewhat
</bodyText>
<page confidence="0.987547">
62
</page>
<figure confidence="0.152595">
Lascarides and Copestake Default Representation
anb=_L a n c = _L bnc=_L
([L cci6[FG %ON LE&apos;
cci ([L abiNg =
</figure>
<figureCaption confidence="0.392704">
Figure 5
</figureCaption>
<bodyText confidence="0.9571715">
Nonassociativity of asymmetric default unification.
controversial assumptions, we will not give details here. However, we will discuss an
alternative example where default values must survive when default reentrancies are
overridden in Section 4.
</bodyText>
<sectionHeader confidence="0.917428" genericHeader="method">
3. Typed Default Feature Structures and YADU
</sectionHeader>
<bodyText confidence="0.9999074">
In Section 1.1, we argued in favor of criterion 1 for default unification—nondefault
information can be distinguished from default information and is always preserved—
on the basis that we want the default portions of linguistic analyses to extend rather
than replace existing monotonic analyses. But the standard language of TFSs does not
distinguish between default and nondefault information. In the previous section, we
informally extended the TFS language with tails. In this section, we give the formal
definition of this richer representation scheme of TDFSs.
First, we consider the interplay between criterion 1 and criterion 5—default uni-
fication is a binary, order-independent operation. This is because the demands placed
by order independence affect the kind of representation scheme we require.
</bodyText>
<subsectionHeader confidence="0.995804">
3.1 Requirements for Order Independence
</subsectionHeader>
<bodyText confidence="0.999927285714286">
As we mentioned in Section 1.2, the versions of default unification based on Kaplan&apos;s
priority union (e.g., Carpenter 1993) are binary operations on TFSs, where the LHS TFS
represents nondefault information and the RHS TFS is default information. Roughly
speaking, the result of unifying these two TFSs together is: the LHS TFS unified with
as much information as possible from the RHS TFS while remaining consistent. An
example is shown in Figure 5. Clearly, such an operation is noncommutative, because
changing the order of the arguments changes which information is default and which
is nondefault. The examples in Figure 5 also show that such an operation is nonasso-
ciative.
Our demand for order independence requires us to define a symmetric operation.
But this means that we cannot treat one TFS as specifying wholly indefeasible infor-
mation and the other as wholly defeasible, as is done in Figure 5. Such an operation
is never commutative, because changing the order of arguments changes what&apos;s de-
fault. So, we must explicitly mark in the information structures which information
is to be treated as default and which is nondefault. Thus, as demonstrated in Young
and Rounds (1993) and Lascarides et al. (1996), criterion 1 above is a prerequisite to
criterion 5.
To achieve both criteria 5 and 6, further extensions to the language of TFSs are
necessary. We must also keep track of certain default information that was overrid-
den during unification, and we must keep track of the specificity of this overridden
information. This is because this information must sometimes influence the results of
</bodyText>
<page confidence="0.997069">
63
</page>
<bodyText confidence="0.153912">
Computational Linguistics Volume 25, Number 1
</bodyText>
<equation confidence="0.913039166666667">
C t2 t3 a ri b = _L dnb=_L a n d = c
TFSi: [ti TFS2: [ TFS3: [ Id
rusi FTFs, =
FTFs2)FTFs3 =
TFs2FTFs3 =
TFsi `ri (TFs2FTFs3)
</equation>
<figureCaption confidence="0.9728555">
Figure 6
Nonassociativity without tails.
</figureCaption>
<bodyText confidence="0.999967066666667">
subsequent default unifications if it is to meet the specified criteria. To see this, sup-
pose we didn&apos;t keep track of this information. And suppose we attempt to default
unify the three TFSs given in Figure 6, according to the principles laid out earlier, that
a maximal amount of default information is incorporated, according to its priority,
into the result.&apos; Then the operation would be nonassociative, as shown.
In Figure 6, when TFS2 and TFS3 are default unified together first, the fact that the
default value d appeared on the attribute F in TFS3 is lost in the result. The default
value d is compatible with the most specific default information given in TFS1, that is
subsequently unified with this result, and so this value d should also be incorporated
into the final result. The fact that it is not contributes to the order dependence. Ensuring
that d plays the necessary part in the unification operation is possible if, upon unifying
TFS2 and TFS3, we record the fact that F:/d with specificity t3 was in the unification
history, although it is currently overridden by conflicting default values. Subsequent
default unification operations on this result can then check these &amp;quot;records,&amp;quot; to see if
information that was overridden earlier should now be incorporated into the current
result. The tails that we introduced in Section 2 serve this &amp;quot;bookkeeping&amp;quot; purpose.
This maintenance of a partial history of default information is the cost we must pay
for being able to define default unification as a binary, order-independent operation.
The assumption of a binary operation is problematic since by the very definition of
nonmonotonicity, one cannot in general divide the premises into groups, work out
the inferences from those groups, and expect the inferences to survive the whole. The
only way to gain order independence, therefore, is to ensure that each time the binary
operation is performed, it is influenced by the necessary pieces of information that are
&amp;quot;outside&amp;quot; the immediate context of the FSs it is unifying. Tails cover this necessary
information by recording sufficient information for us to define the default unification
operation so that it has all the desired properties. In Lascarides et al. (1996), we argued
in formal detail why tails were necessary. Here, we have just hinted at the motivation,
by means of Figure 6. The formal discussion is quite lengthy and technical, and since
it detracts from the main purpose of this paper, which is to produce a more powerful
default unification than was presented there, we refer the reader to Lascarides et al.
</bodyText>
<footnote confidence="0.983908">
2 In this example and those following, we adopt the convention that a, b, c refer to types that correspond
to values in untyped FSs: that is, types for which no feature is appropriate. We&apos;ll refer to these as
simple value types or simple values. We use t, u, v for types that may label a node which has features.
</footnote>
<page confidence="0.999096">
64
</page>
<note confidence="0.412571">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.999697285714286">
(1996) for the relevant details. Indeed, the tails we have defined here contain more
information than those in our previous paper: specifically tails here record default
reentrancies, which were previously recorded only in the default structure, without
associated specificity information. This &amp;quot;bigger&amp;quot; tail is the cost of defining a more
powerful operation than PDU, one in which default reentrancies can be overridden
by default values. However, this also means that YADU does not require that default
TFSs be part of the TDFS.
The purpose of this section is to extend the representation of typed information
so it can go beyond TFSs as required. We do this in a way similar to that of Lascarides
et al. (1996), but omit the default structure. As we suggested in Section 2, from an
informal perspective, a TDFS contains a typed feature structure (IFS), which specifies
what is indefeasible, and tails, which specify defeasible information. For instance in
(1), the value of the path F.G is b by default, but the existence of the paths F.G and
H, and the value a for H are nondefault.
</bodyText>
<equation confidence="0.973798">
(1) [F [GT]] / {([F [G b]],t)}
H a
</equation>
<bodyText confidence="0.999931214285714">
In general, a tail consists of a set of pairs, where the first member of the pair is an
atomic FS (that is, a single path or path equivalence) and the second member is a type.
The atomic FS in such a pair must record default information that (a) is compatible
with the indefeasible information in the TDFS and (b) was introduced by a TDFS F
that was used as an argument to (at least) one of the series of default unifications
that resulted in the TDFS being considered. The second member of the pair is the root
type of F. The position of this root type in the type hierarchy, which we assume to be
a finite bounded complete partial order (Carpenter 1992), determines the specificity
or priority of the default information given by the atomic FS. For original TDFSs
(i.e., those TDFSs that aren&apos;t derived through default unification), the tails are strictly
default information, in the sense that unifying them with the indefeasible information
returns something more specific.&apos; During a series of default unifications, tails will
record (strictly) default information from original TDFSs together with its specificity.
For example, suppose a TDFS has a tail of the following form:
</bodyText>
<equation confidence="0.68772725">
[G I)] ],t1),
a ],t2,
EU 1 t3)}
EU]&apos;
</equation>
<bodyText confidence="0.999689833333333">
This means that to produce the TDFS with this tail, we unified: a TDFS that had
a root type ti and the strictly defeasible path F:G:b; a TDFS that had root type t2
and the strictly defeasible path F:a; and a TDFS that had root type t3 and the strictly
defeasible information that the F- and G-paths share the same value. These TDFSs
may have contained further information; e.g., the latter TDFS may have contained the
information that not only did F- and G- by default share the same values, whatever
</bodyText>
<footnote confidence="0.995124">
3 Furthermore, the elements of the tail of a derived TDFS (i.e., derived via default unification) are also
strictly default, unless somewhere in the unification history two TDFSs F1 and F2 were unified, where
the indefeasible information in F1 is strictly default in F2.
</footnote>
<page confidence="0.998008">
65
</page>
<note confidence="0.441953">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.994817">
they are, but also that, indefeasibly, F&apos;s value is c and G&apos;s value is c (where c is a
supertype of a and b) as given in the TDFS below:
</bodyText>
<equation confidence="0.942966">
r t3
[F c / {([GIF(2)Kt3)}
G : c
The point is that tails don&apos;t record all the information that forms part of the unification
history; they only record the strictly default information that is compatible with the
LHS TFS in the TDFS. We explain this in more detail below.
</equation>
<subsectionHeader confidence="0.999888">
3.2 The Definition of a TDFS
</subsectionHeader>
<bodyText confidence="0.999971428571429">
A TDFS is a TFS (which intuitively represents the indefeasible information), plus a
tail (which intuitively is a record of default information that played a part in building
the TDFS, and which is compatible with the indefeasible TFS). We use the definition
of unification in the definition of TDFSs to constrain the relationship between the TFS
and its tail in the required way (i.e., to ensure any atomic FS in the tail is compatible
with the TFS). And the TFS in a TDFS is adapted from Carpenter&apos;s (1992) definition.
The relevant definitions follow:
</bodyText>
<subsectionHeader confidence="0.796932">
Definition 1: The Type Hierarchy
</subsectionHeader>
<bodyText confidence="0.995028">
A type hierarchy is a finite bounded complete partial order (Type, E).
</bodyText>
<subsectionHeader confidence="0.866332">
Definition 2: Typed Feature Structures
</subsectionHeader>
<bodyText confidence="0.9926255">
A typed feature structure defined on a set of features Feat, a type hierarchy (Type, E)
and a set of indices N is a tuple (Q, r, 6,0), where:
</bodyText>
<listItem confidence="0.988809727272727">
• Q is a finite set of nodes,
• r e Q (this is the root node; see conditions 1 and 2 below)
• 8 : Q Type is a partial typing function (this labels nodes with types).
• : Q x Feat —&gt; Q is a partial feature value function (this connects nodes
with arcs labeled with features).
Note that for notational convenience we may write a sequence of
features F1 . Fk as it, and 6(.. (6(n, Fi), F2) . Fk) as 6(n,7).
Furthermore, the following must hold:
1. r isn&apos;t a 6-descendant.
2. All members of Q except r are 6-descendants of r.
3. There is no node n or path it such that 6(n, = n.
</listItem>
<bodyText confidence="0.999275125">
(Conditions 1 and 2 ensure we have a directed graph rooted at r. Condition 3 ensures
this directed graph is acyclic.)
The unification operation n on TFSs is defined here along the lines of Carpenter
(1993), in terms of subsumption on typed feature structures. First a little notation: Let F
be a TFS. Then it 7r.&apos; means that F contains path equivalence or reentrancy between
the paths it and 7&apos; (i.e., 6(n, 7) = 6(n,7&apos;) where n is the root node of F); and &apos;PF(7r) -= a
means that the type on the path it in F is a (i.e., PF(7) = a if and only if 0(6(n, 7)) =
where n is the root node of F). Subsumption is then defined as follows:
</bodyText>
<page confidence="0.957504">
66
</page>
<table confidence="0.254745666666667">
Lascarides and Copestake Default Representation
Definition 3: Subsumption of Typed Feature Structures
F subsumes F&apos;, written F&apos; E F, if and only if:
</table>
<listItem confidence="0.9991305">
• it =F implies IT
• PF(7) --= t implies &apos;Pp (7) = t&apos; and t&apos; L t
</listItem>
<bodyText confidence="0.994419">
The subsumption hierarchy is a bounded complete partial order.
Unification is defined in terms of subsumption:
</bodyText>
<subsectionHeader confidence="0.921768">
Definition 4: Unification
</subsectionHeader>
<bodyText confidence="0.925532">
The unification FRP of two feature structures F and F&apos; is taken to be the greatest lower
bound of F and F&apos; in the collection of feature structures ordered by subsumption.
Thus F n = F&amp;quot; if and only if F&amp;quot; F, F&amp;quot; E F&apos; and for every Pi&apos; such that F&amp;quot; L F and
F&amp;quot; E F&apos; it is also the case that F&amp;quot; L F&amp;quot;.
With this groundwork in place, we can now give the formal definition of TDFSs:
</bodyText>
<subsectionHeader confidence="0.949676">
Definition 5: Typed Default Feature Structures
</subsectionHeader>
<bodyText confidence="0.9992515">
A typed default feature structure defined on a set of features Feat, a type hierarchy
(Type, L) and a set of indices N is a tuple (I, T) where:
</bodyText>
<listItem confidence="0.99981925">
• I is a typed feature structure (Q, r, 6) on a set of features Feat, a type
hierarchy (Type, E) and a set of indices N, as defined in Definition 2 (i.e.,
it&apos;s a rooted directed acyclic graph);
• &apos;T is a tail: that is, it is a set of pairs, where:
</listItem>
<bodyText confidence="0.96691795">
- the first member of the pair is an atomic FS, as defined in
(Carpenter 1993); that is, a single path, or a path equivalence;
and
- the second member of the pair is a
Furthermore, for any element (F, t) E T, I n F I. (This condition ensures that all
the atomic FSs in the tails are compatible with the indefeasible information. However,
these atomic FSs may be incompatible with each other). (Note that in this paper we
make use of the notational convention that a TDFS (I, T) can be written as //T).
As we have mentioned before, we will use the tail of a TDFS as a device for keeping
a record of the default information from the TDFSs that, through default unification,
were used to form the TDFS in question. The definition of default unification we give
below will ensure that tails do indeed provide this record keeping service. However,
since tails can contain mutually incompatible information, a TDFS (and hence the
result of the default unification operation), does not explicitly represent which default
information in the tail &amp;quot;wins.&amp;quot; So as well as defining default unification, we also define
an operation on TDFSs, which we call DefF&apos;S (standing for default FS), which uses the
indefeasible TFS and the tail to produce a TFS that represents the default information
corresponding to that TDFS. That is, it determines which elements of the tail win.
Because this TFS represents the default information, DefF&apos;S(I/T) is called a default TFS
(relative to I/T).
</bodyText>
<page confidence="0.997684">
67
</page>
<note confidence="0.453815">
Computational Linguistics Volume 25, Number 1
</note>
<subsectionHeader confidence="0.998637">
3.3 The Definition of and DefFS
</subsectionHeader>
<bodyText confidence="0.999857666666667">
We now formally describe the operations fi and DefFS, and in Section 3.7 and the
appendix we will prove that these have the properties we set out in Section 1.1.
The operation n operates over TDFSs to produce a TDFS. 11 is built up from a
combination of set union and of the unification operation n, which operates on TFSs
(that is, typed feature structures as defined in Carpenter [1992]). To define H, we also
need to talk about the first and second members of the pairs in the tail:
</bodyText>
<subsectionHeader confidence="0.396866">
Definition 6: Projection of Tails
</subsectionHeader>
<bodyText confidence="0.485608">
Let T be a tail; i.e., a member of atomic FSs x Type. Then:
</bodyText>
<equation confidence="0.9946505">
pfs ( T) = {0 : (0, E T} and
ot(T) = : (0, t) E T1
</equation>
<bodyText confidence="0.994376">
Informally, takes two TDFSs, and produces a new TDFS I/ T , where I is the
(monotonic) unification of the TFSs in the argument TDFSs, and T is the union of
the tails, with all information that&apos;s incompatible with I removed. In other words, n
serves to accumulate the indefeasible information in I, and accumulate the defaults
that are compatible with I in T. The formal definition is as follows:
</bodyText>
<subsectionHeader confidence="0.634872">
Definition 7: &lt;ET
</subsectionHeader>
<bodyText confidence="0.992832">
Let F1 =d4. 11/T1 and F2 -=d,f 12/T2 be two TDFSs, and let F12 =Al Fl&lt; F2. Furthermore,
assume F12 =df 112
</bodyText>
<listItem confidence="0.605948">
1. The Indefeasible Part:
112 = 11 ri
</listItem>
<bodyText confidence="0.964885166666667">
That is, the indefeasible TFS is the unification of the indefeasible parts of
the arguments.
2. The Tail T12:
T12 =d4. (T1 u T2) \ Bot12
where Bot12 are all the elements T of T1 U T2 where the atomic FS pfs(T)
is incompatible with /12. That is:
</bodyText>
<equation confidence="0.9984115">
Bot12 _ c (T1 Li –2)
1 : pfs(T) n112 =
</equation>
<bodyText confidence="0.987825363636364">
F1 &lt;liF2 returns a TDFS F12. F12 represents the indefeasible information in both F1 and F2.
Also, through the tail T12, it represents the defeasible information that can potentially
play a role in defining what holds by default—either in F12 itself, or in some other
TDFS built from F12 via H. As we mentioned before, we need to define a further
operation that computes which elements in the tail T12 provide the winning default
information relative to F12. This is done in the DefFS operation given below, which
computes a (default) TFS from a TDFS.
DefFS is built up from a combination of the unification and generalization oper-
ations n and LJ, which operate on TFSs. Generalization is the opposite of unification.
The generalization of two feature structures is defined to be the most specific feature
structure that contains only information found in both feature structures.
</bodyText>
<subsectionHeader confidence="0.797344">
Definition 8: Generalization
</subsectionHeader>
<bodyText confidence="0.977173666666667">
The generalization F U F&apos; of two feature structures is defined to be their lowest upper
bound in the subsumption ordering.
/T12. Then 112 and T12 are calculated as follows:
</bodyText>
<page confidence="0.932426">
68
</page>
<note confidence="0.426036">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.996169428571429">
Thus F UF&apos; = F&amp;quot; if and only if F C F&amp;quot;, F&apos; F&amp;quot; and for every F&amp;quot; such that F F&amp;quot; and
F&apos; C F&amp;quot; then F&amp;quot; C F&amp;quot;.
The operation DefFS on TDFSs also uses an extended version of Carpenter&apos;s (1993)
credulous default unification, which in turn is defined in terms of n. We extend it here,
in that the second argument of the operation, instead of it being a single FS, will be a set
of atomic FSs that may be mutually incompatible. Carpenter&apos;s definition of credulous
default unification is as follows:
</bodyText>
<subsectionHeader confidence="0.899491">
Definition 9: (Carpenter 1993)
</subsectionHeader>
<bodyText confidence="0.942860176470588">
The result of credulously adding the default information in G to the strict information
in F is given by:
F h, G = {F n G&apos; : G&apos; G is maximally specific wrt the subsumption hierarchy such
that F ii G&apos; is defined}
An example of the operation is given in (3). Where t&apos; C t:
(3) [ It ,
ta :lia
GT G 1 Gb G : m
. hc F b = a o : , F
One can see that credulous default unification is asymmetric, and doesn&apos;t return a
single result deterministically. It doesn&apos;t meet the criteria we specified in Section 1.1.
Nevertheless, DefFS will have the desired properties, even though it is defined in terms
of h,. The extended definition, which works on a set of atomic FSs, is given below:
F, hca {G1, ,G} = {F1 n F2: F2 is the unification of a maximal subset of
{G1, ..., Gn} such that F1 n F2 is defined}
We will want to iterate credulous default unification in what happens below, and for
this purpose, we define h„ as follows:
</bodyText>
<subsectionHeader confidence="0.672279">
Definition 10: Credulous Default Unification h„ on Sets
</subsectionHeader>
<bodyText confidence="0.9365472">
Let be a set of TFSs {F1, ,F}, and g2 a set of atomic FSs. Then
hcs g2 = {F1 hca C2. Fn hca C2}
The definition of DefFS will manipulate tails in a certain way, and the following
way of partitioning tails plays an important role in defining how defaults behave
under DefFS:
</bodyText>
<subsectionHeader confidence="0.69069">
Definition 11: The Specificity Partition of a Tail
</subsectionHeader>
<bodyText confidence="0.724891">
Let T be a tail. Then pi, , p,„, is e Specificity Partition of T if:
</bodyText>
<equation confidence="0.98745">
T =- U U pin
</equation>
<bodyText confidence="0.69453925">
and
ILl = (0, t) E T: V( , t&apos;) c T,t&apos; t}
= {(0, t) E T: ](0&apos; , t&apos;) c ,u,_i where t&apos; C t and
V(0&amp;quot;, t&amp;quot;) e T\ U U t} 2 &lt; i &lt; m
</bodyText>
<page confidence="0.989008">
69
</page>
<note confidence="0.425868">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.9984475">
Intuitively, pi is the set of pairs in T with the most specific types, /12 is the set
of pairs in T with the next most specific types, and so on. Partitioning tails in order
of specificity enables us to define DefFS so that default information on more specific
types overrides that on more general types.
Having set up this groundwork, we&apos;re now in a position to define DefFS on TDFSs
in terms of LI and 6„ on TFSs.
</bodyText>
<subsectionHeader confidence="0.491212">
Definition 12: The Operation DefFS
</subsectionHeader>
<bodyText confidence="0.551332">
Let F be a TDFS I/T. Then
</bodyText>
<equation confidence="0.994758">
DefFS(F) = u((I h„ Pfs(m)) 6„ • • • h„ ofs(itn))
</equation>
<bodyText confidence="0.969140761904762">
where (pi, is a specificity partition on T.
According to this definition, DefFS (F) is the generalization of the credulous default
unification of: I, the atomic FSs in ., and the atomic FSs in,u, in that order.
By notational convention, we occasionally write D, for DefFS(F,). So DefFS(Fi2) =def
DefF&apos;S(In/T12) =,,fDefFS(Fi n F2) =def D12.
Note that in Lascarides et al. (1996), we defined the default structure as part of
the TDFS. We could have done this here too (and in fact did so in earlier drafts of
this paper) but the current formulation makes the contrast with PDU clearer, since in
YADU the default structure does not carry any information in addition to the tail—it
is calculated purely to see which parts of the tail win, in the current structure. In
Section 3.7 and the appendix, we will prove formally that n together with this defini-
tion DefFS for computing default structures achieve the effects we desire. Note further
that this means the implementation of the n operation itself is a trivial extension to
ordinary unification. Constructing the default structure is more complex, but this is
not a necessary part of the actual unification step. For instance, in the use of defaults
described in Section 2, the default structure is only calculated once for a given lexical
entry: it is not necessary to calculate default structures for intermediate types in the
hierarchy, for instance. We will discuss this further in Section 5.
For completeness, we also define the operation DefFill, which takes a TDFS and
returns a TDFS with the default information incorporated. This is required for non-
persistent defaults, as discussed in Section 4.
</bodyText>
<subsectionHeader confidence="0.590213">
Definition 13: The Operation DefFill
</subsectionHeader>
<bodyText confidence="0.56642">
Let F be a TDFS I/T. Then
</bodyText>
<equation confidence="0.970316">
DefFill(F) = DefFS(F)/{}
</equation>
<bodyText confidence="0.999991">
With respect to the definitions we have given in this section, we should note that
we are using an extremely weak notion of typing, and not imposing any appropriate-
ness conditions on the TFSs or requiring that unification maintain well-formedness.
The basic strategy for making the definitions of n and DefFS respect well-formedness
conditions on types is straightforward, since the set of well-formed TDFSs can be de-
fined as the set of TDFSs for which the indefeasible component meets the applicable
constraints, and the use of n in the definitions can be replaced by a variant of uni-
fication that maintains well-formedness, hence guaranteeing that the default feature
structure is also well-formed. We omit the details, since we want to keep the definition
of the default operation as general as possible, and there are a number of different
constraint languages within feature structure frameworks (e.g., Alshawi et al. 1991;
</bodyText>
<page confidence="0.969432">
70
</page>
<note confidence="0.444845">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.9991512">
Carpenter 1992; D8rre and Eisle 1991; Emele and Zajac 1990; Gerdemann and King
1994; Krieger and Schafer 1994; de Paiva 1993; Smolka 1989). In fact, in the examples
in this paper, the only use we make of types is functionally equivalent to the use of
templates in untyped FS systems, so going into the details of well-formedness with
respect to one particular system seems redundant.
</bodyText>
<subsectionHeader confidence="0.999846">
3.4 Unification at Nonroot Nodes
</subsectionHeader>
<bodyText confidence="0.9999665">
In grammars based on the use of (T)FSs, it is often necessary to be able to unify
structures at nodes other than the root. For example, if grammar rules or schemata
are described as TFSs (as we will assume in Section 4), then the instantiation of a rule
by a daughter will involve unifying a substructure TFS in the rule with the complete
TFS for the daughter. This is straightforward for TFSs, because 6(r, 7), if defined, is
itself the root node of a TFS:
</bodyText>
<subsectionHeader confidence="0.877079">
Definition 14: SubTFS
</subsectionHeader>
<bodyText confidence="0.9989435">
Let I = (Q, r, 6, 0) be a TFS, and let 6(r, 7r) be defined. Then the sub-TFS I&apos; of I that&apos;s
rooted at 6(r, 7r), written SubTFS (I, 7), is the TFS (Q&apos; , 6 (r, 7r), 6&apos; , 0&apos;), where:
</bodyText>
<listItem confidence="0.991714">
1. Q&apos; C Q are those nodes that are 6-descendants of 6(r, 7);
2. 6&apos; is the projection of 6 onto Q&apos;; and
3. 0&apos; is the projection of 0 onto Q&apos;.
</listItem>
<bodyText confidence="0.99938">
For TDFSs, the situation is slightly more complex, because the definition of sub-
structure has to include relevant elements of the tail, as well as the indefeasible
(sub)structure.
</bodyText>
<subsectionHeader confidence="0.82543">
Definition 15: SubTDFS
</subsectionHeader>
<bodyText confidence="0.879559">
Let F be a TDFS I/T and it be a path. Then
</bodyText>
<equation confidence="0.596745">
SubTDFS(F, it) = SubTFS(I,7)/SubTail(T, 7r)
</equation>
<bodyText confidence="0.912672">
where
</bodyText>
<equation confidence="0.994267">
SubTail(T,7) = {(F&apos;, t) E T: There is an element (F, t) c T such that
r is the root node of F and 6(r, 71-) is defined; and
F&apos; = SubTFS(F/701
</equation>
<bodyText confidence="0.984374">
Similarly, we can define the prefixation of a TDFS by a path as follows:
</bodyText>
<subsectionHeader confidence="0.928617">
Definition 16: SuperTFS
</subsectionHeader>
<bodyText confidence="0.989932">
Let I = (Q, r, 6, 0) be a TFS and it = F1 . . .F„ be a path. Then SuperTFS (I, 7), which is
the TFS prefixed with the path it, is (Q&apos; ,r&apos; , 6&apos;, 0&apos;), such that:
</bodyText>
<listItem confidence="0.998534666666667">
1. Q&apos; = Q Ur&apos; U {q1,..., qn-i}
2. 6&apos; is a partial feature value function defined on Q&apos;, such that
6&apos; (r&apos;,F1) = qi, 61(qi,F2) = q2,. • •, 61(qn-2,Fn-1) = qn-1, 61 (qn-1, Fn) = r; and
6&apos; (q,F) = 6(q,F) for all q c Q and all features F.
3. 0&apos; is a partial typing function on Q&apos;, such that 0(q) = 0(q) for all q E Q,
and is undefined otherwise.
</listItem>
<page confidence="0.985062">
71
</page>
<note confidence="0.357156">
Computational Linguistics Volume 25, Number 1
</note>
<equation confidence="0.726428">
Definition 17: SuperTDFS
Let F be a TDFS I/T and it be a path. Then
SuperTDFS(F, 7r) = SuperTFS(I,r)/T&apos; such that
T&apos; = { (F&apos;, t) : (F,t) E T and F&apos; = SuperTFS(F,7)}
</equation>
<bodyText confidence="0.999626666666667">
Note that in both SubTDFS and SuperTDFS, the specificity of the tail elements is
left unchanged and thus the specificity of a tail element may not be in a partial order
relationship with the root type of the indefeasible structure.
</bodyText>
<subsectionHeader confidence="0.999927">
3.5 Basic TDFSs and Abbreviatory Conventions
</subsectionHeader>
<bodyText confidence="0.999990307692308">
When initially specifying TDFSs, as type constraints, for instance, it is useful to al-
low tails to be constructed from pairs of TFSs, where the first member of the pair is
regarded as indefeasible and the second member is treated as defeasible. Obviously
it only makes sense to do this if the default TFS is compatible with the indefeasible
structure, so that the defeasible information FSD augments the indefeasible informa-
tion FS/ in some respects. We will assume that the tail is the most specific strictly
default information that is specified by the indefeasible and defeasible TFSs (you can
compute this by checking which information is in FS&apos; Fl FSD and absent in FS1), plus
the priority given by the (default) root type (which means that the type on the root
node of either FS&apos; or FSD must be defined). We will call the TDFS constructed in this
way from the indefeasible TFS FS/ and defeasible TFS FSD a basic TDFS. Basic TDFSs
are special TDFSs, because there will be no conflict in the tails (since the tail is derived
from two compatible TFSs).
</bodyText>
<subsectionHeader confidence="0.914017">
Definition 18: Basic TDFSs
</subsectionHeader>
<bodyText confidence="0.99895975">
Let FSI and FSD be typed feature structures, where FSI is regarded as indefeasible and
FSD as defeasible. Furthermore, suppose that the root node of FS1 is typed, or the root
node of FSD is typed, and that FS1 n FSD 0 I (so FS/ and FSD are compatible). Then
the basic TDFS BasicTDFS(FSDFSD) of FS/ and FSD is the TDFS FSI/T, such that:
</bodyText>
<equation confidence="0.982199">
T = {(F, t) : t is the root type on FSD n Fs&apos;, and F is an atomic TFS such that:
(a) FS1 V F;
</equation>
<listItem confidence="0.938763">
(b) FSD 11 FS/ L F; and
(c) there&apos;s no other atomic FS F&apos; such that F&apos; c F and F&apos; satisfies
conditions (a) and (b)}
</listItem>
<bodyText confidence="0.998914083333333">
Note that the basic TDFS derived from FS/ and FSD is indeed a TDFS, since FS1
and T satisfy all the conditions given in Definition 5. In particular, for any (F, t) E T,
F 1-1 FSI 1, since FSD n FS1 o I, and ID 17 FSI C F n FS1. It is also important to stress
that, in general, DefF&apos;S(BasicTDFS(FSDFSD)) 0 FSD. To see this, note that it follows
from the definition of DefFS and the fact that all elements in T are compatible with FS/
that DefFS(FSI/T) C FS/. But it is not necessarily the case that FSD L FS&apos;, because FSD
will not in general repeat the information in FS/. However, we will see in Section 3.7
and the appendix that FS1 n FSD = DefFS(BasicTDFS(FSDFSD)).
For basic TDFSs, we can make some abbreviations, as suggested in Section 2.
That is, we can use a single AVM representation where we have IndefeasibleValue/
Defeasible Value (IndefeasibleValue is omitted when it&apos;s T, and the slash is omitted when
IndefeasibleValue = DefeasibleValue). Thus, for example, (4) can be represented as (5).
</bodyText>
<page confidence="0.981119">
72
</page>
<bodyText confidence="0.269188">
Lascarides and Copestake Default Representation
</bodyText>
<listItem confidence="0.45257525">
(4) [GT]] / fqF
a
t
(5) F [G
</listItem>
<bodyText confidence="0.705098">
Ha
Where appropriate, we will use such abbreviations in the examples that follow.
</bodyText>
<subsectionHeader confidence="0.997929">
3.6 Examples
</subsectionHeader>
<bodyText confidence="0.956562833333333">
We will prove that the operation has the properties specified in Section 1.1 in the
next section, but first, we will illustrate how the operation works by means of specific
examples.
3.6.1 Specificity. The following example demonstrates that default reentrancies are
overridden by conflicting default information of a more specific type.
Where t c t&apos; and a [1b = 1, we will calculate:
</bodyText>
<equation confidence="0.979439">
[/a I F /E
G : /b G
</equation>
<bodyText confidence="0.973631">
and the defeasible result of this default unification, i.e.,
</bodyText>
<equation confidence="0.836825625">
DefFS([F /a F
G : /b G
• 112=[ T=t
G T [CT][CT]
• T12 _ (T1
) Boti2
T1-= {([F a],t),([G 13],t)}
T2= {( [ FG 1,f)}
Boti2 = 0
So
• T12 _(T1 U T2) \ Bot12
= {( [ F a],0,([G b],t),([FG ],e)}
Therefore
11 1&lt;l /E T 1 CT {([F : a],0,([G b
: /a [ E ([L&apos;,1=1:11 P&apos;)}
LG /bi LG
</equation>
<bodyText confidence="0.942789">
We now calculate the resulting defeasible structure:
</bodyText>
<listItem confidence="0.882289">
• The specificity partition of T12 is 4/4, p2), where:
</listItem>
<equation confidence="0.9075665">
ti1= {([F a],t),([G 13],0}
/-12 = {({L ],t&apos;)}
</equation>
<page confidence="0.990789">
73
</page>
<figure confidence="0.8450690625">
Computational Linguistics Volume 25, Number 1
• So =def DefFS(I12/T12) = u((112 n of,(111)) 151,5 pfs(A2))
D12 = u( h„ { EF a],[c 13] 1 )
Hence [ EDI
F 1
- u [ G F a hcs [
b
[
a
LG b]
rt D12 DefFS( F /a ]F[F /El 1)
G /b G
= DefFS( F T I/ {([F a ,t),([G b],t),([L )
G T
[ a
G b
</figure>
<subsubsectionHeader confidence="0.467827">
3.6.2 The Nixon Diamond. The credulous unification operation hcs does not generally
</subsubsectionHeader>
<bodyText confidence="0.98909325">
return a deterministic result. However, H does, even though it is defined in terms of
h„. This is because &lt;FT generalizes over the credulous options. The following example
illustrates this.
Where t and t&apos; are in no specificity ordering, and
</bodyText>
<equation confidence="0.620603666666667">
t n t&apos; = t&amp;quot;
al-lb= I
aUb= c
We will calculate:
[. /a F [
G /b G /1=1
</equation>
<bodyText confidence="0.674981">
and the defeasible result of this unification, i.e.,
</bodyText>
<figure confidence="0.7707685">
DefFS([. G/bj IF./
G /0
• 112 = FT H FT
[t
GT GT
FT
G T
• T12 = (T1 U T2) \ Bot12
</figure>
<equation confidence="0.931133333333333">
{( [ F a],t),([G b],t)}
T2 = {([L ],t&apos;)}
Boti2 =
</equation>
<page confidence="0.956491">
74
</page>
<figure confidence="0.925136764705882">
Lascarides and Copestake Default Representation
So
• T12 = (T1 U T2) \ BOt12
{([F a],t),([G b],t),(
Hence [ / a { ( [ F a], t), ( [ c b ], t),
LG:/bi LG ([
We now calculate the resulting defeasible structure:
• The specificity partition of T12 is T12 itself (since both t and t&apos; are the
most specific types in T12).
(
• D12 =df DefFS(F12) (Inh„ )fs(T12)))
[r c
G c
Hence
DefFS4F It la ]&lt;&gt;[t&apos;
G /b n F idt&amp;quot;
G
</figure>
<subsubsectionHeader confidence="0.435058">
3.6.3 Atomic Values and Feature Values. The default unification operation PDU de-
</subsubsectionHeader>
<bodyText confidence="0.957433764705882">
fined in Lascarides et al. (1996) did not respect even the weakest notion of well-
formedness: that there are some atomic types for which no features are appropriate.
Thus the formalism is too weak even to be able to define an analogue of untyped FS
systems, where nodes that have (atomic) values cannot also have features. For exam-
ple, even if a is intended to be a simple value type (i.e., a has no appropriate features
and no subtypes with appropriate features) the following is true: Where t Li t/ and a
n u =
t&apos;
[t PDU F /[7LL -
F la
This problem with PDU stems from the fact that the defeasible results of the operation
for each node were calculated independently. In this example, the result for the path
F:G is independent of the outcome for the path F. This independence allowed the use
of a polynomial algorithm, but at the cost of producing default TFSs that were not
well-formed (both in the way outlined above and also in that the result could contain
nodes typed _L), which causes problems for the definition of DefFill (i.e., the operation
for producing a well-formed IFS from a TDFS). In contrast, YADU is not calculated on
</bodyText>
<equation confidence="0.831555714285714">
(T hcs
[GT]
[ F : a ], [ G
ra , r El a ,
G b G
1
[) = F c
</equation>
<page confidence="0.906981">
75
</page>
<note confidence="0.570113">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.868822666666667">
a path-by-path basis: atomic FSs are incorporated into the default result only if they
unify with the complete structure. As we demonstrate here, one consequence is that
we don&apos;t get ill-formed results of the sort demonstrated above. The cost of this is that
the known algorithms for calculating default TFSs in YADU are worst-case factorial
in complexity, as discussed in Section 5.
Let&apos;s calculate the following: Where t L tf and a n u =
t&apos;
[t:/ard[F:/[1Lb]]
and the defeasible result of this, i.e.,
</bodyText>
<table confidence="0.343787615384615">
DefFS([/a n F
&lt;&gt;
• 112 = n 1.2 = {([F:a],t)} \ Bot12
• T12 = (T1 It 1
= u T2)
T2 = {([F:u],e),([F G b ],e)}
Bot12 = 0
So
T12 = (74 U T2)\Bcit12
{([F a],t),([F u],e),([F [G b
Therefore the specificity partition of T12 is (p1, pa), where:
= {([F a],t)} [c ],tf)}
P2 = f([F ul,t1),([F
</table>
<figure confidence="0.498328769230769">
• D12 = UM/12 h. ofs(iii)) ofs(p2))
Li( ( [T]h„{ [F a] } )
rics{ [F:u],[F:[G:b]] )
u( [.a] h„{ [F:u],[F,[c:b]] )
[taj
(because [ a] n [ F : u] and [ a] n [F: [C b]] both fail)
Hence
G b 1)
DefFS([ la] &lt;n&gt; [ Ft&apos; / G b 1) DefFS([ {( F : &apos;)}
= It a ( F u
([F, [G:13]
76
Lascarides and Copestake Default Representation
</figure>
<subsectionHeader confidence="0.985428">
3.7 The Properties of F and DefFS
</subsectionHeader>
<bodyText confidence="0.999562033333333">
We now state the theorems that demonstrate that defaults have the properties that we
specified in Section 1.1. The proofs for these theorems are given in the appendix. The
rest of the paper can be understood without reading the appendix. In what follows,
we continue to use the notational convention that F, =def and DefFS(Ft) = D,.
First, criterion 1 is satisfied: nondefault information can be distinguished from
default information and is always preserved. Nondefault and default information are
distinguished as a direct result of our definition of ,TDFSs. n preserves all nonde-
fault information, because the nondefault result of 11 is defined in terms of ri on the
nondefault parts of the arguments, and Ii preserves information.
Second, criterion 2 is satisfied: default unification never fails unlesspere is conflict
in nondefault information. We assume F1 11 F2 fails if and only if F1 11 F2 = I/O. But
112 = 1 if only if /1 Ii 12 =- 1. That is, n fails only if there is conflict in the nondefault
information b. and 12. And T12 = 0 whenever 112 = 1 by the definition of T12 given in
H. So default unification fails only if monotonic unification does. And note that when
Fi 11 F2 fails, D12 =do DefFS(I/0) = I.
We have already shown, in Section 3.6, that the definitions of &lt;ri and DefFS sat-
isfy criterion 6: defaults can be given a precedence ordering such that more specific
information overrides less specific information.
Now all that&apos;s left are criteria 3, 4, and 5. We start with criterion 5. We have divided
the operation of computing the default result of two TDFSs into two operations-11
and then DefFS. Default unification is order independent if n is a commutative and
associative binary operation. In that case, the default result DefFS will be the same,
regardless of the order in which the TDFSs were unified, since the argument to DefFS
will be the same. Roughly speaking, the proof that His commutative and associative is
done in parts: we prove that the indefeasible part of&lt;F,1 is commutative and associative,
and we prove that the definition of tails given by n is commutative and associative.
The relevant lemmas and theorems that support this are specified below, and they are
proved in the appendix.
Lemmas land 2 ensure that as far as tails are concerned, the order of arguments
of TDFSs to n doesn&apos;t matter:
</bodyText>
<subsectionHeader confidence="0.466862">
Lemma 1
</subsectionHeader>
<bodyText confidence="0.940548875">
Tails are commutative. That is T12 = T21.
Lemma 2
Tails are associative. That is: T(12)3 = T1(23)
Lemmas 3 and 4 are necessary for proving both criterion 4 (default unification
returns a single result, deterministically), and criterion 5 (default unification is order
independent). Lemmas 3 and 4 ensure that the default unification on TDFSs and the
default results of the operation both return a single result deterministically, as required
by criterion 4.
</bodyText>
<footnote confidence="0.4556365">
Lemma 3
1-1 is a function. That is, TDFS1 FTDFS2 is a unique TDFS.
Lemma 4
DefFS is a function. That is, DefFS(TDFS) is a unique TFS.
</footnote>
<page confidence="0.983094">
77
</page>
<note confidence="0.624862">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.845893">
These lemmas contribute to the proofs of lemmas 5 and 6, which in turn are used
to prove theorem 1, which ensures criterion 5 is met:
Lemma 5
</bodyText>
<equation confidence="0.957083214285714">
11 is commutative. That is:
F12
dtf 1- 12
12IT
F21
=d4. .121 /7-21
Lemma 6
n is associative. That is:
0-(12)3
F(12)3 =d1. /(12)3
F1(23)
df 11(23) /T1(23)
Theorem 1
n is an order-independent binary function.
</equation>
<bodyText confidence="0.999976857142857">
Note that it follows immediately from this theorem that regardless of the order in
which a set of TDFSs are default unified, the default result as given by the operation
DefFS is the same. In particular, D12 — D21, and D(12)3 = Di (23) .
We now turn to criterion 3 from Section 1.1: Default unification behaves like mono-
tonic unification in the cases where monotonic unification would succeed. This crite-
rion is essentially about cases where&lt; there is no conflict in the arguments to be default
unified. When there is no conflict, n should give the same results as H. In our case,
this needs explication for two reasons. First, we have distinguished default from non-
default information in our TDFSs—as required by criterion 1—and so n and n work
on different types of arguments: the former is defined on TDFSs ii,v,hereas the latter
is defined on TFSs. Therefore, we must refine what we mean by: H gives the same
result as n. Here we assume that it means that when there is no conflict between the
arguments, n and DefFS should give the same results as n for the indefeasible TFSs
and the defeasible TFSs respectively. That is, where
</bodyText>
<equation confidence="0.533389333333333">
F12 =,kf 11 IT1F &apos;211&apos;2
and
DefFS(Fi)--.=d1. Di
112 = 11 n 12,
and
D12 = D1 H D2
</equation>
<bodyText confidence="0.999883333333333">
In other words, on the defeasible part, DefFS(Fi&lt; F2) &lt;=,DefFS(Fi) n DefFS(F2). Note
that 112 = 11 n 12 follows trivially from the definition of II , regardless of whether there
is conflict or not. We must show that when there is no conflict between the arguments
to n , D12 = D1 n D2
The second thing that must be explicated is the conditions under which the ar-
guments to n —namely, two TDFSs—are said to conflict. When default unification is
</bodyText>
<page confidence="0.986875">
78
</page>
<note confidence="0.707829">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.984125470588235">
defined in terms of TFSs (e.g., Carpenter 1993), the concept of conflict is clear: the
TFSs conflict if their unification is I. However, because we have distinguished default
from nondefault information in TDFSs to satisfy criterion 1, and we have introduced
tails into TDFSs to satisfy criterion 5, our definition of when two TDFSs conflict is
necessarily a little more complex. What should conflict mean in this case? There are
at least two alternatives. First, two TFDSs conflict if D1 7 D2 = I. In other words, they
conflict if DefF S (F1) H DefF S (F2) = I. Second, two TDFSs conflict if the information
in the two tails conflicts, either with each other or with the accumulated indefeasible
information 112. That is, the two TDFSs conflict if 112 7 ofs (T1) n pfs(T2) = I. Note
that given that T1 potentially contains mutually incompatible information (in the IFS
sense), and similarly for T2, and in general P contains more information than D, (by
the definition of DefF S), the latter condition of conflict is much weaker than the former
condition. As it turns out, we can prove that D12 = D1 7 D2 (i.e., 7 behaves like 7
when there is no conflict) only in the case where there is no conflict in the latter sense.
This is given in theorem 2 below.
Theorem 2: The Typed Unification Property
Let F1, F2 be TDFSs, and suppose
</bodyText>
<equation confidence="0.9281134">
112 7 p11(T1) 17 Pfs (T2 )
Then
D12 = D1 n D2
In other words:
DefF&apos;S(FiF F2) = DefF S (F DefFS(F2)
</equation>
<bodyText confidence="0.923876">
In general, F does not reduce to 7 when there is no conflict between the defeasible
TFSs (i.e., when D1 7 D2 I). The following example demonstrates this:
</bodyText>
<figure confidence="0.829025833333333">
Where
ti IT t2 IT t3
a c =
b c =
a Li b =1
IF T / {([FG .11],t1),
</figure>
<bodyText confidence="0.842170333333333">
LG.Ti ([F ([G b],t2)1 [ r,TT] {([F c],t3)}
We just show how to calculate the defeasible TFS D12 =d,f DefF&apos;S (Fi&lt; d F2):
LI( h„{ [Ll] h„{ [F: a], [G :13] 1 )
</bodyText>
<equation confidence="0.914791857142857">
GT
hcs { [ F c] })
—[ tl hcs { [F a], [G b] h„ [Fc] 1 )
u(IF Wal,IF. b n9{ [Fc] } )
[V, Fa l LI IF mbl
L G 0 j L G j
D12
</equation>
<page confidence="0.929824">
79
</page>
<note confidence="0.361885">
Computational Linguistics Volume 25, Number 1
</note>
<equation confidence="0.781424833333333">
ti
= [F.m
G M
Thus D12 Di ri D2 in this case, even though
Di 17 D2 [F MC]
G M
</equation>
<bodyText confidence="0.999934111111111">
We have suggested that because TDFSs contain more information than the defaults
that win, the notion of conflict is more complex than in Carpenter (1992, 1993) and
Bouma (1992). Their definitions of default unification correspond to the situations
where we are unifying basic TDFSs. This is because these record default information
but, like Carpenter&apos;s and Bouma&apos;s accounts, they don&apos;t track overridden default values
from the unification history (because there hasn&apos;t been any unification history). We can
prove that the typed unification property for basic TDFSs is one where Fl reduces to
ii when there is no conflict among the TFS components. The corollary 1 below follows
from the above theorem and the following lemma:
</bodyText>
<subsectionHeader confidence="0.336932">
Lemma 7: DefFS for Basic TDFSs
</subsectionHeader>
<bodyText confidence="0.779937">
Let F be a basic TDFS I/T derived from the indefeasible TFS I and the defeasible TFS
ID. Then
</bodyText>
<equation confidence="0.451241">
DefFS(F) =-„f D = I 11 ID
</equation>
<figureCaption confidence="0.579515">
Corollary 1: The Typed Unification Property for Basic TDFSs
</figureCaption>
<bodyText confidence="0.99471">
If TDFS1 and TDFS2 are basic TDFSs derived from II and ID,, and /2 and ID2 respectively,
and II n ID, n 1n ID, I (i.e., D1 n D2 0 I), then
</bodyText>
<equation confidence="0.876023">
D12 = D1 n D2
=- .11 n fl 12 ri ID2
</equation>
<bodyText confidence="0.9989735">
Thus criterion 3 holds, refined as required because of the more complex informa-
tion structures (i.e., TDFSs) that are necessary in order to meet the other criteria.
</bodyText>
<subsectionHeader confidence="0.998157">
3.8 Comparison between PDU and YADU
</subsectionHeader>
<bodyText confidence="0.983176">
For convenience, we summarize the major similarities and differences between PDU
and YADU here.
</bodyText>
<listItem confidence="0.9937985">
1. Both PDU and YADU can be defined using conditional logic. However,
unlike PDU, YADU has a simple interpretation as an algebraic operation,
which is what we have given here. This considerably simplifies the
proofs that the operation has the desired properties.
2. PDU does not fully satisfy our criterion 6 from section Section 1.1,
because default coindexations cannot be overridden by default values. In
contrast, YADU allows coindexations to be overridden by default values.
This is because (a) the defaults on paths are not computed independently
of one another, and (b) YADU keeps track of overridden default
reentrancies in the tails.
3. Furthermore, because tails are extended this way in YADU compared
with PDU, the default structure in a TDFS does not need to be defined
separately; it is computable directly from the indefeasible TFS and the
tail.
</listItem>
<page confidence="0.956457">
80
</page>
<bodyText confidence="0.22755">
Lascarides and Copestake Default Representation
</bodyText>
<listItem confidence="0.907534">
4. PDU can result in default TFSs that are not well formed, in that they
contain I, or violate a very basic notion of well-formedness. This
necessitates a complex DefFill operation in order to utilize the default
structures. In contrast, in YADU, the default structures are guaranteed to
be well-formed and DefFill is basically equivalent to treating the default
structure given by DefFS as being nondefault.
5. YADU can be extended to handle default inequalities (see Section 6.1
below); PDU cannot be extended this way.
</listItem>
<sectionHeader confidence="0.674462" genericHeader="method">
4. Linguistic Examples
</sectionHeader>
<bodyText confidence="0.999954756756756">
In this section, we go through some linguistic examples where defaults can be utilized.
The aim of this section is to illustrate ways in which the use of defaults can simplify
and enhance monotonic grammars, concentrating on syntax and (compositional and
lexical) semantics. In particular, of course, our aim is to show the linguistic utility of the
particular assumptions we have made in defining YADU. We do not have space here
to systematically illustrate the ways in which defaults may be utilized in a grammar,
though the papers listed in Section 1 give many more examples, which can, in general,
also be modeled within the current framework (with the main exception being some
uses that require asymmetric defaults, discussed in Section 6.5, below). We have tried
to give examples that have not, to our knowledge, been extensively discussed in the
previous default unification literature.
We assume an HPSG-like framework, but we have tried to give an explanation
in enough detail for readers who are relatively unfamiliar with HPSG to follow the
examples. In HPSG, the basic linguistic structure is the sign. Signs may be lexical or
phrasal, but always correspond to constraints that may be represented using typed fea-
ture structures. In what follows, we will instead use TDFSs, and take various liberties
with the feature geometry for ease of exposition. However, although much simplified,
the treatment in the grammar fragments here is substantially based on that assumed
in the English Resource Grammar (ERG) under development at CSLI (Flickinger, Sag,
and Copestake, in preparation). The ERG itself has been developed without making
use of defaults up to this point, using the DISCO/PAGE system (Uszkoreit et al. 1994),
but it also runs within the LKB system (Copestake 1992). YADU has been implemented
within the latter system, replacing the earlier version of default unification described
in Copestake (1993).
Before giving detailed examples, however, we must make some further remarks
on the assumptions we are making about persistent and nonpersistent defaults in
these fragments. In the example of inflectional morphology that we gave in Section 2,
we mentioned that the defaults could not persist beyond the lexicon, since the values
for the suffixes must be treated as hard information by the parser/generator. More
generally, we distinguish between defaults used in the description of some object in
the grammar (that is, a lexical sign, a lexical rule or a grammar rule/schema), which
we refer to as nonpersistent defaults, and defaults that are intended to contribute to
the output of the grammar (in particular, the logical form), which we term persistent
defaults. In fact, most of the examples we will discuss here make use of nonpersistent
defaults, and where we intend defaults to be persistent we will distinguish this with
a subscript p following the slash in the case of the abbreviatory notation, and with an
annotation on the tail in the full description.&apos;
</bodyText>
<footnote confidence="0.694116">
4 In principle at least, there can be a mixture of defaults of varying persistence in the tail of a TDFS, and
</footnote>
<page confidence="0.989837">
81
</page>
<note confidence="0.6347">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.999955666666667">
In the examples below, as in Section 2, we assume that nonpersistent defaults
may be part of type constraints. This is in line with the practice followed in the ERG,
where some types are purely lexical, that is, they are only utilized in the description
of lexical entries and are irrelevant to the parser/generator. However, some grammar
writers deprecate such use of types, and capture the corresponding generalizations
with macros or templates. For our current purposes, this controversy is of limited
relevance, since the same TDFSs that we are describing as constraints on types could
alternatively be used as the values of templates. The only requirement is that, in order
to have a compatible notion of specificity, the templates must form a partial order
(which will be naturally true if they are used as an inheritance hierarchy) so that
template order can replace type order in the determination of specificity in YADU
(as discussed in Section 6.2). Under these circumstances, nonpersistent defaults would
most naturally be seen as part of the description language. For further discussion
of the type/template distinction, in the context of a default inheritance system, see
Lascarides et al. (1996).
</bodyText>
<subsectionHeader confidence="0.943042">
4.1 Modals
</subsectionHeader>
<bodyText confidence="0.999967">
One very straightforward example of the utility of defaults in descriptions of syntactic
behavior is the treatment of ought in English. It behaves as a modal verb in most re-
spects: it inverts, it can be negated without do, it takes the contracted negation oughtn&apos;t,
and it does not have distinct inflected forms (though we will ignore morphological
issues here). However, unlike most modals, it requires the to-infinitive, as shown in
(6a—d), rather than the base form of the verb. The modal is to behaves similarly, as
shown in (6e), although it inflects.&apos;
</bodyText>
<listItem confidence="0.9964226">
(6) a. I ought to finish this paper.
b. * I ought finish this paper.
c. You oughtn&apos;t to disagree with the boss.
d. Ought we to use this example?
e. You are not to argue.
</listItem>
<bodyText confidence="0.995193333333333">
Here we sketch a treatment of modals that allows ought to be an exception to
the general class, just with respect to the value of the attribute on its complement
specification, which specifies whether ought expects a base or infinitival complement.
We also briefly introduce the syntactic feature geometry we will also assume for all
the examples that follow.
Figure 7 shows a constraint specification for a type modal, which we assume
is the lexical type of all modal verbs including ought. This constraint is responsible
for the characteristic behavior of modals mentioned above. Because we are ignoring
morphology, the figure only shows the SYNSEM (syntax/semantics) part of the lexical
the DefFill operation must be defined so that it is sensitive to this distinction and only incorporates
defaults of the appropriate persistence. To do this, we can define a partial order on persistence markers
and add a persistence marker as an extra component to the definition of a tail. DefFill is then defined
relative to a particular persistence, and incorporates all tails marked as having that persistence or any
persistence that is prior in the partial order. Since none of the examples that follow make use of mixed
persistence tails, we ignore this complication here.
5 For some speakers the infinitive without to is possible or even preferred with ought in nonassertive
contexts. For others, ought does not share the other properties of modals we have listed here. We will
ignore these dialect variations here, and simply assume the grammaticality pattern shown in (6).
</bodyText>
<page confidence="0.98585">
82
</page>
<figure confidence="0.968937">
Lascarides and Copestake Default Representation
[ modal
HEAD AUX true
SYNSEM [ HEAD VFORM
VAL COMPS
VAL COMPS :
</figure>
<figureCaption confidence="0.890882">
Figure 7
</figureCaption>
<figure confidence="0.995547529411765">
Constraint on type modal.
head-comp-phrase
ORTH : could sleep
SYNSEM : [ HEAD :
VAL COMPS 0
modal
ORTH could
SYNSEM : [HEAD:[ AUX : true
VAL COMPS: (E
ORTH : sleep
HD-DTR
: fbse
NON-HD-DTRS
SYNSEM :
[ 1
HEAD VFORM : bse
VAL COMPS : ()
</figure>
<figureCaption confidence="0.995542">
Figure 8
</figureCaption>
<bodyText confidence="0.995796642857143">
Structure of an HPSG-style phrase for could sleep.
sign, and we will ignore the semantics of modals for this example. Subcategorization
properties of signs are specified via valence (VAL) features, which describe a list of
specifications with which the SYNSEM values of other signs may be unified. For verbs,
the relevant valence features are SUBJ (subject) and COMPS (complements), though
we have systematically omitted SUBJ in this example for simplicity The constraint
given on modal for the value of the COMPS list means that it may only contain a
single element. That element must have a verbal HEAD, a single member subject list,
and an empty complements list: i.e., it must be a verb phrase. Thus the constraint
means that modals are restricted to taking a single verb phrase complement, which
we assume here is obligatory. The value of the VFORM of the complement is specified
by default to be bse, which is true of verbs and verb phrases where the head is a base
form, but not of infinitivals (which have VFORM inf).
Figure 8 illustrates the complement selection mechanism in operation. It shows a
simple instance of a head complement phrase consisting of a modal (could) and its verb
phrase complement (sleep). As we mentioned above, phrases are represented as TDFSs.
Generalizations about phrases, or schemata, are represented as constraints on types
and correspond to grammar rules in other frameworks. Here head-comp-phrase is a
type, corresponding to head-complement phrases, as we will discuss in more detail in
Section 4.2. Headed phrases have features HD-DTR, indicating the head of the phrase,
which in this case corresponds to the structure for the modal verb could, and NON-
HD-DTRS, which corresponds to a list of the other daughters. Here NON-HD-DTRS
is a singleton, corresponding to the verb phrase sleep. The head-complement schema
constrains the list of SYNSEMs of the NON-HD-DTRS signs to be equal to the COMPS
list of the head daughter.
The constraint for modal in Figure 7 also specifies that the HEAD AUX value of
modal signs is true indefeasibly. This controls the behavior with respect to various
lexical rules. For instance, negation is implemented via a lexical rule which, among
</bodyText>
<page confidence="0.993353">
83
</page>
<figure confidence="0.980440304347826">
Computational Linguistics Volume 25, Number 1
Description for could: [1131M : could]
Structure for could:
Description for ought:
Structure for ought:
modal
ORTH could
[ HEAD AUX true
VAL COMPS &lt;
VAL COMPS : ()
HEAD VFORM : bse
modal
ORTH : ought
[ ]
SYNSEM VAL COMPS HEAD VFORM : inf )
- modal
ORTH : ought
- HEAD AUX true
HEAD VFORM : inf
VAL COMPS (
VAL COMPS :
SYNSEM
SYNSEM
</figure>
<figureCaption confidence="0.980589">
Figure 9
</figureCaption>
<subsectionHeader confidence="0.52514">
Examples of lexical description for modal verbs.
</subsectionHeader>
<bodyText confidence="0.999974357142857">
other things, adds a structure compatible with not to the beginning of the complement
list of the verb. This lexical rule only applies to verbs such as auxiliaries and modals
which, unlike other verbs, have a value for SYNSEM HEAD AUX compatible with
true. The AUX feature also controls the application of the inversion lexical rule.
The lexical specification for could, which we take as an example of a normal modal,
is also shown in Figure 9. The only idiosyncratic information given here is the morphol-
ogy (though of course in the full entry there is also a specification of the semantics).
Thus the TFS for could inherits all the information from the constraint on the type
modal. Note, however, that the default for the VFORM of the complement must be
nonpersistent, and thus the actual TFS for the lexical sign is calculated via DefFill,
with the result that is shown in Figure 9. The structure for could sleep, which is derived
from the sign shown in Figure 9, is shown in Figure 8 (for details on constructing
this, see Section 4.2). In contrast, the lexical specification for ought overrides the de-
fault value inherited from modal with the nondefault value inf for the VFORM of its
complement. So ought cannot combine with sleep in the manner shown in Figure 8, as
required.
This is a very simple example but it illustrates that the use of defaults allows the
grammar to capture the generalization that most modals take a base form complement.
It is possible to devise a monotonic encoding that would produce the same lexical FSs
but the various behaviors characteristic of modals would have to be split into different
constraints, so that ought could inherit some but not all of them. This would not capture
the intuition that ought is the exceptional case. Furthermore a monotonic encoding
requires extra types. With the example as shown here, it appears that the monotonic
encoding would require two additional types compared to the default encoding, one
for ought and is to (and also used to, for speakers for whom this is a modal) and the
other for all other modals. However, in the full English Resource Grammar, seven
types are duplicated to allow for ought. The real gain in conciseness from allowing
defaults is therefore more significant than our simplified example suggests.
</bodyText>
<page confidence="0.994219">
84
</page>
<note confidence="0.692112">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.963415928571429">
phrase
non-hd-ph hd-ph
hd-adj-ph hd-nexus-ph
hd-fill-ph hd-comp-ph hd-subj-ph hd-spr-ph
non-hd-ph non-headed-phrase
hd-ph headed-phrase
hd-adj-ph head-adjunct-phrase
hd-nexus-ph head-nexus-phrase
hd-fill-ph head-filler-phrase
hd-comp-ph head-complement-phrase
hd-subj-ph head-subject-phrase
hd-spr-ph head-specifier-phrase
Figure 10
Hierarchy of phrases from Sag (1997).
</bodyText>
<subsectionHeader confidence="0.911633">
4.2 Defaults in Constructions
</subsectionHeader>
<bodyText confidence="0.999977038461539">
Default inheritance can also be utilized in the description of rules, schemata, or con-
structions within the grammar itself. Recent work within HPSG has demonstrated
the utility of describing a hierarchy of phrasal types in a manner analogous to the
more familiar lexical hierarchy. As we mentioned in the previous example, in HPSG,
phrasal types play a similar role to rules in other frameworks, systematically relating
the mother of a phrase to its daughters. Sag (1997), in an account of relative clause
constructions, defines a general type phrase, from which various subtypes of phrase
inherit (Figure 10). Sag uses defaults in his description of the phrasal hierarchy, but
we go through the example in detail here, in order to demonstrate more formally how
default unification operates in this case.
We will not go into the full details of the hierarchy here, since our main concern
is to demonstrate the way in which defaults are used. We will just consider the con-
straints on headed-phrase (shown in Figure 11) and the way these are inherited by
some of its subtypes.&apos; The Head Feature Principle (HFP), which states that the HEAD
value of the mother is identical to that of the head daughter, is unchanged from earlier
work on HPSG (e.g., Pollard and Sag 1994, 34) and is not default. In contrast the Va-
lence Principle (VALP) and the Empty Complement Constraint (ECC) are both stated
in terms of defaults. VALP is a reformulation of the Valence Principle in Pollard and
Sag (1994, 348). Here SUBJ (subject), SPEC (specifier) and COMPS (complements) are
all valence features, and the effect of the constraint is to specify that these should be
identical on the head daughter of a phrase and the mother, unless a more specific
phrase type overrides this. The exceptions to the generalization that valence informa-
tion is identical on the head daughter and mother are the phrase types where the
individual valence features are satisfied. For example, in a head-complement phrase,
the complements of the head of the phrase are instantiated and the COMPS list of the
mother will be empty (as shown above in the example in Figure 8). ECC is a simpler
</bodyText>
<footnote confidence="0.876398333333333">
6 The constraints are presented slightly differently from Sag (1997), since he sometimes omits features on
paths when specifying constraints, but for the sake of clarity we show full paths here. We have also
added the feature VAL, as in the ERG and the example above.
</footnote>
<page confidence="0.996462">
85
</page>
<figure confidence="0.991847333333334">
Volume 25, Number 1
Computational Linguistics
Head Feature Principle (HFP):
Valence Principle (VALP):
[SYNSEM HEAD :
HD-DTR SYNSEM HEAD
_ _
SUBJ,
SYNSEM VAL : SPR : / E
: / 1
COMPS
[
: 1 _
SUBJ
HD-DTR SYNSEM VAL : SPR : ig
_ COMPS/ El
El
Empty Complement Constraint (ECC): HD-DTR SYNSEM VAL COMPS /0 I
</figure>
<figureCaption confidence="0.980399">
Figure 11
</figureCaption>
<bodyText confidence="0.792714666666667">
The HFP, VALP, and ECC constraints (abbreviatory notation).
- hd-phr
HEAD 0
</bodyText>
<equation confidence="0.961626">
VAL [ SPR list
SUBJ : list
COMPS : list
HEAD :
VAL [SPR list
COMPS : list
SUBJ list I
[ SS VAL SUBJ :
1` HD-DTR SS VAL SUBJ El hd-phr),
[ SS VAL SPR ], hd-phr),
HD-DTR SS VAL SPR
[SS VAL COMPS : E , hd-phr),
HD-DTR SS VAL COMPS 0
K[ HD-DTR SS VAL COMPS 0 ], hd-phr)}
SS
HD-DTR SS
Figure 12
</equation>
<bodyText confidence="0.999677214285714">
Expressing the HFP, VALP, and ECC constraints as a single TDFS (SS here is an abbreviation
for SYNSEM).
constraint which states that, by default, headed-phrases have head daughters with an
empty complement list. It is important to note that although Sag states HFP, VALP,
and ECC as three separate constraints, this is equivalent to treating their conjunction
as a single constraint on the type headed-phrase, as shown in Figure 12 (for clarity we
use the full indefeasible/tail notation for a TDFS). Note that one advantage of order
independence in our definition of defaults is that the result of specifying constraints
individually is guaranteed to be equivalent to stating them as a single TDFS, leading
to greater perspicuity in the definition of a grammar.
In Figure 13, we show the constraint specifications on the types head-nexus-
phrase, head-complement-phrase, and head-specifier-phrase, as given by Sag (with
modifications as above). The constraint on head-nexus-phrase refers to how the se-
mantic content is shared between mother and head daughter (a default version of this
constraint that actually removes the need for the type head-nexus-phrase will be dis-
cussed in Section 6.1). For head-complement-phrase and head-specifier-phrase, the
attribute NON-HD-DTRS corresponds to a list of the daughters of the phrase excluding
the head. As in Figure 8, elements of the appropriate valence feature are instantiated
by the SYNSEMs of the nonhead daughters.
There is a complication here: the constraint on head-complements-phrase given
by Sag is intended to be read as specifying that an arbitrary number of complements
(possibly zero) correspond to the value of the COMPS feature. In fact, this cannot be
directly implemented as written in the framework we assume here. The required effect
can be achieved with a recursive type, or, on the assumption that the complements list
can contain no more than four elements, multiple subtypes can be specified, each with
a fixed number of complements. For simplicity, we have assumed the latter encoding
style here, which we illustrate with two schemata in Figure 13, for the zero complement
(head-zero-comp-phrase) and the one complement cases (head-one-comp-phrase).
</bodyText>
<page confidence="0.973423">
86
</page>
<figure confidence="0.963352625">
Default Representation
[[[[[SYNSEM CONT
HD-DTR SYNSEM CONT E
SYNSEM VAL SPR : 0
HD-DTR SYNSEM VAL SPR
NON-HD-DTRS : ( [ SYNSEM : E ] )
SYNSEM VAL COMPS: 0
HD-DTR SYNSEM VAL COMPS : (111,.
NON-HD-DTRS : ( [ SYNSEM : [ii ]
SYNSEM VAL COMPS : 0
HD-DTR SYNSEM VAL COMPS 0 1
NON-HD-DTRS : 0
SYNSEM VAL COMPS , 0
HD-DTR SYNSEM VAL COMPS
NON-HD-DTRS : ( [ SYNSEM : 11 ] )
Lascarides and Copestake
head-nexus-phrase
head-spr-phrase
head-comp-phrase
(...notation)
head-zero-comp-phrase
head-one-comp-phrase
,
SYNSEM :
</figure>
<figureCaption confidence="0.7508395">
Figure 13
Constraints on phrases.
</figureCaption>
<bodyText confidence="0.938330878787879">
Figure 14 shows the final structures for the schemata for the head-complement-
phrases and head-specifier-phrase, after inheritance from the supertypes and mak-
ing the default properties nondefault.7 Note that in the head-one-complement-phrase
schema, the ECC is overridden, as is the part of VALP that concerns the coindexa-
tion between the complements of the head-daughter and the mother, while in head-
specifier-phrase, the coindexation of the specifier has been overridden.
One interesting point is that the specification language discussed in Section 3.5
allows for a very succinct encoding of constraints which, like the valence principle,
state that a set of features are identical by default. Figure 15 shows this alternative
encoding for VALP: the default TFS states one coindexation, between the VAL features
for mother and head daughter. Because the definition of BasicTDFS states that a path
equivalence will be present in the tail for each case where a node can be reached by two
distinct paths, the TDFS contains tail elements indicating a path equivalence not only
for VAL, but also for all paths that extend VAL, i.e., SUBJ, COMPS, and SPR (for clarity,
we have shown the type list explicitly, rather than using the angle-bracket notation:
list has no appropriate features). Thus the TDFS has one extra tail element compared
to the version we gave in Figure 12, but this extra element will be overridden in all
of the schemata (except head-zero-comp-phrase, if we use the encoding assumed in
Figure 13).8
Thus the use of defaults allows a more concise specification of the phrase hier-
archy, enabling generalizations to be captured that would be obscured with a purely
monotonic treatment. Although these constraints could be captured in a system with-
out defaults, either they would have to be stipulated redundantly, in multiple points in
the hierarchy, or extra types would have to be introduced so that multiple inheritance
could be used to distribute properties appropriately. This latter option would consid-
7 Sag (1997) suggests that the constraints are converted from default to nondefault for all maximally
specific phrasal types, which means that these must have a special status. A more straightforward
approach is to assume that the terminal schemata are analogous to lexical entries, rather than being
types in their own right.
8 This encoding style also has potential in the representation of lexical rules, where these are represented
as single TDFS with an input and an output feature, since the default TFS in the specification can state
that the input is equal to the output. This allows a much more succinct description than is possible in
an unextended monotonic language, where feature values must be explicitly duplicated.
</bodyText>
<page confidence="0.992784">
87
</page>
<figure confidence="0.997337065217391">
Computational Linguistics Volume 25, Number 1
head-zero-comp-phrase schema
head-one-comp-phrase schema
head-spr-phrase schema
_ NON-HD-DTRS : 0
HEAD : 0
SYNSEM : VAL : SPR : E
SUBJ
COMPS : o
[
CONT : El
HEAD
HD-DTR SYNSEM : VAL : SPR
COMPS:
: El
_ CONT: 0
SUBJ
( E)
- : El
[
- NON-HD-DTRS : ( [ SYNSEM : E ] )
HEAD : 0 _
_
SYNSEM VAL : SPR : 0
SUBJ
[
COMPS : El
CONT : E
HEAD
SYNSEM VAL SPR : E
[
0 1 1
SUBJ El
CONT
COMPS : 0
[ HEAD . E
HD-DTR SYNSEM VAL COMPSSPR 0 i
CONT . SUBJ . 0
HEAD0_
HD-DTR SYNSEM VAL : SPR : (0 )
COMPS
CONT • 0
SUBJ: El 1 1
: 0
[ :
_ NON-HD-DTRS ([SYNSEM : u]&gt;
</figure>
<figureCaption confidence="0.8878885">
Figure 14
Expanded schemata.
</figureCaption>
<bodyText confidence="0.9994841">
erably complicate the hierarchy for the set of default constraints that Sag considers. In
neither case could the valence principle be stated as a single generalization. The only
way in which this would be possible in a monotonic system would be if the constraint
language were enriched. This example shows the utility of allowing overriding of
default path equivalence statements. This would not be possible with some previous
versions of default unification, including Russell, Carroll, and Warwick-Armstrong
(1991), Russell et al. (1993), and the second version of the LKB default unification
given in Copestake (1993). It is also not allowed in Young and Rounds (1993), which
is the only order-independent version of default unification of which we are aware
apart from PDU and YADU.
</bodyText>
<subsectionHeader confidence="0.869988">
4.3 Agreement and Semantic Plurality
</subsectionHeader>
<bodyText confidence="0.9998855">
We turn now to an example involving a more complex use of YADU, in which it
is necessary to allow default values to survive when default path equivalence state-
ments are overridden. The example concerns the relationship of number agreement to
semantic notions of plurality and massness in English nouns. It is partially based on a
treatment given in Copestake (1992), but the use of YADU enables a more satisfactory
encoding of the central intuition, which is that agreement usually, but not always,
follows the semantics.
To explain the account, we must first briefly introduce the style of semantic en-
</bodyText>
<page confidence="0.996973">
88
</page>
<figure confidence="0.681515034482759">
Lascarides and Copestake Default Representation
hd-phr
HEAD .
SS VAL SUBJ list SUBJ [SS VAL D 0
HD-DTR SS SPR list list HD-DTR SS VAL 111 CPS
CPS . list list
HEAD E
VAL SPR
1)
BasicTDFS(
CPS list jj
SS
hd-phr
[ HEAD . El
SUBJ : list
VAL SPR : list
CPS . list
HEAD . E
SUBJ : list
HD-DTR SS: VAL : SPR list
CPS list
{([npv_aiz VAL ],hd-phr),
( SS VAL SUBJ rn
HD-DTR SS VAL SUBJ a], hd-phr),
/ SS VAL SPR Iiihd-phr),
HD-DTR SS VAL SPR
SS VAL CPS pp
HD-DTR SS VAL CPS E &apos; hd-phr),
( [ HD-DTR SS VAL CPS
</figure>
<figureCaption confidence="0.953025">
Figure 15
</figureCaption>
<bodyText confidence="0.971991333333333">
Encoding VALP using the Basic TDFS notation (SYNSEM is abbreviated SS, COMPS is CPS).
coding used in the ERG. This is known as Minimal Recursion Semantics (MRS) and is
described in detail in Copestake et al. (1995), though for the sake of simplicity we will
ignore most of the details in these examples, including all discussion of quantification.
The semantics (i.e., the value for the CONTENT feature) for the lexical sign for dog is
shown in (7a). This has an interpretation roughly equivalent to that of (713).
</bodyText>
<equation confidence="0.9434552">
INDEX E AGR NUM sg
(7) a.
LISZT ( dogrel
INST : E
b. Ax[dog(x)]
</equation>
<bodyText confidence="0.999401944444444">
The feature INDEX in (7a) indicates the equivalent of the lambda variable in (7b).
Features on the index indicate agreement, in the usual way in HPSG: here the noun will
agree with a verb that takes singular agreement (we only consider number agreement
here). The LISZT contains a list of relations, which is a singleton here as for most
lexical signs (composition of the semantics proceeds by appending LISZTs). The type
of the relation, _dog_rel, indicates the predicate name. The initial underscore in the
type name is a notational convention to indicate a lexical predicate, so that we can
for instance distinguish the type noun_rel, which is a general type for noun relations
in the type hierarchy, and _noun_rel, which is the relation corresponding to the noun
noun. The argument to the predicate is indicated by INST, which is coindexed with
the value of INDEX.
This structure has to be augmented to represent semantic plurality and individ-
uation (i.e., the mass/count distinction) since the AGR NUM feature will not always
make the appropriate distinctions. Although in normal count nouns (such as dog in its
most usual interpretation) number agreement and semantics are in step, this is not true
for all nouns. Some nouns, such as scissors or trousers, that denote bipartite objects have
obligatory plural agreement. Other count nouns, such as gallows and barracks, show
variable agreement, even when referring to a single object (Quirk et al. 1985). Mass
</bodyText>
<page confidence="0.999166">
89
</page>
<note confidence="0.733019">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.967805416666667">
terms usually take singular agreement, but there are exceptions, such as clothes, which
although it behaves semantically as a mass term, nonetheless takes plural agreement.
Here we will assume that semantic plurality/individuation is indicated by a predicate
modifier, as illustrated in (8).
single entity Ax[struth(x)] One truth is self-evident.
(8) plural entity Ax[Ptruth(x)] Some truths are self-evident.
unindividuated entity Ax[mtruth(x)] There is much truth in that.
For simplicity, we have assumed that all occurrences of noun predicates are modified
either by M, P. or S (corresponding to mass, singular, and plural) though it does not
matter if one of these subcases is taken to correspond to the unmodified predicate, or
if the structure assumed is more complex, since all that matters for current purposes is
that there is some three-way distinction in the formal semantics. Similarly, we need not
go into the details of the corresponding models (though see Krifka [1987] for example).
One way of capturing the distinction in MRS is to add another feature to the
relation to record the predicate modifier, which we will call PLMOD. The values of
this slot are irrelevant from the perspective of the formal semantics, as long as we
can make a three-way distinction. However, in order to facilitate the provision of the
default relationship between semantic values and agreement, we will use the values
sg, pl, and mass, where the first two types also correspond to possible values of the
AGR NUM slot. The hierarchy for these atomic types is shown at the top of Figure 16.
We can now capture the generalizations about semantics and agreement as shown
in Figure 16. The type noun-sign is a subtype of sign, and mass-sign is a subtype
of noun-sign (along with types for count nouns, pair nouns, and so on, which are
not shown here). Encoding the generalizations at the sign level allows inflectional
information to be included, though we do not show this here. But another reason
for not using the rel hierarchy is that we want types such as _truth_rel to be neutral
between mass and count. The default generalization about nouns is that the value of
the PLMOD and the AGR NUM paths are coindexed. This default will remain intact
for ordinary count nouns. For mass nouns, as shown in Figure 16, there is a default
value for the AGR NUM feature of sg and a nondefault value of mass for the PLMOD
feature. The lexical entries for clothing and clothes both inherit from mass-noun, but
the latter has AGR NUM of pl. We make the usual assumption that DefFill operates
on these lexical entries, to give the results shown in Figure 17. For this example, we
have made use of default reentrancy, and the fact that a default value can survive on
a node for which a default path equivalence was overridden, which was not the case
in PDU.9
</bodyText>
<subsectionHeader confidence="0.999968">
4.4 Persistent Defaults and the Interface between the Lexicon and Pragmatics
</subsectionHeader>
<bodyText confidence="0.99978775">
The utility of persistent defaults in the interface between the lexicon and pragmatics
was discussed in some detail in Lascarides et al. (1996), and from a more linguistic
perspective in Lascarides and Copestake (in press). However, since this sort of use was
an important part of the motivation for developing PDU and YADU, we will give one
</bodyText>
<footnote confidence="0.9228522">
9 Note that it is not actually the case for this example that a default value is overriding a default path
equivalence, although this is allowed in YADU, as we have discussed. The value of AGR NUM is
indefeasibly agrnum, which is incompatible with the indefeasible value mass for the PLMOD of
mass-nouns. However, in PDU this conflict would have resulted in the value I on the relevant node in
the default structure, and the default value sg on AGR NUM would thus in effect have been ignored.
</footnote>
<page confidence="0.992216">
90
</page>
<figure confidence="0.978670705882353">
Lascarides and Copestake Default Representation
number
mass agrnum
sg pl
1)1
noun-sign
[noun_rel
SS CONT LISZT ( PLMOD : number/ El
INST AGR NUM : agrnum/ E
mass-sign
SS CONT LISZT ( noun_rel 1)
PLMOD : mass
INST AGR NUM : /sg
[mass-sign
ORTH clothing
SS CONT LISZT ( [ _clothing_rel ])
mass-sign
[
ORTH clothes
SS CONT LISZT _clothes_rel
INST AGR NUM : pl
Figure 16
Relating agreement and semantics for nouns (SYNSEM is abbreviated SS).
mass-sign
ORTH clothes
[_clothes_rel
SYNSEM CONT LISZT ( PLMOD : mass
INST AGR NUM : pl
mass-sign
ORTH clothing
[_clothing_rel
SYNSEM CONT LISZT ( PLMOD : mass
INST AGR NUM : sg
-
</figure>
<figureCaption confidence="0.707507">
Figure 17
</figureCaption>
<tableCaption confidence="0.587289333333333">
Expanded and DefFilled lexical signs.
example here, which was not discussed in the former paper and was only considered
briefly and informally in the latter.
Verbs such as drink, eat, and bake have both intransitive and strict transitive uses.
Both uses have very similar meanings, since for these verbs, intransitive uses imply
a patient (in contrast to kick, for instance), but as pointed out by Fillmore (1986) and
</tableCaption>
<page confidence="0.993745">
91
</page>
<note confidence="0.720469">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.929048">
others, the intransitive uses have more specific default interpretations. In the absence
of information to the contrary, (9a) means (9b), (10a) means (10b), and (11a) means
(11b):
</bodyText>
<listItem confidence="0.9991798">
(9) a. John drinks all the time.
b. John drinks alcohol all the time.
(10) a. We&apos;ve already eaten.
b. We&apos;ve already eaten a meal.
(11) a. I spent yesterday afternoon baking.
</listItem>
<bodyText confidence="0.972292666666667">
b. I spent yesterday afternoon baking cookies, cakes, or bread.
(as opposed to ham, apples, or potatoes, for example)
These defaults can be overridden by arbitrary background information, for example:
</bodyText>
<listItem confidence="0.833834">
(12) As long as we&apos;re baking anyway, we may as well do the ham now too.
(due to Silverstein, cited in Fillmore [1986])
</listItem>
<bodyText confidence="0.999459464285714">
A purely pragmatic explanation for the default for bake seems implausible, since
there is no evident real-world explanation. For instance it would be difficult to claim
that people usually bake flour-based products as opposed to natural ones. A historical
justification could be suggested, since the Oxford English Dictionary (second edition)
says of bake:
primarily used of preparing bread, then of potatoes, apples, the flesh
of animals.
However, synchronically, the default interpretation seems to have become lexicalized:
even if the speaker often cooks potatoes by baking but very rarely prepares bread or
cakes, (11a) still implies (11b). This implies that the default is associated conventionally
with the word bake, rather than arising as a consequence of its meaning. The assump-
tion we make about the boundary between the lexicon and pragmatics is that the
lexicon is responsible for encoding the relationship between word forms and mean-
ings, and that pragmatics only has access to meanings. Under these assumptions, the
default must be encoded lexically, but in such a way that it can be overridden in
the right discourse context. This motivates the use of a persistent default: that is, one
which is not converted to hard information, in contrast to the descriptive defaults dis-
cussed in the previous examples (for further details and justification of these general
assumptions, see Lascarides and Copestake [in press]).
One way of describing an entry for bake that covers both transitive and intransitive
uses is sketched in Figure 18. We first go over the indefeasible part of the structure,
which will be the same for other verbs that take an optional noun phrase complement.
The VAL COMPS value is a singleton, which is constrained to have HEAD noun and
an empty complements list, i.e., to be a noun phrase. However, unlike in our earlier
examples, we have also shown the feature OPT on the complement. A complement
is treated as optional if its value of OPT is (compatible with) true (the details of the
schemata that achieve this are not relevant here). As before, we use the MRS style of
encoding semantics in TFSs. The INDEX value of the complement is coindexed with
</bodyText>
<page confidence="0.920949">
92
</page>
<figure confidence="0.9108455">
Lascarides and Copestake Default Representation
ORTH bake
OPT : true
HEAD : noun
VAL COMPS :
_bake_rel
CONT LISZT ( [ ARG1 ) E:1
ARG2 . E
</figure>
<figureCaption confidence="0.980039">
Figure 18
</figureCaption>
<bodyText confidence="0.9918763">
Simplified representation for intransitive and transitive bake.
the object slot in the main relation for the semantics of the verb. The MRS structure
for the verb relation corresponds to bake(x,y) where x is coindexed with the index
on the subject, and y with the object. The e in the CONT LISZT value of the verb is
shorthand for a complex feature structure that has the effect of appending the noun
phrase semantics to that of the verb.
The default part of the structure concerns the semantics for the noun phrase
complement, which conventionally would not be instantiated. Here, however, the
LISZT is stated, by default, to be the singleton relation flour-based_re1.10 Note the
subscript p, indicating that the default is persistent: that is, it is not subject to the
lexical DefFill. The effect of this is that the object of bake is given a default semantics
indicating that it is a flour-based substance. We assume that flour-based_rel is a type
that is incompatible with all lexical relational types, and thus any explicit object will
override this specification. However, in an intransitive use the default semantics will
be retained, giving a representation that can be represented in a linear notation as:
bake(e, x, y) A /flour-based(y). The manner in which this can be overridden by prag-
matics in examples such as (12) is outside the scope of this paper, but is discussed in
Lascarides and Copestake (in press).
With a suitably rich constraint language, one could devise a monotonic encoding
that would allow for an underspecified entry for bake, which could be specialized ei-
ther to have an obligatory complement or to be strictly intransitive with the additional
flour-based_rel. However, such a treatment would not allow for overriding by prag-
matics in contexts such as (12). Furthermore, it should be clear that this sort of use of
defaults is only possible if defaults can be distinguished from indefeasible parts of the
structure, and if they persist beyond the lexicon, and that an approach such as PDU
or YADU is therefore required for such examples.
10 In a full treatment the default semantics would also contain a quantifier. In fact the implicit object must
have narrow scope existential quantification. Combining YADU with a semantic representation capable
of representing quantifier scope in an underspecified fashion means that this can be made to follow
from a general default assignment of scope. However the details are outside the scope of this paper.
</bodyText>
<figure confidence="0.986721571428571">
INDEX :
flour-based_rel
LISZT 0 IP
K[ INST : )
SYNSEM
VAL COMPS
CONT :
</figure>
<page confidence="0.744428">
93
</page>
<note confidence="0.5702">
Computational Linguistics Volume 25, Number 1
</note>
<sectionHeader confidence="0.585597" genericHeader="method">
5. Theoretical and Practical Complexity Issues
</sectionHeader>
<bodyText confidence="0.999946551020409">
When discussing the complexity properties of YADU, we have to distinguish between
the n operation, which involves the combination of two TDFSs, and DefFS, the calcu-
lation of the default structure from a TDFS. One reason for drawing this distinction is
that, in a practical system, it may be necessary to carry out the former operation much
more frequently than the latter. As we mentioned in Section 3, it is only necessary to
construct the default feature structure at some interface: for example, at the interface
between the lexicon and the parser/generator (as in the examples in Sections 4.1, 4.2,
and 4.3) or between the grammar and the pragmatic component (as in the examples
in Section 4.4). In fact, only one DefFS operation is necessary per lexical entry (or other
grammar object), regardless of the depth or complexity of the inheritance hierarchy.
Similarly, one DefFS is required per parse for the use of persistant defaults. This is for-
tunate, since the combination operation has considerably better complexity properties
than the calculation of the default structure.
H&gt; simply involves unification of typed feature structures, set union of tails, and
removal of tail elements that are incompatible with the indefeasible structure. Check-
ing path-value tail elements for unifiability with the indefeasible structure involves
checking one pair of types to see if they are compatible. Checking path-equivalence
elements is roughly equivalent to unifying the relevant parts of the indefeasible struc-
ture: in the worst case this could amount to unifying two TFSs of (n —1)/2 nodes each
per tail-element, where n is the number of nodes in the indefeasible structure. But,
although we defined 11 as requiring the elimination of tail elements that are incom-
patible with the default, we could equivalently have left this to the DefFS operation,
and simply accumulated tail elements via set union. This is an insignificant overhead
on normal unification, since the tail elements have very simple structures.
In contrast, the worst-case properties of the calculation of the default TFS are un-
pleasant. Recall that this operation involves partitioning the tail and then carrying out
a step similar to asymmetric default unification as described by Carpenter (1993) for
each partition. The only known algorithms for computing asymmetric default unifi-
cation are factorial in the number of atomic FSs in the worst case (Carpenter 1993).
Thus, for a partition with t tail elements, the worst-case performance is proportional
to t! in the number of individual unification operations, where each unifcation could
involve up to (n — 1)/2 nodes, as above.
In practice, YADU is much better behaved on realistic examples than this would
suggest. The first point to make is that, as far as operations on TFSs themselves are
concerned, the DefFS operation can be implemented as a series of normal unification
steps. One consequence of this is that the use of YADU does not incur any significant
overhead with respect to ordinary unification when there are no tail elements. The
only additional requirement for YADU is that there be a slot at the top level of an
object&apos;s representation to store its tail. We mention this because it contrasts with some
other extensions to TFS formalisms: implementations of disjunction, for example, gen-
erally require that the TFS data structures and unification algorithms be considerably
more complex, with a consequent overhead in performance even for nondisjunctive
structures. Another mitigating factor is that it is easy for a grammar writer to tell in
advance whether a particular use of defaults is likely to be computationally expensive.
The worst-case complexity behavior only occurs in situations where there are interac-
tions between path equivalence statements that are not resolved by specificity. While
it is possible to invent pathological examples that have very deleterious performance
characteristics, it is not clear that comparable cases will often arise in real grammars,
especially if defaults are being used in a rather conservative fashion to extend a mono-
</bodyText>
<page confidence="0.996683">
94
</page>
<note confidence="0.734137">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.999925192307692">
tonic core grammar. Consider the example in Section 4.1, for instance. There is only a
single path value element in the tails of any of the structures described, and this will
straightforwardly either conflict or be compatible with an indefeasible atomic value.
Indeed in examples like this, the reduction in the numbers of types involved compared
to the purely monotonic encoding could potentially lead to an efficiency gain.
As we mentioned above, since PDU is polynomial, it has much better worst-case
behavior than YADU. However, making a realistic comparison is not straightforward.
The combination operation in PDU is more complex than in YADU, since it is neces-
sary to calculate default TFSs at every stage as well as tails. The constant overhead
compared to ordinary unification is higher and the implementation of PDU is trick-
ier than YADU. Furthermore, there is a trade-off between complexity behavior and
intuitiveness. The reason that PDU has better complexity behavior is that it always
accumulates default reentrancies. If there is a clash with default values, the default
reentrancies win—if there is a clash with indefeasible values, the coindexed nodes in
the default structure are set to 1, indicating inconsistency, and the DefFill operation
must subsequently take care of constructing a valid TFS by removing the default path
equivalences. However, this can lead to cases where potentially valid default path
equivalences are removed.
Thus, in PDU the sort of example that leads to the factorial worst-case complexity
in YADU is treated specially, in that maximal information is not incorporated from
the default structure. Roughly speaking, for these corner cases, there is a trade-off
between the complexity behavior in YADU, and the complex behavior of PDU.11 But
our main practical reason for preferring YADU over PDU is that PDU can behave in
unintuitive ways in examples where YADU would have nonproblematic complexity
behavior. It is also worth noting that YADU will not be slow if the tail partitions are
kept small, which is something the grammar writer can control.
</bodyText>
<sectionHeader confidence="0.951139" genericHeader="method">
6. Extensions and Alternatives
</sectionHeader>
<bodyText confidence="0.990765">
In this section, we briefly consider some variants on the definition of YADU that are
useful in specific circumstances.
</bodyText>
<subsectionHeader confidence="0.412677">
6.1 Inequalities
</subsectionHeader>
<bodyText confidence="0.96902875">
We have come across a number of cases where it would be useful to be able to over-
ride a default reentrancy without specifying conflicting values for the paths involved.
For example, consider the type hierarchy shown in Figure 10 and repeated in Fig-
ure 19 for convenience. For most of the subtypes of headed-phrase, the CONTENT
of the mother should be equivalent to the CONTENT of the head daughter. This
holds for head-subject-phrase, head-comps-phrase, and head-specifier-phrase and
their subtypes, but it is not true for head-adjunct-phrases, where the content value
of the mother is equal to the content value of the single non-head-daughter. It would
seem natural to specify the generalization on the supertype headed-phrase as a de-
fault constraint, as shown in Figure 20, and to override the default on the subtype
head-adjunct-phrase. This would allow the simplification of the hierarchy as shown
in Figure 21. However, in standard YADU, there is no way to express the idea that
the coindexation between mother and non-head-daughter should hold instead of the
coindexation between mother and head-daughter, since, as far as this structure goes,
11 Similar remarks also apply to the contrast between Bouma&apos;s (1990, 1992) and Carpenter&apos;s (1993)
versions of asymmetric default unification.
</bodyText>
<page confidence="0.982557">
95
</page>
<figure confidence="0.86579075">
Computational Linguistics Volume 25, Number 1
phrase
non-hd-ph hd-ph
hd-adj-ph hd-nexus-ph
hd-fill-ph hd-comp-ph hd-subj-ph hd-spr-ph
Figure 19
Hierarchy of phrases from Sag (1997).
SYNSEM CONT
L HD-DTR SYNSEM CONT
Figure 20
Default version of the semantics principle.
phrase
non-hd-ph hd-ph
hd-adj-ph hd-fill-ph hd-comp-ph hd-subj-ph hd-spr-ph
Figure 21
Simplification of hierarchy.
SYNSEM CONT
head-adjunct-phrase HD-DTR SYNSEM CONT
NON-HD-DTRS SYNSEM CONT
14+
</figure>
<figureCaption confidence="0.970628">
Figure 22
</figureCaption>
<bodyText confidence="0.980453388888889">
Inequalities overriding default equalities.
these coindexations are mutually compatible. Of course, the CONTENT values of the
head- and non-head- daughters should not be unified in any instantiation of this
schema, but since the range of values for each is indefinitely large, there is no way of
giving them mutually incompatible types. Thus the type head-nexus-phrase had to
be introduced, as a place to state a monotonic constraint on the relationship between
semantics values, but this type is otherwise unmotivated and somewhat unintuitive.
This sort of situation can be avoided by making use of inequalities, as defined
by Carpenter (1992). Intuitively, what is required in order to specify the constraint in
Figure 21 on headed-phrase is to say that the constraint on the schema head-adjunct-
phrase stipulates explicitly that its head-daughter content is not equal to the content
on the mother, as shown in Figure 22.
To achieve this formally takes only a very minor modification to the definitions
already given. First, one must change the definition of TFSs and tails, so that they
include inequalities. The relation gz+C Q x Q is added to the tuple that currently
defines TFSs (Definition 2), and a fifth condition is added to the four that are already
in that definition, which ensures that IA is a relation of the right sort (see Carpenter
[1992]):
</bodyText>
<listItem confidence="0.5100105">
• g4C Q x Q is an anti-reflexive and symmetric relation.
Atomic FSs in tails are extended, so that they include path inequalities (as well as
</listItem>
<page confidence="0.975429">
96
</page>
<note confidence="0.720649">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.9995">
the existing path:values and path equalities). The definition of TDFSs is the same as
before, except that it is now based on this new definition of TFSs and the new tails.
Second, the definition of subsumption changes as in Carpenter (1992). First, some
notation: it OF 7r&apos; means 5(r, it) gi-+ S(r,7&apos;), where r is the root node of the TFS.
</bodyText>
<subsectionHeader confidence="0.663274">
Definition 19: Inequated Subsumption
</subsectionHeader>
<bodyText confidence="0.678734">
F subsumes F&apos;, written F&apos; E F, if and only if:
</bodyText>
<listItem confidence="0.994069666666667">
• it =F it implies it =-F, 7r&apos;
• it OF 7r&apos; implies it Op 7r&apos;
• PF(7r) -= t implies Pp (7) =- t&apos; and t&apos; E t
</listItem>
<bodyText confidence="0.999989230769231">
The definitions of 17 and Li remain the same, save that the new notion of inequated
subsumption is used. The resulting operations are still well behaved, in that they are
order independent, and return a single result deterministically (see Carpenter [1992]).
The definitions of 17 and DefFS DefFill and BasicTDFS all remain the same, and the
lemmas and theorems given in Sections 3.7 still hold, with the proofs unchanged as
given in the appendix. These proofs still hold largely because they depend only on
the well-behaved nature of set union, n, and U.
Note that an inequality can arise in a derived TDFS (or its corresponding default
TFS) only if that inequality existed in one of the TDFSs (or tails) that were used to build
it via n . Inequalities not explicitly in this input never arise in the result. Consequently,
this corresponds to a relatively weak notion of negation. One might learn through n
or through 17 that two nodes cannot be equal because they have incompatible types,
but this does not mean that these nodes stand in the inequality relation defined by 54.
However, one can always convert a TFS into a unique most general fully inequated
TFS, as defined in Carpenter (1992) (where a fully inequated TFS is one where any two
incompatibly typed nodes in the TFS stand in the inequality relation defined by g4).
Thus, one can define a version of DefFS that always outputs a unique fully inequated
default TFS also. Furthermore, every TDFS has a unique most general fully inequated
TDFS: it amounts to the unique most general fully inequated TFS, plus the tail.
As far as we are aware, no other version of default unification has been specified
that allows for inequalities. In particular, PDU cannot be extended straightforwardly
to handle inequalities, because it is computed on a path-by-path basis. Consequently,
an attempt to PDU a TDFS with an indefeasible path equality and a TDFS with a
default inequality on the same paths results in an ill-formed TDFS. We think that the
fact that incorporating default inequalities is possible with such a small change to the
definition of YADU attests to its elegance.
</bodyText>
<subsectionHeader confidence="0.999933">
6.2 Specificity ordering
</subsectionHeader>
<bodyText confidence="0.9995369">
Note that although we have consistently used the type hierarchy to give a specificity
ordering to tail elements, the only real requirement to be able to define DefFS is that the
tail elements have specificity markers that are in a partial order. Hence the defaults that
&amp;quot;win&amp;quot; in a TDFS could be determined by an ordering other than the type hierarchy. In
fact, any partial order could be utilized: all that is necessary is to indicate the specificity
in the tails and to make the definition of the partition of tails sensitive to the relevant
partial order. Specifically, the second member of the pairs in the tails, which we have
defined as types, should be replaced with specificity information of the relevant sort,
and the specificity partition of a tail defined accordingly. The definitions of n and
DefFS then proceed as before.
</bodyText>
<page confidence="0.992149">
97
</page>
<figure confidence="0.905154666666667">
Computational Linguistics Volume 25, Number 1
Standard definition DefFS(BasicTDFS([F : T [F t )11: F u]/{}) = [ F ]
Fine-grained definition DefFS(BasicTDFS([F ], [F t])r-1 F u]/{}) = [F v]
</figure>
<figureCaption confidence="0.7135975">
Figure 23
Effect of fine-grained tails.
</figureCaption>
<bodyText confidence="0.99950575">
The fact that prioritization of defaults need not be linked to the type hierarchy
means that it is straightforward to adapt YADU to untyped feature structures or, in
general, to a system where some form of templatic inheritance is used instead of the
type hierarchy. It also might be useful if an ordering is given by some component ex-
trinsic to the FSs, such as an ordering based on probabilities. It would even be possible
to add a finer grain of specificity to the type hierarchy by creating a specificity order-
ing of tail elements within types, for instance so that for a type t, specificity markers
t1, t2,. t&amp;quot; were defined so that within-type priority followed numerical ordering. The
potential utility of this is shown by the example in Section 2, where the two types verb
and regverb were distinguished simply in order to acheive the correct prioritization.
An alternative would have been to use a single type verb with two distinct specificity
markers verbl and verb2 to get the desired priorities on the defaults.
</bodyText>
<subsectionHeader confidence="0.985068">
6.3 Fine-Grained Structures
</subsectionHeader>
<bodyText confidence="0.999987357142857">
One point that we glossed over slightly is the use of atomic FSs within a typed frame-
work (as opposed to the untyped FSs assumed in Carpenter [1993]). In the definition
for BasicTDFS given in Section 3, we assumed that if a path 7r had a value t, then there
would be one corresponding path-value atomic FS in the tail. But there is another
possibility, which is to have additional structures in the tail, corresponding to each
supertype of t: e.g., if w were a supertype of t, then there would also be an atomic
FS in the tail where the path 7r was associated with the value w. This would give
a finer-grained notion of maximal incorporation of information, since there might be
a situation where t was incompatible with a type u in the nondefault FS (or it was
incompatible with a more specific default FS) but where un w resulted in some more
specific type v, which would survive in the YADU result (see Figure 23).
To extend tails this way, one must change the definition of basic TDFSs, to remove
the condition (c) from the original definition, which ensured that only the most specific
information was included in the tail. So the new definition is:
</bodyText>
<subsectionHeader confidence="0.945449">
Definition 20: Fine-Grained Basic TDFSs
</subsectionHeader>
<bodyText confidence="0.998788">
Let I and ID be typed feature structures, where I is regarded as indefeasible and ID as
defeasible. Furthermore, suppose that I 11 ID 0 _L (so I and ID are compatible). Then
the fine-grained basic TDFS BasicTDFS(I, ID) of I and ID is the TDFS I / T, such that:
</bodyText>
<equation confidence="0.523819666666667">
T = {(F, t) : t is the root type on ID n I, and F is an atomic TFS such that:
(a) I F;
(b) ID 7 I L F}
</equation>
<bodyText confidence="0.882993">
The existing definitions of &lt;ri and DefFS will then provide the finer-grained notion
</bodyText>
<page confidence="0.956156">
98
</page>
<note confidence="0.74572">
Lascarides and Copestake Default Representation
</note>
<bodyText confidence="0.9929418">
of maximal incorporation of default information, from these fine-grained basic TDFSs.
Extending tails this way is useful for the treatment of lexical rules, as discussed
in Briscoe and Copestake (1995). However it has the obvious disadvantage of con-
siderably increasing the number of atomic FSs that must be considered, with adverse
effects on efficiency.
</bodyText>
<subsectionHeader confidence="0.997424">
6.4 Credulous YADU
</subsectionHeader>
<bodyText confidence="0.99859225">
Another way in which the definition could be varied would be to omit the general-
ization step from DefFS, which ensures that the default result of a TDFS is a single
TFS, and to have a credulous variant of DefFS instead, which would be analogous to
Carpenter&apos;s (1993) credulous asymmetric default unification:
</bodyText>
<equation confidence="0.417256666666667">
Definition 21: Credulous DefFS
Let F be a TDFS I/T. Then
DefFS (F) hcs Ofs(1-11)) 6cs • • • hcs Pfs(itn)
</equation>
<bodyText confidence="0.9991768">
where (p,1, , fin) is a specificity partition on T.
We argued in Section 1.1 that a unique result is preferable in order to avoid mul-
tiplication of disjunctive structures, but disjunctive results might be useful in cases
where there are multiple alternative structures (e.g., in modeling dreamed/ dreamt, see
Russell et al. [1993]).
</bodyText>
<subsectionHeader confidence="0.997242">
6.5 Asymmetric Default Unification
</subsectionHeader>
<bodyText confidence="0.999960294117647">
We should point out that although we believe order-independent default unification is
preferable to asymmetric default unification for many applications, there are situations
where the latter is required. YADU could not replace asymmetric default unification
in Grover et al.&apos;s (1994) treatment of ellipsis. It is also not directly suitable for en-
coding lexical rules: it is conventional to write lexical rules using a sort of default
notation that is intended to be interpreted as meaning that the output of the rule is
identical to the input except where otherwise specified, but formalizing this calls for
an asymmetric notion of default (see Briscoe and Copestake [1995]). Similarly, Copes-
take (1992) argues that it is useful to be able to encode irregular lexical entries as
inheriting by default from the output of lexical rule application (e.g., the entry for
children could inherit from the result of applying a lexical rule for plural formation
to the entry for child but override the orthography). This requires asymmetric default
unification, where the TFS that results from the application of the lexical rule is treated
as defeasible and the specification on the lexical entry is treated as hard information.
The current LKB implementation thus allows both types of default unification (which
is straightforward, since YADU is implemented using a series of asymmetric default
unification operations).
</bodyText>
<sectionHeader confidence="0.874759" genericHeader="method">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9999835">
We have argued that for default unification to achieve the combination of perspicuity
and declarativity familiar from normal unification, default unification should share
some of its properties—such as determinacy and order independence. At the same
time, default unification should respect the behavior of defaults, such as the overrid-
ing of default information by more specific conflicting defaults. We have also argued
here and elsewhere (Lascarides and Copestake, in press) that some linguistic phenom-
</bodyText>
<page confidence="0.992767">
99
</page>
<note confidence="0.705918">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.999849739130435">
ena suggest that there are conventional default constraints that persist beyond the
lexicon, and are potentially overridden by more open-ended reasoning with (default)
pragmatic knowledge in a discourse context. This requires a definition of default uni-
fication where the default results of unification are marked as default, and thus distin-
guished from the indefeasible results. We provided a definition of default unification
known as YADU, which intuitively models the incorporation of the maximal amount
of default information into the result, by adapting Carpenter&apos;s (1993) version of asym-
metric default unification to the situation where default and nondefault information
is distinguished in a single structure and defaults may have different priorities. Our
definition was formally proven to meet the above requirements. We suggested that
such a definition of default unification can improve the declarativity of existing uses
of default inheritance within the lexicon because it does not require one to pre-specify
the order in which information is to be accumulated.
Despite YADU&apos;s factorial worst-case complexity behavior, its use does not signifi-
cantly decrease overall system performance when compared to a monotonic encoding
for the examples we have tried in the LKB system. These results are preliminary and
obviously only true relative to our particular implementation and style of grammar
encoding, but they lead us to believe that the worst-case complexity behavior does not
preclude the use of YADU in typed feature structure implementations. Although we
only discussed a few examples in Section 4, we believe these illustrate the potential
utility of defaults in a range of different contexts within a grammar and lexicon. We
hope to report on a comparison between the monotonic and YADU versions of the
English Resource Grammar in a later paper.
</bodyText>
<sectionHeader confidence="0.964246" genericHeader="conclusions">
Appendix
</sectionHeader>
<subsectionHeader confidence="0.848736">
Proof of Lemma 1
</subsectionHeader>
<bodyText confidence="0.861019857142857">
First we prove that Bot12 = Bot21. Note that by order independence of n, 112
= 121. And
by order independence of set union T1 u T2 = T2 U T1. So
Bot12 —d,f { (F, t) E T1 U T2 such that /12 n F = I}
= {(F, t) c T2 U Tl such that 121 n F = _L}
=,,f Bo t21.
So:
</bodyText>
<equation confidence="0.983405333333333">
T12 _ Thief (T1 u 1 ,-,2) ,
\ B0112
= (T2 UTI) \ Bot21
_
df
T21
</equation>
<reference confidence="0.489600166666667">
Proof of Lemma 2 —df (T12 u T3) \ Bot(12)3
T(12)3 =def {(F, t) E T12 U T3 such that 1(12)3 n F = I}
Bot(12)3 = {(F, t) c T12 such that 1(12)3 HF = _Llu
f (F, t) E T3 such that 1(12)3 n F = 1_}
Let (F, t) E TC12)3. Then:
(a) (F, t) E T12 \ Bot(12)3; or
</reference>
<page confidence="0.810525">
100
</page>
<reference confidence="0.886029142857143">
Lascarides and Copestake Default Representation
(b) (F, t) E T3 \ Bot(12)3
Suppose (b). Then (F, t) c T3 and 1(12)3 n F 0 I.
Therefore, (F, t) E T2 U T3. Furthermore, by the definition of typed unification,
1(12)3 = 11(23) E /23. Therefore, since /1(23) [I F 1, 123 n F 0 I. SO (F, t) E T23. Further-
more, 11(23) n F = 1(12)3 n F 0 I. So (F, t) 0 Botl (23) , and therefore (F, t) E T1(23).
Now suppose (a) holds. Then (F, t) E ((T1UT2)\Bot12)\Bot(12)3 . But Bot12 C Bot(12)3.
So either:
(i) (F, t) E TI- \ Bot(12)3 ; or
(ii) (F, t) E T2 \ Bot(12)3
Suppose (i) holds. Then (F, t) E T1 and 1(12)317.F I. So 11(23) HF 0 I by the order
independence of typed unification. So (F, t) e Ti- \ Bot1(23) C T1(23).
Suppose (ii) holds. Then (F, t) E T2 and 1(12)3 HF o I. So 11(23) HF 0 I, and therefore
123 fl F 0 I. So (F, t) 0 Bot23 , and so (F, t) e T23. Furthermore, since /1(23) n F 0 1,
</reference>
<equation confidence="0.7293215">
(F, t) 0 Bot1(23) . And so (F, t) e T1(23).
So y(12)3 c T1(23).
By symmetry, T1(23)
Therefore T1(23) _ T123). 0
</equation>
<bodyText confidence="0.790020666666667">
Proof of Lemma 3
The indefeasible TFS of TDFS1 &lt;FITDFS2 is unique because n is deterministic. The tail
is unique because set union and 17 are deterministic. 0
</bodyText>
<subsectionHeader confidence="0.500886">
Proof of Lemma 4
</subsectionHeader>
<bodyText confidence="0.990550222222222">
The specificity partition of a tail is unique because the type hierarchy is a complete par-
tial order. Furthermore, n and LI are deterministic. Therefore the result of DefFS(TDFS)
is unique. 0
Proof of Lemma 5 &lt;, is the
Let us consider the n of the TDFS,s,F1 . and F2. The indefeasible part of Fi &lt;FTF2 , T21
same as the indefeasible part Of F2 n F1 because n is commutative. The tails T12
by lemma 1. So F12 = F21. SO n is commutative. 0
Proof of Lemma 6
One needs to prove:
</bodyText>
<equation confidence="0.9455365">
1(12)3 = 11(23)
T(12)3 = T(12)3
</equation>
<bodyText confidence="0.712455">
Case 1 follows immediately from the associativity of H. Case 2 holds by lemma 2. So
17 is associative. 0
</bodyText>
<subsectionHeader confidence="0.379436">
Proof of Theorem 1
</subsectionHeader>
<bodyText confidence="0.6013656">
Follows immediately from lemmas 3, 5 and 6. 0
Proof of Theorem 2
T12 _ -.-1
i U T2, because 112 n 0 (T1) o I, and 112 n 0 (T2) o I.
c T(12)3.
</bodyText>
<page confidence="0.943739">
101
</page>
<subsectionHeader confidence="0.180256">
Computational Linguistics Volume 25, Number 1
</subsectionHeader>
<bodyText confidence="0.663134857142857">
Let iti ... nbe a Specificity Partition Tail of r12. Then by the definition of DefFS:
D12 =- L(1.12 rics Pfs(itt) I51cs • • • hcs Kafs(Pn))
But by assumption 112 17 Pfs(tti) I. So by the definition of hcs:
112 hcs Pfs(tii) = 112 ri Rs (ttl) -I-
Similarly (112 17 pfs (iti)) 11 Rs (R2) I. So by the definition of hcs:
((112 6. pfs(mi)) 6cs 80/5(1-t2) = 112 n Pfs(itt) 17 Pfs(ii2)
By similar arguments for /13, . • • , itn:
</bodyText>
<equation confidence="0.938673888888889">
D12 = LI ( (112 hcs Pfs(iii)) • • • hcs fs(un))
= u(112 n pfs(f-ti) n . . . n ofs(itn))
= 112 n pfs(T12)
= h n Ofs(T1) n 1.2 11 pfs(T2)
By a similar argument to that above:
D1 = h n pi CP)
D2 = 12 11 pi (T2)
So
D12 = D1 n D2
</equation>
<bodyText confidence="0.981293">
as required. 0
</bodyText>
<subsectionHeader confidence="0.640241">
Proof of Lemma 7
</subsectionHeader>
<bodyText confidence="0.967408">
For any basic TDFS IIT:
</bodyText>
<listItem confidence="0.950577666666667">
1. T is its own specificity partition;
2. Flpfs(T) _L; and
3. VF E fr(T), 1HF4 I.
</listItem>
<bodyText confidence="0.845938">
So by the definitions of u and hcs:
</bodyText>
<equation confidence="0.916985">
DefFS(I/T) = u(I hcs Pfs(T)) = in N(T)
</equation>
<bodyText confidence="0.9133985">
Thus we need to prove that
/ n ID = 1 n pfs(T)
But this follows immediately by the definition of T for BasicTDFS(I, ID) (T is the set of
atomic TFSs that are subsumed by I n ID but not subsumed by I). 0
</bodyText>
<page confidence="0.991082">
102
</page>
<figure confidence="0.105072375">
Lascarides and Copestake Default Representation
Proof of Corollary 1
Similarly to the proof given in the above lemma: Di = Iflofs(Ti) for i = 1,2. So, since
D1 n D2 0 I:
n of, (T1) n 1211 pfs(T2) = 112 n of-s(T1) n ofs(T2)
So by theorem 2:
D12 = pl n D2
= El ID, n 1.2 F-11D2
</figure>
<sectionHeader confidence="0.964047" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.736985857142857">
This work is sponsored by a grant funded
by ESRC UK (grant number R000236052),
and by the ESRC-funded Human
Communication Research Centre, University
of Edinburgh. This material is also in part
based upon work supported by the ESPRIT
Acquilex-II, project BR-7315, grant to
Cambridge University and National Science
Foundation under grant number
IRI-9612682, to Stanford University. We
would like to thank Ted Briscoe, Dan
Flickinger, Ivan Sag, Hidetoshi Sirai, and
three anonymous reviewers for helpful
comments on previous drafts.
</bodyText>
<sectionHeader confidence="0.825457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944246376812">
Alshawi, Hiyan, Doug J. Arnold, Rolf
Backofen, David M. Carter, Jeremy
Lindop, Klaus Netter, Stephen G. Pulman,
Junichi Tsujii, Hans Uszkoreit. 1991.
Eurotra ET6/1: Rule Formalism And
Virtual Machine Design Study (final
report). CEC, Luxembourg.
Asher, Nicholas and Michael Morreau. 1991.
Common sense entailment: A modal
theory of nonmonotonic reasoning.
Proceedings of the 12th International Joint
Conference on Artificial Intelligence
(IJCAI-91), Sydney, Australia.
van den Berg, Martin and Hub Priist. 1991.
Common denominators and default
unification. Proceedings of the First Meeting,
Computational Linguistics in the Netherlands
(CLIN-91), pages 1-16, Utrecht.
Boguraev, Bran and James Pustejovsky.
1990. Lexical ambiguity and the role of
knowledge representation in lexicon
design. Proceedings of the 13th International
Conference on Computational Linguistics
(COLING-90), pages 36-42, Helsinki.
Bouma, Gosse. 1990. Defaults in unification
grammar. Proceedings of the 28th Annual
Meeting, pages 165-173, Pittsburgh.
Association for Computational
Linguistics.
Bouma, Gosse. 1992. Feature structures and
nonmonotonicity. Computational
Linguistics, 18(2): 183-204.
Brewka, Gerhard. 1991. Cumulative default
logic: In defense of nonmonotonic
inference rules. Artificial Intelligence, 50(2):
183-205.
Briscoe, Edward J., Ann Copestake, and
Bran Boguraev. 1990. Enjoy the paper:
Lexical semantics via lexicology.
Proceedings of the 13th International
Conference on Computational Linguistics
(COLING-90), pages 42-47, Helsinki.
Briscoe, Edward J. and Ann Copestake.
1995. Dative constructions as lexical rules
in the TDFS framework. Acquilex-II
Working Papers 78, University of
Cambridge Computer Laboratory,
Cambridge, England.
Calder, Jo. 1991. Some notes on priority
union. Paper presented at the ACQUILEX
Workshop on Default Inheritance in the
Lexicon, Cambridge, England.
Carpenter, Bob. 1992. The Logic of Typed
Feature Structures. Cambridge University
Press, Cambridge, England.
Carpenter, Bob. 1993. Skeptical and
credulous default unification with
application to templates and inheritance.
In Edward J. Briscoe, Ann Copestake, and
Valeria de Paiva, editors, Inheritance,
Defaults and the Lexicon. Cambridge
University Press, Cambridge, England,
pages 13-37.
Copestake, Ann. 1992. The Representation of
Lexical Semantic Information. D.Phil.
dissertation, University of Sussex,
Brighton, England. Cognitive Science
Research Paper CSRP 280.
Copestake, Ann. 1993. Defaults in lexical
</reference>
<page confidence="0.97482">
103
</page>
<note confidence="0.365545">
Computational Linguistics Volume 25, Number 1
</note>
<reference confidence="0.999946139344262">
representation. In Edward J. Briscoe, Ann
Copestake, and Valeria de Paiva, editors,
Inheritance, Defaults and the Lexicon.
Cambridge University Press, Cambridge,
England, pages 223-245.
Copestake, Ann, Daniel Flickinger, Rob
Malouf, Susanne Riehemann, and Ivan
Sag. 1995. Translation using minimal
recursion semantics. Proceedings of the 6th
International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-95), pages 15-32, Leuven, Belgium.
Daelemans, Walter. 1987. A tool for the
automatic creation, extension and
updating of lexical knowledge bases.
Proceedings of the 3rd Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-87),
pages 70-74. Copenhagen.
Daelemans, Walter, Koenraad de Smedt,
and Gerald Gazdar. 1992. Inheritance in
natural language processing.
Computational Linguistics, 18(2): 205-218.
Done, Jochen and Andreas Eisele. 1991. A
comprehensive unification-based
grammar formalism. DYANA Technical
Report, University of Edinburgh,
Scotland.
Emele, Martin and Remi Zajac. 1990. Typed
unification grammars. Proceedings of the
13th International Conference on
Computational Linguistics (COLING-90),
pages 293-298, Helsinki.
Evans, Roger and Gerald Gazdar. 1989a.
Inference in DATR. Proceedings of the 4th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-89), pages 66-71, Manchester,
England.
Evans, Roger and Gerald Gazdar. 1989b.
The Semantics of DATR. In Anthony
G. Cohn, editor, Proceedings of the Seventh
Conference of the Society for the Study of
Artificial Intelligence and Simulation of
Behavior (AISB-89). Pitman/Morgan
Kaufmann, London, pages 79-87.
Evans, Roger and Gerald Gazdar. 1996.
DATR: A language for lexical knowledge
representation. Computational Linguistics,
22(2): 167-216.
Fillmore, Charles J. 1986. Pragmatically
controlled zero anaphora. BLS, 12: 95-107.
Flickinger, Daniel. 1987. Lexical Rules in the
Hierarchical Lexicon. Ph.D. dissertation,
Stanford University, Stanford, CA.
Flickinger, Daniel and John Nerbonne. 1992.
Inheritance and complementation: A Case
study of easy adjectives and related nouns.
Computational Linguistics, 18(3): 269-310.
Flickinger, Daniel, Carl Pollard, and Tom
Wasow. 1985. Structure sharing in lexical
representation. Proceedings of the 23rd
Annual Meeting, pages 262-268, Chicago.
Association for Computational
Linguistics.
Flickinger, Daniel, Ivan Sag, and Ann
Copestake. (In preparation). A grammar
of English in HPSG: Design and
implementation. CSLI Publications,
Stanford, CA.
Gazdar, Gerald. 1987. Linguistic
applications of default inheritance
mechanisms. In Peter Whitelock, Harold
Somers, Paul Bennett, Rod Johnson, and
Mary McGee Wood, editors, Linguistic
Theory and Computer Applications.
Academic Press, London, pages 37-68.
Gerdemann, Dale and Paul King. 1994. The
correct and efficient implementation of
appropriateness specifications for typed
feature structures. Proceedings of the 15th
International Conference on Computational
Linguistics (COLING-94), Kyoto, Japan.
Grover, Claire, Chris Brew, Suresh
Manandhar, and Marc Moens. 1994.
Priority union and generalization in
discourse grammars. Proceedings of the
32nd Annual Meeting, pages 17-24,
Association for Computational
Linguistics. Las Cruces.
Kaplan, Ronald. 1987. Three seductions of
computational psycholinguistics. In Peter
Whitelock, Harold Somers, Paul Bennett,
Rod Johnson, and Mary McGee Wood,
editors, Linguistic Theory and Computer
Applications. Academic Press, London,
pages 149-88.
Kilgarriff, Adam. 1993. Inheriting verb
alternations. Proceedings of the 6th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-93), pages 213-221, Utrecht, The
Netherlands.
Konolige, Kurt. 1988. Hierarchic
autoepistemic theories for nonmonotonic
reasoning: Preliminary report. Technical
Note No. 446, SRI International, Menlo
Park, CA.
Krieger, Hans-Ulrich and John Nerbonne.
1993. Feature-based inheritance networks
for computational lexicons. In Edward J.
Briscoe, Ann Copestake, and Valeria
de Paiva, editors, Inheritance, Defaults and
the Lexicon. Cambridge University Press,
Cambridge, England, pages 90-136.
Krieger, Hans-Ulrich, and Ulrich Schafer.
1994. TDL-A type description language
for HPSG. DFKI, Saarbriicken, Germany.
Krifka, Manfred. 1987. Nominal reference
and temporal constitution: Towards a
semantics of quantity. Proceedings of the 6th
Amsterdam Colloquium, pages 153-173,
</reference>
<page confidence="0.998786">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965930">
<title confidence="0.999347">Default Representation in Constraint-based Frameworks</title>
<author confidence="0.999771">Alex Lascarides Ann Copestaket</author>
<affiliation confidence="0.999898">University of Edinburgh CSLI, Stanford University</affiliation>
<abstract confidence="0.9966351">Default unification has been used in several linguistic applications. Most of them have utilized defaults at a metalevel, as part of an extended description language. We propose that allowing default unification to be a fully integrated part of a typed feature structure system requires default unification to be a binary, order independent function, so that it acquires the perspicuity and declarativity familiar from normal unification-based frameworks. Furthermore, in order to respect the behavior of defaults, default unification should allow default reentrancies and values on more general types to be overridden by conflicting default information on more specific types. We define what we believe is the first version of default unification to fully satisfy these criteria, and argue that it can improve the representation of a range of phenomena in syntax, semantics and the lexico-pragmatic interface.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proof of Lemma 2 T(12)3 Bot(12)3 —df =def = (T12 u T3) \ Bot(12)3 {(F, t) E T12 U T3 such that 1(12)3 n F = I} {(F, t) c T12 such that 1(12)3 HF = _Llu f (F, t) E T3 such that 1(12)3 n F = 1_</booktitle>
<marker></marker>
<rawString>Proof of Lemma 2 T(12)3 Bot(12)3 —df =def = (T12 u T3) \ Bot(12)3 {(F, t) E T12 U T3 such that 1(12)3 n F = I} {(F, t) c T12 such that 1(12)3 HF = _Llu f (F, t) E T3 such that 1(12)3 n F = 1_}</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>