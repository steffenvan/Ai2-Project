<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013019">
<title confidence="0.993795">
Resource Analysis for Question Answering
</title>
<author confidence="0.998766">
Lucian Vlad Lita Warren A. Hunt Eric Nyberg
</author>
<affiliation confidence="0.998356">
Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University
</affiliation>
<email confidence="0.99806">
llita@cs.cmu.edu whunt@andrew.cmu.edu ehn@cs.cmu.edu
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999674833333333">
This paper attempts to analyze and bound the utility
of various structured and unstructured resources in
Question Answering, independent of a specific sys-
tem or component. We quantify the degree to which
gazetteers, web resources, encyclopedia, web doc-
uments and web-based query expansion can help
Question Answering in general and specific ques-
tion types in particular. Depending on which re-
sources are used, the QA task may shift from com-
plex answer-finding mechanisms to simpler data ex-
traction methods followed by answer re-mapping in
local documents.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984523809524">
During recent years the Question Answering (QA)
field has undergone considerable changes: question
types have diversified, question complexity has in-
creased, and evaluations have become more stan-
dardized - as reflected by the TREC QA track
(Voorhees, 2003). Some recent approaches have
tapped into external data sources such as the Web,
encyclopedias, databases in order to find answer
candidates, which may then be located in the spe-
cific corpus being searched (Dumais et al., 2002; Xu
et al., 2003). As systems improve, the availability
of rich resources will be increasingly critical to QA
performance. While on-line resources such as the
Web, WordNet, gazetteers, and encyclopedias are
becoming more prevalent, no system-independent
study has quantified their impact on the QA task.
This paper focuses on several resources and their
inherent potential to provide answers, without con-
centrating on a particular QA system or component.
The goal is to quantify and bound the potential im-
pact of these resources on the QA process.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999934">
More and more QA systems are using the Web as
a resource. Since the Web is orders of magni-
tude larger than local corpora, redundancy of an-
swers and supporting passages allows systems to
produce more correct, confident answers (Clarke et
al., 2001; Dumais et al., 2002). (Lin, 2002) presents
two different approaches to using the Web: access-
ing the structured data and mining the unstructured
data. Due to their complementary nature of these
approaches, hybrid systems are likely to perform
better (Lin and Katz, 2003).
Definitional questions (“What is X?”, “Who
is X?”) are especially compatible with structured
resources such as gazetteers and encyclopedias.
The top performing definitional systems (Xu et
al., 2003) at TREC extract kernel facts similar to
a question profile built using structured and semi-
structured resources: WordNet (Miller et al., 1990),
Merriam-Webster dictionary www.m-w.com), the
Columbia Encyclopedia (www.bartleby.com),
Wikipedia (www.wikipedia.com), a biog-
raphy dictionary (www.s9.com) and Google
(www.google.com).
</bodyText>
<sectionHeader confidence="0.995917" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999560533333333">
For the purpose of this paper, resources consist of
structured and semi-structured knowledge, such as
the Web, web search engines, gazetteers, and ency-
clopedias. Although many QA systems incorporate
or access such resources, few systems quantify in-
dividual resource impact on their performance and
little work has been done to estimate bounds on re-
source impact to Question Answering. Independent
of a specific QA system, we quantify the degree to
which these resources are able to directly provide
answers to questions.
Experiments are performed on the 2,393 ques-
tions and the corresponding answer keys provided
through NIST (Voorhees, 2003) as part of the TREC
8 through TREC 12 evaluations.
</bodyText>
<sectionHeader confidence="0.997741" genericHeader="method">
4 Gazetteers
</sectionHeader>
<bodyText confidence="0.999254476190476">
Although the Web consists of mostly unstructured
and loosely structured information, the available
structured data is a valuable resource for question
answering. Gazetteers in particular cover several
frequently-asked factoid question types, such as
”What is the population ofX?” or ”What is the cap-
ital of Y?”. The CIA World Factbook is a database
containing geographical, political, and economi-
cal profiles of all the countries in the world. We
also analyzed two additional data sources contain-
ing astronomy information (www.astronomy.com)
and detailed information about the fifty US states
(www.50states.com).
Since gazetteers provide up-to-date information,
some answers will differ from answers in local
corpora or the Web. Moreover, questions requir-
ing interval-type answers (e.g. “How close is the
sun?”) may not match answers from different
sources which are also correct. Gazetteers offer
high precision answers, but have limited recall since
they only cover a limited number of questions (See
</bodyText>
<tableCaption confidence="0.792895">
Table 1).
</tableCaption>
<table confidence="0.999297625">
CIA All
Q-Set #qtions R P R P
TREC8 200 4 100% 6 100%
TREC9 693 8 100% 22 79%
TREC10 500 14 100% 23 96%
TREC11 500 8 100% 20 100%
TREC12 500 2 100% 11 92%
Overall 2393 36 100% 82 91%
</table>
<tableCaption confidence="0.996377">
Table 1: Recall (R): TREC questions can be directly
</tableCaption>
<bodyText confidence="0.992190666666667">
answered directly by gazetteers - shown are results
for CIA Factbook and All gazetteers combined. Our
extractor precision is Precision (P).
</bodyText>
<sectionHeader confidence="0.995486" genericHeader="method">
5 WordNet
</sectionHeader>
<bodyText confidence="0.999046">
Wordnets and ontologies are very common re-
sources and are employed in a wide variety of di-
rect and indirect QA tasks, such as reasoning based
on axioms extracted from WordNet (Moldovan et
al., 2003), probabilistic inference using lexical rela-
tions for passage scoring (Paranjpe et al., 2003), and
answer filtering via WordNet constraints (Leidner et
al., 2003).
</bodyText>
<table confidence="0.892407571428572">
Q-Set #qtions All Gloss Syns Hyper
TREC 8 200 32 22 7 13
TREC 9 693 197 140 73 75
TREC 10 500 206 148 82 88
TREC 11 500 112 80 29 46
TREC 12 500 93 56 10 52
Overall 2393 641 446 201 268
</table>
<tableCaption confidence="0.802754166666667">
Table 2: Number of questions answerable using
WordNet glosses (Gloss), synonyms (Syns), hyper-
nyms and hyponyms (Hyper), and all of them com-
bined All.
Table 2 shows an upper bound on how many
TREC questions could be answered directly using
</tableCaption>
<bodyText confidence="0.999963285714286">
WordNet as an answer source. Question terms and
phrases were extracted and looked up in WordNet
glosses, synonyms, hypernyms, and hyponyms. If
the answer key matched the relevant WordNet data,
then an answer was considered to be found. Since
some answers might occur coincidentally, we these
results to represent upper bounds on possible utility.
</bodyText>
<sectionHeader confidence="0.957065" genericHeader="method">
6 Structured Data Sources
</sectionHeader>
<bodyText confidence="0.99950164">
Encyclopedias, dictionaries, and other web
databases are structured data sources that are often
employed in answering definitional questions (e.g.,
“What is X?”, “Who is X?”). The top-performing
definitional systems at TREC (Xu et al., 2003)
extract kernel facts similar question profiles built
using structured and semi-structured resources:
WordNet (Miller et al., 1990), the Merriam-
Webster dictionary www.m-w.com), the Columbia
Encyclopedia (www.bartleby.com), Wikipedia
(www.wikipedia.com), a biography dictionary
(www.s9.com) and Google (www.google.com).
Table 3 shows a number of data sources and
their impact on answering TREC questions. N-
grams were extracted from each question and run
through Wikipedia and Google’s define operator
(which searches specialized dictionaries, definition
lists, glossaries, abbreviation lists etc). Table 3
show that TREC 10 and 11 questions benefit the
most from the use of an encyclopedia, since they
include many definitional questions. On the other
hand, since TREC 12 has fewer definitional ques-
tions and more procedural questions, it does not
benefit as much from Wikipedia or Google’s define
operator.
</bodyText>
<table confidence="0.999741">
Q-Set #qtions WikiAll Wiki1st DefOp
TREC 8 200 56 5 30
TREC 9 693 297 49 71
TREC 10 500 225 45 34
TREC 11 500 155 19 23
TREC 12 500 124 12 27
Overall 2393 857 130 185
</table>
<tableCaption confidence="0.996283">
Table 3: The answer is found in a definition ex-
</tableCaption>
<bodyText confidence="0.828761666666667">
tracted from Wikipedia WikiAll, in the first defi-
nition extracted from Wikipedia Wiki1st, through
Google’s define operator DefOp.
</bodyText>
<sectionHeader confidence="0.951038" genericHeader="method">
7 Answer Type Coverage
</sectionHeader>
<bodyText confidence="0.997884083333333">
To test coverage of different answer types, we em-
ployed the top level of the answer type hierarchy
used by the JAVELIN system (Nyberg et al., 2003).
The most frequent types are: definition (e.g. “What
is viscosity?”), person-bio (e.g. “Who was La-
can?”), object(e.g. “Name the highest mountain.”),
process (e.g. “How did Cleopatra die?”), lexicon
(“What does CBS stand for?”)temporal(e.g. “When
is the first day of summer?”), numeric (e.g. “How
tall is Mount Everest?”), location (e.g. “Where is
Tokyo?”), and proper-name (e.g. “Who owns the
Raiders?”).
</bodyText>
<table confidence="0.9997784">
AType #qtions WikiAll DefOp Gaz WN
object 1003 426 92 58 309
lexicon 50 25 3 0 26
defn 178 105 9 11 112
pers-bio 39 15 11 0 17
process 138 23 6 9 16
temporal 194 63 14 0 50
numeric 121 27 13 10 18
location 151 69 21 2 47
proper 231 76 10 0 32
</table>
<tableCaption confidence="0.860292666666667">
Table 4: Coverage of TREC questions divided by
most common answer types.
Table 4 shows TREC question coverage broken
</tableCaption>
<bodyText confidence="0.967709045454546">
down by answer type. Due to temporal consistency,
numeric questions are not covered very well. Al-
though the process and object types are broad an-
swer types, the coverage is still reasonably good.
As expected, the definition and person-bio answer
types are covered well by these resources.
8 The Web as a Resource
An increasing number of QA systems are using the
web as a resource. Since the Web is orders of mag-
nitude larger than local corpora, answers occur fre-
quently in simple contexts, which is more conducive
to retrieval and extraction of correct, confident an-
swers (Clarke et al., 2001; Dumais et al., 2002;
Lin and Katz, 2003). The web has been employed
for pattern acquisition (Ravichandran et al., 2003),
document retrieval (Dumais et al., 2002), query ex-
pansion (Yang et al., 2003), structured information
extraction, and answer validation (Magnini et al.,
2002) . Some of these approaches enhance exist-
ing QA systems, while others simplify the question
answering task, allowing a less complex approach
to find correct answers.
</bodyText>
<subsectionHeader confidence="0.99567">
8.1 Web Documents
</subsectionHeader>
<bodyText confidence="0.999375">
Instead of searching a local corpus, some QA sys-
tems retrieve relevant documents from the web (Xu
et al., 2003). Since the density of relevant web doc-
uments can be higher than the density of relevant
local documents, answer extraction may be more
successful from the web. For a TREC evaluation,
answers found on the web must also be mapped to
relevant documents in the local corpus.
</bodyText>
<figure confidence="0.9892385">
Web Retrieval Performance For QA
1000
Correct Doc Density
First Correct Doc
800
700
600
500
400
300
200
100
00 10 20 30 40 50 60 70 80 90 100
document rank
</figure>
<figureCaption confidence="0.9795785">
Figure 1: Web retrieval: relevant document density
and rank of first relevant document.
</figureCaption>
<bodyText confidence="0.999891954545454">
In order to evaluate the impact of web docu-
ments on TREC questions, we performed an ex-
periment where simple queries were submitted to
a web search engine. The questions were to-
kenized and filtered using a standard stop word
list. The resulting keyword queries were used to
retrieve 100 documents through the Google API
(www.google.com/api). Documents containing the
full question, question number, references to TREC,
NIST, AQUAINT, Question Answering and similar
content were filtered out.
Figure 1 shows the density of documents contain-
ing a correct answer, as well as the rank of the first
document containing a correct answer. The sim-
ple word query retrieves a relevant document for
almost half of the questions. Note that for most
systems, the retrieval performance should be supe-
rior since queries are usually more refined and addi-
tional query expansion is performed. However, this
experiment provides an intuition and a very good
lower bound on the precision and density of current
web documents for the TREC QA task.
</bodyText>
<subsectionHeader confidence="0.999128">
8.2 Web-Based Query Expansion
</subsectionHeader>
<bodyText confidence="0.999921076923077">
Several QA systems participating at TREC have
used search engines for query expansion (Yang et
al., 2003). The basic query expansion method
utilizes pseudo-relevance feedback (PRF) (Xu and
Croft, 1996). Content words are selected from ques-
tions and submitted as queries to a search engine.
The top n retrieved documents are selected, and k
terms or phrases are extracted according to an op-
timization criterion (e.g. term frequency, n-gram
frequency, average mutual information using cor-
pus statistics, etc). These k items are used in the
expanded query.
We experimented by using the top 5, 10, 15, 20,
</bodyText>
<figure confidence="0.934217666666667">
# questions
900
Answer frequency using PRF
</figure>
<figureCaption confidence="0.926112">
Figure 2: Finding a correct answer in PRF expan-
</figureCaption>
<bodyText confidence="0.933893923076923">
sion terms - applied to 2183 questions for witch an-
swer keys exist.
50, and 100 documents retrieved via the Google API
for each question, and extracted the most frequent
fifty n-grams (up to trigrams). The goal was to de-
termine the quality of query expansion as measured
by the density of correct answers already present
in the expansion terms. Even without filtering n-
grams matching the expected answer type, simple
PRF produces the correct answer in the top n-grams
for more than half the questions. The best correct
answer density is achieved using PRF with only 20
web documents.
</bodyText>
<subsectionHeader confidence="0.92649">
8.3 Conclusions
</subsectionHeader>
<bodyText confidence="0.9999323125">
This paper quantifies the utility of well-known and
widely-used resources such as WordNet, encyclope-
dias, gazetteers and the Web on question answering.
The experiments presented in this paper represent
loose bounds on the direct use of these resources in
answering TREC questions. We reported the perfor-
mance of these resources on different TREC collec-
tions and on different question types. We also quan-
tified web retrieval performance, and confirmed that
the web contains a consistently high density of rel-
evant documents containing correct answers even
when simple queries are used. The paper also
shows that pseudo-relevance feedback alone using
web documents for query expansions can produce
a correct answer for fifty percent of the questions
examined.
</bodyText>
<sectionHeader confidence="0.996509" genericHeader="conclusions">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.7251905">
This work was supported in part by the Advanced
Research and Development Activity (ARDA)’s
Advanced Question Answering for Intelligence
(AQUAINT) Program.
</bodyText>
<sectionHeader confidence="0.987949" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992728255319149">
C.L.A. Clarke, G.V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. SIGIR.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002. Web question answering: Is more always
better? SIGIR.
J. Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark,
C. Bannard, B. Webber, and M. Steedman. 2003.
Qed: The edinburgh trec-2003 question answer-
ing system. TREC.
J. Lin and B. Katz. 2003. Question answering from
the web using knowledge annotation and knowl-
edge mining techniques. CIKM.
J. Lin. 2002. The web as a resource for question
answering: Perspectives and challenges. LREC.
B. Magnini, M. Negri, R. Pervete, and H. Tanev.
2002. Is it the right answer? exploiting web re-
dundancy for answer validation. ACL.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1990. Five papers on wordnet. In-
ternational Journal ofLexicography.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. ACL.
E. Nyberg, T. Mitamura, J. Callan, J. Carbonell,
R. Frederking, K. Collins-Thompson, L. Hiyaku-
moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,
A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and
B. Vand Durme. 2003. A multi strategy approach
with dynamic planning. TREC.
D. Paranjpe, G. Ramakrishnan, and S. Srinivasan.
2003. Passage scoring for question answering via
bayesian inference on lexical relations. TREC.
D. Ravichandran, A. Ittycheriah, and S. Roukos.
2003. Automatic derivation of surface text pat-
terns for a maximum entropy based question an-
swering system. HLT-NAACL.
E.M. Voorhees. 2003. Overview of the trec 2003
question answering track. TREC.
J. Xu and W.B. Croft. 1996. Query expansion using
local and global analysis. SIGIR.
J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec
2003 qa at bbn: Answering definitional ques-
tions. TREC.
H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003.
Structured use of external knowledge for event-
based open domain question answering. SIGIR.
</reference>
<figure confidence="0.99579">
# PRF terms
# questions
1100
1000
900
800
700
600
200
100
0 5 10 15 20 25 30 35 40 45 50
Top 5 documents
Top 10 documents
Top 15 documents
Top 20 documents
Top 50 documents
Top 100 documents
500
400
300
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975433">
<title confidence="0.999973">Resource Analysis for Question Answering</title>
<author confidence="0.999962">Lucian Vlad Lita Warren A Hunt Eric Nyberg</author>
<affiliation confidence="0.999644">Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University</affiliation>
<email confidence="0.999238">llita@cs.cmu.eduwhunt@andrew.cmu.eduehn@cs.cmu.edu</email>
<abstract confidence="0.998182076923077">This paper attempts to analyze and bound the utility of various structured and unstructured resources in Question Answering, independent of a specific system or component. We quantify the degree to which gazetteers, web resources, encyclopedia, web documents and web-based query expansion can help Question Answering in general and specific question types in particular. Depending on which resources are used, the QA task may shift from complex answer-finding mechanisms to simpler data extraction methods followed by answer re-mapping in local documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C L A Clarke</author>
<author>G V Cormack</author>
<author>T R Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<publisher>SIGIR.</publisher>
<contexts>
<context position="2091" citStr="Clarke et al., 2001" startWordPosition="317" endWordPosition="320">ncyclopedias are becoming more prevalent, no system-independent study has quantified their impact on the QA task. This paper focuses on several resources and their inherent potential to provide answers, without concentrating on a particular QA system or component. The goal is to quantify and bound the potential impact of these resources on the QA process. 2 Related Work More and more QA systems are using the Web as a resource. Since the Web is orders of magnitude larger than local corpora, redundancy of answers and supporting passages allows systems to produce more correct, confident answers (Clarke et al., 2001; Dumais et al., 2002). (Lin, 2002) presents two different approaches to using the Web: accessing the structured data and mining the unstructured data. Due to their complementary nature of these approaches, hybrid systems are likely to perform better (Lin and Katz, 2003). Definitional questions (“What is X?”, “Who is X?”) are especially compatible with structured resources such as gazetteers and encyclopedias. The top performing definitional systems (Xu et al., 2003) at TREC extract kernel facts similar to a question profile built using structured and semistructured resources: WordNet (Miller </context>
<context position="9185" citStr="Clarke et al., 2001" startWordPosition="1469" endWordPosition="1472">C question coverage broken down by answer type. Due to temporal consistency, numeric questions are not covered very well. Although the process and object types are broad answer types, the coverage is still reasonably good. As expected, the definition and person-bio answer types are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of r</context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>C.L.A. Clarke, G.V. Cormack, and T.R. Lynam. 2001. Exploiting redundancy in question answering. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>M Banko</author>
<author>E Brill</author>
<author>J Lin</author>
<author>A Ng</author>
</authors>
<title>Web question answering: Is more always better?</title>
<date>2002</date>
<publisher>SIGIR.</publisher>
<contexts>
<context position="1281" citStr="Dumais et al., 2002" startWordPosition="185" endWordPosition="188">om complex answer-finding mechanisms to simpler data extraction methods followed by answer re-mapping in local documents. 1 Introduction During recent years the Question Answering (QA) field has undergone considerable changes: question types have diversified, question complexity has increased, and evaluations have become more standardized - as reflected by the TREC QA track (Voorhees, 2003). Some recent approaches have tapped into external data sources such as the Web, encyclopedias, databases in order to find answer candidates, which may then be located in the specific corpus being searched (Dumais et al., 2002; Xu et al., 2003). As systems improve, the availability of rich resources will be increasingly critical to QA performance. While on-line resources such as the Web, WordNet, gazetteers, and encyclopedias are becoming more prevalent, no system-independent study has quantified their impact on the QA task. This paper focuses on several resources and their inherent potential to provide answers, without concentrating on a particular QA system or component. The goal is to quantify and bound the potential impact of these resources on the QA process. 2 Related Work More and more QA systems are using t</context>
<context position="9206" citStr="Dumais et al., 2002" startWordPosition="1473" endWordPosition="1476">roken down by answer type. Due to temporal consistency, numeric questions are not covered very well. Although the process and object types are broad answer types, the coverage is still reasonably good. As expected, the definition and person-bio answer types are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents</context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering: Is more always better? SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Leidner</author>
<author>J Bos</author>
<author>T Dalmas</author>
<author>J Curran</author>
<author>S Clark</author>
<author>C Bannard</author>
<author>B Webber</author>
<author>M Steedman</author>
</authors>
<title>Qed: The edinburgh trec-2003 question answering system.</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="5365" citStr="Leidner et al., 2003" startWordPosition="823" endWordPosition="826"> 100% TREC12 500 2 100% 11 92% Overall 2393 36 100% 82 91% Table 1: Recall (R): TREC questions can be directly answered directly by gazetteers - shown are results for CIA Factbook and All gazetteers combined. Our extractor precision is Precision (P). 5 WordNet Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovan et al., 2003), probabilistic inference using lexical relations for passage scoring (Paranjpe et al., 2003), and answer filtering via WordNet constraints (Leidner et al., 2003). Q-Set #qtions All Gloss Syns Hyper TREC 8 200 32 22 7 13 TREC 9 693 197 140 73 75 TREC 10 500 206 148 82 88 TREC 11 500 112 80 29 46 TREC 12 500 93 56 10 52 Overall 2393 641 446 201 268 Table 2: Number of questions answerable using WordNet glosses (Gloss), synonyms (Syns), hypernyms and hyponyms (Hyper), and all of them combined All. Table 2 shows an upper bound on how many TREC questions could be answered directly using WordNet as an answer source. Question terms and phrases were extracted and looked up in WordNet glosses, synonyms, hypernyms, and hyponyms. If the answer key matched the rel</context>
</contexts>
<marker>Leidner, Bos, Dalmas, Curran, Clark, Bannard, Webber, Steedman, 2003</marker>
<rawString>J. Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark, C. Bannard, B. Webber, and M. Steedman. 2003. Qed: The edinburgh trec-2003 question answering system. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>B Katz</author>
</authors>
<title>Question answering from the web using knowledge annotation and knowledge mining techniques.</title>
<date>2003</date>
<publisher>CIKM.</publisher>
<contexts>
<context position="2362" citStr="Lin and Katz, 2003" startWordPosition="360" endWordPosition="363">al is to quantify and bound the potential impact of these resources on the QA process. 2 Related Work More and more QA systems are using the Web as a resource. Since the Web is orders of magnitude larger than local corpora, redundancy of answers and supporting passages allows systems to produce more correct, confident answers (Clarke et al., 2001; Dumais et al., 2002). (Lin, 2002) presents two different approaches to using the Web: accessing the structured data and mining the unstructured data. Due to their complementary nature of these approaches, hybrid systems are likely to perform better (Lin and Katz, 2003). Definitional questions (“What is X?”, “Who is X?”) are especially compatible with structured resources such as gazetteers and encyclopedias. The top performing definitional systems (Xu et al., 2003) at TREC extract kernel facts similar to a question profile built using structured and semistructured resources: WordNet (Miller et al., 1990), Merriam-Webster dictionary www.m-w.com), the Columbia Encyclopedia (www.bartleby.com), Wikipedia (www.wikipedia.com), a biography dictionary (www.s9.com) and Google (www.google.com). 3 Approach For the purpose of this paper, resources consist of structured</context>
<context position="9227" citStr="Lin and Katz, 2003" startWordPosition="1477" endWordPosition="1480">type. Due to temporal consistency, numeric questions are not covered very well. Although the process and object types are broad answer types, the coverage is still reasonably good. As expected, the definition and person-bio answer types are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than t</context>
</contexts>
<marker>Lin, Katz, 2003</marker>
<rawString>J. Lin and B. Katz. 2003. Question answering from the web using knowledge annotation and knowledge mining techniques. CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>The web as a resource for question answering: Perspectives and challenges.</title>
<date>2002</date>
<publisher>LREC.</publisher>
<contexts>
<context position="2126" citStr="Lin, 2002" startWordPosition="325" endWordPosition="326"> system-independent study has quantified their impact on the QA task. This paper focuses on several resources and their inherent potential to provide answers, without concentrating on a particular QA system or component. The goal is to quantify and bound the potential impact of these resources on the QA process. 2 Related Work More and more QA systems are using the Web as a resource. Since the Web is orders of magnitude larger than local corpora, redundancy of answers and supporting passages allows systems to produce more correct, confident answers (Clarke et al., 2001; Dumais et al., 2002). (Lin, 2002) presents two different approaches to using the Web: accessing the structured data and mining the unstructured data. Due to their complementary nature of these approaches, hybrid systems are likely to perform better (Lin and Katz, 2003). Definitional questions (“What is X?”, “Who is X?”) are especially compatible with structured resources such as gazetteers and encyclopedias. The top performing definitional systems (Xu et al., 2003) at TREC extract kernel facts similar to a question profile built using structured and semistructured resources: WordNet (Miller et al., 1990), Merriam-Webster dict</context>
</contexts>
<marker>Lin, 2002</marker>
<rawString>J. Lin. 2002. The web as a resource for question answering: Perspectives and challenges. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Pervete</author>
<author>H Tanev</author>
</authors>
<title>Is it the right answer? exploiting web redundancy for answer validation.</title>
<date>2002</date>
<publisher>ACL.</publisher>
<contexts>
<context position="9466" citStr="Magnini et al., 2002" startWordPosition="1512" endWordPosition="1515">are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than the density of relevant local documents, answer extraction may be more successful from the web. For a TREC evaluation, answers found on the web must also be mapped to relevant documents in the local corpus. Web Retrieval Performance For QA </context>
</contexts>
<marker>Magnini, Negri, Pervete, Tanev, 2002</marker>
<rawString>B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002. Is it the right answer? exploiting web redundancy for answer validation. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1990</date>
<journal>International Journal ofLexicography.</journal>
<contexts>
<context position="2704" citStr="Miller et al., 1990" startWordPosition="410" endWordPosition="413">., 2001; Dumais et al., 2002). (Lin, 2002) presents two different approaches to using the Web: accessing the structured data and mining the unstructured data. Due to their complementary nature of these approaches, hybrid systems are likely to perform better (Lin and Katz, 2003). Definitional questions (“What is X?”, “Who is X?”) are especially compatible with structured resources such as gazetteers and encyclopedias. The top performing definitional systems (Xu et al., 2003) at TREC extract kernel facts similar to a question profile built using structured and semistructured resources: WordNet (Miller et al., 1990), Merriam-Webster dictionary www.m-w.com), the Columbia Encyclopedia (www.bartleby.com), Wikipedia (www.wikipedia.com), a biography dictionary (www.s9.com) and Google (www.google.com). 3 Approach For the purpose of this paper, resources consist of structured and semi-structured knowledge, such as the Web, web search engines, gazetteers, and encyclopedias. Although many QA systems incorporate or access such resources, few systems quantify individual resource impact on their performance and little work has been done to estimate bounds on resource impact to Question Answering. Independent of a sp</context>
<context position="6537" citStr="Miller et al., 1990" startWordPosition="1020" endWordPosition="1023"> and hyponyms. If the answer key matched the relevant WordNet data, then an answer was considered to be found. Since some answers might occur coincidentally, we these results to represent upper bounds on possible utility. 6 Structured Data Sources Encyclopedias, dictionaries, and other web databases are structured data sources that are often employed in answering definitional questions (e.g., “What is X?”, “Who is X?”). The top-performing definitional systems at TREC (Xu et al., 2003) extract kernel facts similar question profiles built using structured and semi-structured resources: WordNet (Miller et al., 1990), the MerriamWebster dictionary www.m-w.com), the Columbia Encyclopedia (www.bartleby.com), Wikipedia (www.wikipedia.com), a biography dictionary (www.s9.com) and Google (www.google.com). Table 3 shows a number of data sources and their impact on answering TREC questions. Ngrams were extracted from each question and run through Wikipedia and Google’s define operator (which searches specialized dictionaries, definition lists, glossaries, abbreviation lists etc). Table 3 show that TREC 10 and 11 questions benefit the most from the use of an encyclopedia, since they include many definitional ques</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Five papers on wordnet. International Journal ofLexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>D Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="5203" citStr="Moldovan et al., 2003" startWordPosition="800" endWordPosition="803">ted number of questions (See Table 1). CIA All Q-Set #qtions R P R P TREC8 200 4 100% 6 100% TREC9 693 8 100% 22 79% TREC10 500 14 100% 23 96% TREC11 500 8 100% 20 100% TREC12 500 2 100% 11 92% Overall 2393 36 100% 82 91% Table 1: Recall (R): TREC questions can be directly answered directly by gazetteers - shown are results for CIA Factbook and All gazetteers combined. Our extractor precision is Precision (P). 5 WordNet Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovan et al., 2003), probabilistic inference using lexical relations for passage scoring (Paranjpe et al., 2003), and answer filtering via WordNet constraints (Leidner et al., 2003). Q-Set #qtions All Gloss Syns Hyper TREC 8 200 32 22 7 13 TREC 9 693 197 140 73 75 TREC 10 500 206 148 82 88 TREC 11 500 112 80 29 46 TREC 12 500 93 56 10 52 Overall 2393 641 446 201 268 Table 2: Number of questions answerable using WordNet glosses (Gloss), synonyms (Syns), hypernyms and hyponyms (Hyper), and all of them combined All. Table 2 shows an upper bound on how many TREC questions could be answered directly using WordNet as </context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano. 2003. Cogex: A logic prover for question answering. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>T Mitamura</author>
<author>J Callan</author>
<author>J Carbonell</author>
<author>R Frederking</author>
<author>K Collins-Thompson</author>
<author>L Hiyakumoto</author>
<author>Y Huang</author>
<author>C Huttenhower</author>
<author>S Judy</author>
<author>J Ko</author>
<author>A Kupsc</author>
<author>L V Lita</author>
<author>V Pedro</author>
<author>D Svoboda</author>
<author>B Vand Durme</author>
</authors>
<title>A multi strategy approach with dynamic planning.</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="7827" citStr="Nyberg et al., 2003" startWordPosition="1229" endWordPosition="1232">ns and more procedural questions, it does not benefit as much from Wikipedia or Google’s define operator. Q-Set #qtions WikiAll Wiki1st DefOp TREC 8 200 56 5 30 TREC 9 693 297 49 71 TREC 10 500 225 45 34 TREC 11 500 155 19 23 TREC 12 500 124 12 27 Overall 2393 857 130 185 Table 3: The answer is found in a definition extracted from Wikipedia WikiAll, in the first definition extracted from Wikipedia Wiki1st, through Google’s define operator DefOp. 7 Answer Type Coverage To test coverage of different answer types, we employed the top level of the answer type hierarchy used by the JAVELIN system (Nyberg et al., 2003). The most frequent types are: definition (e.g. “What is viscosity?”), person-bio (e.g. “Who was Lacan?”), object(e.g. “Name the highest mountain.”), process (e.g. “How did Cleopatra die?”), lexicon (“What does CBS stand for?”)temporal(e.g. “When is the first day of summer?”), numeric (e.g. “How tall is Mount Everest?”), location (e.g. “Where is Tokyo?”), and proper-name (e.g. “Who owns the Raiders?”). AType #qtions WikiAll DefOp Gaz WN object 1003 426 92 58 309 lexicon 50 25 3 0 26 defn 178 105 9 11 112 pers-bio 39 15 11 0 17 process 138 23 6 9 16 temporal 194 63 14 0 50 numeric 121 27 13 10 </context>
</contexts>
<marker>Nyberg, Mitamura, Callan, Carbonell, Frederking, Collins-Thompson, Hiyakumoto, Huang, Huttenhower, Judy, Ko, Kupsc, Lita, Pedro, Svoboda, Durme, 2003</marker>
<rawString>E. Nyberg, T. Mitamura, J. Callan, J. Carbonell, R. Frederking, K. Collins-Thompson, L. Hiyakumoto, Y. Huang, C. Huttenhower, S. Judy, J. Ko, A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and B. Vand Durme. 2003. A multi strategy approach with dynamic planning. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Paranjpe</author>
<author>G Ramakrishnan</author>
<author>S Srinivasan</author>
</authors>
<title>Passage scoring for question answering via bayesian inference on lexical relations.</title>
<date>2003</date>
<tech>TREC.</tech>
<contexts>
<context position="5296" citStr="Paranjpe et al., 2003" startWordPosition="813" endWordPosition="816">TREC9 693 8 100% 22 79% TREC10 500 14 100% 23 96% TREC11 500 8 100% 20 100% TREC12 500 2 100% 11 92% Overall 2393 36 100% 82 91% Table 1: Recall (R): TREC questions can be directly answered directly by gazetteers - shown are results for CIA Factbook and All gazetteers combined. Our extractor precision is Precision (P). 5 WordNet Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovan et al., 2003), probabilistic inference using lexical relations for passage scoring (Paranjpe et al., 2003), and answer filtering via WordNet constraints (Leidner et al., 2003). Q-Set #qtions All Gloss Syns Hyper TREC 8 200 32 22 7 13 TREC 9 693 197 140 73 75 TREC 10 500 206 148 82 88 TREC 11 500 112 80 29 46 TREC 12 500 93 56 10 52 Overall 2393 641 446 201 268 Table 2: Number of questions answerable using WordNet glosses (Gloss), synonyms (Syns), hypernyms and hyponyms (Hyper), and all of them combined All. Table 2 shows an upper bound on how many TREC questions could be answered directly using WordNet as an answer source. Question terms and phrases were extracted and looked up in WordNet glosses,</context>
</contexts>
<marker>Paranjpe, Ramakrishnan, Srinivasan, 2003</marker>
<rawString>D. Paranjpe, G. Ramakrishnan, and S. Srinivasan. 2003. Passage scoring for question answering via bayesian inference on lexical relations. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>Automatic derivation of surface text patterns for a maximum entropy based question answering system.</title>
<date>2003</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="9306" citStr="Ravichandran et al., 2003" startWordPosition="1489" endWordPosition="1492">y well. Although the process and object types are broad answer types, the coverage is still reasonably good. As expected, the definition and person-bio answer types are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than the density of relevant local documents, answer extraction may be more successfu</context>
</contexts>
<marker>Ravichandran, Ittycheriah, Roukos, 2003</marker>
<rawString>D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003. Automatic derivation of surface text patterns for a maximum entropy based question answering system. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the trec</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="1055" citStr="Voorhees, 2003" startWordPosition="150" endWordPosition="151">rs, web resources, encyclopedia, web documents and web-based query expansion can help Question Answering in general and specific question types in particular. Depending on which resources are used, the QA task may shift from complex answer-finding mechanisms to simpler data extraction methods followed by answer re-mapping in local documents. 1 Introduction During recent years the Question Answering (QA) field has undergone considerable changes: question types have diversified, question complexity has increased, and evaluations have become more standardized - as reflected by the TREC QA track (Voorhees, 2003). Some recent approaches have tapped into external data sources such as the Web, encyclopedias, databases in order to find answer candidates, which may then be located in the specific corpus being searched (Dumais et al., 2002; Xu et al., 2003). As systems improve, the availability of rich resources will be increasingly critical to QA performance. While on-line resources such as the Web, WordNet, gazetteers, and encyclopedias are becoming more prevalent, no system-independent study has quantified their impact on the QA task. This paper focuses on several resources and their inherent potential </context>
<context position="3542" citStr="Voorhees, 2003" startWordPosition="530" endWordPosition="531">er, resources consist of structured and semi-structured knowledge, such as the Web, web search engines, gazetteers, and encyclopedias. Although many QA systems incorporate or access such resources, few systems quantify individual resource impact on their performance and little work has been done to estimate bounds on resource impact to Question Answering. Independent of a specific QA system, we quantify the degree to which these resources are able to directly provide answers to questions. Experiments are performed on the 2,393 questions and the corresponding answer keys provided through NIST (Voorhees, 2003) as part of the TREC 8 through TREC 12 evaluations. 4 Gazetteers Although the Web consists of mostly unstructured and loosely structured information, the available structured data is a valuable resource for question answering. Gazetteers in particular cover several frequently-asked factoid question types, such as ”What is the population ofX?” or ”What is the capital of Y?”. The CIA World Factbook is a database containing geographical, political, and economical profiles of all the countries in the world. We also analyzed two additional data sources containing astronomy information (www.astronom</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E.M. Voorhees. 2003. Overview of the trec 2003 question answering track. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>W B Croft</author>
</authors>
<title>Query expansion using local and global analysis.</title>
<date>1996</date>
<publisher>SIGIR.</publisher>
<contexts>
<context position="11533" citStr="Xu and Croft, 1996" startWordPosition="1855" endWordPosition="1858">mple word query retrieves a relevant document for almost half of the questions. Note that for most systems, the retrieval performance should be superior since queries are usually more refined and additional query expansion is performed. However, this experiment provides an intuition and a very good lower bound on the precision and density of current web documents for the TREC QA task. 8.2 Web-Based Query Expansion Several QA systems participating at TREC have used search engines for query expansion (Yang et al., 2003). The basic query expansion method utilizes pseudo-relevance feedback (PRF) (Xu and Croft, 1996). Content words are selected from questions and submitted as queries to a search engine. The top n retrieved documents are selected, and k terms or phrases are extracted according to an optimization criterion (e.g. term frequency, n-gram frequency, average mutual information using corpus statistics, etc). These k items are used in the expanded query. We experimented by using the top 5, 10, 15, 20, # questions 900 Answer frequency using PRF Figure 2: Finding a correct answer in PRF expansion terms - applied to 2183 questions for witch answer keys exist. 50, and 100 documents retrieved via the G</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>J. Xu and W.B. Croft. 1996. Query expansion using local and global analysis. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A Licuanan</author>
<author>R Weischedel</author>
</authors>
<title>Trec</title>
<date>2003</date>
<publisher>TREC.</publisher>
<contexts>
<context position="1299" citStr="Xu et al., 2003" startWordPosition="189" endWordPosition="192">ding mechanisms to simpler data extraction methods followed by answer re-mapping in local documents. 1 Introduction During recent years the Question Answering (QA) field has undergone considerable changes: question types have diversified, question complexity has increased, and evaluations have become more standardized - as reflected by the TREC QA track (Voorhees, 2003). Some recent approaches have tapped into external data sources such as the Web, encyclopedias, databases in order to find answer candidates, which may then be located in the specific corpus being searched (Dumais et al., 2002; Xu et al., 2003). As systems improve, the availability of rich resources will be increasingly critical to QA performance. While on-line resources such as the Web, WordNet, gazetteers, and encyclopedias are becoming more prevalent, no system-independent study has quantified their impact on the QA task. This paper focuses on several resources and their inherent potential to provide answers, without concentrating on a particular QA system or component. The goal is to quantify and bound the potential impact of these resources on the QA process. 2 Related Work More and more QA systems are using the Web as a resour</context>
<context position="2562" citStr="Xu et al., 2003" startWordPosition="388" endWordPosition="391"> than local corpora, redundancy of answers and supporting passages allows systems to produce more correct, confident answers (Clarke et al., 2001; Dumais et al., 2002). (Lin, 2002) presents two different approaches to using the Web: accessing the structured data and mining the unstructured data. Due to their complementary nature of these approaches, hybrid systems are likely to perform better (Lin and Katz, 2003). Definitional questions (“What is X?”, “Who is X?”) are especially compatible with structured resources such as gazetteers and encyclopedias. The top performing definitional systems (Xu et al., 2003) at TREC extract kernel facts similar to a question profile built using structured and semistructured resources: WordNet (Miller et al., 1990), Merriam-Webster dictionary www.m-w.com), the Columbia Encyclopedia (www.bartleby.com), Wikipedia (www.wikipedia.com), a biography dictionary (www.s9.com) and Google (www.google.com). 3 Approach For the purpose of this paper, resources consist of structured and semi-structured knowledge, such as the Web, web search engines, gazetteers, and encyclopedias. Although many QA systems incorporate or access such resources, few systems quantify individual resou</context>
<context position="6406" citStr="Xu et al., 2003" startWordPosition="1003" endWordPosition="1006">g WordNet as an answer source. Question terms and phrases were extracted and looked up in WordNet glosses, synonyms, hypernyms, and hyponyms. If the answer key matched the relevant WordNet data, then an answer was considered to be found. Since some answers might occur coincidentally, we these results to represent upper bounds on possible utility. 6 Structured Data Sources Encyclopedias, dictionaries, and other web databases are structured data sources that are often employed in answering definitional questions (e.g., “What is X?”, “Who is X?”). The top-performing definitional systems at TREC (Xu et al., 2003) extract kernel facts similar question profiles built using structured and semi-structured resources: WordNet (Miller et al., 1990), the MerriamWebster dictionary www.m-w.com), the Columbia Encyclopedia (www.bartleby.com), Wikipedia (www.wikipedia.com), a biography dictionary (www.s9.com) and Google (www.google.com). Table 3 shows a number of data sources and their impact on answering TREC questions. Ngrams were extracted from each question and run through Wikipedia and Google’s define operator (which searches specialized dictionaries, definition lists, glossaries, abbreviation lists etc). Tab</context>
<context position="9761" citStr="Xu et al., 2003" startWordPosition="1561" endWordPosition="1564">, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than the density of relevant local documents, answer extraction may be more successful from the web. For a TREC evaluation, answers found on the web must also be mapped to relevant documents in the local corpus. Web Retrieval Performance For QA 1000 Correct Doc Density First Correct Doc 800 700 600 500 400 300 200 100 00 10 20 30 40 50 60 70 80 90 100 document rank Figure 1: Web retrieval: relevant document density and rank of first relevant document. In order to evaluate the impact of web documents on TREC questions, we performed an </context>
</contexts>
<marker>Xu, Licuanan, Weischedel, 2003</marker>
<rawString>J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec 2003 qa at bbn: Answering definitional questions. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>T S Chua</author>
<author>S Wang</author>
<author>C K Koh</author>
</authors>
<title>Structured use of external knowledge for eventbased open domain question answering.</title>
<date>2003</date>
<publisher>SIGIR.</publisher>
<contexts>
<context position="9385" citStr="Yang et al., 2003" startWordPosition="1502" endWordPosition="1505">till reasonably good. As expected, the definition and person-bio answer types are covered well by these resources. 8 The Web as a Resource An increasing number of QA systems are using the web as a resource. Since the Web is orders of magnitude larger than local corpora, answers occur frequently in simple contexts, which is more conducive to retrieval and extraction of correct, confident answers (Clarke et al., 2001; Dumais et al., 2002; Lin and Katz, 2003). The web has been employed for pattern acquisition (Ravichandran et al., 2003), document retrieval (Dumais et al., 2002), query expansion (Yang et al., 2003), structured information extraction, and answer validation (Magnini et al., 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers. 8.1 Web Documents Instead of searching a local corpus, some QA systems retrieve relevant documents from the web (Xu et al., 2003). Since the density of relevant web documents can be higher than the density of relevant local documents, answer extraction may be more successful from the web. For a TREC evaluation, answers found on the web must also be ma</context>
<context position="11437" citStr="Yang et al., 2003" startWordPosition="1842" endWordPosition="1845">a correct answer, as well as the rank of the first document containing a correct answer. The simple word query retrieves a relevant document for almost half of the questions. Note that for most systems, the retrieval performance should be superior since queries are usually more refined and additional query expansion is performed. However, this experiment provides an intuition and a very good lower bound on the precision and density of current web documents for the TREC QA task. 8.2 Web-Based Query Expansion Several QA systems participating at TREC have used search engines for query expansion (Yang et al., 2003). The basic query expansion method utilizes pseudo-relevance feedback (PRF) (Xu and Croft, 1996). Content words are selected from questions and submitted as queries to a search engine. The top n retrieved documents are selected, and k terms or phrases are extracted according to an optimization criterion (e.g. term frequency, n-gram frequency, average mutual information using corpus statistics, etc). These k items are used in the expanded query. We experimented by using the top 5, 10, 15, 20, # questions 900 Answer frequency using PRF Figure 2: Finding a correct answer in PRF expansion terms - </context>
</contexts>
<marker>Yang, Chua, Wang, Koh, 2003</marker>
<rawString>H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003. Structured use of external knowledge for eventbased open domain question answering. SIGIR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>