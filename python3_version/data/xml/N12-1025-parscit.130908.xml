<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000478">
<title confidence="0.986257">
Transliteration Mining Using Large Training and Test Sets
</title>
<author confidence="0.999142">
Ali El Kahki, Kareem Darwish, Ahmed Saad El Din Mohamed Abd El-Wahab
</author>
<affiliation confidence="0.8421725">
Qatar Computing Research Institute
Qatar Foundation, Doha, Qatar
</affiliation>
<email confidence="0.9507945">
ame2154@columbia.edu,
kdarwish@qf.org.qa, ataei@qf.org.qa
</email>
<note confidence="0.583293">
Faculty of Computers and Information,
Cairo University, Cairo, Egypt
</note>
<email confidence="0.787317">
wahab@writeme.com
</email>
<sectionHeader confidence="0.990161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941794117647">
Much previous work on Transliteration
Mining (TM) was conducted on short
parallel snippets using limited training
data, and successful methods tended to
favor recall. For such methods, increasing
training data may impact precision and
application on large comparable texts may
impact precision and recall. We adapt a
state-of-the-art TM technique with the best
reported scores on the ACL 2010 NEWS
workshop dataset, namely graph
reinforcement, to work with large training
sets. The method models observed
character mappings between language pairs
as a bipartite graph and unseen mappings
are induced using random walks.
Increasing training data yields more correct
initial mappings but induced mappings
become more error prone. We introduce
parameterized exponential penalty to the
formulation of graph reinforcement and we
estimate the proper parameters for training
sets of varying sizes. The new formulation
led to sizable improvements in precision.
Mining from large comparable texts leads
to the presence of phonetically similar
words in target and source texts that may
not be transliterations or may adversely
impact candidate ranking. To overcome
this, we extracted related segments that
have high translation overlap, and then we
performed TM on them. Segment
extraction produced significantly higher
precision for three different TM methods.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999287395348837">
Transliteration Mining (TM) is the process of
finding transliterations in parallel or comparable
texts of different languages. For example, given
the Arabic-English word sequence pairs: ( wU L11WI
ﻲﺳﻼﺳ, Haile Selassie I of Ethiopia), successful TM
would mine the transliterations: (ﻲﻟﺎھﮪﮬﻫ, Haile) and
(ﻲﺳﻼﺳ, Selassie). TM has been shown to be
effective in several Information Retrieval (IR) and
Natural Language Processing (NLP) applications.
For example, in cross language IR, TM was used
to handle out-of-vocabulary query words by
mining transliterations between words in queries
and top n retrieved documents and then using
transliterations to expand queries (Udupa et al.,
2009a). In Machine Translation (MT), TM can
improve alignment at training time and help enrich
phrase tables with named entities that may not
appear in parallel training data. More broadly, TM
is a character mapping problem. Having good
character mapping models can be beneficial in a
variety of applications such as learning stemming
models, learning spelling transformations between
similar languages, and finding variant spellings of
names (Udupa and Kumar, 2010b).
TM has attracted interest in recent years with a
dedicated evaluation in the ACL 2010 NEWS
workshop. In that evaluation, TM was performed
using limited training data, namely 1,000 parallel
transliteration word-pairs, on short parallel text
segments, namely cross-language Wikipedia titles
which were typically a few words long. Since TM
was performed on very short parallel segments, the
chances that two phonetically similar words would
appear within such a short text segment in one
language were typically very low. Also, since TM
training datasets were small, many valid mappings
were not observed in training. For these two
reasons, most of the successful techniques related
to that evaluation have focused on improving
recall, while hurting precision slightly. Some of
these techniques involved the use of letter
conflation based on a SOUNDEX like scheme
(Darwish, 2010; Oh and Choi, 2006) and character
</bodyText>
<page confidence="0.65828">
243
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 243–252,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999892">
n-gram similarity. The most successful technique
on ACL-NEWS dataset, involved the use of graph
reinforcement in which observed mappings
between language pairs were modeled using a
bipartite graph and unseen mappings were induced
using random walks (El-Kahki et al., 2011).
In this paper, we focus on improving TM
between Arabic and English in more realistic
settings, compared to the NEWS workshop dataset.
Specifically, we focus on the cases where:
</bodyText>
<listItem confidence="0.859404806451613">
1. Relatively large TM training sets, which are
typical of production systems, are available. As we
will show, using more training data in conjunction
with recall-oriented techniques that perform well
on small training sets can adversely hurt precision,
leading to drops in F-measure. A more
fundamental question is what constitutes “large”
versus “small” training sets. Ideally, we want a
unified solution for training sets of varying sizes.
2. TM is performed on large comparable texts
which are ubiquitously available from different
sources such as cross language news and
Wikipedia articles. In this case, there are two
phenomena that arise. First, there is an increased
probability (compared to short texts) that words in
the target and source texts may be phonetically
similar, while not being transliterations of each
other. One such example is the Arabic word “ﻦﻣ”,
which means “in” and is pronounced as “min” and
the English word “men”. Such cases adversely
affect precision. Second, given a source language
word, there may be multiple target language words
that are phonetically similar and TM may rank a
wrong word higher than the correct one. For
example, consider the Arabic word “ﻮﺟ”, which is
pronounced as “joe” but is in fact the rendition of
the Chinese name “Zhou”. If the English text has
words such as “jaw”, “joe”, “jo”, “joy”, etc., one of
them may rank higher than “Zhou”. Since only the
top choice is considered, this phenomenon would
hurt precision and recall.
</listItem>
<bodyText confidence="0.997817543478261">
We address these two situations by making the
following two contributions:
1. Modifying the TM technique with the best
reported results on the ACL 2010 NEWS
workshop, namely graph reinforcement (El-Kahki
et al., 2011) to handle training sets of arbitrary
sizes by introducing parameterized exponential
penalty to the mapping induction process. We
show that we can effectively learn the parameters
that tune the penalty for two different training sets
of varying sizes. In doing so, we achieve better
results for graph reinforcement with larger training
sets.
2. For large comparable texts, we use contextual
clues, namely translations of neighboring words, to
constrain TM and to preserve precision.
Specifically, we initially extract text segments that
are “related” based on cross lingual lexical overlap,
and then we perform TM on these segments.
Though there have been some papers on extracting
sub-sentence alignments from comparable text
(Hewavitharana and Vogel, 2011; Munteanu and
Marcu, 2006), extracting related (as opposed to
parallel) text segments may be preferable because:
1) transliterations may not occur in parallel
contexts; 2) using simple lexical overlap is
efficient; and as we will show 3) simultaneous use
of phonetic and contextual evidences may be
sufficient to produce high TM precision. Alternate
solutions focused on performing TM on extracted
named entities only (Udupa et al., 2009b). Some
drawbacks of such an approach are: 1) named
entity recognition (NER) may not be available for
many languages; and 2) NER has inherently low
recall for languages such as Arabic where no
discriminating features such as capitalization exist.
The remainder of the paper is organized as follows:
Section 2 provides background on TM; Section 3
describes the basic TM system that is used in the
paper; Section 4 describes graph reinforcements,
shows how it fairs in the presence of a large
training set, and introduces modifications to graph
reinforcement to improve its effectiveness with
such data; Section 5 introduces the use of
contextual clues to improve TM and reports on its
effectiveness; and Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.971423" genericHeader="introduction">
2. Background
</sectionHeader>
<bodyText confidence="0.918056818181818">
Much work has been done on TM for different
language pairs such as English-Chinese (Kuo et al.,
2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al.
2008;), English-Tamil (Saravanan and Kumaran,
2008; Udupa and Khapra, 2010), English-Korean
(Oh and Isahara, 2006; Oh and Choi, 2006),
English-Japanese (Qu et al., 2000; Brill et al.,
2001; Oh and Isahara, 2006), English-Hindi (Fei et
al., 2003; Mahesh and Sinha, 2009), and English-
Russian (Klementiev and Roth, 2006). TM
typically involves finding character mappings
</bodyText>
<page confidence="0.996739">
244
</page>
<bodyText confidence="0.999493">
between two languages and using these mappings
to ascertain if two words are transliterations or not.
</bodyText>
<subsectionHeader confidence="0.999296">
2.1 Finding Character Mappings
</subsectionHeader>
<bodyText confidence="0.999542133333333">
To find character sequence mappings between two
languages, the most common approach entails
using automatic letter alignment of transliteration
pairs. Automatic alignment can be performed using
different algorithms such as EM (Kuo et al., 2008;
Lee and Chang, 2003) or HMM-based alignment
(Udupa et al., 2009a; Udupa et al., 2009b).
Another method uses automatic speech recognition
confusion tables to extract phonetically equivalent
character sequences to discover monolingual and
cross-lingual pronunciation variations (Kuo and
Yang, 2005). Alternatively, letters can be mapped
into a common character set using a predefined
transliteration scheme (Darwish, 2010; Oh and
Choi, 2006).
</bodyText>
<subsectionHeader confidence="0.997833">
2.2 Transliteration Mining
</subsectionHeader>
<bodyText confidence="0.999993317073171">
For the problem of ascertaining if two words can
be transliterations of each other, a common
approach involves using a generative model that
attempts to generate all possible transliterations of
a source word, given the character mappings
between two languages, and restricting the output
to words in the target language (Fei et al., 2003;
Lee and Chang, 2003, Udupa et al., 2009a). This is
similar to the baseline approach that we used in
this paper. Noeman and Madkour (2010)
implemented this technique using a finite state
automaton by generating all possible
transliterations along with weighted edit distance
and then filtered them using appropriate thresholds
and target language words. El-Kahki et al. (2011)
combined a generative model with so-called graph
reinforcement, which is described in greater detail
in Section 4. They reported the best TM results on
the ACL 2010 NEWS workshop dataset for 4
different languages. Alternatively back-
transliteration can be used to determine if one
sequence could have been generated by
successively mapping character sequences from
one language into another (Brill et al., 2001; Bilac
and Tanaka, 2005; Oh and Isahara, 2006).
Udupa and Khapra (2010) proposed a method in
which transliteration candidates are mapped into a
“low-dimensional common representation space”.
Then, the similarity between the resultant feature
vectors for both candidates can be computed. A
similar approach uses context sensitive hashing
(Udupa and Kumar, 2010).
Jiampojamarn et al. (2010) used classification to
determine if source and target language words
were valid transliterations. They used a variety of
features including edit distance between an English
token and the Romanized versions of the foreign
token, forward and backward transliteration
probabilities, and character n-gram similarity.
Udupa et al. (2009b) used a similar classification-
based approach.
</bodyText>
<sectionHeader confidence="0.984419" genericHeader="method">
3. Baseline Transliteration Mining
</sectionHeader>
<subsectionHeader confidence="0.999891">
3.1 Description of the Baseline System
</subsectionHeader>
<bodyText confidence="0.999995485714286">
We used a generative TM model that was
trained on a set of transliteration pairs. We
automatically aligned these pairs at character level
using an HMM-based aligner akin to that of He
(2007). Alignment produced mappings between
characters from both languages with associated
probabilities. We restricted individual source
language character sequences to be 3 characters at
most. We always treated English as the target
language and Arabic as the source language.
Briefly, we produced all possible segmentations
of a source word along with their associated
mappings into the target language. Valid target
sequences were retained and sorted by the product
of the constituent mapping probabilities. The
candidate with the highest probability was
generated given that the product of the mapping
probabilities was higher than a certain threshold.
Otherwise, no candidate was chosen.
The search for transliterated pairs was
implemented as a variant of depth-first search
(Pearl, 1984), where states represented valid
mappings between source and target substrings. At
each step, the mapping with the best score was
selected and expanded using the mappings learnt
from alignment. This process ran until mapping
combinations produced target word(s) from a
source word or until all possible states were
explored. The pseudo code in Figure 1 describes
the details of the algorithm. The implementation
was optimized via incremental left to right
processing of source words, the use of a radix tree
to prune invalid paths, and the use of a sorted
priority queue to insure that the highest weighing
candidate was at the top of the queue.
</bodyText>
<page confidence="0.984361">
245
</page>
<listItem confidence="0.996037857142857">
1: Input: Mappings, set of mappings from source fragment to a list of target fragments and mapping Probability .
2: Input: SourceWord (Fi E F1� ), Source language word
3: Input: TargetWords, radix tree containing all target language words (E1�)
4: Data Structures: DFS, Priority queue to store candidate transliterations pair ordered by their transliteration score –
Each candidate transliteration tuple = (SourceFragment, TargetTransliteration, TransliterationScore).
5: StartSymbol = (“”, “”, 1.0); DFS={StartSymbol}
7: While (DFS is not empty)
</listItem>
<figure confidence="0.8145630625">
8: SourceFragment= DFS.Top().SourceFragment
9: TargetFragment= DFS.Top().TargetTransliteration
10: FragmentScore =DFS.Top().TransliterationScore
11: If (SourceWord == SourceFragment)
12: If (FragmentScore &gt; Threshold) Return (SourceWord, TargetTransliteration, FragmentScore)
14: Else Return Null
16: DFS.RemoveTop()
17: For SubFragmentLength = 1 to 3
18: SourceSubString = SubString( SourceWord, SourceFragment.Length , SubFragmentLength)
19: Foreach mapping in Mappings[SourceSubString]
20: If ((TargetFragment + mapping.TargetFragemnt) is a sub-string in TargetWords)
21: DFS.Add(SourceFragment + SourceSubString, TargetFragment + mapping.TargetFragement,
mapping.Score * FragmentScore)
22: DFS.RemoveTop()
23: End While
24: Return Null
</figure>
<figureCaption confidence="0.999891">
Figure 1: Pseudo code for transliteration mining
</figureCaption>
<subsectionHeader confidence="0.999507">
3.2 Thresholding
</subsectionHeader>
<bodyText confidence="0.9999486">
We used a threshold on the minimum acceptable
transliteration score to filter out unreliable
transliterations. Fixing a uniform threshold would
have caused the model to filter out long
transliterations. Thus, we tied the threshold to the
length of transliterated words. We assumed a
threshold d for single character mappings and the
transliteration threshold for a target word of length
l would be 𝑑l. Since we did not have a validation
set to estimate d, we created a synthetic validation
set from the training set and then used cross-
validation to estimate d as follows: we split the
training data into 5 folds for cross validation; we
modified each validation fold by adding 5 random
words to each target word in the transliteration
pair; then we performed TM with varying
thresholds on the validation fold and computed F-
measure; and we ascertained the threshold that led
to the highest F-measure for each fold and then
took the average threshold.
</bodyText>
<subsectionHeader confidence="0.993941">
3.3 Linguistic Processing
</subsectionHeader>
<bodyText confidence="0.9999876">
For Arabic, we performed letter normalization of
the different forms of alef, alef maqsoura and ya,
and ta marbouta and ha. For English, we case-
folded all letters and removed accents, umlaut, and
similar diacritic like marks (ex. ‡, ‰, Š, ˆ, ‹, ā, ą).
</bodyText>
<sectionHeader confidence="0.994156" genericHeader="method">
4. Modifying Graph Reinforcement
</sectionHeader>
<subsectionHeader confidence="0.995535">
4.1 Original Graph Reinforcement
</subsectionHeader>
<bodyText confidence="0.999962764705882">
To motivate graph reinforcement, consider the
following example: if alignment produced the
mappings (طﻁ, ti), (طﻁ, ta), (تﺕ, ti), and (تﺕ, t), then the
mappings (طﻁ, t) and (تﺕ, ta) are likely valid –
though not observed. These mappings can be
induced by traversing the following paths: طﻁ + ti
+ تﺕ + t and تﺕ + ti + طﻁ + ta respectively.
In graph reinforcement, observed mappings were
modeled as a bipartite graph with source (S) and
target (T) character sequences and weighted with
the learnt alignment probabilities (M). Thus the
mapping between s E S and t E T was m(s,t).
Graph reinforcement was performed by traversing
the graph from S + T + S + T in order to
deduce new mappings. Given a source sequence
s&apos;E S and a target sequence t&apos; E T, the deduced
mapping weights were computed as follows:
</bodyText>
<equation confidence="0.984864">
𝑚𝑡′ 𝑠′ = 1 − 1 − 𝑚𝑡′ 𝑠𝑚𝑠𝑡𝑚𝑡𝑠′
∀S∈S,t∈T
</equation>
<bodyText confidence="0.999214333333333">
where the term 𝑚 𝑡′ 𝑠 𝑚 𝑠 𝑡 𝑚 𝑡 𝑠′ is the
score of the path between 𝑡′ and s′. De Morgan’s
law was applied to aggregate different paths using
an OR operator, which involved taking the
negation of negations of all possible paths
aggregated by an AND operator. Hence, the
</bodyText>
<page confidence="0.99221">
246
</page>
<bodyText confidence="0.999759545454545">
probability of an inferred mapping would be
boosted if it was obtained from multiple paths.
Since some characters, mainly vowels, have a
tendency to map to many other characters, link
reweighting was applied after each iteration. Link
reweighting had the effect of decreasing the
weights of target character sequences that have
many source character sequences mapping to them
and hence reducing the effect of incorrectly
inducing mappings. Link reweighting was
performed as follows:
</bodyText>
<equation confidence="0.482402">
si∈sm(Si|t)
</equation>
<bodyText confidence="0.999002666666667">
Where si E S is a source sequence that maps to t.
This is akin to normalizing conditional
probabilities.
</bodyText>
<subsectionHeader confidence="0.956153">
4.2 Graph Reinforcement Results
</subsectionHeader>
<bodyText confidence="0.9999735">
We tested graph reinforcement using 10 iterations
in 2 different settings, namely:
</bodyText>
<listItem confidence="0.988747571428571">
1. NEWS-1k: Using the ACL-NEWS workshop
dataset. The dataset contained 1,000 parallel
transliteration word pairs for training and
1,000 parallel Wikipedia titles for testing.
2. NEWS-10k: Using the test part of the ACL-
NEWS dataset, while training with 10,000
manually curated parallel transliterations.
</listItem>
<bodyText confidence="0.995442727272727">
Table 1 reports on the results of the graph
reinforcement results for the two setups. In the
NEWS-1k setup, graph reinforcement generally
had a positive effect on recall at the expense of
precision. However, as we suspected, increasing
the amount of training data (as in the NEWS-10k)
led to more initial mappings from alignment, but
with many erroneously induced mappings that
adversely impacted precision. Though recall
improved significantly, precision deteriorated
significantly, leading to lower F-measure.
</bodyText>
<tableCaption confidence="0.994606">
Table 1. Results for NEWS-1k and NEWS-10k
</tableCaption>
<table confidence="0.999741444444444">
Baseline Reinforcement
NEWS- P 0.988 0.977
1k
R 0.583 0.912
F 0.733 0.943
NEWS- P 0.917 0.689
10k
R 0.759 0.960
F 0.787 0.802
</table>
<subsectionHeader confidence="0.9848895">
4.3 Modifying Graph Reinforcement with
Parameterized Exponential Penalty
</subsectionHeader>
<bodyText confidence="0.999996">
To overcome the problem demonstrated in the
NEWS-10k setup, we adjusted the graph
reinforcement formula to give more confidence to
mappings that were observed due to initial
alignment and to successively penalize mappings
that were induced in later graph reinforcement
iterations. The adjustment was as follows:
</bodyText>
<equation confidence="0.995555333333333">
𝑚i 𝑡′ 𝑠′ = 1 − 1 − 𝑚i-1 𝑡′ 𝑠′ ∙
1 − 𝑒-i&amp;quot; 𝑚`-1 𝑡′ 𝑠 𝑚`-1 𝑠 𝑡 𝑚`-1 𝑡 𝑠′
s∈S,t∈T
</equation>
<bodyText confidence="0.999887533333333">
Where the parameter α adjusts how much we
penalize induced mappings and i is the number of
iterations. 𝑚` 𝑡&apos; 𝑠&apos; is the mapping score at
iteration i. Basically, newly seen links at iteration i
are penalized by 𝑒&amp;quot;. The equation is similar to
the earlier reinforcement equation but with all
paths except the original path 𝑠&apos; → 𝑡&apos; multiplied by
exponential penalty 𝑒-`a. Since the ACL-NEWS
dataset did not have a validation set to help us
estimate α, we opted to use the approach we used
earlier to estimate the proper thresholds, namely:
we split the training data into 5 folds for cross
validation; we modified each validation fold by
adding 5 random words to each target word in the
transliteration pair; and then we performed TM
with varying values of α and with 10 graph
reinforcement iterations on the validation fold and
computed precision and recall. For the 10k
training set, we opted to use a 90/10
training/validation split of the training data, where
the validation part was modified in the same
manner as the validation folds of the ACL-NEWS
datasets. We varied the value of α between 0.0 and
1.0 with increments of 0.1 and with increments of
1 afterwards for values greater than 1. If two
values of α yielded the same F-measure (up to 3
decimal places), we favored the larger α, favoring
precision. Figures 2 and 3 plot the precision and
recall respectively on the validation (-valid) and
test (-test) sets for the 1,000 pair training set.
</bodyText>
<figureCaption confidence="0.9750865">
Figure 2. Precision (y-axis) on test and validation
sets for varying values of a (x-axis) for the 1k set
</figureCaption>
<figure confidence="0.998030285714286">
1 P-Test
0.95 P-Valid
0.9
0.00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
m(s |
𝑚 𝑠|𝑡 =
O
</figure>
<page confidence="0.784416">
247
</page>
<figureCaption confidence="0.9908725">
Figure 3. Recall (y-axis) on test and validation sets
for varying values of α (x-axis) for the 1k set
Figure 4. Precision (y-axis) on test and validation
sets for varying values of α (x-axis) for the 10k set
Figure 5. Recall (y-axis) on test and validation sets
for varying values of α (x-axis) for the 10k set
</figureCaption>
<bodyText confidence="0.98696">
Figures 4 and 5 plot the same for the 10k pair
training set. The precision and recall values on the
validation sets are indicative of their behavior on
the test set. Due to the difference in training data
sizes, the best values of α were significantly larger
for the 10k dataset compared to the 1k dataset.
</bodyText>
<subsectionHeader confidence="0.992646">
4.4 Modified Graph Reinforcement Results
</subsectionHeader>
<bodyText confidence="0.999283774193548">
We applied exponential penalty on graph
reinforcement with the estimated value of α on the
ACL-NEWS dataset as well as the 10k training set.
Table 2 lists the estimated and optimal values of α
for the different datasets on the training and test
sets respectively along with the F-measure
obtained for these values of α. Table 2 also
compares the results to the results from baseline
and graph reinforcement without exponential
penalty. Tables 3 and 4 show precision, recall, and
F-measure results for training using ACL-NEWS
datasets and the larger training set respectively.
For the large dataset of 10k training words, using
exponential penalty improved results noticeably,
with a 16 basis points improvement in F-measure,
and we were able to estimate the optimal α. For the
smaller training set, using exponential penalty with
the estimated α marginally changed overall results
by (-0.006) compared to the optimal α. The change
in overall F-measure was generally small, with
most of the degradation in recall being offset by
improvements in precision. The small error in
estimating α for the ACL-NEWS dataset can be
attributed to the small size of the validation set.
Generally, smaller training sets require smaller
values of α to allow reinforcement to deduce more
unseen mappings, increasing recall. Larger training
sets require larger values of α and exponential
penalty becomes more important. The advantage of
this formulation is that α can be learned to match
training sets of varying sizes.
</bodyText>
<tableCaption confidence="0.9984985">
Table 2. F-measure for baseline, reinforcement, and
exponential penalty at estimated and optimal α
</tableCaption>
<table confidence="0.999305428571429">
NEWS-1k NEWS-10k
Baseline 0.757 0.787
Reinforcement (α=0) 0.941 0.802
Estimated α 0.3 6.0
@ Estimated α 0.935 0.963
Optimal α (on test) 0.1 6.0
@ optimal α 0.943 0.963
</table>
<tableCaption confidence="0.948706">
Table 3. Results for training using 1k training set
</tableCaption>
<table confidence="0.9998575">
P R F1
Baseline 0.975 0.619 0.757
Reinforcement (α=0) 0.975 0.912 0.941
@ estimated α 0.980 0.894 0.935
</table>
<tableCaption confidence="0.985589">
Table 4. Results for training using 10k training set
</tableCaption>
<table confidence="0.99921925">
P R F1
Baseline 0.917 0.759 0.787
Reinforcement (α=0) 0.689 0.960 0.802
@ estimated α 0.976 0.948 0.963
</table>
<sectionHeader confidence="0.904513" genericHeader="method">
5. TM from Large Comparable Text
</sectionHeader>
<subsectionHeader confidence="0.540794">
5.1 Baseline TM to Large Comparable Text
</subsectionHeader>
<bodyText confidence="0.9880028125">
We tested TM using the 1,000 training pairs from
the ACL-NEWS workshop on the longest 30
English Wikipedia articles with equivalent Arabic
Wikipedia articles. The test articles had the
following properties:
Max. Len Min. Len Avg. Len
Arabic 10,165 1,837 3,614
English 10,710 3,133 4,896
The article pairs had 64.7 transliterations on
average (with 1,942 in total).
To show the generality of using contextual clues,
we tested TM using 3 different techniques, namely:
the aforementioned baseline system, graph
reinforcement, and using SOUNDEX-like letter
conflation for English in the manner suggested by
Darwish (2010). This letter conflation involved
</bodyText>
<figure confidence="0.998216866666667">
0.9
0.8
1
0.00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R-Test
R-Valid
0.9
0.4
0 1 2 3 4 5 6 7 8 9 10
P-Valid
P-Test
1
0.9 R-Valid
0.8 R-Test
0 1 2 3 4 5 6 7 8 9 10
</figure>
<page confidence="0.991897">
248
</page>
<bodyText confidence="0.882559">
removing vowels, “H”, and ‘W”; and performing
the following mappings:
</bodyText>
<equation confidence="0.810313">
B, F, P, V + 1 C, G, J, K, Q, S, X, Z + 2
D,T + 3 L + 4
M,N + 5 R + 6
</equation>
<bodyText confidence="0.897861666666667">
Such letter conflation was shown to improve TM
F-measure on the ACL-NEWS workshop from
0.73 to 0.85 (Darwish, 2010).
</bodyText>
<tableCaption confidence="0.990828">
Table 5. Results for TM on full Wikipedia articles
</tableCaption>
<table confidence="0.999791">
Baseline SOUNDEX Reinforcement
P 0.610 0.059 0.650
R 0.415 0.402 0.500
F 0.494 0.103 0.565
</table>
<bodyText confidence="0.949878055555556">
Table 5 reports the TM results on the Wikipedia
articles. The increased size of the comparable text
on which we were performing TM led to adverse
effects on precision and recall for the baseline,
graph reinforcement, and SOUNDEX setups –
with 0.059 precision for SOUNDEX. Graph
reinforcement performed slightly better than the
baseline both in terms of precision and recall, but
with such low precision values, TM may not be
useful for many applications. As highlighted
earlier, the reason behind the drop in precision was
due to phonetically similar words that were in fact
not transliterations. The reason behind the drop in
recall was due to the following: when TM is
performed, often the correct transliteration was
found but not as the first candidate. Given that for
evaluation we were considering the first candidate
only, this hurt both precision and recall.
</bodyText>
<subsectionHeader confidence="0.999032">
5.2 Using Context to Improve TM
</subsectionHeader>
<bodyText confidence="0.99998685106383">
To overcome the precision and recall problems, we
used contextual information to improve TM for
large comparable text. To do so, we filtered articles
to extract potentially related fragments and then we
applied TM on the extracted fragments. The
filtering was performed based on lexical similarity
between fragments. The idea was that words that
do not share enough contexts were not likely to be
transliterations. A byproduct of this approach was
a significant reduction in TM running time since
the search space was reduced. On the downside,
this likely hurt recall as transliterations that do not
share similar contexts could not be mined.
To extract fragments with similar context we
used a phrase table from a phrase-based MT
system, which was akin to Moses (Koehn et al.,
2007), to detect similarity between fragments in
articles. The MT system was trained using 14
million parallel Arabic-English sentence pairs. The
extraction algorithm aimed to extract maximum
length fragments that share contexts greater than a
specific percentage of fragment lengths. The
threshold that we used in our experimentation was
30%. When picking the threshold, our goal was to
find transliterations that appear in similar and not
necessarily identical contexts. The threshold was
determined qualitatively on a validation set.
A brute force fragment extraction approach
would extract all possible fragments in source and
target articles, iterate on each word in each pair of
fragments to find the mappings, and then include a
fragment if the mappings count exceed the
threshold. Such a brute force approach would have
an order of N3M3, where N and M are the number
of words in the source and target articles
respectively. To improve the running time, we first
removed stop words from the source list. Then, we
created a list that contained the positions of each of
the matching pairs in source and target articles
sorted by source words’ position. This operation
had a complexity of O(NlogM). Next, we iterated
on source fragments of different size, which was
O(N2), and added the positions of matches in the
target article in a sorted list. This operation was
O(KlogK) where K is the number of matches.
Then, we iterated on extracted matches to find
target fragment that satisfied the condition:
</bodyText>
<subsectionHeader confidence="0.98842">
Fragment Length
</subsectionHeader>
<bodyText confidence="0.9973355">
≥ .3
number of mappings
The last step was O(K) in the worst case. The total
complexity of this algorithm was O(N2KlogK) in
the worst case, which had a much lower
complexity than the brute force approach. In
practice, the algorithm filtered 30 comparable pairs
of articles with an average of 4.9k words for
English and 3.6k for Arabic in less than 5 minutes.
Details of the algorithm are shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.999545">
5.3 Testing TM on Extracted Segments
</subsectionHeader>
<bodyText confidence="0.9998336">
Table 6 reports TM results on the extracted
segments. As the results show, TM on extracted
segments dramatically improved precision for all
setups compared to TM on the full articles (as in
Table 6). Except for the SOUNDEX setup, recall
dropped by 9.3 and 8.3 basis points for the baseline
and graph reinforcement setups respectively.
Though F-measure dropped slightly for the
baseline case and improved slightly for the
reinforcement case, what is noteworthy is that
</bodyText>
<page confidence="0.998078">
249
</page>
<reference confidence="0.97019636">
1: Input: Matches, a list of matches between word position in source article and its mapping in the target article sorted by
source position
2: Input: Source, list of source words; Target, list of target words
4: Output: ParallelFragments : List of pairs of parallel fragments
5: For startPosition=0 To Source.Lenght
6: For endPosition = startPosition + MinimumFragmentLengh To Source.Lenght
7: SortedList TargetMatches =[ ]
8: ForEach match Between startPosistion And endPosition In Matches
9: TargetMatches.Add(Matching[match].targetPosition)
10: startItr=0; endItr=TargetMatches.Length - 1
12: For i=0 to TargetMatches.Length
13: If( (endItr-startItr+1)/ (TargetMatches [endItr]-TargetMatches[startItr]) &gt;.3) Then
14: ParallelFragments.Add(Source.GetRange(startPosition,
endPosition),Target.GetRange(TargetMatches[startItr], TargetMatches[endItr]))
15: Break
16: Else
17: If(TargetMatches[endItr]-TargetMatches[startItr+1]&gt;TargetMatches[endItr-1]-
TargetMatches[startItr]) Then startItr++
19: Else endItr++
21: End If
22: End If
23: End Loop
24: End Loop
25: End Loop
26: Return ParallelFragments
</reference>
<figureCaption confidence="0.998535">
Figure 2. Pseudo code for the fragment extraction algorithm
</figureCaption>
<bodyText confidence="0.9970845">
precision was high enough to make TM practically
useful for a variety of applications. The major
advantage of the proposed technique is the
achievement of relatively high precision –
comparable to precision on small text snippets.
Though recall is relatively low, the ubiquity of
comparable texts can help produce large mined
transliterations of high quality.
</bodyText>
<tableCaption confidence="0.754944">
Table 6. Results for TM on extracted segments
</tableCaption>
<table confidence="0.9997425">
Baseline SOUNDEX Reinforcement
P 0.962 0.524 0.946
R 0.322 0.418 0.417
F 0.482 0.465 0.579
</table>
<sectionHeader confidence="0.997789" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.99998745">
In this paper, we explored the use of transliteration
mining in the context of using large training and
test sets. Since recent work was conducted on
small parallel text segments that were just a few
words long with limited training data, the state-of-
the-art techniques generally favored recall by
inducing mappings that were unseen in training.
Since the parallel test segments were short,
improvements in recall had a very small effect on
precision. When we applied the best reported
method in the literature using large training data or
when performing TM on large comparable texts,
drops in precision and recall were substantial.
We modified the formulation of graph
reinforcement by introducing a parameterized
exponential penalty to allow for the discovery of
new letter mappings using graph walks while
penalizing mappings that required more graph
walk steps to be induced. We showed how to
effectively estimate the exponential penalty
parameter for training sets of different sizes. In the
context of performing TM on short parallel
segments using 10k training words, we improved
TM precision from 0.689 to 0.976 at the expense
of a small drop in recall from 0.960 to 0.948.
What we observed for graph reinforcement is
symptomatic of algorithms that may fail when
more data is present. Other such examples include
stemming for MT and IR. Generally, with more
MT parallel data or bigger IR collections,
stemming may become less useful or harmful. It is
advantageous to parameterize algorithms for
tuning for dataset of different sizes.
When performing TM on large comparable texts,
we initially filtered the text to produce short
comparable text segments and then we performed
TM on them. Though the approach is relatively
simple, it led to pronounced improvement in TM
precision from 0.650 to 0.946, with a drop in recall
from 0.500 to 0.417. Given that comparable texts
</bodyText>
<page confidence="0.978299">
250
</page>
<bodyText confidence="0.9995314">
are ubiquitous, improvements in precision are
likely more important than drops in recall.
For future work, we want to test the effect of
improved TM in the context of different NLP
applications such as MT and cross language IR.
</bodyText>
<sectionHeader confidence="0.990281" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999317333333333">
Some of the experiments were conducted while the
authors were at the Cairo Microsoft Innovation
Center.
</bodyText>
<sectionHeader confidence="0.998537" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944905882353">
C. Bannard, C. Callison-Burch. 2005. Paraphrasing with
Bilingual Parallel Corpora. ACL-2005, pp. 597—
604.
S. Bilac, H. Tanaka. 2005. Extracting transliteration
pairs from comparable corpora. NLP-2005.
E. Brill, G. Kacmarcik, C. Brockett. 2001.
Automatically harvesting Katakana-English term
pairs from search engine query logs. NLPRS 2001,
pp. 393–399.
K. Darwish. 2010. Transliteration Mining with Phonetic
Conflation and Iterative Training. ACL NEWS
workshop 2010.
A. El Kahki, K. Darwish, A. Saad El Din, M. Abd El-
Wahab, A. Hefny, W. Ammar. Improved
Transliteration Mining Using Graph Reinforcement.
EMNLP-2011. pp. 1384—1393.
H. Fei, S. Vogel, A. Waibel. 2003. Extracting Named
Entity Translingual Equivalence with Limited
Resources. TALIP, 2(2):124–129.
X. He. 2007. Using Word-Dependent Transition Models
in HMM based Word Alignment for Statistical
Machine Translation. ACL-07 2nd SMT workshop.
S. Hewavitharana, S. Vogel. 2011. Extracting parallel
phrases from comparable data. The 4th Workshop on
Building and Using Comparable Corpora:
Comparable Corpora and the Web, June 24-24, 2011,
Portland, Oregon
S. Jiampojamarn, K. Dwyer, S. Bergsma, A. Bhargava,
Q. Dou, M.Y. Kim and G. Kondrak. 2010.
Transliteration Generation and Mining with Limited
Training Resources. ACL NEWS workshop 2010.
C. Jin, D.I. Kim, S.H. Na, J.H. Lee. 2008. Automatic
Extraction of English-Chinese Transliteration Pairs
using Dynamic Window and Tokenizer. Sixth
SIGHAN Workshop on Chinese Language
Processing, 2008.
A. Klementiev, D. Roth. 2006. Named Entity
Transliteration and Discovery from Multilingual
Comparable Corpora. HLT Conf. of the North
American Chapter of the ACL, pages 82–88.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst
(2007). Moses: Open Source Toolkit for Statistical
Machine Translation. ACL-2007, Demo Session.
S. Kok, C. Brockett. 2010. Hitting the Right
Paraphrases in Good Time. Human Language
Technologies: NAACL-2010.
A. Kumaran, M. Khapra, H. Li. 2010. Report of NEWS
2010 Transliteration Mining Shared Task. 2010
Named Entities Workshop, ACL 2010, pages 21–28.
J.S. Kuo, H. Li, Y.K. Yang. 2006. Learning
Transliteration Lexicons from the Web. COLING-
ACL-2006, 1129 – 1136.
J.S. Kuo, H. Li, Y.K. Yang. 2007. A phonetic similarity
model for automatic extraction of transliteration
pairs. TALIP, 2007
J.S. Kuo, H. Li, C.L. Lin. 2008. Mining Transliterations
from Web Query Results: An Incremental Approach.
Sixth SIGHAN Workshop on Chinese Language
Processing, 2008.
J.S. Kuo, Y.K. Yang. 2005. Incorporating Pronunciation
Variation into Extraction of Transliterated-term Pairs
from Web Corpora. Journal of Chinese Language and
Computing, 15 (1): (33-44).
C.J. Lee, J.S. Chang. 2003. Acquisition of English-
Chinese transliterated word pairs from parallel-
aligned texts using a statistical machine
transliteration model. Workshop on Building and
Using Parallel Texts, HLT-NAACL-2003, 2003.
D.S. Munteanu, D. Marcu. 2006. Extracting parallel
sub-sentential fragments from non-parallel corpora.
ACL-2006, p.81-88.
S. Noeman, A. Madkour. 2010. Language Independent
Transliteration Mining System Using Finite State
Automata Framework. ACL NEWS workshop 2010.
R. Mahesh, K. Sinha. 2009. Automated Mining Of
Names Using Parallel Hindi-English Corpus. 7th
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, pages 48–54, 2009.
J.H. Oh, K.S. Choi. 2006. Recognizing transliteration
equivalents for enriching domain specific thesauri.
3rd Intl. WordNet Conf., pp. 231–237, 2006.
Jong-Hoon Oh, Hitoshi Isahara. 2006. Mining the Web
for Transliteration Lexicons: Joint-Validation
</reference>
<page confidence="0.96331">
251
</page>
<reference confidence="0.999938405405405">
Approach. pp.254-261, 2006 IEEE/WIC/ACM Intl.
Conf. on Web Intelligence (WI&apos;06), 2006.
Yan Qu, Gregory Grefenstette, David A. Evans. 2003.
Automatic transliteration for Japanese-to-English text
retrieval. SIGIR 2003:353-360
P. Resnik, N. Smith. 2003. The Web as a parallel
corpus. Computational Linguistics - Special issue on
web as corpus, Vol. 29 Issue 3, Sept. 2003
K Saravanan, A Kumaran. 2008. Some Experiments in
Mining Named Entity Transliteration Pairs from
Comparable Corpora. The 2nd Intl. Workshop on
Cross Lingual Information Access: Addressing the
Need of Multilingual Societies, 2008.
J. Smith, C. Quirk, K. Toutanova. 2010. Extracting
parallel sentences from comparable corpora using
document level alignment, Human Language
Technologies: NAACL-2010, p.403-411.
R. Udupa, K. Saravanan, A. Bakalov, A. Bhole. 2009a.
&amp;quot;They Are Out There, If You Know Where to Look&amp;quot;:
Mining Transliterations of OOV Query Terms for
Cross-Language Information Retrieval. ECIR-2009.
R. Udupa, K. Saravanan, A. Kumaran, J. Jagarlamudi.
2009b. MINT: A Method for Effective and Scalable
Mining of Named Entity Transliterations from Large
Comparable Corpora. EACL 2009.
R. Udupa, M. Khapra. 2010a. Transliteration
Equivalence using Canonical Correlation Analysis.
ECIR-2010, 2010.
R. Udupa, S. Kumar. 2010b. Hashing-based Approaches
to Spelling Correction of Personal Names. EMNLP
2010.
G.W. You, S.W. Hwang, Y.I. Song, L. Jiang, Z. Nie.
2010. Mining Name Translations from Entity Graph
Mapping. EMNLP-2010, pp. 430–439.
S. Zhao, H. Wang, T. Liu, S. Li. 2008. Pivot Approach
for Extracting Paraphrase Patterns from Bilingual
Corpora. ACL-08: HLT, pp. 780–788.
</reference>
<page confidence="0.997437">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.318708">
<title confidence="0.999871">Transliteration Mining Using Large Training and Test Sets</title>
<author confidence="0.999161">Ali El_Kahki</author>
<author confidence="0.999161">Kareem Darwish</author>
<author confidence="0.999161">Ahmed Saad El_Din Mohamed Abd El-Wahab</author>
<affiliation confidence="0.8621855">Qatar Computing Research Qatar Foundation, Doha, Qatar Faculty of Computers and Information, Cairo University, Cairo, Egypt</affiliation>
<email confidence="0.999756">wahab@writeme.com</email>
<abstract confidence="0.999772911764706">Much previous work on Transliteration Mining (TM) was conducted on short parallel snippets using limited training successful methods such methods, increasing training data may impact precision and application on large comparable texts may precision and adapt a TM technique with the best scores on 2010 NEWS workshop dataset, namely graph reinforcement, to work with large training method models observed character mappings between language pairs as a bipartite graph and unseen mappings induced using random training data yields more correct initial mappings but induced mappings more error introduce parameterized exponential penalty to the formulation of graph reinforcement and we estimate the proper parameters for training of varying new formulation led to sizable improvements in precision. Mining from large comparable texts leads presence of phonetically similar in target and source texts that not be transliterations or may adversely candidate overcome this, we extracted related segments that have high translation overlap, and then we TM on extraction produced significantly higher</abstract>
<intro confidence="0.609621">for three different TM</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>1: Input: Matches, a list of matches between word position in source article and its mapping in the target article sorted by source position 2: Input: Source, list of source words; Target, list of target words 4: Output: ParallelFragments : List of pairs of parallel fragments 5:</title>
<booktitle>For startPosition=0 To Source.Lenght 6: For endPosition = startPosition + MinimumFragmentLengh To Source.Lenght 7: SortedList TargetMatches =[ ] 8: ForEach match Between startPosistion And endPosition In Matches 9: TargetMatches.Add(Matching[match].targetPosition) 10: startItr=0; endItr=TargetMatches.Length - 1 12: For i=0 to TargetMatches.Length 13: If( (endItr-startItr+1)/ (TargetMatches [endItr]-TargetMatches[startItr]) &gt;.3) Then 14: ParallelFragments.Add(Source.GetRange(startPosition, endPosition),Target.GetRange(TargetMatches[startItr], TargetMatches[endItr])) 15: Break 16: Else</booktitle>
<marker></marker>
<rawString>1: Input: Matches, a list of matches between word position in source article and its mapping in the target article sorted by source position 2: Input: Source, list of source words; Target, list of target words 4: Output: ParallelFragments : List of pairs of parallel fragments 5: For startPosition=0 To Source.Lenght 6: For endPosition = startPosition + MinimumFragmentLengh To Source.Lenght 7: SortedList TargetMatches =[ ] 8: ForEach match Between startPosistion And endPosition In Matches 9: TargetMatches.Add(Matching[match].targetPosition) 10: startItr=0; endItr=TargetMatches.Length - 1 12: For i=0 to TargetMatches.Length 13: If( (endItr-startItr+1)/ (TargetMatches [endItr]-TargetMatches[startItr]) &gt;.3) Then 14: ParallelFragments.Add(Source.GetRange(startPosition, endPosition),Target.GetRange(TargetMatches[startItr], TargetMatches[endItr])) 15: Break 16: Else</rawString>
</citation>
<citation valid="false">
<title>17: If(TargetMatches[endItr]-TargetMatches[startItr+1]&gt;TargetMatches[endItr-1]-TargetMatches[startItr]) Then startItr++ 19: Else endItr++ 21: End If 22: End If 23: End Loop 24: End Loop 25: End Loop 26: Return ParallelFragments</title>
<marker></marker>
<rawString>17: If(TargetMatches[endItr]-TargetMatches[startItr+1]&gt;TargetMatches[endItr-1]-TargetMatches[startItr]) Then startItr++ 19: Else endItr++ 21: End If 22: End If 23: End Loop 24: End Loop 25: End Loop 26: Return ParallelFragments</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<date>2005</date>
<booktitle>Paraphrasing with Bilingual Parallel Corpora. ACL-2005,</booktitle>
<pages>597--604</pages>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard, C. Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. ACL-2005, pp. 597— 604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bilac</author>
<author>H Tanaka</author>
</authors>
<title>Extracting transliteration pairs from comparable corpora.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="10614" citStr="Bilac and Tanaka, 2005" startWordPosition="1603" endWordPosition="1606">utomaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forwar</context>
</contexts>
<marker>Bilac, Tanaka, 2005</marker>
<rawString>S. Bilac, H. Tanaka. 2005. Extracting transliteration pairs from comparable corpora. NLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>G Kacmarcik</author>
<author>C Brockett</author>
</authors>
<title>Automatically harvesting Katakana-English term pairs from search engine query logs. NLPRS</title>
<date>2001</date>
<pages>393--399</pages>
<contexts>
<context position="8435" citStr="Brill et al., 2001" startWordPosition="1278" endWordPosition="1281">the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alig</context>
<context position="10590" citStr="Brill et al., 2001" startWordPosition="1599" endWordPosition="1602">ing a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of t</context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>E. Brill, G. Kacmarcik, C. Brockett. 2001. Automatically harvesting Katakana-English term pairs from search engine query logs. NLPRS 2001, pp. 393–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Darwish</author>
</authors>
<title>Transliteration Mining with Phonetic Conflation and Iterative Training.</title>
<date>2010</date>
<booktitle>ACL NEWS workshop</booktitle>
<contexts>
<context position="3736" citStr="Darwish, 2010" startWordPosition="543" endWordPosition="544">-language Wikipedia titles which were typically a few words long. Since TM was performed on very short parallel segments, the chances that two phonetically similar words would appear within such a short text segment in one language were typically very low. Also, since TM training datasets were small, many valid mappings were not observed in training. For these two reasons, most of the successful techniques related to that evaluation have focused on improving recall, while hurting precision slightly. Some of these techniques involved the use of letter conflation based on a SOUNDEX like scheme (Darwish, 2010; Oh and Choi, 2006) and character 243 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 243–252, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics n-gram similarity. The most successful technique on ACL-NEWS dataset, involved the use of graph reinforcement in which observed mappings between language pairs were modeled using a bipartite graph and unseen mappings were induced using random walks (El-Kahki et al., 2011). In this paper, we focus on improving TM between Arabic and Engl</context>
<context position="9416" citStr="Darwish, 2010" startWordPosition="1419" endWordPosition="1420">guages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating a</context>
<context position="24060" citStr="Darwish (2010)" startWordPosition="3755" endWordPosition="3756">tested TM using the 1,000 training pairs from the ACL-NEWS workshop on the longest 30 English Wikipedia articles with equivalent Arabic Wikipedia articles. The test articles had the following properties: Max. Len Min. Len Avg. Len Arabic 10,165 1,837 3,614 English 10,710 3,133 4,896 The article pairs had 64.7 transliterations on average (with 1,942 in total). To show the generality of using contextual clues, we tested TM using 3 different techniques, namely: the aforementioned baseline system, graph reinforcement, and using SOUNDEX-like letter conflation for English in the manner suggested by Darwish (2010). This letter conflation involved 0.9 0.8 1 0.00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 R-Test R-Valid 0.9 0.4 0 1 2 3 4 5 6 7 8 9 10 P-Valid P-Test 1 0.9 R-Valid 0.8 R-Test 0 1 2 3 4 5 6 7 8 9 10 248 removing vowels, “H”, and ‘W”; and performing the following mappings: B, F, P, V + 1 C, G, J, K, Q, S, X, Z + 2 D,T + 3 L + 4 M,N + 5 R + 6 Such letter conflation was shown to improve TM F-measure on the ACL-NEWS workshop from 0.73 to 0.85 (Darwish, 2010). Table 5. Results for TM on full Wikipedia articles Baseline SOUNDEX Reinforcement P 0.610 0.059 0.650 R 0.415 0.402 0.500 F 0.494 0.103 0.565 T</context>
</contexts>
<marker>Darwish, 2010</marker>
<rawString>K. Darwish. 2010. Transliteration Mining with Phonetic Conflation and Iterative Training. ACL NEWS workshop 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A El Kahki</author>
<author>K Darwish</author>
<author>A Saad El Din</author>
<author>M Abd ElWahab</author>
<author>A Hefny</author>
<author>W Ammar</author>
</authors>
<title>Improved Transliteration Mining Using Graph Reinforcement.</title>
<date>2011</date>
<pages>1384--1393</pages>
<marker>El Kahki, Darwish, El Din, ElWahab, Hefny, Ammar, 2011</marker>
<rawString>A. El Kahki, K. Darwish, A. Saad El Din, M. Abd ElWahab, A. Hefny, W. Ammar. Improved Transliteration Mining Using Graph Reinforcement. EMNLP-2011. pp. 1384—1393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fei</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Extracting Named Entity Translingual Equivalence with Limited Resources.</title>
<date>2003</date>
<journal>TALIP,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="8491" citStr="Fei et al., 2003" startWordPosition="1287" endWordPosition="1290">ications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Anothe</context>
<context position="9803" citStr="Fei et al., 2003" startWordPosition="1478" endWordPosition="1481">haracter sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternat</context>
</contexts>
<marker>Fei, Vogel, Waibel, 2003</marker>
<rawString>H. Fei, S. Vogel, A. Waibel. 2003. Extracting Named Entity Translingual Equivalence with Limited Resources. TALIP, 2(2):124–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
</authors>
<title>Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>ACL-07 2nd SMT workshop.</booktitle>
<contexts>
<context position="11623" citStr="He (2007)" startWordPosition="1753" endWordPosition="1754">f source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs at character level using an HMM-based aligner akin to that of He (2007). Alignment produced mappings between characters from both languages with associated probabilities. We restricted individual source language character sequences to be 3 characters at most. We always treated English as the target language and Arabic as the source language. Briefly, we produced all possible segmentations of a source word along with their associated mappings into the target language. Valid target sequences were retained and sorted by the product of the constituent mapping probabilities. The candidate with the highest probability was generated given that the product of the mapping</context>
</contexts>
<marker>He, 2007</marker>
<rawString>X. He. 2007. Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation. ACL-07 2nd SMT workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hewavitharana</author>
<author>S Vogel</author>
</authors>
<title>Extracting parallel phrases from comparable data.</title>
<date>2011</date>
<booktitle>The 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the</booktitle>
<location>Portland, Oregon</location>
<contexts>
<context position="6911" citStr="Hewavitharana and Vogel, 2011" startWordPosition="1033" endWordPosition="1036">e show that we can effectively learn the parameters that tune the penalty for two different training sets of varying sizes. In doing so, we achieve better results for graph reinforcement with larger training sets. 2. For large comparable texts, we use contextual clues, namely translations of neighboring words, to constrain TM and to preserve precision. Specifically, we initially extract text segments that are “related” based on cross lingual lexical overlap, and then we perform TM on these segments. Though there have been some papers on extracting sub-sentence alignments from comparable text (Hewavitharana and Vogel, 2011; Munteanu and Marcu, 2006), extracting related (as opposed to parallel) text segments may be preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences may be sufficient to produce high TM precision. Alternate solutions focused on performing TM on extracted named entities only (Udupa et al., 2009b). Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for la</context>
</contexts>
<marker>Hewavitharana, Vogel, 2011</marker>
<rawString>S. Hewavitharana, S. Vogel. 2011. Extracting parallel phrases from comparable data. The 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web, June 24-24, 2011, Portland, Oregon</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jiampojamarn</author>
<author>K Dwyer</author>
<author>S Bergsma</author>
<author>A Bhargava</author>
<author>Q Dou</author>
<author>M Y Kim</author>
<author>G Kondrak</author>
</authors>
<title>Transliteration Generation and Mining with Limited Training Resources.</title>
<date>2010</date>
<booktitle>ACL NEWS workshop</booktitle>
<contexts>
<context position="10979" citStr="Jiampojamarn et al. (2010)" startWordPosition="1655" endWordPosition="1658">rkshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs at character level using an</context>
</contexts>
<marker>Jiampojamarn, Dwyer, Bergsma, Bhargava, Dou, Kim, Kondrak, 2010</marker>
<rawString>S. Jiampojamarn, K. Dwyer, S. Bergsma, A. Bhargava, Q. Dou, M.Y. Kim and G. Kondrak. 2010. Transliteration Generation and Mining with Limited Training Resources. ACL NEWS workshop 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jin</author>
<author>D I Kim</author>
<author>S H Na</author>
<author>J H Lee</author>
</authors>
<title>Automatic Extraction of English-Chinese Transliteration Pairs using Dynamic Window and Tokenizer.</title>
<date>2008</date>
<booktitle>Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<contexts>
<context position="8251" citStr="Jin et al. 2008" startWordPosition="1251" endWordPosition="1254"> as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using a</context>
</contexts>
<marker>Jin, Kim, Na, Lee, 2008</marker>
<rawString>C. Jin, D.I. Kim, S.H. Na, J.H. Lee. 2008. Automatic Extraction of English-Chinese Transliteration Pairs using Dynamic Window and Tokenizer. Sixth SIGHAN Workshop on Chinese Language Processing, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>D Roth</author>
</authors>
<title>Named Entity Transliteration and Discovery from Multilingual Comparable Corpora.</title>
<date>2006</date>
<booktitle>HLT Conf. of the North American Chapter of the ACL,</booktitle>
<pages>82--88</pages>
<contexts>
<context position="8564" citStr="Klementiev and Roth, 2006" startWordPosition="1298" endWordPosition="1301">th such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract ph</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>A. Klementiev, D. Roth. 2006. Named Entity Transliteration and Discovery from Multilingual Comparable Corpora. HLT Conf. of the North American Chapter of the ACL, pages 82–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. ACL-2007, Demo Session.</title>
<date>2007</date>
<contexts>
<context position="26341" citStr="Koehn et al., 2007" startWordPosition="4161" endWordPosition="4164">entially related fragments and then we applied TM on the extracted fragments. The filtering was performed based on lexical similarity between fragments. The idea was that words that do not share enough contexts were not likely to be transliterations. A byproduct of this approach was a significant reduction in TM running time since the search space was reduced. On the downside, this likely hurt recall as transliterations that do not share similar contexts could not be mined. To extract fragments with similar context we used a phrase table from a phrase-based MT system, which was akin to Moses (Koehn et al., 2007), to detect similarity between fragments in articles. The MT system was trained using 14 million parallel Arabic-English sentence pairs. The extraction algorithm aimed to extract maximum length fragments that share contexts greater than a specific percentage of fragment lengths. The threshold that we used in our experimentation was 30%. When picking the threshold, our goal was to find transliterations that appear in similar and not necessarily identical contexts. The threshold was determined qualitatively on a validation set. A brute force fragment extraction approach would extract all possibl</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst (2007). Moses: Open Source Toolkit for Statistical Machine Translation. ACL-2007, Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kok</author>
<author>C Brockett</author>
</authors>
<title>Hitting the Right Paraphrases in Good Time. Human Language Technologies:</title>
<date>2010</date>
<pages>2010</pages>
<marker>Kok, Brockett, 2010</marker>
<rawString>S. Kok, C. Brockett. 2010. Hitting the Right Paraphrases in Good Time. Human Language Technologies: NAACL-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>M Khapra</author>
<author>H Li</author>
</authors>
<title>Transliteration Mining Shared Task.</title>
<date>2010</date>
<journal>Report of NEWS</journal>
<pages>21--28</pages>
<marker>Kumaran, Khapra, Li, 2010</marker>
<rawString>A. Kumaran, M. Khapra, H. Li. 2010. Report of NEWS 2010 Transliteration Mining Shared Task. 2010 Named Entities Workshop, ACL 2010, pages 21–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Kuo</author>
<author>H Li</author>
<author>Y K Yang</author>
</authors>
<date>2006</date>
<booktitle>Learning Transliteration Lexicons from the Web. COLINGACL-2006, 1129 –</booktitle>
<pages>1136</pages>
<contexts>
<context position="8198" citStr="Kuo et al., 2006" startWordPosition="1239" endWordPosition="1242">ization exist. The remainder of the paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between tw</context>
</contexts>
<marker>Kuo, Li, Yang, 2006</marker>
<rawString>J.S. Kuo, H. Li, Y.K. Yang. 2006. Learning Transliteration Lexicons from the Web. COLINGACL-2006, 1129 – 1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Kuo</author>
<author>H Li</author>
<author>Y K Yang</author>
</authors>
<title>A phonetic similarity model for automatic extraction of transliteration pairs. TALIP,</title>
<date>2007</date>
<contexts>
<context position="8216" citStr="Kuo et al., 2007" startWordPosition="1243" endWordPosition="1246"> remainder of the paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the m</context>
</contexts>
<marker>Kuo, Li, Yang, 2007</marker>
<rawString>J.S. Kuo, H. Li, Y.K. Yang. 2007. A phonetic similarity model for automatic extraction of transliteration pairs. TALIP, 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Kuo</author>
<author>H Li</author>
<author>C L Lin</author>
</authors>
<title>Mining Transliterations from Web Query Results: An Incremental Approach.</title>
<date>2008</date>
<booktitle>Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<contexts>
<context position="8234" citStr="Kuo et al., 2008" startWordPosition="1247" endWordPosition="1250">paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approac</context>
</contexts>
<marker>Kuo, Li, Lin, 2008</marker>
<rawString>J.S. Kuo, H. Li, C.L. Lin. 2008. Mining Transliterations from Web Query Results: An Incremental Approach. Sixth SIGHAN Workshop on Chinese Language Processing, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Kuo</author>
<author>Y K Yang</author>
</authors>
<title>Incorporating Pronunciation Variation into Extraction of Transliterated-term Pairs from Web Corpora.</title>
<date>2005</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>15</volume>
<issue>1</issue>
<pages>33--44</pages>
<contexts>
<context position="9293" citStr="Kuo and Yang, 2005" startWordPosition="1400" endWordPosition="1403">tain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that</context>
</contexts>
<marker>Kuo, Yang, 2005</marker>
<rawString>J.S. Kuo, Y.K. Yang. 2005. Incorporating Pronunciation Variation into Extraction of Transliterated-term Pairs from Web Corpora. Journal of Chinese Language and Computing, 15 (1): (33-44).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Lee</author>
<author>J S Chang</author>
</authors>
<title>Acquisition of EnglishChinese transliterated word pairs from parallelaligned texts using a statistical machine transliteration model. Workshop on Building and Using Parallel Texts,</title>
<date>2003</date>
<contexts>
<context position="9017" citStr="Lee and Chang, 2003" startWordPosition="1364" endWordPosition="1367">e (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to</context>
</contexts>
<marker>Lee, Chang, 2003</marker>
<rawString>C.J. Lee, J.S. Chang. 2003. Acquisition of EnglishChinese transliterated word pairs from parallelaligned texts using a statistical machine transliteration model. Workshop on Building and Using Parallel Texts, HLT-NAACL-2003, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Munteanu</author>
<author>D Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from non-parallel corpora.</title>
<date>2006</date>
<booktitle>ACL-2006,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="6938" citStr="Munteanu and Marcu, 2006" startWordPosition="1037" endWordPosition="1040">learn the parameters that tune the penalty for two different training sets of varying sizes. In doing so, we achieve better results for graph reinforcement with larger training sets. 2. For large comparable texts, we use contextual clues, namely translations of neighboring words, to constrain TM and to preserve precision. Specifically, we initially extract text segments that are “related” based on cross lingual lexical overlap, and then we perform TM on these segments. Though there have been some papers on extracting sub-sentence alignments from comparable text (Hewavitharana and Vogel, 2011; Munteanu and Marcu, 2006), extracting related (as opposed to parallel) text segments may be preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences may be sufficient to produce high TM precision. Alternate solutions focused on performing TM on extracted named entities only (Udupa et al., 2009b). Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for languages such as Arabic wher</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>D.S. Munteanu, D. Marcu. 2006. Extracting parallel sub-sentential fragments from non-parallel corpora. ACL-2006, p.81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Noeman</author>
<author>A Madkour</author>
</authors>
<title>Language Independent Transliteration Mining System Using Finite State Automata Framework.</title>
<date>2010</date>
<booktitle>ACL NEWS workshop</booktitle>
<contexts>
<context position="9942" citStr="Noeman and Madkour (2010)" startWordPosition="1503" endWordPosition="1506"> can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences fr</context>
</contexts>
<marker>Noeman, Madkour, 2010</marker>
<rawString>S. Noeman, A. Madkour. 2010. Language Independent Transliteration Mining System Using Finite State Automata Framework. ACL NEWS workshop 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mahesh</author>
<author>K Sinha</author>
</authors>
<title>Automated Mining Of Names Using Parallel Hindi-English Corpus.</title>
<date>2009</date>
<booktitle>7th Workshop on Asian Language Resources, ACLIJCNLP</booktitle>
<pages>48--54</pages>
<contexts>
<context position="8516" citStr="Mahesh and Sinha, 2009" startWordPosition="1291" endWordPosition="1294">reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic s</context>
</contexts>
<marker>Mahesh, Sinha, 2009</marker>
<rawString>R. Mahesh, K. Sinha. 2009. Automated Mining Of Names Using Parallel Hindi-English Corpus. 7th Workshop on Asian Language Resources, ACLIJCNLP 2009, pages 48–54, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Oh</author>
<author>K S Choi</author>
</authors>
<title>Recognizing transliteration equivalents for enriching domain specific thesauri. 3rd Intl. WordNet Conf.,</title>
<date>2006</date>
<pages>231--237</pages>
<contexts>
<context position="3756" citStr="Oh and Choi, 2006" startWordPosition="545" endWordPosition="548">edia titles which were typically a few words long. Since TM was performed on very short parallel segments, the chances that two phonetically similar words would appear within such a short text segment in one language were typically very low. Also, since TM training datasets were small, many valid mappings were not observed in training. For these two reasons, most of the successful techniques related to that evaluation have focused on improving recall, while hurting precision slightly. Some of these techniques involved the use of letter conflation based on a SOUNDEX like scheme (Darwish, 2010; Oh and Choi, 2006) and character 243 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 243–252, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics n-gram similarity. The most successful technique on ACL-NEWS dataset, involved the use of graph reinforcement in which observed mappings between language pairs were modeled using a bipartite graph and unseen mappings were induced using random walks (El-Kahki et al., 2011). In this paper, we focus on improving TM between Arabic and English in more realisti</context>
<context position="8380" citStr="Oh and Choi, 2006" startWordPosition="1269" endWordPosition="1272"> describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (K</context>
</contexts>
<marker>Oh, Choi, 2006</marker>
<rawString>J.H. Oh, K.S. Choi. 2006. Recognizing transliteration equivalents for enriching domain specific thesauri. 3rd Intl. WordNet Conf., pp. 231–237, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Mining the Web for Transliteration Lexicons: Joint-Validation Approach.</title>
<date>2006</date>
<booktitle>IEEE/WIC/ACM Intl. Conf. on Web Intelligence (WI&apos;06),</booktitle>
<pages>254--261</pages>
<contexts>
<context position="8360" citStr="Oh and Isahara, 2006" startWordPosition="1265" endWordPosition="1268">n the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algo</context>
<context position="10637" citStr="Oh and Isahara, 2006" startWordPosition="1607" endWordPosition="1610">ll possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward translit</context>
</contexts>
<marker>Oh, Isahara, 2006</marker>
<rawString>Jong-Hoon Oh, Hitoshi Isahara. 2006. Mining the Web for Transliteration Lexicons: Joint-Validation Approach. pp.254-261, 2006 IEEE/WIC/ACM Intl. Conf. on Web Intelligence (WI&apos;06), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>Gregory Grefenstette</author>
<author>David A Evans</author>
</authors>
<title>Automatic transliteration for Japanese-to-English text retrieval.</title>
<date>2003</date>
<journal>SIGIR</journal>
<pages>2003--353</pages>
<marker>Qu, Grefenstette, Evans, 2003</marker>
<rawString>Yan Qu, Gregory Grefenstette, David A. Evans. 2003. Automatic transliteration for Japanese-to-English text retrieval. SIGIR 2003:353-360</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>N Smith</author>
</authors>
<title>The Web as a parallel corpus. Computational Linguistics - Special issue on web as corpus,</title>
<date>2003</date>
<volume>29</volume>
<marker>Resnik, Smith, 2003</marker>
<rawString>P. Resnik, N. Smith. 2003. The Web as a parallel corpus. Computational Linguistics - Special issue on web as corpus, Vol. 29 Issue 3, Sept. 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Saravanan</author>
<author>A Kumaran</author>
</authors>
<title>Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora.</title>
<date>2008</date>
<booktitle>The 2nd Intl. Workshop on Cross Lingual Information Access: Addressing the Need of Multilingual Societies,</booktitle>
<contexts>
<context position="8297" citStr="Saravanan and Kumaran, 2008" startWordPosition="1256" endWordPosition="1259">ckground on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration p</context>
</contexts>
<marker>Saravanan, Kumaran, 2008</marker>
<rawString>K Saravanan, A Kumaran. 2008. Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora. The 2nd Intl. Workshop on Cross Lingual Information Access: Addressing the Need of Multilingual Societies, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Smith</author>
<author>C Quirk</author>
<author>K Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment, Human Language Technologies:</title>
<date>2010</date>
<volume>2010</volume>
<pages>403--411</pages>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>J. Smith, C. Quirk, K. Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment, Human Language Technologies: NAACL-2010, p.403-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>K Saravanan</author>
<author>A Bakalov</author>
<author>A Bhole</author>
</authors>
<title>They Are Out There, If You Know Where to Look&amp;quot;: Mining Transliterations of OOV Query Terms for Cross-Language Information Retrieval.</title>
<date>2009</date>
<contexts>
<context position="2396" citStr="Udupa et al., 2009" startWordPosition="338" endWordPosition="341">ations in parallel or comparable texts of different languages. For example, given the Arabic-English word sequence pairs: ( wU L11WI ﻲﺳﻼﺳ, Haile Selassie I of Ethiopia), successful TM would mine the transliterations: (ﻲﻟﺎھﮪﮬﻫ, Haile) and (ﻲﺳﻼﺳ, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al., 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem. Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (Udupa and Kumar, 2010b). TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop. In that evaluation, TM was perform</context>
<context position="7349" citStr="Udupa et al., 2009" startWordPosition="1100" endWordPosition="1103">ical overlap, and then we perform TM on these segments. Though there have been some papers on extracting sub-sentence alignments from comparable text (Hewavitharana and Vogel, 2011; Munteanu and Marcu, 2006), extracting related (as opposed to parallel) text segments may be preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences may be sufficient to produce high TM precision. Alternate solutions focused on performing TM on extracted named entities only (Udupa et al., 2009b). Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for languages such as Arabic where no discriminating features such as capitalization exist. The remainder of the paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such dat</context>
<context position="9060" citStr="Udupa et al., 2009" startWordPosition="1371" endWordPosition="1374">d Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of </context>
<context position="11311" citStr="Udupa et al. (2009" startWordPosition="1702" endWordPosition="1705">nsliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs at character level using an HMM-based aligner akin to that of He (2007). Alignment produced mappings between characters from both languages with associated probabilities. We restricted individual source language character sequences to be 3 characters at most. We always treated English as the target language and Arabic as the source language. Briefly, we pro</context>
</contexts>
<marker>Udupa, Saravanan, Bakalov, Bhole, 2009</marker>
<rawString>R. Udupa, K. Saravanan, A. Bakalov, A. Bhole. 2009a. &amp;quot;They Are Out There, If You Know Where to Look&amp;quot;: Mining Transliterations of OOV Query Terms for Cross-Language Information Retrieval. ECIR-2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>K Saravanan</author>
<author>A Kumaran</author>
<author>J Jagarlamudi</author>
</authors>
<title>MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora. EACL</title>
<date>2009</date>
<contexts>
<context position="2396" citStr="Udupa et al., 2009" startWordPosition="338" endWordPosition="341">ations in parallel or comparable texts of different languages. For example, given the Arabic-English word sequence pairs: ( wU L11WI ﻲﺳﻼﺳ, Haile Selassie I of Ethiopia), successful TM would mine the transliterations: (ﻲﻟﺎھﮪﮬﻫ, Haile) and (ﻲﺳﻼﺳ, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al., 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem. Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (Udupa and Kumar, 2010b). TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop. In that evaluation, TM was perform</context>
<context position="7349" citStr="Udupa et al., 2009" startWordPosition="1100" endWordPosition="1103">ical overlap, and then we perform TM on these segments. Though there have been some papers on extracting sub-sentence alignments from comparable text (Hewavitharana and Vogel, 2011; Munteanu and Marcu, 2006), extracting related (as opposed to parallel) text segments may be preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences may be sufficient to produce high TM precision. Alternate solutions focused on performing TM on extracted named entities only (Udupa et al., 2009b). Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for languages such as Arabic where no discriminating features such as capitalization exist. The remainder of the paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such dat</context>
<context position="9060" citStr="Udupa et al., 2009" startWordPosition="1371" endWordPosition="1374">d Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of </context>
<context position="11311" citStr="Udupa et al. (2009" startWordPosition="1702" endWordPosition="1705">nsliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs at character level using an HMM-based aligner akin to that of He (2007). Alignment produced mappings between characters from both languages with associated probabilities. We restricted individual source language character sequences to be 3 characters at most. We always treated English as the target language and Arabic as the source language. Briefly, we pro</context>
</contexts>
<marker>Udupa, Saravanan, Kumaran, Jagarlamudi, 2009</marker>
<rawString>R. Udupa, K. Saravanan, A. Kumaran, J. Jagarlamudi. 2009b. MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora. EACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>M Khapra</author>
</authors>
<title>Transliteration Equivalence using Canonical Correlation Analysis.</title>
<date>2010</date>
<contexts>
<context position="8322" citStr="Udupa and Khapra, 2010" startWordPosition="1260" endWordPosition="1263">cribes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings 244 between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment</context>
<context position="10662" citStr="Udupa and Khapra (2010)" startWordPosition="1611" endWordPosition="1614">tions along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, an</context>
</contexts>
<marker>Udupa, Khapra, 2010</marker>
<rawString>R. Udupa, M. Khapra. 2010a. Transliteration Equivalence using Canonical Correlation Analysis. ECIR-2010, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>S Kumar</author>
</authors>
<title>Hashing-based Approaches to Spelling Correction of Personal Names.</title>
<date>2010</date>
<publisher>EMNLP</publisher>
<contexts>
<context position="2857" citStr="Udupa and Kumar, 2010" startWordPosition="407" endWordPosition="410">ry words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al., 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem. Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (Udupa and Kumar, 2010b). TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop. In that evaluation, TM was performed using limited training data, namely 1,000 parallel transliteration word-pairs, on short parallel text segments, namely cross-language Wikipedia titles which were typically a few words long. Since TM was performed on very short parallel segments, the chances that two phonetically similar words would appear within such a short text segment in one language were typically very low. Also, since TM training datasets were small, many valid mappings were not obs</context>
<context position="10951" citStr="Udupa and Kumar, 2010" startWordPosition="1651" endWordPosition="1654"> on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs</context>
</contexts>
<marker>Udupa, Kumar, 2010</marker>
<rawString>R. Udupa, S. Kumar. 2010b. Hashing-based Approaches to Spelling Correction of Personal Names. EMNLP 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W You</author>
<author>S W Hwang</author>
<author>Y I Song</author>
<author>L Jiang</author>
<author>Z Nie</author>
</authors>
<date>2010</date>
<booktitle>Mining Name Translations from Entity Graph Mapping. EMNLP-2010,</booktitle>
<pages>430--439</pages>
<marker>You, Hwang, Song, Jiang, Nie, 2010</marker>
<rawString>G.W. You, S.W. Hwang, Y.I. Song, L. Jiang, Z. Nie. 2010. Mining Name Translations from Entity Graph Mapping. EMNLP-2010, pp. 430–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhao</author>
<author>H Wang</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>Pivot Approach for Extracting Paraphrase Patterns from</title>
<date>2008</date>
<booktitle>Bilingual Corpora. ACL-08: HLT,</booktitle>
<pages>780--788</pages>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>S. Zhao, H. Wang, T. Liu, S. Li. 2008. Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora. ACL-08: HLT, pp. 780–788.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>