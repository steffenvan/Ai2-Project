<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.398184">
BUILDING A LARGE THESAURUS FOR INFORMATION RETRIEVAL
Edward A. Fox** and J. Terry Nutter
</note>
<affiliation confidence="0.759866142857143">
Department of Computer Science
Virginia Tech, Blacksburg VA 24061
Thomas Ahlswede and Martha Evens
Computer Science Department
Illinois Institute of Technology, Chicago IL 60616
Judith Markowitz
Navistar International
</affiliation>
<address confidence="0.691385">
Oakbrook Terrace, IL 60181
</address>
<email confidence="0.692706">
ABSTRACT
</email>
<bodyText confidence="0.9755653">
Information retrieval systems that support searching
of large textual databases are typically accessed by trained
search intermediaries who provide assistance to end users
in bridging the gap between the languages of authors and
inquirers. We are building a thesaurus in the form of a
large semantic network .to support interactive query expan-
sion and search by end users. Our lexicon is being built by
analyzing and merging data from several large English dic-
tionaries; testing of its value for retrieval is with the
SMART and CODER systems.
</bodyText>
<sectionHeader confidence="0.572076" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999898701298702">
Though computer systems aiding retrieval from
bibliographic or full-text databases have been available for
more than two decades, it is only in recent years that many
people are becoming concerned about the serious limi-
tations of those systems regarding effectiveness in finding
desired references (Blair and Maron 1985). Like others,
though, we are convinced that easy-to-apply automatic
methods can help solve this problem (see argument in
Salton 1986). Indeed, automatic approaches seem essential
since many end-users want to search without involving
trained intermediaries (Ojala 1986). However, since the
fundamental issue is one of mismatch between language
use of document authors and inquirers, leading to
uncertainties regarding whether a particular item should be
retrieved (Chen and Dhar 1987), we are also convinced that
computational linguistics is essential for a complete
solution.
Since many queries are simply sets of lexemes or
phrases, and all queries can be reduced to that form, we
believe that focusing on lexical and phrasal issues may be
the most appropriate strategy for applying computational
linguistics to information retrieval. While good results
have been achieved in applying automatic procedures to
find lexically related words based on local context in a
* This material is based in part upon work supported by
the National Science Foundation under Grant No&apos;s. 1ST-
8418877, IST-8510069, and IRI-8703580; by the Virginia
Center for Innovative Technology under Grant No. INF-
85-016; and by AT&amp;T equipment contributions.
**All correspondence regarding this paper should be
addressed to the first author.
particular collection (Attar and Fraenkel 1977), no reliable
techniques exist for using collection statistics to build a
thesaurus automatically, and manual construction of
thesauri is both problematic and expensive (Svenonius
1986). Efforts to represent a significant portion of common
sense knowledge will take years (Lenat et al. 1986), but
developing knowledge resources to aid text processing is
now feasible (Walker 1985). Encouraged by activities in
identifying lexical and semantic information in other
languages (especially since that reported in Apresyan et al.
1970), we decided to build a large, comprehensive lexicon
for English that would include lexical and semantic
relations (see comments below and survey in Evens et al.
1980) and be of use for information retrieval. Clearly this
might also help with question answering (Evens and Smith
1978).
Lexical and semantic relations provide a formal
means for expressing relationships between words and
concepts. Most commercial dictionaries give explicit
mention to the classical relations, synonymy and antonymy,
but many other relations are used in the definitions.
Perhaps the most common is taxonomy, the relation
between a word and its genus term, as in lion —
Many noun definitions also contain the part-whole relation.
Grading and queuing relations help describe words that
come in sequences like Monday — Tuesday — Wednesday
and hot — warm — cool — cold. Collocation relations are
used to express the relationships between words that
cooccur frequently like hole and dig. As a basis for
automated thesaurus construction, we are trying to extract
from machine-readable dictionaries triples, consisting of
words or phrases linked by a labelled arc representing the
relation. We plan to include phrases as well as words for
both practical and theoretical reasons. It is well known that
thesauri that include phrases are much more effective than
those without, and we are believers in the phrasal lexicon
described by Becker (1975).
Our approach is first (see section 2) to apply text
processing methods to machine readable dictionaries
(Amsler 1984); next (see section 3) to analyze definitions
from those dictionaries; then (see section 4) to merge that
information (along with data in a large synonym file made
available by Microlytics Inc.) into a large semantic network
(see early work in Quillian 1968, and survey in Ritchie and
Hanna 1984); and finally (see section 5) to test the utility
of the resulting thesaurus using the SMART and CODER
</bodyText>
<page confidence="0.998068">
101
</page>
<bodyText confidence="0.731700555555556">
experimental retrieval systems. We discuss preliminary
results of all these aspects of our research program.
2. Dictionary text processing
Since we wish to build a thesaurus with broad
coverage, and since each dictionary has unique advantages,
we plan to use several. The bulk of our work to date has
been with the Collins Dictionary of the English Language
(CDEL) and Webster&apos;s Seventh New Collegiate Dictionary
(W7).
</bodyText>
<subsectionHeader confidence="0.973437">
2.1. Collins Dictionary of the English Language
</subsectionHeader>
<bodyText confidence="0.987255967741936">
In 1985 we obtained a magnetic tape containing the
typesetter&apos;s form of CDEL from the Oxford Text Archive
and embarked upon the task of converting that to a fact
base in the form of Prolog relations (Wohlwend 1986).
CDEL is a large dictionary with about 82,000 headwords.
Thus it is larger than W7 (which has roughly 69,000
headwords), and also has separate fields not found in W7,
such as sample usages (roughly 17,000), first names,
compare-to lists, related adjectives, and abbreviations.
Like W7, there are on average two definitions per head-
word, though while W7 has at most 26 senses per entry
there are many words in CDEL with more senses, on up
through one entry with 50 senses.
Wohlwend used various UNIX (trademark of
AT&amp;T Bell Laboratories) text processing tools and
developed analyzers with lex and yacc. The main
processing involved nine passes through the data by our
analyzers, with a small amount of manual checking and
correction between steps. By the fall of 1986 Wohlwend,
France and Chen had extracted all suitable data from the
CDEL tape, placed it in the form of facts that could be
loaded into a Prolog system, and collected statistics
regarding occurrences (Fox et al. 1986). Valuable
information was present to aid in our work with the
CODER &amp;quot;expert&amp;quot; information retrieval system (France and
Fox 1986). Recently J. Weiss has completed manual
checking and editing of the data, and has refined some of
the automatic analysis (e.g., separating past tense forms
from other irregulars). Later we will load the bulk of that
into a semantic network and carry out further processing on
the definition portions.
</bodyText>
<subsectionHeader confidence="0.997482">
2.2. Webster&apos;s Seventh Collegiate Dictionary
</subsectionHeader>
<bodyText confidence="0.958272235294118">
We received our machine-readable W7 from Raoul
Smith on five tapes in Olney&apos;s original format (Olney
1968). Our first piece of text processing was to compress
this huge mass of data (approximately 120 megabytes) into
a manageable format. AhLswede wrote a C program
compress which converted the tape data into a format based
on that of Peterson (1982), with a few differences to
simplify our analysis. The resulting version occupied
15,676,249 bytes.
The synonymy relation is particularly easy to
recognize, since it is explicitly tagged — in the printed
dictionary synonyms appear in small capitals. Thus the
first step in adding relations to our lexical database was to
extract the 45,910 synonymy relationships marked
explicitly in this way using the UNIX awk utility and insert
them in a table of word-relation-word triples (AhLswede
1985).
Morphology also provides an analytical tool for the
extraction of relations. One fruitful source is the prefix lists
of words beginning with non-, re-, and un- that are printed
in the dictionary but are never defined. These were left out
of the Olney version of the dictionary, but one of our
colleagues, Sharon King, typed in the lists and wrote
routines that generate definitions for these words, e.g.,
readjust vt to adjust again
redefinition n the act or process of defining
again
reexporter n that which exports again
unflattering aj not flattering
For many of these words it is possible to derive relational
information automatically also:
redefinition ACT redefine
redefine REPEAT define
reexporter AGENT reexport
unflattering ANTI flattering
Almost twenty percent of the words and phrases presented
in W7 do not have main entries of their own but appear as
&amp;quot;run-ons&amp;quot; at the foot of other entries. Most of the run-ons
are formed by adding productive suffixes to the main entry,
such as -ness, -able, and -ly. Fortunately, the suffixes
themselves are often defined in entries of their own that
make clear how the root and the derived word are related.
Word-relation-word triples can easily be generated between
words derived using these suffixes and their roots.
We have long agreed with Becker&apos;s (1975)
assertion that many phrases are best treated as lexical units.
We were delighted to find that the editors of W7 apparently
agree: out of 68,656 main entries, 8,878 are phrases of
more than one word (separated by spaces). Another student
is currently analyzing phrasal entries in W7, and looking
for systematic relationships.
</bodyText>
<sectionHeader confidence="0.696042" genericHeader="method">
3. Analysis of Definitions
</sectionHeader>
<subsectionHeader confidence="0.975709">
3.1. Preprocessing for the parser
</subsectionHeader>
<bodyText confidence="0.999888782608696">
The definition texts proper are the largest and (to
us) the most interesting part of dictionary entries.
Considerable preprocessing was necessary in order to
prepare the W7 definition texts for parsing with the LSP.
The Peterson/Ahlswede format uses semicolons as
delimiters to separate several fields in a definition text.
Since the LSP treats the semicolon as a word, we replaced
the semicolon delimiters with spaces. The entry word,
homograph number, sense number, and part of speech were
necessary to identify the sense being defined and must be
part of the text input to the LSP. Texts longer than 72
characters had to be broken up into multiple lines and texts
had to be converted to all upper case. Finally, each text had
to end with a period (separated from the last word by a
space).
A shell script, 1_SPformat, used the UNIX utilities
sed and tr to perform all of these operations except the
breaking of long lines. Since no UNIX utility performed
line breaks in quite the way we needed, we wrote a C
program foldx which did. However, we have not made
much use of LSPformat because we have found it useful to
combine preprocessing of definition texts with the solution
to another problem.
</bodyText>
<subsectionHeader confidence="0.85898">
3.2. The LSP Word Dictionary
</subsectionHeader>
<page confidence="0.991187">
102
</page>
<bodyText confidence="0.999983470588236">
The LSP uses a lexicon of its own, the Word
Dictionary, and cannot parse any text unless all words
(strings separated by blanks) contained in the text are
defined in the Word Dictionary. The Linguistic String
Project has created a Word Dictionary of over 10,000
words (White 1983), but this cannot be used to parse even
one W7 definition text without some additions.
In particular, W7&apos;s part of speech codes (n, vi, vt,
aj, etc.) are absent from the Word Dictionary. These were
easily added. The homograph and sense numbers were also
absent. We found it convenient to generate a list of all the
different homograph and sense numbers that appeared in
W7 (there were 410) and enter them as &amp;quot;words&amp;quot; in the
Word Dictionary. With these additions it became possible
to parse a total of 8,832 of the 126,879 definition texts
without adding any more entries to the Word Dictionary.
Identification of this subset required another special
program, showsubset, which steps through all of the W7
definition texts, comparing each word with a list of entries
in the Word Dictionary. It prints those texts out again, each
word in upper case if it appears in the Word Dictionary and
in lower case if it does not. A shell script, showsubset.com,
combines the text processing functions of L5Pforniat with a
call to showsubset to generate a complete set of definition
texts in this mixed upper and lower case format. Definition
texts consisting entirely of upper case words are then ready
to parse.
Further enlarging the Word Dictionary to include
the entire defining vocabulary (as well as all the entry
words) in W7 is a major effort, still in the early stages.
There are two kinds of words occurring in W7 for which
we need only trivial Word Dictionary entries. These are (1)
words defined but not used in definitions, and (2) proper
names, of which the most numerous and easiest to identify
are scientific names. The LSP definition grammar requires
no information about the word or phrase in the entry-word
position except its part of speech (and it needs this only to
avoid creating numerous &amp;quot;junk&amp;quot; parses). All proper names
have a simple, fixed set of Word Dictionary parameters,
thus their Word Dictionary entries are all identical except
for the names themselves. Adding these two categories to
the Word Dictionary would allow parsing of 29,692
definitions.
We have not yet added these words to the Word
Dictionary because up to now we have concentrated
exclusively on development of the definition grammar
based on the original 8,832 word subset. There are
logistical problems involved in dealing with a much larger
Word Dictionary; furthermore there is at least one major
and distinctive type of definition text (those involving
biological taxonomy) missing from the subset.
</bodyText>
<subsectionHeader confidence="0.5654475">
33. Determining what relations to look for in
definitions
</subsectionHeader>
<bodyText confidence="0.997782085106383">
One relation, taxonomy, is at least formally present
in virtually all definitions. A definition text consists of a
phrase (sometimes a very long one) consisting of one or
more head words plus zero or more modifying phrases.
The head word is a taxonomic superordinate of the word
being defined. Sometimes the taxonomy relation is
intuitively clear and useful:
hole 0 1 n an opening into or through a thing
curtain 1 1 n a hanging screen usu. capable of
being drawn...
Sometimes the formal taxonomy is obscure or of little
semantic value, and the important relations are to
modifying words or phrases:
customer 0 la n one that purchases a commodity
or service...
(customer AGENT purchase)
aged 0 la aj of an advanced age
(aged ISMIPL age)
Other times the head word is best understood as a pointer to
another relation:
baptistery 0 0 n a part of a church ... used for
baptism
(baptistery PART church)
agglutinate 2 1 vt to cause to adhere
(agglutinate v-v-CAUSE adhere)
Computational study of dictionary definitions has
focused heavily on taxonomy because it can often be
identified fairly reliably without parsing the definitions
(Amsler 1980, Chodorow et al. 1985). Our emphasis in
identifying relations, whether for information retrieval or
for other purposes, has been on relations other than
taxonomy (Evens et al. 1987).
An important tool for identifying relations is the
phrase count. A frequently occurring phrase, especially at
the beginning of a definition, is often a &amp;quot;defining formula&amp;quot;
(Smith 1985, Ahlswede and Evens 1987) that marks a
relation.
Another useful tool was very simplified hand
parsing of definitions. This consisted of manually
identifying the headword(s) and blocking off the &amp;quot;left
adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (to use LSP terminology) of
the headwords. For noun definitions, this was a very
productive procedure; the right adjuncts of noun
headwords, in particular, included many stereotyped phrase
structures which could be associated with relations.
Adjuncts in verb definitions were more varied and of less
help in identifying relations.
</bodyText>
<subsectionHeader confidence="0.999727">
3.4. Developing a grammar
</subsectionHeader>
<bodyText confidence="0.992233857142857">
The grammar of dictionary definitions is based on
the grammar provided in Sager (1981). The great majority
of changes to that grammar have been of two types:
1. Adaptations to the top-level syntax of W7. The
LSP grammar is based on a medical sublanguage consisting
of complete declarative sentences, as well as questions,
imperatives, and some other sentence types. The texts of
W7 are never complete sentences; rather they are phrases
syntactically parallel to the words they define: noun
phrases for nouns, infinitive verb phrases for verbs, etc.
It would be possible in principle to use existing LSP
phrase structures to represent most definition texts,
although some enhancement of the grammar would still be
necessary to include the entry words, homograph and sense
numbers, and part-of-speech symbols, as well as some
syntactic peculiarities like the zeroed direct object which is
part of every transitive verb definition. However, we have
chosen, for greater ease of finding relational arcs as well as
to make the grammar more efficient, to define special
phrase structures for the various definition types. Shown
below is part of the grammar for noun definitions:
</bodyText>
<page confidence="0.991046">
103
</page>
<figure confidence="0.990998266666667">
&lt;NDEF&gt; %%= &lt;NDEF-HEAD&gt; &lt;NDEF-RN&gt; .
&lt;NDEF-HEAD&gt; %%= &lt;PROHEAD&gt; / &lt;NOUNHEAD&gt; .
&lt;PROHEAD&gt; %%= ANY / ONE / SOMETHING .
&lt;NOUNHEAD&gt; %%= &lt;LN&gt; &lt;NVAR&gt; .
&lt;NDEF-RN&gt; %%= &lt;OFX&gt; / &lt;PVINGO&gt; / &lt;PN&gt; /
&lt;ASX&gt; / &lt;ASOFX&gt; / &lt;ASDFROMX&gt; /
&lt;ADJINRN&gt; / &lt;WHETHS&gt; / &lt;PWHS&gt; /
&lt;THATS-N&gt; / &lt;TOVO&gt; / &lt;TOBE&gt; /
&lt;VENPASS&gt; / -&lt;VENO&gt; / &lt;VINGO&gt; /
&lt;WHS-N&gt; / NULL .
&lt;OFX&gt; %%= &lt;VN-P&gt; &lt;NDEF&gt; . (note recursion)
&lt;ASX&gt; %%= AS &lt;NSTG&gt; &apos;)&apos; &lt;NDEF-RN&gt; .
&lt;ASOFX&gt; %%= &apos;(&apos; AS &lt;PN&gt; &apos;)&apos; &lt;NDEF-RN&gt; .
&lt;ASDFROMX&gt; %%= AS DISTINGUISHED FROM &lt;NSTG&gt; .
&lt;VN-P&gt; %%= OF / FOR .
</figure>
<bodyText confidence="0.801122666666667">
2. Routines to identify and print out relational arcs.
The following is an example of a routine which identifies
the formula used to/in/for and prints out a relational triple
</bodyText>
<equation confidence="0.951050857142857">
(X) instr CO:
T-W-N-USE = IN VENPASS%
IF IMMEDIATE-NODE IS NDEF-RN THEN $1.
$1 = DESCEND TO LVENR; STORE IN Xl;
AT COELEMENT PASSOBJ X2 IF $USE-FORMULA
THEN $N-N-USE.
$USE-FORMULA = BOTH X1 SUBSUMES &apos;USE&apos;
</equation>
<construct confidence="0.6887518">
AND X2 SUBSUMES &apos;TO&apos; OR &apos;IN&apos; OR &apos;FOR&apos;.
$N-N-USE = DESCEND TO VERB OR NSTGO OR VINGO
PASSING THROUGH STRING; DO GET-DWORD;
WRITE &apos; instr &apos;; DO WRITECORE; WRITE END
OF LINE.
</construct>
<bodyText confidence="0.976869727272727">
A few miscellaneous changes have also been made.
For example, adverbs are used more freely in W7 than in
the LSP grammar&apos;s sublanguage. Nouns appear more often
without an article, and transitive verbs are more often used
intransitively. The most spectacular difference between the
sublanguages at this level is that W7 uses the conjunction
or with a freedom that is barely if at all acceptable in
standard English. especially in adjective definitions:
abbatial 0 0 aj of or relating to an abbot...
abaxial 00 aj situated out of or directed away
from the axis
</bodyText>
<subsectionHeader confidence="0.987914">
3.5. Results of parsing
</subsectionHeader>
<bodyText confidence="0.998633">
2949 noun definitions, 1451 adjective definitions,
1272 intransitive verb definitions, and 2549 transitive verb
definitions were parsed. The LSP&apos;s performance was
significantly different in these four categories:
</bodyText>
<table confidence="0.778647285714286">
Part Percent Parses Time (sec.) Arcs
of speech success per success per parse generated
nouns 77.63 1.70 11.05 26225
adjectives 68.15 1.85 10.59 5393
int. verbs 64.62 1.59 11.96 5438
tr. verbs 60.29 1.50 43.33 14059
average/total 68.65 1.66 18.89 51115
</table>
<bodyText confidence="0.9994655">
The count of arcs generated includes duplications;
the total number of unique arcs was 25,178. These
included 5,086 taxonomies, 7,971 modification relations
(e.g., the definition &amp;quot;puppy n a young dog&amp;quot; yielded the
modification arc (puppy) mod (young)), and 125 other
relations, in three principal categories.
The first category was &amp;quot;traditional&amp;quot; relations such as
taxonomy, part-whole, etc., which we felt were amenable to
axiomatic treatment. Parsing produced relatively few of
these: 334 causation arcs, 232 part-whole arcs, and a few
hundred others.
The second category was a set of recurring syntactic
relations that we speculated would prove to have consistent
semantic significance. Some of these were familiar case
relations: there were 448 verbal nouns, 124 adjectives
derived in one way or another from verbs, etc. This
category also included, for example, relations such as &amp;quot;v-
obj&amp;quot;, the relation between a verb and the direct object of its
defining headword.
The third category consisted of syntactic relations
which we simply noted with the idea of later doing cluster
analysis to determine selectional categories in the
dictionary, much as described by Sager (1981). These
included 2,694 &amp;quot;permissible modifiers&amp;quot;, adjectives
modifier-noun pairs; 182 &amp;quot;permissible subjects&amp;quot;, nouns
appearing as subjects of verbs; and so on.
Definitions which failed to parse did so for a great
many reasons; we may be near the point of diminishing
returns in terms of refining the grammar to parse every
definition. As the third column indicates, many definitions
yielded multiple parses. Multiple parses were responsible
for most of the duplicate relational arcs that were
generated.
The quality of these parses is an important issue. A
&amp;quot;good&amp;quot; parse is one consistent with the way competent
English readers would agree to interpret the text; flaws
include acceptance of ungrammatical forms (which must be
corrected by changing the grammar) or, more often,
resolution of syntactic or semantic ambiguities in ways
which the human reader can recognize as not intended.
Quality analysis of the 8,910 parse trees is still at an early
stage.
It is not clear why transitive verbs took so much
time to parse; our best guess at this time is that this was
caused by the difficulty of locating the zeroed direct object
(see above).
</bodyText>
<subsectionHeader confidence="0.999002">
3.6. Extracting relational triples by text processing
</subsectionHeader>
<bodyText confidence="0.99987295">
As the &amp;quot;time per parse&amp;quot; column suggests, parsing
definitions is slow. (Total parsing time for our 8,832
definitions was 176 hours.) It is also intensive with respect
to development effort. Although we have by no means
given up on parsing as a powerful tool of analysis for
dictionary definitions, it seems unsuited for the task of
finding relational triples. Consequently, we have experi-
mented with a text processing approach based on the identi-
fication of defining formulas; it yields more triples in far
less time, and in many cases their quality is much better.
Our initial text processing effort involved isolating
intransitive verb definitions containing three of the most
common intransitive verb headwords: become (774
occurrences in 8,482 definitions), make (526 occurrences),
and move. Become, in intransitive verb definitions, is
almost invariably followed by an adjective — in a handful
of cases it is followed by a noun (marked by the article a)
or an adverb which, in turn, precedes an adjective. These
exceptions can be easily weeded out of the 774 definition
texts. Conjoined adjectives to the right of become are also
</bodyText>
<page confidence="0.998079">
104
</page>
<bodyText confidence="0.958930725">
easily identified and an crwk program expands the
definitions containing them into pairs (triplets, etc.) of
definitions. These are then reduced to triple form:
ablate 0 0 vi to become ablated
(ablate 00 vi) incep (ablated)
abort 0 2 vi to become checked in develop-
ment so as to remain rudimentary or to
shrink away
(abort 02 vi) incep (checked)
abotmd 0 2 vi to become copiously supplied
(abound 0 2 vi) incep (supplied)
addle 2 1 vi to become rotten
(addle 2 1 vi) incep (rotten)
age 2 2b vi to become mellow or mature
(age 2 2b vi) incep (mature)
(age 2 2b vi) incep (mellow)
By this means we extracted 2,076 triples
representing five relations, which we called incep (from
become), vncause (from make preceding a noun), move
(from move), vacause (from make preceding an adjective),
and vnbe (from several head verbs followed by as and a
noun phrase — the verb be, though occurring 389 times as
a headword, governed a variety of definition structures and
therefore conveyed no consistent relational meaning). The
whole process took about three hours.
We have also tried to extract taxonomies of
intransitive verbs, assuming as a first approximation that
the head verb constitutes a genus term. We have extracted
9,520 triples; but the quality of these was not as good as
that of the other relations, since our head finding algorithm
does not yet catch adverbial particles such as those in give
up or bring about, or idiomatic direct objects as in take
place.
We are now in the process of extracting relations by
this process for nouns, adjectives, and transitive verbs. The
improvement in performance time is less dramatic,
particularly in the case of noun definitions, where
postnominal phrases are much more common than in
intransitive verb definitions. These phrases are harder to
identify, so more manual intervention is necessary.
</bodyText>
<sectionHeader confidence="0.874909" genericHeader="method">
4. Constructing the semantic network
</sectionHeader>
<bodyText confidence="0.999981772151899">
The SNePS semantic network system (Shapiro
1979b) is one of relatively few knowledge representation
schemes that permit a unified representation of associative
information and predicate-logic-style inference (for details
on the logic, see Shapiro 1979a and Hull 1986) enhanced
with default reasoning capability (Nutter 1983). In SNePS,
every node represents a concept (concepts include lexemes,
word senses, individuals, relations, propositions, and any
other potential objects of the system&apos;s knowledge).
Conversely, every concept is represented by a node, and no
two nodes represent the same concept (although the
concepts they represent may refer to the same object; for
more on this and other principles underlying the design of
SNePS, see Shapiro and Rapaport 1986). Logical and
structuring information are carried on arcs. All explicit
information about a concept is directly linked to its node,
with structure sharing across propositions. It follows that
every detected synonym of a given word sense, for
instance, is connected to that sense by simply definable
paths to the nodes representing the original word sense and
the lexical relation SYNONYM. This simplifies finding
related terms, eliminating much of the look-up necessary in
schemes that are superficially more logic-like.
The lexicon we are forming contains three different
kinds of information: lexical-semantic relations among
specific word senses, axioms for lexical relations, and
definitional assertions. Most of the semantic information is
captured in the form of lexical relations between the sense
being defined and some other word or word sense.
Instances of lexical relations have a natural implementation
in SNePS, with base nodes representing lexemes, word
senses, and individual lexical-semantic relations and
molecular nodes (non-base nodes labeled Mx for some
integer x) representing propositions and rules about the
base nodes. To give content to individual lexical relations,
relation axioms are included for the various relations. For
instance, the lexical relation CHILD has a single axiom
which says that if an x is a child y, then any x is a y and any
x is young. While most word sense meanings determined
from definitions can be represented fully by lexical
relations, occasionally some aspect of meaning will remain
after full analysis of the lexical relations between this and
other word senses. In that case the entry is completed by a
definitional assertion.
The SNePS representations of instances of lexical
relations, the relation axioms, and assertions completing
definitions are not only compatible but linked, and hence
can readily be combined. In addition, because these
various kinds of information all have representations as
SNePS subnetworks, they are all immediately available to
the inferencing package (SNIP) without conversion or
instantiation.
Thus in most cases the semantic representations of
the different kinds of lexical information is straightforward,
and has the advantage that all information is uniformly
available to the inference system. There is a second
inferential advantage which derives from the nature of the
semantic network representation. There are two different
kinds of inference available in semantic networks: rule-
based inference and path-based inference. Rule-based
inference, the more common form of inference in most
SNePS applications, involves predicate-calculus-like
reasoning based on the existence of rule nodes (nodes
representing complex predicate calculus formulas). Insofar
as SNIP operates like a (slightly exotic) notational variant
of predicate logic, the inference in use is rule-based. In
rule-based reasoning, the semantic information resides in
the nodes; the arcs are used for accessing and to manage
implementation. Path-based inference can be viewed as the
precise inverse: inference which relies only on the
existence of paths (that is, concatenations of arcs) from one
node to another. Conceptually, rule-based inference
represents conscious reasoning from principles, while
path-based inference represents (unconscious) reasoning by
traversing associational links. The most natural
implementation for path-based reasoning is hierarchical
inheritance, but it can be applied more generally, for
example to locate synonyms. In choosing related terms for
expanding retrieval queries, it turns out that path-based
</bodyText>
<page confidence="0.998359">
105
</page>
<bodyText confidence="0.99100314893617">
reasoning is by far the more important. Path-based
inference is more efficient than rule-based inference,
because given a starting point it eliminates the need to
search the network for unifying matches. That is, where
path-based inference is possible the system does not have
to look for rules which might apply; it need only traverse a
very limited subgraph from a given starting point along a
limited set of predefined paths.
As a simple example, consider the following
definitions (quoted directly from the Oxford Advanced
Learner&apos;s Dictionary of Current English; in each case, the
definition given is the first sense for the first homonym;
pronunciations, references and examples have been
omitted).
sheep n (pl unchanged) grass-eating animal
kept for its flesh as food
(mutton) and its wool.
wool n [U] 1 soft hair of sheep, goats, and
some other animals ...
ram n 1 uncastrated male sheep.
ewe n female sheep.
lamb n 1 [C] young of the sheep
From these definitions, we extract (among others) the
following lexical relations:
sheep TAX animal
sheep TFOOD grass
wool PART sheep
wool PART goat
wool TAX hair
ram MALE sheep
ewe FEMALE sheep
lamb CHILD sheep
A simple post-processor transforms each triple of the form
R y into a SNePS User Language command of the form
&amp;quot;(build arg 1 x rel R arg2 y)&amp;quot;, which results in creating a
molecular node representing the proposition that x bears the
relation R to y, yielding the network shown in Figure 1
(simplified by omitting relation axioms and the network
linking nodes representing lexemes to those representing a
particular word sense). Since all SNePS arcs have inverses,
simple arc traversals from the node for sheep will locate
such related terms as ewe, ram, lamb, wool, animal, and so
on. Likewise, starting from a query containing wool, the
system can rapidly find sheep, etc. A more complete net-
work, including other definitions, would allow going down
as well as up the taxonomic tree, finding e.g. &amp;quot;merino&amp;quot; and
other sheep varieties from either sheep or wool.
</bodyText>
<figureCaption confidence="0.6186395">
Figure 1. Representation of Lexical Relations involving
Sheep
</figureCaption>
<bodyText confidence="0.991699363636364">
5. Testing with SMART and CODER
Several studies have been undertaken regarding the
use of lexical and semantic relations in information
retrieval. Though one investigation involved use of a
special system constructed at HT (Wang et al. 1985), most
of the other work has involved the SMART system. The
first version of SMART ran on IBM mainframes; a more
modern form was developed to run under the UNIX
operating system (Fox 1983b). In SMART, queries and
documents are represented simply as sets of terms, so a
multi-dimensional vector space can be constructed wherein
each term is associated with a different dimension in that
space. Queries and documents can then be associated with
points in that space, and documents can be retrieved if
&amp;quot;near&amp;quot; to the query. But since queries are typically short, it
can be valuable to expand a query with terms related to the
original set (especially due to variations in naming
practices like those considered in Fumas et al. 1982).
In our first experiment, involving a small collection
of 82 documents, we found a mild improvement in system
performance when all types of related terms (except
antonyms) were involved in query expansion (Fox 1980).
Similar benefits resulted when using a different, larger
collection (Evens et al. 1985). In two later studies we used
SMART but worked with Boolean queries. Query
expansion then involved &amp;quot;ORing&amp;quot; in related terms with the
original ones. Once again, improvements resulted,
especially when the p-norm scheme for interpreting
Boolean queries was applied (Fox 1983a, 1984). In all of
these studies, lexical-semantic relations were identified
manually for all query terms that were expanded.
In other recent work with SMART, Fox, Miller and
Sridaran used the same Boolean queries, but varied the
</bodyText>
<figure confidence="0.996163272727273">
TAX
TAX
ram
MALE UNMARKED
FEMALE
UNMARKED
CHILD UNMARKED
HAS-PART PART
TAX
HAS-PART
PART
</figure>
<page confidence="0.994513">
106
</page>
<bodyText confidence="0.999967285714286">
source of related words. They compared with the base case
of original queries the results of the following sources for
expansion: all words based on manually derived lexical-
semantic relations, all words (except for antonyms) taken
from the Merriam Webster Thesaurus, and all words
(except those in a &amp;quot;stop&amp;quot; list) from the definition appearing
in a dictionary for the correct word sense. All expansion
schemes gave better results han the base case. While the
lexical-semantic relation method seemed best overall, the
dictionary results were comparable and the thesaurus
approach was only slightly worse.
We are convinced that much larger improvements
are possible if end-users can be more directly involved in
the process, so they can decide which words should be
expanded, and can select which related terms to include
from the lists produced from our thesaurus. Testing this
hypothesis, however, requires a more flexible processing
paradigm than we have employed in the past. Furthermore,
we believe that inferencing using the information in the
semantic network we are building can allow us to develop
an effective automatic or semi-automatic scheme for
&amp;quot;intelligent&amp;quot; query expansion. The CODER system should
support these approaches.
Building upon early efforts to build intelligent
retrieval systems (Guida and Tasso 1983, Pollitt 1984) and
learning from experiences with similar systems (Croft and
Thompson 1987), we have been developing the CODER
(COmposite Document Expert/effective/extended Retriev-
al) system (Fox and France 1987, Fox 1987) for the last
three years. Though part of that effort deals with new
approaches to automatic text analysis (Fox and Chen 1987),
in the current context the most important aspect of CODER
is that it is built as a distributed collection of &amp;quot;expert&amp;quot;
modules (according to the models discussed in BeIlcin et al.
1987) programmed in Prolog or C, to support flexible
testing of various Al approaches to information retrieval.
Weaver and France have developed modules for handling
lexical and semantic relations and a server module
providing access to our version of the contents of CDEL.
In the future, a module will be added to interface CODER
with the SNePS semantic network so that further
experiments can be undertaken.
</bodyText>
<sectionHeader confidence="0.99624" genericHeader="conclusions">
6. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999919545454545">
Based on preliminary investigations, it appears that
lexical-semantic relations can be used to give small
improvements in the effectiveness of information retrieval
systems. Work with Webster&apos;s Seventh New Collegiate
Dictionary and the Collins Dictionary of the English
Language has demonstrated that text processing and natural
language parsing techniques can be used to extract and
organize important data about English that will be of value
for information retrieval and for a variety of natural
language processing applications. Furthermore, it is clear
that the SNePS semantic network system can be used to
store that type of data in a form that will permit both rule
and path-based inference.
Future work will include further processing of
dictionaries (including the Longman Dictionary of
Contemporary English and others from the Oxford Text
Archive), merging the resulting output into a large semantic
network, extending the capabilities of SNePS to handle a
very large thesaurus, integrating SNePS with the SMART
and CODER systems, and further testing of the utility of
that thesaurus to support interactive information retrieval
sessions.
</bodyText>
<sectionHeader confidence="0.99102" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999395290909091">
AhLswede, Thomas, 1985. &amp;quot;A Tool Kit for Lexicon Build-
ing.&amp;quot; In Proc. 23rd Annual Meeting of the Associa-
tion for Computational Linguistics, 268-276.
Ahlswede, Thomas and Martha Evens, 1987. &amp;quot;Generating
a Relational Lexicon from a Machine-Readable
Dictionary.&amp;quot; In Visible Language, W. Frawley and
R. Smith (eds.), Oxford University Press, to appear.
Amsler, Robert A., 1980. &amp;quot;The Structure of the Merriam-
Webster Pocket Dictionary.&amp;quot; Ph.D. Dissertation.
TR-164, Computer Science Dept., Univ. of Texas,
Austin, Dec. 1980.
Amsler, Robert A., 1984. &amp;quot;Machine-Readable
Dictionaries.&amp;quot; Annual Review of Information
Science and Technology, 19:161-209.
Apresyan, Yu. D., I. A. Mel&apos;euk and A. K. 2olkovsky,
1970. &amp;quot;Semantics and Lexicography: Towards a
New Type of Unilingual Dictionary.&amp;quot; In Studies in
Syntax and Semantics, F. Kiefer (ed.), Reidel,
Dordrecht, Holland, 1-33.
Attar, R. and Aviezri S. Fraenkel, 1977. &amp;quot;Local Feedback
in Full-Text Retrieval Systems.&amp;quot; J. ACM, 24(3):
397-417.
Becker, Joseph, 1975. &amp;quot;The Phrasal Lexicon.&amp;quot; In
Theoretical Issues in Natural Language Processing,
R. Schank and B. Nash-Webber (eds.), ACL, 38-41.
Belkin, N. et al., 1987. &amp;quot;Distributed Expert-Based Infor-
mation Systems: An Interdisciplinary Approach.&amp;quot;
Information Processing and Management, to
appear.
Blair, D.C. and M.E. Maron, 1985. &amp;quot;An Evaluation of
Retrieval Effectiveness for a Full-Text Document-
Retrieval System.&amp;quot; Commun. ACM, 28(3):289-299.
Chen, Hsinchun and Vasant Dhar, 1987. &amp;quot;Reducing
Indeterminism in Consultation: A Cognitive Model
of User/Librarian Interactions.&amp;quot; In Proc. AAAI-87,
285-289.
Chodorow, Martin S., Roy Byrd, and George Heidorn,
1985. &amp;quot;Extracting Semantic Hierarchies from a
Large On-Line Dictionary.&amp;quot; In Proc. of the 23rd
Annual Meeting of the Association for
Computational Linguistics, 299-304.
Croft, W. Bruce and Roger Thompson, 1987. &amp;quot;I3R: A New
Approach to the Design of Document Retrieval
Systems.&amp;quot; J. Am. Soc. Inf. Sci., in press.
Evens, Martha W. and Raoul N. Smith, 1978. &amp;quot;A Lexicon
for a Computer Question-Answering System.&amp;quot; Am.
J. Comp. Ling., No. 4, Microfiche 83: 1-96.
Evens, Martha W., Bonnie C. Litowitz, Judith A.
Markowitz, Raoul N. Smith, and Oswald Werner,
1980. Lexical-Semantic Relations: A Comparative
Survey. Linguistic Research, Inc., Edmonton,
Alberta.
Evens, Martha W., James Vandendorpe, and Yih-Chen
Wang, 1985. &amp;quot;Lexical-Semantic Relations in
Information Retrieval.&amp;quot; In Humans and Machines:
</reference>
<page confidence="0.994213">
107
</page>
<reference confidence="0.994807772357724">
The Interface Through Language, S. Williams (ed.),
Ablex, Norwood, NJ, 73-100.
Evens, Martha W., Judith Markowitz, Thomas Ahlswede,
and Kay Rossi, 1987. &amp;quot;Digging in the Dictionary:
Building a Relational Lexicon to Support Natural
Language Processing Applications.&amp;quot; IDEAL (Issues
and Developments in English and Applied
Linguistics) 2(33-44).
Fox, E.A., 1980. &amp;quot;Lexical Relations: Enhancing
Effectiveness of Information Retrieval Systems.&amp;quot;
ACM SIGIR Forum, 15(3):5-36.
Fox, Edward A., 1983a. &amp;quot;Extending the Boolean and
Vector Space Models of Information Retrieval with
P-Norm Queries and Multiple Concept Types.&amp;quot;
Dissertation, Cornell University, University
Microfilms Int., Ann Arbor MI.
Fox, Edward A., 1983b. &amp;quot;Some Considerations for
Implementing the SMART Information Retrieval
System under UNIX.&amp;quot; TR 83-560, Cornell Univ.,
Dept. of Comp. Sci..
Fox, Edward A., 1984. &amp;quot;Improved Retrieval Using a
Relational Thesaurus Expansion of Boolean Logic
Queries.&amp;quot; In Proc. Workshop Relational Models of
the Lexicon, Martha W. Evens (ed.), Stanford, CA,
to appear.
Fox, Edward A., 1987. &amp;quot;Development of the CODER
System: A Testbed for Artificial Intelligence
Methods in Information Retrieval.&amp;quot; Information
Processing and Management, 23(4).
Fox, Edward A. and Qi-Fang Chen, 1987. &amp;quot;Text Analysis
in the CODER System.&amp;quot; In Proc. Fourth Annual
USC Computer Science Symposium: Language and
Data in Systems, Columbia SC, 7-14.
Fox, Edward A. and Robert K. France, 1987. &amp;quot;Architecture
of an Expert System for Composite Document
Analysis, Representation and Retrieval.&amp;quot; Int. J. of
Approximate Reasoning, 1(2).
Fox, Edward A., Robert C. Wohlwend, Phyllis R. Sheldon,
Qi Fan Chen, and Robert K. France, 1986.
&amp;quot;Building the CODER Lexicon The Collins English
Dictionary and Its Adverb Definitions.&amp;quot; TR-86-23,
VPI&amp;SU Computer Science Dept., Blacksburg, VA.
France, Robert K. and Edward A. Fox, 1986. &amp;quot;Knowledge
Structures for Information Retrieval: Representation
in the CODER Project.&amp;quot; In Proceedings IEEE
Expert Systems in Government Conference, October
20-24, 1986, McLean VA, 135-141.
Furnas, George W. et al., 1982. &amp;quot;Statistical semantics: how
can a computer use what people name things to
guess what things people mean when they name
things.&amp;quot; In Proc. of the Human Factors in Computer
Systems Conference, Gaithersburg, MD, Assoc. for
Computing Machinery, New York.
Guida, G., and C. Tasso, 1983. &amp;quot;An expert intermediary
system for interactive document retrieval.&amp;quot;
Automatica 19(6): 759-766.
Hull, Richard G., 1986. &amp;quot;A New Design for SNIP the
SNePS Inference Package.&amp;quot; SNeRG Technical
Report 86-10, Department of Computer Science,
SUNY at Buffalo.
Lenat, Doug et al., 1986. &amp;quot;CYC: Using Common Sense
Knowledge to Overcome Brittleness and
Knowledge Acquisition Bottlenecks.&amp;quot; The Al
Magazine, 65-84.
Nutter, J. Terry, 1983. &amp;quot;Default reasoning using monotonic
logic: a modest proposal.&amp;quot; In Proc. AAAI-83, The
National Conf. on AI, Washington D.C., 297-300.
Ojala, Marydee, 1986. &amp;quot;Views on End-User Searching.&amp;quot; J.
Am. Soc. If. Sci., 37(4), 197-203.
Olney, John, 1968. &apos;To All Interested in the Merriam-
Webster Transcripts and Data Derived from Them.&amp;quot;
Systems Development Corporation Document L-
13579.
Peterson, James L., 1982. &amp;quot;Webster&apos;s Seventh New
Collegiate Dictionary: A Computer-Readable File
Format.&amp;quot; TR-196, Dept. of Comp. Sci., Univ. of
Texas, Austin.
Pollitt, S.E., 1984. &amp;quot;A &apos;front-end&apos; system: an Expert
System as an online search intermediary.&amp;quot; Aslib
Proceedings, 36(5): 229-234.
Quillian. M. Ross, 1968. &amp;quot;Semantic Memory.&amp;quot; In Semantic
Information Processing, Marvin Minsky ed.
Cambridge, MA: MIT Press, 227-270.
Ritchie, C.D. and Hanna, F.K., 1984. &amp;quot;Semantic Networks:
a General Definition and a Survey.&amp;quot; lnf Tech.: Res.
Dev. Applications, 3(1):33-42.
Sager, Naomi, 1981, Natural Language Information
Processing.&amp;quot; Addison-Wesley, Reading, MA.
Salton, G., 1986. &amp;quot;Another Look at Automatic Text-Re-
trieval Systems.&amp;quot; Commun. ACM, 29(7): 648-656.
Shapiro, Stuart C., 1979a. &amp;quot;Generalized Augmented
Transition Network Grammars for Generation from
Semantic Networks.&amp;quot; In Proc. of the 17th Annual
Meeting of the Association for Computational
Linguistics, 25-29.
Shapiro, Stuart C., 1979b. &amp;quot;The SNePS Semantic Network
Processing System.&amp;quot; In Associative Networks, Nick
Findler (ed.), Academic Press, NY, 179-203.
Shapiro, Stuart C. and William Rapaport, 1986. &amp;quot;SNePS
Considered as a Fully Intensional Propositional
Semantic Network.&amp;quot; In Proc. AAAI-86, Fifth
National Conf on Al, Phil. PA, 278-283.
Smith, Raoul N., 1985. &amp;quot;Conceptual primitives in the
English lexicon.&amp;quot; Papers in Linguistics 18: 99-137.
Svenonius, Elaine, 1986. &amp;quot;Unanswered Questions in the
Design of Controlled Vocabularies.&amp;quot; J. Am. Soc. Inf.
Sci., 37(5):331-340.
Walker, Donald E., 1985. &amp;quot;Knowledge Resource Tools for
Accessing Large Text Files.&amp;quot; In Proc. First
Conference of the UW Centre for the New Oxford
English Dictionary: Information in Data. Nov. 6-7,
1985, Waterloo, Canada, 11-24.
Wang, Yih-Chen, James Vandendorpe, and Martha Evens,
1985. &amp;quot;Relational Thesauri in Information
Retrieval.&amp;quot; J. Am. Soc. Inf Sci., 36(1): 15-27.
White, Carolyn, 1983. &amp;quot;The Linguistic String Project
Dictionary for Automatic Text Analysis.&amp;quot; In Proc.
Workshop on Machine Readable Dictionaries, SRI,
Menlo Park, CA.
Wohlwend, Robert C., 1986. &amp;quot;Creation of a Prolog Fact
Base from the Collins English Dictionary.&amp;quot; MS
Report, VPI&amp;SU Computer Science Dept.,
Blacksburg, VA.
</reference>
<page confidence="0.998368">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495913">
<title confidence="0.999705">BUILDING A LARGE THESAURUS FOR INFORMATION RETRIEVAL</title>
<author confidence="0.999971">Edward A Fox</author>
<author confidence="0.999971">J Terry Nutter</author>
<affiliation confidence="0.999897">Department of Computer Science</affiliation>
<address confidence="0.991419">Virginia Tech, Blacksburg VA 24061</address>
<author confidence="0.998617">Thomas Ahlswede</author>
<author confidence="0.998617">Martha Evens</author>
<affiliation confidence="0.99987">Computer Science Department</affiliation>
<address confidence="0.530388">Illinois Institute of Technology, Chicago IL 60616</address>
<author confidence="0.9077">Judith Markowitz</author>
<affiliation confidence="0.999523">Navistar International</affiliation>
<address confidence="0.999137">Oakbrook Terrace, IL 60181</address>
<abstract confidence="0.995756181818182">Information retrieval systems that support searching of large textual databases are typically accessed by trained search intermediaries who provide assistance to end users in bridging the gap between the languages of authors and inquirers. We are building a thesaurus in the form of a large semantic network .to support interactive query expansion and search by end users. Our lexicon is being built by analyzing and merging data from several large English dictionaries; testing of its value for retrieval is with the SMART and CODER systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas AhLswede</author>
</authors>
<title>A Tool Kit for Lexicon Building.&amp;quot; In</title>
<date>1985</date>
<booktitle>Proc. 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--276</pages>
<contexts>
<context position="8035" citStr="AhLswede 1985" startWordPosition="1254" endWordPosition="1255">e format. AhLswede wrote a C program compress which converted the tape data into a format based on that of Peterson (1982), with a few differences to simplify our analysis. The resulting version occupied 15,676,249 bytes. The synonymy relation is particularly easy to recognize, since it is explicitly tagged — in the printed dictionary synonyms appear in small capitals. Thus the first step in adding relations to our lexical database was to extract the 45,910 synonymy relationships marked explicitly in this way using the UNIX awk utility and insert them in a table of word-relation-word triples (AhLswede 1985). Morphology also provides an analytical tool for the extraction of relations. One fruitful source is the prefix lists of words beginning with non-, re-, and un- that are printed in the dictionary but are never defined. These were left out of the Olney version of the dictionary, but one of our colleagues, Sharon King, typed in the lists and wrote routines that generate definitions for these words, e.g., readjust vt to adjust again redefinition n the act or process of defining again reexporter n that which exports again unflattering aj not flattering For many of these words it is possible to de</context>
</contexts>
<marker>AhLswede, 1985</marker>
<rawString>AhLswede, Thomas, 1985. &amp;quot;A Tool Kit for Lexicon Building.&amp;quot; In Proc. 23rd Annual Meeting of the Association for Computational Linguistics, 268-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Ahlswede</author>
<author>Martha Evens</author>
</authors>
<title>Generating a Relational Lexicon from a Machine-Readable Dictionary.&amp;quot;</title>
<date>1987</date>
<editor>In Visible Language, W. Frawley and R. Smith (eds.),</editor>
<publisher>University Press,</publisher>
<location>Oxford</location>
<note>to appear.</note>
<contexts>
<context position="15321" citStr="Ahlswede and Evens 1987" startWordPosition="2461" endWordPosition="2464">e 2 1 vt to cause to adhere (agglutinate v-v-CAUSE adhere) Computational study of dictionary definitions has focused heavily on taxonomy because it can often be identified fairly reliably without parsing the definitions (Amsler 1980, Chodorow et al. 1985). Our emphasis in identifying relations, whether for information retrieval or for other purposes, has been on relations other than taxonomy (Evens et al. 1987). An important tool for identifying relations is the phrase count. A frequently occurring phrase, especially at the beginning of a definition, is often a &amp;quot;defining formula&amp;quot; (Smith 1985, Ahlswede and Evens 1987) that marks a relation. Another useful tool was very simplified hand parsing of definitions. This consisted of manually identifying the headword(s) and blocking off the &amp;quot;left adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (to use LSP terminology) of the headwords. For noun definitions, this was a very productive procedure; the right adjuncts of noun headwords, in particular, included many stereotyped phrase structures which could be associated with relations. Adjuncts in verb definitions were more varied and of less help in identifying relations. 3.4. Developing a grammar The grammar of dictionary definitions</context>
</contexts>
<marker>Ahlswede, Evens, 1987</marker>
<rawString>Ahlswede, Thomas and Martha Evens, 1987. &amp;quot;Generating a Relational Lexicon from a Machine-Readable Dictionary.&amp;quot; In Visible Language, W. Frawley and R. Smith (eds.), Oxford University Press, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Amsler</author>
</authors>
<title>The Structure of the MerriamWebster Pocket Dictionary.&amp;quot;</title>
<date>1980</date>
<tech>Ph.D. Dissertation. TR-164,</tech>
<institution>Computer Science Dept., Univ. of Texas,</institution>
<location>Austin,</location>
<contexts>
<context position="14929" citStr="Amsler 1980" startWordPosition="2403" endWordPosition="2404"> value, and the important relations are to modifying words or phrases: customer 0 la n one that purchases a commodity or service... (customer AGENT purchase) aged 0 la aj of an advanced age (aged ISMIPL age) Other times the head word is best understood as a pointer to another relation: baptistery 0 0 n a part of a church ... used for baptism (baptistery PART church) agglutinate 2 1 vt to cause to adhere (agglutinate v-v-CAUSE adhere) Computational study of dictionary definitions has focused heavily on taxonomy because it can often be identified fairly reliably without parsing the definitions (Amsler 1980, Chodorow et al. 1985). Our emphasis in identifying relations, whether for information retrieval or for other purposes, has been on relations other than taxonomy (Evens et al. 1987). An important tool for identifying relations is the phrase count. A frequently occurring phrase, especially at the beginning of a definition, is often a &amp;quot;defining formula&amp;quot; (Smith 1985, Ahlswede and Evens 1987) that marks a relation. Another useful tool was very simplified hand parsing of definitions. This consisted of manually identifying the headword(s) and blocking off the &amp;quot;left adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (t</context>
</contexts>
<marker>Amsler, 1980</marker>
<rawString>Amsler, Robert A., 1980. &amp;quot;The Structure of the MerriamWebster Pocket Dictionary.&amp;quot; Ph.D. Dissertation. TR-164, Computer Science Dept., Univ. of Texas, Austin, Dec. 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Amsler</author>
</authors>
<title>Machine-Readable Dictionaries.&amp;quot; Annual Review</title>
<date>1984</date>
<booktitle>of Information Science and Technology,</booktitle>
<pages>19--161</pages>
<contexts>
<context position="4650" citStr="Amsler 1984" startWordPosition="702" endWordPosition="703">cooccur frequently like hole and dig. As a basis for automated thesaurus construction, we are trying to extract from machine-readable dictionaries triples, consisting of words or phrases linked by a labelled arc representing the relation. We plan to include phrases as well as words for both practical and theoretical reasons. It is well known that thesauri that include phrases are much more effective than those without, and we are believers in the phrasal lexicon described by Becker (1975). Our approach is first (see section 2) to apply text processing methods to machine readable dictionaries (Amsler 1984); next (see section 3) to analyze definitions from those dictionaries; then (see section 4) to merge that information (along with data in a large synonym file made available by Microlytics Inc.) into a large semantic network (see early work in Quillian 1968, and survey in Ritchie and Hanna 1984); and finally (see section 5) to test the utility of the resulting thesaurus using the SMART and CODER 101 experimental retrieval systems. We discuss preliminary results of all these aspects of our research program. 2. Dictionary text processing Since we wish to build a thesaurus with broad coverage, an</context>
</contexts>
<marker>Amsler, 1984</marker>
<rawString>Amsler, Robert A., 1984. &amp;quot;Machine-Readable Dictionaries.&amp;quot; Annual Review of Information Science and Technology, 19:161-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;euk D</author>
<author>A K</author>
</authors>
<title>Semantics and Lexicography: Towards a New Type of Unilingual Dictionary.&amp;quot;</title>
<date>1970</date>
<booktitle>In Studies in Syntax and Semantics,</booktitle>
<pages>1--33</pages>
<editor>F. Kiefer (ed.), Reidel,</editor>
<location>Dordrecht, Holland,</location>
<marker>D, K, 1970</marker>
<rawString>Apresyan, Yu. D., I. A. Mel&apos;euk and A. K. 2olkovsky, 1970. &amp;quot;Semantics and Lexicography: Towards a New Type of Unilingual Dictionary.&amp;quot; In Studies in Syntax and Semantics, F. Kiefer (ed.), Reidel, Dordrecht, Holland, 1-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Attar</author>
<author>Aviezri S Fraenkel</author>
</authors>
<title>Local Feedback in Full-Text Retrieval Systems.&amp;quot;</title>
<date>1977</date>
<journal>J. ACM,</journal>
<volume>24</volume>
<issue>3</issue>
<pages>397--417</pages>
<contexts>
<context position="2581" citStr="Attar and Fraenkel 1977" startWordPosition="382" endWordPosition="385">may be the most appropriate strategy for applying computational linguistics to information retrieval. While good results have been achieved in applying automatic procedures to find lexically related words based on local context in a * This material is based in part upon work supported by the National Science Foundation under Grant No&apos;s. 1ST8418877, IST-8510069, and IRI-8703580; by the Virginia Center for Innovative Technology under Grant No. INF85-016; and by AT&amp;T equipment contributions. **All correspondence regarding this paper should be addressed to the first author. particular collection (Attar and Fraenkel 1977), no reliable techniques exist for using collection statistics to build a thesaurus automatically, and manual construction of thesauri is both problematic and expensive (Svenonius 1986). Efforts to represent a significant portion of common sense knowledge will take years (Lenat et al. 1986), but developing knowledge resources to aid text processing is now feasible (Walker 1985). Encouraged by activities in identifying lexical and semantic information in other languages (especially since that reported in Apresyan et al. 1970), we decided to build a large, comprehensive lexicon for English that </context>
</contexts>
<marker>Attar, Fraenkel, 1977</marker>
<rawString>Attar, R. and Aviezri S. Fraenkel, 1977. &amp;quot;Local Feedback in Full-Text Retrieval Systems.&amp;quot; J. ACM, 24(3): 397-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Becker</author>
</authors>
<title>The Phrasal Lexicon.&amp;quot;</title>
<date>1975</date>
<booktitle>In Theoretical Issues in Natural Language Processing,</booktitle>
<pages>38--41</pages>
<editor>R. Schank and B. Nash-Webber (eds.), ACL,</editor>
<contexts>
<context position="4531" citStr="Becker (1975)" startWordPosition="684" endWordPosition="685"> Wednesday and hot — warm — cool — cold. Collocation relations are used to express the relationships between words that cooccur frequently like hole and dig. As a basis for automated thesaurus construction, we are trying to extract from machine-readable dictionaries triples, consisting of words or phrases linked by a labelled arc representing the relation. We plan to include phrases as well as words for both practical and theoretical reasons. It is well known that thesauri that include phrases are much more effective than those without, and we are believers in the phrasal lexicon described by Becker (1975). Our approach is first (see section 2) to apply text processing methods to machine readable dictionaries (Amsler 1984); next (see section 3) to analyze definitions from those dictionaries; then (see section 4) to merge that information (along with data in a large synonym file made available by Microlytics Inc.) into a large semantic network (see early work in Quillian 1968, and survey in Ritchie and Hanna 1984); and finally (see section 5) to test the utility of the resulting thesaurus using the SMART and CODER 101 experimental retrieval systems. We discuss preliminary results of all these as</context>
</contexts>
<marker>Becker, 1975</marker>
<rawString>Becker, Joseph, 1975. &amp;quot;The Phrasal Lexicon.&amp;quot; In Theoretical Issues in Natural Language Processing, R. Schank and B. Nash-Webber (eds.), ACL, 38-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Belkin</author>
</authors>
<title>Distributed Expert-Based Information Systems: An Interdisciplinary Approach.&amp;quot; Information Processing and Management,</title>
<date>1987</date>
<note>to appear.</note>
<marker>Belkin, 1987</marker>
<rawString>Belkin, N. et al., 1987. &amp;quot;Distributed Expert-Based Information Systems: An Interdisciplinary Approach.&amp;quot; Information Processing and Management, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Blair</author>
<author>M E Maron</author>
</authors>
<title>An Evaluation of Retrieval Effectiveness for a Full-Text DocumentRetrieval System.&amp;quot; Commun.</title>
<date>1985</date>
<pages>28--3</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1222" citStr="Blair and Maron 1985" startWordPosition="180" endWordPosition="183">thesaurus in the form of a large semantic network .to support interactive query expansion and search by end users. Our lexicon is being built by analyzing and merging data from several large English dictionaries; testing of its value for retrieval is with the SMART and CODER systems. 1. Introduction Though computer systems aiding retrieval from bibliographic or full-text databases have been available for more than two decades, it is only in recent years that many people are becoming concerned about the serious limitations of those systems regarding effectiveness in finding desired references (Blair and Maron 1985). Like others, though, we are convinced that easy-to-apply automatic methods can help solve this problem (see argument in Salton 1986). Indeed, automatic approaches seem essential since many end-users want to search without involving trained intermediaries (Ojala 1986). However, since the fundamental issue is one of mismatch between language use of document authors and inquirers, leading to uncertainties regarding whether a particular item should be retrieved (Chen and Dhar 1987), we are also convinced that computational linguistics is essential for a complete solution. Since many queries are </context>
</contexts>
<marker>Blair, Maron, 1985</marker>
<rawString>Blair, D.C. and M.E. Maron, 1985. &amp;quot;An Evaluation of Retrieval Effectiveness for a Full-Text DocumentRetrieval System.&amp;quot; Commun. ACM, 28(3):289-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsinchun Chen</author>
<author>Vasant Dhar</author>
</authors>
<title>Reducing Indeterminism in Consultation: A Cognitive Model of User/Librarian Interactions.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proc. AAAI-87,</booktitle>
<pages>285--289</pages>
<contexts>
<context position="1706" citStr="Chen and Dhar 1987" startWordPosition="249" endWordPosition="252">g concerned about the serious limitations of those systems regarding effectiveness in finding desired references (Blair and Maron 1985). Like others, though, we are convinced that easy-to-apply automatic methods can help solve this problem (see argument in Salton 1986). Indeed, automatic approaches seem essential since many end-users want to search without involving trained intermediaries (Ojala 1986). However, since the fundamental issue is one of mismatch between language use of document authors and inquirers, leading to uncertainties regarding whether a particular item should be retrieved (Chen and Dhar 1987), we are also convinced that computational linguistics is essential for a complete solution. Since many queries are simply sets of lexemes or phrases, and all queries can be reduced to that form, we believe that focusing on lexical and phrasal issues may be the most appropriate strategy for applying computational linguistics to information retrieval. While good results have been achieved in applying automatic procedures to find lexically related words based on local context in a * This material is based in part upon work supported by the National Science Foundation under Grant No&apos;s. 1ST8418877</context>
</contexts>
<marker>Chen, Dhar, 1987</marker>
<rawString>Chen, Hsinchun and Vasant Dhar, 1987. &amp;quot;Reducing Indeterminism in Consultation: A Cognitive Model of User/Librarian Interactions.&amp;quot; In Proc. AAAI-87, 285-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin S Chodorow</author>
<author>Roy Byrd</author>
<author>George Heidorn</author>
</authors>
<title>Extracting Semantic Hierarchies from a Large On-Line Dictionary.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proc. of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>299--304</pages>
<contexts>
<context position="14952" citStr="Chodorow et al. 1985" startWordPosition="2405" endWordPosition="2408">he important relations are to modifying words or phrases: customer 0 la n one that purchases a commodity or service... (customer AGENT purchase) aged 0 la aj of an advanced age (aged ISMIPL age) Other times the head word is best understood as a pointer to another relation: baptistery 0 0 n a part of a church ... used for baptism (baptistery PART church) agglutinate 2 1 vt to cause to adhere (agglutinate v-v-CAUSE adhere) Computational study of dictionary definitions has focused heavily on taxonomy because it can often be identified fairly reliably without parsing the definitions (Amsler 1980, Chodorow et al. 1985). Our emphasis in identifying relations, whether for information retrieval or for other purposes, has been on relations other than taxonomy (Evens et al. 1987). An important tool for identifying relations is the phrase count. A frequently occurring phrase, especially at the beginning of a definition, is often a &amp;quot;defining formula&amp;quot; (Smith 1985, Ahlswede and Evens 1987) that marks a relation. Another useful tool was very simplified hand parsing of definitions. This consisted of manually identifying the headword(s) and blocking off the &amp;quot;left adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (to use LSP terminology) </context>
</contexts>
<marker>Chodorow, Byrd, Heidorn, 1985</marker>
<rawString>Chodorow, Martin S., Roy Byrd, and George Heidorn, 1985. &amp;quot;Extracting Semantic Hierarchies from a Large On-Line Dictionary.&amp;quot; In Proc. of the 23rd Annual Meeting of the Association for Computational Linguistics, 299-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
<author>Roger Thompson</author>
</authors>
<title>I3R: A New Approach to the Design of Document Retrieval Systems.&amp;quot;</title>
<date>1987</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<note>in press.</note>
<contexts>
<context position="34293" citStr="Croft and Thompson 1987" startWordPosition="5460" endWordPosition="5463"> related terms to include from the lists produced from our thesaurus. Testing this hypothesis, however, requires a more flexible processing paradigm than we have employed in the past. Furthermore, we believe that inferencing using the information in the semantic network we are building can allow us to develop an effective automatic or semi-automatic scheme for &amp;quot;intelligent&amp;quot; query expansion. The CODER system should support these approaches. Building upon early efforts to build intelligent retrieval systems (Guida and Tasso 1983, Pollitt 1984) and learning from experiences with similar systems (Croft and Thompson 1987), we have been developing the CODER (COmposite Document Expert/effective/extended Retrieval) system (Fox and France 1987, Fox 1987) for the last three years. Though part of that effort deals with new approaches to automatic text analysis (Fox and Chen 1987), in the current context the most important aspect of CODER is that it is built as a distributed collection of &amp;quot;expert&amp;quot; modules (according to the models discussed in BeIlcin et al. 1987) programmed in Prolog or C, to support flexible testing of various Al approaches to information retrieval. Weaver and France have developed modules for handl</context>
</contexts>
<marker>Croft, Thompson, 1987</marker>
<rawString>Croft, W. Bruce and Roger Thompson, 1987. &amp;quot;I3R: A New Approach to the Design of Document Retrieval Systems.&amp;quot; J. Am. Soc. Inf. Sci., in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha W Evens</author>
<author>Raoul N Smith</author>
</authors>
<title>A Lexicon for a Computer Question-Answering System.&amp;quot;</title>
<date>1978</date>
<journal>Am. J. Comp. Ling.,</journal>
<volume>4</volume>
<pages>1--96</pages>
<contexts>
<context position="3395" citStr="Evens and Smith 1978" startWordPosition="504" endWordPosition="507">s to represent a significant portion of common sense knowledge will take years (Lenat et al. 1986), but developing knowledge resources to aid text processing is now feasible (Walker 1985). Encouraged by activities in identifying lexical and semantic information in other languages (especially since that reported in Apresyan et al. 1970), we decided to build a large, comprehensive lexicon for English that would include lexical and semantic relations (see comments below and survey in Evens et al. 1980) and be of use for information retrieval. Clearly this might also help with question answering (Evens and Smith 1978). Lexical and semantic relations provide a formal means for expressing relationships between words and concepts. Most commercial dictionaries give explicit mention to the classical relations, synonymy and antonymy, but many other relations are used in the definitions. Perhaps the most common is taxonomy, the relation between a word and its genus term, as in lion — Many noun definitions also contain the part-whole relation. Grading and queuing relations help describe words that come in sequences like Monday — Tuesday — Wednesday and hot — warm — cool — cold. Collocation relations are used to ex</context>
</contexts>
<marker>Evens, Smith, 1978</marker>
<rawString>Evens, Martha W. and Raoul N. Smith, 1978. &amp;quot;A Lexicon for a Computer Question-Answering System.&amp;quot; Am. J. Comp. Ling., No. 4, Microfiche 83: 1-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha W Evens</author>
<author>Bonnie C Litowitz</author>
<author>Judith A Markowitz</author>
<author>Raoul N Smith</author>
<author>Oswald Werner</author>
</authors>
<title>Lexical-Semantic Relations: A Comparative Survey. Linguistic Research,</title>
<date>1980</date>
<publisher>Inc.,</publisher>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="3278" citStr="Evens et al. 1980" startWordPosition="485" endWordPosition="488">urus automatically, and manual construction of thesauri is both problematic and expensive (Svenonius 1986). Efforts to represent a significant portion of common sense knowledge will take years (Lenat et al. 1986), but developing knowledge resources to aid text processing is now feasible (Walker 1985). Encouraged by activities in identifying lexical and semantic information in other languages (especially since that reported in Apresyan et al. 1970), we decided to build a large, comprehensive lexicon for English that would include lexical and semantic relations (see comments below and survey in Evens et al. 1980) and be of use for information retrieval. Clearly this might also help with question answering (Evens and Smith 1978). Lexical and semantic relations provide a formal means for expressing relationships between words and concepts. Most commercial dictionaries give explicit mention to the classical relations, synonymy and antonymy, but many other relations are used in the definitions. Perhaps the most common is taxonomy, the relation between a word and its genus term, as in lion — Many noun definitions also contain the part-whole relation. Grading and queuing relations help describe words that c</context>
</contexts>
<marker>Evens, Litowitz, Markowitz, Smith, Werner, 1980</marker>
<rawString>Evens, Martha W., Bonnie C. Litowitz, Judith A. Markowitz, Raoul N. Smith, and Oswald Werner, 1980. Lexical-Semantic Relations: A Comparative Survey. Linguistic Research, Inc., Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha W Evens</author>
<author>James Vandendorpe</author>
<author>Yih-Chen Wang</author>
</authors>
<title>Lexical-Semantic Relations in Information Retrieval.&amp;quot;</title>
<date>1985</date>
<booktitle>In Humans and Machines: The Interface Through Language,</booktitle>
<pages>73--100</pages>
<editor>S. Williams (ed.),</editor>
<location>Ablex, Norwood, NJ,</location>
<contexts>
<context position="32280" citStr="Evens et al. 1985" startWordPosition="5152" endWordPosition="5155">can then be associated with points in that space, and documents can be retrieved if &amp;quot;near&amp;quot; to the query. But since queries are typically short, it can be valuable to expand a query with terms related to the original set (especially due to variations in naming practices like those considered in Fumas et al. 1982). In our first experiment, involving a small collection of 82 documents, we found a mild improvement in system performance when all types of related terms (except antonyms) were involved in query expansion (Fox 1980). Similar benefits resulted when using a different, larger collection (Evens et al. 1985). In two later studies we used SMART but worked with Boolean queries. Query expansion then involved &amp;quot;ORing&amp;quot; in related terms with the original ones. Once again, improvements resulted, especially when the p-norm scheme for interpreting Boolean queries was applied (Fox 1983a, 1984). In all of these studies, lexical-semantic relations were identified manually for all query terms that were expanded. In other recent work with SMART, Fox, Miller and Sridaran used the same Boolean queries, but varied the TAX TAX ram MALE UNMARKED FEMALE UNMARKED CHILD UNMARKED HAS-PART PART TAX HAS-PART PART 106 sour</context>
</contexts>
<marker>Evens, Vandendorpe, Wang, 1985</marker>
<rawString>Evens, Martha W., James Vandendorpe, and Yih-Chen Wang, 1985. &amp;quot;Lexical-Semantic Relations in Information Retrieval.&amp;quot; In Humans and Machines: The Interface Through Language, S. Williams (ed.), Ablex, Norwood, NJ, 73-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha W Evens</author>
<author>Judith Markowitz</author>
<author>Thomas Ahlswede</author>
<author>Kay Rossi</author>
</authors>
<title>Digging in the Dictionary: Building a Relational Lexicon to Support Natural Language Processing Applications.&amp;quot;</title>
<date>1987</date>
<booktitle>IDEAL (Issues and Developments in English and Applied Linguistics)</booktitle>
<pages>2--33</pages>
<contexts>
<context position="15111" citStr="Evens et al. 1987" startWordPosition="2429" endWordPosition="2432">n advanced age (aged ISMIPL age) Other times the head word is best understood as a pointer to another relation: baptistery 0 0 n a part of a church ... used for baptism (baptistery PART church) agglutinate 2 1 vt to cause to adhere (agglutinate v-v-CAUSE adhere) Computational study of dictionary definitions has focused heavily on taxonomy because it can often be identified fairly reliably without parsing the definitions (Amsler 1980, Chodorow et al. 1985). Our emphasis in identifying relations, whether for information retrieval or for other purposes, has been on relations other than taxonomy (Evens et al. 1987). An important tool for identifying relations is the phrase count. A frequently occurring phrase, especially at the beginning of a definition, is often a &amp;quot;defining formula&amp;quot; (Smith 1985, Ahlswede and Evens 1987) that marks a relation. Another useful tool was very simplified hand parsing of definitions. This consisted of manually identifying the headword(s) and blocking off the &amp;quot;left adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (to use LSP terminology) of the headwords. For noun definitions, this was a very productive procedure; the right adjuncts of noun headwords, in particular, included many stereotyped ph</context>
</contexts>
<marker>Evens, Markowitz, Ahlswede, Rossi, 1987</marker>
<rawString>Evens, Martha W., Judith Markowitz, Thomas Ahlswede, and Kay Rossi, 1987. &amp;quot;Digging in the Dictionary: Building a Relational Lexicon to Support Natural Language Processing Applications.&amp;quot; IDEAL (Issues and Developments in English and Applied Linguistics) 2(33-44).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Fox</author>
</authors>
<title>Lexical Relations: Enhancing Effectiveness of Information Retrieval Systems.&amp;quot;</title>
<date>1980</date>
<journal>ACM SIGIR Forum,</journal>
<pages>15--3</pages>
<contexts>
<context position="32191" citStr="Fox 1980" startWordPosition="5141" endWordPosition="5142">m is associated with a different dimension in that space. Queries and documents can then be associated with points in that space, and documents can be retrieved if &amp;quot;near&amp;quot; to the query. But since queries are typically short, it can be valuable to expand a query with terms related to the original set (especially due to variations in naming practices like those considered in Fumas et al. 1982). In our first experiment, involving a small collection of 82 documents, we found a mild improvement in system performance when all types of related terms (except antonyms) were involved in query expansion (Fox 1980). Similar benefits resulted when using a different, larger collection (Evens et al. 1985). In two later studies we used SMART but worked with Boolean queries. Query expansion then involved &amp;quot;ORing&amp;quot; in related terms with the original ones. Once again, improvements resulted, especially when the p-norm scheme for interpreting Boolean queries was applied (Fox 1983a, 1984). In all of these studies, lexical-semantic relations were identified manually for all query terms that were expanded. In other recent work with SMART, Fox, Miller and Sridaran used the same Boolean queries, but varied the TAX TAX </context>
</contexts>
<marker>Fox, 1980</marker>
<rawString>Fox, E.A., 1980. &amp;quot;Lexical Relations: Enhancing Effectiveness of Information Retrieval Systems.&amp;quot; ACM SIGIR Forum, 15(3):5-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
</authors>
<title>Extending the Boolean and Vector Space Models of Information Retrieval with P-Norm Queries and Multiple Concept Types.&amp;quot; Dissertation,</title>
<date>1983</date>
<institution>Cornell University, University Microfilms Int.,</institution>
<location>Ann Arbor MI.</location>
<contexts>
<context position="31434" citStr="Fox 1983" startWordPosition="5017" endWordPosition="5018">ing down as well as up the taxonomic tree, finding e.g. &amp;quot;merino&amp;quot; and other sheep varieties from either sheep or wool. Figure 1. Representation of Lexical Relations involving Sheep 5. Testing with SMART and CODER Several studies have been undertaken regarding the use of lexical and semantic relations in information retrieval. Though one investigation involved use of a special system constructed at HT (Wang et al. 1985), most of the other work has involved the SMART system. The first version of SMART ran on IBM mainframes; a more modern form was developed to run under the UNIX operating system (Fox 1983b). In SMART, queries and documents are represented simply as sets of terms, so a multi-dimensional vector space can be constructed wherein each term is associated with a different dimension in that space. Queries and documents can then be associated with points in that space, and documents can be retrieved if &amp;quot;near&amp;quot; to the query. But since queries are typically short, it can be valuable to expand a query with terms related to the original set (especially due to variations in naming practices like those considered in Fumas et al. 1982). In our first experiment, involving a small collection of </context>
</contexts>
<marker>Fox, 1983</marker>
<rawString>Fox, Edward A., 1983a. &amp;quot;Extending the Boolean and Vector Space Models of Information Retrieval with P-Norm Queries and Multiple Concept Types.&amp;quot; Dissertation, Cornell University, University Microfilms Int., Ann Arbor MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
</authors>
<title>Some Considerations for Implementing the SMART Information Retrieval System under UNIX.&amp;quot;</title>
<date>1983</date>
<tech>TR 83-560,</tech>
<institution>Cornell Univ., Dept. of Comp. Sci..</institution>
<contexts>
<context position="31434" citStr="Fox 1983" startWordPosition="5017" endWordPosition="5018">ing down as well as up the taxonomic tree, finding e.g. &amp;quot;merino&amp;quot; and other sheep varieties from either sheep or wool. Figure 1. Representation of Lexical Relations involving Sheep 5. Testing with SMART and CODER Several studies have been undertaken regarding the use of lexical and semantic relations in information retrieval. Though one investigation involved use of a special system constructed at HT (Wang et al. 1985), most of the other work has involved the SMART system. The first version of SMART ran on IBM mainframes; a more modern form was developed to run under the UNIX operating system (Fox 1983b). In SMART, queries and documents are represented simply as sets of terms, so a multi-dimensional vector space can be constructed wherein each term is associated with a different dimension in that space. Queries and documents can then be associated with points in that space, and documents can be retrieved if &amp;quot;near&amp;quot; to the query. But since queries are typically short, it can be valuable to expand a query with terms related to the original set (especially due to variations in naming practices like those considered in Fumas et al. 1982). In our first experiment, involving a small collection of </context>
</contexts>
<marker>Fox, 1983</marker>
<rawString>Fox, Edward A., 1983b. &amp;quot;Some Considerations for Implementing the SMART Information Retrieval System under UNIX.&amp;quot; TR 83-560, Cornell Univ., Dept. of Comp. Sci..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
</authors>
<title>Improved Retrieval Using a Relational Thesaurus Expansion of Boolean Logic Queries.&amp;quot; In</title>
<date>1984</date>
<booktitle>Proc. Workshop Relational Models of the Lexicon,</booktitle>
<editor>Martha W. Evens (ed.), Stanford, CA,</editor>
<note>to appear.</note>
<marker>Fox, 1984</marker>
<rawString>Fox, Edward A., 1984. &amp;quot;Improved Retrieval Using a Relational Thesaurus Expansion of Boolean Logic Queries.&amp;quot; In Proc. Workshop Relational Models of the Lexicon, Martha W. Evens (ed.), Stanford, CA, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
</authors>
<title>Development of the CODER System: A Testbed for Artificial Intelligence Methods in Information Retrieval.&amp;quot;</title>
<date>1987</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="34424" citStr="Fox 1987" startWordPosition="5480" endWordPosition="5481">gm than we have employed in the past. Furthermore, we believe that inferencing using the information in the semantic network we are building can allow us to develop an effective automatic or semi-automatic scheme for &amp;quot;intelligent&amp;quot; query expansion. The CODER system should support these approaches. Building upon early efforts to build intelligent retrieval systems (Guida and Tasso 1983, Pollitt 1984) and learning from experiences with similar systems (Croft and Thompson 1987), we have been developing the CODER (COmposite Document Expert/effective/extended Retrieval) system (Fox and France 1987, Fox 1987) for the last three years. Though part of that effort deals with new approaches to automatic text analysis (Fox and Chen 1987), in the current context the most important aspect of CODER is that it is built as a distributed collection of &amp;quot;expert&amp;quot; modules (according to the models discussed in BeIlcin et al. 1987) programmed in Prolog or C, to support flexible testing of various Al approaches to information retrieval. Weaver and France have developed modules for handling lexical and semantic relations and a server module providing access to our version of the contents of CDEL. In the future, a mo</context>
</contexts>
<marker>Fox, 1987</marker>
<rawString>Fox, Edward A., 1987. &amp;quot;Development of the CODER System: A Testbed for Artificial Intelligence Methods in Information Retrieval.&amp;quot; Information Processing and Management, 23(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
<author>Qi-Fang Chen</author>
</authors>
<title>Text Analysis in the CODER System.&amp;quot; In</title>
<date>1987</date>
<booktitle>Proc. Fourth Annual USC Computer Science Symposium: Language and Data in Systems, Columbia SC,</booktitle>
<pages>7--14</pages>
<contexts>
<context position="34550" citStr="Fox and Chen 1987" startWordPosition="5500" endWordPosition="5503">network we are building can allow us to develop an effective automatic or semi-automatic scheme for &amp;quot;intelligent&amp;quot; query expansion. The CODER system should support these approaches. Building upon early efforts to build intelligent retrieval systems (Guida and Tasso 1983, Pollitt 1984) and learning from experiences with similar systems (Croft and Thompson 1987), we have been developing the CODER (COmposite Document Expert/effective/extended Retrieval) system (Fox and France 1987, Fox 1987) for the last three years. Though part of that effort deals with new approaches to automatic text analysis (Fox and Chen 1987), in the current context the most important aspect of CODER is that it is built as a distributed collection of &amp;quot;expert&amp;quot; modules (according to the models discussed in BeIlcin et al. 1987) programmed in Prolog or C, to support flexible testing of various Al approaches to information retrieval. Weaver and France have developed modules for handling lexical and semantic relations and a server module providing access to our version of the contents of CDEL. In the future, a module will be added to interface CODER with the SNePS semantic network so that further experiments can be undertaken. 6. Conclu</context>
</contexts>
<marker>Fox, Chen, 1987</marker>
<rawString>Fox, Edward A. and Qi-Fang Chen, 1987. &amp;quot;Text Analysis in the CODER System.&amp;quot; In Proc. Fourth Annual USC Computer Science Symposium: Language and Data in Systems, Columbia SC, 7-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
<author>Robert K France</author>
</authors>
<title>Architecture of an Expert System for Composite Document Analysis, Representation and Retrieval.&amp;quot;</title>
<date>1987</date>
<journal>Int. J. of Approximate Reasoning,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="34413" citStr="Fox and France 1987" startWordPosition="5476" endWordPosition="5479">ble processing paradigm than we have employed in the past. Furthermore, we believe that inferencing using the information in the semantic network we are building can allow us to develop an effective automatic or semi-automatic scheme for &amp;quot;intelligent&amp;quot; query expansion. The CODER system should support these approaches. Building upon early efforts to build intelligent retrieval systems (Guida and Tasso 1983, Pollitt 1984) and learning from experiences with similar systems (Croft and Thompson 1987), we have been developing the CODER (COmposite Document Expert/effective/extended Retrieval) system (Fox and France 1987, Fox 1987) for the last three years. Though part of that effort deals with new approaches to automatic text analysis (Fox and Chen 1987), in the current context the most important aspect of CODER is that it is built as a distributed collection of &amp;quot;expert&amp;quot; modules (according to the models discussed in BeIlcin et al. 1987) programmed in Prolog or C, to support flexible testing of various Al approaches to information retrieval. Weaver and France have developed modules for handling lexical and semantic relations and a server module providing access to our version of the contents of CDEL. In the f</context>
</contexts>
<marker>Fox, France, 1987</marker>
<rawString>Fox, Edward A. and Robert K. France, 1987. &amp;quot;Architecture of an Expert System for Composite Document Analysis, Representation and Retrieval.&amp;quot; Int. J. of Approximate Reasoning, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
<author>Robert C Wohlwend</author>
<author>Phyllis R Sheldon</author>
<author>Qi Fan Chen</author>
<author>Robert K France</author>
</authors>
<title>Building the CODER Lexicon The Collins English Dictionary and Its Adverb Definitions.&amp;quot;</title>
<date>1986</date>
<tech>TR-86-23,</tech>
<institution>VPI&amp;SU Computer Science Dept.,</institution>
<location>Blacksburg, VA.</location>
<contexts>
<context position="6718" citStr="Fox et al. 1986" startWordPosition="1044" endWordPosition="1047"> senses per entry there are many words in CDEL with more senses, on up through one entry with 50 senses. Wohlwend used various UNIX (trademark of AT&amp;T Bell Laboratories) text processing tools and developed analyzers with lex and yacc. The main processing involved nine passes through the data by our analyzers, with a small amount of manual checking and correction between steps. By the fall of 1986 Wohlwend, France and Chen had extracted all suitable data from the CDEL tape, placed it in the form of facts that could be loaded into a Prolog system, and collected statistics regarding occurrences (Fox et al. 1986). Valuable information was present to aid in our work with the CODER &amp;quot;expert&amp;quot; information retrieval system (France and Fox 1986). Recently J. Weiss has completed manual checking and editing of the data, and has refined some of the automatic analysis (e.g., separating past tense forms from other irregulars). Later we will load the bulk of that into a semantic network and carry out further processing on the definition portions. 2.2. Webster&apos;s Seventh Collegiate Dictionary We received our machine-readable W7 from Raoul Smith on five tapes in Olney&apos;s original format (Olney 1968). Our first piece o</context>
</contexts>
<marker>Fox, Wohlwend, Sheldon, Chen, France, 1986</marker>
<rawString>Fox, Edward A., Robert C. Wohlwend, Phyllis R. Sheldon, Qi Fan Chen, and Robert K. France, 1986. &amp;quot;Building the CODER Lexicon The Collins English Dictionary and Its Adverb Definitions.&amp;quot; TR-86-23, VPI&amp;SU Computer Science Dept., Blacksburg, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert K France</author>
<author>Edward A Fox</author>
</authors>
<title>Knowledge Structures for Information Retrieval: Representation in the CODER Project.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings IEEE Expert Systems in Government Conference,</booktitle>
<pages>135--141</pages>
<location>McLean VA,</location>
<contexts>
<context position="6846" citStr="France and Fox 1986" startWordPosition="1064" endWordPosition="1067">us UNIX (trademark of AT&amp;T Bell Laboratories) text processing tools and developed analyzers with lex and yacc. The main processing involved nine passes through the data by our analyzers, with a small amount of manual checking and correction between steps. By the fall of 1986 Wohlwend, France and Chen had extracted all suitable data from the CDEL tape, placed it in the form of facts that could be loaded into a Prolog system, and collected statistics regarding occurrences (Fox et al. 1986). Valuable information was present to aid in our work with the CODER &amp;quot;expert&amp;quot; information retrieval system (France and Fox 1986). Recently J. Weiss has completed manual checking and editing of the data, and has refined some of the automatic analysis (e.g., separating past tense forms from other irregulars). Later we will load the bulk of that into a semantic network and carry out further processing on the definition portions. 2.2. Webster&apos;s Seventh Collegiate Dictionary We received our machine-readable W7 from Raoul Smith on five tapes in Olney&apos;s original format (Olney 1968). Our first piece of text processing was to compress this huge mass of data (approximately 120 megabytes) into a manageable format. AhLswede wrote </context>
</contexts>
<marker>France, Fox, 1986</marker>
<rawString>France, Robert K. and Edward A. Fox, 1986. &amp;quot;Knowledge Structures for Information Retrieval: Representation in the CODER Project.&amp;quot; In Proceedings IEEE Expert Systems in Government Conference, October 20-24, 1986, McLean VA, 135-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George W Furnas</author>
</authors>
<title>Statistical semantics: how can a computer use what people name things to guess what things people mean when they name things.&amp;quot;</title>
<date>1982</date>
<booktitle>In Proc. of the Human Factors in Computer Systems Conference,</booktitle>
<publisher>for Computing Machinery,</publisher>
<location>Gaithersburg, MD, Assoc.</location>
<marker>Furnas, 1982</marker>
<rawString>Furnas, George W. et al., 1982. &amp;quot;Statistical semantics: how can a computer use what people name things to guess what things people mean when they name things.&amp;quot; In Proc. of the Human Factors in Computer Systems Conference, Gaithersburg, MD, Assoc. for Computing Machinery, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Guida</author>
<author>C Tasso</author>
</authors>
<title>An expert intermediary system for interactive document retrieval.&amp;quot;</title>
<date>1983</date>
<journal>Automatica</journal>
<volume>19</volume>
<issue>6</issue>
<pages>759--766</pages>
<contexts>
<context position="34201" citStr="Guida and Tasso 1983" startWordPosition="5447" endWordPosition="5450"> in the process, so they can decide which words should be expanded, and can select which related terms to include from the lists produced from our thesaurus. Testing this hypothesis, however, requires a more flexible processing paradigm than we have employed in the past. Furthermore, we believe that inferencing using the information in the semantic network we are building can allow us to develop an effective automatic or semi-automatic scheme for &amp;quot;intelligent&amp;quot; query expansion. The CODER system should support these approaches. Building upon early efforts to build intelligent retrieval systems (Guida and Tasso 1983, Pollitt 1984) and learning from experiences with similar systems (Croft and Thompson 1987), we have been developing the CODER (COmposite Document Expert/effective/extended Retrieval) system (Fox and France 1987, Fox 1987) for the last three years. Though part of that effort deals with new approaches to automatic text analysis (Fox and Chen 1987), in the current context the most important aspect of CODER is that it is built as a distributed collection of &amp;quot;expert&amp;quot; modules (according to the models discussed in BeIlcin et al. 1987) programmed in Prolog or C, to support flexible testing of variou</context>
</contexts>
<marker>Guida, Tasso, 1983</marker>
<rawString>Guida, G., and C. Tasso, 1983. &amp;quot;An expert intermediary system for interactive document retrieval.&amp;quot; Automatica 19(6): 759-766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard G Hull</author>
</authors>
<title>A New Design for SNIP the SNePS Inference Package.&amp;quot;</title>
<date>1986</date>
<tech>SNeRG Technical Report 86-10,</tech>
<institution>Department of Computer Science, SUNY at Buffalo.</institution>
<contexts>
<context position="24820" citStr="Hull 1986" startWordPosition="3998" endWordPosition="3999">ouns, adjectives, and transitive verbs. The improvement in performance time is less dramatic, particularly in the case of noun definitions, where postnominal phrases are much more common than in intransitive verb definitions. These phrases are harder to identify, so more manual intervention is necessary. 4. Constructing the semantic network The SNePS semantic network system (Shapiro 1979b) is one of relatively few knowledge representation schemes that permit a unified representation of associative information and predicate-logic-style inference (for details on the logic, see Shapiro 1979a and Hull 1986) enhanced with default reasoning capability (Nutter 1983). In SNePS, every node represents a concept (concepts include lexemes, word senses, individuals, relations, propositions, and any other potential objects of the system&apos;s knowledge). Conversely, every concept is represented by a node, and no two nodes represent the same concept (although the concepts they represent may refer to the same object; for more on this and other principles underlying the design of SNePS, see Shapiro and Rapaport 1986). Logical and structuring information are carried on arcs. All explicit information about a conce</context>
</contexts>
<marker>Hull, 1986</marker>
<rawString>Hull, Richard G., 1986. &amp;quot;A New Design for SNIP the SNePS Inference Package.&amp;quot; SNeRG Technical Report 86-10, Department of Computer Science, SUNY at Buffalo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Lenat</author>
</authors>
<title>CYC: Using Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks.&amp;quot; The Al Magazine,</title>
<date>1986</date>
<pages>65--84</pages>
<marker>Lenat, 1986</marker>
<rawString>Lenat, Doug et al., 1986. &amp;quot;CYC: Using Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks.&amp;quot; The Al Magazine, 65-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Terry Nutter</author>
</authors>
<title>Default reasoning using monotonic logic: a modest proposal.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proc. AAAI-83, The National Conf. on AI, Washington D.C.,</booktitle>
<pages>297--300</pages>
<contexts>
<context position="24877" citStr="Nutter 1983" startWordPosition="4005" endWordPosition="4006"> in performance time is less dramatic, particularly in the case of noun definitions, where postnominal phrases are much more common than in intransitive verb definitions. These phrases are harder to identify, so more manual intervention is necessary. 4. Constructing the semantic network The SNePS semantic network system (Shapiro 1979b) is one of relatively few knowledge representation schemes that permit a unified representation of associative information and predicate-logic-style inference (for details on the logic, see Shapiro 1979a and Hull 1986) enhanced with default reasoning capability (Nutter 1983). In SNePS, every node represents a concept (concepts include lexemes, word senses, individuals, relations, propositions, and any other potential objects of the system&apos;s knowledge). Conversely, every concept is represented by a node, and no two nodes represent the same concept (although the concepts they represent may refer to the same object; for more on this and other principles underlying the design of SNePS, see Shapiro and Rapaport 1986). Logical and structuring information are carried on arcs. All explicit information about a concept is directly linked to its node, with structure sharing</context>
</contexts>
<marker>Nutter, 1983</marker>
<rawString>Nutter, J. Terry, 1983. &amp;quot;Default reasoning using monotonic logic: a modest proposal.&amp;quot; In Proc. AAAI-83, The National Conf. on AI, Washington D.C., 297-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marydee Ojala</author>
</authors>
<title>Views on End-User Searching.&amp;quot;</title>
<date>1986</date>
<journal>J. Am. Soc. If. Sci.,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>197--203</pages>
<contexts>
<context position="1491" citStr="Ojala 1986" startWordPosition="219" endWordPosition="220">tems. 1. Introduction Though computer systems aiding retrieval from bibliographic or full-text databases have been available for more than two decades, it is only in recent years that many people are becoming concerned about the serious limitations of those systems regarding effectiveness in finding desired references (Blair and Maron 1985). Like others, though, we are convinced that easy-to-apply automatic methods can help solve this problem (see argument in Salton 1986). Indeed, automatic approaches seem essential since many end-users want to search without involving trained intermediaries (Ojala 1986). However, since the fundamental issue is one of mismatch between language use of document authors and inquirers, leading to uncertainties regarding whether a particular item should be retrieved (Chen and Dhar 1987), we are also convinced that computational linguistics is essential for a complete solution. Since many queries are simply sets of lexemes or phrases, and all queries can be reduced to that form, we believe that focusing on lexical and phrasal issues may be the most appropriate strategy for applying computational linguistics to information retrieval. While good results have been ach</context>
</contexts>
<marker>Ojala, 1986</marker>
<rawString>Ojala, Marydee, 1986. &amp;quot;Views on End-User Searching.&amp;quot; J. Am. Soc. If. Sci., 37(4), 197-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Olney</author>
</authors>
<title>To All Interested in the MerriamWebster Transcripts and Data Derived from Them.&amp;quot; Systems Development Corporation Document L13579.</title>
<date>1968</date>
<contexts>
<context position="7299" citStr="Olney 1968" startWordPosition="1137" endWordPosition="1138"> occurrences (Fox et al. 1986). Valuable information was present to aid in our work with the CODER &amp;quot;expert&amp;quot; information retrieval system (France and Fox 1986). Recently J. Weiss has completed manual checking and editing of the data, and has refined some of the automatic analysis (e.g., separating past tense forms from other irregulars). Later we will load the bulk of that into a semantic network and carry out further processing on the definition portions. 2.2. Webster&apos;s Seventh Collegiate Dictionary We received our machine-readable W7 from Raoul Smith on five tapes in Olney&apos;s original format (Olney 1968). Our first piece of text processing was to compress this huge mass of data (approximately 120 megabytes) into a manageable format. AhLswede wrote a C program compress which converted the tape data into a format based on that of Peterson (1982), with a few differences to simplify our analysis. The resulting version occupied 15,676,249 bytes. The synonymy relation is particularly easy to recognize, since it is explicitly tagged — in the printed dictionary synonyms appear in small capitals. Thus the first step in adding relations to our lexical database was to extract the 45,910 synonymy relatio</context>
</contexts>
<marker>Olney, 1968</marker>
<rawString>Olney, John, 1968. &apos;To All Interested in the MerriamWebster Transcripts and Data Derived from Them.&amp;quot; Systems Development Corporation Document L13579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L Peterson</author>
</authors>
<title>Webster&apos;s Seventh New Collegiate Dictionary: A Computer-Readable File Format.&amp;quot;</title>
<date>1982</date>
<tech>TR-196,</tech>
<institution>Dept. of Comp. Sci., Univ. of Texas,</institution>
<location>Austin.</location>
<contexts>
<context position="7543" citStr="Peterson (1982)" startWordPosition="1178" endWordPosition="1179"> refined some of the automatic analysis (e.g., separating past tense forms from other irregulars). Later we will load the bulk of that into a semantic network and carry out further processing on the definition portions. 2.2. Webster&apos;s Seventh Collegiate Dictionary We received our machine-readable W7 from Raoul Smith on five tapes in Olney&apos;s original format (Olney 1968). Our first piece of text processing was to compress this huge mass of data (approximately 120 megabytes) into a manageable format. AhLswede wrote a C program compress which converted the tape data into a format based on that of Peterson (1982), with a few differences to simplify our analysis. The resulting version occupied 15,676,249 bytes. The synonymy relation is particularly easy to recognize, since it is explicitly tagged — in the printed dictionary synonyms appear in small capitals. Thus the first step in adding relations to our lexical database was to extract the 45,910 synonymy relationships marked explicitly in this way using the UNIX awk utility and insert them in a table of word-relation-word triples (AhLswede 1985). Morphology also provides an analytical tool for the extraction of relations. One fruitful source is the pr</context>
</contexts>
<marker>Peterson, 1982</marker>
<rawString>Peterson, James L., 1982. &amp;quot;Webster&apos;s Seventh New Collegiate Dictionary: A Computer-Readable File Format.&amp;quot; TR-196, Dept. of Comp. Sci., Univ. of Texas, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Pollitt</author>
</authors>
<title>A &apos;front-end&apos; system: an Expert System as an online search intermediary.&amp;quot;</title>
<date>1984</date>
<journal>Aslib Proceedings,</journal>
<volume>36</volume>
<issue>5</issue>
<pages>229--234</pages>
<contexts>
<context position="34216" citStr="Pollitt 1984" startWordPosition="5451" endWordPosition="5452">ey can decide which words should be expanded, and can select which related terms to include from the lists produced from our thesaurus. Testing this hypothesis, however, requires a more flexible processing paradigm than we have employed in the past. Furthermore, we believe that inferencing using the information in the semantic network we are building can allow us to develop an effective automatic or semi-automatic scheme for &amp;quot;intelligent&amp;quot; query expansion. The CODER system should support these approaches. Building upon early efforts to build intelligent retrieval systems (Guida and Tasso 1983, Pollitt 1984) and learning from experiences with similar systems (Croft and Thompson 1987), we have been developing the CODER (COmposite Document Expert/effective/extended Retrieval) system (Fox and France 1987, Fox 1987) for the last three years. Though part of that effort deals with new approaches to automatic text analysis (Fox and Chen 1987), in the current context the most important aspect of CODER is that it is built as a distributed collection of &amp;quot;expert&amp;quot; modules (according to the models discussed in BeIlcin et al. 1987) programmed in Prolog or C, to support flexible testing of various Al approaches</context>
</contexts>
<marker>Pollitt, 1984</marker>
<rawString>Pollitt, S.E., 1984. &amp;quot;A &apos;front-end&apos; system: an Expert System as an online search intermediary.&amp;quot; Aslib Proceedings, 36(5): 229-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ross</author>
</authors>
<title>Semantic Memory.&amp;quot;</title>
<date>1968</date>
<booktitle>In Semantic Information Processing, Marvin Minsky ed.</booktitle>
<pages>227--270</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA:</location>
<marker>Ross, 1968</marker>
<rawString>Quillian. M. Ross, 1968. &amp;quot;Semantic Memory.&amp;quot; In Semantic Information Processing, Marvin Minsky ed. Cambridge, MA: MIT Press, 227-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Ritchie</author>
<author>F K Hanna</author>
</authors>
<title>Semantic Networks: a General Definition and a Survey.&amp;quot; lnf Tech.: Res.</title>
<date>1984</date>
<journal>Dev. Applications,</journal>
<pages>3--1</pages>
<contexts>
<context position="4946" citStr="Ritchie and Hanna 1984" startWordPosition="749" endWordPosition="752">r both practical and theoretical reasons. It is well known that thesauri that include phrases are much more effective than those without, and we are believers in the phrasal lexicon described by Becker (1975). Our approach is first (see section 2) to apply text processing methods to machine readable dictionaries (Amsler 1984); next (see section 3) to analyze definitions from those dictionaries; then (see section 4) to merge that information (along with data in a large synonym file made available by Microlytics Inc.) into a large semantic network (see early work in Quillian 1968, and survey in Ritchie and Hanna 1984); and finally (see section 5) to test the utility of the resulting thesaurus using the SMART and CODER 101 experimental retrieval systems. We discuss preliminary results of all these aspects of our research program. 2. Dictionary text processing Since we wish to build a thesaurus with broad coverage, and since each dictionary has unique advantages, we plan to use several. The bulk of our work to date has been with the Collins Dictionary of the English Language (CDEL) and Webster&apos;s Seventh New Collegiate Dictionary (W7). 2.1. Collins Dictionary of the English Language In 1985 we obtained a magn</context>
</contexts>
<marker>Ritchie, Hanna, 1984</marker>
<rawString>Ritchie, C.D. and Hanna, F.K., 1984. &amp;quot;Semantic Networks: a General Definition and a Survey.&amp;quot; lnf Tech.: Res. Dev. Applications, 3(1):33-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
</authors>
<title>Natural Language Information Processing.&amp;quot;</title>
<date>1981</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="15970" citStr="Sager (1981)" startWordPosition="2560" endWordPosition="2561">l tool was very simplified hand parsing of definitions. This consisted of manually identifying the headword(s) and blocking off the &amp;quot;left adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (to use LSP terminology) of the headwords. For noun definitions, this was a very productive procedure; the right adjuncts of noun headwords, in particular, included many stereotyped phrase structures which could be associated with relations. Adjuncts in verb definitions were more varied and of less help in identifying relations. 3.4. Developing a grammar The grammar of dictionary definitions is based on the grammar provided in Sager (1981). The great majority of changes to that grammar have been of two types: 1. Adaptations to the top-level syntax of W7. The LSP grammar is based on a medical sublanguage consisting of complete declarative sentences, as well as questions, imperatives, and some other sentence types. The texts of W7 are never complete sentences; rather they are phrases syntactically parallel to the words they define: noun phrases for nouns, infinitive verb phrases for verbs, etc. It would be possible in principle to use existing LSP phrase structures to represent most definition texts, although some enhancement of </context>
<context position="20347" citStr="Sager (1981)" startWordPosition="3270" endWordPosition="3271">category was a set of recurring syntactic relations that we speculated would prove to have consistent semantic significance. Some of these were familiar case relations: there were 448 verbal nouns, 124 adjectives derived in one way or another from verbs, etc. This category also included, for example, relations such as &amp;quot;vobj&amp;quot;, the relation between a verb and the direct object of its defining headword. The third category consisted of syntactic relations which we simply noted with the idea of later doing cluster analysis to determine selectional categories in the dictionary, much as described by Sager (1981). These included 2,694 &amp;quot;permissible modifiers&amp;quot;, adjectives modifier-noun pairs; 182 &amp;quot;permissible subjects&amp;quot;, nouns appearing as subjects of verbs; and so on. Definitions which failed to parse did so for a great many reasons; we may be near the point of diminishing returns in terms of refining the grammar to parse every definition. As the third column indicates, many definitions yielded multiple parses. Multiple parses were responsible for most of the duplicate relational arcs that were generated. The quality of these parses is an important issue. A &amp;quot;good&amp;quot; parse is one consistent with the way co</context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, Naomi, 1981, Natural Language Information Processing.&amp;quot; Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Another Look at Automatic Text-Retrieval Systems.&amp;quot;</title>
<date>1986</date>
<journal>Commun. ACM,</journal>
<volume>29</volume>
<issue>7</issue>
<pages>648--656</pages>
<contexts>
<context position="1356" citStr="Salton 1986" startWordPosition="202" endWordPosition="203">y analyzing and merging data from several large English dictionaries; testing of its value for retrieval is with the SMART and CODER systems. 1. Introduction Though computer systems aiding retrieval from bibliographic or full-text databases have been available for more than two decades, it is only in recent years that many people are becoming concerned about the serious limitations of those systems regarding effectiveness in finding desired references (Blair and Maron 1985). Like others, though, we are convinced that easy-to-apply automatic methods can help solve this problem (see argument in Salton 1986). Indeed, automatic approaches seem essential since many end-users want to search without involving trained intermediaries (Ojala 1986). However, since the fundamental issue is one of mismatch between language use of document authors and inquirers, leading to uncertainties regarding whether a particular item should be retrieved (Chen and Dhar 1987), we are also convinced that computational linguistics is essential for a complete solution. Since many queries are simply sets of lexemes or phrases, and all queries can be reduced to that form, we believe that focusing on lexical and phrasal issues</context>
</contexts>
<marker>Salton, 1986</marker>
<rawString>Salton, G., 1986. &amp;quot;Another Look at Automatic Text-Retrieval Systems.&amp;quot; Commun. ACM, 29(7): 648-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart C Shapiro</author>
</authors>
<title>Generalized Augmented Transition Network Grammars for Generation from Semantic Networks.&amp;quot;</title>
<date>1979</date>
<booktitle>In Proc. of the 17th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--29</pages>
<contexts>
<context position="24600" citStr="Shapiro 1979" startWordPosition="3968" endWordPosition="3969">r head finding algorithm does not yet catch adverbial particles such as those in give up or bring about, or idiomatic direct objects as in take place. We are now in the process of extracting relations by this process for nouns, adjectives, and transitive verbs. The improvement in performance time is less dramatic, particularly in the case of noun definitions, where postnominal phrases are much more common than in intransitive verb definitions. These phrases are harder to identify, so more manual intervention is necessary. 4. Constructing the semantic network The SNePS semantic network system (Shapiro 1979b) is one of relatively few knowledge representation schemes that permit a unified representation of associative information and predicate-logic-style inference (for details on the logic, see Shapiro 1979a and Hull 1986) enhanced with default reasoning capability (Nutter 1983). In SNePS, every node represents a concept (concepts include lexemes, word senses, individuals, relations, propositions, and any other potential objects of the system&apos;s knowledge). Conversely, every concept is represented by a node, and no two nodes represent the same concept (although the concepts they represent may ref</context>
</contexts>
<marker>Shapiro, 1979</marker>
<rawString>Shapiro, Stuart C., 1979a. &amp;quot;Generalized Augmented Transition Network Grammars for Generation from Semantic Networks.&amp;quot; In Proc. of the 17th Annual Meeting of the Association for Computational Linguistics, 25-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart C Shapiro</author>
</authors>
<title>The SNePS Semantic Network Processing System.&amp;quot;</title>
<date>1979</date>
<pages>179--203</pages>
<editor>In Associative Networks, Nick Findler (ed.),</editor>
<publisher>Academic Press,</publisher>
<location>NY,</location>
<contexts>
<context position="24600" citStr="Shapiro 1979" startWordPosition="3968" endWordPosition="3969">r head finding algorithm does not yet catch adverbial particles such as those in give up or bring about, or idiomatic direct objects as in take place. We are now in the process of extracting relations by this process for nouns, adjectives, and transitive verbs. The improvement in performance time is less dramatic, particularly in the case of noun definitions, where postnominal phrases are much more common than in intransitive verb definitions. These phrases are harder to identify, so more manual intervention is necessary. 4. Constructing the semantic network The SNePS semantic network system (Shapiro 1979b) is one of relatively few knowledge representation schemes that permit a unified representation of associative information and predicate-logic-style inference (for details on the logic, see Shapiro 1979a and Hull 1986) enhanced with default reasoning capability (Nutter 1983). In SNePS, every node represents a concept (concepts include lexemes, word senses, individuals, relations, propositions, and any other potential objects of the system&apos;s knowledge). Conversely, every concept is represented by a node, and no two nodes represent the same concept (although the concepts they represent may ref</context>
</contexts>
<marker>Shapiro, 1979</marker>
<rawString>Shapiro, Stuart C., 1979b. &amp;quot;The SNePS Semantic Network Processing System.&amp;quot; In Associative Networks, Nick Findler (ed.), Academic Press, NY, 179-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart C Shapiro</author>
<author>William Rapaport</author>
</authors>
<title>SNePS Considered as a Fully Intensional Propositional Semantic Network.&amp;quot; In</title>
<date>1986</date>
<booktitle>Proc. AAAI-86, Fifth National Conf on Al,</booktitle>
<pages>278--283</pages>
<location>Phil. PA,</location>
<contexts>
<context position="25323" citStr="Shapiro and Rapaport 1986" startWordPosition="4071" endWordPosition="4074"> of associative information and predicate-logic-style inference (for details on the logic, see Shapiro 1979a and Hull 1986) enhanced with default reasoning capability (Nutter 1983). In SNePS, every node represents a concept (concepts include lexemes, word senses, individuals, relations, propositions, and any other potential objects of the system&apos;s knowledge). Conversely, every concept is represented by a node, and no two nodes represent the same concept (although the concepts they represent may refer to the same object; for more on this and other principles underlying the design of SNePS, see Shapiro and Rapaport 1986). Logical and structuring information are carried on arcs. All explicit information about a concept is directly linked to its node, with structure sharing across propositions. It follows that every detected synonym of a given word sense, for instance, is connected to that sense by simply definable paths to the nodes representing the original word sense and the lexical relation SYNONYM. This simplifies finding related terms, eliminating much of the look-up necessary in schemes that are superficially more logic-like. The lexicon we are forming contains three different kinds of information: lexic</context>
</contexts>
<marker>Shapiro, Rapaport, 1986</marker>
<rawString>Shapiro, Stuart C. and William Rapaport, 1986. &amp;quot;SNePS Considered as a Fully Intensional Propositional Semantic Network.&amp;quot; In Proc. AAAI-86, Fifth National Conf on Al, Phil. PA, 278-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raoul N Smith</author>
</authors>
<title>Conceptual primitives in the English lexicon.&amp;quot;</title>
<date>1985</date>
<journal>Papers in Linguistics</journal>
<volume>18</volume>
<pages>99--137</pages>
<contexts>
<context position="15295" citStr="Smith 1985" startWordPosition="2459" endWordPosition="2460">) agglutinate 2 1 vt to cause to adhere (agglutinate v-v-CAUSE adhere) Computational study of dictionary definitions has focused heavily on taxonomy because it can often be identified fairly reliably without parsing the definitions (Amsler 1980, Chodorow et al. 1985). Our emphasis in identifying relations, whether for information retrieval or for other purposes, has been on relations other than taxonomy (Evens et al. 1987). An important tool for identifying relations is the phrase count. A frequently occurring phrase, especially at the beginning of a definition, is often a &amp;quot;defining formula&amp;quot; (Smith 1985, Ahlswede and Evens 1987) that marks a relation. Another useful tool was very simplified hand parsing of definitions. This consisted of manually identifying the headword(s) and blocking off the &amp;quot;left adjuncts&amp;quot; and &amp;quot;right adjuncts&amp;quot; (to use LSP terminology) of the headwords. For noun definitions, this was a very productive procedure; the right adjuncts of noun headwords, in particular, included many stereotyped phrase structures which could be associated with relations. Adjuncts in verb definitions were more varied and of less help in identifying relations. 3.4. Developing a grammar The grammar</context>
</contexts>
<marker>Smith, 1985</marker>
<rawString>Smith, Raoul N., 1985. &amp;quot;Conceptual primitives in the English lexicon.&amp;quot; Papers in Linguistics 18: 99-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Svenonius</author>
</authors>
<title>Unanswered Questions in the Design of Controlled Vocabularies.&amp;quot;</title>
<date>1986</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<pages>37--5</pages>
<contexts>
<context position="2766" citStr="Svenonius 1986" startWordPosition="409" endWordPosition="410">ated words based on local context in a * This material is based in part upon work supported by the National Science Foundation under Grant No&apos;s. 1ST8418877, IST-8510069, and IRI-8703580; by the Virginia Center for Innovative Technology under Grant No. INF85-016; and by AT&amp;T equipment contributions. **All correspondence regarding this paper should be addressed to the first author. particular collection (Attar and Fraenkel 1977), no reliable techniques exist for using collection statistics to build a thesaurus automatically, and manual construction of thesauri is both problematic and expensive (Svenonius 1986). Efforts to represent a significant portion of common sense knowledge will take years (Lenat et al. 1986), but developing knowledge resources to aid text processing is now feasible (Walker 1985). Encouraged by activities in identifying lexical and semantic information in other languages (especially since that reported in Apresyan et al. 1970), we decided to build a large, comprehensive lexicon for English that would include lexical and semantic relations (see comments below and survey in Evens et al. 1980) and be of use for information retrieval. Clearly this might also help with question ans</context>
</contexts>
<marker>Svenonius, 1986</marker>
<rawString>Svenonius, Elaine, 1986. &amp;quot;Unanswered Questions in the Design of Controlled Vocabularies.&amp;quot; J. Am. Soc. Inf. Sci., 37(5):331-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Walker</author>
</authors>
<title>Knowledge Resource Tools for Accessing Large Text Files.&amp;quot; In</title>
<date>1985</date>
<booktitle>Proc. First Conference of the UW Centre for the New Oxford English Dictionary: Information in Data.</booktitle>
<pages>11--24</pages>
<location>Waterloo, Canada,</location>
<contexts>
<context position="2961" citStr="Walker 1985" startWordPosition="439" endWordPosition="440">inia Center for Innovative Technology under Grant No. INF85-016; and by AT&amp;T equipment contributions. **All correspondence regarding this paper should be addressed to the first author. particular collection (Attar and Fraenkel 1977), no reliable techniques exist for using collection statistics to build a thesaurus automatically, and manual construction of thesauri is both problematic and expensive (Svenonius 1986). Efforts to represent a significant portion of common sense knowledge will take years (Lenat et al. 1986), but developing knowledge resources to aid text processing is now feasible (Walker 1985). Encouraged by activities in identifying lexical and semantic information in other languages (especially since that reported in Apresyan et al. 1970), we decided to build a large, comprehensive lexicon for English that would include lexical and semantic relations (see comments below and survey in Evens et al. 1980) and be of use for information retrieval. Clearly this might also help with question answering (Evens and Smith 1978). Lexical and semantic relations provide a formal means for expressing relationships between words and concepts. Most commercial dictionaries give explicit mention to</context>
</contexts>
<marker>Walker, 1985</marker>
<rawString>Walker, Donald E., 1985. &amp;quot;Knowledge Resource Tools for Accessing Large Text Files.&amp;quot; In Proc. First Conference of the UW Centre for the New Oxford English Dictionary: Information in Data. Nov. 6-7, 1985, Waterloo, Canada, 11-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yih-Chen Wang</author>
<author>James Vandendorpe</author>
<author>Martha Evens</author>
</authors>
<title>Relational Thesauri in Information Retrieval.&amp;quot;</title>
<date>1985</date>
<journal>J. Am. Soc. Inf Sci.,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>15--27</pages>
<contexts>
<context position="31247" citStr="Wang et al. 1985" startWordPosition="4981" endWordPosition="4984">e, ram, lamb, wool, animal, and so on. Likewise, starting from a query containing wool, the system can rapidly find sheep, etc. A more complete network, including other definitions, would allow going down as well as up the taxonomic tree, finding e.g. &amp;quot;merino&amp;quot; and other sheep varieties from either sheep or wool. Figure 1. Representation of Lexical Relations involving Sheep 5. Testing with SMART and CODER Several studies have been undertaken regarding the use of lexical and semantic relations in information retrieval. Though one investigation involved use of a special system constructed at HT (Wang et al. 1985), most of the other work has involved the SMART system. The first version of SMART ran on IBM mainframes; a more modern form was developed to run under the UNIX operating system (Fox 1983b). In SMART, queries and documents are represented simply as sets of terms, so a multi-dimensional vector space can be constructed wherein each term is associated with a different dimension in that space. Queries and documents can then be associated with points in that space, and documents can be retrieved if &amp;quot;near&amp;quot; to the query. But since queries are typically short, it can be valuable to expand a query with</context>
</contexts>
<marker>Wang, Vandendorpe, Evens, 1985</marker>
<rawString>Wang, Yih-Chen, James Vandendorpe, and Martha Evens, 1985. &amp;quot;Relational Thesauri in Information Retrieval.&amp;quot; J. Am. Soc. Inf Sci., 36(1): 15-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn White</author>
</authors>
<title>The Linguistic String Project Dictionary for Automatic Text Analysis.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proc. Workshop on Machine Readable Dictionaries, SRI,</booktitle>
<location>Menlo Park, CA.</location>
<contexts>
<context position="11245" citStr="White 1983" startWordPosition="1788" endWordPosition="1789"> breaking of long lines. Since no UNIX utility performed line breaks in quite the way we needed, we wrote a C program foldx which did. However, we have not made much use of LSPformat because we have found it useful to combine preprocessing of definition texts with the solution to another problem. 3.2. The LSP Word Dictionary 102 The LSP uses a lexicon of its own, the Word Dictionary, and cannot parse any text unless all words (strings separated by blanks) contained in the text are defined in the Word Dictionary. The Linguistic String Project has created a Word Dictionary of over 10,000 words (White 1983), but this cannot be used to parse even one W7 definition text without some additions. In particular, W7&apos;s part of speech codes (n, vi, vt, aj, etc.) are absent from the Word Dictionary. These were easily added. The homograph and sense numbers were also absent. We found it convenient to generate a list of all the different homograph and sense numbers that appeared in W7 (there were 410) and enter them as &amp;quot;words&amp;quot; in the Word Dictionary. With these additions it became possible to parse a total of 8,832 of the 126,879 definition texts without adding any more entries to the Word Dictionary. Identi</context>
</contexts>
<marker>White, 1983</marker>
<rawString>White, Carolyn, 1983. &amp;quot;The Linguistic String Project Dictionary for Automatic Text Analysis.&amp;quot; In Proc. Workshop on Machine Readable Dictionaries, SRI, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Wohlwend</author>
</authors>
<title>Creation of a Prolog Fact Base from the Collins English Dictionary.&amp;quot;</title>
<date>1986</date>
<tech>MS Report, VPI&amp;SU</tech>
<institution>Computer Science Dept.,</institution>
<location>Blacksburg, VA.</location>
<contexts>
<context position="5734" citStr="Wohlwend 1986" startWordPosition="881" endWordPosition="882">ll these aspects of our research program. 2. Dictionary text processing Since we wish to build a thesaurus with broad coverage, and since each dictionary has unique advantages, we plan to use several. The bulk of our work to date has been with the Collins Dictionary of the English Language (CDEL) and Webster&apos;s Seventh New Collegiate Dictionary (W7). 2.1. Collins Dictionary of the English Language In 1985 we obtained a magnetic tape containing the typesetter&apos;s form of CDEL from the Oxford Text Archive and embarked upon the task of converting that to a fact base in the form of Prolog relations (Wohlwend 1986). CDEL is a large dictionary with about 82,000 headwords. Thus it is larger than W7 (which has roughly 69,000 headwords), and also has separate fields not found in W7, such as sample usages (roughly 17,000), first names, compare-to lists, related adjectives, and abbreviations. Like W7, there are on average two definitions per headword, though while W7 has at most 26 senses per entry there are many words in CDEL with more senses, on up through one entry with 50 senses. Wohlwend used various UNIX (trademark of AT&amp;T Bell Laboratories) text processing tools and developed analyzers with lex and yac</context>
</contexts>
<marker>Wohlwend, 1986</marker>
<rawString>Wohlwend, Robert C., 1986. &amp;quot;Creation of a Prolog Fact Base from the Collins English Dictionary.&amp;quot; MS Report, VPI&amp;SU Computer Science Dept., Blacksburg, VA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>