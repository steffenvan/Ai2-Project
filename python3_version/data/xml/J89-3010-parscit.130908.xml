<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<note confidence="0.491562">
Book Reviews Information-based Syntax and Semantics. Vol 1: Fundamentals
</note>
<bodyText confidence="0.999138555555555">
ity over the technical or implementational point of view.
In this regard, it cannot be criticized as designed in a too
rigid way from the implementational viewpoint and not
adaptable to new situations and unforeseen phenomena.
The separation of data structures from the procedures
and the modularity of the system are features that are
essential to the extendability of the system to other
domains.
In general, the work is a good example of:
</bodyText>
<listItem confidence="0.507409625">
1. the necessity of creating extensive lexicons,
where &amp;quot;extensive&amp;quot; must be intended both in
breadth (i.e., in quantitative terms) and in depth
(i.e., from a qualitative viewpoint, as to the types
of information associated with the entries);
2. the necessity of working with large textual cor-
pora, both for obtaining linguistic data and for
testing systems.
</listItem>
<bodyText confidence="0.9997415">
This is encouraging for a trend that is in recent years
showing up, and having, for example, in Europe, great
success also in projects sponsored by national and
international organizations.
</bodyText>
<sectionHeader confidence="0.989308" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.92316">
Harris, Z.S. 1963 Discourse analysis reprints. The Hague: Mouton.
Harris, Z.S. 1964 String analysis in sentence structure. The Hague:
Mouton.
Harris, Z.S. 1982 A grammar of English on mathematical principles.
</bodyText>
<subsectionHeader confidence="0.866019">
New York: Wiley-Interscience.
</subsectionHeader>
<bodyText confidence="0.993056666666667">
Nicoletta Calzolari is a researcher at the Department of
Linguistics of the University of Pisa and at the Institute of
Computational Linguistics of CNR, Pisa. Her main research
areas are in the field of computational lexicography and
lexicology. Calzolari&apos;s address is: Dipartimento di Linguis-
tica, Universita di Pisa, Via S. Maria 36, I 56100 Pisa, Italy.
</bodyText>
<email confidence="0.759067">
E-mail: glottolo@icnucevm.bitnet
</email>
<sectionHeader confidence="0.994081" genericHeader="method">
INFORMATION-BASED SYNTAX AND SEMANTICS. VOL 1:
FUNDAMENTALS
</sectionHeader>
<subsectionHeader confidence="0.740803">
Carl Pollard and Ivan A. Sag
</subsectionHeader>
<bodyText confidence="0.877206333333333">
(Carnegie Mellon University and Stanford University,
resp.)
Stanford: Center for the Study of Language and
Information, Stanford University, 1987, x + 227
pp.
(CSLI lecture notes no. 13)
Distributed by the University of Chicago Press
ISBN 0-937073-23-7, $39.95 (hb); ISBN 0-937073-24-5,
$17.95 (sb)
</bodyText>
<equation confidence="0.548096">
Reviewed by
Edward P. Stabler, Jr.
</equation>
<affiliation confidence="0.603446">
University of Western Ontario
</affiliation>
<bodyText confidence="0.999791951612903">
This book is an introductory text in linguistics, a very
pleasant and readable introduction to head-driven
phrase structure grammar (HPSG). HPSG will be of
particular interest to computational linguists who have
wanted to see situation semantics integrated with a
unification-based phrase structure syntax. However,
computational linguists should be warned, in the first
place, that the book is truly introductory, focusing on
the preliminaries to a sophisticated account of the
language. Many of the serious problems to be faced are
not discussed at all. In other places good, hard problems
are posed, only to reward the reader&apos;s anticipation of a
resolution with a promissory note about the forthcom-
ing Volume 2. There are so many promissory notes at
crucial places that it becomes clear that Volume 2 will
be the real test of the framework. The two volumes are
apparently organized not by topic, but by difficulty. All
the difficult material on a whole range of topics—
syntactic and semantic—is left for the second volume.
The second warning for readers of this journal is that
this book does not consider the computational proper-
ties of HPSG at all. No standard characterization of the
HPSG-definable languages, no algorithm for unification
or for parsing, and no complexity results are presented.
As one expects in all but the most superficial or artificial
approaches to human language, the grammar is incom-
plete in both its universal and its language-specific
components. The grammatical principles, rules, and
lexical entries are feature-based, where feature values
can be complex (i.e., lists or sets), and computationally
oriented readers might wonder how many features are
needed and whether the set of possible values of syn-
tactic features is finite, but we are not told.
Pollard and Sag describe HPSG as a &amp;quot;synthetic and
eclectic&amp;quot; theory that draws on the insights of GPSG,
LFG, GB, FUG, categorial grammar, situation seman-
tics, and other approaches to language, which makes
the title rather puzzling. How is HPSG &amp;quot;information-
based&amp;quot;? Even when acquainted with the contents of
this volume, I was still puzzled: I am not attuned to the
Californian sense of &amp;quot;information.&amp;quot; HPSG is &amp;quot;an infor-
mation-based (or unification-based) theory of lan-
guage,- and it fundamentally regards &amp;quot;the objects that
make up a human language as bearers of information
within the community of people who know how to use
them.- To call HPSG information-based for both rea-
sons, because it unifies the partial information struc-
tures called features and because utterances bear infor-
mation, strikes me as a pun. But if smoke meaning fire
is very much the same as fire meaning fire, then it is no
doubt natural to think that fire meaning fire is quite a lot
like a feature&apos;s being a partial specification, an element
of a meet semilattice under subsumption where unifica-
tion corresponds to the greatest lower bound.
HPSG includes a rather complex array of different
kinds of propositions. We are given, in the first place,
some basic facts about what types of linguistic objects
there are. For example, there are two mutually exclu-
sive types of signs, lexical and phrasal. We are told that
&amp;quot;in general, such facts about relationships among types
of linguistic objects are obvious, and we will not explic-
itly state them,&amp;quot; but then in later pages we are infor-
</bodyText>
<page confidence="0.937753">
198 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
<note confidence="0.33979">
Book Reviews Information-Based Syntax and Semantics. Vol 1: Fundamentals
</note>
<bodyText confidence="0.999952947368421">
mally given an extensive inventory of non-obvious facts
of this kind. Signs have attributes of PHONOLOGY,
SYNTAX, and SEMANTICS, and phrasal signs have in
addition a DAUGHTERS attribute. The values of
DAUGHTERS are HEAD-DTR (head), COMP-DTR
(complement), FILLER-DTR (filler), CONJ-DTR (con-
junct), ADJ-DTR (adjunct), etc. The list of possibilities
and restrictions on features and values continues
through the book, and summaries of some of them are
graphically presented. These presumably universal
facts are not obvious, and they should be given the same
attention as any other aspect of the theory. The more
fundamental distinctive aspects of the approach to
features and unification, such as the use of intuitionistic
negation, are not emphasized or defended here either,
though references are provided. The syntax and seman-
tics of the very rich language for defining the signs and
restrictions on feature values is presented informally.
An HPSG for a particular language comprises, in
addition to these basic facts, a lexicon, principles, and
grammar rules. The lexicon contains basic lexical signs
together with lexical rules. The relation between active
and passive constructions, for example, is handled by
an LFG-like rule stating the relation between active and
passive verb forms, rather than by a metarule as in
GPSG, or by a movement as in GB. (Seven lexical rules
are mentioned altogether.) The principles are either
universal or language-specific and are all constraints on
the signs. For example, the universal &amp;quot;head feature
principle&amp;quot; says that the &amp;quot;head daughters&amp;quot; of a phrase
must have the same &amp;quot;head features&amp;quot; as the head. This
volume mentions six universal principles but only one
language-specific principle, by my count. As in GPSG,
constituent structures are specified by a combination of
immediate domination constraints and linear prece-
dence constraints. The only language-specific principle
presented here is the &amp;quot;constituent ordering principle,&amp;quot;
which defines the linear precedence constraints. A very
tentative formulation of the English constraints is be-
gun: a lexical head precedes its sisters; phrasal comple-
ments other than VP and S occur in order of increasing
obliqueness; and fillers precede gapped clauses. Pollard
and Sag note that a focused complement can follow a
more oblique —N sister, as in:
Sandy gave [to Kim] [the book she bought in Vienna]
but they do not work out an account of focus and a
modification of the linear precedence restrictions that
works here.1 HPSG grammar rules constrain the imme-
diate dominance relations. The presence of other gen-
eral principles allows these rules to be very general, and
consequently it is rather hard to get a good intuitive
grasp of the range of structures they license. One of the
four rules presented here simply says that a phrasal sign
with an empty subcategory list can have as constituents
a single complement and a phrasal head. Of computa-
tional interest is the fact that some of the rules formu-
lated in the text allow purely structural ambiguities that
do not correspond to any semantic ambiguity.
So in spite of the fact that this volume presents only
the fundamentals or preliminaries for a sophisticated
theory, the syntactic theory is already getting fairly
complex. Pollard and Sag provide honest discussions of
many of the issues that have not been worked out yet,
and provide useful references to the more technical
literature at these points. As noted, a good deal is left to
Volume 2. We are promised there an account of many
important things that are just left out of this volume. I
kept a list of them: agreement relations, the distribution
of expletive pronouns, long-distance dependencies and
head-filler constructions, a semantically motivated ac-
count of indices, quantifier-scoping ambiguities, syntax
and semantics of various raising constructions, control
theory, binding theory, alternative approaches to sub-
categorization, parasitic gaps, relative clauses, tough-
movement, extraposition with expletive it, and subject
extractions from sentential complements.
For the phenomena that are treated by the theory,
the presentation of motivating linguistic data is nice and
the main lines of argument are easy to follow. My main
complaint is that although the grammar is eclectic and
synthetic, its presentation is by and large not compara-
tive. That is, the volume introduces HPSG without
explicitly presenting an argument that HPSG is a better
linguistic theory than the competing approaches. For
example, the question of whether complement order
really is defined primarily by independent linear prece-
dence constraints together with immediate dominance
constraints is never raised: alternative accounts from
other traditions are not mentioned. At only a few points
is the HPSG account of a phenomenon explicitly com-
pared with alternatives. I like these sections and wish
they came more frequently and went into more detail. It
is at these points where HPSG needs to make its case.
Pollard and Sag say that &amp;quot;in one important respect,
HPSG differs from all the syntactic theories which have
influenced its development,&amp;quot; namely, that &amp;quot;syntactic
and semantic aspects of grammatical theory are built up
in an integrated way from the start&amp;quot; rather than simply
being added as an afterthought. Why should they be
developed in an integrated way? We are not told. One
imagines that it must be because semantic and syntactic
constraints are interdependent and cannot be isolated
(without introducing excessive and unnecessary com-
plication), but no evidence of this kind of interaction is
provided here. The HPSG semantics presented in this
volume is pretty well limited to one chapter that pre-
sents a unification-based naive semantics for a tiny
fragment of English. A notation inspired by situation
semantics is used and rather superficially contrasted
with first-order logic.2 The linking of argument positions
to items in the subcategorization list of a constituent is
done in the most straightforward way. No treatment is
given to quantifier scope ambiguities, to intensional
contexts, or to the foundational problems with utter-
</bodyText>
<table confidence="0.289887666666667">
Computational Linguistics, Volume 15, Number 3, September 1989 199
Machine Translation Systems
Book Reviews
</table>
<bodyText confidence="0.999603533333333">
ances that carry &amp;quot;false information.&amp;quot; But more impor-
tantly for the claim about the distinctiveness of HPSG,
we do not see substantial interactions between semantic
and syntactic phenomena. This contrasts with GB the-
ory, for example, in which the subtleties of quantifier
interaction are supposed to depend in a very direct way
on details of syntactic structure, and at least some of the
semantic rules operate under the same substantial con-
straints as syntactic rules. In HPSG, on the other hand,
the &amp;quot;flow&amp;quot; of semantic information is defined by a
special semantic principle tailored to fit the needs of the
fragment considered here. The work done so far could
just as well have been done as an afterthought. How-
ever, if the promissory notes are redeemed in Volume 2,
I expect that the semantics will play a larger role.
</bodyText>
<sectionHeader confidence="0.322211" genericHeader="method">
NOTES
</sectionHeader>
<bodyText confidence="0.964844088235294">
1. They mention the &amp;quot;speculative&amp;quot; solution that simply disjoins the
increasing obliqueness constraint with a constraint saying that —N
constituents precede focused constituents, but this idea obviously
needs further development to work even on the range of cases
considered in the text. Pollard and Sag refer to technical reports
by Uszkoreit on this problem.
2. The contrast is not clearly formulated. For example, Pollard and
Sag note that whereas the first-order formulas laugh(rebecca) and
run(rebecca) may both denote the same truth value (in the actual
world at a time), the formulas ((laugh, laugherrebecca; 1)) and
((run, runner:rebecca; 1)) (in the actual world at a time) will
always be &amp;quot;more contentful.&amp;quot; In an introduction, though, it is
worth considering the clear sense in which the first-order formu-
las, like the sentences Rebecca laughs and Rebecca runs, have
more content: they assert (under the intended interpretation)
something about the world, whereas the others (under the in-
tended interpretation) simply denote abstract objects without
telling us anything true or false. What is the motivation for going
to the lengths of saying that a situation in which the circumstance
holds is a fact? Furthermore, the latter expressions denote
different circumstances only if the run relation is different from
the laugh relation, and it would be useful, even in an introduction,
to alert a student to the reasons that defining appropriate identity
conditions on these relations is a very tricky business. The
situation is not clarified by Pollard and Sag&apos;s further suggestion
that while ((believe, believer:claire, believed:((laugh, laugher:
rebecca; 1)); 1)) is well formed, the first-order formula believe-
(claire,laugh (rebecca)) is syntactically ill-formed. This is not
even correct, since laugh can be both a predicate and a function
in a first-order language. In fact, we can define the function laugh
in such a way that laugh(rebecca) denotes the very fact of a
situation in which the circumstance denoted by ((laugh, laugher:
rebecca; 1)) holds. The real issues are missed without a slightly
more careful development.
</bodyText>
<footnote confidence="0.620449">
Edward P. Stabler, Jr. is an assistant professor of computer
science at the University of Western Ontario and an Institute
Scholar in the Artificial Intelligence and Robotics Program of
the Canadian Institute for Advanced Research. He received
his Ph.D. in philosophy from MIT, and has recently been
working on parsing as deduction and transparent implemen-
tations of GB theories. Stabler&apos;s address is: Department of
Computer Science, University of Western Ontario, London,
Ontario N6A 5B7, Canada. E-mail: stabler@uwo.uunet
</footnote>
<table confidence="0.751355555555556">
MACHINE TRANSLATION SYSTEMS
Jonathan Slocum (ed.)
Cambridge, England: Cambridge University Press,
1988, ix + 341 pp.
(Studies in natural language processing)
ISBN 0-521-35166-9, $49.50 (hb); ISBN 0-521-35963-5,
$16.95 (sb) [20% discount to ACL members]
Reviewed by
John S. White
</table>
<subsubsectionHeader confidence="0.618599">
Planning Research Corporation
</subsubsectionHeader>
<bodyText confidence="0.993759214285714">
Machine translation systems is a successful attempt at
presenting the breadth of issues on machine translation
from the most relevant of perspectives, namely the
systems that exist. The orientation toward presenting
the research platform systems, the prototypical sys-
tems, and the systems in use enhances the current
efforts toward finding the common ground between
researchers and implementers. Such convergence leads
to fresh insight for the implementers, and, for the
researchers, solutions to the practical but vexing prob-
lems that the production systems have already solved.
The papers in this volume are a new presentation of
articles in the special two-issue Computational Linguis-
tics coverage of machine translation. There have been
some updates to the content of these articles, though
more updates would have painted a more accurate
picture about changes, for better and worse, in the
fortunes of these systems.
Though there is no explicit explanation of the format
of the articles, it is apparent that they were written in
accordance with some suggested outline or question-
naire. Thus the heading numbering and organization of
the articles are roughly parallel. The advantage of this
organization is, of course, that the different systems can
be readily compared on the basis of design, theory, and
performance. The disadvantage is that there is a ten-
dency to respond to the guidelines without giving a clear
indication of what the guidelines were.
The papers in the volume are the following:
Jonathan Slocum, &amp;quot;A survey of machine translation: its
history, current status, and future prospects&amp;quot;
This paper is a version of the invited paper Slocum
presented at the 1984 COLING conference at Stanford.
It is a valuable statement about machine translation,
and one which could bear up well with periodic updated
republication. The theme is that an understanding of the
issues of machine translation presupposes an under-
standing of translation itself. The need for translation,
the way in which professional human translation is done
today, and therefore the way that machine translation
approaches fit in, should be critical components in any
machine translation design. Yet it frequently is not,
</bodyText>
<page confidence="0.940484">
200 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010163">
<title confidence="0.578433">Book Reviews Information-based Syntax and Semantics. Vol 1: Fundamentals</title>
<abstract confidence="0.986569238095238">ity over the technical or implementational point of view. regard, it cannot be criticized as designed in a too rigid way from the implementational viewpoint and not adaptable to new situations and unforeseen phenomena. The separation of data structures from the procedures and the modularity of the system are features that are essential to the extendability of the system to other domains. the work is a good example of: 1. the necessity of creating extensive lexicons, where &amp;quot;extensive&amp;quot; must be intended both in breadth (i.e., in quantitative terms) and in depth (i.e., from a qualitative viewpoint, as to the types of information associated with the entries); 2. the necessity of working with large textual corpora, both for obtaining linguistic data and for testing systems. This is encouraging for a trend that is in recent years showing up, and having, for example, in Europe, great success also in projects sponsored by national and international organizations.</abstract>
<note confidence="0.916601">REFERENCES Z.S. 1963 analysis reprints. Hague: Mouton. Z.S. 1964 analysis in sentence structure. Hague: Mouton. Z.S. 1982 A of English on mathematical principles. New York: Wiley-Interscience. Calzolari is researcher at the Department of</note>
<abstract confidence="0.838896">Linguistics of the University of Pisa and at the Institute of Computational Linguistics of CNR, Pisa. Her main research areas are in the field of computational lexicography and lexicology. Calzolari&apos;s address is: Dipartimento di Linguis-</abstract>
<address confidence="0.98242">tica, Universita di Pisa, Via S. Maria 36, I 56100 Pisa, Italy.</address>
<email confidence="0.996658">E-mail:glottolo@icnucevm.bitnet</email>
<note confidence="0.509414">INFORMATION-BASED SYNTAX AND SEMANTICS. VOL 1:</note>
<title confidence="0.975886">FUNDAMENTALS</title>
<author confidence="0.999952">Carl Pollard</author>
<author confidence="0.999952">Ivan A Sag</author>
<affiliation confidence="0.99965">Carnegie Mellon University and Stanford University,</affiliation>
<email confidence="0.751955">resp.)</email>
<address confidence="0.7057165">Stanford: Center for the Study of Language and Information, Stanford University, 1987, x + 227</address>
<email confidence="0.531203">pp.</email>
<note confidence="0.927252">(CSLI lecture notes no. 13) Distributed by the University of Chicago Press ISBN 0-937073-23-7, $39.95 (hb); ISBN 0-937073-24-5, $17.95 (sb) Reviewed by Jr. University of Western Ontario This book is an introductory text in linguistics, a very</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>