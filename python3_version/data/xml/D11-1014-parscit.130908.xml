<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.9860555">
Semi-Supervised Recursive Autoencoders
for Predicting Sentiment Distributions
</title>
<author confidence="0.998792">
Richard Socher Jeffrey Pennington* Eric H. Huang Andrew Y. Ng Christopher D. Manning
</author>
<affiliation confidence="0.944627">
Computer Science Department, Stanford University, Stanford, CA 94305, USA
*SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA
</affiliation>
<email confidence="0.995675">
richard@socher.org {jpennin,ehhuang,ang,manning}@stanford.edu ang@cs.stanford.edu
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99775735">
We introduce a novel machine learning frame-
work based on recursive autoencoders for
sentence-level prediction of sentiment label
distributions. Our method learns vector space
representations for multi-word phrases. In
sentiment prediction tasks these represen-
tations outperform other state-of-the-art ap-
proaches on commonly used datasets, such as
movie reviews, without using any pre-defined
sentiment lexica or polarity shifting rules. We
also evaluate the model’s ability to predict
sentiment distributions on a new dataset based
on confessions from the experience project.
The dataset consists of personal user stories
annotated with multiple labels which, when
aggregated, form a multinomial distribution
that captures emotional reactions. Our al-
gorithm can more accurately predict distri-
butions over such labels compared to several
competitive baselines.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999573466666667">
The ability to identify sentiments about personal ex-
periences, products, movies etc. is crucial to un-
derstand user generated content in social networks,
blogs or product reviews. Detecting sentiment in
these data is a challenging task which has recently
spawned a lot of interest (Pang and Lee, 2008).
Current baseline methods often use bag-of-words
representations which cannot properly capture more
complex linguistic phenomena in sentiment analy-
sis (Pang et al., 2002). For instance, while the two
phrases “white blood cells destroying an infection”
and “an infection destroying white blood cells” have
the same bag-of-words representation, the former is
a positive reaction while the later is very negative.
More advanced methods such as (Nakagawa et al.,
</bodyText>
<figureCaption confidence="0.993752">
Figure 1: Illustration of our recursive autoencoder archi-
</figureCaption>
<bodyText confidence="0.874672724137931">
tecture which learns semantic vector representations of
phrases. Word indices (orange) are first mapped into a
semantic vector space (blue). Then they are recursively
merged by the same autoencoder network into a fixed
length sentence representation. The vectors at each node
are used as features to predict a distribution over senti-
ment labels.
2010) that can capture such phenomena use many
manually constructed resources (sentiment lexica,
parsers, polarity-shifting rules). This limits the ap-
plicability of these methods to a broader range of
tasks and languages. Lastly, almost all previous
work is based on single, positive/negative categories
or scales such as star ratings. Examples are movie
reviews (Pang and Lee, 2005), opinions (Wiebe et
al., 2005), customer reviews (Ding et al., 2008) or
multiple aspects of restaurants (Snyder and Barzilay,
2007). Such a one-dimensional scale does not accu-
rately reflect the complexity of human emotions and
sentiments.
In this work, we seek to address three issues. (i)
Instead of using a bag-of-words representation, our
model exploits hierarchical structure and uses com-
positional semantics to understand sentiment. (ii)
Our system can be trained both on unlabeled do-
main data and on supervised sentiment data and does
not require any language-specific sentiment lexica,
Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow
i walked into a parked car
</bodyText>
<figure confidence="0.998550625">
Recursive Autoencoder
Predicted
Sentiment
Distribution
Semantic
Representations
Indices
Words
</figure>
<page confidence="0.97241">
151
</page>
<note confidence="0.9575255">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999831">
parsers, etc. (iii) Rather than limiting sentiment to
a positive/negative scale, we predict a multidimen-
sional distribution over several complex, intercon-
nected sentiments.
We introduce an approach based on semi-
supervised, recursive autoencoders (RAE) which
use as input continuous word vectors. Fig. 1 shows
an illustration of the model which learns vector rep-
resentations of phrases and full sentences as well as
their hierarchical structure from unsupervised text.
We extend our model to also learn a distribution over
sentiment labels at each node of the hierarchy.
We evaluate our approach on several standard
datasets where we achieve state-of-the art perfor-
mance. Furthermore, we show results on the re-
cently introduced experience project (EP) dataset
(Potts, 2010) that captures a broader spectrum of
human sentiments and emotions. The dataset con-
sists of very personal confessions anonymously
made by people on the experience project website
www.experienceproject.com. Confessions are la-
beled with a set of five reactions by other users. Re-
action labels are you rock (expressing approvement),
tehee (amusement), I understand, Sorry, hugs and
Wow, just wow (displaying shock). For evaluation on
this dataset we predict both the label with the most
votes as well as the full distribution over the senti-
ment categories. On both tasks our model outper-
forms competitive baselines. A set of over 31,000
confessions as well as the code of our model are
available at www.socher.org.
After describing the model in detail, we evalu-
ate it qualitatively by analyzing the learned n-gram
vector representations and compare quantitatively
against other methods on standard datasets and the
EP dataset.
</bodyText>
<sectionHeader confidence="0.9994125" genericHeader="introduction">
2 Semi-Supervised Recursive
Autoencoders
</sectionHeader>
<bodyText confidence="0.9998347">
Our model aims to find vector representations for
variable-sized phrases in either unsupervised or
semi-supervised training regimes. These representa-
tions can then be used for subsequent tasks. We first
describe neural word representations and then pro-
ceed to review a related recursive model based on
autoencoders, introduce our recursive autoencoder
(RAE) and describe how it can be modified to jointly
learn phrase representations, phrase structure and
sentiment distributions.
</bodyText>
<subsectionHeader confidence="0.978702">
2.1 Neural Word Representations
</subsectionHeader>
<bodyText confidence="0.999964928571429">
We represent words as continuous vectors of param-
eters. We explore two settings. In the first setting
we simply initialize each word vector x E Rn by
sampling it from a zero mean Gaussian distribution:
x — N(0, U2). These word vectors are then stacked
into a word embedding matrix L E Rn×|V |, where
|V  |is the size of the vocabulary. This initialization
works well in supervised settings where a network
can subsequently modify these vectors to capture
certain label distributions.
In the second setting, we pre-train the word vec-
tors with an unsupervised neural language model
(Bengio et al., 2003; Collobert and Weston, 2008).
These models jointly learn an embedding of words
into a vector space and use these vectors to predict
how likely a word occurs given its context. After
learning via gradient ascent the word vectors cap-
ture syntactic and semantic information from their
co-occurrence statistics.
In both cases we can use the resulting matrix of
word vectors L for subsequent tasks as follows. As-
sume we are given a sentence as an ordered list of
m words. Each word has an associated vocabulary
index k into the embedding matrix which we use to
retrieve the word’s vector representation. Mathemat-
ically, this look-up operation can be seen as a sim-
ple projection layer where we use a binary vector b
which is zero in all positions except at the kth index,
</bodyText>
<equation confidence="0.891223">
xi = Lbk E Rn. (1)
</equation>
<bodyText confidence="0.999811461538461">
In the remainder of this paper, we represent a sen-
tence (or any n-gram) as an ordered list of these
vectors (x1, ... , xm). This word representation is
better suited to autoencoders than the binary number
representations used in previous related autoencoder
models such as the recursive autoassociative mem-
ory (RAAM) model (Pollack, 1990; Voegtlin and
Dominey, 2005) or recurrent neural networks (El-
man, 1991) since sigmoid units are inherently con-
tinuous. Pollack circumvented this problem by hav-
ing vocabularies with only a handful of words and
by manually defining a threshold to binarize the re-
sulting vectors.
</bodyText>
<page confidence="0.99795">
152
</page>
<figureCaption confidence="0.9683866">
Figure 2: Illustration of an application of a recursive au-
toencoder to a binary tree. The nodes which are not filled
are only used to compute reconstruction errors. A stan-
dard autoencoder (in box) is re-used at each node of the
tree.
</figureCaption>
<subsectionHeader confidence="0.99868">
2.2 Traditional Recursive Autoencoders
</subsectionHeader>
<bodyText confidence="0.999669541666667">
The goal of autoencoders is to learn a representation
of their inputs. In this section we describe how to
obtain a reduced dimensional vector representation
for sentences.
In the past autoencoders have only been used in
setting where the tree structure was given a-priori.
We review this setting before continuing with our
model which does not require a given tree structure.
Fig. 2 shows an instance of a recursive autoencoder
(RAE) applied to a given tree. Assume we are given
a list of word vectors x = (x1,... , xm) as described
in the previous section as well as a binary tree struc-
ture for this input in the form of branching triplets
of parents with children: (p -+ c1c2). Each child
can be either an input word vector xi or a nontermi-
nal node in the tree. For the example in Fig. 2, we
have the following triplets: ((y1 -+ x3x4), (y2 -+
x2y1), (y1 -+ x1y2)). In order to be able to apply
the same neural network to each pair of children, the
hidden representations yi have to have the same di-
mensionality as the xi’s.
Given this tree structure, we can now compute the
parent representations. The first parent vector y1 is
computed from the children (c1, c2) = (x3, x4):
</bodyText>
<equation confidence="0.998246">
p = f(W(1)[c1;c2] + b(1)), (2)
</equation>
<bodyText confidence="0.998502428571429">
where we multiplied a matrix of parameters W (1) E
Rnx2n by the concatenation of the two children.
After adding a bias term we applied an element-
wise activation function such as tanh to the result-
ing vector. One way of assessing how well this n-
dimensional vector represents its children is to try to
reconstruct the children in a reconstruction layer:
</bodyText>
<equation confidence="0.9869845">
[ci; c� ] = W (2)p + b(2). (3)
2
</equation>
<bodyText confidence="0.99954675">
During training, the goal is to minimize the recon-
struction errors of this input pair. For each pair, we
compute the Euclidean distance between the original
input and its reconstruction:
</bodyText>
<equation confidence="0.996353">
1 Erec([c1; c2]) = 2 I I [c1; c2] − [c1; c2] 112
. (4)
</equation>
<bodyText confidence="0.981356352941176">
This model of a standard autoencoder is boxed in
Fig. 2. Now that we have defined how an autoen-
coder can be used to compute an n-dimensional vec-
tor representation (p) of two n-dimensional children
(c1, c2), we can describe how such a network can be
used for the rest of the tree.
Essentially, the same steps repeat. Now that y1
is given, we can use Eq. 2 to compute y2 by setting
the children to be (c1, c2) = (x2, y1). Again, after
computing the intermediate parent vector y2, we can
assess how well this vector capture the content of
the children by computing the reconstruction error
as in Eq. 4. The process repeat until the full tree
is constructed and we have a reconstruction error at
each nonterminal node. This model is similar to the
RAAM model (Pollack, 1990) which also requires a
fixed tree structure.
</bodyText>
<subsectionHeader confidence="0.999548">
2.3 Unsupervised Recursive Autoencoder for
Structure Prediction
</subsectionHeader>
<bodyText confidence="0.999991333333333">
Now, assume there is no tree structure given for
the input vectors in x. The goal of our structure-
prediction RAE is to minimize the reconstruction er-
ror of all vector pairs of children in a tree. We de-
fine A(x) as the set of all possible trees that can be
built from an input sentence x. Further, let T (y) be
a function that returns the triplets of a tree indexed
by s of all the non-terminal nodes in a tree. Using
the reconstruction error of Eq. 4, we compute
</bodyText>
<equation confidence="0.979261333333333">
1:
RAEθ(x) = arg min Erec([c1; c2]s) (5)
yEA(x) sET(y)
</equation>
<bodyText confidence="0.9987645">
We now describe a greedy approximation that con-
structs such a tree.
</bodyText>
<equation confidence="0.993945">
x3 x4
x1
x2
y3=f(W(1)[x1;y2] + b)
y2=f(W(1)[x2;y1] + b)
y1=f(W(1)[x3;x4] + b)
</equation>
<page confidence="0.990112">
153
</page>
<bodyText confidence="0.99958176">
Greedy Unsupervised RAE. For a sentence with
m words, we apply the autoencoder recursively. It
takes the first pair of neighboring vectors, defines
them as potential children of a phrase (c1; c2) =
(x1; x2), concatenates them and gives them as in-
put to the autoencoder. For each word pair, we save
the potential parent node p and the resulting recon-
struction error.
After computing the score for the first pair, the
network is shifted by one position and takes as input
vectors (c1, c2) = (x2, x3) and again computes a po-
tential parent node and a score. This process repeats
until it hits the last pair of words in the sentence:
(c1, c2) = (xm−1, xm). Next, it selects the pair
which had the lowest reconstruction error (ETec) and
its parent representation p will represent this phrase
and replace both children in the sentence word list.
For instance, consider the sequence (x1, x2, x3, x4)
and assume the lowest ETec was obtained by the pair
(x3, x4). After the first pass, the new sequence then
consists of (x1, x2, p(3,4)). The process repeats and
treats the new vector p(3,4) like any other input vec-
tor. For instance, subsequent states could be either:
(x1,p(2,(3,4))) or (p(1,2),p(3,4)). Both states would
then finish with a deterministic choice of collapsing
the remaining two states into one parent to obtain
(p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree
is then recovered by unfolding the collapsing deci-
sions.
The resulting tree structure captures as much of
the single-word information as possible (in order
to allow reconstructing the word vectors) but does
not necessarily follow standard syntactic constraints.
We also experimented with a method that finds bet-
ter solutions to Eq. 5 based on CKY-like beam
search algorithms (Socher et al., 2010; Socher et al.,
2011) but the performance is similar and the greedy
version is much faster.
Weighted Reconstruction. One problem with
simply using the reconstruction error of both chil-
dren equally as describe in Eq. 4 is that each child
could represent a different number of previously
collapsed words and is hence of bigger importance
for the overall meaning reconstruction of the sen-
tence. For instance in the case of (x1,p(2,(3,4)))
one would like to give more importance to recon-
structing p than x1. We capture this desideratum
by adjusting the reconstruction error. Let n1, n2 be
the number of words underneath a current poten-
tial child, we re-define the reconstruction error to be
</bodyText>
<equation confidence="0.702663">
ETec([c1; c2]; θ) =
</equation>
<bodyText confidence="0.993432071428571">
Length Normalization. One of the goals of
RAEs is to induce semantic vector representations
that allow us to compare n-grams of different
lengths. The RAE tries to lower reconstruction error
of not only the bigrams but also of nodes higher in
the tree. Unfortunately, since the RAE computes the
hidden representations it then tries to reconstruct, it
can just lower reconstruction error by making the
hidden layer very small in magnitude. To prevent
such undesirable behavior, we modify the hidden
layer such that the resulting parent representation al-
ways has length one, after computing p as in Eq. 2,
we simply set: p = p
||p||.
</bodyText>
<subsectionHeader confidence="0.999044">
2.4 Semi-Supervised Recursive Autoencoders
</subsectionHeader>
<bodyText confidence="0.999977384615385">
So far, the RAE was completely unsupervised and
induced general representations that capture the se-
mantics of multi-word phrases.In this section, we
extend RAEs to a semi-supervised setting in order
to predict a sentence- or phrase-level target distribu-
tion t.1
One of the main advantages of the RAE is that
each node of the tree built by the RAE has associ-
ated with it a distributed vector representation (the
parent vector p) which could also be seen as fea-
tures describing that phrase. We can leverage this
representation by adding on top of each parent node
a simple softmax layer to predict class distributions:
</bodyText>
<equation confidence="0.985943">
d(p; θ) = softmax(Wlabelp). (7)
</equation>
<bodyText confidence="0.838804111111111">
Assuming there are K labels, d E RK is
a K-dimensional multinomial distribution and
P
k=1 dk = 1. Fig. 3 shows such a semi-supervised
RAE unit. Let tk be the kth element of the multino-
mial target label distribution t for one entry. The
softmax layer’s outputs are interpreted as condi-
tional probabilities dk = p(kJ[c1; c2]), hence the
cross-entropy error is
</bodyText>
<equation confidence="0.979172">
EcE(p, t; θ) = − XK tk log dk(p; θ). (8)
k=1
</equation>
<bodyText confidence="0.433827">
1For the binary label classification case, the distribution is
of the form [1, 0] for class 1 and [0, 1] for class 2.
</bodyText>
<equation confidence="0.97308">
n1
n1 + n2
~~~~~~2 + n2
~~c1 − c� 1
n
1 + n2
~~~c2 − c� ~~~~2 (6)
�
2
</equation>
<page confidence="0.999518">
154
</page>
<figure confidence="0.328693">
3 Learning
Reconstruction error Cross-entropy error
</figure>
<figureCaption confidence="0.9594">
Figure 3: Illustration of an RAE unit at a nonterminal tree
node. Red nodes show the supervised sojtnax layer for
label distribution prediction.
</figureCaption>
<bodyText confidence="0.98377675">
Using this cross-entropy error for the label and the
reconstruction error from Eq. 6, the final semi-
supervised RAE objective over (sentences,label)
pairs (x, t) in a corpus becomes
</bodyText>
<equation confidence="0.998712">
1 � λ
J = E(x, t; θ) + 2||θ||2, (9)
N (x,t)
</equation>
<bodyText confidence="0.999597">
where we have an error for each entry in the training
set that is the sum over the error at the nodes of the
tree that is constructed by the greedy RAE:
</bodyText>
<equation confidence="0.997803">
W(2)
W(label)
W(1)
</equation>
<bodyText confidence="0.999867666666667">
Let θ = (W (1), b(1), W(2), b(1), Wlabel, L) be the set
of our model parameters, then the gradient becomes:
To compute this gradient, we first greedily construct
all trees and then derivatives for these trees are com-
puted efficiently via backpropagation through struc-
ture (Goller and K¨uchler, 1996). Because the algo-
rithm is greedy and the derivatives of the supervised
cross-entropy error also modify the matrix W(1),
this objective is not necessarily continuous and a
step in the gradient descent direction may not nec-
essarily decrease the objective. However, we found
that L-BFGS run over the complete training data
(batch mode) to minimize the objective works well
in practice, and that convergence is smooth, with the
algorithm typically finding a good solution quickly.
</bodyText>
<equation confidence="0.99812475">
1 N
(x,t)
∂E(x, t; θ) θ.
∂θ + λ (10)
∂J
∂θ =
E(x, t; θ) = � E([c1; c2]s, ps, t, θ). 4 Experiments
s∈T(RAEθ(x))
</equation>
<bodyText confidence="0.992821">
The error at each nonterminal node is the weighted
sum of reconstruction and cross-entropy errors,
</bodyText>
<equation confidence="0.9068175">
E([c1; c2]s, ps, t, θ) =
αErec([c1; c2]s; θ) + (1 − α)EcE(ps, t; θ).
</equation>
<bodyText confidence="0.999952828571429">
The hyperparameter α weighs reconstruction and
cross-entropy error. When minimizing the cross-
entropy error of this softmax layer, the error will
backpropagate and influence both the RAE param-
eters and the word representations. Initially, words
such as good and bad have very similar representa-
tions. This is also the case for Brown clusters and
other methods that use only cooccurrence statistics
in a small window around each word. When learn-
ing with positive/negative sentiment, the word em-
beddings get modified and capture less syntactic and
more sentiment information.
In order to predict the sentiment distribution of a
sentence with this model, we use the learned vector
representation of the top tree node and train a simple
logistic regression classifier.
We first describe the new experience project (EP)
dataset, results of standard classification tasks on
this dataset and how to predict its sentiment label
distributions. We then show results on other com-
monly used datasets and conclude with an analysis
of the important parameters of the model.
In all experiments involving our model, we repre-
sent words using 100-dimensional word vectors. We
explore the two settings mentioned in Sec. 2.1. We
compare performance on standard datasets when us-
ing randomly initialized word vectors (random word
init.) or word vectors trained by the model of Col-
lobert and Weston (2008) and provided by Turian
et al. (2010).2 These vectors were trained on an
unlabeled corpus of the English Wikipedia. Note
that alternatives such as Brown clusters are not suit-
able since they do not capture sentiment information
(good and bad are usually in the same cluster) and
cannot be modified via backpropagation.
</bodyText>
<footnote confidence="0.9971265">
2http://metaoptimize.com/projects/
wordreprs/
</footnote>
<page confidence="0.979015">
155
</page>
<table confidence="0.9998264">
Corpus K Instances Distr.(+/-) Avg|W|
MPQA 2 10,624 0.31/0.69 3
MR 2 10,662 0.5/0.5 22
EP 5 31,675 .2/.2/.1/.4/.1 113
EP≥ 4 5 6,129 .2/.2/.1/.4/.1 129
</table>
<tableCaption confidence="0.840296666666667">
Table 1: Statistics on the different datasets. K is the num-
ber of classes. Distr. is the distribution of the different
classes (in the case of 2, the positive/negative classes, for
EP the rounded distribution of total votes in each class).
|W  |is the average number of words per instance. We use
EP≥ 4, a subset of entries with at least 4 votes.
</tableCaption>
<subsectionHeader confidence="0.851015">
4.1 EP Dataset: The Experience Project
</subsectionHeader>
<bodyText confidence="0.9999096">
The confessions section of the experience project
website3 lets people anonymously write short per-
sonal stories or “confessions”. Once a story is on
the site, each user can give a single vote to one of
five label categories (with our interpretation):
</bodyText>
<listItem confidence="0.9914158">
1 Sorry, Hugs: User offers condolences to author.
2. You Rock: Indicating approval, congratulations.
3. Teehee: User found the anecdote amusing.
4. I Understand: Show of empathy.
5. Wow, Just Wow: Expression of surprise,shock.
</listItem>
<bodyText confidence="0.989107954545454">
The EP dataset has 31,676 confession entries, a to-
tal number of 74,859 votes for the 5 labels above, the
average number of votes per entry is 2.4 (with a vari-
ance of 33). For the five categories, the numbers of
votes are [14, 816;13, 325;10, 073; 30, 844; 5, 801].
Since an entry with less than 4 votes is not very well
identified, we train and test only on entries with at
least 4 total votes. There are 6,129 total such entries.
The distribution over total votes in the 5 classes
is similar: [0.22; 0.2; 0.11; 0.37; 0.1]. The average
length of entries is 129 words. Some entries con-
tain multiple sentences. In these cases, we average
the predicted label distributions from the sentences.
Table 1 shows statistics of this and other commonly
used sentiment datasets (which we compare on in
later experiments). Table 2 shows example entries
as well as gold and predicted label distributions as
described in the next sections.
Compared to other datasets, the EP dataset con-
tains a wider range of human emotions that goes far
beyond positive/negative product or movie reviews.
Each item is labeled with a multinomial distribu-
</bodyText>
<footnote confidence="0.9021585">
3http://www.experienceproject.com/
confessions.php
</footnote>
<bodyText confidence="0.999743222222222">
tion over interconnected response categories. This
is in contrast to most other datasets (including multi-
aspect rating) where several distinct aspects are rated
independently but on the same scale. The topics
range from generic happy statements, daily clumsi-
ness reports, love, loneliness, to relationship abuse
and suicidal notes. As is evident from the total num-
ber of label votes, the most common user reaction
is one of empathy and an ability to relate to the au-
thors experience. However, some stories describe
horrible scenarios that are not common and hence
receive more offers of condolence. In the following
sections we show some examples of stories with pre-
dicted and true distributions but refrain from listing
the most horrible experiences.
For all experiments on the EP dataset, we split the
data into train (49%), development (21%) and test
data (30%).
</bodyText>
<subsectionHeader confidence="0.76259">
4.2 EP: Predicting the Label with Most Votes
</subsectionHeader>
<bodyText confidence="0.999144703703704">
The first task for our evaluation on the EP dataset is
to simply predict the single class that receives the
most votes. In order to compare our novel joint
phrase representation and classifier learning frame-
work to traditional methods, we use the following
baselines:
Random Since there are five classes, this gives 20%
accuracy.
Most Frequent Selecting the class which most fre-
quently has the most votes (the class I under-
stand).
Baseline 1: Binary BoW This baseline uses logis-
tic regression on binary bag-of-word represen-
tations that are 1 if a word is present and 0 oth-
erwise.
Baseline 2: Features This model is similar to tra-
ditional approaches to sentiment classification
in that it uses many hand-engineered resources.
We first used a spell-checker and Wordnet to
map words and their misspellings to synsets to
reduce the total number of words. We then re-
placed sentiment words with a sentiment cat-
egory identifier using the sentiment lexica of
the Harvard Inquirer (Stone, 1966) and LIWC
(Pennebaker et al., 2007). Lastly, we used tf-idf
weighting on the bag-of-word representations
and trained an SVM.
</bodyText>
<page confidence="0.997778">
156
</page>
<bodyText confidence="0.5377496">
KL Predicted&amp;Gold V. Entry (Shortened if it ends with ...)
.03 .16 .16 .16 .33 .16 6 I reguarly shoplift. I got caught once and went to jail, but I’ve found that this was not a deterrent. I don’t buy
groceries, I don’t buy school supplies for my kids, I don’t buy gifts for my kids, we don’t pay for movies, and I
dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...
.03 .38 .04 .06 .35 .14 165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1
hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.
i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict...
.05 .14 .28 .14 .28 .14 7 Hi there, Im a guy that loves a girl, the same old bloody story... I met her a while ago, while studying, she Is so
perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so
did I with her, she has been the first person, male or female that has ever made that bond with me,...
.07 .27 .18 .00 .45 .09 11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i’ve ruined everything. i’ve
piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn’t feel because
i’ve never had you close enough. we’ve never touched, but i still feel as though a part of me is missing....
.05 23 Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more
and more frustrated that I have not found you yet. I’m also tired of spending so much heart on an old dream....
.05 5 I wish I knew somone to talk to here.
.06 24 I loved her but I screwed it up. Now she’s moved on. I’ll never have her again. I don’t know if I’ll ever stop
thinking about her.
.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do’s not care about how it affects me or my
sisters i want to care but the truthis i dont care if he dies
.13 6 well i think hairy women are attractive
.35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard. I need it. It’ll make my soul
feel a bit better :)
.36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12
years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just
can’t meet men. (before you judge, no Im not terribly picky!) What is wrong with me?
.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy. Then the teacher found out
it was one of us and made us go two days without freetime. It might be a little late now, but sorry guys it was
me haha
.92 4 My paper is due in less than 24 hours and I’m still dancing round my room!
</bodyText>
<tableCaption confidence="0.8691775">
Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,
left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The
5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher,
our model makes reasonable alternative label choices. Some entries are shortened.
</tableCaption>
<bodyText confidence="0.982777647058824">
Baseline 3: Word Vectors We can ignore the RAE
tree structure and only train softmax layers di-
rectly on the pre-trained words in order to influ-
ence the word vectors. This is followed by an
SVM trained on the average of the word vec-
tors.
We also experimented with latent Dirichlet alloca-
tion (Blei et al., 2003) but performance was very
low.
Table 3 shows the results for predicting the class
with the most votes. Even the approach that is based
on sentiment lexica and other resources is outper-
formed by our model by almost 3%, showing that
for tasks involving complex broad-range human sen-
timent, the often used sentiment lexica lack in cover-
age and traditional bag-of-words representations are
not powerful enough.
</bodyText>
<subsectionHeader confidence="0.736594">
4.3 EP: Predicting Sentiment Distributions
</subsectionHeader>
<bodyText confidence="0.9964305">
We now turn to evaluating our distribution-
prediction approach. In both this and the previous
</bodyText>
<table confidence="0.998057428571429">
Method Accuracy
Random 20.0
Most Frequent 38.1
Baseline 1: Binary BoW 46.4
Baseline 2: Features 47.0
Baseline 3: Word Vectors 45.5
RAE (our method) 50.1
</table>
<tableCaption confidence="0.999906">
Table 3: Accuracy of predicting the class with most votes.
</tableCaption>
<bodyText confidence="0.9998743">
maximum label task, we backprop using the gold
multinomial distribution as a target. Since we max-
imize likelihood and because we want to predict a
distribution that is closest to the distribution of labels
that people would assign to a story, we evaluate us-
ing KL divergence: KL(g||p) = Ei gi log(gi/pi),
where g is the gold distribution and p is the predicted
one. We report the average KL divergence, where a
smaller value indicates better predictive power. To
get an idea of the values of KL divergence, predict-
</bodyText>
<page confidence="0.995958">
157
</page>
<figureCaption confidence="0.995836">
Figure 4: Average KL-divergence between gold and pre-
dicted sentiment distributions (lower is better).
</figureCaption>
<bodyText confidence="0.999470142857143">
ing random distributions gives a an average of 1.2 in
KL divergence, predicting simply the average distri-
bution in the training data give 0.83. Fig. 4 shows
that our RAE-based model outperforms the other
baselines. Table 2 shows EP example entries with
predicted and gold distributions, as well as numbers
of votes.
</bodyText>
<subsectionHeader confidence="0.998768">
4.4 Binary Polarity Classification
</subsectionHeader>
<bodyText confidence="0.999978111111111">
In order to compare our approach to other meth-
ods we also show results on commonly used sen-
timent datasets: movie reviews4 (MR) (Pang and
Lee, 2005) and opinions5 (MPQA) (Wiebe et al.,
2005).We give statistical information on these and
the EP corpus in Table 1.
We compare to the state-of-the-art system of
(Nakagawa et al., 2010), a dependency tree based
classification method that uses CRFs with hidden
variables. We use the same training and testing regi-
men (10-fold cross validation) as well as their base-
lines: majority phrase voting using sentiment and
reversal lexica; rule-based reversal using a depen-
dency tree; Bag-of-Features and their full Tree-CRF
model. As shown in Table 4, our algorithm outper-
forms their approach on both datasets. For the movie
review (MR) data set, we do not use any hand-
designed lexica. An error analysis on the MPQA
dataset showed several cases of single words which
never occurred in the training set. Correctly classify-
ing these instances can only be the result of having
them in the original sentiment lexicon. Hence, for
the experiment on MPQA we added the same sen-
timent lexicon that (Nakagawa et al., 2010) used in
their system to our training set. This improved ac-
curacy from 86.0 to 86.4.Using the pre-trained word
vectors boosts performance by less than 1% com-
</bodyText>
<footnote confidence="0.999634666666667">
4www.cs.cornell.edu/people/pabo/
movie-review-data/
5www.cs.pitt.edu/mpqa/
</footnote>
<table confidence="0.999424285714286">
Method MR MPQA
Voting with two lexica 63.1 81.7
Rule-based reversal on trees 62.9 82.8
Bag of features with reversal 76.4 84.1
Tree-CRF (Nakagawa et al,’10) 77.3 86.1
RAE (random word init.) 76.8 85.7
RAE (our method) 77.7 86.4
</table>
<tableCaption confidence="0.9833115">
Table 4: Accuracy of sentiment classification on movie
review polarity (MR) and the MPQA dataset.
</tableCaption>
<figureCaption confidence="0.637065">
Figure 5: Accuracy on the development split of the MR
polarity dataset for different weightings of reconstruction
error and supervised cross-entropy error: err = αErec +
(1 − α)EcE.
</figureCaption>
<bodyText confidence="0.999933">
pared to randomly initialized word vectors (setting:
random word init). This shows that our method can
work well even in settings with little training data.
We visualize the semantic vectors that the recursive
autoencoder learns by listing n-grams that give the
highest probability for each polarity. Table 5 shows
such n-grams for different lengths when the RAE is
trained on the movie review polarity dataset.
On a 4-core machine, training time for the smaller
corpora such as the movie reviews takes around 3
hours and for the larger EP corpus around 12 hours
until convergence. Testing of hundreds of movie re-
views takes only a few seconds.
</bodyText>
<subsectionHeader confidence="0.999399">
4.5 Reconstruction vs. Classification Error
</subsectionHeader>
<bodyText confidence="0.999642333333333">
In this experiment, we show how the hyperparame-
ter α influences accuracy on the development set of
one of the cross-validation splits of the MR dataset.
This parameter essentially trade-off the supervised
and unsupervised parts of the objective. Fig. 5 shows
that a larger focus on the supervised objective is im-
portant but that a weight of α = 0.2 for the recon-
struction error prevents overfitting and achieves the
highest performance.
</bodyText>
<figure confidence="0.998945133333333">
0.8
0.7
0.6
Avg.Distr. BoW Features Word Vec. RAE
0.83
0.81
0.72
0.73
0.70
0.87
0.86
0.85
0.84
0.83
0 0.2 0.4 0.6 0.8 1
</figure>
<page confidence="0.987091">
158
</page>
<table confidence="0.862476095238095">
n Most negative n-grams Most positive n-grams
1 bad; boring; dull; flat; pointless; tv; neither; pretentious; badly; touching; enjoyable; powerful; warm; moving; culture; flaws;
worst; lame; mediocre; lack; routine; loud; bore; barely; stupid; provides; engrossing; wonderful; beautiful; quiet; socio-political;
tired; poorly; suffers; heavy;nor; choppy; superficial thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid;
2 how bad; by bad; dull .; for bad; to bad; boring .; , dull; are bad; the beautiful; moving,; thoughtful and; , inventive; solid and; a
that bad; boring ,; , flat; pointless .; badly by; on tv; so routine; lack beautiful; a beautifully; and hilarious; with dazzling; provides the;
the; mediocre .; a generic; stupid,; abysmally pathetic provides.; and inventive; as powerful; moving and; a moving; a
powerful
3 . too bad; exactly how bad; and never dull; shot but dull; is more funny and touching; a small gem; with a moving; cuts, fast; , fine
boring; to the dull; dull, UNK; it is bad; or just plain; by turns music; smart and taut; culture into a; romantic , riveting; ... a solid;
pretentious; manipulative and contrived; bag of stale; is a bad; the beautifully acted .; , gradually reveals; with the chilling; cast of
whole mildly; contrived pastiche of; from this choppy; stale mate- solid; has a solid; spare yet audacious; ... a polished; both the
rial. beauty;
5 boring than anything else.; a major waste ... generic; nothing i reminded us that a feel-good; engrossing, seldom UNK,; between
hadn’t already; ,UNK plotting;superficial; problem ? no laughs.; realistic characters showing honest; a solid piece of journalistic;
,just horribly mediocre .; dull, UNK feel.; there’s nothing exactly easily the most thoughtful fictional; cute, funny, heartwarming;
wrong; movie is about a boring; essentially a collection of bits with wry humor and genuine; engrossing and ultimately tragic.;
8 loud, silly, stupid and pointless.; dull, dumb and derivative horror shot in rich , shadowy black-and-white , devils an escapist con-
film.; UNK’s film, a boring, pretentious; this film biggest problem fection that ’s pure entertainment .; , deeply absorbing piece that
? no laughs.; film in the series looks and feels tired; do draw easy works as a; ... one of the most ingenious and entertaining; film is a
chuckles but lead nowhere.; stupid, infantile, redundant, sloppy riveting, brisk delight.; bringing richer meaning to the story ’s;
</table>
<tableCaption confidence="0.9397815">
Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model
predicts the most positive and most negative responses.
</tableCaption>
<sectionHeader confidence="0.999914" genericHeader="related work">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999392">
5.1 Autoencoders and Deep Learning
</subsectionHeader>
<bodyText confidence="0.999910875">
Autoencoders are neural networks that learn a re-
duced dimensional representation of fixed-size in-
puts such as image patches or bag-of-word repre-
sentations of text documents. They can be used to
efficiently learn feature encodings which are useful
for classification. Recently, Mirowski et al. (2010)
learn dynamic autoencoders for documents in a bag-
of-words format which, like ours, combine super-
vised and reconstruction objectives.
The idea of applying an autoencoder in a recursive
setting was introduced by Pollack (1990). Pollack’s
recursive auto-associative memories (RAAMs) are
similar to ours in that they are a connectionst, feed-
forward model. However, RAAMs learn vector
representations only for fixed recursive data struc-
tures, whereas our RAE builds this recursive data
structure. More recently, (Voegtlin and Dominey,
2005) introduced a linear modification to RAAMs
that is able to better generalize to novel combina-
tions of previously seen constituents. One of the
major shortcomings of previous applications of re-
cursive autoencoders to natural language sentences
was their binary word representation as discussed in
Sec. 2.1.
Recently, (Socher et al., 2010; Socher et al., 2011)
introduced a max-margin framework based on recur-
sive neural networks (RNNs) for labeled structure
prediction. Their models are applicable to natural
language and computer vision tasks such as parsing
or object detection. The current work is related in
that it uses a recursive deep learning model. How-
ever, RNNs require labeled tree structures and use a
supervised score at each node. Instead, RAEs learn
hierarchical structures that are trying to capture as
much of the the original word vectors as possible.
The learned structures are not necessarily syntacti-
cally plausible but can capture more of the semantic
content of the word vectors. Other recent deep learn-
ing methods for sentiment analysis include (Maas et
al., 2011).
</bodyText>
<subsectionHeader confidence="0.999289">
5.2 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999969333333333">
Pang et al. (2002) were one of the first to experiment
with sentiment classification. They show that sim-
ple bag-of-words approaches based on Naive Bayes,
MaxEnt models or SVMs are often insufficient for
predicting sentiment of documents even though they
work well for general topic-based document classi-
fication. Even adding specific negation words, bi-
grams or part-of-speech information to these mod-
els did not add significant improvements. Other
document-level sentiment work includes (Turney,
2002; Dave et al., 2003; Beineke et al., 2004; Pang
and Lee, 2004). For further references, see (Pang
and Lee, 2008).
Instead of document level sentiment classifica-
tion, (Wilson et al., 2005) analyze the contextual
polarity of phrases and incorporate many well de-
signed features including dependency trees. They
also show improvements by first distinguishing be-
</bodyText>
<page confidence="0.996491">
159
</page>
<bodyText confidence="0.999991196428572">
tween neutral and polar sentences. Our model natu-
rally incorporates the recursive interaction between
context and polarity words in sentences in a unified
framework while simultaneously learning the neces-
sary features to make accurate predictions. Other ap-
proaches for sentence-level sentiment detection in-
clude (Yu and Hatzivassiloglou, 2003; Grefenstette
et al., 2004; Ikeda et al., 2008).
Most previous work is centered around a given
sentiment lexicon or building one via heuristics
(Kim and Hovy, 2007; Esuli and Sebastiani, 2007),
manual annotation (Das and Chen, 2001) or machine
learning techniques (Turney, 2002). In contrast, we
do not require an initial or constructed sentiment lex-
icon of positive and negative words. In fact, when
training our approach on documents or sentences, it
jointly learns such lexica for both single words and
n-grams (see Table 5). (Mao and Lebanon, 2007)
propose isotonic conditional random fields and dif-
ferentiate between local, sentence-level and global,
document-level sentiment.
The work of (Polanyi and Zaenen, 2006; Choi and
Cardie, 2008) focuses on manually constructing sev-
eral lexica and rules for both polar words and re-
lated content-word negators, such as “prevent can-
cer”, where prevent reverses the negative polarity of
cancer. Like our approach they capture composi-
tional semantics. However, our model does so with-
out manually constructing any rules or lexica.
Recently, (Velikovich et al., 2010) showed how to
use a seed lexicon and a graph propagation frame-
work to learn a larger sentiment lexicon that also in-
cludes polar multi-word phrases such as “once in a
life time”. While our method can also learn multi-
word phrases it does not require a seed set or a large
web graph. (Nakagawa et al., 2010) introduced an
approach based on CRFs with hidden variables with
very good performance. We compare to their state-
of-the-art system. We outperform them on the stan-
dard corpora that we tested on without requiring
external systems such as POS taggers, dependency
parsers and sentiment lexica. Our approach jointly
learns the necessary features and tree structure.
In multi-aspect rating (Snyder and Barzilay, 2007)
one finds several distinct aspects such as food or ser-
vice in a restaurant and then rates them on a fixed
linear scale such as 1-5 stars, where all aspects could
obtain just 1 star or all aspects could obtain 5 stars
independently. In contrast, in our method a single
aspect (a complex reaction to a human experience)
is predicted not in terms of a fixed scale but in terms
of a multinomial distribution over several intercon-
nected, sometimes mutually exclusive emotions. A
single story cannot simultaneously obtain a strong
reaction in different emotional responses (by virtue
of having to sum to one).
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999755">
We presented a novel algorithm that can accurately
predict sentence-level sentiment distributions. With-
out using any hand-engineered resources such as
sentiment lexica, parsers or sentiment shifting rules,
our model achieves state-of-the-art performance on
commonly used sentiment datasets. Furthermore,
we introduce a new dataset that contains distribu-
tions over a broad range of human emotions. Our
evaluation shows that our model can more accu-
rately predict these distributions than other models.
</bodyText>
<sectionHeader confidence="0.997314" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999197785714286">
We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of DARPA, AFRL, or
the US government. This work was also supported in part
by the DARPA Deep Learning program under contract
number FA8650-10-C-7020.
We thank Chris Potts for help with the EP data set, Ray-
mond Hsu, Bozhi See, and Alan Wu for letting us use
their system as a baseline and Jiquan Ngiam, Quoc Le,
Gabor Angeli and Andrew Maas for their feedback.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997461818181818">
P. Beineke, T. Hastie, C. D. Manning, and
S. Vaithyanathan. 2004. Exploring sentiment
summarization. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137–1155.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet allocation. Journal of Machine Learning Re-
search., 3:993–1022.
</reference>
<page confidence="0.968658">
160
</page>
<reference confidence="0.998291944444445">
Y. Choi and C. Cardie. 2008. Learning with composi-
tional semantics as structural inference for subsenten-
tial sentiment analysis. In EMNLP.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160–167.
S. Das and M. Chen. 2001. Yahoo! for Amazon: Ex-
tracting market sentiment from stock message boards.
In Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519–528.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings of
the Conference on Web Search and Web Data Mining
(WSDM).
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3):195–225.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Proceedings of the Association for Computational Lin-
guistics (ACL).
C. Goller and A. K¨uchler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of Recherche d’Information Assist´ee par Ordinateur
(RIAO).
D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to shift the polarity of words for senti-
ment classification. In IJCNLP.
S. Kim and E. Hovy. 2007. Crystal: Analyzing predic-
tive opinions on the web. In EMNLP-CoNLL.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,
and C. Potts. 2011. Learning accurate, compact, and
interpretable tree annotation. In Proceedings ofACL.
Y. Mao and G. Lebanon. 2007. Isotonic Conditional
Random Fields and Local Sentiment Flow. In NIPS.
P. Mirowski, M. Ranzato, and Y. LeCun. 2010. Dynamic
auto-encoders for semantic indexing. In Proceedings
of the NIPS 2010 Workshop on Deep Learning.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115–124.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In EMNLP.
J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007.
Linguistic inquiry and word count: Liwc2007 opera-
tors manual. University of Texas.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46:77–105, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the Good Grief algorithm. In HLT-NAACL.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.
2011. Parsing Natural Scenes and Natural Language
with Recursive Neural Networks. In ICML.
P. J. Stone. 1966. The General Inquirer: A Computer
Approach to Content Analysis. The MIT Press.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384–394.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In ACL.
L. Velikovich, S. Blair-Goldensohn, K. Hannan, and
R. McDonald. 2010. The viability of web-derived po-
larity lexicons. In NAACL, HLT.
T. Voegtlin and P. Dominey. 2005. Linear Recursive Dis-
tributed Representations. Neural Networks, 18(7).
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT/EMNLP.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
EMNLP.
</reference>
<page confidence="0.998054">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938225">
<title confidence="0.99754">Semi-Supervised Recursive for Predicting Sentiment Distributions</title>
<author confidence="0.988736">Socher Jeffrey Eric H Huang Andrew Y Ng Christopher D</author>
<affiliation confidence="0.979924">Computer Science Department, Stanford University, Stanford, CA 94305,</affiliation>
<address confidence="0.977575">National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA</address>
<email confidence="0.999495">ang@cs.stanford.edu</email>
<abstract confidence="0.999634619047619">We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Beineke</author>
<author>T Hastie</author>
<author>C D Manning</author>
<author>S Vaithyanathan</author>
</authors>
<title>Exploring sentiment summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</booktitle>
<contexts>
<context position="37408" citStr="Beineke et al., 2004" startWordPosition="6226" endWordPosition="6229">ods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-le</context>
</contexts>
<marker>Beineke, Hastie, Manning, Vaithyanathan, 2004</marker>
<rawString>P. Beineke, T. Hastie, C. D. Manning, and S. Vaithyanathan. 2004. Exploring sentiment summarization. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="6651" citStr="Bengio et al., 2003" startWordPosition="971" endWordPosition="974">ord Representations We represent words as continuous vectors of parameters. We explore two settings. In the first setting we simply initialize each word vector x E Rn by sampling it from a zero mean Gaussian distribution: x — N(0, U2). These word vectors are then stacked into a word embedding matrix L E Rn×|V |, where |V |is the size of the vocabulary. This initialization works well in supervised settings where a network can subsequently modify these vectors to capture certain label distributions. In the second setting, we pre-train the word vectors with an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008). These models jointly learn an embedding of words into a vector space and use these vectors to predict how likely a word occurs given its context. After learning via gradient ascent the word vectors capture syntactic and semantic information from their co-occurrence statistics. In both cases we can use the resulting matrix of word vectors L for subsequent tasks as follows. Assume we are given a sentence as an ordered list of m words. Each word has an associated vocabulary index k into the embedding matrix which we use to retrieve the word’s vector representation. </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.,</journal>
<pages>3--993</pages>
<contexts>
<context position="27356" citStr="Blei et al., 2003" startWordPosition="4621" endWordPosition="4624">r on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The 5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher, our model makes reasonable alternative label choices. Some entries are shortened. Baseline 3: Word Vectors We can ignore the RAE tree structure and only train softmax layers directly on the pre-trained words in order to influence the word vectors. This is followed by an SVM trained on the average of the word vectors. We also experimented with latent Dirichlet allocation (Blei et al., 2003) but performance was very low. Table 3 shows the results for predicting the class with the most votes. Even the approach that is based on sentiment lexica and other resources is outperformed by our model by almost 3%, showing that for tasks involving complex broad-range human sentiment, the often used sentiment lexica lack in coverage and traditional bag-of-words representations are not powerful enough. 4.3 EP: Predicting Sentiment Distributions We now turn to evaluating our distributionprediction approach. In both this and the previous Method Accuracy Random 20.0 Most Frequent 38.1 Baseline 1</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>C Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="38814" citStr="Choi and Cardie, 2008" startWordPosition="6437" endWordPosition="6440">ing one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it do</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Y. Choi and C. Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6680" citStr="Collobert and Weston, 2008" startWordPosition="975" endWordPosition="978">e represent words as continuous vectors of parameters. We explore two settings. In the first setting we simply initialize each word vector x E Rn by sampling it from a zero mean Gaussian distribution: x — N(0, U2). These word vectors are then stacked into a word embedding matrix L E Rn×|V |, where |V |is the size of the vocabulary. This initialization works well in supervised settings where a network can subsequently modify these vectors to capture certain label distributions. In the second setting, we pre-train the word vectors with an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008). These models jointly learn an embedding of words into a vector space and use these vectors to predict how likely a word occurs given its context. After learning via gradient ascent the word vectors capture syntactic and semantic information from their co-occurrence statistics. In both cases we can use the resulting matrix of word vectors L for subsequent tasks as follows. Assume we are given a sentence as an ordered list of m words. Each word has an associated vocabulary index k into the embedding matrix which we use to retrieve the word’s vector representation. Mathematically, this look-up </context>
<context position="19143" citStr="Collobert and Weston (2008)" startWordPosition="3133" endWordPosition="3137">assifier. We first describe the new experience project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions. We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model. In all experiments involving our model, we represent words using 100-dimensional word vectors. We explore the two settings mentioned in Sec. 2.1. We compare performance on standard datasets when using randomly initialized word vectors (random word init.) or word vectors trained by the model of Collobert and Weston (2008) and provided by Turian et al. (2010).2 These vectors were trained on an unlabeled corpus of the English Wikipedia. Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation. 2http://metaoptimize.com/projects/ wordreprs/ 155 Corpus K Instances Distr.(+/-) Avg|W| MPQA 2 10,624 0.31/0.69 3 MR 2 10,662 0.5/0.5 22 EP 5 31,675 .2/.2/.1/.4/.1 113 EP≥ 4 5 6,129 .2/.2/.1/.4/.1 129 Table 1: Statistics on the different datasets. K is the number of classes. Distr.</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Das</author>
<author>M Chen</author>
</authors>
<title>Yahoo! for Amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).</booktitle>
<contexts>
<context position="38303" citStr="Das and Chen, 2001" startWordPosition="6359" endWordPosition="6362"> improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and relat</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>S. Das and M. Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="37386" citStr="Dave et al., 2003" startWordPosition="6222" endWordPosition="6225"> deep learning methods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other appr</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>K. Dave, S. Lawrence, and D. M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ding</author>
<author>B Liu</author>
<author>P S Yu</author>
</authors>
<title>A holistic lexiconbased approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Web Search and Web Data Mining (WSDM).</booktitle>
<contexts>
<context position="2902" citStr="Ding et al., 2008" startWordPosition="403" endWordPosition="406"> same autoencoder network into a fixed length sentence representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow i walked into a park</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexiconbased approach to opinion mining. In Proceedings of the Conference on Web Search and Web Data Mining (WSDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--2</pages>
<contexts>
<context position="7846" citStr="Elman, 1991" startWordPosition="1177" endWordPosition="1179"> representation. Mathematically, this look-up operation can be seen as a simple projection layer where we use a binary vector b which is zero in all positions except at the kth index, xi = Lbk E Rn. (1) In the remainder of this paper, we represent a sentence (or any n-gram) as an ordered list of these vectors (x1, ... , xm). This word representation is better suited to autoencoders than the binary number representations used in previous related autoencoder models such as the recursive autoassociative memory (RAAM) model (Pollack, 1990; Voegtlin and Dominey, 2005) or recurrent neural networks (Elman, 1991) since sigmoid units are inherently continuous. Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors. 152 Figure 2: Illustration of an application of a recursive autoencoder to a binary tree. The nodes which are not filled are only used to compute reconstruction errors. A standard autoencoder (in box) is re-used at each node of the tree. 2.2 Traditional Recursive Autoencoders The goal of autoencoders is to learn a representation of their inputs. In this section we describe how to obtain a re</context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>J. L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7(2-3):195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Pageranking wordnet synsets: An application to opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="38263" citStr="Esuli and Sebastiani, 2007" startWordPosition="6353" endWordPosition="6356">tures including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica </context>
</contexts>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>A. Esuli and F. Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Goller</author>
<author>A K¨uchler</author>
</authors>
<title>Learning taskdependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Neural Networks (ICNN-96).</booktitle>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>C. Goller and A. K¨uchler. 1996. Learning taskdependent distributed representations by backpropagation through structure. In Proceedings of the International Conference on Neural Networks (ICNN-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
<author>Y Qu</author>
<author>J G Shanahan</author>
<author>D A Evans</author>
</authors>
<title>Coupling niche browsers and affect analysis for an opinion mining application.</title>
<date>2004</date>
<booktitle>In Proceedings of Recherche d’Information Assist´ee par Ordinateur (RIAO).</booktitle>
<contexts>
<context position="38097" citStr="Grefenstette et al., 2004" startWordPosition="6326" endWordPosition="6329">, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between loca</context>
</contexts>
<marker>Grefenstette, Qu, Shanahan, Evans, 2004</marker>
<rawString>G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans. 2004. Coupling niche browsers and affect analysis for an opinion mining application. In Proceedings of Recherche d’Information Assist´ee par Ordinateur (RIAO).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ikeda</author>
<author>H Takamura</author>
<author>L Ratinov</author>
<author>M Okumura</author>
</authors>
<title>Learning to shift the polarity of words for sentiment classification.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="38118" citStr="Ikeda et al., 2008" startWordPosition="6330" endWordPosition="6333">t level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and</context>
</contexts>
<marker>Ikeda, Takamura, Ratinov, Okumura, 2008</marker>
<rawString>D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura. 2008. Learning to shift the polarity of words for sentiment classification. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Crystal: Analyzing predictive opinions on the web. In EMNLP-CoNLL.</title>
<date>2007</date>
<contexts>
<context position="38234" citStr="Kim and Hovy, 2007" startWordPosition="6349" endWordPosition="6352">ny well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually</context>
</contexts>
<marker>Kim, Hovy, 2007</marker>
<rawString>S. Kim and E. Hovy. 2007. Crystal: Analyzing predictive opinions on the web. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Maas</author>
<author>R E Daly</author>
<author>P T Pham</author>
<author>D Huang</author>
<author>A Y Ng</author>
<author>C Potts</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="36842" citStr="Maas et al., 2011" startWordPosition="6141" endWordPosition="6144">Their models are applicable to natural language and computer vision tasks such as parsing or object detection. The current work is related in that it uses a recursive deep learning model. However, RNNs require labeled tree structures and use a supervised score at each node. Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible. The learned structures are not necessarily syntactically plausible but can capture more of the semantic content of the word vectors. Other recent deep learning methods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. 2011. Learning accurate, compact, and interpretable tree annotation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mao</author>
<author>G Lebanon</author>
</authors>
<title>Isotonic Conditional Random Fields and Local Sentiment Flow.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="38623" citStr="Mao and Lebanon, 2007" startWordPosition="6411" endWordPosition="6414">ence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a gra</context>
</contexts>
<marker>Mao, Lebanon, 2007</marker>
<rawString>Y. Mao and G. Lebanon. 2007. Isotonic Conditional Random Fields and Local Sentiment Flow. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mirowski</author>
<author>M Ranzato</author>
<author>Y LeCun</author>
</authors>
<title>Dynamic auto-encoders for semantic indexing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS 2010 Workshop on Deep Learning.</booktitle>
<contexts>
<context position="35219" citStr="Mirowski et al. (2010)" startWordPosition="5892" endWordPosition="5895">d, infantile, redundant, sloppy riveting, brisk delight.; bringing richer meaning to the story ’s; Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model predicts the most positive and most negative responses. 5 Related Work 5.1 Autoencoders and Deep Learning Autoencoders are neural networks that learn a reduced dimensional representation of fixed-size inputs such as image patches or bag-of-word representations of text documents. They can be used to efficiently learn feature encodings which are useful for classification. Recently, Mirowski et al. (2010) learn dynamic autoencoders for documents in a bagof-words format which, like ours, combine supervised and reconstruction objectives. The idea of applying an autoencoder in a recursive setting was introduced by Pollack (1990). Pollack’s recursive auto-associative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model. However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure. More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to bette</context>
</contexts>
<marker>Mirowski, Ranzato, LeCun, 2010</marker>
<rawString>P. Mirowski, M. Ranzato, and Y. LeCun. 2010. Dynamic auto-encoders for semantic indexing. In Proceedings of the NIPS 2010 Workshop on Deep Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>K Inui</author>
<author>S Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In NAACL, HLT.</booktitle>
<contexts>
<context position="29412" citStr="Nakagawa et al., 2010" startWordPosition="4959" endWordPosition="4962">vergence, predicting simply the average distribution in the training data give 0.83. Fig. 4 shows that our RAE-based model outperforms the other baselines. Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes. 4.4 Binary Polarity Classification In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1. We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model. As shown in Table 4, our algorithm outperforms their approach on both datasets. For the movie review (MR) data set, we do not use any handdesigned lexica. An error analysis on the MPQA dataset showed several cases of single words which never occurred in the trai</context>
<context position="39485" citStr="Nakagawa et al., 2010" startWordPosition="6552" endWordPosition="6555">and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it does not require a seed set or a large web graph. (Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance. We compare to their stateof-the-art system. We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica. Our approach jointly learns the necessary features and tree structure. In multi-aspect rating (Snyder and Barzilay, 2007) one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1-5 stars, where all aspects could obtain just 1 star or all </context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In NAACL, HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="37429" citStr="Pang and Lee, 2004" startWordPosition="6230" endWordPosition="6233">ysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detecti</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2833" citStr="Pang and Lee, 2005" startWordPosition="392" endWordPosition="395"> semantic vector space (blue). Then they are recursively merged by the same autoencoder network into a fixed length sentence representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Sorry,</context>
<context position="29230" citStr="Pang and Lee, 2005" startWordPosition="4929" endWordPosition="4932">gence, predict157 Figure 4: Average KL-divergence between gold and predicted sentiment distributions (lower is better). ing random distributions gives a an average of 1.2 in KL divergence, predicting simply the average distribution in the training data give 0.83. Fig. 4 shows that our RAE-based model outperforms the other baselines. Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes. 4.4 Binary Polarity Classification In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1. We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model. As shown in Table 4, our algorithm outperforms their approach on both datasets. </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1587" citStr="Pang and Lee, 2008" startWordPosition="207" endWordPosition="210">experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. 1 Introduction The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews. Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (Pang and Lee, 2008). Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis (Pang et al., 2002). For instance, while the two phrases “white blood cells destroying an infection” and “an infection destroying white blood cells” have the same bag-of-words representation, the former is a positive reaction while the later is very negative. More advanced methods such as (Nakagawa et al., Figure 1: Illustration of our recursive autoencoder architecture which learns semantic vector representations of phrases. Word indices (oran</context>
<context position="37479" citStr="Pang and Lee, 2008" startWordPosition="6238" endWordPosition="6241">alysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefens</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1758" citStr="Pang et al., 2002" startWordPosition="230" endWordPosition="233">onal reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. 1 Introduction The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews. Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (Pang and Lee, 2008). Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis (Pang et al., 2002). For instance, while the two phrases “white blood cells destroying an infection” and “an infection destroying white blood cells” have the same bag-of-words representation, the former is a positive reaction while the later is very negative. More advanced methods such as (Nakagawa et al., Figure 1: Illustration of our recursive autoencoder architecture which learns semantic vector representations of phrases. Word indices (orange) are first mapped into a semantic vector space (blue). Then they are recursively merged by the same autoencoder network into a fixed length sentence representation. The</context>
<context position="36885" citStr="Pang et al. (2002)" startWordPosition="6148" endWordPosition="6151">uage and computer vision tasks such as parsing or object detection. The current work is related in that it uses a recursive deep learning model. However, RNNs require labeled tree structures and use a supervised score at each node. Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible. The learned structures are not necessarily syntactically plausible but can capture more of the semantic content of the word vectors. Other recent deep learning methods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Inst</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>R J Booth</author>
<author>M E Francis</author>
</authors>
<title>Linguistic inquiry and word count: Liwc2007 operators manual.</title>
<date>2007</date>
<institution>University of Texas.</institution>
<contexts>
<context position="23629" citStr="Pennebaker et al., 2007" startWordPosition="3873" endWordPosition="3876">st votes (the class I understand). Baseline 1: Binary BoW This baseline uses logistic regression on binary bag-of-word representations that are 1 if a word is present and 0 otherwise. Baseline 2: Features This model is similar to traditional approaches to sentiment classification in that it uses many hand-engineered resources. We first used a spell-checker and Wordnet to map words and their misspellings to synsets to reduce the total number of words. We then replaced sentiment words with a sentiment category identifier using the sentiment lexica of the Harvard Inquirer (Stone, 1966) and LIWC (Pennebaker et al., 2007). Lastly, we used tf-idf weighting on the bag-of-word representations and trained an SVM. 156 KL Predicted&amp;Gold V. Entry (Shortened if it ends with ...) .03 .16 .16 .16 .33 .16 6 I reguarly shoplift. I got caught once and went to jail, but I’ve found that this was not a deterrent. I don’t buy groceries, I don’t buy school supplies for my kids, I don’t buy gifts for my kids, we don’t pay for movies, and I dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)... .03 .38 .04 .06 .35 .14 165 i am a very succesfull buissnes man.i make good money but i have been addicted to c</context>
</contexts>
<marker>Pennebaker, Booth, Francis, 2007</marker>
<rawString>J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007. Linguistic inquiry and word count: Liwc2007 operators manual. University of Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual valence shifters.</title>
<date>2006</date>
<contexts>
<context position="38790" citStr="Polanyi and Zaenen, 2006" startWordPosition="6433" endWordPosition="6436">sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>L. Polanyi and A. Zaenen. 2006. Contextual valence shifters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<contexts>
<context position="7774" citStr="Pollack, 1990" startWordPosition="1167" endWordPosition="1168">ex k into the embedding matrix which we use to retrieve the word’s vector representation. Mathematically, this look-up operation can be seen as a simple projection layer where we use a binary vector b which is zero in all positions except at the kth index, xi = Lbk E Rn. (1) In the remainder of this paper, we represent a sentence (or any n-gram) as an ordered list of these vectors (x1, ... , xm). This word representation is better suited to autoencoders than the binary number representations used in previous related autoencoder models such as the recursive autoassociative memory (RAAM) model (Pollack, 1990; Voegtlin and Dominey, 2005) or recurrent neural networks (Elman, 1991) since sigmoid units are inherently continuous. Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors. 152 Figure 2: Illustration of an application of a recursive autoencoder to a binary tree. The nodes which are not filled are only used to compute reconstruction errors. A standard autoencoder (in box) is re-used at each node of the tree. 2.2 Traditional Recursive Autoencoders The goal of autoencoders is to learn a repres</context>
<context position="10935" citStr="Pollack, 1990" startWordPosition="1736" endWordPosition="1737">tation (p) of two n-dimensional children (c1, c2), we can describe how such a network can be used for the rest of the tree. Essentially, the same steps repeat. Now that y1 is given, we can use Eq. 2 to compute y2 by setting the children to be (c1, c2) = (x2, y1). Again, after computing the intermediate parent vector y2, we can assess how well this vector capture the content of the children by computing the reconstruction error as in Eq. 4. The process repeat until the full tree is constructed and we have a reconstruction error at each nonterminal node. This model is similar to the RAAM model (Pollack, 1990) which also requires a fixed tree structure. 2.3 Unsupervised Recursive Autoencoder for Structure Prediction Now, assume there is no tree structure given for the input vectors in x. The goal of our structureprediction RAE is to minimize the reconstruction error of all vector pairs of children in a tree. We define A(x) as the set of all possible trees that can be built from an input sentence x. Further, let T (y) be a function that returns the triplets of a tree indexed by s of all the non-terminal nodes in a tree. Using the reconstruction error of Eq. 4, we compute 1: RAEθ(x) = arg min Erec([c</context>
<context position="35444" citStr="Pollack (1990)" startWordPosition="5928" endWordPosition="5929">t positive and most negative responses. 5 Related Work 5.1 Autoencoders and Deep Learning Autoencoders are neural networks that learn a reduced dimensional representation of fixed-size inputs such as image patches or bag-of-word representations of text documents. They can be used to efficiently learn feature encodings which are useful for classification. Recently, Mirowski et al. (2010) learn dynamic autoencoders for documents in a bagof-words format which, like ours, combine supervised and reconstruction objectives. The idea of applying an autoencoder in a recursive setting was introduced by Pollack (1990). Pollack’s recursive auto-associative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model. However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure. More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents. One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discus</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J. B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46:77–105, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Potts</author>
</authors>
<title>On the negativity of negation.</title>
<date>2010</date>
<booktitle>In David Lutz and Nan Li, editors, Proceedings of Semantics and Linguistic Theory 20.</booktitle>
<publisher>CLC Publications,</publisher>
<location>Ithaca, NY.</location>
<contexts>
<context position="4574" citStr="Potts, 2010" startWordPosition="651" endWordPosition="652"> sentiments. We introduce an approach based on semisupervised, recursive autoencoders (RAE) which use as input continuous word vectors. Fig. 1 shows an illustration of the model which learns vector representations of phrases and full sentences as well as their hierarchical structure from unsupervised text. We extend our model to also learn a distribution over sentiment labels at each node of the hierarchy. We evaluate our approach on several standard datasets where we achieve state-of-the art performance. Furthermore, we show results on the recently introduced experience project (EP) dataset (Potts, 2010) that captures a broader spectrum of human sentiments and emotions. The dataset consists of very personal confessions anonymously made by people on the experience project website www.experienceproject.com. Confessions are labeled with a set of five reactions by other users. Reaction labels are you rock (expressing approvement), tehee (amusement), I understand, Sorry, hugs and Wow, just wow (displaying shock). For evaluation on this dataset we predict both the label with the most votes as well as the full distribution over the sentiment categories. On both tasks our model outperforms competitiv</context>
</contexts>
<marker>Potts, 2010</marker>
<rawString>C. Potts. 2010. On the negativity of negation. In David Lutz and Nan Li, editors, Proceedings of Semantics and Linguistic Theory 20. CLC Publications, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Multiple aspect ranking using the Good Grief algorithm.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="2965" citStr="Snyder and Barzilay, 2007" startWordPosition="412" endWordPosition="415">representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow i walked into a parked car Recursive Autoencoder Predicted Sentiment Distribution S</context>
<context position="39896" citStr="Snyder and Barzilay, 2007" startWordPosition="6614" endWordPosition="6617">iment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it does not require a seed set or a large web graph. (Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance. We compare to their stateof-the-art system. We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica. Our approach jointly learns the necessary features and tree structure. In multi-aspect rating (Snyder and Barzilay, 2007) one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1-5 stars, where all aspects could obtain just 1 star or all aspects could obtain 5 stars independently. In contrast, in our method a single aspect (a complex reaction to a human experience) is predicted not in terms of a fixed scale but in terms of a multinomial distribution over several interconnected, sometimes mutually exclusive emotions. A single story cannot simultaneously obtain a strong reaction in different emotional responses (by virtue of having to sum to o</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>B. Snyder and R. Barzilay. 2007. Multiple aspect ranking using the Good Grief algorithm. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="13485" citStr="Socher et al., 2010" startWordPosition="2170" endWordPosition="2173">her: (x1,p(2,(3,4))) or (p(1,2),p(3,4)). Both states would then finish with a deterministic choice of collapsing the remaining two states into one parent to obtain (p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree is then recovered by unfolding the collapsing decisions. The resulting tree structure captures as much of the single-word information as possible (in order to allow reconstructing the word vectors) but does not necessarily follow standard syntactic constraints. We also experimented with a method that finds better solutions to Eq. 5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster. Weighted Reconstruction. One problem with simply using the reconstruction error of both children equally as describe in Eq. 4 is that each child could represent a different number of previously collapsed words and is hence of bigger importance for the overall meaning reconstruction of the sentence. For instance in the case of (x1,p(2,(3,4))) one would like to give more importance to reconstructing p than x1. We capture this desideratum by adjusting the reconstruction error. Let n1, n2 be the number of </context>
<context position="36091" citStr="Socher et al., 2010" startWordPosition="6021" endWordPosition="6024">ssociative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model. However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure. More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents. One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec. 2.1. Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction. Their models are applicable to natural language and computer vision tasks such as parsing or object detection. The current work is related in that it uses a recursive deep learning model. However, RNNs require labeled tree structures and use a supervised score at each node. Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible. The learned structures are not necessarily syntactically plausible </context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C C Lin</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="13507" citStr="Socher et al., 2011" startWordPosition="2174" endWordPosition="2177">or (p(1,2),p(3,4)). Both states would then finish with a deterministic choice of collapsing the remaining two states into one parent to obtain (p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree is then recovered by unfolding the collapsing decisions. The resulting tree structure captures as much of the single-word information as possible (in order to allow reconstructing the word vectors) but does not necessarily follow standard syntactic constraints. We also experimented with a method that finds better solutions to Eq. 5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster. Weighted Reconstruction. One problem with simply using the reconstruction error of both children equally as describe in Eq. 4 is that each child could represent a different number of previously collapsed words and is hence of bigger importance for the overall meaning reconstruction of the sentence. For instance in the case of (x1,p(2,(3,4))) one would like to give more importance to reconstructing p than x1. We capture this desideratum by adjusting the reconstruction error. Let n1, n2 be the number of words underneath a cur</context>
<context position="36113" citStr="Socher et al., 2011" startWordPosition="6025" endWordPosition="6028">RAAMs) are similar to ours in that they are a connectionst, feedforward model. However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure. More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents. One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec. 2.1. Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction. Their models are applicable to natural language and computer vision tasks such as parsing or object detection. The current work is related in that it uses a recursive deep learning model. However, RNNs require labeled tree structures and use a supervised score at each node. Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible. The learned structures are not necessarily syntactically plausible but can capture more o</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="23594" citStr="Stone, 1966" startWordPosition="3869" endWordPosition="3870">t frequently has the most votes (the class I understand). Baseline 1: Binary BoW This baseline uses logistic regression on binary bag-of-word representations that are 1 if a word is present and 0 otherwise. Baseline 2: Features This model is similar to traditional approaches to sentiment classification in that it uses many hand-engineered resources. We first used a spell-checker and Wordnet to map words and their misspellings to synsets to reduce the total number of words. We then replaced sentiment words with a sentiment category identifier using the sentiment lexica of the Harvard Inquirer (Stone, 1966) and LIWC (Pennebaker et al., 2007). Lastly, we used tf-idf weighting on the bag-of-word representations and trained an SVM. 156 KL Predicted&amp;Gold V. Entry (Shortened if it ends with ...) .03 .16 .16 .16 .33 .16 6 I reguarly shoplift. I got caught once and went to jail, but I’ve found that this was not a deterrent. I don’t buy groceries, I don’t buy school supplies for my kids, I don’t buy gifts for my kids, we don’t pay for movies, and I dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)... .03 .38 .04 .06 .35 .14 165 i am a very succesfull buissnes man.i make good </context>
</contexts>
<marker>Stone, 1966</marker>
<rawString>P. J. Stone. 1966. The General Inquirer: A Computer Approach to Content Analysis. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="19180" citStr="Turian et al. (2010)" startWordPosition="3141" endWordPosition="3144">ce project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions. We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model. In all experiments involving our model, we represent words using 100-dimensional word vectors. We explore the two settings mentioned in Sec. 2.1. We compare performance on standard datasets when using randomly initialized word vectors (random word init.) or word vectors trained by the model of Collobert and Weston (2008) and provided by Turian et al. (2010).2 These vectors were trained on an unlabeled corpus of the English Wikipedia. Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation. 2http://metaoptimize.com/projects/ wordreprs/ 155 Corpus K Instances Distr.(+/-) Avg|W| MPQA 2 10,624 0.31/0.69 3 MR 2 10,662 0.5/0.5 22 EP 5 31,675 .2/.2/.1/.4/.1 113 EP≥ 4 5 6,129 .2/.2/.1/.4/.1 129 Table 1: Statistics on the different datasets. K is the number of classes. Distr. is the distribution of the different</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="37367" citStr="Turney, 2002" startWordPosition="6220" endWordPosition="6221">. Other recent deep learning methods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate pred</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Velikovich</author>
<author>S Blair-Goldensohn</author>
<author>K Hannan</author>
<author>R McDonald</author>
</authors>
<title>The viability of web-derived polarity lexicons.</title>
<date>2010</date>
<booktitle>In NAACL, HLT.</booktitle>
<contexts>
<context position="39180" citStr="Velikovich et al., 2010" startWordPosition="6494" endWordPosition="6497">oth single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it does not require a seed set or a large web graph. (Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance. We compare to their stateof-the-art system. We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica. Our a</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>L. Velikovich, S. Blair-Goldensohn, K. Hannan, and R. McDonald. 2010. The viability of web-derived polarity lexicons. In NAACL, HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Voegtlin</author>
<author>P Dominey</author>
</authors>
<title>Linear Recursive Distributed Representations.</title>
<date>2005</date>
<journal>Neural Networks,</journal>
<volume>18</volume>
<issue>7</issue>
<contexts>
<context position="7803" citStr="Voegtlin and Dominey, 2005" startWordPosition="1169" endWordPosition="1172">mbedding matrix which we use to retrieve the word’s vector representation. Mathematically, this look-up operation can be seen as a simple projection layer where we use a binary vector b which is zero in all positions except at the kth index, xi = Lbk E Rn. (1) In the remainder of this paper, we represent a sentence (or any n-gram) as an ordered list of these vectors (x1, ... , xm). This word representation is better suited to autoencoders than the binary number representations used in previous related autoencoder models such as the recursive autoassociative memory (RAAM) model (Pollack, 1990; Voegtlin and Dominey, 2005) or recurrent neural networks (Elman, 1991) since sigmoid units are inherently continuous. Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors. 152 Figure 2: Illustration of an application of a recursive autoencoder to a binary tree. The nodes which are not filled are only used to compute reconstruction errors. A standard autoencoder (in box) is re-used at each node of the tree. 2.2 Traditional Recursive Autoencoders The goal of autoencoders is to learn a representation of their inputs. In </context>
<context position="35755" citStr="Voegtlin and Dominey, 2005" startWordPosition="5970" endWordPosition="5973">arn feature encodings which are useful for classification. Recently, Mirowski et al. (2010) learn dynamic autoencoders for documents in a bagof-words format which, like ours, combine supervised and reconstruction objectives. The idea of applying an autoencoder in a recursive setting was introduced by Pollack (1990). Pollack’s recursive auto-associative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model. However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure. More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents. One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec. 2.1. Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction. Their models are applicable to natural language and computer vision tasks such as parsing or object detection. The current work is </context>
</contexts>
<marker>Voegtlin, Dominey, 2005</marker>
<rawString>T. Voegtlin and P. Dominey. 2005. Linear Recursive Distributed Representations. Neural Networks, 18(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<contexts>
<context position="2864" citStr="Wiebe et al., 2005" startWordPosition="397" endWordPosition="400">Then they are recursively merged by the same autoencoder network into a fixed length sentence representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Sorry, Hugs You Rock Teehee I Underst</context>
<context position="29272" citStr="Wiebe et al., 2005" startWordPosition="4936" endWordPosition="4939">ergence between gold and predicted sentiment distributions (lower is better). ing random distributions gives a an average of 1.2 in KL divergence, predicting simply the average distribution in the training data give 0.83. Fig. 4 shows that our RAE-based model outperforms the other baselines. Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes. 4.4 Binary Polarity Classification In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1. We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model. As shown in Table 4, our algorithm outperforms their approach on both datasets. For the movie review (MR) data set, we do </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="37554" citStr="Wilson et al., 2005" startWordPosition="6249" endWordPosition="6252">nt classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered aro</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="38070" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="6322" endWordPosition="6325">r references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing be159 tween neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>