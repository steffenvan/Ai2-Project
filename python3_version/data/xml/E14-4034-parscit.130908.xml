<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015041">
<title confidence="0.9852305">
Simple and Effective Approach for Consistent Training of Hierarchical
Phrase-based Translation Models
</title>
<author confidence="0.627275">
Stephan Peitz&apos; and David Vilar2 and Hermann Ney&apos;
&apos; Lehrstuhl f¨ur Informatik 6
</author>
<affiliation confidence="0.792904">
Computer Science Department 2 Pixformance GmbH
RWTH Aachen University D-10587 Berlin, Germany
</affiliation>
<address confidence="0.829453">
D-52056 Aachen, Germany david.vilar@gmail.com
</address>
<email confidence="0.998558">
{peitz,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.994787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920235294118">
In this paper, we present a simple ap-
proach for consistent training of hierarchi-
cal phrase-based translation models. In
order to consistently train a translation
model, we perform hierarchical phrase-
based decoding on training data to find
derivations between the source and tar-
get sentences. This is done by syn-
chronous parsing the given sentence pairs.
After extracting k-best derivations, we
reestimate the translation model proba-
bilities based on collected rule counts.
We show the effectiveness of our proce-
dure on the IWSLT German—*English and
English—*French translation tasks. Our
results show improvements of up to 1.6
points BLEU.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982535714286">
In state of the art statistical machine translation
systems, the translation model is estimated by fol-
lowing heuristic: Given bilingual training data,
a word alignment is trained with tools such as
GIZA++ (Och and Ney, 2003) or fast align (Dyer
et al., 2013). Then, all valid translation pairs are
extracted and the translation probabilities are com-
puted as relative frequencies (Koehn et al., 2003).
However, this extraction method causes several
problems. First, this approach does not consider,
whether a translation pair is extracted from a likely
alignment or not. Further, during the extraction
process, models employed in decoding are not
considered.
For phrase-based translation, a successful ap-
proach addressing these issues is presented in
(Wuebker et al., 2010). By applying a phrase-
based decoder on the source sentences of the train-
ing data and constraining the translations to the
corresponding target sentences, k-best segmenta-
tions are produced. Then, the phrases used for
these segmentations are extracted and counted.
Based on the counts, the translation model prob-
abilities are recomputed. To avoid over-fitting,
leave-one-out is applied.
However, for hierarchical phrase-based transla-
tion an equivalent approach is still missing.
In this paper, we present a simple and effec-
tive approach for consistent reestimation of the
translation model probabilities in a hierarchical
phrase-based translation setup. Using a heuristi-
cally extracted translation model as starting point,
the training data are parsed bilingually. From the
resulting hypergraphs, we extract k-best deriva-
tions and the rules applied in each derivation. This
is done with a top-down k-best parsing algorithm.
Finally, the translation model probabilities are re-
computed based on the counts of the extracted
rules. In our procedure, we employ leave-one-out
to avoid over-fitting. Further, we consider all mod-
els which are used in translation to ensure a con-
sistent training.
Experimental results are presented on the
German—*English and English—*French IWSLT
shared machine translation task (Cettolo et al.,
2013). We are able to gain improvements of up to
1.6% BLEU absolute and 1.4% TER over a com-
petitive baseline. On all tasks and test sets, the
improvements are statistically significant with at
least 99% confidence.
The paper is structured as follow. First, we re-
vise the state of the art hierarchical phrase-based
extraction and translation process. In Section 3,
we propose our training procedure. Finally, ex-
perimental results are given in Section 4 and we
conclude with Section 5.
</bodyText>
<sectionHeader confidence="0.930794" genericHeader="method">
2 Hierarchical Phrase-based Translation
</sectionHeader>
<bodyText confidence="0.99952075">
In hierarchical phrase-based translation (Chiang,
2005), discontinuous phrases with “gaps” are
allowed. The translation model is formalized
as a synchronous context-free grammar (SCFG)
</bodyText>
<page confidence="0.976113">
174
</page>
<note confidence="0.689553">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–179,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997745333333333">
and consists of bilingual rules, which are based
on bilingual standard phrases and discontinuous
phrases. Each bilingual rule rewrites a generic
non-terminal X into a pair of strings f˜ and e˜
with both terminals and non-terminals in both lan-
guages
</bodyText>
<equation confidence="0.993225">
X → h˜f, ˜ei. (1)
</equation>
<bodyText confidence="0.999582875">
In a standard hierarchical phrase-based translation
setup, obtaining these rules is based on a heuristic
extraction from automatically word-aligned bilin-
gual training data. Just like in the phrase-based
approach, all bilingual rules of a sentence pair
are extracted given an alignment. The standard
phrases are stored as lexical rules in the rule set.
In addition, whenever a phrase contains a sub-
phrase, this sub-phrase is replaced by a generic
non-terminal X. With these hierarchical phrases
we can define the hierarchical rules in the SCFG.
The rule probabilities which are in general defined
as relative frequencies are computed based on the
joint counts C(X → h ˜f, ˜ei) of a bilingual rule
The translation probabilities are computed in
source-to-target as well as in target-to-source di-
rection. In the translation processes, these proba-
bilities are integrated in the log-linear combination
among other models such as a language model,
word lexicon models, word and phrase penalty and
binary features marking hierarchical phrases, glue
rule and rules with non-terminals at the bound-
aries.
The translation process of hierarchical phrase-
based approach can be considered as parsing prob-
lem. Given an input sentence in the source lan-
guage, this sentence is parsed using the source lan-
guage part of the SCFG. In this work, we perform
this step with a modified version of the CYK+ al-
gorithm (Chappelier and Rajman, 1998). The out-
put of this algorithm is a hypergraph, which rep-
resents all possible derivations of the input sen-
tence. A derivation represents an application of
rules from the grammar to generate the given in-
put sentence. Using the the associated target part
of the applied rule, for each derivation a transla-
tion can be constructed. In a second step, the lan-
guage model score is incorporated. Given the hy-
pergraph, this is done with the cube pruning algo-
rithm presented in (Chiang, 2007).
</bodyText>
<sectionHeader confidence="0.992863" genericHeader="method">
3 Translation Model Training
</sectionHeader>
<bodyText confidence="0.999946266666667">
We propose following pipeline for consistent hi-
erarchical phrase-based training: First we train a
word alignment, from which the baseline trans-
lation model is extracted as described in the pre-
vious section. The log-linear parameter weights
are tuned with MERT (Och, 2003) on a develop-
ment set to produce the baseline system. Next,
we perform decoding on the training data. As the
translations are constrained to the given target sen-
tences, we name this step forced decoding in the
following. Details are given in the next subsection.
Given the counts CFD(X → h ˜f, ˜ei) of the rules,
which have been applied in the forced decoding
step, the translation probabilities pFD(˜f|˜e) for the
translation model are recomputed:
</bodyText>
<equation confidence="0.997134">
˜f|˜e) = CFD(X → h
</equation>
<bodyText confidence="0.999987">
Finally, using the translation model with the
reestimated probabilities, we retune the log-linear
parameter weights and obtain our final system.
</bodyText>
<subsectionHeader confidence="0.99738">
3.1 Forced Decoding
</subsectionHeader>
<bodyText confidence="0.999973153846154">
In this section, we describe the forced decoding
for hierarchical phrase-based translation in detail.
Given a sentence pair of the training data, we
constrain the translation of the source sentence to
produce the corresponding target sentence. For
this constrained decoding process, the language
model score is constant as the translation is fixed.
Hence, the incorporation of the a language model
is not needed. This results in a simplification of
the decoding process as we do not have to employ
the cube pruning algorithm as described in the pre-
vious section. Consequently, forced decoding for
hierarchical phrase-based translation is equivalent
to synchronous parsing of the training data. Dyer
(2010) has described an approach to reduce the
average-case run-time of synchronous parsing by
splitting one bilingual parse into two successive
monolingual parses. We adopt this method and
first parse the source sentence and then the target
sentence with CYK+.
If the given sentence pair has been parsed suc-
cessfully, we employ a top-down k-best parsing
algorithm (Chiang and Huang, 2005) on the re-
sulting hypergraph to find the k-best derivations
between the given source and target sentence. In
this step, all models of the translation process are
</bodyText>
<equation confidence="0.99687725">
X → h
˜f|˜e) =
C(X → h˜f, ˜ei) ˜f�, ˜ei).(2)
˜f, ˜ei
pH(
E
˜f, C(X → h
pFD(
E
˜f, CFD(X → h
˜f, ˜ei) (3)
f&amp;quot; ei)
</equation>
<page confidence="0.986442">
175
</page>
<bodyText confidence="0.999955">
included (except for the language model). Further,
leave-one-out is applied to counteract overfitting.
Note, that the model weights of the baseline sys-
tem are used to perform forced decoding.
Finally, we extract and count the rules which
have been applied in the derivations. These counts
are used to recompute the translation probabilities.
</bodyText>
<subsectionHeader confidence="0.998936">
3.2 Recombination
</subsectionHeader>
<bodyText confidence="0.996546260869565">
In standard hierarchical phrase-based decoding,
partial derivations that are indistinguishable from
each other are recombined. In (Huck et al., 2013)
two schemes are presented. Either derivations that
produce identical translations or derivations with
identical language model context are recombined.
As in forced decoding the translation is fixed and
a language model is missing, both schemes are not
suitable.
However, a recombination scheme is necessary
to avoid derivations with the same application
of rules. Further, recombining such derivations
increases simultaneously the amounts of consid-
ered derivations during k-best parsing. Given two
derivations with the same set of applied rules, the
order of application of the rules may be different.
Thus, we propose following scheme for recom-
bining derivations in forced decoding: Derivations
that produce identical sets of applied rules are re-
combined. Figure 1 shows an example for k = 3.
Employing the proposed scheme, derivations d1
and d2 are recombined since both share the same
set of applied rules ({r1, r3, r2}).
</bodyText>
<figureCaption confidence="0.9937495">
Figure 1: Example search space before (a) and af-
ter (b) applying recombination.
</figureCaption>
<sectionHeader confidence="0.998469" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.950611">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.6095945">
The experiments were carried out on the IWSLT
2013 German→English shared translation task.1
</bodyText>
<footnote confidence="0.99294">
1http://www.iwslt2013.org
</footnote>
<table confidence="0.998748">
German English English French
Sentences 4.32M 5.23M
Run. Words 108M 109M 133M 147M
Vocabulary 836K 792K 845K 888K
</table>
<tableCaption confidence="0.985799">
Table 1: Statistics for the bilingual training
</tableCaption>
<bodyText confidence="0.996649181818182">
data of the IWSLT 2013 German→English and
English→French task.
It is focusing the translation of TED talks. Bilin-
gual data statistics are given in Table 1. The base-
line system was trained on all available bilingual
data and used a 4-gram LM with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998), trained with the SRILM toolkit
(Stolcke, 2002). As additional data sources for the
LM we selected parts of the Shuffled News and
LDC English Gigaword corpora based on cross-
entropy difference (Moore and Lewis, 2010). In
all experiments, the hierarchical search was per-
formed as described in Section 2.
To confirm the efficacy of our approach, addi-
tional experiments were run on the IWSLT 2013
English→French task. Statistics are given in Ta-
ble 1.
The training pipeline was set up as described
in the previous section. Tuning of the log-linear
parameter weights was done with MERT on a pro-
vided development set. As optimization criterion
we used BLEU (Papineni et al., 2001).
Forced decoding was performed on the TED
talks portion of the training data (∼140K sen-
tences). In both tasks, around 5% of the sentences
could not be parsed. In this work, we just skipped
those sentences.
We report results in BLEU [%] and TER [%]
(Snover et al., 2006). All reported results are av-
erages over three independent MERT runs, and
we evaluated statistical significance with MultE-
val (Clark et al., 2011).
</bodyText>
<subsectionHeader confidence="0.573304">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9998278">
Figure 2 shows the performance of setups us-
ing translation models with reestimated translation
probabilities. The setups vary in the k-best deriva-
tion size extracted in the forced decoding (fd) step.
Based on the performance on the development set,
we selected two setups with k = 500 using leave-
one-out (+l1o) and k = 750 without leave-one-
out (-l1o). Table 2 shows the final results for
the German→English task. Performing consistent
translation model training improves the translation
</bodyText>
<equation confidence="0.99283725">
d1 : {r1, r3, r2} d1 : {r1, r3, r2}
d2 : {r3, r2, r1} d3 : {r4, r5, r1, r2}
d3 : {r4, r5, r1, r2} d4 : {r6, r5, r2, r3}
(a) (b)
</equation>
<page confidence="0.996943">
176
</page>
<table confidence="0.9984938">
dev* eval11 test
BLEU1%1 TER1%1 BLEU1%1 TER1%1 BLEU1%1 TER1%1
baseline 33.1 46.8 35.7 44.1 30.5 49.7
forced decoding -l1o 33.2 46.3 36.3 43.4 31.2 48.8
forced decoding +l1o 33.6 46.2 36.6 43.0 31.8 48.3
</table>
<tableCaption confidence="0.964906333333333">
Table 2: Results for the IWSLT 2013 German-*English task. The development set used for MERT is
marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the
baseline are printed in boldface.
</tableCaption>
<table confidence="0.999805">
dev* eval11 test
BLEU1%1 TER1%1 BLEU1%1 TER1%1 BLEU1%1 TER1%1
baseline 28.1 55.7 37.5 42.7 31.7 49.5
forced decoding +l1o 28.8 55.0 39.1 41.6 32.4 49.0
</table>
<tableCaption confidence="0.957649333333333">
Table 3: Results for the IWSLT 2013 English-*French task. The development set used for MERT is
marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the
baseline are printed in boldface.
</tableCaption>
<figure confidence="0.819983">
1 10 100 1000 10000
k
</figure>
<figureCaption confidence="0.9930524">
Figure 2: BLEU scores on the IWSLT
German-*English task of setups using trans-
lation models trained with different k-best
derivation sizes. Results are reported on dev with
(+l1o) and without leave-one-out (-l1o).
</figureCaption>
<bodyText confidence="0.9990013125">
quality on all test sets significantly. We gain an
improvement of up to 0.7 points in BLEU and 0.9
points in TER. Applying leave-one-out results in
an additional improvement by up to 0.4 % BLEU
and 0.5 % TER. The results for English-*French
are given in Table 3. We observe a similar im-
provement by up to 1.6 % BLEU and 1.1 % TER.
The improvements could be the effect of do-
main adaptation since we performed forced decod-
ing on the TED talks portion of the training data.
Thus, rules which were applied to decode the in-
domain data might get higher translation probabil-
ities.
Furthermore, employing leave-one-out seems to
avoid overfitting as the average source rule length
in training is reduced from 5.0 to 3.5 (k = 500).
</bodyText>
<sectionHeader confidence="0.992718" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999757875">
We have presented a simple and effective approach
for consistent training of hierarchical phrase-based
translation models. By reducing hierarchical de-
coding on parallel training data to synchronous
parsing, we were able to reestimate the trans-
lation probabilities including all models applied
during the translation process. On the IWSLT
German-*English and English-*French tasks, the
final results show statistically significant improve-
ments of up to 1.6 points in BLEU and 1.4 points
in TER.
Our implementation was released as part of Jane
(Vilar et al., 2010; Vilar et al., 2012; Huck et al.,
2012; Freitag et al., 2014), the RWTH Aachen
University open source statistical machine trans-
lation toolkit.2
</bodyText>
<sectionHeader confidence="0.987032" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999381">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreements no 287658 and no 287755.
</bodyText>
<footnote confidence="0.707925">
2http://www.hltpr.rwth-aachen.de/jane/
</footnote>
<figure confidence="0.90735">
BLEU[%]
33.5
32.5
31.5
34
33
32
dev fd +l1o
dev fd -l1o
dev baseline
</figure>
<page confidence="0.98361">
177
</page>
<sectionHeader confidence="0.975695" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999677285714286">
Mauro Cettolo, Jan Nieheus, Sebastian St¨uker, Luisa
Bentivogli, and Marcello Federico. 2013. Report on
the 10th iwslt evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Heidelberg, Germany, December.
J.-C. Chappelier and M. Rajman. 1998. A general-
ized CYK algorithm for parsing stochastic CFG. In
Proceedings of the First Workshop on Tabulation in
Parsing and Deduction, pages 133–137, April.
Stanley F. Chen and Joshuo Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
David Chiang and Liang Huang. 2005. Better k-best
Parsing. In Proceedings of the 9th Internation Work-
shop on Parsing Technologies, pages 53–64, Octo-
ber.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 263–270,
Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228,
June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176–181, Portland, Oregon,
June.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A Simple, Fast, and Effective Reparameter-
ization of IBM Model 2. In Proceedings of NAACL-
HLT, pages 644–648, Atlanta, Georgia, June.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In In Proc. of HLT-
NAACL.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open source machine translation sys-
tem combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April. To appear.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin ofMathematical Linguistics, 98:37–
50, October.
Matthias Huck, David Vilar, Markus Freitag, and Her-
mann Ney. 2013. A performance study of cube
pruning for large-scale hierarchical machine transla-
tion. In Proceedings of the NAACL 7th Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 29–38, Atlanta, Georgia, USA, June.
Reinerd Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181–184, May.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127–133, Edmonton, Alberta.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220–224, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901–904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197–216, September.
</reference>
<page confidence="0.980237">
178
</page>
<reference confidence="0.9963664">
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475–484, Uppsala, Sweden, July.
</reference>
<page confidence="0.9988">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.268829">
<title confidence="0.994268">Simple and Effective Approach for Consistent Training of Phrase-based Translation Models</title>
<affiliation confidence="0.706759">f¨ur Informatik 6 Science Department 2Pixformance GmbH</affiliation>
<address confidence="0.9756765">RWTH Aachen University D-10587 Berlin, Germany Aachen, Germany</address>
<abstract confidence="0.977942647058824">In this paper, we present a simple approach for consistent training of hierarchical phrase-based translation models. In order to consistently train a translation model, we perform hierarchical phrasebased decoding on training data to find derivations between the source and target sentences. This is done by synchronous parsing the given sentence pairs. extracting derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our proceon the IWSLT and translation tasks. Our results show improvements of up to 1.6</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Jan Nieheus</author>
<author>Sebastian St¨uker</author>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
</authors>
<title>Report on the 10th iwslt evaluation campaign.</title>
<date>2013</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<location>Heidelberg, Germany,</location>
<marker>Cettolo, Nieheus, St¨uker, Bentivogli, Federico, 2013</marker>
<rawString>Mauro Cettolo, Jan Nieheus, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th iwslt evaluation campaign. In Proc. of the International Workshop on Spoken Language Translation, Heidelberg, Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Chappelier</author>
<author>M Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of the First Workshop on Tabulation in Parsing and Deduction,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="5716" citStr="Chappelier and Rajman, 1998" startWordPosition="858" endWordPosition="861">irection. In the translation processes, these probabilities are integrated in the log-linear combination among other models such as a language model, word lexicon models, word and phrase penalty and binary features marking hierarchical phrases, glue rule and rules with non-terminals at the boundaries. The translation process of hierarchical phrasebased approach can be considered as parsing problem. Given an input sentence in the source language, this sentence is parsed using the source language part of the SCFG. In this work, we perform this step with a modified version of the CYK+ algorithm (Chappelier and Rajman, 1998). The output of this algorithm is a hypergraph, which represents all possible derivations of the input sentence. A derivation represents an application of rules from the grammar to generate the given input sentence. Using the the associated target part of the applied rule, for each derivation a translation can be constructed. In a second step, the language model score is incorporated. Given the hypergraph, this is done with the cube pruning algorithm presented in (Chiang, 2007). 3 Translation Model Training We propose following pipeline for consistent hierarchical phrase-based training: First </context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>J.-C. Chappelier and M. Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proceedings of the First Workshop on Tabulation in Parsing and Deduction, pages 133–137, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshuo Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="10638" citStr="Chen and Goodman, 1998" startWordPosition="1644" endWordPosition="1647"> Experiments 4.1 Setup The experiments were carried out on the IWSLT 2013 German→English shared translation task.1 1http://www.iwslt2013.org German English English French Sentences 4.32M 5.23M Run. Words 108M 109M 133M 147M Vocabulary 836K 792K 845K 888K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English and English→French task. It is focusing the translation of TED talks. Bilingual data statistics are given in Table 1. The baseline system was trained on all available bilingual data and used a 4-gram LM with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on crossentropy difference (Moore and Lewis, 2010). In all experiments, the hierarchical search was performed as described in Section 2. To confirm the efficacy of our approach, additional experiments were run on the IWSLT 2013 English→French task. Statistics are given in Table 1. The training pipeline was set up as described in the previous section. Tuning of the log-linear parameter weights was done with MERT on a provided devel</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshuo Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Liang Huang</author>
</authors>
<title>Better k-best Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Internation Workshop on Parsing Technologies,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="8221" citStr="Chiang and Huang, 2005" startWordPosition="1259" endWordPosition="1262">cess as we do not have to employ the cube pruning algorithm as described in the previous section. Consequently, forced decoding for hierarchical phrase-based translation is equivalent to synchronous parsing of the training data. Dyer (2010) has described an approach to reduce the average-case run-time of synchronous parsing by splitting one bilingual parse into two successive monolingual parses. We adopt this method and first parse the source sentence and then the target sentence with CYK+. If the given sentence pair has been parsed successfully, we employ a top-down k-best parsing algorithm (Chiang and Huang, 2005) on the resulting hypergraph to find the k-best derivations between the given source and target sentence. In this step, all models of the translation process are X → h ˜f|˜e) = C(X → h˜f, ˜ei) ˜f�, ˜ei).(2) ˜f, ˜ei pH( E ˜f, C(X → h pFD( E ˜f, CFD(X → h ˜f, ˜ei) (3) f&amp;quot; ei) 175 included (except for the language model). Further, leave-one-out is applied to counteract overfitting. Note, that the model weights of the baseline system are used to perform forced decoding. Finally, we extract and count the rules which have been applied in the derivations. These counts are used to recompute the transla</context>
</contexts>
<marker>Chiang, Huang, 2005</marker>
<rawString>David Chiang and Liang Huang. 2005. Better k-best Parsing. In Proceedings of the 9th Internation Workshop on Parsing Technologies, pages 53–64, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3685" citStr="Chiang, 2005" startWordPosition="544" endWordPosition="545">translation task (Cettolo et al., 2013). We are able to gain improvements of up to 1.6% BLEU absolute and 1.4% TER over a competitive baseline. On all tasks and test sets, the improvements are statistically significant with at least 99% confidence. The paper is structured as follow. First, we revise the state of the art hierarchical phrase-based extraction and translation process. In Section 3, we propose our training procedure. Finally, experimental results are given in Section 4 and we conclude with Section 5. 2 Hierarchical Phrase-based Translation In hierarchical phrase-based translation (Chiang, 2005), discontinuous phrases with “gaps” are allowed. The translation model is formalized as a synchronous context-free grammar (SCFG) 174 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–179, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics and consists of bilingual rules, which are based on bilingual standard phrases and discontinuous phrases. Each bilingual rule rewrites a generic non-terminal X into a pair of strings f˜ and e˜ with both terminals and non-terminals in both languages X → h˜</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6198" citStr="Chiang, 2007" startWordPosition="944" endWordPosition="945">ge part of the SCFG. In this work, we perform this step with a modified version of the CYK+ algorithm (Chappelier and Rajman, 1998). The output of this algorithm is a hypergraph, which represents all possible derivations of the input sentence. A derivation represents an application of rules from the grammar to generate the given input sentence. Using the the associated target part of the applied rule, for each derivation a translation can be constructed. In a second step, the language model score is incorporated. Given the hypergraph, this is done with the cube pruning algorithm presented in (Chiang, 2007). 3 Translation Model Training We propose following pipeline for consistent hierarchical phrase-based training: First we train a word alignment, from which the baseline translation model is extracted as described in the previous section. The log-linear parameter weights are tuned with MERT (Och, 2003) on a development set to produce the baseline system. Next, we perform decoding on the training data. As the translations are constrained to the given target sentences, we name this step forced decoding in the following. Details are given in the next subsection. Given the counts CFD(X → h ˜f, ˜ei)</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="11728" citStr="Clark et al., 2011" startWordPosition="1828" endWordPosition="1831">as set up as described in the previous section. Tuning of the log-linear parameter weights was done with MERT on a provided development set. As optimization criterion we used BLEU (Papineni et al., 2001). Forced decoding was performed on the TED talks portion of the training data (∼140K sentences). In both tasks, around 5% of the sentences could not be parsed. In this work, we just skipped those sentences. We report results in BLEU [%] and TER [%] (Snover et al., 2006). All reported results are averages over three independent MERT runs, and we evaluated statistical significance with MultEval (Clark et al., 2011). 4.2 Results Figure 2 shows the performance of setups using translation models with reestimated translation probabilities. The setups vary in the k-best derivation size extracted in the forced decoding (fd) step. Based on the performance on the development set, we selected two setups with k = 500 using leaveone-out (+l1o) and k = 750 without leave-oneout (-l1o). Table 2 shows the final results for the German→English task. Performing consistent translation model training improves the translation d1 : {r1, r3, r2} d1 : {r1, r3, r2} d2 : {r3, r2, r1} d3 : {r4, r5, r1, r2} d3 : {r4, r5, r1, r2} d</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A Simple, Fast, and Effective Reparameterization of IBM Model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<pages>644--648</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1273" citStr="Dyer et al., 2013" startWordPosition="182" endWordPosition="185">s is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German—*English and English—*French translation tasks. Our results show improvements of up to 1.6 points BLEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and c</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A Simple, Fast, and Effective Reparameterization of IBM Model 2. In Proceedings of NAACLHLT, pages 644–648, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Two monolingual parses are better than one (synchronous parse). In</title>
<date>2010</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="7838" citStr="Dyer (2010)" startWordPosition="1201" endWordPosition="1202">detail. Given a sentence pair of the training data, we constrain the translation of the source sentence to produce the corresponding target sentence. For this constrained decoding process, the language model score is constant as the translation is fixed. Hence, the incorporation of the a language model is not needed. This results in a simplification of the decoding process as we do not have to employ the cube pruning algorithm as described in the previous section. Consequently, forced decoding for hierarchical phrase-based translation is equivalent to synchronous parsing of the training data. Dyer (2010) has described an approach to reduce the average-case run-time of synchronous parsing by splitting one bilingual parse into two successive monolingual parses. We adopt this method and first parse the source sentence and then the target sentence with CYK+. If the given sentence pair has been parsed successfully, we employ a top-down k-best parsing algorithm (Chiang and Huang, 2005) on the resulting hypergraph to find the k-best derivations between the given source and target sentence. In this step, all models of the translation process are X → h ˜f|˜e) = C(X → h˜f, ˜ei) ˜f�, ˜ei).(2) ˜f, ˜ei pH</context>
</contexts>
<marker>Dyer, 2010</marker>
<rawString>Chris Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In In Proc. of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source machine translation system combination.</title>
<date>2014</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Gothenburg, Sweden,</location>
<note>To appear.</note>
<marker>Freitag, Huck, Ney, 2014</marker>
<rawString>Markus Freitag, Matthias Huck, and Hermann Ney. 2014. Jane: Open source machine translation system combination. In Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, April. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Jan-Thorsten Peter</author>
<author>Markus Freitag</author>
<author>Stephan Peitz</author>
<author>Hermann Ney</author>
</authors>
<date>2012</date>
<booktitle>Hierarchical Phrase-Based Translation with Jane 2. The Prague Bulletin ofMathematical Linguistics, 98:37– 50,</booktitle>
<marker>Huck, Peter, Freitag, Peitz, Ney, 2012</marker>
<rawString>Matthias Huck, Jan-Thorsten Peter, Markus Freitag, Stephan Peitz, and Hermann Ney. 2012. Hierarchical Phrase-Based Translation with Jane 2. The Prague Bulletin ofMathematical Linguistics, 98:37– 50, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>David Vilar</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>A performance study of cube pruning for large-scale hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the NAACL 7th Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>29--38</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="9008" citStr="Huck et al., 2013" startWordPosition="1391" endWordPosition="1394">f|˜e) = C(X → h˜f, ˜ei) ˜f�, ˜ei).(2) ˜f, ˜ei pH( E ˜f, C(X → h pFD( E ˜f, CFD(X → h ˜f, ˜ei) (3) f&amp;quot; ei) 175 included (except for the language model). Further, leave-one-out is applied to counteract overfitting. Note, that the model weights of the baseline system are used to perform forced decoding. Finally, we extract and count the rules which have been applied in the derivations. These counts are used to recompute the translation probabilities. 3.2 Recombination In standard hierarchical phrase-based decoding, partial derivations that are indistinguishable from each other are recombined. In (Huck et al., 2013) two schemes are presented. Either derivations that produce identical translations or derivations with identical language model context are recombined. As in forced decoding the translation is fixed and a language model is missing, both schemes are not suitable. However, a recombination scheme is necessary to avoid derivations with the same application of rules. Further, recombining such derivations increases simultaneously the amounts of considered derivations during k-best parsing. Given two derivations with the same set of applied rules, the order of application of the rules may be differen</context>
</contexts>
<marker>Huck, Vilar, Freitag, Ney, 2013</marker>
<rawString>Matthias Huck, David Vilar, Markus Freitag, and Hermann Ney. 2013. A performance study of cube pruning for large-scale hierarchical machine translation. In Proceedings of the NAACL 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 29–38, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinerd Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processingw,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="10613" citStr="Kneser and Ney, 1995" startWordPosition="1640" endWordPosition="1643">lying recombination. 4 Experiments 4.1 Setup The experiments were carried out on the IWSLT 2013 German→English shared translation task.1 1http://www.iwslt2013.org German English English French Sentences 4.32M 5.23M Run. Words 108M 109M 133M 147M Vocabulary 836K 792K 845K 888K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English and English→French task. It is focusing the translation of TED talks. Bilingual data statistics are given in Table 1. The baseline system was trained on all available bilingual data and used a 4-gram LM with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on crossentropy difference (Moore and Lewis, 2010). In all experiments, the hierarchical search was performed as described in Section 2. To confirm the efficacy of our approach, additional experiments were run on the IWSLT 2013 English→French task. Statistics are given in Table 1. The training pipeline was set up as described in the previous section. Tuning of the log-linear parameter weights was done with</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinerd Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processingw, volume 1, pages 181–184, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1414" citStr="Koehn et al., 2003" startWordPosition="203" endWordPosition="206">ities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German—*English and English—*French translation tasks. Our results show improvements of up to 1.6 points BLEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced. Then, the phrases used for these segme</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03), pages 127–133, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>220--224</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10854" citStr="Moore and Lewis, 2010" startWordPosition="1679" endWordPosition="1682">M 147M Vocabulary 836K 792K 845K 888K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English and English→French task. It is focusing the translation of TED talks. Bilingual data statistics are given in Table 1. The baseline system was trained on all available bilingual data and used a 4-gram LM with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on crossentropy difference (Moore and Lewis, 2010). In all experiments, the hierarchical search was performed as described in Section 2. To confirm the efficacy of our approach, additional experiments were run on the IWSLT 2013 English→French task. Statistics are given in Table 1. The training pipeline was set up as described in the previous section. Tuning of the log-linear parameter weights was done with MERT on a provided development set. As optimization criterion we used BLEU (Papineni et al., 2001). Forced decoding was performed on the TED talks portion of the training data (∼140K sentences). In both tasks, around 5% of the sentences cou</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R.C. Moore and W. Lewis. 2010. Intelligent Selection of Language Model Training Data. In ACL (Short Papers), pages 220–224, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1239" citStr="Och and Ney, 2003" startWordPosition="175" endWordPosition="178">e source and target sentences. This is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German—*English and English—*French translation tasks. Our results show improvements of up to 1.6 points BLEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source se</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6500" citStr="Och, 2003" startWordPosition="990" endWordPosition="991">mar to generate the given input sentence. Using the the associated target part of the applied rule, for each derivation a translation can be constructed. In a second step, the language model score is incorporated. Given the hypergraph, this is done with the cube pruning algorithm presented in (Chiang, 2007). 3 Translation Model Training We propose following pipeline for consistent hierarchical phrase-based training: First we train a word alignment, from which the baseline translation model is extracted as described in the previous section. The log-linear parameter weights are tuned with MERT (Och, 2003) on a development set to produce the baseline system. Next, we perform decoding on the training data. As the translations are constrained to the given target sentences, we name this step forced decoding in the following. Details are given in the next subsection. Given the counts CFD(X → h ˜f, ˜ei) of the rules, which have been applied in the forced decoding step, the translation probabilities pFD(˜f|˜e) for the translation model are recomputed: ˜f|˜e) = CFD(X → h Finally, using the translation model with the reestimated probabilities, we retune the log-linear parameter weights and obtain our f</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, P.O. Box</journal>
<volume>218</volume>
<pages>10598</pages>
<location>Yorktown Heights, NY</location>
<contexts>
<context position="11312" citStr="Papineni et al., 2001" startWordPosition="1756" endWordPosition="1759"> additional data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on crossentropy difference (Moore and Lewis, 2010). In all experiments, the hierarchical search was performed as described in Section 2. To confirm the efficacy of our approach, additional experiments were run on the IWSLT 2013 English→French task. Statistics are given in Table 1. The training pipeline was set up as described in the previous section. Tuning of the log-linear parameter weights was done with MERT on a provided development set. As optimization criterion we used BLEU (Papineni et al., 2001). Forced decoding was performed on the TED talks portion of the training data (∼140K sentences). In both tasks, around 5% of the sentences could not be parsed. In this work, we just skipped those sentences. We report results in BLEU [%] and TER [%] (Snover et al., 2006). All reported results are averages over three independent MERT runs, and we evaluated statistical significance with MultEval (Clark et al., 2011). 4.2 Results Figure 2 shows the performance of setups using translation models with reestimated translation probabilities. The setups vary in the k-best derivation size extracted in t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="11582" citStr="Snover et al., 2006" startWordPosition="1805" endWordPosition="1808">cy of our approach, additional experiments were run on the IWSLT 2013 English→French task. Statistics are given in Table 1. The training pipeline was set up as described in the previous section. Tuning of the log-linear parameter weights was done with MERT on a provided development set. As optimization criterion we used BLEU (Papineni et al., 2001). Forced decoding was performed on the TED talks portion of the training data (∼140K sentences). In both tasks, around 5% of the sentences could not be parsed. In this work, we just skipped those sentences. We report results in BLEU [%] and TER [%] (Snover et al., 2006). All reported results are averages over three independent MERT runs, and we evaluated statistical significance with MultEval (Clark et al., 2011). 4.2 Results Figure 2 shows the performance of setups using translation models with reestimated translation probabilities. The setups vary in the k-best derivation size extracted in the forced decoding (fd) step. Based on the performance on the development set, we selected two setups with k = 500 using leaveone-out (+l1o) and k = 750 without leave-oneout (-l1o). Table 2 shows the final results for the German→English task. Performing consistent trans</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO,</location>
<contexts>
<context position="10686" citStr="Stolcke, 2002" startWordPosition="1653" endWordPosition="1654">n the IWSLT 2013 German→English shared translation task.1 1http://www.iwslt2013.org German English English French Sentences 4.32M 5.23M Run. Words 108M 109M 133M 147M Vocabulary 836K 792K 845K 888K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English and English→French task. It is focusing the translation of TED talks. Bilingual data statistics are given in Table 1. The baseline system was trained on all available bilingual data and used a 4-gram LM with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on crossentropy difference (Moore and Lewis, 2010). In all experiments, the hierarchical search was performed as described in Section 2. To confirm the efficacy of our approach, additional experiments were run on the IWSLT 2013 English→French task. Statistics are given in Table 1. The training pipeline was set up as described in the previous section. Tuning of the log-linear parameter weights was done with MERT on a provided development set. As optimization criterion we used BL</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source hierarchical translation, extended with reordering and lexicon models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open source hierarchical translation, extended with reordering and lexicon models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: an advanced freely available hierarchical machine translation toolkit.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Vilar, Stein, Huck, Ney, 2012</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2012. Jane: an advanced freely available hierarchical machine translation toolkit. Machine Translation, 26(3):197–216, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>475--484</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1787" citStr="Wuebker et al., 2010" startWordPosition="257" endWordPosition="260">a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced. Then, the phrases used for these segmentations are extracted and counted. Based on the counts, the translation model probabilities are recomputed. To avoid over-fitting, leave-one-out is applied. However, for hierarchical phrase-based translation an equivalent approach is still missing. In this paper, we present a simple and effective approach for consistent reestimation of the translation model probabilitie</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics, pages 475–484, Uppsala, Sweden, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>