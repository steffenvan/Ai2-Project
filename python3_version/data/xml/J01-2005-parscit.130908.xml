<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.871809">
Squibs and Discussions
Nonminimal Derivations in Unification-based
Parsing
</title>
<author confidence="0.997928">
Noriko Tomuro* Steven L. Lytinent
</author>
<affiliation confidence="0.998443">
DePaul University DePaul University
</affiliation>
<bodyText confidence="0.95996475">
Shieber&apos;s abstract parsing algorithm (Shieber 1992) for unification grammars is an extension of
Earley&apos;s algorithm (Earley 1970) for context-free grammars to feature structures. In this paper,
we show that, under certain conditions, Shieber&apos;s algorithm produces what we call a nonminimal
derivation: a parse tree which contains additional features that are not in the licensing productions.
While Shieber&apos;s definition of parse tree allows for such nonminimal derivations, we claim that they
should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and
propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm
which ensures minimality, although at some computational cost.
</bodyText>
<sectionHeader confidence="0.990292" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999671">
Unification grammar is a term often used to describe a family of feature-based gram-
mar formalisms, including GPSG (Gazdar et al. 1985), PATR-II (Shieber 1986), DCG
(Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize
the common elements of unification-style grammars, Shieber (1992) developed a logic
for describing them, and used this logic to define an abstract parsing algorithm. The
algorithm uses the same set of operations as Earley&apos;s (1970) algorithm for context-free
grammars, but modified for unification grammars.
In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces
unintended, spurious parses in addition to the intended ones. We call these spurious
parses nonminimal derivations (or nonminimal parse trees), because they contain
extra features which are not in the productions that license the parse.1 We claim that
such nonminimal derivations are invalid. The basis of our claim is that the unifica-
tion operation as set union preserves minimality; thus any correct unification-based
parsing algorithm should produce parses that contain all and only features from the
licensing productions (i.e., minimal derivations or minimal parse trees). Nonminimal
derivations are also undesirable in practice because, given a parse tree, we cannot tell
whether a particular feature should be in the model or not unless we reconstruct the
whole tree.
Despite the nonminimal derivations, Shieber (1992) proved the correctness of his
algorithm. As it turned out, his definition of parse tree, which his proof relied on, was
</bodyText>
<affiliation confidence="0.821201666666667">
* School of Computer Science, Telecommunications and Information Systems, Chicago, IL 60604. E-mail:
tomuro@cs.depaul.edu
School of Computer Science, Telecommunications and Information Systems, Chicago, IL 60604. E-mail:
</affiliation>
<email confidence="0.83033">
lytinen@cs.depaul.edu
</email>
<footnote confidence="0.969454666666667">
1 In this paper, we use &amp;quot;nonminimal derivations&amp;quot; synonymously with &amp;quot;nonminimal parses&amp;quot;. Normally
the notions of derivation and parse tree are different. However, in this paper we focus on parse trees as
the final result of derivation, thus we mean that a derivation is nonminimal when its result is a
nonminimal parse, in contrast to a minimal derivation which produces a minimal parse. Unfortunately,
formal definitions of minimal and nonminimal derivations are outside the scope of this short paper;
interested readers are encouraged to read Tomuro (1999).
</footnote>
<note confidence="0.846086111111111">
© 2001 Association for Computational Linguistics
Computational Linguistics Volume 27, Number 2
{ (cat) = S (cat) -= NP
(1 cat) Zr NP pi = (&amp;quot;John&amp;quot;, 4)1 : { (head agr pers) 3rd )
(2 cat) = VP (head agr num) =sing)
po (2, &apos;DO : (cat) = V
(head) = (2 head) P3 = (&amp;quot;sleeps&amp;quot;, : (head agr pers) = 3rd ,
(head subj) =- (1 head) (head agr num) = sing
(head agr) rr (1 head agr) (head tense) = pres
</note>
<equation confidence="0.7526665">
{ (cat) -= VP
(1 cat) = V
P2 (1, (1)2 (head) -= (1 head) )
(head type) = intrans
</equation>
<figureCaption confidence="0.828349">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.974884">
Examples of productions.
</subsectionHeader>
<bodyText confidence="0.99996375">
not constraining enough to disallow nonminimal derivations. To solve this twofold
problem, we propose an alternate definition of minimal parse tree for unification gram-
mars, and present a modification to Shieber &apos;s algorithm which ensures minimality.
It is important to note that the same spurious parses also occur in context-free
parsing, specifically in Earley&apos;s algorithm. However, since the only information a con-
stituent carries in context-free grammar is the grammar symbol, the spurious deriva-
tions only produce exactly the same results as the normal ones. When the algorithm
is extended to unification grammar, however, these spurious parses are a problem.
</bodyText>
<subsectionHeader confidence="0.94314">
2. Unification Grammar and Parse Trees
</subsectionHeader>
<bodyText confidence="0.999182722222222">
Shieber (1992) defines a unification grammar as a 3-tuple (E, P, po), where E is the
vocabulary of the grammar, P is the set of productions, and po E P is the start pro-
duction. E contains L, a set of labels (feature names); C, a set of constants (feature
values); and W, a set of terminals. There are two kinds of productions in P: phrasal
and lexical. A phrasal production is a 2-tuple (a, (1), where a is the arity of the rule (the
number of right-hand-side [RHS] constituents), and 4) is a logical formula. Typically,
is a conjunction of equations of the form pi =p2 or p1 c, where pi, 132 E L* are
paths, and c E C. In an equation, any path which begins with an integer i (1 &lt; i &lt; a)
represents the ith RHS constituent of the rule.2 A lexical production is a 2-tuple (w,
where w E W and 4) is the same as above, except that there are no RHS constituents.
Figure 1 shows some example phrasal and lexical productions (po corresponds to the
context-free rule S NP VP and is the start production). Then a model M relates to
a formula 4) by a satisfaction relation as usual (M 4)), and when 4) is the formula
in a production p -= (a, 43), p is said to license M.
Based on the logic above, Shieber defines a parse tree and the language of a
grammar expressed in his formalism. To define a valid parse tree, he first defines the
set of possible parse trees II u,„ II, for a given grammar G, where each ll is defined
as follows:
</bodyText>
<subsectionHeader confidence="0.911297">
Definition
</subsectionHeader>
<bodyText confidence="0.9999125">
A parse tree 7- is a model that is a member of the infinite union of sets of bounded-
depth parse trees II = Iji&gt;011,, where each TI is defined as:
</bodyText>
<footnote confidence="0.728638333333333">
2 Shieber (1992) also uses a path that begins with 0 for the left-hand-side (LHS) constituent of a rule. In
this paper, we omit the 0 arcs and place the features of the LHS constituent directly at the root. This
change does not affect the formalism for the purpose of this paper.
</footnote>
<page confidence="0.987387">
278
</page>
<note confidence="0.313333">
Tomuro and Lytinen Nonminimal Derivations
</note>
<listItem confidence="0.9941556">
1. no is the set of models T for which there is a lexical production
p = (w,4) E G such that 7 4).
2. Ili(i &gt; 0) is the set of models 7- for which there is a phrasal production
p =- (a, 4)) E G such that T (1) and, for all 1 &lt;i &lt; a, r / (i) is defined and
TO) E U1&lt; Hi.
</listItem>
<bodyText confidence="0.99559775">
In the second condition, the extraction operator, denoted by /, retrieves the feature
structure found at the end of a particular path; so for instance 7/(1) retrieves the first
subconstituent on the RI-IS of the production that licenses T. In the definition above,
Ho contains all models that satisfy any lexical production in the grammar, while Hi
contains all models that satisfy a phrasal production, and whose subconstituents are
all in U1&lt;1•
To specify what constitutes a valid parse for a particular sentence, the next step is
to define the yield of a parse tree. It is defined recursively as follows: if T is licensed by
some lexical production p = (w, 4)), then the yield of T is w; or if T is licensed by some
phrasal production (a, 4)) and al, , a, are the yields of &apos;71(1), , r / (a) respectively,
then the yield of T is al . • • aa•
Finally, Shieber defines a valid parse tree T E H for sentence wi w, as follows:
</bodyText>
<listItem confidence="0.793214666666667">
1. The yield of 7 iS W1 • • •Wn
2. 7 is licensed by the start production po
Notice that this definition allows extra features in a parse tree, because a parse tree
</listItem>
<bodyText confidence="0.998295714285714">
r is defined by the satisfaction relation (7 43), which allows the existence of features
in the model that are not in the licensing production&apos;s formula. Given this definition,
for any valid parse tree 7-, we can construct another parse tree T1 by simply adding an
arbitrary (nonnumeric) feature to any node in T. Such a parse tree 71 is nonminimal
because extra features are nonminimal with respect to the minimal features in the
licensing productions. We will return to the issue of minimal and nonminimal parse
trees in Section 4.
</bodyText>
<sectionHeader confidence="0.806839" genericHeader="method">
3. The Abstract Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999962375">
Based on the logic described above, Shieber defines an abstract parsing algorithm as a
set of four logical deduction rules. Each rule derives a new item, from previous items
and/or productions in the grammar. An item is a 5-tuple (i, j, p. M, d), where i and j are
indices into the sentence and specify which words in the sentence have been used to
construct the item; p is the production used to construct the item; M is a model; and d
is the position of the &amp;quot;dot&amp;quot;; i.e., how many subconstituents in p have been completed
so far.
The logical rules of the abstract algorithm are shown in Figure 2. The Initial Item
rule produces the first item, and is constructed from the start production po. It spans
none of the input (i and j are both 0), and its model is the minimal model (mm) of 130.
The Prediction rule is essentially the top-down rewriting of the expectation (a
subconstituent just after the dot) in a prior item. In this rule, the extraction of M/ (d +
1) retrieves the d + 1st submodel in M (i.e., expectation). The function p, which is
left underspecified as a parameter in the abstract algorithm, filters out some features
predefined in the various instantiations of the algorithm. Here, it is applied to the
expectation, by which it effectively controls the top-down predictive power of the
</bodyText>
<page confidence="0.989815">
279
</page>
<figure confidence="0.4539885">
Computational Linguistics Volume 27, Number 2
INITIAL ITEM:
</figure>
<equation confidence="0.871268789473684">
(0,0, pa, mm(40) , 0)
(i, j,p = (a, cl.) , M, d)
PREDICTION: where d &lt;a arid p&apos; = (a&apos;,&apos;)E P
(j, j,p&apos;, p(M/ (d+1)) mm(19
.,0)
SCANNING: where d &lt; a and (w1+1, E P
(i, j,p = (a, c11), M, d)
(i, j+1, p , M u (mm(&apos;) \ (d+1)), d+1)
COMPLETION:
(i, j,p = (a, (I)), M, d) (j,k,p&apos; = (a&apos;, (1)/ ) , M&apos; , a&apos;)
,
(i,k, p, M U (M&apos; (d+1)), d+1) where d &lt;a
Figure 2
Shieber&apos;s parsing operations.
(0, 0, po, mm(4)0), 0)
(0,1, po,A4i, 1)
12 = (1, 1, p2, M2, 0)
13 = (I, 2, p2, M3, 1)
(0, 2, po, M4, 2)
</equation>
<figure confidence="0.923825571428572">
3rd sing
114:
VP
at
V
type
Pres intrans
</figure>
<figureCaption confidence="0.900684">
Figure 3
</figureCaption>
<bodyText confidence="0.989986888888889">
Items produced in the parse of John sleeps, and the final parse.
algorithm and provides flexibility to the instantiated algorithms. Then the expectation
is unified with a production (V), which can consistently rewrite it. By this operation,
some features in the expectation may be propagated down in the production.
The remaining two rules advance the dot in a prior item, by unifying the sub-
constituent to the right of the dot with either a lexical item from the input string (the
Scanning rule) or some other completed higher-level item (the Completion rule). Both
rules perform the correct unification by utilizing the embedding operator (signified
by \), which places a model M under a path p (M\p).
We illustrate these operators with a simple step-by-step example parse. Consider
the grammar that consists of the rules presented in Figure 1. Using this grammar,
Figure 3 shows the parse of the sentence John sleeps. First, the Initial Item operator
is applied, producing item 10, whose model is mm(130). Next, the Scanning operator
scans the word John, producing 11. The Prediction operator then produces 12. Next,
the word sleeps is scanned (since the first subconstituent of the model in 12 is a V),
producing 13. Finally, since the item in /3 is complete (d 1, the arity of production
p2), Completion is applied to items II, and 13, producing 14. Model M4 is the final parse
of the sentence.
</bodyText>
<sectionHeader confidence="0.996278" genericHeader="method">
4. Nonminimal Derivations
</sectionHeader>
<bodyText confidence="0.9991694">
In Section 2, we noted that Shieber&apos;s definition of parse trees allows them to be non-
minimal. We consider these to be invalid based on a principle that, since the unification
operation as set union preserves minimality (as proved in Shieber, [1992]), repeated
applications of unification using licensing productions should result in parses that
contain features only from those productions and nothing more. In this section, we
</bodyText>
<page confidence="0.910214">
280
</page>
<note confidence="0.520699">
Tomuro and Lytinen Noruninimal Derivations
</note>
<equation confidence="0.985602909090909">
{(cat) -= VP
(1 cat) = VP
P4 = (2, (1)4 : (2 cat)--:= ADV
(head) = (1 head)
(head modified) =-- true
Figure 4
A phrasal production that results in a nonminimal derivation.
(1, 1, p4, 0)
(1, 1, p2, A/112&apos;, 0)
= (1, 2, p2, A4 1)
A = (0,2, Po, A, 2)
</equation>
<figureCaption confidence="0.6580025">
Figure 5
Nonminimal derivation of John sleeps.
</figureCaption>
<figure confidence="0.525703">
p es.
per intrans
3rd sing
</figure>
<bodyText confidence="0.9752441">
formally define minimal and nonminimal parse trees, and show an example in which
nonminimal parse trees are produced by Shieber&apos;s algorithm.
Our definition of minimal parse tree is to a large extent similar to Shieber&apos;s def-
inition, but to ensure minimality, our definition uses the equality relation instead of
and inductively specifies a minimal parse tree bottom-up.
Definition
Given a grammar G, a minimal parse tree T admitted by G is a model that is a member
of the infinite union of sets of bounded-depth parse trees H&apos; = H
where each
H&apos;, is defined as:
</bodyText>
<listItem confidence="0.997521">
1. For each lexical production p = (w, 43) E G, mm() E H.
2. For each phrasal production p = (a, cD) E G, let i,. • . E Li1&lt;1 Hir11
7 = mm() LJ 7-1 \ (1) U ... U -iVa), then 7 E H.
</listItem>
<bodyText confidence="0.9995146">
It is obvious that H&apos; is a subset of H in Shieber&apos;s definition. Then, a nonminimal parse
tree is defined as a model that is a member of the difference of the two sets (H —
Here is a simple example in which a nonminimal parse is produced in Shieber&apos;s
algorithm. Say that we add the production in Figure 4 to the grammar in the previous
section. The intent of this production is to mark the verb with the feature modified if an
adverb follows. Using this grammar, Shieber&apos;s algorithm will produce a nonminimal
parse for the sentence John sleeps, in addition to the minimal parse shown in the
previous section.4 The nonminimal parse, shown in Figure 5, arises as follows: after
scanning John, Prediction can produce items I and Ig , first using production 134 (thus
inserting (head modified) = true into the model), and then p2. Scanning the word
</bodyText>
<footnote confidence="0.97711575">
3 Note that using subsumption (which we will discuss in Section 5) here does not work, for instance by
saying &amp;quot;a model T&amp;quot; is a nonminimal parse tree if T&amp;quot; E H and there exists 7&apos; E H such that T&apos; &lt;
because some r&amp;quot;s are minimal. See the example in Section 5.
4 Here, we are assuming that the filtering function p is the identity function.
</footnote>
<page confidence="0.991046">
281
</page>
<note confidence="0.642357">
Computational Linguistics Volume 27, Number 2
</note>
<bodyText confidence="0.999660553191489">
sleeps then produces I from l&apos;2&apos;. Completion then can be applied directly to Ii and A by
skipping a completion using I and A, thereby producing item I,. The feature modified
remains in I, even though an adverb was never encountered in the sentence. The
final parse AT4, shown in Figure 5, is clearly nonminimal according to our definition
because of this feature.
Note that the example grammar can be changed to prevent the nonminimal parse,
by moving the feature modified off of the head path in &apos;1&apos;4 (i.e., (modified) = true
instead of (head modified) = true).5 However, the point of the example is not to argue
whether or not well-designed grammars will produce erroneous parses. A formally
defined parser (see the discussion below) should in principle produce correct parses
regardless of the grammar used; otherwise, the grammar formalism (i.e., Shieber&apos;s logic
for unification grammars) must be revised and properly constrained to allow only the
kinds of productions with which the parser produces correct results.
In general, nonminimal derivations may arise whenever two or more predictions
that are not mutually exclusive can be produced at the same point in the sentence;
i.e., two prediction items (i, p. M, 0) and (i, p&apos;, M&apos;, 0) are produced such that M
M&apos; and M and M&apos; are unifiable. In the example, items /2 = (1, 1, P2, M2, 0) and A
1, p4, M&apos;2, 0) (as well as 12 and Ig = (1, 1, P2, M&apos;, 0)) are two such items. Since the two
predictions did not have any conflicting features from the beginning, a situation may
occur where a completion generated from one prediction can fill the other prediction
without causing conflict. When this happens, features that were in the other prediction
but not the original one become nonminimal in the resulting model.
As to what causes nonminimal situations, we speculate that there are a number
of possibilities. First, nonminimal derivations occur when a prediction is filled by a
complete item that was not generated from the prediction. This mismatch will not
happen if parsing is done in one direction only (e.g. purely top-down or bottom-up
parsing). Thus, the mixed-direction parsing strategy is a contributing factor.
Second, wrong complete items are retrieved because Shieber&apos;s item-based algo-
rithm makes all partial results available during parsing, as if they are kept in a global
structure (such as a chart in chart parsing). But if the accessibility of items were some-
how restricted, prediction-completion mismatch would not happen. In this respect,
other chart-based algorithms for unification grammars which adopt mixed-direction
parsing strategy, including head-corner parsing (van Noord 1997) and left-corner pars-
ing (Alshawi 1992), are subject to the same problem.
Third, extra features can only appear when the grammar contains rules which
interact in a certain way (such as rules p2 and p4 above). If the grammar contained
no such rules, or if p (the filtering function applied in Prediction) filtered out those
features, even the prediction-completion mismatch would not produce nonminimal
derivations.
As we stated in the beginning of this section, we consider nonminimal parses to
be invalid on the basis of minimality. It then immediately follows that any parsing
algorithm that produces nonminimal parses is considered to be unsound; in particular,
Shieber&apos;s algorithm is unsound. However, since nonminimal parse trees have the same
yield as their minimal counterparts, his algorithm does indeed recognize exactly the
language of a given grammar. So, Shieber&apos;s algorithm is sound as a recognizer,6 but
not as a transducer or parser (as in van Noord, [1997]) where the correctness of output
models (i.e., parse trees) is critical. In other words, Shieber&apos;s algorithm is correct up to
</bodyText>
<footnote confidence="0.986468666666667">
5 Note that adding (head modified) false to 432 (VP —* V) or 4)3 (sleeps) is not feasible, because they
cannot specify the modified feature at their level.
6 In fact, Shieber hints at this: &amp;quot;The process of parsing (more properly, recognition)...&amp;quot; (Shieber 1992, 78).
</footnote>
<page confidence="0.984246">
282
</page>
<note confidence="0.702651">
Tomuro and Lytinen Nonminimal Derivations
</note>
<bodyText confidence="0.963059333333333">
licensing, but incorrect on the basis of a stronger criteria of minimality. Thus, to guar-
antee correctness based on minimality, we need another algorithm; such an algorithm
is exactly the solution to the nonminimal derivation problem.
</bodyText>
<sectionHeader confidence="0.61057" genericHeader="method">
5. Practical Techniques
</sectionHeader>
<bodyText confidence="0.999976">
Before presenting our solution to the nonminimal derivation problem, we discuss
several possible practical techniques to get around the problem in implemented sys-
tems. These are known techniques, which have been applied to solve other problems
in unification-based systems. However, most of them only offer partial solutions to
the nonminimal derivation problem. First, whenever Shieber&apos;s algorithm produces a
nonminimal derivation, it also produces a corresponding minimal derivation (Tomuro
1999). Thus, one possible solution is to use subsumption to discard items that are more
specific than any other items that are produced. Subsumption has often been used in
unification-based systems to pack items or models (e.g., Alshawi 1992). However,
simple subsumption may filter out valid parses for some grammars, thus sacrificing
completeness.&apos;
Another possibility is to filter out problematic features in the Prediction step by
using the function p. However, automatic detection of such features (i.e., automatic
derivation of p) is undecidable for the same reason as the prediction nontermination
problem (caused by left recursion) for unification grammars (Shieber 1985). Manual
detection is also problematic: when a grammar is large, particularly if semantic fea-
tures are included, complete detection is nearly impossible. As for the techniques
developed so far which (partially) solve prediction nontermination (e.g., Shieber 1985;
Haas 1989; Samuelsson 1993), they do not apply to nonminimal derivations because
nonminimal derivations may arise without left recursion or recursion in general.&apos; One
way is to define p to filter out all features except the context-free backbone of predic-
tions. However, this severely restricts the range of possible instantiations of Shieber&apos;s
algorithm.&apos;
A third possibility is to manually fix the grammar so that nonminimal derivations
do not occur, as we noted in Section 4. However, this approach is problematic for the
same reason as the manual derivation of p mentioned above.
</bodyText>
<sectionHeader confidence="0.795131" genericHeader="method">
6. Modified Algorithm
</sectionHeader>
<bodyText confidence="0.991965571428571">
Finally, we propose an algorithm that does not produce nonminimal derivations. It is a
modification of Shieber&apos;s algorithm that incorporates parent pointers. Figure 6 shows
7 For example, when there are two predictions M1 and M2 for category C and a production p where
M1 =- {(cat) = C, (x) = al, M2 =- {(cat) = C, (y) = b} , and p = (1, {(cat) = C, (1 cat) = D, (x) = al)
respectively, the resulting model M2&apos; = {(cat) -= C, (1 cat) rr D, (x) rr a, (y) = b} will have strictly more
information than the other resulting model M1&apos; = {(cat) zr C, (1 cat) D, (x) = a}, although both
models are minimal.
</bodyText>
<footnote confidence="0.825404636363636">
8 We do not show any particular example here, but if we change the left-recursive VP rule in the earlier
example to a non-left-recursive rule, for instance VP --+ VP2 ADV, and add some rules, a nonminimal
parse will indeed arise.
Note also that some (but not all) cases of prediction nontermination will produce nonminimal
derivations. Those cases occur when there is a prediction for a category, and repeated applications of
some left-recursive rule(s) generate predictions for the same category that are not mutually exclusive to
the original prediction or each other.
9 In head-corner parsing, Sikkel (1997) proposes the use of transitive features: features that propagate
only through head arcs. However, this method does not solve nonminimal derivations either, because
problematic features may be subfeatures of a head (such as the example case shown earlier), which will
not be filtered.
</footnote>
<page confidence="0.99322">
283
</page>
<note confidence="0.4800272">
Computational Linguistics id is new Volume 27, Number 2
INITIAL ITEM: where a (a&apos; symbol
PREDICTION: , E P
SCANNING: )) where d &lt;a d (wi+i,5 E P
COMPLETION: an 4.
</note>
<bodyText confidence="0.923860111111111">
, where d &lt;a
(id, nil , (0, 0, pa, mm(4)0) , 0))
(id, pid, (i, j , p = (a, 43) , M, d))
(id&apos;, id, (j, j, p&apos;, p(M / (d+1)) U mm(4.&apos; ), 0))
where id&apos; is a new symbol, and d &lt; a and p&apos; =
(id, pid, (i, j, p = (a, (I)), M, d))
(id, pid , (i, j+1,p , M mm(cla) \ (d+1) , d+1))
(id, pid, (i, j,p, M, d)) (id&amp;quot;, id, (j, k, p&apos;, A4&apos;,
(id, pid , (i, k,p, M (M&apos; \ (d+1)), d+1))
</bodyText>
<figureCaption confidence="0.614637">
Figure 6
</figureCaption>
<bodyText confidence="0.9843264">
Shieber&apos;s parsing operations modified.
the modified algorithm. In the figure, an item is represented by a nested 3-tuple, where
the first argument is the self index, the second is the parent index/pointer, and the
third is the old 5-tuple used in Shieber&apos;s original algorithm. A parent pointer, then,
is set in Prediction—the resulting item has the index of the antecedent item (id) as
its parent. By generating a new symbol for the self index in every Prediction item
(id&apos;), parent pointers in those items are threaded to form a prediction path. Then in
Completion, the parent pointer is used to restrict the antecedent items: the complete
item (on the right) must have the prior expectation (on the left) as its parent (id),
thereby ensuring a prediction path to be precisely restored.
While this modified algorithm offers a complete solution on the level of logic, it
has some undesirable implications in implemented systems. The most prominent one
is that the parent pointer scheme makes implementation of memoization rather diffi-
cult. Normally, memoization is used to avoid storing duplicate items that are identical;
however, in the modified algorithm, many items that are otherwise identical will have
different parent pointers, thereby changing the polynomial time algorithm (0(n3); Ear-
ley [1970]) to an exponential one. To avoid computational inefficiency, a way must be
devised for items that are identical except for parent pointers to share information,
especially models, and thus avoid the expense of duplicate identical unification opera-
tions. One possibility is to represent the 5-tuple from Shieber &apos;s original algorithm by a
separate structure and have an index to it in the new 3-tuple item. This way, not only
can the items be shared, they can still be memoized in the usual way as well. Another
possibility is to adopt an efficiency technique along the line of selective memoization
(van Noord 1997). Implementation and empirical analysis is our future research.
Whatever the practical performance will turn out to be, it is important to note
that the proposed algorithm is a formal solution that guarantees minimality for any
grammar defined in Shieber&apos;s logic. Moreover the algorithm preserves the same gen-
erality and flexibility as Shieber&apos;s: a mixed top-down, bottom-up parsing with the
filtering function p to allow various instantiations of the algorithm to characterize
their algorithms.
</bodyText>
<sectionHeader confidence="0.996668" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999324727272727">
Alshawi, H., editor. 1992. The Core Language
Engine. MIT Press.
Earley, J. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 13(2).
Gazdar, G., E. Klein, G. Pullum, and I. Sag.
1985. Generalized Phrase Structure Grammar.
Blackwell Publishing.
Haas, A. 1989. A parsing algorithm for
unification grammar. Computational
Linguistics, 15(4):219-232.
</reference>
<page confidence="0.977723">
284
</page>
<note confidence="0.614658">
Tomuro and Lytinen Nonminimal Derivations
</note>
<reference confidence="0.999363714285714">
Pereira, F. and D. Warren. 1980. Definite
clause grammars for language analysis.
Artificial Intelligence, 13:231-278.
Pollard, C. and I. Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI. University
of Chicago Press.
Samuelsson, C. 1993. Avoiding
non-termination in unification grammars.
In Natural Language Understanding and Logic
Programming IV.
Shieber, S. 1985. Using restriction to extend
parsing algorithms for complex-feature-
based formalisms. In Proceedings of the 23rd
Annual Meeting, Association for
Computational Linguistics.
Shieber, S. 1986. An Introduction to
Unification-Based Approaches to Grammar.
CSLI. University of Chicago Press.
Shieber, S. 1992. Constraint-based Grammar
Formalisms. MIT Press.
Sikkel, K. 1997. Parsing Schemata.
Springer-Verlag.
Tomuro, N. 1999. Left-Corner Parsing
Algorithm for Unification Grammars. Ph.D.
thesis, DePaul University.
van Noord, G. 1997. An efficient
implementation of the head-corner parser.
Computational Linguistics, 23(3):425-456.
</reference>
<page confidence="0.998527">
285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564068">
<title confidence="0.999107">Squibs and Discussions Nonminimal Derivations in Unification-based Parsing</title>
<author confidence="0.989041">Noriko Tomuro Steven L Lytinent</author>
<affiliation confidence="0.999419">DePaul University DePaul University</affiliation>
<abstract confidence="0.944133625">Shieber&apos;s abstract parsing algorithm (Shieber 1992) for unification grammars is an extension of Earley&apos;s algorithm (Earley 1970) for context-free grammars to feature structures. In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces what we call a nonminimal derivation: a parse tree which contains additional features that are not in the licensing productions. While Shieber&apos;s definition of parse tree allows for such nonminimal derivations, we claim that they should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm which ensures minimality, although at some computational cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>editor</author>
</authors>
<title>The Core Language Engine.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<marker>Alshawi, editor, 1992</marker>
<rawString>Alshawi, H., editor. 1992. The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Earley, 1970</marker>
<rawString>Earley, J. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Blackwell Publishing.</publisher>
<contexts>
<context position="1045" citStr="Gazdar et al. 1985" startWordPosition="146" endWordPosition="149">inimal derivation: a parse tree which contains additional features that are not in the licensing productions. While Shieber&apos;s definition of parse tree allows for such nonminimal derivations, we claim that they should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm which ensures minimality, although at some computational cost. 1. Introduction Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG (Gazdar et al. 1985), PATR-II (Shieber 1986), DCG (Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize the common elements of unification-style grammars, Shieber (1992) developed a logic for describing them, and used this logic to define an abstract parsing algorithm. The algorithm uses the same set of operations as Earley&apos;s (1970) algorithm for context-free grammars, but modified for unification grammars. In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces unintended, spurious parses in addition to the intended ones. We call these spurious parses n</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., E. Klein, G. Pullum, and I. Sag. 1985. Generalized Phrase Structure Grammar. Blackwell Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haas</author>
</authors>
<title>A parsing algorithm for unification grammar.</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<pages>15--4</pages>
<contexts>
<context position="20214" citStr="Haas 1989" startWordPosition="3456" endWordPosition="3457">eness.&apos; Another possibility is to filter out problematic features in the Prediction step by using the function p. However, automatic detection of such features (i.e., automatic derivation of p) is undecidable for the same reason as the prediction nontermination problem (caused by left recursion) for unification grammars (Shieber 1985). Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible. As for the techniques developed so far which (partially) solve prediction nontermination (e.g., Shieber 1985; Haas 1989; Samuelsson 1993), they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general.&apos; One way is to define p to filter out all features except the context-free backbone of predictions. However, this severely restricts the range of possible instantiations of Shieber&apos;s algorithm.&apos; A third possibility is to manually fix the grammar so that nonminimal derivations do not occur, as we noted in Section 4. However, this approach is problematic for the same reason as the manual derivation of p mentioned above. 6. Modified Algorithm Fin</context>
</contexts>
<marker>Haas, 1989</marker>
<rawString>Haas, A. 1989. A parsing algorithm for unification grammar. Computational Linguistics, 15(4):219-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>Definite clause grammars for language analysis.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="1100" citStr="Pereira and Warren 1980" startWordPosition="154" endWordPosition="157">itional features that are not in the licensing productions. While Shieber&apos;s definition of parse tree allows for such nonminimal derivations, we claim that they should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm which ensures minimality, although at some computational cost. 1. Introduction Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG (Gazdar et al. 1985), PATR-II (Shieber 1986), DCG (Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize the common elements of unification-style grammars, Shieber (1992) developed a logic for describing them, and used this logic to define an abstract parsing algorithm. The algorithm uses the same set of operations as Earley&apos;s (1970) algorithm for context-free grammars, but modified for unification grammars. In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces unintended, spurious parses in addition to the intended ones. We call these spurious parses nonminimal derivations (or nonminimal parse trees), beca</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F. and D. Warren. 1980. Definite clause grammars for language analysis. Artificial Intelligence, 13:231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>CSLI. University of Chicago Press.</publisher>
<contexts>
<context position="1133" citStr="Pollard and Sag 1994" startWordPosition="160" endWordPosition="163"> licensing productions. While Shieber&apos;s definition of parse tree allows for such nonminimal derivations, we claim that they should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm which ensures minimality, although at some computational cost. 1. Introduction Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG (Gazdar et al. 1985), PATR-II (Shieber 1986), DCG (Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize the common elements of unification-style grammars, Shieber (1992) developed a logic for describing them, and used this logic to define an abstract parsing algorithm. The algorithm uses the same set of operations as Earley&apos;s (1970) algorithm for context-free grammars, but modified for unification grammars. In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces unintended, spurious parses in addition to the intended ones. We call these spurious parses nonminimal derivations (or nonminimal parse trees), because they contain extra features w</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C. and I. Sag. 1994. Head-driven Phrase Structure Grammar. CSLI. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Avoiding non-termination in unification grammars.</title>
<date>1993</date>
<booktitle>In Natural Language Understanding and Logic Programming IV.</booktitle>
<contexts>
<context position="20232" citStr="Samuelsson 1993" startWordPosition="3458" endWordPosition="3459">ther possibility is to filter out problematic features in the Prediction step by using the function p. However, automatic detection of such features (i.e., automatic derivation of p) is undecidable for the same reason as the prediction nontermination problem (caused by left recursion) for unification grammars (Shieber 1985). Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible. As for the techniques developed so far which (partially) solve prediction nontermination (e.g., Shieber 1985; Haas 1989; Samuelsson 1993), they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general.&apos; One way is to define p to filter out all features except the context-free backbone of predictions. However, this severely restricts the range of possible instantiations of Shieber&apos;s algorithm.&apos; A third possibility is to manually fix the grammar so that nonminimal derivations do not occur, as we noted in Section 4. However, this approach is problematic for the same reason as the manual derivation of p mentioned above. 6. Modified Algorithm Finally, we propose a</context>
</contexts>
<marker>Samuelsson, 1993</marker>
<rawString>Samuelsson, C. 1993. Avoiding non-termination in unification grammars. In Natural Language Understanding and Logic Programming IV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Using restriction to extend parsing algorithms for complex-featurebased formalisms.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19941" citStr="Shieber 1985" startWordPosition="3417" endWordPosition="3418">d items that are more specific than any other items that are produced. Subsumption has often been used in unification-based systems to pack items or models (e.g., Alshawi 1992). However, simple subsumption may filter out valid parses for some grammars, thus sacrificing completeness.&apos; Another possibility is to filter out problematic features in the Prediction step by using the function p. However, automatic detection of such features (i.e., automatic derivation of p) is undecidable for the same reason as the prediction nontermination problem (caused by left recursion) for unification grammars (Shieber 1985). Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible. As for the techniques developed so far which (partially) solve prediction nontermination (e.g., Shieber 1985; Haas 1989; Samuelsson 1993), they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general.&apos; One way is to define p to filter out all features except the context-free backbone of predictions. However, this severely restricts the range of possible instantiations of S</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, S. 1985. Using restriction to extend parsing algorithms for complex-featurebased formalisms. In Proceedings of the 23rd Annual Meeting, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar.</title>
<date>1986</date>
<publisher>CSLI. University of Chicago Press.</publisher>
<contexts>
<context position="1069" citStr="Shieber 1986" startWordPosition="151" endWordPosition="152">e which contains additional features that are not in the licensing productions. While Shieber&apos;s definition of parse tree allows for such nonminimal derivations, we claim that they should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm which ensures minimality, although at some computational cost. 1. Introduction Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG (Gazdar et al. 1985), PATR-II (Shieber 1986), DCG (Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize the common elements of unification-style grammars, Shieber (1992) developed a logic for describing them, and used this logic to define an abstract parsing algorithm. The algorithm uses the same set of operations as Earley&apos;s (1970) algorithm for context-free grammars, but modified for unification grammars. In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces unintended, spurious parses in addition to the intended ones. We call these spurious parses nonminimal derivations (o</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, S. 1986. An Introduction to Unification-Based Approaches to Grammar. CSLI. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Constraint-based Grammar Formalisms.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1226" citStr="Shieber (1992)" startWordPosition="175" endWordPosition="176">ons, we claim that they should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and propose a precise definition of minimal parse tree, as well as a modification to Shieber&apos;s algorithm which ensures minimality, although at some computational cost. 1. Introduction Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG (Gazdar et al. 1985), PATR-II (Shieber 1986), DCG (Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize the common elements of unification-style grammars, Shieber (1992) developed a logic for describing them, and used this logic to define an abstract parsing algorithm. The algorithm uses the same set of operations as Earley&apos;s (1970) algorithm for context-free grammars, but modified for unification grammars. In this paper, we show that, under certain conditions, Shieber&apos;s algorithm produces unintended, spurious parses in addition to the intended ones. We call these spurious parses nonminimal derivations (or nonminimal parse trees), because they contain extra features which are not in the productions that license the parse.1 We claim that such nonminimal deriva</context>
<context position="4522" citStr="Shieber (1992)" startWordPosition="697" endWordPosition="698">alternate definition of minimal parse tree for unification grammars, and present a modification to Shieber &apos;s algorithm which ensures minimality. It is important to note that the same spurious parses also occur in context-free parsing, specifically in Earley&apos;s algorithm. However, since the only information a constituent carries in context-free grammar is the grammar symbol, the spurious derivations only produce exactly the same results as the normal ones. When the algorithm is extended to unification grammar, however, these spurious parses are a problem. 2. Unification Grammar and Parse Trees Shieber (1992) defines a unification grammar as a 3-tuple (E, P, po), where E is the vocabulary of the grammar, P is the set of productions, and po E P is the start production. E contains L, a set of labels (feature names); C, a set of constants (feature values); and W, a set of terminals. There are two kinds of productions in P: phrasal and lexical. A phrasal production is a 2-tuple (a, (1), where a is the arity of the rule (the number of right-hand-side [RHS] constituents), and 4) is a logical formula. Typically, is a conjunction of equations of the form pi =p2 or p1 c, where pi, 132 E L* are paths, and c</context>
<context position="6106" citStr="Shieber (1992)" startWordPosition="1009" endWordPosition="1010"> production). Then a model M relates to a formula 4) by a satisfaction relation as usual (M 4)), and when 4) is the formula in a production p -= (a, 43), p is said to license M. Based on the logic above, Shieber defines a parse tree and the language of a grammar expressed in his formalism. To define a valid parse tree, he first defines the set of possible parse trees II u,„ II, for a given grammar G, where each ll is defined as follows: Definition A parse tree 7- is a model that is a member of the infinite union of sets of boundeddepth parse trees II = Iji&gt;011,, where each TI is defined as: 2 Shieber (1992) also uses a path that begins with 0 for the left-hand-side (LHS) constituent of a rule. In this paper, we omit the 0 arcs and place the features of the LHS constituent directly at the root. This change does not affect the formalism for the purpose of this paper. 278 Tomuro and Lytinen Nonminimal Derivations 1. no is the set of models T for which there is a lexical production p = (w,4) E G such that 7 4). 2. Ili(i &gt; 0) is the set of models 7- for which there is a phrasal production p =- (a, 4)) E G such that T (1) and, for all 1 &lt;i &lt; a, r / (i) is defined and TO) E U1&lt; Hi. In the second condit</context>
<context position="18457" citStr="Shieber 1992" startWordPosition="3202" endWordPosition="3203">e yield as their minimal counterparts, his algorithm does indeed recognize exactly the language of a given grammar. So, Shieber&apos;s algorithm is sound as a recognizer,6 but not as a transducer or parser (as in van Noord, [1997]) where the correctness of output models (i.e., parse trees) is critical. In other words, Shieber&apos;s algorithm is correct up to 5 Note that adding (head modified) false to 432 (VP —* V) or 4)3 (sleeps) is not feasible, because they cannot specify the modified feature at their level. 6 In fact, Shieber hints at this: &amp;quot;The process of parsing (more properly, recognition)...&amp;quot; (Shieber 1992, 78). 282 Tomuro and Lytinen Nonminimal Derivations licensing, but incorrect on the basis of a stronger criteria of minimality. Thus, to guarantee correctness based on minimality, we need another algorithm; such an algorithm is exactly the solution to the nonminimal derivation problem. 5. Practical Techniques Before presenting our solution to the nonminimal derivation problem, we discuss several possible practical techniques to get around the problem in implemented systems. These are known techniques, which have been applied to solve other problems in unification-based systems. However, most </context>
</contexts>
<marker>Shieber, 1992</marker>
<rawString>Shieber, S. 1992. Constraint-based Grammar Formalisms. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sikkel</author>
</authors>
<title>Parsing Schemata.</title>
<date>1997</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="22021" citStr="Sikkel (1997)" startWordPosition="3767" endWordPosition="3768">l. 8 We do not show any particular example here, but if we change the left-recursive VP rule in the earlier example to a non-left-recursive rule, for instance VP --+ VP2 ADV, and add some rules, a nonminimal parse will indeed arise. Note also that some (but not all) cases of prediction nontermination will produce nonminimal derivations. Those cases occur when there is a prediction for a category, and repeated applications of some left-recursive rule(s) generate predictions for the same category that are not mutually exclusive to the original prediction or each other. 9 In head-corner parsing, Sikkel (1997) proposes the use of transitive features: features that propagate only through head arcs. However, this method does not solve nonminimal derivations either, because problematic features may be subfeatures of a head (such as the example case shown earlier), which will not be filtered. 283 Computational Linguistics id is new Volume 27, Number 2 INITIAL ITEM: where a (a&apos; symbol PREDICTION: , E P SCANNING: )) where d &lt;a d (wi+i,5 E P COMPLETION: an 4. , where d &lt;a (id, nil , (0, 0, pa, mm(4)0) , 0)) (id, pid, (i, j , p = (a, 43) , M, d)) (id&apos;, id, (j, j, p&apos;, p(M / (d+1)) U mm(4.&apos; ), 0)) where id&apos; </context>
</contexts>
<marker>Sikkel, 1997</marker>
<rawString>Sikkel, K. 1997. Parsing Schemata. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tomuro</author>
</authors>
<title>Left-Corner Parsing Algorithm for Unification Grammars.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>DePaul University.</institution>
<contexts>
<context position="3298" citStr="Tomuro (1999)" startWordPosition="481" endWordPosition="482">stems, Chicago, IL 60604. E-mail: lytinen@cs.depaul.edu 1 In this paper, we use &amp;quot;nonminimal derivations&amp;quot; synonymously with &amp;quot;nonminimal parses&amp;quot;. Normally the notions of derivation and parse tree are different. However, in this paper we focus on parse trees as the final result of derivation, thus we mean that a derivation is nonminimal when its result is a nonminimal parse, in contrast to a minimal derivation which produces a minimal parse. Unfortunately, formal definitions of minimal and nonminimal derivations are outside the scope of this short paper; interested readers are encouraged to read Tomuro (1999). © 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 2 { (cat) = S (cat) -= NP (1 cat) Zr NP pi = (&amp;quot;John&amp;quot;, 4)1 : { (head agr pers) 3rd ) (2 cat) = VP (head agr num) =sing) po (2, &apos;DO : (cat) = V (head) = (2 head) P3 = (&amp;quot;sleeps&amp;quot;, : (head agr pers) = 3rd , (head subj) =- (1 head) (head agr num) = sing (head agr) rr (1 head agr) (head tense) = pres { (cat) -= VP (1 cat) = V P2 (1, (1)2 (head) -= (1 head) ) (head type) = intrans Figure 1 Examples of productions. not constraining enough to disallow nonminimal derivations. To solve this twofold problem, we p</context>
<context position="19267" citStr="Tomuro 1999" startWordPosition="3317" endWordPosition="3318">lgorithm; such an algorithm is exactly the solution to the nonminimal derivation problem. 5. Practical Techniques Before presenting our solution to the nonminimal derivation problem, we discuss several possible practical techniques to get around the problem in implemented systems. These are known techniques, which have been applied to solve other problems in unification-based systems. However, most of them only offer partial solutions to the nonminimal derivation problem. First, whenever Shieber&apos;s algorithm produces a nonminimal derivation, it also produces a corresponding minimal derivation (Tomuro 1999). Thus, one possible solution is to use subsumption to discard items that are more specific than any other items that are produced. Subsumption has often been used in unification-based systems to pack items or models (e.g., Alshawi 1992). However, simple subsumption may filter out valid parses for some grammars, thus sacrificing completeness.&apos; Another possibility is to filter out problematic features in the Prediction step by using the function p. However, automatic detection of such features (i.e., automatic derivation of p) is undecidable for the same reason as the prediction nontermination </context>
</contexts>
<marker>Tomuro, 1999</marker>
<rawString>Tomuro, N. 1999. Left-Corner Parsing Algorithm for Unification Grammars. Ph.D. thesis, DePaul University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<marker>van Noord, 1997</marker>
<rawString>van Noord, G. 1997. An efficient implementation of the head-corner parser. Computational Linguistics, 23(3):425-456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>