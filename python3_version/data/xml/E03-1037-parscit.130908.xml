<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018846">
<title confidence="0.994019">
Experiments on the Choice of Features for Learning Verb Classes
</title>
<author confidence="0.945386">
Sabine Schulte im Walde
</author>
<affiliation confidence="0.703623">
Institut fur Maschinelle Sprachverarbeitung
Universitat Stuttgart
</affiliation>
<address confidence="0.946266">
AzenbergstraBe 12, 70174 Stuttgart, Germany
</address>
<email confidence="0.998144">
schulte@ims.uni—stuttgart.de
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981333333333">
The choice of verb features is crucial for
the learning of verb classes. This pa-
per presents clustering experiments on
168 German verbs, which explore the
relevance of features on three levels of
verb description, purely syntactic frame
types, prepositional phrase information
and selectional preferences. In contrast
to previous approaches concentrating on
the sparse data problem, we present ev-
idence for a linguistically defined limit
on the usefulness of features which is
driven by the idiosyncratic properties of
the verbs and the specific attributes of
the desired verb classification.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999722538461539">
The verb is central to the meaning and the struc-
ture of a sentence, and lexical verb information
represents the core in supporting NLP-tasks such
as word sense disambiguation (Dorr and Jones,
1996; Prescher et al., 2000), machine transla-
tion (Don, 1997), document classification (Kla-
vans and Kan, 1998), and subcategorisation acqui-
sition and filtering (Korhonen, 2002). A means
to generalise over and predict common properties
of verbs is captured by the constitution of verb
classes. Levin (1993) has established an extensive
manual classification for English verbs; computa-
tional approaches adopt the linguistic hypothesis
that verb meaning components to a certain extent
determine verb behaviour as basis for automati-
cally inducing semantic verb classes from corpus-
based features (Schulte im Walde, 2000; Merlo
and Stevenson, 2001; Joanis, 2002).
Computational approaches on verb classifica-
tion which take advantage of corpus-based and
knowledge-based verb information offered by
available tools and resources such as statistical
parsers and semantic ontologies, suffer from se-
vere problems to encode and benefit from the
information, especially with respect to selec-
tional preferences, cf. Schulte im Walde (2000);
Joanis (2002). This paper presents clustering ex-
periments on German verbs which explore the
relevance of features on three levels of verb de-
scription, purely syntactic frame types, preposi-
tional phrase information and selectional prefer-
ences. The clustering results show that the choice
and implementation of verb features is crucial for
the induction of the verb classes. Intuitively, one
might want to add and refine features ad infinitum,
but we present evidence for a linguistically defined
limit on the usefulness of features which is driven
by the idiosyncratic properties of the verbs and the
verb classification.
</bodyText>
<sectionHeader confidence="0.982731" genericHeader="method">
2 German Verb Classes
</sectionHeader>
<bodyText confidence="0.999989642857143">
A set of 168 German verbs is manually classified
into 43 concise semantic verb classes. The pur-
pose of the manual classification is (i) to evaluate
the reliability and performance of the clustering
experiments on a preliminary set of verbs, and (ii)
to explore the potential and limit to apply the clus-
tering method to large-scale verb data. The Ger-
man classes are closely related to the English pen-
dant in (Levin, 1993) and agree with the German
verb classification in (Schumacher, 1986) as far as
the relevant verbs appear in his semantic &apos;fields&apos;.
Table 1 presents the manual verb classification.
The class size is between 2 and 7, with an aver-
age of 3.9 verbs per class. Eight verbs are am-
</bodyText>
<page confidence="0.998455">
315
</page>
<table confidence="0.998580109375">
Aspect: anfangen, aufhoren, beenden, beginnen, enden
Propositional Attitude: ahnen, denken, glauben,
vermuten, wissen
Desire:
Wish: erhoffen, wollen, wiinschen
Need: bean-fen, benotigen, brauchen
Transfer of Possession (Obtaining): bekommen, erhalten,
erlangen, kriegen
Transfer of Possession (Giving):
Gift: geben, leihen, schenken, spenden, stiften,
vermachen, iiberschreiben
Supply: bringen, liefern, schicken, vermittelni, zustellen
Manner of Motion:
Locomotion: gehen, klettern, kriechen, laufen, rennen,
schleichen, wandern
Rotation: drehen, rotieren
Rush: eilen, hasten
Means: fahren, fliegen, rudern, segeln
Flotation: flief3en, gleiten, treiben
Emotion:
Origin: argern, freuen
Expression: heuleni, lacheni, weinen
Objection: angstigen, ekeln, ftirchten, scheuen
Face Look: giihnen, grinsen, lachen2, litcheln, stan-en
Perception: empfinden, erfahreni , fiihlen, horen,
riechen, sehen, wahrnehmen
Manner of Articulation: fltistern, rufen, schreien
Moaning: heulen2, jammern, klagen, lamentieren
Communication: kommunizieren, korrespondieren,
reden, sprechen, verhandeln
Statement:
Announcement: anktindigen, bekanntgeben, eroffnen,
verktinden
Constitution: anordnen, bestimmen, festlegen
Promise: versichern, versprechen, zusagen
Observation: bemerken, erkennen, erfahren2,
feststellen, realisieren, registrieren
Description: beschreiben, charakterisieren, darstellent ,
interpretieren
Presentation: darstellen2, demonstrieren, prasentieren,
veranschaulichen, vorfiihren
Speculation: grtibeln, nachdenken, phantasieren,
spekulieren
Insistence: behan-en, besteheni, insistieren, pochen
Teaching: beibringen, lehren, unterrichten, vermitteln2
Position:
Bring into Position: legen, setzen, stellen
Be in Position: liegen, sitzen, stehen
Production: bilden, erzeugen, herstellen,
hervorbri ngen, produzieren
Renovation: dekorieren, erneuern, renovieren, reparieren
Support: dienen, folgeni, helfen, unterstiitzen
Quantum Change: erhöhen, erniedrigen, senken,
steigern, vergroBern, verkleinern
Opening: offnen, schlieBeni
Existence: bestehen2, existieren, leben
Consumption: essen, konsumieren, lesen, saufen, trinken
Elimination: eliminieren, entfernen, exekutieren,
Viten, vernichten
Basis: basieren, beruhen, griinden, stfitzen
Inference: folgern, schliel3en2
Result: ergeben, erwachsen, folgen2, resultieren
Weather: blitzen, donnern, dammern, nieseln, regnen,
schneien
</table>
<tableCaption confidence="0.999958">
Table 1: German semantic verb classes
</tableCaption>
<bodyText confidence="0.9998725625">
biguous and marked by subscripts. The classes in-
clude both high and low frequency verbs,1 in order
to exercise the clustering technology in both data-
rich and data-poor situations. The class labels are
given on two semantic levels; coarse labels such
as Manner of Motion are sub-divided into finer la-
bels, such as Locomotion, Rotation. The fine la-
bels are relevant for the clustering experiments, as
indicated by the numbering in the left column.
The classification is primarily based on seman-
tic intuition, not on knowledge about the syn-
tactic behaviour. As an extreme example, the
Support class (34) contains the verb unterstiitzen,
which syntactically requires a direct object, to-
gether with the three verbs dienen, folgen, helfen
which mainly subcategorise an indirect object.
</bodyText>
<sectionHeader confidence="0.955664" genericHeader="method">
3 Clustering Methodology
</sectionHeader>
<bodyText confidence="0.999227038461538">
Clustering is a standard procedure in multivariate
data analysis. It is designed to uncover an inher-
ent natural structure of data objects, and the in-
duced equivalence classes provide a means to gen-
eralise over the objects. We perform clustering by
the k-Means algorithm (Forgy, 1965), an unsuper-
vised hard clustering method assigning is data ob-
jects to k clusters.2 Initial verb clusters are iter-
atively re-organised by assigning each verb to its
closest cluster and re-calculating cluster centroids
until no further changes take place. The cluster-
ing methodology in this work is based on parame-
ter investigations in (Schulte im Walde and Brew,
2002): the clustering input is obtained from a hi-
erarchical analysis on the German verbs (Ward&apos;s
amalgamation method), the number of clusters be-
ing the number of manual classes; similarity mea-
sure is performed by the skew divergence, a variant
of the Kullback-Leibler divergence.
The 168 verbs are associated with probabilistic
frame descriptions on various levels of verb infor-
mation, and assigned to starting clusters by hierar-
chical clustering. The k-Means algorithm is then
allowed to run until no further changes take place,
and the resulting clusters are evaluated and inter-
preted against the manual classes.
</bodyText>
<footnote confidence="0.9807055">
&apos;The verb frequency range in 35 million words newspaper
data is 8-71,604.
2Hard clustering is an oversimplification for representing
ambiguous verbs, but it facilitates interpretation.
</footnote>
<page confidence="0.999214">
316
</page>
<sectionHeader confidence="0.98344" genericHeader="method">
4 Clustering Evaluation
</sectionHeader>
<bodyText confidence="0.999980607142857">
Evaluating the result of a cluster analysis against
the known gold standard of hand-constructed verb
classes requires to assess the similarity between
two partitions on the set of n verbs. The evaluation
is performed by an adjusted version of the Rand
index (Hubert and Arabie, 1985): The Rand index
measures the agreement between object pairs in
the partitions and is corrected for chance in com-
parison to the null model that the partitions are
picked at random, given the original number of
classes and objects.
The agreement in the two partitions is repre-
sented by a contingency table C x M: t,j denotes
the number of verbs common to classes C, in the
clustering partition C and M3 in the manual clas-
sification M; the marginals ti. and t.3 refer to the
number of objects in C, and M3, respectively. The
adjusted Rand index Radl is given in Equation (1);
the expected number of common object pairs at-
tributable to a particular cell (C,, M) in the con-
tingency table is defined by (t2i ) (t)/ (3) . The
range of Rad3 is 0 &lt; Rd3 &lt; 1, with only extreme
cases below zero. We choose Rad3 as evaluation
measure compared to e.g. the measures presented
in (Schulte im Walde and Brew, 2002), because
(a) it does not show a bias towards extreme cluster
sizes, and (b) it facilitates the interpretation with
its normally used bounds of 0 and 1.
</bodyText>
<equation confidence="0.986711666666667">
Itii E) )
CO Ed
(2&apos;)
(Ei + (tA) ) E3 (ti)
(Z)
(I)
</equation>
<sectionHeader confidence="0.992522" genericHeader="method">
5 Verb Description
</sectionHeader>
<bodyText confidence="0.999408644067797">
The German verbs are described on three levels
of subcategorisation definition D1 to D3, each re-
fining the previous level by additional informa-
tion. All information is extracted from a lexi-
calised probabilistic grammar which is unsuper-
vised trained on 35 million words of a German
newspaper corpus, using the EM-algorithm.
D1 provides a coarse syntactic definition of
subcategorisation. The verbs are described by a
probability distribution over 38 frame types. Pos-
sible arguments in the frames are nominative (n),
dative (d) and accusative (a) noun phrases, reflex-
ive pronouns (r), prepositional phrases (p), exple-
tive es (x), non-finite clauses (i), finite clauses (s-2
for verb second clauses, s-dass for dass-clauses, s-
ob for oh-clauses, s-w for indirect wh-questions),
and copula constructions (k). For example, sub-
categorising a direct (accusative case) object and a
non-finite clause next to the obligatory nominative
subject is labelled `nai&apos;.
On D2, the verbs are given a syntactico-
semantic definition of subcategorisation with
prepositional preferences. In addition to the syn-
tactic frame information, D2 discriminates be-
tween different kinds of pp-arguments. This is
done by distributing the probability mass of prepo-
sitional phrase frame types over the prepositional
phrases, according to their frequencies in the cor-
pus. Prepositional phrases are referred to by
case and preposition, such as &apos;mit])&apos;, &apos;, with
D=Dative and A=Accusative. We define 30 differ-
ent PPs, according to the most frequent PPs which
appear with at least 10 different verbs.
D3 gives a syntactico-semantic definition of
subcategorisation with prepositional and selec-
tional preferences. The argument slots within a
subcategorisation frame type are specified accord-
ing to which &apos;kind&apos; of argument they require. The
grammar provides selectional preference informa-
tion on a fine-grained level: it specifies argument
realisations for a specific verb-frame-slot combi-
nation in form of lexical heads. For example, the
most prominent nominal argument heads for the
verb verfolgen &apos;to follow&apos; in the accusative NP slot
of the transitive frame type &apos;rm.&apos; (the considered
frame slot is underlined) are Ziel &apos;goal&apos;, Strategie
&apos;strategy&apos;, Politik &apos;policy&apos;. Obviously, we would
run into a sparse data problem if we tried to in-
corporate selectional preferences on the nominal
level into the verb descriptions. We need a gen-
eralisation of the selectional preference definition,
for which we use the noun hierarchy in GennaNet
(Kunze, 2000), the German pendant of the seman-
tic ontology WordNet (Fellbaum, 1998).
The hierarchy is realised as synsets, sets of syn-
onymous nouns, which are organised into multiple
inheritance hypernym relationships. A noun may
appear in several synsets, according to its number
of senses. For each nominal argument in a verb-
</bodyText>
<equation confidence="0.973761">
Radi =
</equation>
<page confidence="0.986887">
317
</page>
<bodyText confidence="0.97474171875">
frame-slot combination, the joint frequency is split
over the different senses of the noun and prop-
agated upwards the hierarchy. In case of multi-
ple hypernym synsets, the frequency is split, such
that the sum of frequencies over the disjoint top
synsets equals the total joint frequency. Repeat-
ing the frequency assignment and propagation for
all nouns appearing in a verb-frame-slot combi-
nation, we define a frequency distribution of the
verb-frame-slot combination over all GermaNet
synsets. To restrict the variety of noun concepts,
we consider only the 15 top GermaNet nodes:
Lebewesen &apos;creature&apos;, Sache &apos;thing&apos;, Besitz &apos;prop-
erty&apos;, Substanz &apos;substance&apos;, Nahrung &apos;food&apos;, Mit-
tel &apos;means&apos;, Situation &apos;situation&apos;, Zustand &apos;state&apos;,
Struktur &apos;structure&apos;, Physis &apos;body&apos;, Zeit &apos;time&apos;,
Ort &apos;space&apos;, Attribut &apos;attribute&apos;, Kognitives Ob-
jekt `cognitive object&apos;, Kognitiver Prozess &apos;cogni-
tive process&apos;.3 Since the 15 nodes exclude each
other and the frequencies sum to the total joint
verb-frame frequency, we can define a probabil-
ity distribution over the top nodes representing
coarse selectional preferences for the respective
verb-frame-slot combination. To obtain D3, the
verb-frame probability is distributed over those se-
lectional preferences .4
Table 2 presents three verbs from different verb
classes and their ten most frequent frame types
with respect to the three levels of verb definition,
accompanied by the probability values. D1 for be-
ginnen `to begin&apos; defines `np&apos; and &apos;n&apos; as the most
probable frame types. Even by splitting the &apos;rip&apos;
probability over the different PP types in D2, a
number of prominent PPs are left, the time indi-
cating umA and nachD, mitD referring to the be-
gun event, anD as date and inD as place indicator.
It is obvious that not all PPs are argument PPs,
but also adjunct PPs describe a part of the verb
behaviour. D3 illustrates that typical selectional
preferences for beginner roles are Situation, Zus-
tand, Zeit, Sache. D3 has the potential to indicate
verb alternation behaviour, e.g. `na(Situation)&apos;
refers to the same role for the direct object in a
&apos;Little manual intervention was necessary to define a co-
herent set of top level nodes, since GermaNet had not been
completed.
4Strictly speaking, we do not have a probability distribu-
tion any longer, since multiple frame slots may be refined.
The skew divergence still works well.
transitive frame as &apos;n(Situation)&apos; in an intransitive
frame.
essen `to eat&apos; as an object drop verb shows strong
preferences for both an intransitive and transitive
usage. As desired, the argument roles are strongly
determined by Lebewesen for both &apos;n&apos; and `na&apos; and
Nahrung for `na&apos;.
fahren `to drive&apos; chooses typical manner of mo-
tion frames (&apos;n&apos;, `np&apos;, `na&apos;) with the refining PPs
being directional (inA, zap, nachD) or referring to
a means of motion (mitD, inD, aufD). The selec-
tional preferences represent a correct alternation
behaviour: Lebewesen in the object drop case for
&apos;n&apos; and `na&apos;, Sache in the inchoative/causative case
for &apos;n&apos; and &apos;rm.&apos;.
</bodyText>
<table confidence="0.999877235294117">
D1 D2 D3
beginnen &apos;to begin&apos;
np 0.43 n 0.28 n(Situation) 0.12
n 0.28 np:umA 0.16 np:umA (Situation) 0.09
ni 0.09 ni 0.09 np:mitD (Situation) 0.04
na 0.07 np:mitD 0.08 ni(Lebewesen) 0.03
nd 0.04 na 0.07 n(Zustand) 0.03
nap 0.03 np: anD 0.06 lip: anD (Situation) 0.03
nad 0.03 np:inD 0.06 np:inD (Situation) 0.03
nir 0.01 nd 0.04 n(Zeit) 0.03
ns-2 0.01 nad 0.02 n(Sache) 0.02
xp 0.01 np:nachD 0.01 na(Situation) 0.02
essen &apos;to eat&apos;
na 0.42 na 0.42 na(Lebewesen) 0.33
n 0.26 n 0.26 na(Nahrung) 0.17
nad 0.10 nad 0.10 na(Sache) 0.09
np 0.06 nd 0.05 n(Lebewesen) 0.08
nd 0.05 ns-2 0.02 na(Lebewesen) 0.07
nap 0.04 np:aufD 0.02 n(Nahrung) 0.06
ns-2 0.02 ns-w 0.01 n(Sache) 0.04
ns-w 0.01 ni 0.01 nd(Lebewesen) 0.04
ni 0.01 np: mitD 0.01 nd(Nahrung) 0.02
nas-2 0.01 np:inD 0.01 na(Attribut) 0.02
fahren &apos;to drive&apos;
n 0.34 n 0.34 n(Sache) 0.12
np 0.29 na 0.19 n(Lebewesen) 0.10
na 0.19 np:inA 0.05 na(Lebewesen) 0.08
nap 0.06 nad 0.04 na(Sache) 0.06
nad 0.04 np:zuD 0.04 n(Ort) 0.06
nd 0.04 nd 0.04 na(Sache) 0.05
ni 0.01 np:nachD 0.04 np:inA(Sache) 0.02
ns-2 0.01 np:mitD 0.03 np:zuD(Sache) 0.02
ndp 0.01 np:inD 0.03 np:inA(Lebewesen) 0.02
ns-w 0.01 np:aufD 0.02 np: nachD (S ache) 0.02
</table>
<tableCaption confidence="0.998556">
Table 2: Examples of most probable frame types
</tableCaption>
<sectionHeader confidence="0.995522" genericHeader="method">
6 Feature Variation
</sectionHeader>
<bodyText confidence="0.999631">
The previous section introduced the verb descrip-
tion in an &apos;as is&apos; fashion, but obviously one can
</bodyText>
<page confidence="0.99798">
318
</page>
<bodyText confidence="0.99959275">
find multiple variations. In order to illustrate
that the most plausible variations have been
considered, we describe and use linguistically
intuitive mutations of the verb descriptions.5
</bodyText>
<listItem confidence="0.9221605">
• On Dl, there is little room to vary the
verb information, since the valency encod-
ing is close to standard German grammar, cf.
Helbig and Buscha (1998).
• On D2, we vary the amount of PP information:
(a) Following standard German grammar books
we define a more restricted set of prepositional
phrases for argument usage, and (b) ignoring
any frequency constraint on the PP information
increases the kinds of PPs in the relevant frame
types up to 140.
• On D3, there is most room for variation:
Role Choice: Instead of using the 15 top level
nodes in GermaNet, (a) we use selectional prefer-
</listItem>
<bodyText confidence="0.976429392857143">
ences on a more fine-grained level, the word level,
and (b) we define a more generalised description
of selectional preferences, by merging the fre-
quencies of the 15 top level nodes in GermaNet
to only 2 (Lebewesen, Objekt) or 3 (Lebewesen,
Sache, Abstraktum).
Role Integration: To integrate the selectional
preferences into the verb description, either (a)
each argument slot in a subcategorisation frame
is substituted by selectional roles separately, e.g.
the joint frequency of a verb and transitive `na&apos; is
distributed over the nominative slot preferences
`na(Lebewesen)&apos; , `na(Sache)&apos;, etc. and also over
the accusative slot preferences `na(Lebewesen)&apos;,
`na(Sache)&apos;, etc. (as in Table 2). In this case,
the argument slots of frame types with several
arguments are considered independently, but
the number of features remains in a reasonable
magnitude, 15 per frame slot. Or (b) the subcate-
gorisation frames are substituted by the combina-
tions of selectional preferences for the argument
slots, e.g. the joint probability of a verb and `na&apos;
is distributed over cna(Lebewesen:Nahrung)&apos;,
na(Lebewesen : S ache) &apos; , `na(Sache:Nahrung)&apos;,
etc. This encoding would directly represent
the linguistic idea of alternations, but no direct
frequencies are available, and the number of
features explodes (15 features for an intransitive,
</bodyText>
<footnote confidence="0.6831825">
5We do not attempt to optimise the feature set algorithmi-
cally, because that would lead to overfitting.
</footnote>
<bodyText confidence="0.997566">
152 for a transitive, 153 for a ditransitive) and
leads to differing magnitudes of probabilities.
Role Means: We could use a different means for
selectional role representation than GermaNet.
But since the ontological idea of WordNet has
been widely and successfully used and we do not
have any comparable source at hand, we have to
exclude this variation.
</bodyText>
<sectionHeader confidence="0.88658" genericHeader="method">
7 Clustering Results
</sectionHeader>
<bodyText confidence="0.999968380952381">
The baseline for the clustering experiments is
Radj — —0.004 and refers to 50 random cluster-
ings: The verbs are randomly assigned to a cluster
(with a cluster number between 1 and the number
of manual classes 43), and the resulting cluster-
ing is evaluated. The baseline value is the average
value of the 50 repetitions. The upper bound is
Radj = 0.909 and calculated on a hard version
of the manual classification, i.e. multiple senses
of verbs are reduced to a single class affiliation,
which represents the optimum for the hard clus-
tering algorithm.
Table 3 presents the clustering results for D1
and D2, with D2 distinguishing the amount of PP
information (arg for arguments only, chosen for
the manually defined PPs, all for all possible PPs).
As stated by Schulte im Walde and Brew (2002),
refining the syntactic verb information by prepo-
sitional phrases is helpful for the clustering; and
the usage is not restricted to argument PPs, but ex-
tended by the more variable PP information.
</bodyText>
<table confidence="0.8949802">
Distribution Radj
D1 0.094
D2 ppa„ 0.151
PPchosen 0.151
PPau 0.160
</table>
<tableCaption confidence="0.999628">
Table 3: Clustering results on D1 and D2
</tableCaption>
<bodyText confidence="0.9998469">
Underlying the results in Table 4, the argument
roles for selectional preference information in D3
are varied. The left part presents the results when
refining only a single argument within a single
frame, in addition to D2. Obviously, the results
do not match linguistic intuition. For example, we
would expect the arguments in the two highly fre-
quent intransitive &apos;n&apos; and transitive `na&apos; to provide
valuable information with respect to their selec-
tional preferences, but only those in `na&apos; improve
</bodyText>
<page confidence="0.998574">
319
</page>
<bodyText confidence="0.999833875">
D2. On the other hand, &apos;Ili&apos; which is not expected
to provide variable definitions of selectional pref-
erences for the nominative slot, does work bet-
ter than &apos;n&apos;. The right part in Table 4 illustrates
the clustering results for example combinations of
argument slots refined by selectional preferences,
e.g. n/na means that the nominative slot in &apos;n&apos;, and
both the nominative and accusative slot in `na&apos; are
refined by selectional preferences. The combined
information does not necessarily improve the sin-
gle slot clustering results, e.g. n/na achieves re-
sults below the ones for refining only na or na. The
overall best result (including non-illustrated exper-
iment results) is achieved by defining selectional
preferences on n/na/nd/nad/ns-dass, better than re-
fining all NP slots or all NP and all PP slots in the
frame types. Summarising, Table 4 illustrates that
a linguistic choice of features is worthwhile, but
linguistic intuition and algorithmic clustering re-
sults do not necessarily align. On selected argu-
ment roles, the selectional preference information
in D3 once more improves the clustering results
compared to D2, but the improvement is not as
persuasive as D2 improving D1.
</bodyText>
<table confidence="0.998728285714286">
Single Slots Slot Combinations
n 0.125 na 0.137
na 0.176 n/na 0.128
na 0.164 nad 0.088
nad 0.144 n/na/nad 0.118
nad 0.115 nd 0.150
nad 0.161 n/na/nd 0.124
nd 0.152 n/na/nad/nd 0.161
nd 0.143 n/na/nd/nad/ns-dass 0.182
np 0.133 np/ni/nr/ns-2/ns-dass 0.131
ni 0.148 all NP 0.158
or 0.136 all NPs+PPs 0.176
ns -2 0.121
ns-dass 0.156
</table>
<tableCaption confidence="0.99967">
Table 4: Clustering results on varying D3
</tableCaption>
<bodyText confidence="0.999477266666667">
With respect to further feature variation, merg-
ing the frequencies of the 15 top level nodes in
GermaNet to 2 or 3 roles results in noisy distri-
butions and destroys the coherence of the cluster
analyses. Experiment setups which either include
a nominal level of selectional preference informa-
tion or an alternation-like combination of selec-
tional roles were tried, but they suffer from their
time demands and result in far worse analyses.
Finally, we present representative parts of the
cluster analysis based on D3, with selectional
roles &apos;n&apos;, `na&apos;, &apos;rid&apos;, `ns-dass&apos;, and com-
pares the respective clusters with their pendants
under D1 and D2. The manual class numbers as
defined in Table 1 are given as subscripts.
</bodyText>
<figure confidence="0.9854639">
(a) beginneni bestehen37 endeni existieren37
laufen8 liegenn sitzenn stehenn
(b) eilenio gleiteni2 kriechen8 rennen8 starren16
(c) fahrenn fliegenn flie13en12 klettern8 segelnii
wandern8
(d) bilden32 erhohen35 festlegen22 senken35
steigern35 vergro13ern35 verkleinern35
(e) toten39 unterrichten29
(f) nieseln43 regnen43 schneien43
(g) dammern43
</figure>
<bodyText confidence="0.999163878787879">
The weather verbs in cluster (f) strongly agree in
their syntactic expression on D1 and do not need
D2 or D3 refinements for a successful class con-
stitution. dammern in cluster (g) is ambiguous
between a weather verb and expressing a sense
of understanding; this ambiguity is idiosyncrati-
cally expressed in D1 frames already, so dammem
is never clustered together with the other weather
verbs on D1 — 3.
Manner of Motion, Existence, Position and As-
pect verbs are similar in their syntactic frame us-
age and therefore merged together on D1, but
adding PP information distinguishes the respec-
tive verb classes: Manner of Motion verbs primar-
ily demand directional PPs, Aspect verbs are dis-
tinguished by patient mitp and time and location
prepositions, and Existence and Position verbs are
distinguished by locative prepositions, with Posi-
tion verbs showing more PP variation. The PP in-
formation is essential for successfully distinguish-
ing these verb classes, and the coherence is partly
destroyed by D3: Manner of Motion verbs (from
the sub-classes 8-12) are captured well by clus-
ters (b) and (c), since they inhibit strong com-
mon alternations, but cluster (a) merges the Ex-
istence, Position and Aspect verbs, since verb-
idiosyncratic demands on selectional roles destroy
the D2 class demarcation. Admittedly, the verbs
in cluster (a) are close in their semantics, with a
common sense of (bringing into vs. being in) exis-
tence. Schumacher (1986) actually classifies most
of the verbs into one existence class. lactfen fits
into the cluster with its sense of `to function&apos;.
</bodyText>
<page confidence="0.993393">
320
</page>
<bodyText confidence="0.999793083333333">
Cluster (d) contains most verbs of Quantum
Change, together with one verb of Production and
Constitution each. The semantics of the cluster is
therefore rather pure. The verbs in the cluster typi-
cally subcategorise a direct object, alternating with
a reflexive usage, `nr&apos; and `npr&apos; with mostly aufA
and um. The selectional preferences help to dis-
tinguish this cluster: the verbs agree in demanding
a thing or situation as subject, and various objects
such as attribute, cognitive object, state, structure
or thing as object. Without selectional preferences
(on D1 and D2), the change of quantum verbs are
not found together with the same degree of purity.
There are verbs as in cluster (e), whose proper-
ties are correctly stated as similar on D1 — 3, so
a common cluster is justified; but the verbs only
have coarse common meaning components, in this
case toten and unterrichten agree in an action of
one person or institution towards another.
Summarising the cluster description, some
verbs and verb classes are distinctive on a coarse
feature level, some need fine-grained extensions,
and some are not distinctive with respect to any
combination of features.
</bodyText>
<sectionHeader confidence="0.996771" genericHeader="discussions">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999995728571429">
We have presented a clustering methodology for
German verbs whose results agree with a manual
classification in many respects and should prove
useful as automatic basis for a large-scale cluster-
ing. Without any doubt the cluster analysis would
need manual correction and completion, but rep-
resents a plausible basis.
The various verb descriptions illustrate that
step-wise refining the features does improve the
clustering. But the linguistic feature refinements
not necessarily align with expected changes in
clustering. This effect could be due to (i) noisy
or (ii) sparse data, but (i) the example distribu-
tions in Table 2 demonstrate that —even if noisy—
our basic verb descriptions appear reliable with
respect to their desired linguistic content. In ad-
dition, the subcategorisation information on D1
and D2 has been evaluated against manual defi-
nitions in a dictionary and proven useful (Schulte
im Walde, 2002). And (ii) Table 4 illustrates that
even with adding little information (e.g. refining
a single argument by 15 selectional roles results
in 186 instead of 171 features) linguistic intuition
and clustering results do not necessarily align.
Related work on automatic verb classes con-
firms the difficulty of selecting and encoding
verb features. Schulte im Walde (2000) clusters
153 English verbs into 30 verb classes as taken
from (Levin, 1993), using a hierarchical cluster-
ing method. The clustering is most successful
when utilising syntactic subcategorisation frames
enriched with PP information (comparable to our
D2); selectional preferences are encoded by role
combinations taken from WordNet. Schulte im
Walde claims the detailed encoding and there-
fore sparse data to make the clustering worse
with than without the selectional preference in-
formation. Merlo and Stevenson (2001) classify a
smaller number of 60 English verbs into three verb
classes, by utilising supervised decision trees. The
features of the verbs are restricted to those which
should capture the basic differences between the
verb classes, and the feature values are approached
by corpus-based heuristics (e.g. measuring the de-
gree of animacy by personal pronoun realisation
in the transitive subject slot). An extension of
their work by Joanis (2002) uses 802 verbs from
14 classes in (Levin, 1993). He defines an exten-
sive feature space with 219 core features (such as
part of speech, auxiliary frequency, syntactic cat-
egories, animacy as above) and 1,140 selectional
preference features taken from WordNet. As in
our approach, the selectional preferences do not
improve the clustering.
Why do we encounter such unpredictability
concerning the encoding and effect of verb fea-
tures, especially with respect to selectional prefer-
ences? In contrast to previous approaches concen-
trating on the sparse data problem, we have pre-
sented evidence for a linguistically defined limit
on the usefulness of the verb features, driven by
the idiosyncratic properties of the verbs. Recall
the underlying idea of verb classes, that the mean-
ing components of verbs to a certain extent deter-
mine their behaviour. This does not mean that all
properties of all verbs in a common class are sim-
ilar and we could extend and refine the feature de-
scription endlessly, still improving the clustering.
The meaning of verbs comprises both (i) prop-
erties which are general for the respective verb
</bodyText>
<page confidence="0.995721">
321
</page>
<bodyText confidence="0.999991580645161">
classes, and (ii) idiosyncratic properties which dis-
tinguish the verbs from each other. As long as we
define the verbs by those properties which repre-
sent the common parts of the verb classes, a clus-
tering can succeed. But with step-wise refining the
verb description by including lexical idiosyncrasy,
the emphasis of the common properties vanishes.
The exemplary description of cluster outcomes
in the previous section confirms that it is impos-
sible to determine an overall appropriate level of
feature specification which suffices all kinds of
verb classes defined in Table 1. Some verbs and
verb classes are distinctive on a coarse feature
level, some need fine-grained extensions, some are
not distinctive with respect to any combination of
features. There is no unique perfect choice and en-
coding of the verb features, even more with respect
to a potential large-scale extension of verbs and
classes. Further work on the verb classes should
concern a choice of verb features with respect to
the specific properties of the desired verb clas-
sification. We could think of either (i) performing
several cluster analyses on the same set of verbs,
but with different choices of verb features, and
then find a way to merge the results to a unique
classification, or (ii) not aiming for a fine-grained
clustering, but create fewer but larger clusters on
coarse features, which classify the verbs on a more
general level. Both solutions should facilitate the
demarcation of common and idiosyncratic verb
features and improve the clustering results.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915686567164">
Bonnie J. Dorr and Doug Jones. 1996. Role of Word
Sense Disambiguation in Lexical Acquisition: Pre-
dicting Semantics from Syntactic Cues. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pages 322-327, Copenhagen,
Denmark.
Bonnie J. Don. 1997. Large-Scale Dictionary Con-
struction for Foreign Language Tutoring and Inter-
lingual Machine Translation. Machine Translation,
12(4 ):271-322.
Christiane Fellbaum, editor. 1998. WordNet — An Elec-
tronic Lexical Database. Language, Speech, and
Communication. MIT Press, Cambridge, MA.
Edward W. Forgy. 1965. Cluster Analysis of Multi-
variate Data: Efficiency vs. Interpretability of Clas-
sifications. Biometrics, 21:768-780.
Gerhard Helbig and Joachim Buscha. 1998. Deutsche
Grammatik. Langenscheidt — Verlag Enzyklopadie,
18th edition.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing Partitions. Journal of Classification, 2:193-218.
Eric Joanis. 2002. Automatic Verb Classification
using a General Feature Space. Master&apos;s thesis,
Department of Computer Science, University of
Toronto.
Judith L. Klavans and Min-Yen Kan. 1998. The Role
of Verbs in Document Analysis. In Proceedings of
the 17th International Conference on Computational
Linguistics, pages 680-686, Montreal, Canada.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, Computer
Laboratory. Technical Report UCAM-CL-TR-530.
Claudia Kunze. 2000. Extension and Use of Ger-
maNet, a Lexical-Semantic Database. In Proceed-
ings of the 2nd International Conference on Lan-
guage Resources and Evaluation, pages 999-1002,
Athens, Greece.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic Verb Classification Based on Statistical Distri-
butions of Argument Structure. Computational Lin-
guistics, 27(3):373-408.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a Probabilistic Class-Based Lexicon for Lex-
ical Ambiguity Resolution. In Proceedings of the
18th International Conference on Computational
Linguistics, pages 649-655, Saarbriicken, Germany.
Sabine Schulte im Walde and Chris Brew. 2002. In-
ducing German Semantic Verb Classes from Purely
Syntactic Subcategorisation Information. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 223-230,
Philadelphia, PA.
Sabine Schulte im Walde. 2000. Clustering Verbs
Semantically According to their Alternation Be-
haviour. In Proceedings of the 18th International
Conference on Computational Linguistics, pages
747-753, Saarbriicken, Germany.
Sabine Schulte im Walde. 2002. Evaluating Verb
Subcategorisation Frames learned by a German Sta-
tistical Grammar against Manual Definitions in the
Duden Dictionary. In Proceedings of the 10th
EURALEX International Congress, pages 187-197,
Copenhagen, Denmark.
Helmut Schumacher. 1986. Verben in Feldem. de
Gruyter, Berlin.
</reference>
<page confidence="0.998351">
322
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973345">
<title confidence="0.999657">Experiments on the Choice of Features for Learning Verb Classes</title>
<author confidence="0.999179">Sabine Schulte im Walde</author>
<affiliation confidence="0.998338">Institut fur Maschinelle Sprachverarbeitung Universitat Stuttgart</affiliation>
<address confidence="0.988299">AzenbergstraBe 12, 70174 Stuttgart, Germany</address>
<abstract confidence="0.9992593125">The choice of verb features is crucial for the learning of verb classes. This paper presents clustering experiments on 168 German verbs, which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Doug Jones</author>
</authors>
<title>Role of Word Sense Disambiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>322--327</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1040" citStr="Dorr and Jones, 1996" startWordPosition="147" endWordPosition="150">e levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im W</context>
</contexts>
<marker>Dorr, Jones, 1996</marker>
<rawString>Bonnie J. Dorr and Doug Jones. 1996. Role of Word Sense Disambiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues. In Proceedings of the 16th International Conference on Computational Linguistics, pages 322-327, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Don</author>
</authors>
<title>Large-Scale Dictionary Construction for Foreign Language Tutoring and Interlingual Machine Translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<volume>12</volume>
<issue>4</issue>
<pages>271--322</pages>
<contexts>
<context position="1097" citStr="Don, 1997" startWordPosition="158" endWordPosition="159">tional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Com</context>
</contexts>
<marker>Don, 1997</marker>
<rawString>Bonnie J. Don. 1997. Large-Scale Dictionary Construction for Foreign Language Tutoring and Interlingual Machine Translation. Machine Translation, 12(4 ):271-322.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>WordNet — An Electronic Lexical Database. Language, Speech, and Communication.</booktitle>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="17037" citStr="(1998)" startWordPosition="2580" endWordPosition="2580"> 0.02 ndp 0.01 np:inD 0.03 np:inA(Lebewesen) 0.02 ns-w 0.01 np:aufD 0.02 np: nachD (S ache) 0.02 Table 2: Examples of most probable frame types 6 Feature Variation The previous section introduced the verb description in an &apos;as is&apos; fashion, but obviously one can 318 find multiple variations. In order to illustrate that the most plausible variations have been considered, we describe and use linguistically intuitive mutations of the verb descriptions.5 • On Dl, there is little room to vary the verb information, since the valency encoding is close to standard German grammar, cf. Helbig and Buscha (1998). • On D2, we vary the amount of PP information: (a) Following standard German grammar books we define a more restricted set of prepositional phrases for argument usage, and (b) ignoring any frequency constraint on the PP information increases the kinds of PPs in the relevant frame types up to 140. • On D3, there is most room for variation: Role Choice: Instead of using the 15 top level nodes in GermaNet, (a) we use selectional preferences on a more fine-grained level, the word level, and (b) we define a more generalised description of selectional preferences, by merging the frequencies of the</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet — An Electronic Lexical Database. Language, Speech, and Communication. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward W Forgy</author>
</authors>
<title>Cluster Analysis of Multivariate Data: Efficiency vs.</title>
<date>1965</date>
<journal>Interpretability of Classifications. Biometrics,</journal>
<pages>21--768</pages>
<contexts>
<context position="6906" citStr="Forgy, 1965" startWordPosition="943" endWordPosition="944">y based on semantic intuition, not on knowledge about the syntactic behaviour. As an extreme example, the Support class (34) contains the verb unterstiitzen, which syntactically requires a direct object, together with the three verbs dienen, folgen, helfen which mainly subcategorise an indirect object. 3 Clustering Methodology Clustering is a standard procedure in multivariate data analysis. It is designed to uncover an inherent natural structure of data objects, and the induced equivalence classes provide a means to generalise over the objects. We perform clustering by the k-Means algorithm (Forgy, 1965), an unsupervised hard clustering method assigning is data objects to k clusters.2 Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster and re-calculating cluster centroids until no further changes take place. The clustering methodology in this work is based on parameter investigations in (Schulte im Walde and Brew, 2002): the clustering input is obtained from a hierarchical analysis on the German verbs (Ward&apos;s amalgamation method), the number of clusters being the number of manual classes; similarity measure is performed by the skew divergence, a va</context>
</contexts>
<marker>Forgy, 1965</marker>
<rawString>Edward W. Forgy. 1965. Cluster Analysis of Multivariate Data: Efficiency vs. Interpretability of Classifications. Biometrics, 21:768-780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Helbig</author>
<author>Joachim Buscha</author>
</authors>
<date>1998</date>
<booktitle>Deutsche Grammatik. Langenscheidt — Verlag Enzyklopadie, 18th edition.</booktitle>
<contexts>
<context position="17037" citStr="Helbig and Buscha (1998)" startWordPosition="2577" endWordPosition="2580">0.03 np:zuD(Sache) 0.02 ndp 0.01 np:inD 0.03 np:inA(Lebewesen) 0.02 ns-w 0.01 np:aufD 0.02 np: nachD (S ache) 0.02 Table 2: Examples of most probable frame types 6 Feature Variation The previous section introduced the verb description in an &apos;as is&apos; fashion, but obviously one can 318 find multiple variations. In order to illustrate that the most plausible variations have been considered, we describe and use linguistically intuitive mutations of the verb descriptions.5 • On Dl, there is little room to vary the verb information, since the valency encoding is close to standard German grammar, cf. Helbig and Buscha (1998). • On D2, we vary the amount of PP information: (a) Following standard German grammar books we define a more restricted set of prepositional phrases for argument usage, and (b) ignoring any frequency constraint on the PP information increases the kinds of PPs in the relevant frame types up to 140. • On D3, there is most room for variation: Role Choice: Instead of using the 15 top level nodes in GermaNet, (a) we use selectional preferences on a more fine-grained level, the word level, and (b) we define a more generalised description of selectional preferences, by merging the frequencies of the</context>
</contexts>
<marker>Helbig, Buscha, 1998</marker>
<rawString>Gerhard Helbig and Joachim Buscha. 1998. Deutsche Grammatik. Langenscheidt — Verlag Enzyklopadie, 18th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hubert</author>
<author>Phipps Arabie</author>
</authors>
<title>Comparing Partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<pages>2--193</pages>
<contexts>
<context position="8379" citStr="Hubert and Arabie, 1985" startWordPosition="1170" endWordPosition="1173">o run until no further changes take place, and the resulting clusters are evaluated and interpreted against the manual classes. &apos;The verb frequency range in 35 million words newspaper data is 8-71,604. 2Hard clustering is an oversimplification for representing ambiguous verbs, but it facilitates interpretation. 316 4 Clustering Evaluation Evaluating the result of a cluster analysis against the known gold standard of hand-constructed verb classes requires to assess the similarity between two partitions on the set of n verbs. The evaluation is performed by an adjusted version of the Rand index (Hubert and Arabie, 1985): The Rand index measures the agreement between object pairs in the partitions and is corrected for chance in comparison to the null model that the partitions are picked at random, given the original number of classes and objects. The agreement in the two partitions is represented by a contingency table C x M: t,j denotes the number of verbs common to classes C, in the clustering partition C and M3 in the manual classification M; the marginals ti. and t.3 refer to the number of objects in C, and M3, respectively. The adjusted Rand index Radl is given in Equation (1); the expected number of com</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Lawrence Hubert and Phipps Arabie. 1985. Comparing Partitions. Journal of Classification, 2:193-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
</authors>
<title>Automatic Verb Classification using a General Feature Space.</title>
<date>2002</date>
<tech>Master&apos;s thesis,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="1692" citStr="Joanis, 2002" startWordPosition="244" endWordPosition="245">lation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb information offered by available tools and resources such as statistical parsers and semantic ontologies, suffer from severe problems to encode and benefit from the information, especially with respect to selectional preferences, cf. Schulte im Walde (2000); Joanis (2002). This paper presents clustering experiments on German verbs which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional</context>
<context position="28516" citStr="Joanis (2002)" startWordPosition="4402" endWordPosition="4403">de claims the detailed encoding and therefore sparse data to make the clustering worse with than without the selectional preference information. Merlo and Stevenson (2001) classify a smaller number of 60 English verbs into three verb classes, by utilising supervised decision trees. The features of the verbs are restricted to those which should capture the basic differences between the verb classes, and the feature values are approached by corpus-based heuristics (e.g. measuring the degree of animacy by personal pronoun realisation in the transitive subject slot). An extension of their work by Joanis (2002) uses 802 verbs from 14 classes in (Levin, 1993). He defines an extensive feature space with 219 core features (such as part of speech, auxiliary frequency, syntactic categories, animacy as above) and 1,140 selectional preference features taken from WordNet. As in our approach, the selectional preferences do not improve the clustering. Why do we encounter such unpredictability concerning the encoding and effect of verb features, especially with respect to selectional preferences? In contrast to previous approaches concentrating on the sparse data problem, we have presented evidence for a lingu</context>
</contexts>
<marker>Joanis, 2002</marker>
<rawString>Eric Joanis. 2002. Automatic Verb Classification using a General Feature Space. Master&apos;s thesis, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>Min-Yen Kan</author>
</authors>
<title>The Role of Verbs in Document Analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>680--686</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1146" citStr="Klavans and Kan, 1998" startWordPosition="162" endWordPosition="166">onal preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification whic</context>
</contexts>
<marker>Klavans, Kan, 1998</marker>
<rawString>Judith L. Klavans and Min-Yen Kan. 1998. The Role of Verbs in Document Analysis. In Proceedings of the 17th International Conference on Computational Linguistics, pages 680-686, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="1212" citStr="Korhonen, 2002" startWordPosition="173" endWordPosition="174"> sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb informat</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge, Computer Laboratory. Technical Report UCAM-CL-TR-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Kunze</author>
</authors>
<title>Extension and Use of GermaNet, a Lexical-Semantic Database.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>999--1002</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="12007" citStr="Kunze, 2000" startWordPosition="1764" endWordPosition="1765"> realisations for a specific verb-frame-slot combination in form of lexical heads. For example, the most prominent nominal argument heads for the verb verfolgen &apos;to follow&apos; in the accusative NP slot of the transitive frame type &apos;rm.&apos; (the considered frame slot is underlined) are Ziel &apos;goal&apos;, Strategie &apos;strategy&apos;, Politik &apos;policy&apos;. Obviously, we would run into a sparse data problem if we tried to incorporate selectional preferences on the nominal level into the verb descriptions. We need a generalisation of the selectional preference definition, for which we use the noun hierarchy in GennaNet (Kunze, 2000), the German pendant of the semantic ontology WordNet (Fellbaum, 1998). The hierarchy is realised as synsets, sets of synonymous nouns, which are organised into multiple inheritance hypernym relationships. A noun may appear in several synsets, according to its number of senses. For each nominal argument in a verbRadi = 317 frame-slot combination, the joint frequency is split over the different senses of the noun and propagated upwards the hierarchy. In case of multiple hypernym synsets, the frequency is split, such that the sum of frequencies over the disjoint top synsets equals the total join</context>
</contexts>
<marker>Kunze, 2000</marker>
<rawString>Claudia Kunze. 2000. Extension and Use of GermaNet, a Lexical-Semantic Database. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, pages 999-1002, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1341" citStr="Levin (1993)" startWordPosition="194" endWordPosition="195">iosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb information offered by available tools and resources such as statistical parsers and semantic ontologies, suffer from severe problems to </context>
<context position="3126" citStr="Levin, 1993" startWordPosition="469" endWordPosition="470">esent evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the verb classification. 2 German Verb Classes A set of 168 German verbs is manually classified into 43 concise semantic verb classes. The purpose of the manual classification is (i) to evaluate the reliability and performance of the clustering experiments on a preliminary set of verbs, and (ii) to explore the potential and limit to apply the clustering method to large-scale verb data. The German classes are closely related to the English pendant in (Levin, 1993) and agree with the German verb classification in (Schumacher, 1986) as far as the relevant verbs appear in his semantic &apos;fields&apos;. Table 1 presents the manual verb classification. The class size is between 2 and 7, with an average of 3.9 verbs per class. Eight verbs are am315 Aspect: anfangen, aufhoren, beenden, beginnen, enden Propositional Attitude: ahnen, denken, glauben, vermuten, wissen Desire: Wish: erhoffen, wollen, wiinschen Need: bean-fen, benotigen, brauchen Transfer of Possession (Obtaining): bekommen, erhalten, erlangen, kriegen Transfer of Possession (Giving): Gift: geben, leihen,</context>
<context position="27633" citStr="Levin, 1993" startWordPosition="4270" endWordPosition="4271">tent. In addition, the subcategorisation information on D1 and D2 has been evaluated against manual definitions in a dictionary and proven useful (Schulte im Walde, 2002). And (ii) Table 4 illustrates that even with adding little information (e.g. refining a single argument by 15 selectional roles results in 186 instead of 171 features) linguistic intuition and clustering results do not necessarily align. Related work on automatic verb classes confirms the difficulty of selecting and encoding verb features. Schulte im Walde (2000) clusters 153 English verbs into 30 verb classes as taken from (Levin, 1993), using a hierarchical clustering method. The clustering is most successful when utilising syntactic subcategorisation frames enriched with PP information (comparable to our D2); selectional preferences are encoded by role combinations taken from WordNet. Schulte im Walde claims the detailed encoding and therefore sparse data to make the clustering worse with than without the selectional preference information. Merlo and Stevenson (2001) classify a smaller number of 60 English verbs into three verb classes, by utilising supervised decision trees. The features of the verbs are restricted to tho</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
</authors>
<date>2001</date>
<booktitle>Automatic Verb Classification Based on Statistical Distributions of Argument Structure. Computational Linguistics,</booktitle>
<pages>27--3</pages>
<contexts>
<context position="1677" citStr="Merlo and Stevenson, 2001" startWordPosition="240" endWordPosition="243">t al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb information offered by available tools and resources such as statistical parsers and semantic ontologies, suffer from severe problems to encode and benefit from the information, especially with respect to selectional preferences, cf. Schulte im Walde (2000); Joanis (2002). This paper presents clustering experiments on German verbs which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information </context>
<context position="28074" citStr="Merlo and Stevenson (2001)" startWordPosition="4331" endWordPosition="4334">tomatic verb classes confirms the difficulty of selecting and encoding verb features. Schulte im Walde (2000) clusters 153 English verbs into 30 verb classes as taken from (Levin, 1993), using a hierarchical clustering method. The clustering is most successful when utilising syntactic subcategorisation frames enriched with PP information (comparable to our D2); selectional preferences are encoded by role combinations taken from WordNet. Schulte im Walde claims the detailed encoding and therefore sparse data to make the clustering worse with than without the selectional preference information. Merlo and Stevenson (2001) classify a smaller number of 60 English verbs into three verb classes, by utilising supervised decision trees. The features of the verbs are restricted to those which should capture the basic differences between the verb classes, and the feature values are approached by corpus-based heuristics (e.g. measuring the degree of animacy by personal pronoun realisation in the transitive subject slot). An extension of their work by Joanis (2002) uses 802 verbs from 14 classes in (Levin, 1993). He defines an extensive feature space with 219 core features (such as part of speech, auxiliary frequency, s</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>Paola Merlo and Suzanne Stevenson. 2001. Automatic Verb Classification Based on Statistical Distributions of Argument Structure. Computational Linguistics, 27(3):373-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity Resolution.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>649--655</pages>
<location>Saarbriicken, Germany.</location>
<contexts>
<context position="1064" citStr="Prescher et al., 2000" startWordPosition="151" endWordPosition="154">iption, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and St</context>
</contexts>
<marker>Prescher, Riezler, Rooth, 2000</marker>
<rawString>Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000. Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity Resolution. In Proceedings of the 18th International Conference on Computational Linguistics, pages 649-655, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Chris Brew</author>
</authors>
<title>Inducing German Semantic Verb Classes from Purely Syntactic Subcategorisation Information.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>223--230</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7273" citStr="Walde and Brew, 2002" startWordPosition="1000" endWordPosition="1003">edure in multivariate data analysis. It is designed to uncover an inherent natural structure of data objects, and the induced equivalence classes provide a means to generalise over the objects. We perform clustering by the k-Means algorithm (Forgy, 1965), an unsupervised hard clustering method assigning is data objects to k clusters.2 Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster and re-calculating cluster centroids until no further changes take place. The clustering methodology in this work is based on parameter investigations in (Schulte im Walde and Brew, 2002): the clustering input is obtained from a hierarchical analysis on the German verbs (Ward&apos;s amalgamation method), the number of clusters being the number of manual classes; similarity measure is performed by the skew divergence, a variant of the Kullback-Leibler divergence. The 168 verbs are associated with probabilistic frame descriptions on various levels of verb information, and assigned to starting clusters by hierarchical clustering. The k-Means algorithm is then allowed to run until no further changes take place, and the resulting clusters are evaluated and interpreted against the manual</context>
<context position="9278" citStr="Walde and Brew, 2002" startWordPosition="1337" endWordPosition="1340">ted by a contingency table C x M: t,j denotes the number of verbs common to classes C, in the clustering partition C and M3 in the manual classification M; the marginals ti. and t.3 refer to the number of objects in C, and M3, respectively. The adjusted Rand index Radl is given in Equation (1); the expected number of common object pairs attributable to a particular cell (C,, M) in the contingency table is defined by (t2i ) (t)/ (3) . The range of Rad3 is 0 &lt; Rd3 &lt; 1, with only extreme cases below zero. We choose Rad3 as evaluation measure compared to e.g. the measures presented in (Schulte im Walde and Brew, 2002), because (a) it does not show a bias towards extreme cluster sizes, and (b) it facilitates the interpretation with its normally used bounds of 0 and 1. Itii E) ) CO Ed (2&apos;) (Ei + (tA) ) E3 (ti) (Z) (I) 5 Verb Description The German verbs are described on three levels of subcategorisation definition D1 to D3, each refining the previous level by additional information. All information is extracted from a lexicalised probabilistic grammar which is unsupervised trained on 35 million words of a German newspaper corpus, using the EM-algorithm. D1 provides a coarse syntactic definition of subcategor</context>
<context position="20081" citStr="Walde and Brew (2002)" startWordPosition="3069" endWordPosition="3072">n 1 and the number of manual classes 43), and the resulting clustering is evaluated. The baseline value is the average value of the 50 repetitions. The upper bound is Radj = 0.909 and calculated on a hard version of the manual classification, i.e. multiple senses of verbs are reduced to a single class affiliation, which represents the optimum for the hard clustering algorithm. Table 3 presents the clustering results for D1 and D2, with D2 distinguishing the amount of PP information (arg for arguments only, chosen for the manually defined PPs, all for all possible PPs). As stated by Schulte im Walde and Brew (2002), refining the syntactic verb information by prepositional phrases is helpful for the clustering; and the usage is not restricted to argument PPs, but extended by the more variable PP information. Distribution Radj D1 0.094 D2 ppa„ 0.151 PPchosen 0.151 PPau 0.160 Table 3: Clustering results on D1 and D2 Underlying the results in Table 4, the argument roles for selectional preference information in D3 are varied. The left part presents the results when refining only a single argument within a single frame, in addition to D2. Obviously, the results do not match linguistic intuition. For example,</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>Sabine Schulte im Walde and Chris Brew. 2002. Inducing German Semantic Verb Classes from Purely Syntactic Subcategorisation Information. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 223-230, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Clustering Verbs Semantically According to their Alternation Behaviour.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>747--753</pages>
<location>Saarbriicken, Germany.</location>
<contexts>
<context position="1650" citStr="Walde, 2000" startWordPosition="238" endWordPosition="239">6; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb information offered by available tools and resources such as statistical parsers and semantic ontologies, suffer from severe problems to encode and benefit from the information, especially with respect to selectional preferences, cf. Schulte im Walde (2000); Joanis (2002). This paper presents clustering experiments on German verbs which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepos</context>
<context position="27557" citStr="Walde (2000)" startWordPosition="4257" endWordPosition="4258">rb descriptions appear reliable with respect to their desired linguistic content. In addition, the subcategorisation information on D1 and D2 has been evaluated against manual definitions in a dictionary and proven useful (Schulte im Walde, 2002). And (ii) Table 4 illustrates that even with adding little information (e.g. refining a single argument by 15 selectional roles results in 186 instead of 171 features) linguistic intuition and clustering results do not necessarily align. Related work on automatic verb classes confirms the difficulty of selecting and encoding verb features. Schulte im Walde (2000) clusters 153 English verbs into 30 verb classes as taken from (Levin, 1993), using a hierarchical clustering method. The clustering is most successful when utilising syntactic subcategorisation frames enriched with PP information (comparable to our D2); selectional preferences are encoded by role combinations taken from WordNet. Schulte im Walde claims the detailed encoding and therefore sparse data to make the clustering worse with than without the selectional preference information. Merlo and Stevenson (2001) classify a smaller number of 60 English verbs into three verb classes, by utilisin</context>
</contexts>
<marker>Walde, 2000</marker>
<rawString>Sabine Schulte im Walde. 2000. Clustering Verbs Semantically According to their Alternation Behaviour. In Proceedings of the 18th International Conference on Computational Linguistics, pages 747-753, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Evaluating Verb Subcategorisation Frames learned by a German Statistical Grammar against Manual Definitions in the Duden Dictionary.</title>
<date>2002</date>
<booktitle>In Proceedings of the 10th EURALEX International Congress,</booktitle>
<pages>187--197</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="27191" citStr="Walde, 2002" startWordPosition="4201" endWordPosition="4202">ible basis. The various verb descriptions illustrate that step-wise refining the features does improve the clustering. But the linguistic feature refinements not necessarily align with expected changes in clustering. This effect could be due to (i) noisy or (ii) sparse data, but (i) the example distributions in Table 2 demonstrate that —even if noisy— our basic verb descriptions appear reliable with respect to their desired linguistic content. In addition, the subcategorisation information on D1 and D2 has been evaluated against manual definitions in a dictionary and proven useful (Schulte im Walde, 2002). And (ii) Table 4 illustrates that even with adding little information (e.g. refining a single argument by 15 selectional roles results in 186 instead of 171 features) linguistic intuition and clustering results do not necessarily align. Related work on automatic verb classes confirms the difficulty of selecting and encoding verb features. Schulte im Walde (2000) clusters 153 English verbs into 30 verb classes as taken from (Levin, 1993), using a hierarchical clustering method. The clustering is most successful when utilising syntactic subcategorisation frames enriched with PP information (co</context>
</contexts>
<marker>Walde, 2002</marker>
<rawString>Sabine Schulte im Walde. 2002. Evaluating Verb Subcategorisation Frames learned by a German Statistical Grammar against Manual Definitions in the Duden Dictionary. In Proceedings of the 10th EURALEX International Congress, pages 187-197, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schumacher</author>
</authors>
<date>1986</date>
<booktitle>Verben in Feldem. de Gruyter,</booktitle>
<location>Berlin.</location>
<contexts>
<context position="3194" citStr="Schumacher, 1986" startWordPosition="479" endWordPosition="480">ness of features which is driven by the idiosyncratic properties of the verbs and the verb classification. 2 German Verb Classes A set of 168 German verbs is manually classified into 43 concise semantic verb classes. The purpose of the manual classification is (i) to evaluate the reliability and performance of the clustering experiments on a preliminary set of verbs, and (ii) to explore the potential and limit to apply the clustering method to large-scale verb data. The German classes are closely related to the English pendant in (Levin, 1993) and agree with the German verb classification in (Schumacher, 1986) as far as the relevant verbs appear in his semantic &apos;fields&apos;. Table 1 presents the manual verb classification. The class size is between 2 and 7, with an average of 3.9 verbs per class. Eight verbs are am315 Aspect: anfangen, aufhoren, beenden, beginnen, enden Propositional Attitude: ahnen, denken, glauben, vermuten, wissen Desire: Wish: erhoffen, wollen, wiinschen Need: bean-fen, benotigen, brauchen Transfer of Possession (Obtaining): bekommen, erhalten, erlangen, kriegen Transfer of Possession (Giving): Gift: geben, leihen, schenken, spenden, stiften, vermachen, iiberschreiben Supply: bring</context>
<context position="24952" citStr="Schumacher (1986)" startWordPosition="3844" endWordPosition="3845"> with Position verbs showing more PP variation. The PP information is essential for successfully distinguishing these verb classes, and the coherence is partly destroyed by D3: Manner of Motion verbs (from the sub-classes 8-12) are captured well by clusters (b) and (c), since they inhibit strong common alternations, but cluster (a) merges the Existence, Position and Aspect verbs, since verbidiosyncratic demands on selectional roles destroy the D2 class demarcation. Admittedly, the verbs in cluster (a) are close in their semantics, with a common sense of (bringing into vs. being in) existence. Schumacher (1986) actually classifies most of the verbs into one existence class. lactfen fits into the cluster with its sense of `to function&apos;. 320 Cluster (d) contains most verbs of Quantum Change, together with one verb of Production and Constitution each. The semantics of the cluster is therefore rather pure. The verbs in the cluster typically subcategorise a direct object, alternating with a reflexive usage, `nr&apos; and `npr&apos; with mostly aufA and um. The selectional preferences help to distinguish this cluster: the verbs agree in demanding a thing or situation as subject, and various objects such as attribut</context>
</contexts>
<marker>Schumacher, 1986</marker>
<rawString>Helmut Schumacher. 1986. Verben in Feldem. de Gruyter, Berlin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>