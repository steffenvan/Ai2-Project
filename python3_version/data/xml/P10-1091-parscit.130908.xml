<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989059">
Estimating Strictly Piecewise Distributions
</title>
<author confidence="0.999145">
Jeffrey Heinz James Rogers
</author>
<affiliation confidence="0.999043">
University of Delaware Earlham College
</affiliation>
<address confidence="0.858765">
Newark, Delaware, USA Richmond, Indiana, USA
</address>
<email confidence="0.999569">
heinz@udel.edu jrogers@quark.cs.earlham.edu
</email>
<sectionHeader confidence="0.994811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999516818181818">
Strictly Piecewise (SP) languages are a
subclass of regular languages which en-
code certain kinds of long-distance de-
pendencies that are found in natural lan-
guages. Like the classes in the Chom-
sky and Subregular hierarchies, there are
many independently converging character-
izations of the SP class (Rogers et al., to
appear). Here we define SP distributions
and show that they can be efficiently esti-
mated from positive data.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959059701493">
Long-distance dependencies in natural language
are of considerable interest. Although much at-
tention has focused on long-distance dependencies
which are beyond the expressive power of models
with finitely many states (Chomsky, 1956; Joshi,
1985; Shieber, 1985; Kobele, 2006), there are
some long-distance dependencies in natural lan-
guage which permit finite-state characterizations.
For example, although it is well-known that vowel
and consonantal harmony applies across any ar-
bitrary number of intervening segments (Ringen,
1988; Bakovi´c, 2000; Hansson, 2001; Rose and
Walker, 2004) and that phonological patterns are
regular (Johnson, 1972; Kaplan and Kay, 1994),
it is less well-known that harmony patterns are
largely characterizable by the Strictly Piecewise
languages, a subregular class of languages with
independently-motivated, converging characteri-
zations (see Heinz (2007, to appear) and especially
Rogers et al. (2009)).
As shown by Rogers et al. (to appear), the
Strictly Piecewise (SP) languages, which make
distinctions on the basis of (potentially) discon-
tiguous subsequences, are precisely analogous to
the Strictly Local (SL) languages (McNaughton
and Papert, 1971; Rogers and Pullum, to appear),
which make distinctions on the basis of contigu-
ous subsequences. The Strictly Local languages
are the formal-language theoretic foundation for
n-gram models (Garcia et al., 1990), which are
widely used in natural language processing (NLP)
in part because such distributions can be estimated
from positive data (i.e. a corpus) (Jurafsky and
Martin, 2008). N-gram models describe prob-
ability distributions over all strings on the basis
of the Markov assumption (Markov, 1913): that
the probability of the next symbol only depends
on the previous contiguous sequence of length
n − 1. From the perspective of formal language
theory, these distributions are perhaps properly
called Strictly k-Local distributions (SLk) where
k = n. It is well-known that one limitation of the
Markov assumption is its inability to express any
kind of long-distance dependency.
This paper defines Strictly k-Piecewise (SPk)
distributions and shows how they too can be effi-
ciently estimated from positive data. In contrast
with the Markov assumption, our assumption is
that the probability of the next symbol is condi-
tioned on the previous set of discontiguous subse-
quences of length k − 1 in the string. While this
suggests the model has too many parameters (one
for each subset of all possible subsequences), in
fact the model has on the order of |E|k+1 parame-
ters because of an independence assumption: there
is no interaction between different subsequences.
As a result, SP distributions are efficiently com-
putable even though they condition the probabil-
ity of the next symbol on the occurrences of ear-
lier (possibly very distant) discontiguous subse-
quences. Essentially, these SP distributions reflect
a kind of long-term memory.
On the other hand, SP models have no short-
term memory and are unable to make distinctions
on the basis of contiguous subsequences. We do
not intend SP models to replace n-gram models,
but instead expect them to be used alongside of
</bodyText>
<page confidence="0.977779">
886
</page>
<note confidence="0.9434115">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 886–896,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988">
them. Exactly how this is to be done is beyond the
scope of this paper and is left for future research.
Since SP languages are the analogue of SL lan-
guages, which are the formal-language theoretical
foundation for n-gram models, which are widely
used in NLP, it is expected that SP distributions
and their estimation will also find wide applica-
tion. Apart from their interest to problems in the-
oretical phonology such as phonotactic learning
(Coleman and Pierrehumbert, 1997; Hayes and
Wilson, 2008; Heinz, to appear), it is expected that
their use will have application, in conjunction with
n-gram models, in areas that currently use them;
e.g. augmentative communication (Newell et al.,
1998), part of speech tagging (Brill, 1995), and
speech recognition (Jelenik, 1997).
§2 provides basic mathematical notation. §3
provides relevant background on the subregular hi-
erarchy. §4 describes automata-theoretic charac-
terizations of SP languages. §5 defines SP distri-
butions. §6 shows how these distributions can be
efficiently estimated from positive data and pro-
vides a demonstration. §7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.991795" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.980188483333334">
We start with some mostly standard notation. E
denotes a finite set of symbols and a string over
E is a finite sequence of symbols drawn from
that set. Ek, E&lt;k, Elk, and E* denote all
strings over this alphabet of length k, of length
less than or equal to k, of length greater than
or equal to k, and of any finite length, respec-
tively. E denotes the empty string. |w |denotes
the length of string w. The prefixes of a string
w are Pfx(w) = {v : lu E E* such that vu = w}.
When discussing partial functions, the notation T
and J indicates that the function is undefined, re-
spectively is defined, for particular arguments.
A language L is a subset of E*. A stochastic
language D is a probability distribution over E*.
The probability p of word w with respect to D is
written PrD(w) = p. Recall that all distributions
D must satisfy E.EE∗ PrD(w) = 1. If L is lan-
guage then PrD(L) = E.EL PrD(w).
A Deterministic Finite-state Automaton (DFA)
is a tuple M = (Q, E, q0, S, F) where Q is the
state set, E is the alphabet, q0 is the start state,
S is a deterministic transition function with do-
main Q x E and codomain Q, F is the set of
accepting states. Let d : Q x E* —* Q be
the (partial) path function of M, i.e., d(q, w)
is the (unique) state reachable from state q
via the sequence w, if any, or d(q, w)T other-
wise. The language recognized by a DFA M is
L(M) def = {w E E*  |�d(q0, w)J E F}.
A state is useful iff for all q E Q, there exists
w E E* such that S(q0, w) = q and there exists
w E E* such that S(q, w) E F. Useless states
are not useful. DFAs without useless states are
trimmed.
Two strings w and v over E are distinguished
by a DFA M iff �d(q0, w) =� �d(q0, v). They are
Nerode equivalent with respect to a language L
if and only if wu E L vu E L for
all u E E*. All DFAs which recognize L must
distinguish strings which are inequivalent in this
sense, but no DFA recognizing L necessarily dis-
tinguishes any strings which are equivalent. Hence
the number of equivalence classes of strings over
E modulo Nerode equivalence with respect to L
gives a (tight) lower bound on the number of states
required to recognize L.
A DFA is minimal if the size of its state set
is minimal among DFAs accepting the same lan-
guage. The product of n DFAs M1 ... Mn is
given by the standard construction over the state
space Q1 x ... x Qn (Hopcroft et al., 2001).
A Probabilistic Deterministic Finite-
state Automaton (PDFA) is a tuple
M = (Q, E, q0, S, F, T) where Q is the state
set, E is the alphabet, q0 is the start state, S is
a deterministic transition function, F and T are
the final-state and transition probabilities. In
particular, T : Q x E —* R+ and F : Q —* R+
such that
</bodyText>
<equation confidence="0.8399325">
for all q E Q, F(q) + 1: T(q,a) = 1. (1)
aEE
</equation>
<bodyText confidence="0.9949772">
Like DFAs, for all w E E*, there is at most one
state reachable from q0. PDFAs are typically rep-
resented as labeled directed graphs as in Figure 1.
A PDFA M generates a stochastic language
DM. If it exists, the (unique) path for a word w =
a0 ... ak belonging to E* through a PDFA is a
sequence ((q0, a0), (q1, a1), . . . , (qk, ak)), where
qZ+1 = S(qZ, aZ). The probability a PDFA assigns
to w is obtained by multiplying the transition prob-
abilities with the final probability along w’s path if
</bodyText>
<page confidence="0.990922">
887
</page>
<figure confidence="0.366735">
c:1/9 be the normalization term; and
</figure>
<figureCaption confidence="0.792830333333333">
Figure 1: A picture of a PDFA with states labeled
A and B. The probabilities of T and F are located
to the right of the colon.
</figureCaption>
<bodyText confidence="0.704066">
it exists, and zero otherwise.
</bodyText>
<equation confidence="0.867007">
�T (qi−1, ai−1) · F(qk+1) (2)
</equation>
<construct confidence="0.661677">
if �d(q0, w)I and 0 otherwise
</construct>
<bodyText confidence="0.868501315789474">
A probability distribution is regular deterministic
iff there is a PDFA which generates it.
The structural components of a PDFA M are
its states Q, its alphabet E, its transitions δ, and
its initial state q0. By structure of a PDFA, we
mean its structural components. Each PDFA M
defines a family of distributions given by the pos-
sible instantiations of T and F satisfying Equa-
tion 1. These distributions have JQJ· (JEJ + 1) in-
dependent parameters (since for each state there
are JEJ possible transitions plus the possibility of
finality.)
We define the product of PDFA in terms of co-
emission probabilities (Vidal et al., 2005a).
Definition 1 Let A be a vector of PDFAs and let
JAJ = n. For each 1 &lt; i &lt; n let Mi =
(Qi, E, q0i, δi, Fi, Ti) be the ith PDFA in A. The
probability that σ is co-emitted from q1, ... , qn in
Q1, . . . , Qn, respectively, is
</bodyText>
<equation confidence="0.991722222222222">
n
CT((σ,q1 ... qn)) = Ti(qi, σ).
i=1
Similarly, the probability that a word simultane-
ously ends at q1 E Q1 ... qn E Qn is
n
CF((q1 ... qn)) = Fi(qi).
i=1
Then ®A= (Q, E,q0,δ,F,T) where
</equation>
<listItem confidence="0.579178">
1. Q, q0, and δ are defined as with DFA product.
2. For all (q1 ... qn) E Q, let
</listItem>
<equation confidence="0.968737222222222">
Z((q1 ... qn)) =
CF((q1 ... qn)) + � CT((σ,q1 ... qn))
σEE
(a) let F((q1 ... qn)) = CF((q1 ... qn));
Z((q1 ... qn))
and
(b) for all σ E E, let
CT((σ, q1 ... qn))
T ((q1 ... qn), σ) = Z((ql ... qn))
</equation>
<bodyText confidence="0.999612">
In other words, the numerators of T and F are de-
fined to be the co-emission probabilities (Vidal et
al., 2005a), and division by Z ensures that M de-
fines a well-formed probability distribution. Sta-
tistically speaking, the co-emission product makes
an independence assumption: the probability of σ
being co-emitted from q1, ... , qn is exactly what
one expects if there is no interaction between the
individual factors; that is, between the probabil-
ities of σ being emitted from any qi. Also note
order of product is irrelevant up to renaming of
the states, and so therefore we also speak of tak-
ing the product of a set of PDFAs (as opposed to
an ordered vector).
Estimating regular deterministic distributions is
well-studied problem (Vidal et al., 2005a; Vidal et
al., 2005b; de la Higuera, in press). We limit dis-
cussion to cases when the structure of the PDFA is
known. Let S be a finite sample of words drawn
from a regular deterministic distribution D. The
problem is to estimate parameters T and F of M
so that DM approaches D. We employ the widely-
adopted maximum likelihood (ML) criterion for
this estimation.
</bodyText>
<equation confidence="0.922927">
F) = argmax rl PrM (w) (3)
T,F wES
</equation>
<bodyText confidence="0.999364647058823">
It is well-known that if D is generated by some
PDFA M′ with the same structural components as
M, then optimizing the ML estimate guarantees
that DM approaches D as the size of S goes to
infinity (Vidal et al., 2005a; Vidal et al., 2005b;
de la Higuera, in press).
The optimization problem (3) is simple for de-
terministic automata with known structural com-
ponents. Informally, the corpus is passed through
the PDFA, and the paths of each word through the
corpus are tracked to obtain counts, which are then
normalized by state. Let M = (Q, E, δ, q0, F, T)
be the PDFA whose parameters F and T are to be
estimated. For all states q E Q and symbols a E
E, The ML estimation of the probability of T (q, a)
is obtained by dividing the number of times this
transition is used in parsing the sample S by the
</bodyText>
<equation confidence="0.573743416666667">
b:2/9
a:2/9
B:4/9
c:3/10
a:3/10
A:2/10
b:2/10
k
PrDM(w) =
i=1
(
T�,
</equation>
<page confidence="0.943499">
888
</page>
<figureCaption confidence="0.977402">
Figure 2: The automata shows the counts
obtained by parsing M with sample
</figureCaption>
<figure confidence="0.8883755">
S = {ab, bba, E, cab, acb, cc}.
Reg MSO
</figure>
<figureCaption confidence="0.999759">
Figure 3: Parallel Sub-regular Hierarchies.
</figureCaption>
<bodyText confidence="0.999869636363636">
number of times state q is encountered in the pars-
ing of S. Similarly, the ML estimation of F(q) is
obtained by calculating the relative frequency of
state q being final with state q being encountered
in the parsing of S. For both cases, the division is
normalizing; i.e. it guarantees that there is a well-
formed probability distribution at each state. Fig-
ure 2 illustrates the counts obtained for a machine
M with sample S = {ab, bba, E, cab, acb, cc}.1
Figure 1 shows the PDFA obtained after normaliz-
ing these counts.
</bodyText>
<sectionHeader confidence="0.986469" genericHeader="method">
3 Subregular Hierarchies
</sectionHeader>
<bodyText confidence="0.999980435483871">
Within the class of regular languages there are
dual hierarchies of language classes (Figure 3),
one in which languages are defined in terms of
their contiguous substrings (up to some length k,
known as k-factors), starting with the languages
that are Locally Testable in the Strict Sense (SL),
and one in which languages are defined in terms
of their not necessarily contiguous subsequences,
starting with the languages that are Piecewise
1Technically, this acceptor is neither a simple DFA or
PDFA; rather, it has been called a Frequency DFA. We do
not formally define them here, see (de la Higuera, in press).
Testable in the Strict Sense (SP). Each language
class in these hierarchies has independently mo-
tivated, converging characterizations and each has
been claimed to correspond to specific, fundamen-
tal cognitive capabilities (McNaughton and Pa-
pert, 1971; Brzozowski and Simon, 1973; Simon,
1975; Thomas, 1982; Perrin and Pin, 1986; Garcia
and Ruiz, 1990; Beauquier and Pin, 1991; Straub-
ing, 1994; Garcia and Ruiz, 1996; Rogers and Pul-
lum, to appear; Kontorovich et al., 2008; Rogers et
al., to appear).
Languages in the weakest of these classes are
defined only in terms of the set of factors (SL)
or subsequences (SP) which are licensed to oc-
cur in the string (equivalently the complement of
that set with respect to E≤k, the forbidden fac-
tors or forbidden subsequences). For example, the
set containing the forbidden 2-factors {ab, ba} de-
fines a Strictly 2-Local language which includes
all strings except those with contiguous substrings
{ab, ba}. Similarly since the parameters of n-
gram models (Jurafsky and Martin, 2008) assign
probabilities to symbols given the preceding con-
tiguous substrings up to length n − 1, we say they
describe Strictly n-Local distributions.
These hierarchies have a very attractive model-
theoretic characterization. The Locally Testable
(LT) and Piecewise Testable languages are exactly
those that are definable by propositional formulae
in which the atomic formulae are blocks of sym-
bols interpreted factors (LT) or subsequences (PT)
of the string. The languages that are testable in the
strict sense (SL and SP) are exactly those that are
definable by formulae of this sort restricted to con-
junctions of negative literals. Going the other way,
the languages that are definable by First-Order for-
mulae with adjacency (successor) but not prece-
dence (less-than) are exactly the Locally Thresh-
old Testable (LTT) languages. The Star-Free lan-
guages are those that are First-Order definable
with precedence alone (adjacency being FO defin-
able from precedence). Finally, by extending to
Monadic Second-Order formulae (with either sig-
nature, since they are MSO definable from each
other), one obtains the full class of Regular lan-
guages (McNaughton and Papert, 1971; Thomas,
1982; Rogers and Pullum, to appear; Rogers et al.,
to appear).
The relation between strings which is funda-
mental along the Piecewise branch is the subse-
</bodyText>
<figure confidence="0.996052058823529">
c:3
b:2
b:2
a:2
A:2
a:3
B:4
c:1
LT
SP
SL
SF
FO
LTT
PT
Prop
+1 &lt;
</figure>
<page confidence="0.973512">
889
</page>
<bodyText confidence="0.927220833333333">
quence relation, which is a partial order on E∗:
w ⊑ v⇐⇒ w = ε or w = σ1 ··· σn and
def
(∃w0, ... , wn ∈ E∗)[v = w0σ1w1 ··· σnwn].
in which case we say w is a subsequence of v.
For w ∈ E∗, let
</bodyText>
<equation confidence="0.802037">
Pk(w) def = {v ∈ Ek  |v ⊑ w} and
P≤k(w) def = {v ∈ E≤k  |v ⊑ w},
</equation>
<bodyText confidence="0.9736285">
the set of subsequences of length k, respectively
length no greater than k, of w. Let Pk(L) and
P≤k(L) be the natural extensions of these to sets
of strings. Note that P0(w) = {ε}, for all w ∈ E∗,
that P1(w) is the set of symbols occurring in w and
that P≤k(L) is finite, for all L ⊆ E∗.
Similar to the Strictly Local languages, Strictly
Piecewise languages are defined only in terms of
the set of subsequences (up to some length k)
which are licensed to occur in the string.
Definition 2 (SPk Grammar, SP) A SPk gram-
mar is a pair G = hE, Gi where G ⊆ Ek. The
language licensed by a SPk grammar is
L(G) def= {w ∈ E∗  |P≤k(w) ⊆ P≤k(G)}.
A language is SPk iff it is L(G) for some SPk
grammar G. It is SP iff it is SPk for some k.
This paper is primarily concerned with estimat-
ing Strictly Piecewise distributions, but first we
examine in greater detail properties of SP lan-
guages, in particular DFA representations.
</bodyText>
<sectionHeader confidence="0.991389" genericHeader="method">
4 DFA representations of SP Languages
</sectionHeader>
<bodyText confidence="0.99378375">
Following Sakarovitch and Simon (1983),
Lothaire (1997) and Kontorovich, et al. (2008),
we call the set of strings that contain w as a
subsequence the principal shuffle ideal2 of w:
</bodyText>
<equation confidence="0.68643">
SI(w) = {v ∈ E∗  |w ⊑ v}.
The shuffle ideal of a set of strings is defined as
SI(S) = ∪w∈SSI(w)
</equation>
<bodyText confidence="0.85269">
Rogers et al. (to appear) establish that the SP lan-
guages have a variety of characteristic properties.
Theorem 1 The following are equivalent:3
</bodyText>
<footnote confidence="0.998158333333333">
2Properly SI(w) is the principal ideal generated by {w}
wrt the inverse of ⊑.
3For a complete proof, see Rogers et al. (to appear). We
only note that 5 implies 1 by DeMorgan’s theorem and the
fact that every shuffle ideal is finitely generated (see also
Lothaire (1997)).
</footnote>
<figureCaption confidence="0.997957">
Figure 4: The DFA representation of SI(aa).
</figureCaption>
<listItem confidence="0.992491285714286">
1. L = nw∈S[SI(w)], S finite,
2. L ∈ SP
3. (∃k)[P≤k(w) ⊆ P≤k(L) ⇒ w ∈ L],
4. w ∈ L and v ⊑ w ⇒ v ∈ L (L is subse-
quence closed),
5. L = SI(X), X ⊆ E∗ (L is the complement
of a shuffle ideal).
</listItem>
<bodyText confidence="0.903165857142857">
The DFA representation of the complement of a
shuffle ideal is especially important.
Lemma 1 Let w ∈ Ek, w = σ1 · · · σk,
and MSI(w) = hQ, E, q0, δ, Fi, where Q =
{i  |1 ≤ i ≤ k}, q0 = 1, F = Q and for all
qi ∈ Q,σ ∈ E:
δ(qi, σ) = { qi+1 if σ = σi and i &lt; k,
↑ if σ = σi and i = k,
qi otherwise.
Then M SI(w) is a minimal, trimmed DFA that rec-
ognizes the complement of SI(w), i.e., SI(w) =
L(MSI(w)).
Figure 4 illustrates the DFA representation of
the complement of SI(aa) with E = {a, b, c}. It is
easy to verify that the machine in Figure 4 accepts
all and only those words which do not contain an
aa subsequence.
For any SPk language L = L(hE, Gi) =6 E∗,
the first characterization (1) in Theorem 1 above
yields a non-deterministic finite-state representa-
tion of L, which is a set A of DFA representations
of complements of principal shuffle ideals of the
elements of G. The trimmed automata product of
this set yields a DFA, with the properties below
(Rogers et al., to appear).
Lemma 2 Let M be a trimmed DFA recognizing
a SPk language constructed as described above.
Then:
</bodyText>
<figure confidence="0.994774814814814">
1. All states of M are accepting states: F = Q.
c c
b
1
a
b
2
890
b
b
b
a
E,b
a E,a,b
E,a,b
b
b
a
E E,a
c
b a
c
c
c
c
E,a,c
b
E,b
b
c
c
b
a
a
b
c
E,c
b
b
E,a,b,c
b
b
a
E,b,c
a E,a,b,c
a
a
E E,a
E,b,c
c
c
c
b c b
a
</figure>
<figureCaption confidence="0.999707">
Figure 5: The DFA representation of the of the
</figureCaption>
<bodyText confidence="0.90309675">
SP language given by G = ({a, b, c}, {aa, bc}).
Names of the states reflect subsets of subse-
quences up to length 1 of prefixes of the language.
Note this DFA is trimmed, but not minimal.
</bodyText>
<figure confidence="0.698242">
E,c
E,a,c
</figure>
<figureCaption confidence="0.995582">
Figure 6: A DFA representation of the of the SP2
</figureCaption>
<bodyText confidence="0.93254425">
language given by G = ({a, b, c}, E2). Names
of the states reflect subsets of subsequences up to
length 1 of prefixes of the language. Note this
DFA is trimmed, but not minimal.
2. For all q1, q2 E Q and σ E E, if �d(q1, σ)↑
and �d(q1, w) = q2 for some w E E* then
�d(q2, σ)↑. (Missing edges propagate down.)
a
Figure 5 illustrates with the DFA representa-
tion of the of the SP2 language given by G =
({a, b, c}, {aa, bc}). It is straightforward to ver-
ify that this DFA is identical (modulo relabeling of
state names) to one obtained by the trimmed prod-
uct of the DFA representations of the complement
of the principal shuffle ideals of aa and bc, which
are the prohibited subsequences.
States in the DFA in Figure 5 correspond to the
subsequences up to length 1 of the prefixes of the
language. With this in mind, it follows that the
DFA of E* = L(E, Ek) has states which corre-
spond to the subsequences up to length k − 1 of
the prefixes of E*. Figure 6 illustrates such a DFA
when k = 2 and E = {a, b, c}.
In fact, these DFAs reveal the differences be-
tween SP languages and PT languages: they are
exactly those expressed in Lemma 2. Within the
state space defined by the subsequences up to
length k − 1 of the prefixes of the language, if the
conditions in Lemma 2 are violated, then the DFAs
describe languages that are PT but not SP. Pictori-
ally, PT2 languages are obtained by arbitrarily re-
moving arcs, states, and the finality of states from
the DFA in Figure 6, and SP2 ones are obtained by
non-arbitrarily removing them in accordance with
Lemma 2. The same applies straightforwardly for
any k (see Definition 3 below).
</bodyText>
<sectionHeader confidence="0.997161" genericHeader="method">
5 SP Distributions
</sectionHeader>
<bodyText confidence="0.995164714285714">
In the same way that SL distributions (n-gram
models) generalize SL languages, SP distributions
generalize SP languages. Recall that SP languages
are characterizable by the intersection of the com-
plements of principal shuffle ideals. SP distribu-
tions are similarly characterized.
We begin with Piecewise-Testable distributions.
</bodyText>
<figure confidence="0.713389">
Definition 3 A distribution D is k-Piecewise
def
Testable (written D E PTDk)⇐⇒ D can be de-
scribed by a PDFA M = (Q, E, q0, δ, F, T) with
</figure>
<listItem confidence="0.7094452">
1. Q = {P&lt;k−1(w) : w E E*}
2. q0 = P&lt;k−1(E)
3. For all w E E* and all σ E E,
δ(P&lt;k−1(w),a) = P&lt;k−1(wa)
4. F and T satisfy Equation 1.
</listItem>
<bodyText confidence="0.999816428571428">
In other words, a distribution is k-Piecewise
Testable provided it can be represented by a PDFA
whose structural components are the same (mod-
ulo renaming of states) as those of the DFA dis-
cussed earlier where states corresponded to the
subsequences up to length k − 1 of the prefixes
of the language. The DFA in Figure 6 shows the
</bodyText>
<page confidence="0.996548">
891
</page>
<bodyText confidence="0.997675">
structure of a PDFA which describes a PT2 distri-
bution as long as the assigned probabilities satisfy
Equation 1.
The following lemma follows directly from the
finite-state representation of PTk distributions.
Lemma 3 Let D belong to PTDk and let M =
hQ, E, q0, δ, F, Ti be a PDFA representing D de-
fined according to Definition 3.
</bodyText>
<equation confidence="0.99792225">
PrD(σ1 ... σn) = T(P≤k−1(ǫ), σ1) ·
(n T (P≤k−1(σ1 ... σi−1), σi) �(4)
2≤i≤n
· F(P≤k−1(w))
</equation>
<bodyText confidence="0.93678675">
PTk distributions have 2|Σ|k−1(|E|+1) parameters
(since there are 2|Σ|k−1 states and |E |+ 1 possible
events, i.e. transitions and finality).
Let Pr(σ  |#) and Pr(#  |P≤k(w)) denote
the probability (according to some D ∈ PTDk)
that a word begins with σ and ends after observ-
ing P≤k(w). Then Equation 4 can be rewritten in
terms of conditional probability as
</bodyText>
<equation confidence="0.9988745">
PrD(σ1 ... σn) = Pr(σ1  |#)
(n Pr(σi  |P≤k−1(σ1 ... σi−1)) (5)
2≤i≤n
· Pr(#  |P≤k−1(w))
</equation>
<bodyText confidence="0.999878333333333">
Thus, the probability assigned to a word depends
not on the observed contiguous sequences as in a
Markov model, but on observed subsequences.
Like SP languages, SP distributions can be de-
fined in terms of the product of machines very sim-
ilar to the complement of principal shuffle ideals.
</bodyText>
<equation confidence="0.705888714285714">
Definition 4 Let w ∈ Ek−1 and w = σ1 · · · σk−1.
Mw = hQ, E, q0, δ, F, Ti is a w-subsequence-
distinguishing PDFA (w-SD-PDFA) iff
Q = Pfx(w), q0 = ǫ, for all u ∈ Pfx(w)
and each σ ∈ E,
δ(u, σ) = uσ iff uσ ∈ Pfx(w) and
u otherwise
</equation>
<bodyText confidence="0.79729">
and F and T satisfy Equation 1.
</bodyText>
<figureCaption confidence="0.941418888888889">
Figure 7 shows the structure of Ma which is
almost the same as the complement of the princi-
pal shuffle ideal in Figure 4. The only difference
is the additional self-loop labeled a on the right-
most state labeled a. Ma defines a family of dis-
tributions over E∗, and its states distinguish those
Figure 7: The structure of PDFA Ma. It is the
same (modulo state names) as the DFA in Figure 4
except for the self-loop labeled a on state a.
</figureCaption>
<bodyText confidence="0.999642944444445">
strings which contain a (state a) from those that
do not (state ǫ). A set of PDFAs is a k-set of SD-
PDFAs iff, for each w ∈ E≤k−1, it contains ex-
actly one w-SD-PDFA.
In the same way that missing edges propagate
down in DFA representations of SP languages
(Lemma 2), the final and transitional probabili-
ties must propagate down in PDFA representa-
tions of SPk distributions. In other words, the fi-
nal and transitional probabilities at states further
along paths beginning at the start state must be de-
termined by final and transitional probabilities at
earlier states non-increasingly. This is captured by
defining SP distributions as a product of k-sets of
SD-PDFAs (see Definition 5 below).
While the standard product based on co-
emission probability could be used for this pur-
pose, we adopt a modified version of it defined
for k-sets of SD-PDFAs: the positive co-emission
probability. The automata product based on the
positive co-emission probability not only ensures
that the probabilities propagate as necessary, but
also that such probabilities are made on the ba-
sis of observed subsequences, and not unobserved
ones. This idea is familiar from n-gram models:
the probability of σn given the immediately pre-
ceding sequence σ1 ... σn−1 does not depend on
the probability of σn given the other (n − 1)-long
sequences which do not immediately precede it,
though this is a logical possibility.
Let A be a k-set of SD-PDFAs. For each
w ∈ E≤k−1, let Mw = hQw, E, q0w, δw, Fw, Twi
be the w-subsequence-distinguishing PDFA in A.
The positive co-emission probability that σ is si-
multaneously emitted from states qǫ, ... , qu from
the statesets Qǫ,... Qu, respectively, of each SD-
</bodyText>
<figure confidence="0.996747">
c
b
b
a
a
ǫ
a
c
</figure>
<page confidence="0.964018">
892
</page>
<bodyText confidence="0.535127714285714">
PDFA in A is Tw(qw,σ) (6) Θ(|Σ|k+1). Furthermore, since each SD-PDFA
� only has one state contributing |Σ|+1 probabilities
PCT(hσ,qǫ ... qui) = to the product, and since there are |Σ&lt;k |= |E|k−1
qwE(qǫ...qu) |E|−1
qw=w many SD-PDFAs in a k-set, there are
Similarly, the probability that a word simultane-
ously ends at n states qǫ ∈ Qǫ, ... , qu ∈ Qu is
</bodyText>
<equation confidence="0.940389333333333">
PCF(hqǫ ... qui) = � Fw(qw) (7)
qwE(qǫ...qu)
qw=w
</equation>
<bodyText confidence="0.84246875">
In other words, the positive co-emission proba-
bility is the product of the probabilities restricted
to those assigned to the maximal states in each
Mw. For example, consider a 2-set of SD-
PDFAs A with Σ = {a, b, c}. A contains four
PDFAs Mǫ, Ma, Mb, Mc. Consider state q =
hǫ, ǫ, b, ci ∈ ® A (this is the state labeled ǫ, b, c in
Figure 6). Then
</bodyText>
<equation confidence="0.9750355">
CT(a, q) = Tǫ(ǫ, a)· Ta(ǫ, a)· Tb(b, a)· Tc(c, a)
|Σ|k − 1
|Σ |− 1 · (|Σ |+ 1) = |Σ|k+1 + |Σ|k − |Σ |− 1
|Σ |− 1
</equation>
<bodyText confidence="0.9515666">
parameters, which is Θ(|Σ|k).
Lemma 5 Let D ∈ SPDk. Then D ∈ PTDk.
Proof Since D ∈ SPDk, there is a k-set of
subsequence-distinguishing PDFAs. The product
of this set has the same structure as the PDFA
given in Definition 3. ⊣
Theorem 2 A distribution D ∈ SPDk if D can
be described by a PDFA M = hQ, Σ, q0, δ, F, Ti
satisfying Definition 3 and the following.
For all w ∈ Σ* and all σ ∈ Σ, let
</bodyText>
<equation confidence="0.984374">
but Z(w) = � F(P&lt;k−1(s)) +
PCT(a, q) = Tǫ(ǫ, a)· Tb(b, a)· Tc(c, a)
</equation>
<bodyText confidence="0.949067615384615">
since in PDFA Ma, the state ǫ is not the maximal
state.
The positive co-emission product (⊗+) is de-
fined just as with co-emission probabilities, sub-
stituting PCT and PCF for CT and CF, respec-
tively, in Definition 1. The definition of ⊗+ en-
sures that the probabilities propagate on the basis
of observed subsequences, and not on the basis of
unobserved ones.
Lemma 4 Let k ≥ 1 and let A be a k-set of SD-
PDFAs. Then ⊗+S defines a well-formed proba-
bility distribution over Σ*.
Proof Since Mǫ belongs to A, it is always
the case that PCT and PCF are defined. Well-
formedness follows from the normalization term
as in Definition 1. ⊣
Definition 5 A distribution D is k-Strictly Piece-
def
wise (written D ∈ SPDk)⇐⇒ D can be described
by a PDFA which is the positive co-emission
product of a k-set of subsequence-distinguishing
PDFAs.
By Lemma 4, SP distributions are well-formed.
Unlike PDFAs for PT distributions, which distin-
guish 2|E|k−1 states, the number of states in a k-
set of SD-PDFAs is K&lt;k(i + 1)|Σ|i, which is
</bodyText>
<equation confidence="0.972722777777778">
sEP&lt;k−1(w)
(n T (P&lt;k−1(s), σ&amp;quot;) �(8)
sEP&lt;k−1(w)
(This is the normalization term.) Then T must sat-
isfy: T (P&lt;k−1(w), σ) =
Z(w)
and F must satisfy: F(P&lt;k−1(w)) =
H
sEP&lt;k−1(w) F(P&lt;k−1(s))
</equation>
<bodyText confidence="0.965157">
Proof That SPDk satisfies Definition 3 Follows
directly from Lemma 5. Equations 8-10 follow
from the definition of positive co-emission proba-
bility. ⊣
The way in which final and transitional proba-
bilities propagate down in SP distributions is re-
flected in the conditional probability as defined by
Equations 9 and 10. In terms of conditional prob-
ability, Equations 9 and 10 mean that the prob-
ability that σi follows a sequence σ1 . . . σi−1 is
not only a function of P&lt;k−1(σ1 . . . σi−1) (Equa-
tion 4) but further that it is a function of each
subsequence in σ1 . . . σi−1 up to length k − 1.
</bodyText>
<equation confidence="0.5085758">
�
σ′EE
H
sEP&lt;k−1(w) T (P&lt;k−1(s), σ)
Z(w)
</equation>
<page confidence="0.994451">
893
</page>
<bodyText confidence="0.990468666666667">
In particular, Pr(σi  |P&lt;_k−1(σ1 ... σi−1)) is ob-
tained by substituting Pr(σi  |P&lt;_ k−1(s)) for
T(P&lt;_ k−1(s),σ) and Pr(#  |P&lt;_ k−1(s)) for
F(P&lt;_k−1(s)) in Equations 8, 9 and 10. For ex-
ample, for a SP2 distribution, the probability of
a given P&lt;_1(bc) (state ǫ, b, c in Figure 6) is the
normalized product of the probabilities of a given
P&lt;_1(ǫ), a given P&lt;_1(b), and a given P&lt;_1(c).
To summarize, SP and PT distributions are reg-
ular deterministic. Unlike PT distributions, how-
ever, SP distributions can be modeled with only
O(|E|k) parameters and O(|E|k+1) states. This
is true even though SP distributions distinguish
2 Σ k−1 states! Since SP distributions can be rep-
resented by a single PDFA, computing Pr(w) oc-
curs in only O(|w|) for such PDFA. While such
PDFA might be too large to be practical, Pr(w)
can also be computed from the k-set of SD-PDFAs
in O(|w|k) (essentially building the path in the
product machine on the fly using Equations 4, 8, 9
and 10).
</bodyText>
<sectionHeader confidence="0.996322" genericHeader="method">
6 Estimating SP Distributions
</sectionHeader>
<bodyText confidence="0.998730673076924">
The problem of ML estimation of SPk distribu-
tions is reduced to estimating the parameters of the
SD-PDFAs. Training (counting and normaliza-
tion) occurs over each of these machines (i.e. each
machine parses the entire corpus), which gives the
ML estimates of the parameters of the distribution.
It trivially follows that this training successfully
estimates any D E SPDk.
Theorem 3 For any D E SPDk, let D generate
sample S. Let A be the k-set of SD-PDFAs which
describes exactly D. Then optimizing the MLE of
S with respect to each M E Aguarantees that the
distribution described by the positive co-emission
product of ®+ A approaches D as |S |increases.
Proof The MLE estimate of S with respect to
SPDk returns the parameter values that maximize
the likelihood of S. The parameters of D E SPDk
are found on the maximal states of each M E A.
By definition, each M E A describes a proba-
bility distribution over E*, and similarly defines
a family of distributions. Therefore finding the
MLE of S with respect to SPDk means finding the
MLE estimate of S with respect to each of the fam-
ily of distributions which each M E A defines,
respectively.
Optimizing the ML estimate of S for each
M E A means that as |S |increases, the estimates
TM and FM approach the true values TM and
FM. It follows that as |S |increases, T®+ A and
F®+ A approach the true values of T®+ A and
F®+ A and consequently D®+ A approaches D. ⊣
We demonstrate learning long-distance depen-
dencies by estimating SP2 distributions given a
corpus from Samala (Chumash), a language with
sibilant harmony.4 There are two classes of sibi-
lants in Samala: [-anterior] sibilants like [s] and
[&gt;ts] and [+anterior] sibilants like [S] and [&gt;tS].5
Samala words are subject to a phonological pro-
cess wherein the last sibilant requires earlier sibi-
lants to have the same value for the feature [an-
terior], no matter how many sounds intervene
(Applegate, 1972). As a consequence of this
rule, there are generally no words in Samala
where [-anterior] sibilants follow [+anterior]. E.g.
[ftojonowonowaf] ‘it stood upright’ (Applegate
1972:72) is licit but not *[ftojonowonowas].
The results of estimating D E SPD2 with
the corpus is shown in Table 6. The results
clearly demonstrate the effectiveness of the model:
the probability of a [α anterior] sibilant given
P&lt;_1([-α anterior]) sounds is orders of magnitude
less than given P&lt;_1(α anterior]) sounds.
</bodyText>
<table confidence="0.997205">
Pr(x I P≤1(y)) x
&gt; &gt;
s ts S tS
s 0.0335 0.0051 0.0011 0.0002
⁀ts 0.0218 0.0113 0.0009 0.
y S 0.0009 0. 0.0671 0.0353
&gt;tS 0.0006 0. 0.0455 0.0313
</table>
<tableCaption confidence="0.99775">
Table 1: Results of SP2 estimation on the Samala
corpus. Only sibilants are shown.
</tableCaption>
<sectionHeader confidence="0.996652" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999940666666667">
SP distributions are the stochastic version of SP
languages, which model long-distance dependen-
cies. Although SP distributions distinguish 2 Σ k−1
states, they do so with tractably many parameters
and states because of an assumption that distinct
subsequences do not interact. As shown, these
distributions are efficiently estimable from posi-
tive data. As previously mentioned, we anticipate
these models to find wide application in NLP.
</bodyText>
<footnote confidence="0.990292">
4The corpus was kindly provided by Dr. Richard Apple-
gate and drawn from his 2007 dictionary of Samala.
5Samala actually contrasts glottalized, aspirated, and
plain variants of these sounds (Applegate, 1972). These la-
ryngeal distinctions are collapsed here for easier exposition.
</footnote>
<page confidence="0.982379">
894
</page>
<note confidence="0.861259375">
Jeffrey Heinz. 2007. The Inductive Learning of
Phonotactic Patterns. Ph.D. thesis, University of
California, Los Angeles.
References
R.B. Applegate. 1972. Inese˜no Chumash Grammar.
Ph.D. thesis, University of California, Berkeley.
R.B. Applegate. 2007. Samala-English dictionary: a
guide to the Samala language of the Inese˜no Chu-
</note>
<reference confidence="0.99542470212766">
mash People. Santa Ynez Band of Chumash Indi-
ans.
Eric Bakovi´c. 2000. Harmony, Dominance and Con-
trol. Ph.D. thesis, Rutgers University.
D. Beauquier and Jean-Eric Pin. 1991. Languages and
scanners. Theoretical Computer Science, 84:3–21.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543–566.
J. A. Brzozowski and Imre Simon. 1973. Character-
izations of locally testable events. Discrete Mathe-
matics, 4:243–271.
Noam Chomsky. 1956. Three models for the descrip-
tion of language. IRE Transactions on Information
Theory. IT-2.
J. S. Coleman and J. Pierrehumbert. 1997. Stochastic
phonological grammars and acceptability. In Com-
putational Phonology, pages 49–56. Somerset, NJ:
Association for Computational Linguistics. Third
Meeting of the ACL Special Interest Group in Com-
putational Phonology.
Colin de la Higuera. in press. Grammatical Infer-
ence: Learning Automata and Grammars. Cam-
bridge University Press.
Pedro Garcia and Jos´e Ruiz. 1990. Inference of k-
testable languages in the strict sense and applica-
tions to syntactic pattern recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 9:920–925.
Pedro Garcia and Jos´e Ruiz. 1996. Learning k-
piecewise testable languages from positive data. In
Laurent Miclet and Colin de la Higuera, editors,
Grammatical Interference: Learning Syntax from
Sentences, volume 1147 of Lecture Notes in Com-
puter Science, pages 203–210. Springer.
Pedro Garcia, Enrique Vidal, and Jos´e Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325–338.
Gunnar Hansson. 2001. Theoretical and typological
issues inconsonant harmony. Ph.D. thesis, Univer-
sity of California, Berkeley.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379–440.
Jeffrey Heinz. to appear. Learning long distance
phonotactics. Linguistic Inquiry.
John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
2001. Introduction to Automata Theory, Languages,
and Computation. Addison-Wesley.
Frederick Jelenik. 1997. Statistical Methods for
Speech Recognition. MIT Press.
C. Douglas Johnson. 1972. Formal Aspects of Phono-
logical Description. The Hague: Mouton.
A. K. Joshi. 1985. Tree-adjoining grammars: How
much context sensitivity is required to provide rea-
sonable structural descriptions? In D. Dowty,
L. Karttunen, and A. Zwicky, editors, Natural Lan-
guage Parsing, pages 206–250. Cambridge Univer-
sity Press.
Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, 2nd edi-
tion.
Ronald Kaplan and Martin Kay. 1994. Regular models
of phonological rule systems. Computational Lin-
guistics, 20(3):331–378.
Gregory Kobele. 2006. Generating Copies: An In-
vestigation into Structural Identity in Language and
Grammar. Ph.D. thesis, University of California,
Los Angeles.
Leonid (Aryeh) Kontorovich, Corinna Cortes, and
Mehryar Mohri. 2008. Kernel methods for learn-
ing languages. Theoretical Computer Science,
405(3):223 – 236. Algorithmic Learning Theory.
M. Lothaire, editor. 1997. Combinatorics on Words.
Cambridge University Press, Cambridge, UK, New
York.
A. A. Markov. 1913. An example of statistical study
on the text of ‘eugene onegin’ illustrating the linking
of events to a chain.
Robert McNaughton and Simon Papert. 1971.
Counter-Free Automata. MIT Press.
A. Newell, S. Langer, and M. Hickey. 1998. The
rˆole of natural language processing in alternative and
augmentative communication. Natural Language
Engineering, 4(1):1–16.
Dominique Perrin and Jean-Eric Pin. 1986. First-
Order logic and Star-Free sets. Journal of Computer
and System Sciences, 32:393–406.
Catherine Ringen. 1988. Vowel Harmony: Theoretical
Implications. Garland Publishing, Inc.
</reference>
<page confidence="0.987707">
895
</page>
<reference confidence="0.999559025">
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.
James Rogers, Jeffrey Heinz, Matt Edlefsen, Dylan
Leeman, Nathan Myers, Nathaniel Smith, Molly
Visscher, and David Wellcome. to appear. On lan-
guages piecewise testable in the strict sense. In Pro-
ceedings of the 11th Meeting of the Assocation for
Mathematics ofLanguage.
Sharon Rose and Rachel Walker. 2004. A typology of
consonant agreement as correspondence. Language,
80(3):475–531.
Jacques Sakarovitch and Imre Simon. 1983. Sub-
words. In M. Lothaire, editor, Combinatorics on
Words, volume 17 of Encyclopedia of Mathemat-
ics and Its Applications, chapter 6, pages 105–134.
Addison-Wesley, Reading, Massachusetts.
Stuart Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Phi-
losophy, 8:333–343.
Imre Simon. 1975. Piecewise testable events. In
Automata Theory and Formal Languages: 2nd
Grammatical Inference conference, pages 214–222,
Berlin; New York. Springer-Verlag.
Howard Straubing. 1994. Finite Automata, Formal
Logic and Circuit Complexity. Birkh¨auser.
Wolfgang Thomas. 1982. Classifying regular events in
symbolic logic. Journal of Computer and Systems
Sciences, 25:360–376.
Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005a. Probabilistic finite-state machines-part I.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(7):1013–1025.
Enrique Vidal, Frank Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005b. Probabilistic finite-state machines-part II.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(7):1026–1039.
</reference>
<page confidence="0.999002">
896
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922161">
<title confidence="0.999902">Estimating Strictly Piecewise Distributions</title>
<author confidence="0.999908">Jeffrey Heinz James Rogers</author>
<affiliation confidence="0.999974">University of Delaware Earlham College</affiliation>
<address confidence="0.981125">Newark, Delaware, USA Richmond, Indiana, USA</address>
<email confidence="0.999772">heinz@udel.edujrogers@quark.cs.earlham.edu</email>
<abstract confidence="0.994912">Strictly Piecewise (SP) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>mash People</author>
</authors>
<title>Santa Ynez Band of Chumash Indians.</title>
<marker>People, </marker>
<rawString>mash People. Santa Ynez Band of Chumash Indians.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bakovi´c</author>
</authors>
<title>Harmony, Dominance and Control.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Rutgers University.</institution>
<marker>Bakovi´c, 2000</marker>
<rawString>Eric Bakovi´c. 2000. Harmony, Dominance and Control. Ph.D. thesis, Rutgers University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beauquier</author>
<author>Jean-Eric Pin</author>
</authors>
<title>Languages and scanners.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>84--3</pages>
<contexts>
<context position="13630" citStr="Beauquier and Pin, 1991" startWordPosition="2391" endWordPosition="2394">ly contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Similarly since the paramet</context>
</contexts>
<marker>Beauquier, Pin, 1991</marker>
<rawString>D. Beauquier and Jean-Eric Pin. 1991. Languages and scanners. Theoretical Computer Science, 84:3–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4732" citStr="Brill, 1995" startWordPosition="717" endWordPosition="718">es are the analogue of SL languages, which are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application. Apart from their interest to problems in theoretical phonology such as phonotactic learning (Coleman and Pierrehumbert, 1997; Hayes and Wilson, 2008; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (Newell et al., 1998), part of speech tagging (Brill, 1995), and speech recognition (Jelenik, 1997). §2 provides basic mathematical notation. §3 provides relevant background on the subregular hierarchy. §4 describes automata-theoretic characterizations of SP languages. §5 defines SP distributions. §6 shows how these distributions can be efficiently estimated from positive data and provides a demonstration. §7 concludes the paper. 2 Preliminaries We start with some mostly standard notation. E denotes a finite set of symbols and a string over E is a finite sequence of symbols drawn from that set. Ek, E&lt;k, Elk, and E* denote all strings over this alphabe</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Brzozowski</author>
<author>Imre Simon</author>
</authors>
<title>Characterizations of locally testable events.</title>
<date>1973</date>
<booktitle>Discrete Mathematics,</booktitle>
<pages>4--243</pages>
<contexts>
<context position="13533" citStr="Brzozowski and Simon, 1973" startWordPosition="2375" endWordPosition="2378">ble in the Strict Sense (SL), and one in which languages are defined in terms of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which i</context>
</contexts>
<marker>Brzozowski, Simon, 1973</marker>
<rawString>J. A. Brzozowski and Imre Simon. 1973. Characterizations of locally testable events. Discrete Mathematics, 4:243–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Three models for the description of language.</title>
<date>1956</date>
<journal>IRE Transactions on Information Theory.</journal>
<pages>2</pages>
<contexts>
<context position="880" citStr="Chomsky, 1956" startWordPosition="123" endWordPosition="124"> languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-mot</context>
</contexts>
<marker>Chomsky, 1956</marker>
<rawString>Noam Chomsky. 1956. Three models for the description of language. IRE Transactions on Information Theory. IT-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Coleman</author>
<author>J Pierrehumbert</author>
</authors>
<title>Stochastic phonological grammars and acceptability.</title>
<date>1997</date>
<journal>Computational Linguistics. Third Meeting of the ACL Special Interest Group in Computational Phonology.</journal>
<booktitle>In Computational Phonology,</booktitle>
<pages>49--56</pages>
<publisher>Association for</publisher>
<location>Somerset, NJ:</location>
<contexts>
<context position="4474" citStr="Coleman and Pierrehumbert, 1997" startWordPosition="675" endWordPosition="678"> Meeting of the Association for Computational Linguistics, pages 886–896, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics them. Exactly how this is to be done is beyond the scope of this paper and is left for future research. Since SP languages are the analogue of SL languages, which are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application. Apart from their interest to problems in theoretical phonology such as phonotactic learning (Coleman and Pierrehumbert, 1997; Hayes and Wilson, 2008; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (Newell et al., 1998), part of speech tagging (Brill, 1995), and speech recognition (Jelenik, 1997). §2 provides basic mathematical notation. §3 provides relevant background on the subregular hierarchy. §4 describes automata-theoretic characterizations of SP languages. §5 defines SP distributions. §6 shows how these distributions can be efficiently estimated from positive data and provides a demons</context>
</contexts>
<marker>Coleman, Pierrehumbert, 1997</marker>
<rawString>J. S. Coleman and J. Pierrehumbert. 1997. Stochastic phonological grammars and acceptability. In Computational Phonology, pages 49–56. Somerset, NJ: Association for Computational Linguistics. Third Meeting of the ACL Special Interest Group in Computational Phonology.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Colin de</author>
</authors>
<title>la Higuera. in press. Grammatical Inference: Learning Automata and Grammars.</title>
<publisher>Cambridge University Press.</publisher>
<marker>de, </marker>
<rawString>Colin de la Higuera. in press. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Jos´e Ruiz</author>
</authors>
<title>Inference of ktestable languages in the strict sense and applications to syntactic pattern recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>9--920</pages>
<contexts>
<context position="13605" citStr="Garcia and Ruiz, 1990" startWordPosition="2387" endWordPosition="2390"> of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Si</context>
</contexts>
<marker>Garcia, Ruiz, 1990</marker>
<rawString>Pedro Garcia and Jos´e Ruiz. 1990. Inference of ktestable languages in the strict sense and applications to syntactic pattern recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9:920–925.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Jos´e Ruiz</author>
</authors>
<title>Learning kpiecewise testable languages from positive data.</title>
<date>1996</date>
<booktitle>In Laurent Miclet and Colin de la Higuera, editors, Grammatical Interference: Learning Syntax from Sentences,</booktitle>
<volume>1147</volume>
<pages>203--210</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13670" citStr="Garcia and Ruiz, 1996" startWordPosition="2398" endWordPosition="2401">the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Similarly since the parameters of ngram models (Jurafsky and Martin</context>
</contexts>
<marker>Garcia, Ruiz, 1996</marker>
<rawString>Pedro Garcia and Jos´e Ruiz. 1996. Learning kpiecewise testable languages from positive data. In Laurent Miclet and Colin de la Higuera, editors, Grammatical Interference: Learning Syntax from Sentences, volume 1147 of Lecture Notes in Computer Science, pages 203–210. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Enrique Vidal</author>
<author>Jos´e Oncina</author>
</authors>
<title>Learning locally testable languages in the strict sense.</title>
<date>1990</date>
<booktitle>In Proceedings of the Workshop on Algorithmic Learning Theory,</booktitle>
<pages>325--338</pages>
<contexts>
<context position="2044" citStr="Garcia et al., 1990" startWordPosition="286" endWordPosition="289">, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on the basis of contiguous subsequences. The Strictly Local languages are the formal-language theoretic foundation for n-gram models (Garcia et al., 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated from positive data (i.e. a corpus) (Jurafsky and Martin, 2008). N-gram models describe probability distributions over all strings on the basis of the Markov assumption (Markov, 1913): that the probability of the next symbol only depends on the previous contiguous sequence of length n − 1. From the perspective of formal language theory, these distributions are perhaps properly called Strictly k-Local distributions (SLk) where k = n. It is well-known that one limitation of the Markov </context>
</contexts>
<marker>Garcia, Vidal, Oncina, 1990</marker>
<rawString>Pedro Garcia, Enrique Vidal, and Jos´e Oncina. 1990. Learning locally testable languages in the strict sense. In Proceedings of the Workshop on Algorithmic Learning Theory, pages 325–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunnar Hansson</author>
</authors>
<title>Theoretical and typological issues inconsonant harmony.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="1210" citStr="Hansson, 2001" startWordPosition="168" endWordPosition="169">iently estimated from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNa</context>
</contexts>
<marker>Hansson, 2001</marker>
<rawString>Gunnar Hansson. 2001. Theoretical and typological issues inconsonant harmony. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
<author>Colin Wilson</author>
</authors>
<title>A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry,</title>
<date>2008</date>
<pages>39--379</pages>
<contexts>
<context position="4498" citStr="Hayes and Wilson, 2008" startWordPosition="679" endWordPosition="682">omputational Linguistics, pages 886–896, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics them. Exactly how this is to be done is beyond the scope of this paper and is left for future research. Since SP languages are the analogue of SL languages, which are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application. Apart from their interest to problems in theoretical phonology such as phonotactic learning (Coleman and Pierrehumbert, 1997; Hayes and Wilson, 2008; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (Newell et al., 1998), part of speech tagging (Brill, 1995), and speech recognition (Jelenik, 1997). §2 provides basic mathematical notation. §3 provides relevant background on the subregular hierarchy. §4 describes automata-theoretic characterizations of SP languages. §5 defines SP distributions. §6 shows how these distributions can be efficiently estimated from positive data and provides a demonstration. §7 concludes th</context>
</contexts>
<marker>Hayes, Wilson, 2008</marker>
<rawString>Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39:379–440.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeffrey Heinz</author>
</authors>
<title>to appear. Learning long distance phonotactics. Linguistic Inquiry.</title>
<marker>Heinz, </marker>
<rawString>Jeffrey Heinz. to appear. Learning long distance phonotactics. Linguistic Inquiry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hopcroft</author>
<author>Rajeev Motwani</author>
<author>Jeffrey Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>2001</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="7475" citStr="Hopcroft et al., 2001" startWordPosition="1243" endWordPosition="1246"> and only if wu E L vu E L for all u E E*. All DFAs which recognize L must distinguish strings which are inequivalent in this sense, but no DFA recognizing L necessarily distinguishes any strings which are equivalent. Hence the number of equivalence classes of strings over E modulo Nerode equivalence with respect to L gives a (tight) lower bound on the number of states required to recognize L. A DFA is minimal if the size of its state set is minimal among DFAs accepting the same language. The product of n DFAs M1 ... Mn is given by the standard construction over the state space Q1 x ... x Qn (Hopcroft et al., 2001). A Probabilistic Deterministic Finitestate Automaton (PDFA) is a tuple M = (Q, E, q0, S, F, T) where Q is the state set, E is the alphabet, q0 is the start state, S is a deterministic transition function, F and T are the final-state and transition probabilities. In particular, T : Q x E —* R+ and F : Q —* R+ such that for all q E Q, F(q) + 1: T(q,a) = 1. (1) aEE Like DFAs, for all w E E*, there is at most one state reachable from q0. PDFAs are typically represented as labeled directed graphs as in Figure 1. A PDFA M generates a stochastic language DM. If it exists, the (unique) path for a wor</context>
</contexts>
<marker>Hopcroft, Motwani, Ullman, 2001</marker>
<rawString>John Hopcroft, Rajeev Motwani, and Jeffrey Ullman. 2001. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelenik</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4772" citStr="Jelenik, 1997" startWordPosition="722" endWordPosition="723">hich are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application. Apart from their interest to problems in theoretical phonology such as phonotactic learning (Coleman and Pierrehumbert, 1997; Hayes and Wilson, 2008; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (Newell et al., 1998), part of speech tagging (Brill, 1995), and speech recognition (Jelenik, 1997). §2 provides basic mathematical notation. §3 provides relevant background on the subregular hierarchy. §4 describes automata-theoretic characterizations of SP languages. §5 defines SP distributions. §6 shows how these distributions can be efficiently estimated from positive data and provides a demonstration. §7 concludes the paper. 2 Preliminaries We start with some mostly standard notation. E denotes a finite set of symbols and a string over E is a finite sequence of symbols drawn from that set. Ek, E&lt;k, Elk, and E* denote all strings over this alphabet of length k, of length less than or eq</context>
</contexts>
<marker>Jelenik, 1997</marker>
<rawString>Frederick Jelenik. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Douglas Johnson</author>
</authors>
<title>Formal Aspects of Phonological Description.</title>
<date>1972</date>
<publisher>The Hague: Mouton.</publisher>
<contexts>
<context position="1292" citStr="Johnson, 1972" startWordPosition="180" endWordPosition="181">natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on</context>
</contexts>
<marker>Johnson, 1972</marker>
<rawString>C. Douglas Johnson. 1972. Formal Aspects of Phonological Description. The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In</title>
<date>1985</date>
<booktitle>Natural Language Parsing,</booktitle>
<pages>206--250</pages>
<editor>D. Dowty, L. Karttunen, and A. Zwicky, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="893" citStr="Joshi, 1985" startWordPosition="125" endWordPosition="126">h encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, conve</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>A. K. Joshi. 1985. Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In D. Dowty, L. Karttunen, and A. Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, 2nd edition.</title>
<date>2008</date>
<contexts>
<context position="2219" citStr="Jurafsky and Martin, 2008" startWordPosition="313" endWordPosition="316">by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on the basis of contiguous subsequences. The Strictly Local languages are the formal-language theoretic foundation for n-gram models (Garcia et al., 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated from positive data (i.e. a corpus) (Jurafsky and Martin, 2008). N-gram models describe probability distributions over all strings on the basis of the Markov assumption (Markov, 1913): that the probability of the next symbol only depends on the previous contiguous sequence of length n − 1. From the perspective of formal language theory, these distributions are perhaps properly called Strictly k-Local distributions (SLk) where k = n. It is well-known that one limitation of the Markov assumption is its inability to express any kind of long-distance dependency. This paper defines Strictly k-Piecewise (SPk) distributions and shows how they too can be efficien</context>
<context position="14277" citStr="Jurafsky and Martin, 2008" startWordPosition="2499" endWordPosition="2502">rcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Similarly since the parameters of ngram models (Jurafsky and Martin, 2008) assign probabilities to symbols given the preceding contiguous substrings up to length n − 1, we say they describe Strictly n-Local distributions. These hierarchies have a very attractive modeltheoretic characterization. The Locally Testable (LT) and Piecewise Testable languages are exactly those that are definable by propositional formulae in which the atomic formulae are blocks of symbols interpreted factors (LT) or subsequences (PT) of the string. The languages that are testable in the strict sense (SL and SP) are exactly those that are definable by formulae of this sort restricted to conj</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="1315" citStr="Kaplan and Kay, 1994" startWordPosition="182" endWordPosition="185">e are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on the basis of contiguou</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Kobele</author>
</authors>
<title>Generating Copies: An Investigation into Structural Identity in Language and Grammar.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>Los Angeles.</location>
<contexts>
<context position="923" citStr="Kobele, 2006" startWordPosition="129" endWordPosition="130">ng-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see H</context>
</contexts>
<marker>Kobele, 2006</marker>
<rawString>Gregory Kobele. 2006. Generating Copies: An Investigation into Structural Identity in Language and Grammar. Ph.D. thesis, University of California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Kontorovich</author>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
</authors>
<title>Kernel methods for learning languages.</title>
<date>2008</date>
<journal>Theoretical Computer Science,</journal>
<volume>405</volume>
<issue>3</issue>
<contexts>
<context position="13726" citStr="Kontorovich et al., 2008" startWordPosition="2408" endWordPosition="2411">cceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Similarly since the parameters of ngram models (Jurafsky and Martin, 2008) assign probabilities to symbols given the preced</context>
<context position="16964" citStr="Kontorovich, et al. (2008)" startWordPosition="2988" endWordPosition="2991">(up to some length k) which are licensed to occur in the string. Definition 2 (SPk Grammar, SP) A SPk grammar is a pair G = hE, Gi where G ⊆ Ek. The language licensed by a SPk grammar is L(G) def= {w ∈ E∗ |P≤k(w) ⊆ P≤k(G)}. A language is SPk iff it is L(G) for some SPk grammar G. It is SP iff it is SPk for some k. This paper is primarily concerned with estimating Strictly Piecewise distributions, but first we examine in greater detail properties of SP languages, in particular DFA representations. 4 DFA representations of SP Languages Following Sakarovitch and Simon (1983), Lothaire (1997) and Kontorovich, et al. (2008), we call the set of strings that contain w as a subsequence the principal shuffle ideal2 of w: SI(w) = {v ∈ E∗ |w ⊑ v}. The shuffle ideal of a set of strings is defined as SI(S) = ∪w∈SSI(w) Rogers et al. (to appear) establish that the SP languages have a variety of characteristic properties. Theorem 1 The following are equivalent:3 2Properly SI(w) is the principal ideal generated by {w} wrt the inverse of ⊑. 3For a complete proof, see Rogers et al. (to appear). We only note that 5 implies 1 by DeMorgan’s theorem and the fact that every shuffle ideal is finitely generated (see also Lothaire (1</context>
</contexts>
<marker>Kontorovich, Cortes, Mohri, 2008</marker>
<rawString>Leonid (Aryeh) Kontorovich, Corinna Cortes, and Mehryar Mohri. 2008. Kernel methods for learning languages. Theoretical Computer Science, 405(3):223 – 236. Algorithmic Learning Theory.</rawString>
</citation>
<citation valid="true">
<title>Combinatorics on Words.</title>
<date>1997</date>
<editor>M. Lothaire, editor.</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK, New York.</location>
<contexts>
<context position="16933" citStr="(1997)" startWordPosition="2986" endWordPosition="2986">bsequences (up to some length k) which are licensed to occur in the string. Definition 2 (SPk Grammar, SP) A SPk grammar is a pair G = hE, Gi where G ⊆ Ek. The language licensed by a SPk grammar is L(G) def= {w ∈ E∗ |P≤k(w) ⊆ P≤k(G)}. A language is SPk iff it is L(G) for some SPk grammar G. It is SP iff it is SPk for some k. This paper is primarily concerned with estimating Strictly Piecewise distributions, but first we examine in greater detail properties of SP languages, in particular DFA representations. 4 DFA representations of SP Languages Following Sakarovitch and Simon (1983), Lothaire (1997) and Kontorovich, et al. (2008), we call the set of strings that contain w as a subsequence the principal shuffle ideal2 of w: SI(w) = {v ∈ E∗ |w ⊑ v}. The shuffle ideal of a set of strings is defined as SI(S) = ∪w∈SSI(w) Rogers et al. (to appear) establish that the SP languages have a variety of characteristic properties. Theorem 1 The following are equivalent:3 2Properly SI(w) is the principal ideal generated by {w} wrt the inverse of ⊑. 3For a complete proof, see Rogers et al. (to appear). We only note that 5 implies 1 by DeMorgan’s theorem and the fact that every shuffle ideal is finitely </context>
</contexts>
<marker>1997</marker>
<rawString>M. Lothaire, editor. 1997. Combinatorics on Words. Cambridge University Press, Cambridge, UK, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A A Markov</author>
</authors>
<title>An example of statistical study on the text of ‘eugene onegin’ illustrating the linking of events to a chain.</title>
<date>1913</date>
<contexts>
<context position="2339" citStr="Markov, 1913" startWordPosition="333" endWordPosition="334">ous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on the basis of contiguous subsequences. The Strictly Local languages are the formal-language theoretic foundation for n-gram models (Garcia et al., 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated from positive data (i.e. a corpus) (Jurafsky and Martin, 2008). N-gram models describe probability distributions over all strings on the basis of the Markov assumption (Markov, 1913): that the probability of the next symbol only depends on the previous contiguous sequence of length n − 1. From the perspective of formal language theory, these distributions are perhaps properly called Strictly k-Local distributions (SLk) where k = n. It is well-known that one limitation of the Markov assumption is its inability to express any kind of long-distance dependency. This paper defines Strictly k-Piecewise (SPk) distributions and shows how they too can be efficiently estimated from positive data. In contrast with the Markov assumption, our assumption is that the probability of the </context>
</contexts>
<marker>Markov, 1913</marker>
<rawString>A. A. Markov. 1913. An example of statistical study on the text of ‘eugene onegin’ illustrating the linking of events to a chain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert McNaughton</author>
<author>Simon Papert</author>
</authors>
<title>Counter-Free Automata.</title>
<date>1971</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1833" citStr="McNaughton and Papert, 1971" startWordPosition="255" endWordPosition="258">2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971; Rogers and Pullum, to appear), which make distinctions on the basis of contiguous subsequences. The Strictly Local languages are the formal-language theoretic foundation for n-gram models (Garcia et al., 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated from positive data (i.e. a corpus) (Jurafsky and Martin, 2008). N-gram models describe probability distributions over all strings on the basis of the Markov assumption (Markov, 1913): that the probability of the next symbol only depends on the previous contiguous sequence of </context>
<context position="13505" citStr="McNaughton and Papert, 1971" startWordPosition="2370" endWordPosition="2374">guages that are Locally Testable in the Strict Sense (SL), and one in which languages are defined in terms of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Stric</context>
<context position="15441" citStr="McNaughton and Papert, 1971" startWordPosition="2679" endWordPosition="2682"> that are definable by formulae of this sort restricted to conjunctions of negative literals. Going the other way, the languages that are definable by First-Order formulae with adjacency (successor) but not precedence (less-than) are exactly the Locally Threshold Testable (LTT) languages. The Star-Free languages are those that are First-Order definable with precedence alone (adjacency being FO definable from precedence). Finally, by extending to Monadic Second-Order formulae (with either signature, since they are MSO definable from each other), one obtains the full class of Regular languages (McNaughton and Papert, 1971; Thomas, 1982; Rogers and Pullum, to appear; Rogers et al., to appear). The relation between strings which is fundamental along the Piecewise branch is the subsec:3 b:2 b:2 a:2 A:2 a:3 B:4 c:1 LT SP SL SF FO LTT PT Prop +1 &lt; 889 quence relation, which is a partial order on E∗: w ⊑ v⇐⇒ w = ε or w = σ1 ··· σn and def (∃w0, ... , wn ∈ E∗)[v = w0σ1w1 ··· σnwn]. in which case we say w is a subsequence of v. For w ∈ E∗, let Pk(w) def = {v ∈ Ek |v ⊑ w} and P≤k(w) def = {v ∈ E≤k |v ⊑ w}, the set of subsequences of length k, respectively length no greater than k, of w. Let Pk(L) and P≤k(L) be the natu</context>
</contexts>
<marker>McNaughton, Papert, 1971</marker>
<rawString>Robert McNaughton and Simon Papert. 1971. Counter-Free Automata. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Newell</author>
<author>S Langer</author>
<author>M Hickey</author>
</authors>
<title>The rˆole of natural language processing in alternative and augmentative communication.</title>
<date>1998</date>
<journal>Natural Language Engineering,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4694" citStr="Newell et al., 1998" startWordPosition="709" endWordPosition="712"> is left for future research. Since SP languages are the analogue of SL languages, which are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application. Apart from their interest to problems in theoretical phonology such as phonotactic learning (Coleman and Pierrehumbert, 1997; Hayes and Wilson, 2008; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (Newell et al., 1998), part of speech tagging (Brill, 1995), and speech recognition (Jelenik, 1997). §2 provides basic mathematical notation. §3 provides relevant background on the subregular hierarchy. §4 describes automata-theoretic characterizations of SP languages. §5 defines SP distributions. §6 shows how these distributions can be efficiently estimated from positive data and provides a demonstration. §7 concludes the paper. 2 Preliminaries We start with some mostly standard notation. E denotes a finite set of symbols and a string over E is a finite sequence of symbols drawn from that set. Ek, E&lt;k, Elk, and E</context>
</contexts>
<marker>Newell, Langer, Hickey, 1998</marker>
<rawString>A. Newell, S. Langer, and M. Hickey. 1998. The rˆole of natural language processing in alternative and augmentative communication. Natural Language Engineering, 4(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Perrin</author>
<author>Jean-Eric Pin</author>
</authors>
<title>FirstOrder logic and Star-Free sets.</title>
<date>1986</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>32--393</pages>
<contexts>
<context position="13582" citStr="Perrin and Pin, 1986" startWordPosition="2383" endWordPosition="2386">s are defined in terms of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous </context>
</contexts>
<marker>Perrin, Pin, 1986</marker>
<rawString>Dominique Perrin and Jean-Eric Pin. 1986. FirstOrder logic and Star-Free sets. Journal of Computer and System Sciences, 32:393–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Ringen</author>
</authors>
<title>Vowel Harmony: Theoretical Implications.</title>
<date>1988</date>
<publisher>Garland Publishing, Inc.</publisher>
<contexts>
<context position="1179" citStr="Ringen, 1988" startWordPosition="164" endWordPosition="165">nd show that they can be efficiently estimated from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Stri</context>
</contexts>
<marker>Ringen, 1988</marker>
<rawString>Catherine Ringen. 1988. Vowel Harmony: Theoretical Implications. Garland Publishing, Inc.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Rogers</author>
<author>Geoffrey Pullum</author>
</authors>
<title>to appear. Aural pattern recognition experiments and the subregular hierarchy.</title>
<journal>Journal of Logic, Language and Information.</journal>
<marker>Rogers, Pullum, </marker>
<rawString>James Rogers and Geoffrey Pullum. to appear. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Rogers</author>
<author>Jeffrey Heinz</author>
<author>Matt Edlefsen</author>
<author>Dylan Leeman</author>
<author>Nathan Myers</author>
<author>Nathaniel Smith</author>
<author>Molly Visscher</author>
<author>David Wellcome</author>
</authors>
<title>to appear. On languages piecewise testable in the strict sense.</title>
<booktitle>In Proceedings of the 11th Meeting of the Assocation for Mathematics ofLanguage.</booktitle>
<marker>Rogers, Heinz, Edlefsen, Leeman, Myers, Smith, Visscher, Wellcome, </marker>
<rawString>James Rogers, Jeffrey Heinz, Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel Smith, Molly Visscher, and David Wellcome. to appear. On languages piecewise testable in the strict sense. In Proceedings of the 11th Meeting of the Assocation for Mathematics ofLanguage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Rose</author>
<author>Rachel Walker</author>
</authors>
<title>A typology of consonant agreement as correspondence.</title>
<date>2004</date>
<journal>Language,</journal>
<volume>80</volume>
<issue>3</issue>
<contexts>
<context position="1234" citStr="Rose and Walker, 2004" startWordPosition="170" endWordPosition="173">d from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially Rogers et al. (2009)). As shown by Rogers et al. (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages (McNaughton and Papert, 1971;</context>
</contexts>
<marker>Rose, Walker, 2004</marker>
<rawString>Sharon Rose and Rachel Walker. 2004. A typology of consonant agreement as correspondence. Language, 80(3):475–531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Sakarovitch</author>
<author>Imre Simon</author>
</authors>
<date>1983</date>
<booktitle>Combinatorics on Words,</booktitle>
<volume>17</volume>
<pages>105--134</pages>
<editor>Subwords. In M. Lothaire, editor,</editor>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="16916" citStr="Sakarovitch and Simon (1983)" startWordPosition="2981" endWordPosition="2984"> defined only in terms of the set of subsequences (up to some length k) which are licensed to occur in the string. Definition 2 (SPk Grammar, SP) A SPk grammar is a pair G = hE, Gi where G ⊆ Ek. The language licensed by a SPk grammar is L(G) def= {w ∈ E∗ |P≤k(w) ⊆ P≤k(G)}. A language is SPk iff it is L(G) for some SPk grammar G. It is SP iff it is SPk for some k. This paper is primarily concerned with estimating Strictly Piecewise distributions, but first we examine in greater detail properties of SP languages, in particular DFA representations. 4 DFA representations of SP Languages Following Sakarovitch and Simon (1983), Lothaire (1997) and Kontorovich, et al. (2008), we call the set of strings that contain w as a subsequence the principal shuffle ideal2 of w: SI(w) = {v ∈ E∗ |w ⊑ v}. The shuffle ideal of a set of strings is defined as SI(S) = ∪w∈SSI(w) Rogers et al. (to appear) establish that the SP languages have a variety of characteristic properties. Theorem 1 The following are equivalent:3 2Properly SI(w) is the principal ideal generated by {w} wrt the inverse of ⊑. 3For a complete proof, see Rogers et al. (to appear). We only note that 5 implies 1 by DeMorgan’s theorem and the fact that every shuffle i</context>
</contexts>
<marker>Sakarovitch, Simon, 1983</marker>
<rawString>Jacques Sakarovitch and Imre Simon. 1983. Subwords. In M. Lothaire, editor, Combinatorics on Words, volume 17 of Encyclopedia of Mathematics and Its Applications, chapter 6, pages 105–134. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Evidence against the contextfreeness of natural language. Linguistics and Philosophy,</title>
<date>1985</date>
<pages>8--333</pages>
<contexts>
<context position="908" citStr="Shieber, 1985" startWordPosition="127" endWordPosition="128">ain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data. 1 Introduction Long-distance dependencies in natural language are of considerable interest. Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states (Chomsky, 1956; Joshi, 1985; Shieber, 1985; Kobele, 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations. For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments (Ringen, 1988; Bakovi´c, 2000; Hansson, 2001; Rose and Walker, 2004) and that phonological patterns are regular (Johnson, 1972; Kaplan and Kay, 1994), it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging character</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Stuart Shieber. 1985. Evidence against the contextfreeness of natural language. Linguistics and Philosophy, 8:333–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imre Simon</author>
</authors>
<title>Piecewise testable events.</title>
<date>1975</date>
<booktitle>In Automata Theory and Formal Languages: 2nd Grammatical Inference conference,</booktitle>
<pages>214--222</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin; New York.</location>
<contexts>
<context position="13546" citStr="Simon, 1975" startWordPosition="2379" endWordPosition="2380">, and one in which languages are defined in terms of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all s</context>
</contexts>
<marker>Simon, 1975</marker>
<rawString>Imre Simon. 1975. Piecewise testable events. In Automata Theory and Formal Languages: 2nd Grammatical Inference conference, pages 214–222, Berlin; New York. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Straubing</author>
</authors>
<title>Finite Automata, Formal Logic and Circuit Complexity.</title>
<date>1994</date>
<publisher>Birkh¨auser.</publisher>
<contexts>
<context position="13647" citStr="Straubing, 1994" startWordPosition="2395" endWordPosition="2397">s, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except those with contiguous substrings {ab, ba}. Similarly since the parameters of ngram mode</context>
</contexts>
<marker>Straubing, 1994</marker>
<rawString>Howard Straubing. 1994. Finite Automata, Formal Logic and Circuit Complexity. Birkh¨auser.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Thomas</author>
</authors>
<title>Classifying regular events in symbolic logic.</title>
<date>1982</date>
<journal>Journal of Computer and Systems Sciences,</journal>
<pages>25--360</pages>
<contexts>
<context position="13560" citStr="Thomas, 1982" startWordPosition="2381" endWordPosition="2382">which languages are defined in terms of their not necessarily contiguous subsequences, starting with the languages that are Piecewise 1Technically, this acceptor is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see (de la Higuera, in press). Testable in the Strict Sense (SP). Each language class in these hierarchies has independently motivated, converging characterizations and each has been claimed to correspond to specific, fundamental cognitive capabilities (McNaughton and Papert, 1971; Brzozowski and Simon, 1973; Simon, 1975; Thomas, 1982; Perrin and Pin, 1986; Garcia and Ruiz, 1990; Beauquier and Pin, 1991; Straubing, 1994; Garcia and Ruiz, 1996; Rogers and Pullum, to appear; Kontorovich et al., 2008; Rogers et al., to appear). Languages in the weakest of these classes are defined only in terms of the set of factors (SL) or subsequences (SP) which are licensed to occur in the string (equivalently the complement of that set with respect to E≤k, the forbidden factors or forbidden subsequences). For example, the set containing the forbidden 2-factors {ab, ba} defines a Strictly 2-Local language which includes all strings except </context>
<context position="15455" citStr="Thomas, 1982" startWordPosition="2683" endWordPosition="2684">ae of this sort restricted to conjunctions of negative literals. Going the other way, the languages that are definable by First-Order formulae with adjacency (successor) but not precedence (less-than) are exactly the Locally Threshold Testable (LTT) languages. The Star-Free languages are those that are First-Order definable with precedence alone (adjacency being FO definable from precedence). Finally, by extending to Monadic Second-Order formulae (with either signature, since they are MSO definable from each other), one obtains the full class of Regular languages (McNaughton and Papert, 1971; Thomas, 1982; Rogers and Pullum, to appear; Rogers et al., to appear). The relation between strings which is fundamental along the Piecewise branch is the subsec:3 b:2 b:2 a:2 A:2 a:3 B:4 c:1 LT SP SL SF FO LTT PT Prop +1 &lt; 889 quence relation, which is a partial order on E∗: w ⊑ v⇐⇒ w = ε or w = σ1 ··· σn and def (∃w0, ... , wn ∈ E∗)[v = w0σ1w1 ··· σnwn]. in which case we say w is a subsequence of v. For w ∈ E∗, let Pk(w) def = {v ∈ Ek |v ⊑ w} and P≤k(w) def = {v ∈ E≤k |v ⊑ w}, the set of subsequences of length k, respectively length no greater than k, of w. Let Pk(L) and P≤k(L) be the natural extensions</context>
</contexts>
<marker>Thomas, 1982</marker>
<rawString>Wolfgang Thomas. 1982. Classifying regular events in symbolic logic. Journal of Computer and Systems Sciences, 25:360–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
<author>Franck Thollard</author>
<author>Colin de la Higuera</author>
<author>Francisco Casacuberta</author>
<author>Rafael C Carrasco</author>
</authors>
<title>Probabilistic finite-state machines-part I.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="9222" citStr="Vidal et al., 2005" startWordPosition="1578" endWordPosition="1581">ility distribution is regular deterministic iff there is a PDFA which generates it. The structural components of a PDFA M are its states Q, its alphabet E, its transitions δ, and its initial state q0. By structure of a PDFA, we mean its structural components. Each PDFA M defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1. These distributions have JQJ· (JEJ + 1) independent parameters (since for each state there are JEJ possible transitions plus the possibility of finality.) We define the product of PDFA in terms of coemission probabilities (Vidal et al., 2005a). Definition 1 Let A be a vector of PDFAs and let JAJ = n. For each 1 &lt; i &lt; n let Mi = (Qi, E, q0i, δi, Fi, Ti) be the ith PDFA in A. The probability that σ is co-emitted from q1, ... , qn in Q1, . . . , Qn, respectively, is n CT((σ,q1 ... qn)) = Ti(qi, σ). i=1 Similarly, the probability that a word simultaneously ends at q1 E Q1 ... qn E Qn is n CF((q1 ... qn)) = Fi(qi). i=1 Then ®A= (Q, E,q0,δ,F,T) where 1. Q, q0, and δ are defined as with DFA product. 2. For all (q1 ... qn) E Q, let Z((q1 ... qn)) = CF((q1 ... qn)) + � CT((σ,q1 ... qn)) σEE (a) let F((q1 ... qn)) = CF((q1 ... qn)); Z((q1 </context>
<context position="10666" citStr="Vidal et al., 2005" startWordPosition="1866" endWordPosition="1869">res that M defines a well-formed probability distribution. Statistically speaking, the co-emission product makes an independence assumption: the probability of σ being co-emitted from q1, ... , qn is exactly what one expects if there is no interaction between the individual factors; that is, between the probabilities of σ being emitted from any qi. Also note order of product is irrelevant up to renaming of the states, and so therefore we also speak of taking the product of a set of PDFAs (as opposed to an ordered vector). Estimating regular deterministic distributions is well-studied problem (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, in press). We limit discussion to cases when the structure of the PDFA is known. Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of M so that DM approaches D. We employ the widelyadopted maximum likelihood (ML) criterion for this estimation. F) = argmax rl PrM (w) (3) T,F wES It is well-known that if D is generated by some PDFA M′ with the same structural components as M, then optimizing the ML estimate guarantees that DM approaches D as the size of S goes to infinity (Vida</context>
</contexts>
<marker>Vidal, Thollard, Higuera, Casacuberta, Carrasco, 2005</marker>
<rawString>Enrique Vidal, Franck Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005a. Probabilistic finite-state machines-part I. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1013–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
<author>Frank Thollard</author>
<author>Colin de la Higuera</author>
<author>Francisco Casacuberta</author>
<author>Rafael C Carrasco</author>
</authors>
<title>Probabilistic finite-state machines-part II.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="9222" citStr="Vidal et al., 2005" startWordPosition="1578" endWordPosition="1581">ility distribution is regular deterministic iff there is a PDFA which generates it. The structural components of a PDFA M are its states Q, its alphabet E, its transitions δ, and its initial state q0. By structure of a PDFA, we mean its structural components. Each PDFA M defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1. These distributions have JQJ· (JEJ + 1) independent parameters (since for each state there are JEJ possible transitions plus the possibility of finality.) We define the product of PDFA in terms of coemission probabilities (Vidal et al., 2005a). Definition 1 Let A be a vector of PDFAs and let JAJ = n. For each 1 &lt; i &lt; n let Mi = (Qi, E, q0i, δi, Fi, Ti) be the ith PDFA in A. The probability that σ is co-emitted from q1, ... , qn in Q1, . . . , Qn, respectively, is n CT((σ,q1 ... qn)) = Ti(qi, σ). i=1 Similarly, the probability that a word simultaneously ends at q1 E Q1 ... qn E Qn is n CF((q1 ... qn)) = Fi(qi). i=1 Then ®A= (Q, E,q0,δ,F,T) where 1. Q, q0, and δ are defined as with DFA product. 2. For all (q1 ... qn) E Q, let Z((q1 ... qn)) = CF((q1 ... qn)) + � CT((σ,q1 ... qn)) σEE (a) let F((q1 ... qn)) = CF((q1 ... qn)); Z((q1 </context>
<context position="10666" citStr="Vidal et al., 2005" startWordPosition="1866" endWordPosition="1869">res that M defines a well-formed probability distribution. Statistically speaking, the co-emission product makes an independence assumption: the probability of σ being co-emitted from q1, ... , qn is exactly what one expects if there is no interaction between the individual factors; that is, between the probabilities of σ being emitted from any qi. Also note order of product is irrelevant up to renaming of the states, and so therefore we also speak of taking the product of a set of PDFAs (as opposed to an ordered vector). Estimating regular deterministic distributions is well-studied problem (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, in press). We limit discussion to cases when the structure of the PDFA is known. Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of M so that DM approaches D. We employ the widelyadopted maximum likelihood (ML) criterion for this estimation. F) = argmax rl PrM (w) (3) T,F wES It is well-known that if D is generated by some PDFA M′ with the same structural components as M, then optimizing the ML estimate guarantees that DM approaches D as the size of S goes to infinity (Vida</context>
</contexts>
<marker>Vidal, Thollard, Higuera, Casacuberta, Carrasco, 2005</marker>
<rawString>Enrique Vidal, Frank Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005b. Probabilistic finite-state machines-part II. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1026–1039.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>