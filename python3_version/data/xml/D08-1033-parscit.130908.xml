<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.76672">
Sampling Alignment Structure under a Bayesian Translation Model
</title>
<author confidence="0.996636">
John DeNero, Alexandre Bouchard-Cˆot´e and Dan Klein
</author>
<affiliation confidence="0.999098">
Computer Science Department
University of California, Berkeley
</affiliation>
<email confidence="0.959766">
{denero, bouchard, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999200916666667">
We describe the first tractable Gibbs sam-
pling procedure for estimating phrase pair
frequencies under a probabilistic model of
phrase alignment. We propose and evalu-
ate two nonparametric priors that successfully
avoid the degenerate behavior noted in previ-
ous work, where overly large phrases mem-
orize the training data. Phrase table weights
learned under our model yield an increase in
BLEU score over the word-alignment based
heuristic estimates used regularly in phrase-
based translation systems.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954730769231">
In phrase-based translation, statistical knowledge
of translation equivalence is primarily captured by
counts of how frequently various phrase pairs occur
in training bitexts. Since bitexts do not come seg-
mented and aligned into phrase pairs, these counts
are typically gathered by fixing a word alignment
and applying phrase extraction heuristics to this
word-aligned training corpus. Alternatively, phrase
pair frequencies can be learned via a probabilistic
model of phrase alignment, but this approach has
presented several practical challenges.
In this paper, we address the two most signifi-
cant challenges in phrase alignment modeling. The
first challenge is with inference: computing align-
ment expectations under general phrase models is
#P-hard (DeNero and Klein, 2008). Previous phrase
alignment work has sacrificed consistency for effi-
ciency, employing greedy hill-climbing algorithms
and constraining inference with word alignments
(Marcu and Wong, 2002; DeNero et al., 2006; Birch
et al., 2006). We describe a Gibbs sampler that con-
sistently and efficiently approximates expectations,
using only polynomial-time computable operators.
Despite the combinatorial complexity of the phrase
alignment space, our sampled phrase pair expecta-
tions are guaranteed to converge to the true poste-
rior distributions under the model (in theory) and do
converge to effective values (in practice).
The second challenge in learning phrase align-
ments is avoiding a degenerate behavior of the gen-
eral model class: as with many models which can
choose between large and small structures, the larger
structures win out in maximum likelihood estima-
tion. Indeed, the maximum likelihood estimate of
a joint phrase alignment model analyzes each sen-
tence pair as one large phrase with no internal struc-
ture (Marcu and Wong, 2002). We describe two non-
parametric priors that empirically avoid this degen-
erate solution.
Fixed word alignments are used in virtually ev-
ery statistical machine translation system, if not to
extract phrase pairs or rules directly, then at least
to constrain the inference procedure for higher-level
models. We estimate phrase translation features
consistently using an inference procedure that is not
constrained by word alignments, or any other heuris-
tic. Despite this substantial change in approach, we
report translation improvements over the standard
word-alignment-based heuristic estimates of phrase
table weights. We view this result as an important
step toward building fully model-based translation
systems that rely on fewer procedural heuristics.
</bodyText>
<sectionHeader confidence="0.996601" genericHeader="method">
2 Phrase Alignment Model
</sectionHeader>
<bodyText confidence="0.999950714285714">
While state-of-the-art phrase-based translation sys-
tems include an increasing number of features,
translation behavior is largely driven by the phrase
pair count ratios φ(e|f) and φ(f|e). These features
are typically estimated heuristically using the counts
c(he, fi) of all phrase pairs in a training corpus that
are licensed by word alignments:
</bodyText>
<equation confidence="0.950582">
c(he, fi)
φ(e|f) = c(he�, fi) .
</equation>
<page confidence="0.988834">
314
</page>
<note confidence="0.970725">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<figure confidence="0.995738642857143">
Thank , I shall gladly.
you do so
Gracias
,
lo
haré
de
muy
buen
grado
.
Thank , I shall gladly.
you do so
(a) example word alignment (b) example phrase alignment
</figure>
<figureCaption confidence="0.997806333333333">
Figure 1: In this corpus example, the phrase
alignment model found the non-literal translation
pair hgladly, de muy buen gradoi while heuristically-
combined word alignment models did not. (a) is a grow-
diag-final-and combined IBM Model 4 word alignment;
(b) is a phrase alignment under our model.
</figureCaption>
<bodyText confidence="0.999966285714286">
In contrast, a generative model that explicitly
aligns pairs of phrases he, fi gives us well-founded
alternatives for estimating phrase pair scores. For
instance, we could use the model’s parameters as
translation features. In this paper, we compute the
expected counts of phrase pairs in the training data
according to our model, and derive features from
these expected counts. This approach endows phrase
pair scores with well-defined semantics relative to a
probabilistic model. Practically, phrase models can
discover high-quality phrase pairs that often elude
heuristics, as in Figure 1. In addition, the model-
based approach fits neatly into the framework of sta-
tistical learning theory for unsupervised problems.
</bodyText>
<sectionHeader confidence="0.805501" genericHeader="method">
2.1 Generative Model Description
</sectionHeader>
<bodyText confidence="0.875484857142857">
We first describe the symmetric joint model of
Marcu and Wong (2002), which we will extend. A
two-step generative process constructs an ordered
set of English phrases e1:m, an ordered set of for-
eign phrases f1:,,, and a phrase-to-phrase alignment
between them, a = {(j, k)} indicating that hej, fki
is an aligned pair.
</bodyText>
<listItem confidence="0.9961698">
1. Choose a number of components ` and generate
each of ` phrase pairs independently.
2. Choose an ordering for the phrases in the for-
eign language; the ordering for English is fixed
by the generation order.1
</listItem>
<footnote confidence="0.746571">
1We choose the foreign to reorder without loss of generality.
</footnote>
<bodyText confidence="0.99558125">
In this process, m = n = |a|; all phrases in both
sentences are aligned one-to-one.
We parameterize the choice of ` using a geometric
distribution, denoted PG, with stop parameter p$:
</bodyText>
<equation confidence="0.997605">
P(`) = PG(`; p$) = p$ · (1 − p$)&amp;quot; .
</equation>
<bodyText confidence="0.982935166666667">
Each aligned phrase pair he, fi is drawn from a
multinomial distribution θ7 which is unknown. We
fix a simple distortion model, setting the probability
of a permutation of the foreign phrases proportional
to the product of position-based distortion penalties
for each phrase:
</bodyText>
<equation confidence="0.998734">
P(a|{he, fi}) ∝ 11 δ(a)
aEa
δ(a = (j, k)) = b|po�(ej)−po8(fk)·3 |,
</equation>
<bodyText confidence="0.999923">
where pos(·) denotes the word position of the start
of a phrase, and s the ratio of the length of the En-
glish to the length of the foreign sentence. This po-
sitional distortion model was deemed to work best
by Marcu and Wong (2002).
We can now state the joint probability for a
phrase-aligned sentence consisting of ` phrase pairs:
</bodyText>
<equation confidence="0.996007">
P({he,fi},a) = PG(`;p$)P(a|{he,fi}) 11 θ7(he,fi) .
(e,f)
</equation>
<bodyText confidence="0.999948333333333">
While this model has several free parameters in ad-
dition to θ7, we fix them to reasonable values to fo-
cus learning on the phrase pair distribution.2
</bodyText>
<subsectionHeader confidence="0.998325">
2.2 Unaligned Phrases
</subsectionHeader>
<bodyText confidence="0.999636363636364">
Sentence pairs do not always contain equal informa-
tion on both sides, and so we revise the generative
story to include unaligned phrases in both sentences.
When generating each component of a sentence pair,
we first decide whether to generate an aligned phrase
pair or, with probability po, an unaligned phrase.3
Then, we either generate an aligned phrase pair from
θ7 or an unaligned phrase from θN, where θN is a
multinomial over phrases. Now, when generating
e1:m, f1:,, and alignment a, the number of phrases
m + n can be greater than 2 · |a|.
</bodyText>
<footnote confidence="0.932193">
2Parameters were chosen by hand during development on a
small training corpus. p$ = 0.1, b = 0.85 in experiments.
3We strongly discouraged unaligned phrases in order to
align as much of the corpus as possible: po = 10−10 in ex-
periments.
</footnote>
<figure confidence="0.896098222222222">
Gracias
,
lo
haré
de
muy
buen
grado
.
</figure>
<page confidence="0.998116">
315
</page>
<bodyText confidence="0.997841666666667">
To unify notation, we denote unaligned phrases as
phrase pairs with one side equal to null: (e, null) or
(null, f). Then, the revised model takes the form:
</bodyText>
<equation confidence="0.99254125">
�
P({(e,f)},a) = PG(�;p$)P(a|{(e, f)}) PM((e,f))
(e,f)
PM((e, f)) = p00N((e, f)) + (1 − po)0J((e, f)) �
</equation>
<bodyText confidence="0.99980675">
In this definition, the distribution 0N gives non-
zero weight only to unaligned phrases of the form
(e, null) or (null, f), while 0J gives non-zero
weight only to aligned phrase pairs.
</bodyText>
<subsectionHeader confidence="0.570203">
3 Model Training and Expectations
</subsectionHeader>
<bodyText confidence="0.999877487804878">
Our model involves observed sentence pairs, which
in aggregate we can call x, latent phrase segmenta-
tions and alignments, which we can call z, and pa-
rameters 0J and 0N, which together we can call 0.
A model such as ours could be used either for the
learning of the key phrase pair parameters in 0, or
to compute expected counts of phrase pairs in our
data. These two uses are very closely related, but
we focus on the computation of phrase pair expecta-
tions. For exposition purposes, we describe a Gibbs
sampling algorithm for computing expected counts
of phrases under P(z|x, 0) for fixed 0. Such ex-
pectations would be used, for example, to compute
maximum likelihood estimates in the E-step of EM.
In Section 4, we instead compute expectations under
P(z|x), with 0 marginalized out entirely.
In a Gibbs sampler, we start with a complete
phrase segmentation and alignment, state z0, which
sets all latent variables to some initial configuration.
We then produce a sequence of sample states zz,
each of which differs from the last by some small
local change. The samples zz are guaranteed (in the
limit) to consistently approximate the conditional
distribution P(z|x, 0) (or P(z|x) later). Therefore,
the average counts of phrase pairs in the samples
converge to expected counts under the model. Nor-
malizing these expected counts yields estimates for
the features 0(e|f) and 0(f|e).
Gibbs sampling is not new to the natural language
processing community (Teh, 2006; Johnson et al.,
2007). However, it is usually used as a search pro-
cedure akin to simulated annealing, rather than for
approximating expectations (Goldwater et al., 2006;
Finkel et al., 2007). Our application is also atypical
for an NLP application in that we use an approxi-
mate sampler not only to include Bayesian prior in-
formation (section 4), but also because computing
phrase alignment expectations exactly is a #P-hard
problem (DeNero and Klein, 2008). That is, we
could not run EM exactly, even if we wanted maxi-
mum likelihood estimates.
</bodyText>
<subsectionHeader confidence="0.760929">
3.1 Related Work
</subsectionHeader>
<bodyText confidence="0.999946413793103">
Expected phrase pair counts under P(z|x, 0) have
been approximated before in order to run EM.
Marcu and Wong (2002) employed local search
from a heuristic initialization and collected align-
ment counts during a hill climb through the align-
ment space. DeNero et al. (2006) instead proposed
an exponential-time dynamic program pruned using
word alignments. Subsequent work has relied heav-
ily on word alignments to constrain inference, even
under reordering models that admit polynomial-time
E-steps (Cherry and Lin, 2007; Zhang et al., 2008).
None of these approximations are consistent, and
they offer no method of measuring their biases.
Gibbs sampling is not only consistent in the limit,
but also allows us to add Bayesian priors conve-
niently (section 4). Of course, sampling has liabili-
ties as well: we do not know in advance how long we
need to run the sampler to approximate the desired
expectations “closely enough.”
Snyder and Barzilay (2008) describe a Gibbs sam-
pler for a bilingual morphology model very similar
in structure to ours. However, the basic sampling
step they propose – resampling all segmentations
and alignments for a sequence at once – requires a
#P-hard computation. While this asymptotic com-
plexity was apparently not prohibitive in the case of
morphological alignment, where the sequences are
short, it is prohibitive in phrase alignment, where the
sentences are often very long.
</bodyText>
<subsectionHeader confidence="0.999944">
3.2 Sampling with the SWAP Operator
</subsectionHeader>
<bodyText confidence="0.99999175">
Our Gibbs sampler repeatedly applies each of five
operators to each position in each training sentence
pair. Each operator freezes all of the current state zz
except a small local region, determines all the ways
that region can be reconfigured, and then chooses a
(possibly) slightly different zz+1 from among those
outcomes according to the conditional probability of
each, given the frozen remainder of the state. This
</bodyText>
<page confidence="0.996048">
316
</page>
<bodyText confidence="0.99985335">
frozen region of the state is called a Markov blanket
(denoted m), and plays a critical role in proving the
correctness of the sampler.
The first operator we consider is SWAP, which
changes alignments but not segmentations. It freezes
the set of phrases, then picks two English phrases e1
and e2 (or two foreign phrases, but we focus on the
English case). All alignments are frozen except the
phrase pairs (e1, f1) and (e2, f2). SWAP chooses be-
tween keeping (e1, f1) and (e2, f2) aligned as they
are (outcome o0), or swapping their alignments to
create (e1, f2) and (e2, f1) (outcome o1).
SWAP chooses stochastically in proportion to
each outcome’s posterior probability: P(o0|m, x, B)
and P(o1|m, x, B). Each phrase pair in each out-
come contributes to these posteriors the probability
of adding a new pair, deciding whether it is null, and
generating the phrase pair along with its contribu-
tion to the distortion probability. This is all captured
in a succinct potential function 0((e, f)) =
</bodyText>
<equation confidence="0.9984785">
r(1−pg) (1−p0) BJ((e, f)) S((e, f)) e &amp; f non-null
(1−pg) · p0 · BN((e, f)) otherwise
.
Thus, outcome o0 is chosen with probability
P(o0|m, x, B) =
,, &apos;,/ &apos;//,
4&apos;((e1, f1))0((e2, f2))
0((e1, f1))0((e2, f2)) + 0((e1, f2))4&apos;((e2, f1)) �
</equation>
<bodyText confidence="0.99997975">
Operators in a Gibbs sampler require certain con-
ditions to guarantee the correctness of the sampler.
First, they must choose among all possible configu-
rations of the unfrozen local state. Second, imme-
diately re-applying the operator from any outcome
must yield the same set of outcome options as be-
fore.4 If these conditions are not met, the sampler
may no longer be guaranteed to yield consistent ap-
proximations of the posterior distribution.
A subtle issue arises with SWAP as defined:
should it also consider an outcome o2 of (e1, null)
and (e2, null) that removes alignments? No part
of the frozen state is changed by removing these
alignments, so the first Gibbs condition dictates that
we must include o2. However, after choosing o2,
when we reapply the operator to positions e1 and
</bodyText>
<footnote confidence="0.9302">
4These are two sufficient conditions to guarantee that the
Metropolis-Hastings acceptance ratio of the sampling step is 1.
</footnote>
<figureCaption confidence="0.945209571428571">
Figure 2: Each local operator manipulates a small portion
of a single alignment. Relevant phrases are exaggerated
for clarity. The outcome sets (depicted by arrows) of each
possible configuration are fully connected. Certain con-
figurations cannot be altered by certain operators, such as
the final configuration in SWAP. Unalterable configura-
tions for TOGGLE have been omitted for space.
</figureCaption>
<bodyText confidence="0.997487421052632">
e2, we freeze all alignments except (e1, null) and
(e2, null), which prevents us from returning to o0.
Thus, we fail to satisfy the second condition. This
point is worth emphasizing because some prior work
has treated Gibbs sampling as randomized search
and, intentionally or otherwise, proposed inconsis-
tent operators.
Luckily, the problem is not with SWAP, but with
our justification of it: we can salvage SWAP by aug-
menting its Markov blanket. Given that we have se-
lected (e1, f1) and (e2, f2), we not only freeze all
other alignments and phrase boundaries, but also the
number of aligned phrase pairs. With this count held
invariant, o2 is not among the possible outcomes of
SWAP given m. Moreover, regardless of the out-
come chosen, SWAP can immediately be reapplied
at the same location with the same set of outcomes.
All the possible starting configurations and out-
come sets for SWAP appear in Figure 2(a).
</bodyText>
<figure confidence="0.999725">
(d) FLIP TWO
(e) MOVE
(a) SWAP
(c) TOGGLE
(b) FLIP
</figure>
<page confidence="0.680024">
317
</page>
<figureCaption confidence="0.995408833333333">
Figure 3: The three steps involved in applying the FLIP
operator. The Markov blanket freezes all segmentations
except English position 1 and all alignments except those
for Ellos and The boys. The blanket also freezes the num-
ber of alignments, which disallows the lower right out-
come.
</figureCaption>
<subsectionHeader confidence="0.990227">
3.3 The FLIP operator
</subsectionHeader>
<bodyText confidence="0.999851941176471">
SWAP can arbitrarily shuffle alignments, but we
need a second operator to change the actual phrase
boundaries. The FLIP operator changes the status of
a single segmentation position5 to be either a phrase
boundary or not. In this sense FLIP is a bilingual
analog of the segmentation boundary flipping oper-
ator of Goldwater et al. (2006).
Figure 3 diagrams the operator and its Markov
blanket. First, FLIP chooses any between-word po-
sition in either sentence. The outcome sets for FLIP
vary based on the current segmentation and adjacent
alignments, and are depicted in Figure 2.
Again, for FLIP to satisfy the Gibbs conditions,
we must augment its Markov blanket to freeze not
only all other segmentation points and alignments,
but also the number of aligned phrase pairs. Oth-
erwise, we end up allowing outcomes from which
</bodyText>
<footnote confidence="0.955283333333333">
5A segmentation position is a position between two words
that is also potentially a boundary between two phrases in an
aligned sentence pair.
</footnote>
<bodyText confidence="0.992184">
we cannot return to the original state by reapply-
ing FLIP. Consequently, when a position is already
segmented and both adjacent phrases are currently
aligned, FLIP cannot unsegment the point because
it can’t create two aligned phrase pairs with the one
larger phrase that results (see bottom of Figure 2(b)).
</bodyText>
<subsectionHeader confidence="0.941061">
3.4 The TOGGLE operator
</subsectionHeader>
<bodyText confidence="0.9996606">
Both SWAP and FLIP freeze the number of align-
ments in a sentence. The TOGGLE operator, on the
other hand, can add or remove individual alignment
links. In TOGGLE, we first choose an e1 and f1. If
(e1, f1) E a or both e1 and f1 are null, we freeze
all segmentations and the rest of the alignments, and
choose between including (e1, f1) in the alignment
or leaving both e1 and f1 unaligned. If only one of
e1 and f1 are aligned, or they are not aligned to each
other, then TOGGLE does nothing.
</bodyText>
<subsectionHeader confidence="0.86801">
3.5 A Complete Sampler
</subsectionHeader>
<bodyText confidence="0.999981842105263">
Together, FLIP, SWAP and TOGGLE constitute a
complete Gibbs sampler that consistently samples
from the posterior P(z|x, B). Not only are these
operators valid Gibbs steps, but they also can form
a path of positive probability from any source state
to any target state in the space of phrase alignments
(formally, the induced Markov chain is irreducible).
Such a path can at worst be constructed by unalign-
ing all phrases in the source state with TOGGLE,
composing applications of FLIP to match the target
phrase boundaries, then applying TOGGLE to match
the target alignments.
We include two more local operators to speed up
the rate at which the sampler explores the hypothesis
space. In short, FLIP TWO simultaneously flips an
English and a foreign segmentation point (to make a
large phrase out of two smaller ones or vice versa),
while MOVE shifts an aligned phrase boundary to
the left or right. We omit details for lack of space.
</bodyText>
<subsectionHeader confidence="0.973363">
3.6 Phrase Pair Count Estimation
</subsectionHeader>
<bodyText confidence="0.9018598">
With our sampling procedure in place, we can now
estimate the expected number of times a given
phrase pair occurs in our data, for fixed 0, using a
Monte-Carlo average,
count(,,f)(x, zi) &amp;quot;&amp;quot;) E [count(,,f)(x, ·)] .
</bodyText>
<figure confidence="0.9629647">
eating
The boys are
Ellos
comen
Outcomes
An exhaustive set of
possibilities given
the Markov blanket
Current State
Includes segmentations
and alignments for all
sentence pairs
Markov Blanket
Freezes most of the
segmentations and
alignments, along with
the alignment count
2
7 7
?
Compute the conditional
probability of each outcome
3
Finally, select a new state proportional
to its conditional probability
1
Apply the FLIP operator
to English position 1
1
N
</figure>
<page confidence="0.669933666666667">
N
i=1
318
</page>
<bodyText confidence="0.99998">
The left hand side is simple to compute; we count
aligned phrase pairs in each sample we generate.
In practice, we only count phrase pairs after apply-
ing every operator to every position in every sen-
tence (one iteration).6 Appropriate normalizations
of these expected counts can be used either in an M-
step as maximum likelihood estimates, or to com-
pute values for features 0(f|e) and 0(e|f).
</bodyText>
<sectionHeader confidence="0.992798" genericHeader="method">
4 Nonparametric Bayesian Priors
</sectionHeader>
<bodyText confidence="0.999858285714286">
The Gibbs sampler we presented addresses the infer-
ence challenges of learning phrase alignment mod-
els. With slight modifications, it also enables us to
include prior information into the model. In this sec-
tion, we treat B as a random variable and shape its
prior distribution in order to correct the well-known
degenerate behavior of the model.
</bodyText>
<subsectionHeader confidence="0.99696">
4.1 Model Degeneracy
</subsectionHeader>
<bodyText confidence="0.999850555555555">
The structure of our joint model penalizes explana-
tions that use many small phrase pairs. Each phrase
pair token incurs the additional expense of genera-
tion and distortion. In fact, the maximum likelihood
estimate of the model puts mass on (e, f) pairs that
span entire sentences, explaining the training corpus
with one phrase pair per sentence.
Previous phrase alignment work has primarily
mitigated this tendency by constraining the in-
ference procedure, for example with word align-
ments and linguistic features (Birch et al., 2006),
or by disallowing large phrase pairs using a non-
compositional constraint (Cherry and Lin, 2007;
Zhang et al., 2008). However, the problem lies with
the model, and therefore should be corrected in the
model, rather than the inference procedure.
Model-based solutions appear in the literature as
well, though typically combined with word align-
ment constraints on inference. A sparse Dirichlet
prior coupled with variational EM was explored by
Zhang et al. (2008), but it did not avoid the degen-
erate solution. Moore and Quirk (2007) proposed a
new conditional model structure that does not cause
large and small phrases to compete for probabil-
ity mass. May and Knight (2007) added additional
model terms to balance the cost of long and short
derivations in a syntactic alignment model.
</bodyText>
<footnote confidence="0.952986">
6For experiments, we ran the sampler for 100 iterations.
</footnote>
<subsectionHeader confidence="0.976426">
4.2 A Dirichlet Process Prior
</subsectionHeader>
<bodyText confidence="0.99998075">
We control this degenerate behavior by placing a
Dirichlet process (DP) prior over BJ, the distribution
over aligned phrase pairs (Ferguson, 1973).
If we were to assume a maximum number K of
phrase pair types, a (finite) Dirichlet distribution
would be an appropriate prior. A draw from a K-
dimensional Dirichlet distribution is a list of K real
numbers in [0, 1] that sum to one, which can be in-
terpreted as a distribution over K phrase pair types.
However, since the event space of possible phrase
pairs is in principle unbounded, we instead use a
Dirichlet process. A draw from a DP is a countably
infinite list of real numbers in [0, 1] that sum to one,
which we interpret as a distribution over a countably
infinite list of phrase pair types.7
The Dirichlet distribution and the DP distribution
have similar parameterizations. A K-dimensional
Dirichlet can be parameterized with a concentration
parameter α &gt; 0 and a base distribution M0 =
(µ1, ... , µK_1), with µi E (0, 1).8 This parameteri-
zation has an intuitive interpretation: under these pa-
rameters, the average of independent samples from
the Dirichlet will converge to M0. That is, the aver-
age of the ith element of the samples will converge
to µi. Hence, the base distribution M0 characterizes
the sample mean. The concentration parameter α
only affects the variance of the draws.
Similarly, we can parameterize the Dirichlet pro-
cess with a concentration parameter α (that affects
only the variance) and a base distribution M0 that
determines the mean of the samples. Just as in the
finite Dirichlet case, M0 is simply a probability dis-
tribution, but now with countably infinite support:
all possible phrase pairs in our case. In practice, we
can use an unnormalized M0 (a base measure) by
appropriately rescaling α.
In our model, we select a base measure that
strongly prefers shorter phrases, encouraging the
model to use large phrases only when it has suffi-
cient evidence for them. We continue the model:
</bodyText>
<footnote confidence="0.588326714285714">
7Technical note: to simplify exposition, we restrict the dis-
cussion to settings such as ours where the base measure of the
DP has countable support.
8This parametrization is equivalent to the standard pseudo-
counts parametrization of K positive real numbers. The bi-
jection is given by α = EKi�1 &amp;i and pi = &amp;i/α, where
(&amp;1, ... , &amp;K) are the pseudo-counts.
</footnote>
<page confidence="0.982687">
319
</page>
<equation confidence="0.999606125">
θJ ∼ DP(M0, α)
M0(he,fi) = [Pf(f)PWA(e|f) · Pe(e)PWA(f|e)] 2
1 |f|
Pf (f = PG(|f|; ps) · 1 n
f
)
\
Pe(e) = PG(|e|;ps) · (n.Je|
</equation>
<bodyText confidence="0.999737434782609">
PWA is the IBM model 1 likelihood of one phrase
conditioned on the other (Brown et al., 1994). Pf
and Pe are uniform over types for each phrase
length: the constants nf and ne denote the vocab-
ulary size of the foreign and English languages, re-
spectively, and PG is a geometric distribution.
Above, θJ is drawn from a DP centered on the ge-
ometric mean of two joint distributions over phrase
pairs, each of which is composed of a monolingual
unigram model and a lexical translation component.
This prior has two advantages. First, we pressure
the model to use smaller phrases by increasing ps
(ps = 0.8 in experiments). Second, we encour-
age good phrase pairs by incorporating IBM Model
1 distributions. This use of word alignment distri-
butions is notably different from lexical weighting
or word alignment constraints: we are supplying
prior knowledge that phrases will generally follow
word alignments, though with enough corpus evi-
dence they need not (and often do not) do so in the
posterior samples. The model proved largely insen-
sitive to changes in the sparsity parameter α, which
we set to 100 for experiments.
</bodyText>
<subsectionHeader confidence="0.999598">
4.3 Unaligned phrases and the DP Prior
</subsectionHeader>
<bodyText confidence="0.999963190476191">
Introducing unaligned phrases invites further degen-
erate megaphrase behavior: a sentence pair can be
generated cheaply as two unaligned phrases that
each span an entire sentence. We attempted to place
a similar DP prior over θN, but surprisingly, this
modeling choice invoked yet another degenerate be-
havior. The DP prior imposes a rich-get-richer prop-
erty over the phrase pair distribution, strongly en-
couraging the model to reuse existing pairs rather
than generate new ones. As a result, common
words consistently aligned to null, even while suit-
able translations were present, simply because each
null alignment reinforced the next. For instance, the
was always unaligned.
Instead, we fix θN to a simple unigram model that
is uniform over word types. This way, we discour-
age unaligned phrases while focusing learning on θJ.
For simplicity, we reuse Pf(f) and Pe(e) from the
prior over θJ.
The 2 represents a choice of whether the aligned
phrase is in the foreign or English sentence.
</bodyText>
<subsectionHeader confidence="0.99381">
4.4 Collapsed Sampling with a DP Prior
</subsectionHeader>
<bodyText confidence="0.999947153846154">
Our entire model now has the general form
P(x, z, θJ); all other model parameters have been
fixed. Instead of searching for a suitable θJ,9 we
sample from the posterior distribution P(z|x) with
θJ marginalized out.
To this end, we convert our Gibbs sampler into
a collapsed Gibbs sampler10 using the Chinese
Restaurant Process (CRP) representation of the DP
(Aldous, 1985). With the CRP, we avoid the prob-
lem of explicitely representing samples from the
DP. CRP-based samplers have served the commu-
nity well in related language tasks, such as word seg-
mentation and coreference resolution (Goldwater et
al., 2006; Haghighi and Klein, 2007).
Under this representation, the probability of each
sampling outcome is a simple expression in terms
of the state of the rest of the training corpus (the
Markov blanket), rather than explicitly using θJ.
Let zm be the set of aligned phrase pair tokens ob-
served in the rest of the corpus. Then, when he, fi is
aligned (that is, neither e nor f are null), the condi-
tional probability for a pair he, fi takes the form:
where count(e,f)(zm) is the number of times that
he, fi appears in zm. We can write this expression
thanks to the exchangeability of the model. For fur-
ther exposition of this collapsed sampler posterior,
</bodyText>
<footnote confidence="0.901860666666667">
9For instance, using approximate MAP EM.
10A collapsed sampler is simply one in which the model pa-
rameters have been marginalized out.
</footnote>
<equation confidence="0.913551285714286">
�12 · Pe(e) if f =null
1
θN(he,fi) =
2 · Pf(f) if e = null
.
τ(he,fi|zm) = count(e,f)(zm) + α · M0(he, fi) ,
|zm |+ α
</equation>
<page confidence="0.929281">
320
</page>
<figure confidence="0.98288425">
Minimal extracted phrases
Sampled phrases
All extracted phrases
75
50
25
0
100
</figure>
<subsectionHeader confidence="0.984272">
4.6 A Hierarchical Dirichlet Process Prior
</subsectionHeader>
<bodyText confidence="0.99875875">
We also evaluate a hierarchical Dirichlet process
(HDP) prior over θJ, which draws monolingual dis-
tributions θE and θF from a DP and θJ from their
cross-product:
</bodyText>
<figure confidence="0.391458">
1x1 1x2 &amp; 2x1 1x3 &amp; 3x1 2x2 2x3 &amp; 3x2 3x3 and up
</figure>
<figureCaption confidence="0.9278515">
Figure 4: The distribution of phrase pair sizes (denoted
English length x foreign length) favors small phrases un-
</figureCaption>
<figure confidence="0.35827">
100
der the model.
</figure>
<figureCaption confidence="0.129798">
see Goldwater et al. (2006).11
</figureCaption>
<bodyText confidence="0.996963666666667">
The sampler remains exactly the same as de-
scribed in Section 3, except that the posterior con-
ditional probability of each outcome uses a revised
</bodyText>
<page confidence="0.510848">
25
</page>
<bodyText confidence="0.731544">
potential function ψDP((e, f)) _
�
</bodyText>
<equation confidence="0.9996255">
(1—p$) (1—pø) τ((e, f)) δ((e, f)) e &amp; f non-null
(1—p$) · pø · θN((e, f)) otherwise .
</equation>
<bodyText confidence="0.869278">
ψDP is like ψ, but the fixed θJ is replaced with the
constantly-updated τ function.
</bodyText>
<subsectionHeader confidence="0.997341">
4.5 Degeneracy Analysis
</subsectionHeader>
<bodyText confidence="0.970799">
Figure 4 shows a histogram of phrase pair sizes in
the distribution of expected counts under the model.
As reference, we show the size distribution of both
minimal and all phrase pairs extracted from word
alignments using the standard heuristic. Our model
tends to select minimal phrases, only using larger
phrases when well motivated.12
This result alone is important: a model-based
solution with no inference constraint has yielded
a non-degenerate distribution over phrase lengths.
Note that our sampler does find the degenerate solu-
tion quickly under a uniform prior, confirming that
the model, and not the inference procedure, is select-
ing these small phrases.
11Note that the expression for T changes slightly under con-
ditions where two phrase pairs being changed simultaneously
coincidentally share the same lexical content. Details of these
fringe conditions have been omitted for space, but were in-
cluded in our implementation.
12The largest phrase pair found was 13 English words by 7
Spanish words.
</bodyText>
<equation confidence="0.99996775">
θJ — DP(M&apos;0, α)
M&apos;0((e,f)) _ [θF(f)PWA(e|f) · θE(e)PWA(f|e)]�2
θF — DP(Pf, α&apos;)
θE — DP(Pe, α&apos;) .
</equation>
<bodyText confidence="0.585456">
This prior encourages novel phrase pairs to be com-
</bodyText>
<equation confidence="0.990008333333333">
3+ x 3+
posed of phrases that have been used before. In the
2 x 3, 3 x 2
</equation>
<bodyText confidence="0.510581">
sampler,we approximate table counts for θE and
</bodyText>
<equation confidence="0.993898333333333">
2 x 2
θF with their expectations, which can be computed
x 2, 2 x
</equation>
<bodyText confidence="0.990365">
from phrase pair counts (see the appendix of Gold-
water et al. (2006) for details). The HDP prior gives
a similar distribution over phrase sizes.
</bodyText>
<sectionHeader confidence="0.990961" genericHeader="method">
5 Translation Results
</sectionHeader>
<bodyText confidence="0.999795">
We evaluate our new estimates using the baseline
translation pipeline from the 2007 Statistical Ma-
chine Translation Workshop shared task.
</bodyText>
<subsectionHeader confidence="0.98616">
5.1 Baseline System
</subsectionHeader>
<bodyText confidence="0.999981">
We trained Moses on all Spanish-English Europarl
sentences up to length 20 (177k sentences) using
GIZA++ Model 4 word alignments and the grow-
diag-final-and combination heuristic (Koehn et al.,
2007; Och and Ney, 2003; Koehn, 2002), which
performed better than any alternative combination
heuristic.13 The baseline estimates (Heuristic) come
from extracting phrases up to length 7 from the word
alignment. We used a bidirectional lexicalized dis-
tortion model that conditions on both foreign and
English phrases, along with their orientations. Our
5-gram language model was trained on 38.3 million
words of Europarl using Kneser-Ney smoothing. We
report results with and without lexical weighting,
denoted lex.
We tuned and tested on development corpora for
the 2006 translation workshop. The parameters for
each phrase table were tuned separately using min-
imum error rate training (Och, 2003). Results are
</bodyText>
<footnote confidence="0.609134666666667">
13Sampling iteration time scales quadratically with sentence
length. Short sentences were chosen to speed up our experiment
cycle.
</footnote>
<page confidence="0.989042">
321
</page>
<table confidence="0.999899230769231">
Estimate Phrase NIST Exact
Pair BLEU Match
Count METEOR
Heuristic 4.4M 29.8 52.4
DP 0.6M 28.8 51.7
HDP 0.3M 29.1 52.0
DP-composed 3.7M 30.1 52.7
HDP-composed 3.1M 30.1 52.6
DP-smooth 4.8M 30.1 52.5
HDP-smooth 4.6M 30.2 52.7
Heuristic + lex 4.4M 30.5 52.9
DP-smooth + lex 4.8M 30.4 53.0
HDP-smooth + lex 4.6M 30.7 53.2
</table>
<tableCaption confidence="0.74496925">
Table 1: BLEU results for learned distributions improve
over a heuristic baseline. Estimate labels are described
fully in section 5.3. The label lex indicates the addition
of a lexical weighting feature.
</tableCaption>
<bodyText confidence="0.998581375">
scored with lowercased, tokenized NIST BLEU, and
exact match METEOR (Papineni et al., 2002; Lavie
and Agarwal, 2007).
The baseline system gives a BLEU score of 29.8,
which increases to 30.5 with lex, as shown in Table
1. For reference, training on all sentences of length
less than 40 (the shared task baseline default) gives
32.4 BLEU with lex.
</bodyText>
<subsectionHeader confidence="0.995133">
5.2 Learned Distribution Performance
</subsectionHeader>
<bodyText confidence="0.999967777777778">
We initialized the sampler with a configuration de-
rived from the word alignments generated by the
baseline. We greedily constructed a phrase align-
ment from the word alignment by identifying min-
imal phrase pairs consistent with the word align-
ment in each region of the sentence. We then ran
the sampler for 100 iterations through the training
data. Each iteration required 12 minutes under the
DP prior, and 30 minutes under the HDP prior. Total
running time for the HDP model neared two days on
an eight-processor machine with 16 Gb of RAM.
Estimating phrase counts under the DP prior de-
creases BLEU to 28.8, or 29.1 under the HDP prior.
This gap is not surprising: heuristic extraction dis-
covers many more phrase pairs than sampling. Note
that sacrificing only 0.7 BLEU while shrinking the
phrase table by 92% is an appealing trade-off in
resource-constrained settings.
</bodyText>
<subsectionHeader confidence="0.996836">
5.3 Increasing Phrase Pair Coverage
</subsectionHeader>
<bodyText confidence="0.999948476190476">
The estimates DP-composed and HDP-composed in
Table 1 take expectations of a more liberal count
function. While sampling, we count not only aligned
phrase pairs, but also larger ones composed of two or
more contiguous aligned pairs. This count function
is similar to the phrase pair extraction heuristic, but
never includes unaligned phrases in any way. Expec-
tations of these composite phrases still have a proba-
bilistic interpretation, but they are not the structures
we are directly modeling. Notably, these estimates
outperform the baseline by 0.3 BLEU without ever
extracting phrases from word alignments, and per-
formance increases despite a reduction in table size.
We can instead increase coverage by smooth-
ing the learned estimates with the heuristic counts.
The estimates DP-smooth and HDP-smooth add
counts extracted from word alignments to the sam-
pler’s running totals, which improves performance
by 0.4 BLEU over the baseline. This smoothing bal-
ances the lower-bias sampler counts with the lower-
variance heuristics ones.
</bodyText>
<sectionHeader confidence="0.999014" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999984357142857">
Our novel Gibbs sampler and nonparametric pri-
ors together address two open problems in learn-
ing phrase alignment models, approximating infer-
ence consistently and efficiently while avoiding de-
generate solutions. While improvements are mod-
est relative to the highly developed word-alignment-
centered baseline, we show for the first time com-
petitive results from a system that uses word align-
ments only for model initialization and smoothing,
rather than inference and estimation. We view this
milestone as critical to eventually developing a clean
probabilistic approach to machine translation that
unifies model structure across both estimation and
decoding, and decreases the use of heuristics.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99943">
David Aldous. 1985. Exchangeability and related topics.
In ´Ecole d’´et´e de probabiliti´es de Saint-Flour, Berlin.
Springer.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In The Con-
</reference>
<page confidence="0.980387">
322
</page>
<reference confidence="0.99942678021978">
ference for the Association for Machine Translation in
the Americas.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
Workshop on Syntax and Structure in Statistical Trans-
lation.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In The Annual Confer-
ence of the Association for Computational Linguistics:
Short Paper Track.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics Workshop on Statistical Ma-
chine Translation.
Thomas S Ferguson. 1973. A bayesian analysis of some
nonparametric problems. In Annals of Statistics.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In The Annual Conference of the
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In The Annual Conference of the Association
for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In The Annual Conference of the
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In The An-
nual Conference of the Association for Computational
Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In The Annual
Conference of the Association for Computational Lin-
guistics Workshop on Statistical Machine Translation.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In The Conference on Empirical Methods in
Natural Language Processing.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics
Workshop on Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In The Annual Conference of the Association
for Computational Linguistics.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In The Annual
Conference of the Association for Computational Lin-
guistics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
The Annual Conference of the Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.999389">
323
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865179">
<title confidence="0.99317">Sampling Alignment Structure under a Bayesian Translation Model</title>
<author confidence="0.979733">John DeNero</author>
<author confidence="0.979733">Alexandre Bouchard-Cˆot´e</author>
<author confidence="0.979733">Dan</author>
<affiliation confidence="0.9999595">Computer Science University of California,</affiliation>
<email confidence="0.889864">bouchard,</email>
<abstract confidence="0.999753076923077">We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrasebased translation systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Aldous</author>
</authors>
<title>Exchangeability and related topics. In ´Ecole d’´et´e de probabiliti´es de Saint-Flour,</title>
<date>1985</date>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<contexts>
<context position="26492" citStr="Aldous, 1985" startWordPosition="4355" endWordPosition="4356">ed phrases while focusing learning on θJ. For simplicity, we reuse Pf(f) and Pe(e) from the prior over θJ. The 2 represents a choice of whether the aligned phrase is in the foreign or English sentence. 4.4 Collapsed Sampling with a DP Prior Our entire model now has the general form P(x, z, θJ); all other model parameters have been fixed. Instead of searching for a suitable θJ,9 we sample from the posterior distribution P(z|x) with θJ marginalized out. To this end, we convert our Gibbs sampler into a collapsed Gibbs sampler10 using the Chinese Restaurant Process (CRP) representation of the DP (Aldous, 1985). With the CRP, we avoid the problem of explicitely representing samples from the DP. CRP-based samplers have served the community well in related language tasks, such as word segmentation and coreference resolution (Goldwater et al., 2006; Haghighi and Klein, 2007). Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using θJ. Let zm be the set of aligned phrase pair tokens observed in the rest of the corpus. Then, when he, fi is aligned (that is, neithe</context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>David Aldous. 1985. Exchangeability and related topics. In ´Ecole d’´et´e de probabiliti´es de Saint-Flour, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Constraining the phrase-based, joint probability statistical translation model.</title>
<date>2006</date>
<booktitle>In The Conference for the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="1750" citStr="Birch et al., 2006" startWordPosition="243" endWordPosition="246">Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the larger structures win o</context>
<context position="20607" citStr="Birch et al., 2006" startWordPosition="3360" endWordPosition="3363">der to correct the well-known degenerate behavior of the model. 4.1 Model Degeneracy The structure of our joint model penalizes explanations that use many small phrase pairs. Each phrase pair token incurs the additional expense of generation and distortion. In fact, the maximum likelihood estimate of the model puts mass on (e, f) pairs that span entire sentences, explaining the training corpus with one phrase pair per sentence. Previous phrase alignment work has primarily mitigated this tendency by constraining the inference procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause larg</context>
</contexts>
<marker>Birch, Callison-Burch, Osborne, 2006</marker>
<rawString>Alexandra Birch, Chris Callison-Burch, and Miles Osborne. 2006. Constraining the phrase-based, joint probability statistical translation model. In The Conference for the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="24031" citStr="Brown et al., 1994" startWordPosition="3945" endWordPosition="3948">dence for them. We continue the model: 7Technical note: to simplify exposition, we restrict the discussion to settings such as ours where the base measure of the DP has countable support. 8This parametrization is equivalent to the standard pseudocounts parametrization of K positive real numbers. The bijection is given by α = EKi�1 &amp;i and pi = &amp;i/α, where (&amp;1, ... , &amp;K) are the pseudo-counts. 319 θJ ∼ DP(M0, α) M0(he,fi) = [Pf(f)PWA(e|f) · Pe(e)PWA(f|e)] 2 1 |f| Pf (f = PG(|f|; ps) · 1 n f ) \ Pe(e) = PG(|e|;ps) · (n.Je| PWA is the IBM model 1 likelihood of one phrase conditioned on the other (Brown et al., 1994). Pf and Pe are uniform over types for each phrase length: the constants nf and ne denote the vocabulary size of the foreign and English languages, respectively, and PG is a geometric distribution. Above, θJ is drawn from a DP centered on the geometric mean of two joint distributions over phrase pairs, each of which is composed of a monolingual unigram model and a lexical translation component. This prior has two advantages. First, we pressure the model to use smaller phrases by increasing ps (ps = 0.8 in experiments). Second, we encourage good phrase pairs by incorporating IBM Model 1 distrib</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1994. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="10666" citStr="Cherry and Lin, 2007" startWordPosition="1698" endWordPosition="1701">That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentation</context>
<context position="20703" citStr="Cherry and Lin, 2007" startWordPosition="3375" endWordPosition="3378">ure of our joint model penalizes explanations that use many small phrase pairs. Each phrase pair token incurs the additional expense of generation and distortion. In fact, the maximum likelihood estimate of the model puts mass on (e, f) pairs that span entire sentences, explaining the training corpus with one phrase pair per sentence. Previous phrase alignment work has primarily mitigated this tendency by constraining the inference procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional mode</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In The Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics: Short Paper Track.</booktitle>
<contexts>
<context position="1521" citStr="DeNero and Klein, 2008" startWordPosition="211" endWordPosition="214">r in training bitexts. Since bitexts do not come segmented and aligned into phrase pairs, these counts are typically gathered by fixing a word alignment and applying phrase extraction heuristics to this word-aligned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values</context>
<context position="10044" citStr="DeNero and Klein, 2008" startWordPosition="1600" endWordPosition="1603">malizing these expected counts yields estimates for the features 0(e|f) and 0(f|e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In The Annual Conference of the Association for Computational Linguistics: Short Paper Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1729" citStr="DeNero et al., 2006" startWordPosition="239" endWordPosition="242">ned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the la</context>
<context position="10419" citStr="DeNero et al. (2006)" startWordPosition="1664" endWordPosition="1667">also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to app</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In The Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<booktitle>In Annals of Statistics.</booktitle>
<contexts>
<context position="21626" citStr="Ferguson, 1973" startWordPosition="3525" endWordPosition="3526">ed with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6For experiments, we ran the sampler for 100 iterations. 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over BJ, the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an appropriate prior. A draw from a Kdimensional Dirichlet distribution is a list of K real numbers in [0, 1] that sum to one, which can be interpreted as a distribution over K phrase pair types. However, since the event space of possible phrase pairs is in principle unbounded, we instead use a Dirichlet process. A draw from a DP is a countably infinite list of real numbers in [0, 1] that sum to one, which we interpret as a distribution over a countably infinite list of phrase pair types.</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S Ferguson. 1973. A bayesian analysis of some nonparametric problems. In Annals of Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9778" citStr="Finkel et al., 2007" startWordPosition="1557" endWordPosition="1560">me small local change. The samples zz are guaranteed (in the limit) to consistently approximate the conditional distribution P(z|x, 0) (or P(z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features 0(e|f) and 0(f|e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9756" citStr="Goldwater et al., 2006" startWordPosition="1553" endWordPosition="1556">fers from the last by some small local change. The samples zz are guaranteed (in the limit) to consistently approximate the conditional distribution P(z|x, 0) (or P(z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features 0(e|f) and 0(f|e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during</context>
<context position="16155" citStr="Goldwater et al. (2006)" startWordPosition="2612" endWordPosition="2615">steps involved in applying the FLIP operator. The Markov blanket freezes all segmentations except English position 1 and all alignments except those for Ellos and The boys. The blanket also freezes the number of alignments, which disallows the lower right outcome. 3.3 The FLIP operator SWAP can arbitrarily shuffle alignments, but we need a second operator to change the actual phrase boundaries. The FLIP operator changes the status of a single segmentation position5 to be either a phrase boundary or not. In this sense FLIP is a bilingual analog of the segmentation boundary flipping operator of Goldwater et al. (2006). Figure 3 diagrams the operator and its Markov blanket. First, FLIP chooses any between-word position in either sentence. The outcome sets for FLIP vary based on the current segmentation and adjacent alignments, and are depicted in Figure 2. Again, for FLIP to satisfy the Gibbs conditions, we must augment its Markov blanket to freeze not only all other segmentation points and alignments, but also the number of aligned phrase pairs. Otherwise, we end up allowing outcomes from which 5A segmentation position is a position between two words that is also potentially a boundary between two phrases </context>
<context position="26731" citStr="Goldwater et al., 2006" startWordPosition="4392" endWordPosition="4395">a DP Prior Our entire model now has the general form P(x, z, θJ); all other model parameters have been fixed. Instead of searching for a suitable θJ,9 we sample from the posterior distribution P(z|x) with θJ marginalized out. To this end, we convert our Gibbs sampler into a collapsed Gibbs sampler10 using the Chinese Restaurant Process (CRP) representation of the DP (Aldous, 1985). With the CRP, we avoid the problem of explicitely representing samples from the DP. CRP-based samplers have served the community well in related language tasks, such as word segmentation and coreference resolution (Goldwater et al., 2006; Haghighi and Klein, 2007). Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using θJ. Let zm be the set of aligned phrase pair tokens observed in the rest of the corpus. Then, when he, fi is aligned (that is, neither e nor f are null), the conditional probability for a pair he, fi takes the form: where count(e,f)(zm) is the number of times that he, fi appears in zm. We can write this expression thanks to the exchangeability of the model. For further </context>
<context position="28128" citStr="Goldwater et al. (2006)" startWordPosition="4646" endWordPosition="4649">zed out. �12 · Pe(e) if f =null 1 θN(he,fi) = 2 · Pf(f) if e = null . τ(he,fi|zm) = count(e,f)(zm) + α · M0(he, fi) , |zm |+ α 320 Minimal extracted phrases Sampled phrases All extracted phrases 75 50 25 0 100 4.6 A Hierarchical Dirichlet Process Prior We also evaluate a hierarchical Dirichlet process (HDP) prior over θJ, which draws monolingual distributions θE and θF from a DP and θJ from their cross-product: 1x1 1x2 &amp; 2x1 1x3 &amp; 3x1 2x2 2x3 &amp; 3x2 3x3 and up Figure 4: The distribution of phrase pair sizes (denoted English length x foreign length) favors small phrases un100 der the model. see Goldwater et al. (2006).11 The sampler remains exactly the same as described in Section 3, except that the posterior conditional probability of each outcome uses a revised 25 potential function ψDP((e, f)) _ � (1—p$) (1—pø) τ((e, f)) δ((e, f)) e &amp; f non-null (1—p$) · pø · θN((e, f)) otherwise . ψDP is like ψ, but the fixed θJ is replaced with the constantly-updated τ function. 4.5 Degeneracy Analysis Figure 4 shows a histogram of phrase pair sizes in the distribution of expected counts under the model. As reference, we show the size distribution of both minimal and all phrase pairs extracted from word alignments usi</context>
<context position="29919" citStr="Goldwater et al. (2006)" startWordPosition="4952" endWordPosition="4956">incidentally share the same lexical content. Details of these fringe conditions have been omitted for space, but were included in our implementation. 12The largest phrase pair found was 13 English words by 7 Spanish words. θJ — DP(M&apos;0, α) M&apos;0((e,f)) _ [θF(f)PWA(e|f) · θE(e)PWA(f|e)]�2 θF — DP(Pf, α&apos;) θE — DP(Pe, α&apos;) . This prior encourages novel phrase pairs to be com3+ x 3+ posed of phrases that have been used before. In the 2 x 3, 3 x 2 sampler,we approximate table counts for θE and 2 x 2 θF with their expectations, which can be computed x 2, 2 x from phrase pair counts (see the appendix of Goldwater et al. (2006) for details). The HDP prior gives a similar distribution over phrase sizes. 5 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come fr</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26758" citStr="Haghighi and Klein, 2007" startWordPosition="4396" endWordPosition="4399">del now has the general form P(x, z, θJ); all other model parameters have been fixed. Instead of searching for a suitable θJ,9 we sample from the posterior distribution P(z|x) with θJ marginalized out. To this end, we convert our Gibbs sampler into a collapsed Gibbs sampler10 using the Chinese Restaurant Process (CRP) representation of the DP (Aldous, 1985). With the CRP, we avoid the problem of explicitely representing samples from the DP. CRP-based samplers have served the community well in related language tasks, such as word segmentation and coreference resolution (Goldwater et al., 2006; Haghighi and Klein, 2007). Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using θJ. Let zm be the set of aligned phrase pair tokens observed in the rest of the corpus. Then, when he, fi is aligned (that is, neither e nor f are null), the conditional probability for a pair he, fi takes the form: where count(e,f)(zm) is the number of times that he, fi appears in zm. We can write this expression thanks to the exchangeability of the model. For further exposition of this collapse</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9609" citStr="Johnson et al., 2007" startWordPosition="1530" endWordPosition="1533">gnment, state z0, which sets all latent variables to some initial configuration. We then produce a sequence of sample states zz, each of which differs from the last by some small local change. The samples zz are guaranteed (in the limit) to consistently approximate the conditional distribution P(z|x, 0) (or P(z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features 0(e|f) and 0(f|e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approx</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="30373" citStr="Koehn et al., 2007" startWordPosition="5021" endWordPosition="5024">ximate table counts for θE and 2 x 2 θF with their expectations, which can be computed x 2, 2 x from phrase pair counts (see the appendix of Goldwater et al. (2006) for details). The HDP prior gives a similar distribution over phrase sizes. 5 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parame</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<contexts>
<context position="30406" citStr="Koehn, 2002" startWordPosition="5029" endWordPosition="5030"> with their expectations, which can be computed x 2, 2 x from phrase pair counts (see the appendix of Goldwater et al. (2006) for details). The HDP prior gives a similar distribution over phrase sizes. 5 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phrase table were t</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="31854" citStr="Lavie and Agarwal, 2007" startWordPosition="5250" endWordPosition="5253">e NIST Exact Pair BLEU Match Count METEOR Heuristic 4.4M 29.8 52.4 DP 0.6M 28.8 51.7 HDP 0.3M 29.1 52.0 DP-composed 3.7M 30.1 52.7 HDP-composed 3.1M 30.1 52.6 DP-smooth 4.8M 30.1 52.5 HDP-smooth 4.6M 30.2 52.7 Heuristic + lex 4.4M 30.5 52.9 DP-smooth + lex 4.8M 30.4 53.0 HDP-smooth + lex 4.6M 30.7 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3. The label lex indicates the addition of a lexical weighting feature. scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007). The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1. For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex. 5.2 Learned Distribution Performance We initialized the sampler with a configuration derived from the word alignments generated by the baseline. We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence. We then ran the sampler for 100 iterations through the </context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In The Annual Conference of the Association for Computational Linguistics Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Daniel Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1708" citStr="Marcu and Wong, 2002" startWordPosition="235" endWordPosition="238">tics to this word-aligned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and sma</context>
<context position="5173" citStr="Marcu and Wong (2002)" startWordPosition="764" endWordPosition="767">as translation features. In this paper, we compute the expected counts of phrase pairs in the training data according to our model, and derive features from these expected counts. This approach endows phrase pair scores with well-defined semantics relative to a probabilistic model. Practically, phrase models can discover high-quality phrase pairs that often elude heuristics, as in Figure 1. In addition, the modelbased approach fits neatly into the framework of statistical learning theory for unsupervised problems. 2.1 Generative Model Description We first describe the symmetric joint model of Marcu and Wong (2002), which we will extend. A two-step generative process constructs an ordered set of English phrases e1:m, an ordered set of foreign phrases f1:,,, and a phrase-to-phrase alignment between them, a = {(j, k)} indicating that hej, fki is an aligned pair. 1. Choose a number of components ` and generate each of ` phrase pairs independently. 2. Choose an ordering for the phrases in the foreign language; the ordering for English is fixed by the generation order.1 1We choose the foreign to reorder without loss of generality. In this process, m = n = |a|; all phrases in both sentences are aligned one-to</context>
<context position="6488" citStr="Marcu and Wong (2002)" startWordPosition="997" endWordPosition="1000">rameter p$: P(`) = PG(`; p$) = p$ · (1 − p$)&amp;quot; . Each aligned phrase pair he, fi is drawn from a multinomial distribution θ7 which is unknown. We fix a simple distortion model, setting the probability of a permutation of the foreign phrases proportional to the product of position-based distortion penalties for each phrase: P(a|{he, fi}) ∝ 11 δ(a) aEa δ(a = (j, k)) = b|po�(ej)−po8(fk)·3 |, where pos(·) denotes the word position of the start of a phrase, and s the ratio of the length of the English to the length of the foreign sentence. This positional distortion model was deemed to work best by Marcu and Wong (2002). We can now state the joint probability for a phrase-aligned sentence consisting of ` phrase pairs: P({he,fi},a) = PG(`;p$)P(a|{he,fi}) 11 θ7(he,fi) . (e,f) While this model has several free parameters in addition to θ7, we fix them to reasonable values to focus learning on the phrase pair distribution.2 2.2 Unaligned Phrases Sentence pairs do not always contain equal information on both sides, and so we revise the generative story to include unaligned phrases in both sentences. When generating each component of a sentence pair, we first decide whether to generate an aligned phrase pair or, w</context>
<context position="10264" citStr="Marcu and Wong (2002)" startWordPosition="1639" endWordPosition="1642"> search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and Daniel Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic realignment models for machine translation.</title>
<date>2007</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="21281" citStr="May and Knight (2007)" startWordPosition="3469" endWordPosition="3472">positional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6For experiments, we ran the sampler for 100 iterations. 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over BJ, the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an appropriate prior. A draw from a Kdimensional Dirichlet distribution is a list of K real numbers in [0, 1] that sum to one, which can be interpre</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jonathan May and Kevin Knight. 2007. Syntactic realignment models for machine translation. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>Chris Quirk</author>
</authors>
<title>An iterativelytrained segmentation-free phrase translation model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="21139" citStr="Moore and Quirk (2007)" startWordPosition="3445" endWordPosition="3448">e procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6For experiments, we ran the sampler for 100 iterations. 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over BJ, the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an app</context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Robert Moore and Chris Quirk. 2007. An iterativelytrained segmentation-free phrase translation model for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="30392" citStr="Och and Ney, 2003" startWordPosition="5025" endWordPosition="5028">for θE and 2 x 2 θF with their expectations, which can be computed x 2, 2 x from phrase pair counts (see the appendix of Goldwater et al. (2006) for details). The HDP prior gives a similar distribution over phrase sizes. 5 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phras</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31067" citStr="Och, 2003" startWordPosition="5127" endWordPosition="5128">nation heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phrase table were tuned separately using minimum error rate training (Och, 2003). Results are 13Sampling iteration time scales quadratically with sentence length. Short sentences were chosen to speed up our experiment cycle. 321 Estimate Phrase NIST Exact Pair BLEU Match Count METEOR Heuristic 4.4M 29.8 52.4 DP 0.6M 28.8 51.7 HDP 0.3M 29.1 52.0 DP-composed 3.7M 30.1 52.7 HDP-composed 3.1M 30.1 52.6 DP-smooth 4.8M 30.1 52.5 HDP-smooth 4.6M 30.2 52.7 Heuristic + lex 4.4M 30.5 52.9 DP-smooth + lex 4.8M 30.4 53.0 HDP-smooth + lex 4.6M 30.7 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31828" citStr="Papineni et al., 2002" startWordPosition="5246" endWordPosition="5249">cle. 321 Estimate Phrase NIST Exact Pair BLEU Match Count METEOR Heuristic 4.4M 29.8 52.4 DP 0.6M 28.8 51.7 HDP 0.3M 29.1 52.0 DP-composed 3.7M 30.1 52.7 HDP-composed 3.1M 30.1 52.6 DP-smooth 4.8M 30.1 52.5 HDP-smooth 4.6M 30.2 52.7 Heuristic + lex 4.4M 30.5 52.9 DP-smooth + lex 4.8M 30.4 53.0 HDP-smooth + lex 4.6M 30.7 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3. The label lex indicates the addition of a lexical weighting feature. scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007). The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1. For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex. 5.2 Learned Distribution Performance We initialized the sampler with a configuration derived from the word alignments generated by the baseline. We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence. We then ran the sampler for 1</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11097" citStr="Snyder and Barzilay (2008)" startWordPosition="1771" endWordPosition="1774">m pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentations and alignments for a sequence at once – requires a #P-hard computation. While this asymptotic complexity was apparently not prohibitive in the case of morphological alignment, where the sequences are short, it is prohibitive in phrase alignment, where the sentences are often very long. 3.2 Sampling with the SWAP Operator Our Gibbs sampler repeatedly applies each of five operators to each position in each training sentence pai</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9586" citStr="Teh, 2006" startWordPosition="1528" endWordPosition="1529">ion and alignment, state z0, which sets all latent variables to some initial configuration. We then produce a sequence of sample states zz, each of which differs from the last by some small local change. The samples zz are guaranteed (in the limit) to consistently approximate the conditional distribution P(z|x, 0) (or P(z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features 0(e|f) and 0(f|e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10687" citStr="Zhang et al., 2008" startWordPosition="1702" endWordPosition="1705">run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentations and alignments for </context>
<context position="20724" citStr="Zhang et al., 2008" startWordPosition="3379" endWordPosition="3382"> penalizes explanations that use many small phrase pairs. Each phrase pair token incurs the additional expense of generation and distortion. In fact, the maximum likelihood estimate of the model puts mass on (e, f) pairs that span entire sentences, explaining the training corpus with one phrase pair per sentence. Previous phrase alignment work has primarily mitigated this tendency by constraining the inference procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance th</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>