<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.065453">
<title confidence="0.981365">
TKLBLIIR: Detecting Twitter Paraphrases with TweetingJay
</title>
<author confidence="0.9811905">
Mladen Karan&apos;, Goran Glavaˇs&apos;, Jan ˇSnajder&apos;, Bojana Dalbelo Baˇsi´c&apos;,
Ivan Vuli´c2, and Marie-Francine Moens2
</author>
<affiliation confidence="0.910532">
&apos;University of Zagreb, Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab, Unska 3, 10000 Zagreb, Croatia
</affiliation>
<email confidence="0.824154">
{goran.glavas, mladen.karan, jan.snajder, bojana.dalbelo}@fer.hr
</email>
<address confidence="0.6211855">
2KU Leuven, Department of Computer Science
Language Intelligence &amp; Information Retrieval Group, Celestijnenlaan 200A, Leuven, Belgium
</address>
<email confidence="0.99579">
{ivan.vulic, sien.moens}@cs.kuleuven.be
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986">
When tweeting on a topic, Twitter users often
post messages that convey the same or similar
meaning. We describe TweetingJay, a system
for detecting paraphrases and semantic simi-
larity of tweets, with which we participated in
Task 1 of SemEval 2015. TweetingJay uses a
supervised model that combines semantic over-
lap and word alignment features, previously
shown to be effective for detecting semantic
textual similarity. TweetingJay reaches 65.9%
F1-score and ranked fourth among the 18 par-
ticipating systems. We additionally provide
an analysis of the dataset and point to some
peculiarities of the evaluation setup.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836909090909">
Recognizing tweets that convey the same meaning
(paraphrases) or similar meaning is useful in applica-
tions such as event detection (Petrovi´c et al., 2012),
tweet summarization (Yang et al., 2011), and tweet
retrieval (Naveed et al., 2011). Paraphrase detection
in tweets is a more challenging task than paraphrase
detection in other domains such as news (Xu et al.,
2013). Besides brevity (max. 140 characters), tweets
exhibit all the irregularities typical of social media
text (Baldwin et al., 2013), such as informality, un-
grammaticality, disfluency, and excessive use of jar-
gon.
In this paper we present the TweetingJay system
for detecting paraphrases in tweets, with which we
participated in Task 1 of SemEval 2015 evaluation
exercise (Xu et al., 2015). Our system builds on
findings from a large body of work on semantic tex-
tual similarity (STS) (ˇSari´c et al., 2012; Sultan et al.,
2014) and recent breakthroughs in distributed word
representations (Mikolov et al., 2013a). We design a
set of measures that capture the semantic similarity
of tweets and train a support vector machine (SVM)
using these measures as features. Positioning of our
system at rank four among 18 teams, with only point
and a half lower performance compared to the the
best-performing system, suggests that STS measures
are useful for detecting paraphrases in Twitter. We
make our system freely available.1
Besides providing the description of the Tweeting-
Jay system, in this paper we analyze the evaluation
setup, with special focus on the provided dataset and
its subsets (train, validation, and test), and discuss
the stability of the evaluation results.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999853666666667">
There is a large body of work on automated para-
phrase detection; see (Madnani and Dorr, 2010) for a
comprehensive overview. The majority of research
efforts focus on detecting paraphrases in standard
texts such as news (Das and Smith, 2009; Madnani
et al., 2012) or artificially generated text (Madnani
et al., 2012). State-of-the-art approaches typically
combine several measures of semantic similarity be-
tween text fragments. For instance, Madnani et al.
(2012) achieve state-of-the-art performance by com-
bining eight different machine translation metrics in
a supervised fashion.
A task closely related to paraphrase detection is
semantic textual similarity (STS), introduced at Se-
mEval 2012 (Agirre et al., 2012). There is now a
</bodyText>
<footnote confidence="0.988796">
1http://takelab.fer.hr/tweetingjay
</footnote>
<page confidence="0.980687">
70
</page>
<bodyText confidence="0.934015666666666">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
significant amount of work on this task. The best
performing STS systems employ various methods
for aligning semantically corresponding words or
otherwise quantifying the amount of semantically
congruent content between two sentences (Sultan et
al., 2014; ˇSari´c et al., 2012).
In contrast, STS research on Twitter data has been
scarce. Zanzotto et al. (2011) detect content redun-
dancy between tweets, where redundant means para-
phrased or entailed content. They achieve reasonable
performance with SVM using vector-comparison and
syntactic tree kernels. Xu et al. (2014) propose MUL-
TIP, a latent variable model for joint inference of
correspondence of words and sentences. An unsu-
pervised model based on representing sentences in
latent space is presented by Guo and Diab (2012).
</bodyText>
<sectionHeader confidence="0.998499" genericHeader="method">
3 TweetingJay
</sectionHeader>
<bodyText confidence="0.999459">
TweetingJay is essentially a supervised machine
learning model, which employs a number of semantic
similarity features (18 features in total). Because the
number of features is relatively small, we use SVM
with a non-linear (RBF) kernel. Our features can be
divided into (1) semantic overlap features, most of
which are adaptations of STS features proposed by
ˇSari´c et al. (2012), and (2) word alignment features,
based on (a) the output of the word alignment model
by Sultan et al. (2014) and (b) a re-implementation
of the MULTIP model by Xu et al. (2014).
In the dataset provided by the organizers, each
tweet is associated with a topic, with 10 to 100 tweet
pairs per topic. An important preprocessing step is
to remove tokens that can be found in the name of a
topic. For example, for the topic “Roberto Mancini”,
we trim the tweets “Roberto Mancini gets the boot
from the Man City” and “City sacked Mancini” to
“gets the boot from the Man City” and “City sacked”,
respectively, and then compute the features on the
trimmed tweets. The rationale is that, given a topic,
there is an overlap in topic words between both para-
phrase and non-paraphrase tweet pairs, which dimin-
ishes the discriminative power of the model’s com-
parison features.
</bodyText>
<subsectionHeader confidence="0.999051">
3.1 Semantic Overlap Features
</subsectionHeader>
<bodyText confidence="0.999178727272727">
Semantic overlap features compare the content words
(nouns, verbs, adjectives, adverbs, and numbers).
Ngram overlap. We compute the number of
matching n-grams between two tweets. This number
is normalized by the length of the first and the second
tweet, respectively, and the harmonic mean of these
two measures is taken as the similarity score. These
features are computed separately for unigrams and
bigrams. We also compute a weighted version by
weighting the matched words w with their informa-
tion content:
</bodyText>
<equation confidence="0.999691333333333">
freq(w) + 1
ic(w) = −log E
w&apos;∈C freq(w&apos;) + 1
</equation>
<bodyText confidence="0.999380291666667">
where C is the set of all words in the corpus and
freq(w) is the word’s frequency. We obtained the
frequencies from the Google Books Ngrams (GBN)
(Michel et al., 2011). In the weighted version of the
ngram overlap, the overlap is normalized by the sum
of information contents of all words in the first and
second tweet, respectively, and the resulting similar-
ity score is the harmonic mean of these two scores.
Greedy word alignment overlap (GWAO). To
compute this feature, we iteratively pair the words –
one word from each tweet – according to their seman-
tic similarity. In each iteration we greedily select the
pair of words with the largest semantic similarity, and
remove the words from their corresponding tweets,
until no words are left in shorter of the two tweets.
The similarity between words is computed as the
cosine between their corresponding 300-dimension
embedding vectors obtained using word2vec tool
(Mikolov et al., 2013b) on a 100 billion words por-
tion of the Google News dataset. Let P(t1, t2) be
the set of word pairs obtained through the alignment
on a pair of tweets (t1, t2) and let vec(w) be the em-
bedding vector of the word w. The GWAO score is
computed as:
</bodyText>
<equation confidence="0.998209">
�gwao(t1, t2) = α · cos (vec(w1), vec(w2))
(w1,w2)
∈P(t1,t2)
</equation>
<bodyText confidence="0.999873833333333">
where α is the larger of the information contents
of the two words, α = max (ic(w1), ic(w1)). The
gwao(t1, t2) score is normalized with the sum of
information contents of words from t1 and t2, respec-
tively, and the harmonic mean of the two normalized
scores is taken as the feature value.
</bodyText>
<page confidence="0.993758">
71
</page>
<bodyText confidence="0.99984425">
Tweet embedding similarity. Linear combina-
tions of word embedding vectors have been shown
to correspond well to the semantic composition of
the individual words (Mikolov et al., 2013a; Mikolov
et al., 2013b). Building on this finding, we embed
a tweet as a weighted sum of the embeddings of its
content words, where we use information content of
words as their weights:
</bodyText>
<equation confidence="0.9647655">
�vec(t) = ic(w) · vec(w).
w∈t
</equation>
<bodyText confidence="0.998463090909091">
As the tweet embedding similarity, we simply com-
pute the cosine between the corresponding tweet em-
beddings, i.e., cos (vec(t1), vec(t2)).
Topic-specific information content. While infor-
mation content computed on a general corpus such
as GBN indicates how informative the word is in
general, we also wanted to have a measure of how
informative each word is within a tweet’s topic. To
this end we also compute topic-specific versions of
all the above features using topic-specific instead of
GBN information contents.
</bodyText>
<subsectionHeader confidence="0.998202">
3.2 Word Alignment Features
</subsectionHeader>
<bodyText confidence="0.999946">
We adopt the word alignment features from two
alignment-based systems: (1) the DLS@CU system
of Sultan et al. (2014), which achieved the best per-
formance on the STS task at SemEval 2014 (Agirre et
al., 2014), and (2) our implementation of the MULTIP
latent variables model (Xu et al., 2014), which uti-
lizes the concept of an anchor: a pair of semantically
aligned words from a paraphrased pair of tweets.
Aligned word pairs (AWP). A state-of-the art
monolingual word alignment model by Sultan et al.
(2014) outputs pairs of semantically aligned words
between two given sentences.2 We used the output
of the DLS@CU model to generate two features: (1)
the raw count of the aligned word pairs, and (2) the
normalized count, which is the harmonic mean of the
scores obtained by normalizing the raw count with
the length of the first and second tweet, respectively.
We computed two versions for both of these features,
one considering all the tokens in tweets, and the other
taking into account only content words.
</bodyText>
<footnote confidence="0.8989085">
2https://github.com/ma-sultan/
monolingual-word-aligner
</footnote>
<bodyText confidence="0.998956533333333">
Anchor count (ANC). We re-implemented the
MULTIP model of Xu et al. (2014).3 As anchor candi-
dates we consider all pairs of content words from the
two tweets. We use a minimalistic set of features in-
cluding (1) Levenshtein distance between candidate
words, (2) several binary features indicating related-
ness of words (e.g., lowercased tokens match, POS-
tags match), and (3) semantic similarity obtained as
the cosine of word embeddings, obtained with the
GloVe model (Pennington et al., 2014) trained on
Twitter data.4 To account for feature interactions,
following (Xu et al., 2014), we also use conjunction
features. We use the number of anchors identified by
this method for a pair of tweets as a feature for our
SVM model.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99744">
Each team was allowed to submit two runs on the
test set provided by the task organizers (Xu et al.,
2015). Participants were provided with a training set
(13,063 pairs) and a development set (4,727 pairs).
We used the train and development set to optimize
the hyperparameters C and -y of our SVM model
with the RBF kernel. For the final evaluation, the
organizers used a test set of 972 tweet pairs.
Feature sets. We divided the features in three
groups: (1) semantic overlap features (SO) from Sec-
tion 3.1, (2) aligned word pairs (AWP) features, and
(3) the anchor count feature (ANC) from Section 3.2.
Model optimization. There are three ways how
the optimization of the SVM model (hyperparameters
C and -y) could have been carried out: (1) training and
optimization on the train set using 10-folded cross-
validation, with no use of the development set (model
M1); (2) training on the train set and optimization on
the development set (model M2), and (3) training on
the union of the train and development set using 10-
folded cross-validation (model M3). Following the
advice of the task organizers, we removed debatable
cases from both the train and dev sets. We submitted
models M1 and M2 for the official evaluation (our
team name was TKLBLIIR).
</bodyText>
<footnote confidence="0.98319225">
3We obtain lower results on the test set (61.3% Fl vs. 69.6%).
This is likely caused by the use of slightly different features and
perhaps by differences in implementation.
4http://nlp.stanford.edu/projects/glove/
</footnote>
<page confidence="0.99177">
72
</page>
<table confidence="0.9999426">
Team P R Fl Rank
ASOBEK 68.0 66.9 67.4 1
MITRE 80.6 56.9 66.7 2
ECNU 76.7 58.3 66.2 3
FBK-HLT 68.5 63.4 65.9 4
TKLBLIIR M1 64.5 67.4 65.9 5
TKLBLIIR M2 46.1 81.7 59.0 19
MULTIP 71.9 67.4 69.6 –
Baseline (log.reg.) 67.9 52.0 58.9 21
Baseline (WTMF) 45.0 66.3 53.6 28
</table>
<tableCaption confidence="0.982895">
Table 1: Official SemEval Task 1 evaluation.
</tableCaption>
<table confidence="0.999713666666667">
M1 M2 M3
Features dev test dev test dev test
SO 63.3 63.4 64.9 59.0 63.3 61.5
SO+AWP 64.0 61.6 64.7 60.4 64.0 61.6
SO+ANC 60.8 65.9 64.6 60.8 64.5 62.5
SO+AWP+ANC 64.1 63.2 64.9 59.0 64.4 61.2
</table>
<tableCaption confidence="0.993989">
Table 2: Model optimization using different datasets.
</tableCaption>
<subsectionHeader confidence="0.996253">
4.1 Official Results
</subsectionHeader>
<bodyText confidence="0.999986181818182">
A subset of the official ranking is shown in Table
4.1. Our model M1 ranked fourth (sharing that place
with FBK-HLT) in the official evaluation with a 1.5%
lower Fl score than the best-performing system. Our
model M2 outperforms both baselines. The state-of-
the-art model MULTIP outperforms all participating
systems. There is a notable performance gap be-
tween our two runs. We believe this comes from
the high sensitivity of the performance on the test
set to small changes in hyperparameter values. We
elaborate more on this in the next section.
</bodyText>
<subsectionHeader confidence="0.997782">
4.2 Dataset Analysis
</subsectionHeader>
<bodyText confidence="0.999994393939394">
In Table 4.2 we show the performance of the models
M1, M2, and M3 on the development and test set.
We observe an unusual behavior for all three mod-
els: a model that performs good on the development
set typically performs bad on the test set, and vice
versa. Furthermore, optimal cross-validated Fl per-
formance on the train set is 72%, which is 7 points
above the best performance on the validation set. We
believe this may be indicative of significant differ-
ences in the distributions underlying the datasets.
To investigate this further, we applied the
Kolmogorov-Smirnov two-sample goodness-of-fit
test (K-S test) (Daniel, 1990) for each of the used
features to determine whether the train set is drawn
from the same distribution as the development and
test set. The K-S test is a nonparametric test that
determines whether two independent samples differ
in some respect, both in the measure of locations
(means, median) and the shapes of the distributions
(skewness, dispersion, kurtosis). The assumptions
for the K-S test (independence of random samples
and continuous variables) are met for all our features.
We tested all features at the level of significance of
0.05 and rejected the null hypothesis for all features
but one (bigram overlap). This confirms our initial
assumption that the features in the train set are not
identically distributed to those in the test set, bring-
ing into question the representativeness of the test
set. Reasons for this may include different annotation
sources (crowdsourcing vs experts) and differences in
time periods of tweets. Moreover, due to differences
in the datasets, the performance is very much affected
by the choice of the model optimization setup.
</bodyText>
<subsectionHeader confidence="0.99823">
4.3 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999952">
Due to volatile performance, it is difficult to say much
about which features are most useful. However, we
have observed consistent performance boosts in all
settings when introducing topic-specific versions of
features.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999957714285714">
We described TweetingJay, a supervised model for
detecting Twitter paraphrases with which we partici-
pated in Task 1 of SemEval 2015. TweetingJay relies
on features capturing semantic similarity and word
alignments between tweets and achieves performance
comparable to best-performing models on the task.
On the methodological side, we investigated the
cause for unusual behavior of our models on the
different datasets. Our preliminary statistical analysis
of the datasets seems to suggest that the underlying
distributions datasets are significantly different. We
believe this makes the performance estimates less
reliable and suggest that the results should be taken
with caution.
</bodyText>
<page confidence="0.998437">
73
</page>
<sectionHeader confidence="0.989609" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973921686747">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
SemEval 2012, pages 385–393.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,
Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada
Mihalcea, German Rigau, and Janyce Wiebe. 2014.
SemEval-2014 task 10: Multilingual semantic textual
similarity. pages 81–91.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources. In Pro-
ceedings of IJCNLP 2013, pages 356–364.
Wayne W. Daniel. 1990. Applied nonparametric statistics.
The Duxbury advanced series in statistics and decision
sciences.
Dipanjan Das and Noah A Smith. 2009. Paraphrase identi-
fication as probabilistic quasi-synchronous recognition.
In Proceedings ofACL 2009, pages 468–476.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings ofACL 2012, pages
864–872.
Nitin Madnani and Bonnie J Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3):341–
387.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of NAACL
2012, pages 182–190.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K Gray, Joseph P Pickett,
Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant,
et al. 2011. Quantitative analysis of culture using mil-
lions of digitized books. Science, 331(6014):176–182.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositionality.
In Proceedings of NIPS 2013, pages 3111–3119.
Nasir Naveed, Thomas Gottron, J´erˆome Kunegis, and Ari-
fah Che Alhadi. 2011. Searching microblogs: coping
with sparsity and document quality. In Proceedings of
CIKM 2011, pages 183–188.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP 2014, pages
1532–1541.
Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2012.
Using paraphrases for improving first story detection
in news and twitter. In Proceedings of NAACL 2012,
pages 338–346.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: systems
for measuring semantic text similarity. In Proceedings
of SemEval 2012, pages 441–448.
Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2014. DLS@CU: Sentence similarity from word align-
ment. In Proceedings of SemEval 2014, pages 241–
245.
Wei Xu, Alan Ritter, and Ralph Grishman. 2013. Gath-
ering and generating paraphrases from twitter with ap-
plication to normalization. In Proceedings of the Sixth
Workshop on Building and Using Comparable Corpora,
pages 121–128.
Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. 2014. Extracting lexically
divergent paraphrases from Twitter. Transactions of
the Association for Computational Linguistics (TACL),
2(1).
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proceedings of SemEval
2015.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and
Juanzi Li. 2011. Social context summarization. In
Proceedings of ACM SIGIR 2011, pages 255–264.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis. 2011. Linguistic redundancy
in twitter. In Proceedings of EMNLP 2011, pages 659–
669.
</reference>
<page confidence="0.999123">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.253153">
<title confidence="0.999643">TKLBLIIR: Detecting Twitter Paraphrases with TweetingJay</title>
<author confidence="0.993944">Goran Jan Bojana Dalbelo</author>
<affiliation confidence="0.721633">and Marie-Francine of Zagreb, Faculty of Electrical Engineering and</affiliation>
<address confidence="0.711839">Text Analysis and Knowledge Engineering Lab, Unska 3, 10000 Zagreb, Croatia</address>
<email confidence="0.958956">mladen.karan,jan.snajder,</email>
<affiliation confidence="0.993934">Leuven, Department of Computer Science</affiliation>
<address confidence="0.647222">Language Intelligence &amp; Information Retrieval Group, Celestijnenlaan 200A, Leuven,</address>
<abstract confidence="0.999392533333333">When tweeting on a topic, Twitter users often post messages that convey the same or similar We describe a system for detecting paraphrases and semantic similarity of tweets, with which we participated in Task 1 of SemEval 2015. TweetingJay uses a supervised model that combines semantic overlap and word alignment features, previously shown to be effective for detecting semantic similarity. TweetingJay reaches F1-score and ranked fourth among the 18 participating systems. We additionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>385--393</pages>
<contexts>
<context position="3571" citStr="Agirre et al., 2012" startWordPosition="526" endWordPosition="529">r a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et al., 2014; ˇSari´c et al., 2012). In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) det</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of SemEval 2012, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>SemEval-2014 task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<pages>81--91</pages>
<contexts>
<context position="9123" citStr="Agirre et al., 2014" startWordPosition="1441" endWordPosition="1444">(t2)). Topic-specific information content. While information content computed on a general corpus such as GBN indicates how informative the word is in general, we also wanted to have a measure of how informative each word is within a tweet’s topic. To this end we also compute topic-specific versions of all the above features using topic-specific instead of GBN information contents. 3.2 Word Alignment Features We adopt the word alignment features from two alignment-based systems: (1) the DLS@CU system of Sultan et al. (2014), which achieved the best performance on the STS task at SemEval 2014 (Agirre et al., 2014), and (2) our implementation of the MULTIP latent variables model (Xu et al., 2014), which utilizes the concept of an anchor: a pair of semantically aligned words from a paraphrased pair of tweets. Aligned word pairs (AWP). A state-of-the art monolingual word alignment model by Sultan et al. (2014) outputs pairs of semantically aligned words between two given sentences.2 We used the output of the DLS@CU model to generate two features: (1) the raw count of the aligned word pairs, and (2) the normalized count, which is the harmonic mean of the scores obtained by normalizing the raw count with th</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. pages 81–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Paul Cook</author>
<author>Marco Lui</author>
<author>Andrew MacKinlay</author>
<author>Li Wang</author>
</authors>
<title>How noisy social media text, how diffrnt social media sources.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP 2013,</booktitle>
<pages>356--364</pages>
<contexts>
<context position="1699" citStr="Baldwin et al., 2013" startWordPosition="234" endWordPosition="237">dditionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual similarity (STS) (ˇSari´c et al., 2012; Sultan et al., 2014) and recent breakthroughs in distributed word representations (Mikolov et al., 2013a). We design a set of measures that capture the semantic similarity of tweets and train a support vector machine (SVM) us</context>
</contexts>
<marker>Baldwin, Cook, Lui, MacKinlay, Wang, 2013</marker>
<rawString>Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text, how diffrnt social media sources. In Proceedings of IJCNLP 2013, pages 356–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne W Daniel</author>
</authors>
<title>Applied nonparametric statistics. The Duxbury advanced series in statistics and decision sciences.</title>
<date>1990</date>
<contexts>
<context position="13966" citStr="Daniel, 1990" startWordPosition="2254" endWordPosition="2255">how the performance of the models M1, M2, and M3 on the development and test set. We observe an unusual behavior for all three models: a model that performs good on the development set typically performs bad on the test set, and vice versa. Furthermore, optimal cross-validated Fl performance on the train set is 72%, which is 7 points above the best performance on the validation set. We believe this may be indicative of significant differences in the distributions underlying the datasets. To investigate this further, we applied the Kolmogorov-Smirnov two-sample goodness-of-fit test (K-S test) (Daniel, 1990) for each of the used features to determine whether the train set is drawn from the same distribution as the development and test set. The K-S test is a nonparametric test that determines whether two independent samples differ in some respect, both in the measure of locations (means, median) and the shapes of the distributions (skewness, dispersion, kurtosis). The assumptions for the K-S test (independence of random samples and continuous variables) are met for all our features. We tested all features at the level of significance of 0.05 and rejected the null hypothesis for all features but on</context>
</contexts>
<marker>Daniel, 1990</marker>
<rawString>Wayne W. Daniel. 1990. Applied nonparametric statistics. The Duxbury advanced series in statistics and decision sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL 2009,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="3094" citStr="Das and Smith, 2009" startWordPosition="459" endWordPosition="462">suggests that STS measures are useful for detecting paraphrases in Twitter. We make our system freely available.1 Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setup, with special focus on the provided dataset and its subsets (train, validation, and test), and discuss the stability of the evaluation results. 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings ofACL 2009, pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL 2012,</booktitle>
<pages>864--872</pages>
<contexts>
<context position="4586" citStr="Guo and Diab (2012)" startWordPosition="673" endWordPosition="676">ifying the amount of semantically congruent content between two sentences (Sultan et al., 2014; ˇSari´c et al., 2012). In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose MULTIP, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇSari´c et al. (2012), and (2) word alignment features, based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the MULTIP model by Xu et al. (2014). In the dataset provided</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings ofACL 2012, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<pages>387</pages>
<contexts>
<context position="2948" citStr="Madnani and Dorr, 2010" startWordPosition="436" endWordPosition="439">ures. Positioning of our system at rank four among 18 teams, with only point and a half lower performance compared to the the best-performing system, suggests that STS measures are useful for detecting paraphrases in Twitter. We make our system freely available.1 Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setup, with special focus on the provided dataset and its subsets (train, validation, and test), and discuss the stability of the evaluation results. 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 201</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Computational Linguistics, 36(3):341– 387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL 2012,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="3117" citStr="Madnani et al., 2012" startWordPosition="463" endWordPosition="466">sures are useful for detecting paraphrases in Twitter. We make our system freely available.1 Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setup, with special focus on the provided dataset and its subsets (train, validation, and test), and discuss the stability of the evaluation results. 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages </context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of NAACL 2012, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva Presser Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
<author>Joseph P Pickett</author>
<author>Dale Hoiberg</author>
<author>Dan Clancy</author>
<author>Peter Norvig</author>
<author>Jon Orwant</author>
</authors>
<title>Quantitative analysis of culture using millions of digitized books.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>331</volume>
<issue>6014</issue>
<contexts>
<context position="6601" citStr="Michel et al., 2011" startWordPosition="1013" endWordPosition="1016">gram overlap. We compute the number of matching n-grams between two tweets. This number is normalized by the length of the first and the second tweet, respectively, and the harmonic mean of these two measures is taken as the similarity score. These features are computed separately for unigrams and bigrams. We also compute a weighted version by weighting the matched words w with their information content: freq(w) + 1 ic(w) = −log E w&apos;∈C freq(w&apos;) + 1 where C is the set of all words in the corpus and freq(w) is the word’s frequency. We obtained the frequencies from the Google Books Ngrams (GBN) (Michel et al., 2011). In the weighted version of the ngram overlap, the overlap is normalized by the sum of information contents of all words in the first and second tweet, respectively, and the resulting similarity score is the harmonic mean of these two scores. Greedy word alignment overlap (GWAO). To compute this feature, we iteratively pair the words – one word from each tweet – according to their semantic similarity. In each iteration we greedily select the pair of words with the largest semantic similarity, and remove the words from their corresponding tweets, until no words are left in shorter of the two t</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, Pickett, Hoiberg, Clancy, Norvig, Orwant, 2011</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K Gray, Joseph P Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, et al. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2177" citStr="Mikolov et al., 2013" startWordPosition="311" endWordPosition="314">t al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual similarity (STS) (ˇSari´c et al., 2012; Sultan et al., 2014) and recent breakthroughs in distributed word representations (Mikolov et al., 2013a). We design a set of measures that capture the semantic similarity of tweets and train a support vector machine (SVM) using these measures as features. Positioning of our system at rank four among 18 teams, with only point and a half lower performance compared to the the best-performing system, suggests that STS measures are useful for detecting paraphrases in Twitter. We make our system freely available.1 Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setup, with special focus on the provided dataset and its subsets (train, validation, a</context>
<context position="7373" citStr="Mikolov et al., 2013" startWordPosition="1139" endWordPosition="1142"> respectively, and the resulting similarity score is the harmonic mean of these two scores. Greedy word alignment overlap (GWAO). To compute this feature, we iteratively pair the words – one word from each tweet – according to their semantic similarity. In each iteration we greedily select the pair of words with the largest semantic similarity, and remove the words from their corresponding tweets, until no words are left in shorter of the two tweets. The similarity between words is computed as the cosine between their corresponding 300-dimension embedding vectors obtained using word2vec tool (Mikolov et al., 2013b) on a 100 billion words portion of the Google News dataset. Let P(t1, t2) be the set of word pairs obtained through the alignment on a pair of tweets (t1, t2) and let vec(w) be the embedding vector of the word w. The GWAO score is computed as: �gwao(t1, t2) = α · cos (vec(w1), vec(w2)) (w1,w2) ∈P(t1,t2) where α is the larger of the information contents of the two words, α = max (ic(w1), ic(w1)). The gwao(t1, t2) score is normalized with the sum of information contents of words from t1 and t2, respectively, and the harmonic mean of the two normalized scores is taken as the feature value. 71 T</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS 2013,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2177" citStr="Mikolov et al., 2013" startWordPosition="311" endWordPosition="314">t al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual similarity (STS) (ˇSari´c et al., 2012; Sultan et al., 2014) and recent breakthroughs in distributed word representations (Mikolov et al., 2013a). We design a set of measures that capture the semantic similarity of tweets and train a support vector machine (SVM) using these measures as features. Positioning of our system at rank four among 18 teams, with only point and a half lower performance compared to the the best-performing system, suggests that STS measures are useful for detecting paraphrases in Twitter. We make our system freely available.1 Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setup, with special focus on the provided dataset and its subsets (train, validation, a</context>
<context position="7373" citStr="Mikolov et al., 2013" startWordPosition="1139" endWordPosition="1142"> respectively, and the resulting similarity score is the harmonic mean of these two scores. Greedy word alignment overlap (GWAO). To compute this feature, we iteratively pair the words – one word from each tweet – according to their semantic similarity. In each iteration we greedily select the pair of words with the largest semantic similarity, and remove the words from their corresponding tweets, until no words are left in shorter of the two tweets. The similarity between words is computed as the cosine between their corresponding 300-dimension embedding vectors obtained using word2vec tool (Mikolov et al., 2013b) on a 100 billion words portion of the Google News dataset. Let P(t1, t2) be the set of word pairs obtained through the alignment on a pair of tweets (t1, t2) and let vec(w) be the embedding vector of the word w. The GWAO score is computed as: �gwao(t1, t2) = α · cos (vec(w1), vec(w2)) (w1,w2) ∈P(t1,t2) where α is the larger of the information contents of the two words, α = max (ic(w1), ic(w1)). The gwao(t1, t2) score is normalized with the sum of information contents of words from t1 and t2, respectively, and the harmonic mean of the two normalized scores is taken as the feature value. 71 T</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS 2013, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nasir Naveed</author>
<author>Thomas Gottron</author>
<author>J´erˆome Kunegis</author>
<author>Arifah Che Alhadi</author>
</authors>
<title>Searching microblogs: coping with sparsity and document quality.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM 2011,</booktitle>
<pages>183--188</pages>
<contexts>
<context position="1436" citStr="Naveed et al., 2011" startWordPosition="193" endWordPosition="196">etingJay uses a supervised model that combines semantic overlap and word alignment features, previously shown to be effective for detecting semantic textual similarity. TweetingJay reaches 65.9% F1-score and ranked fourth among the 18 participating systems. We additionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual si</context>
</contexts>
<marker>Naveed, Gottron, Kunegis, Alhadi, 2011</marker>
<rawString>Nasir Naveed, Thomas Gottron, J´erˆome Kunegis, and Arifah Che Alhadi. 2011. Searching microblogs: coping with sparsity and document quality. In Proceedings of CIKM 2011, pages 183–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP 2014,</booktitle>
<pages>1532--1541</pages>
<contexts>
<context position="10472" citStr="Pennington et al., 2014" startWordPosition="1657" endWordPosition="1660">l the tokens in tweets, and the other taking into account only content words. 2https://github.com/ma-sultan/ monolingual-word-aligner Anchor count (ANC). We re-implemented the MULTIP model of Xu et al. (2014).3 As anchor candidates we consider all pairs of content words from the two tweets. We use a minimalistic set of features including (1) Levenshtein distance between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and -y of our SVM model with the RBF kernel. For the final evaluation, th</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP 2014, pages 1532–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Using paraphrases for improving first story detection in news and twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL 2012,</booktitle>
<pages>338--346</pages>
<marker>Petrovi´c, Osborne, Lavrenko, 2012</marker>
<rawString>Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2012. Using paraphrases for improving first story detection in news and twitter. In Proceedings of NAACL 2012, pages 338–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsi´c.</title>
<date></date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>441--448</pages>
<marker>ˇSari´c, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: systems for measuring semantic text similarity. In Proceedings of SemEval 2012, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>DLS@CU: Sentence similarity from word alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>241--245</pages>
<contexts>
<context position="2094" citStr="Sultan et al., 2014" startWordPosition="300" endWordPosition="303">more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual similarity (STS) (ˇSari´c et al., 2012; Sultan et al., 2014) and recent breakthroughs in distributed word representations (Mikolov et al., 2013a). We design a set of measures that capture the semantic similarity of tweets and train a support vector machine (SVM) using these measures as features. Positioning of our system at rank four among 18 teams, with only point and a half lower performance compared to the the best-performing system, suggests that STS measures are useful for detecting paraphrases in Twitter. We make our system freely available.1 Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setu</context>
<context position="4061" citStr="Sultan et al., 2014" startWordPosition="591" endWordPosition="594">sk closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et al., 2014; ˇSari´c et al., 2012). In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose MULTIP, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning mo</context>
<context position="9032" citStr="Sultan et al. (2014)" startWordPosition="1424" endWordPosition="1427">mply compute the cosine between the corresponding tweet embeddings, i.e., cos (vec(t1), vec(t2)). Topic-specific information content. While information content computed on a general corpus such as GBN indicates how informative the word is in general, we also wanted to have a measure of how informative each word is within a tweet’s topic. To this end we also compute topic-specific versions of all the above features using topic-specific instead of GBN information contents. 3.2 Word Alignment Features We adopt the word alignment features from two alignment-based systems: (1) the DLS@CU system of Sultan et al. (2014), which achieved the best performance on the STS task at SemEval 2014 (Agirre et al., 2014), and (2) our implementation of the MULTIP latent variables model (Xu et al., 2014), which utilizes the concept of an anchor: a pair of semantically aligned words from a paraphrased pair of tweets. Aligned word pairs (AWP). A state-of-the art monolingual word alignment model by Sultan et al. (2014) outputs pairs of semantically aligned words between two given sentences.2 We used the output of the DLS@CU model to generate two features: (1) the raw count of the aligned word pairs, and (2) the normalized co</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. DLS@CU: Sentence similarity from word alignment. In Proceedings of SemEval 2014, pages 241– 245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Ralph Grishman</author>
</authors>
<title>Gathering and generating paraphrases from twitter with application to normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="1569" citStr="Xu et al., 2013" startWordPosition="215" endWordPosition="218">ng semantic textual similarity. TweetingJay reaches 65.9% F1-score and ranked fourth among the 18 participating systems. We additionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual similarity (STS) (ˇSari´c et al., 2012; Sultan et al., 2014) and recent breakthroughs in distributed word representations (Mikolov et a</context>
</contexts>
<marker>Xu, Ritter, Grishman, 2013</marker>
<rawString>Wei Xu, Alan Ritter, and Ralph Grishman. 2013. Gathering and generating paraphrases from twitter with application to normalization. In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from Twitter.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="4378" citStr="Xu et al. (2014)" startWordPosition="639" endWordPosition="642">15 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et al., 2014; ˇSari´c et al., 2012). In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose MULTIP, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇSari´c et al. (</context>
<context position="9206" citStr="Xu et al., 2014" startWordPosition="1455" endWordPosition="1458">ral corpus such as GBN indicates how informative the word is in general, we also wanted to have a measure of how informative each word is within a tweet’s topic. To this end we also compute topic-specific versions of all the above features using topic-specific instead of GBN information contents. 3.2 Word Alignment Features We adopt the word alignment features from two alignment-based systems: (1) the DLS@CU system of Sultan et al. (2014), which achieved the best performance on the STS task at SemEval 2014 (Agirre et al., 2014), and (2) our implementation of the MULTIP latent variables model (Xu et al., 2014), which utilizes the concept of an anchor: a pair of semantically aligned words from a paraphrased pair of tweets. Aligned word pairs (AWP). A state-of-the art monolingual word alignment model by Sultan et al. (2014) outputs pairs of semantically aligned words between two given sentences.2 We used the output of the DLS@CU model to generate two features: (1) the raw count of the aligned word pairs, and (2) the normalized count, which is the harmonic mean of the scores obtained by normalizing the raw count with the length of the first and second tweet, respectively. We computed two versions for </context>
<context position="10563" citStr="Xu et al., 2014" startWordPosition="1671" endWordPosition="1674">a-sultan/ monolingual-word-aligner Anchor count (ANC). We re-implemented the MULTIP model of Xu et al. (2014).3 As anchor candidates we consider all pairs of content words from the two tweets. We use a minimalistic set of features including (1) Levenshtein distance between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and -y of our SVM model with the RBF kernel. For the final evaluation, the organizers used a test set of 972 tweet pairs. Feature sets. We divided the features in t</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics (TACL), 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of SemEval</booktitle>
<contexts>
<context position="1956" citStr="Xu et al., 2015" startWordPosition="275" endWordPosition="278">t al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic textual similarity (STS) (ˇSari´c et al., 2012; Sultan et al., 2014) and recent breakthroughs in distributed word representations (Mikolov et al., 2013a). We design a set of measures that capture the semantic similarity of tweets and train a support vector machine (SVM) using these measures as features. Positioning of our system at rank four among 18 teams, with only point and a half lower performance compared to the the best-performing system, suggests that STS measures are useful for detecting paraphrases in Twitter. We ma</context>
<context position="10826" citStr="Xu et al., 2015" startWordPosition="1721" endWordPosition="1724">ce between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and -y of our SVM model with the RBF kernel. For the final evaluation, the organizers used a test set of 972 tweet pairs. Feature sets. We divided the features in three groups: (1) semantic overlap features (SO) from Section 3.1, (2) aligned word pairs (AWP) features, and (3) the anchor count feature (ANC) from Section 3.2. Model optimization. There are three ways how the optimization of the SVM model (hyperparameters C and</context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of SemEval 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zi Yang</author>
<author>Keke Cai</author>
<author>Jie Tang</author>
<author>Li Zhang</author>
<author>Zhong Su</author>
<author>Juanzi Li</author>
</authors>
<title>Social context summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACM SIGIR</booktitle>
<pages>255--264</pages>
<contexts>
<context position="1393" citStr="Yang et al., 2011" startWordPosition="186" endWordPosition="189">rticipated in Task 1 of SemEval 2015. TweetingJay uses a supervised model that combines semantic overlap and word alignment features, previously shown to be effective for detecting semantic textual similarity. TweetingJay reaches 65.9% F1-score and ranked fourth among the 18 participating systems. We additionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from </context>
</contexts>
<marker>Yang, Cai, Tang, Zhang, Su, Li, 2011</marker>
<rawString>Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li. 2011. Social context summarization. In Proceedings of ACM SIGIR 2011, pages 255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Kostas Tsioutsiouliklis</author>
</authors>
<title>Linguistic redundancy in twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>659--669</pages>
<contexts>
<context position="4167" citStr="Zanzotto et al. (2011)" startWordPosition="609" endWordPosition="612">2012 (Agirre et al., 2012). There is now a 1http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et al., 2014; ˇSari´c et al., 2012). In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose MULTIP, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of </context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Tsioutsiouliklis, 2011</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in twitter. In Proceedings of EMNLP 2011, pages 659– 669.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>