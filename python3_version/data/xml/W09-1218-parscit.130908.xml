<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.998911">
Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage
Approximate Max-Margin Linear Models
</title>
<author confidence="0.997855">
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
</author>
<affiliation confidence="0.9985515">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.57074">
8916-5 Takayama, Ikoma, Nara, Japan, 630-0192
</address>
<email confidence="0.998626">
{yotaro-w, masayu-a, matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998105625">
This paper describes a system for syntactic-
semantic dependency parsing for multiple lan-
guages. The system consists of three parts: a
state-of-the-art higher-order projective depen-
dency parser for syntactic dependency pars-
ing, a predicate classifier, and an argument
classifier for semantic dependency parsing.
For semantic dependency parsing, we ex-
plore use of global features. All components
are trained with an approximate max-margin
learning algorithm.
In the closed challenge of the CoNLL-2009
Shared Task (Hajiˇc et al., 2009), our system
achieved the 3rd best performances for En-
glish and Czech, and the 4th best performance
for Japanese.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957921052631">
In recent years, joint inference of syntactic and se-
mantic dependencies has attracted attention in NLP
communities. Ideally, we would like to choose the
most plausible syntactic-semantic structure among
all possible structures in that syntactic dependencies
and semantic dependencies are correlated. How-
ever, solving this problem is too difficult because
the search space of the problem is extremely large.
Therefore we focus on improving performance for
each subproblem: dependency parsing and semantic
role labeling.
In the past few years, research investigating
higher-order dependency parsing algorithms has
found its superiority to first-order parsing algo-
rithms. To reap the benefits of these advances, we
use a higher-order projective dependency parsing al-
gorithm (Carreras, 2007) which is an extension of
the span-based parsing algorithm (Eisner, 1996), for
syntactic dependency parsing.
In terms of semantic role labeling, we would
like to capture global information about predicate-
argument structures in order to accurately predict the
correct predicate-argument structure. Previous re-
search dealt with such information using re-ranking
(Toutanova et al., 2005; Johansson and Nugues,
2008). We explore a different approach to deal
with such information using global features. Use
of global features for structured prediction problem
has been explored by several NLP applications such
as sequential labeling (Finkel et al., 2005; Krishnan
and Manning, 2006; Kazama and Torisawa, 2007)
and dependency parsing (Nakagawa, 2007) with a
great deal of success. We attempt to use global fea-
tures for argument classification in which the most
plausible semantic role assignment is selected using
both local and global information. We present an
approximate max-margin learning algorithm for ar-
gument classifiers with global features.
</bodyText>
<sectionHeader confidence="0.991953" genericHeader="introduction">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.998632333333333">
As in previous work, we use a linear model for de-
pendency parsing. The score function used in our
dependency parser is defined as follows.
</bodyText>
<equation confidence="0.981514">
s(y) = ∑ F(h, m, x) (1)
(h,m)∈y
</equation>
<bodyText confidence="0.999974666666667">
where h and m denote the head and the dependent
of the dependency edge in y, and F(h, m, x) is a
Factor that specifies dependency edge scores.
</bodyText>
<page confidence="0.981517">
114
</page>
<note confidence="0.6721065">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 114–119,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.992118">
We used a second-order factorization as in (Car-
reras, 2007). The second-order factor F is defined
as follows.
</bodyText>
<equation confidence="0.9854745">
F(h,m,x) = w·Φ(h,m,x)+w·Φ(h,m, ch, x)
+ w · Φ(h, m, cmz, x) + w · Φ(h, m, cmo, x) (2)
</equation>
<bodyText confidence="0.989763933333333">
where w is a parameter vector, Φ is a feature vector,
ch is the child of h in the span [h...m] that is closest
to m, cmz is the child of m in the span [h...m] that is
farthest from m and cmo is the child of m outside the
span [h...m] that is farthest from m. For more details
of the second-order parsing algorithm, see (Carreras,
2007).
For parser training, we use the Passive Aggres-
sive Algorithm (Crammer et al., 2006), which is an
approximate max-margin variant of the perceptron
algorithm. Also, we apply an efficient parameter av-
eraging technique (Daum´e III, 2006). The resulting
learning algorithm is shown in Algorithm 1.
Algorithm 1 A Passive Aggressive Algorithm with
parameter averaging
</bodyText>
<equation confidence="0.933421909090909">
input Training set T = {xt, yt}Tt=1, Number of iterations
N and Parameter C
w + -0, v + -0, c + -1
for i +- 0 to N do
for (xt, yt) E T do
yˆ = arg maxy w · Φ(xt, y) + ρ(yt, ˆy)
τt = min “C w·Φ(xt ,ˆy)-w·Φ(xt,yt)+ρ(yt,ˆy) 1
||Φ(xt,yt)-Φ(xt,ˆy)||2 J
w +- w + τt(Φ(xt, yt) − Φ(xt, ˆy))
v +- v + cτt(Φ(xt, yt) − Φ(xt, ˆy))
c +- c + 1
</equation>
<subsectionHeader confidence="0.869674">
end for
end for
</subsectionHeader>
<bodyText confidence="0.986308291666667">
return w − v/c
We set p(yt, ˆy) as the number of incorrect head
predictions in the ˆy, and C as 1.0.
Among the 7 languages of the task, 4 languages
(Czech, English, German and Japanese) contain
non-projective edges (13.94 %, 3.74 %, 25.79 %
and 0.91 % respectively), therefore we need to deal
with non-projectivity. In order to avoid losing the
benefits of higher-order parsing, we considered ap-
plying pseudo-projective transformation (Nivre and
Nilsson, 2005). However, growth of the number of
dependency labels by pseudo-projective transforma-
tion increases the dependency parser training time,
so we did not adopt transformations. Therefore, the
parser ignores the presence of non-projective edges
in the training and the testing phases.
The features used for our dependency parser are
based on those listed in (Johansson, 2008). In addi-
tion, distance features are used. We use shorthand
notations in order to simplify the feature represen-
tations: ’h’, ’d’, ’c’, ’l’, ’p’, ’−1’ and ’+1’ cor-
respond to head, dependent, head’s or dependent’s
child, lemma, POS, left position and right position
respectively.
</bodyText>
<sectionHeader confidence="0.649107" genericHeader="method">
First-order Features
</sectionHeader>
<construct confidence="0.409579444444444">
Token features: hl, hp, hl+hp, dl, dp and dl+dp.
Head-Dependent features: hp+dp, hl+dl, hl+dl,
hl+hp+dl, hl+hp+dp, hl+dl+dp, hp+dl+dp and
hl+hp+dl+dp.
Context features: hp+hp+1+dp−1+dp,
hp−1+hp+dp−1+dp, hp+hp+1+dp+dp+1 and
hp−1+hp+dp+dp+1.
Distance features: The number of tokens between the
head and the dependent.
</construct>
<subsectionHeader confidence="0.656452">
Second-order Features
</subsectionHeader>
<bodyText confidence="0.923451">
Head-Dependent-Head’s or Dependent’s Child:
hl+cl, hl+cl+cp, hp+cl, hp+cp, hp+dp+cp, dp+cp,
dp+cl+cp, dl+cp, dl+cp+cl
</bodyText>
<sectionHeader confidence="0.977671" genericHeader="method">
3 Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.999944833333333">
Our SRL module consists of two parts: a predicate
classifier and an argument classifier. First, our sys-
tem determines the word sense for each predicate
with the predicate classifier, and then it detects the
highest scored argument assignment using the argu-
ment classifier with global features.
</bodyText>
<subsectionHeader confidence="0.999204">
3.1 Predicate Classification
</subsectionHeader>
<bodyText confidence="0.99998">
The first phase of SRL in our system is to detect
the word sense for each predicate. WSD can be for-
malized as a multi-class classification problem given
lemmas. We created a linear model for each lemma
and used the Passive Aggressive Algorithm with pa-
rameter averaging to train the models.
</bodyText>
<sectionHeader confidence="0.768095" genericHeader="method">
3.1.1 Features for Predicate Classification
</sectionHeader>
<bodyText confidence="0.8642802">
Word features: Predicted lemma and the predicted POS
of the predicate, predicate’s head, and its conjunc-
tions.
Dependency label: The dependency label between the
predicate and the predicate’s head.
</bodyText>
<page confidence="0.994918">
115
</page>
<bodyText confidence="0.9781442">
in Algorithm 2. In this algorithm, the weights cor-
respond to local factor features ΦL and global factor
features ΦG are updated simultaneously.
Dependency label sequence: The concatenation of the
dependency labels of the predicate dependents.
Since effective features for predicate classifica-
tion are different for each language, we performed
greedy forward feature selection.
Algorithm 2 Learning with Global Features for Ar-
gument Classification
</bodyText>
<subsectionHeader confidence="0.999461">
3.2 Argument Classification
</subsectionHeader>
<bodyText confidence="0.999872166666667">
In order to capture global clues of predicate-
argument structures, we consider introducing global
features for linear models. Let A(p) be a joint
assignment of role labels for argument candidates
given the predicate p. Then we define a score func-
tion s(A(p)) for argument label assignments A(p).
</bodyText>
<equation confidence="0.9971855">
s(A(p)) = ∑ Fk(x, A(p)) (3)
k
</equation>
<bodyText confidence="0.9952705">
We introduce two factors: Local Factor FL and
Global Factor FG defined as follows.
</bodyText>
<equation confidence="0.99787">
FL(x, a(p)) = w · ΦL(x, a(p)) (4)
FG(x, A(p)) = w · ΦG(x, A(p)) (5)
</equation>
<bodyText confidence="0.999985285714286">
where ΦL, ΦG denote feature vectors for the local
factor and the global factor respectively. FL scores a
particular role assignment for each argument candi-
date individually, and FG treats global features that
capture what structure the assignment A has. Re-
sulting scoring function for the assignment A(p) is
as follows.
</bodyText>
<equation confidence="0.991163333333333">
s(A(p)) = ∑ w·ΦL(x, a(p))+w·ΦG(x, A(p))
a(p)∈A(p)
(6)
</equation>
<bodyText confidence="0.9998756">
Use of global features is problematic, because it
becomes difficult to find the highest assignment ef-
ficiently. In order to deal with the problem, we use
a simple approach, n-best relaxation as in (Kazama
and Torisawa, 2007). At first we generate n-best as-
signments using only the local factor, and then add
the global factor score for each n-best assignment, fi-
nally select the best scoring assignment from them.
In order to generate n-best assignments, we used a
beam-search algorithm.
</bodyText>
<subsectionHeader confidence="0.875699">
3.2.1 Learning the Model
</subsectionHeader>
<bodyText confidence="0.999787666666667">
As in dependency parser and predicate classifier,
we train the model using the PA algorithm with pa-
rameter averaging. The learning algorithm is shown
</bodyText>
<equation confidence="0.847794941176471">
input Training set T = {xt, At}Tt=1, Number of iterations
N and Parameter C
w ← 0, v ← 0, c ← 1
for i ← 0 to N do
for (xt, At) ∈ T do
let Φ(xt, A) = Pa∈A ΦL(xt, a) + ΦG(xt, A)
generate n-best assignments {An} using FL
Aˆ = arg maxA∈{An} w · Φ(xt, A) + p(At, A)
Tt = min “”
li w·Φ(xt, ˆA)−w·Φ(xt,At)+ρ(At,ˆA)
||Φ(xt,At)−Φ(xt, ˆA)||2
w ← w + Tt(Φ(xt, At) − Φ(xt, ˆA))
v ← v + cTt(Φ(xt, At) − Φ(xt, ˆA))
c ← c + 1
end for
end for
return w − v/c
</equation>
<bodyText confidence="0.988353818181818">
We set the margin value ρ(A, ˆA) as the number
of incorrect assignments plus S(A, ˆA), and C as 1.0.
The delta function returns 1 if at least one assign-
ment is different from the correct assignment and 0
otherwise.
The model is similar to re-ranking (Toutanova et
al., 2005; Johansson and Nugues, 2008). However
in contrast to re-ranking, we only have to prepare
one model. The re-ranking approach requires other
training datasets that are different from the data used
in local model training.
</bodyText>
<sectionHeader confidence="0.926104" genericHeader="method">
3.2.2 Features for Argument Classification
</sectionHeader>
<bodyText confidence="0.8954276">
The local features used in our system are the same
as our previous work (Watanabe et al., 2008) except
for language dependent features. The global features
that used in our system are based on (Johansson and
Nugues, 2008) that used for re-ranking.
</bodyText>
<sectionHeader confidence="0.481346" genericHeader="method">
Local Features
</sectionHeader>
<figureCaption confidence="0.9587407">
Word features: Predicted lemma and predicted POS of
the predicate, predicate’s head, argument candidate,
argument candidate’s head, leftmost/rightmost de-
pendent and leftmost/rightmost sibling.
Dependency label: The dependency label of predicate,
argument candidate and argument candidate’s de-
pendent.
Family: The position of the argument candicate with re-
spect to the predicate position in the dependency
tree (e.g. child, sibling).
</figureCaption>
<page confidence="0.937953">
116
</page>
<table confidence="0.9998529">
Average Catalan Chinese Czech English German Japanese Spanish
Macro F1 Score 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
(78.00*) (74.83*) (73.43*) (81.38*) (86.40*) (68.39*) (84.84*) (76.74*)
Semantic Labeled F1 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
(75.17*) (71.05*) (74.17*) (84.66*) (84.26*) (61.94*) (77.91*) (72.25*)
Labeled Syntactic Accuracy 81.16 79.48 72.66 78.17 88.54 75.85 91.69 81.74
(80.77*) (78.62*) (72.66*) (78.10*) (88.54*) (74.60*) (91.66*) (81.23*)
Macro F1 Score 84.30 84.79 81.63 83.08 87.93 83.25 85.54 83.94
Semantic Labeled F1 81.58 80.99 79.99 86.67 85.09 79.46 79.03 79.85
Labeled Syntactic Accuracy 87.02 88.59 83.27 79.48 90.77 87.03 91.96 88.04
</table>
<tableCaption confidence="0.485952333333333">
Table 1: Scores of our system.
Position: The position of the head of the dependency re-
lation with respect to the predicate position in the
sentence.
Pattern: The left-to-right chain of the predicted
POS/dependency labels of the predicate’s children.
Path features: Predicted lemma, predicted POS and de-
pendency label paths between the predicate and the
argument candidate.
</tableCaption>
<bodyText confidence="0.549003">
Distance: The number of dependency edges between the
predicate and the argument candidate.
</bodyText>
<subsectionHeader confidence="0.487266">
Global Features
</subsectionHeader>
<bodyText confidence="0.872972428571429">
Predicate-argument label sequence: The sequence of
the predicate sense and argument labels in the
predicate-argument strucuture.
Presence of labels defined in frame files: Whether the
semantic roles defined in the frame present in the
predicate-argument structure (e.g. MISSING:A1 or
CONTAINS:A1.)
</bodyText>
<subsectionHeader confidence="0.754265">
3.2.3 Argument Pruning
</subsectionHeader>
<bodyText confidence="0.9977131875">
We observe that most arguments tend to be not far
from its predicate, so we can prune argument candi-
dates to reduce search space. Since the characteris-
tics of the languages are slightly different, we apply
two types of pruning algorithms.
Pruning Algorithm 1: Let S be an argument candi-
date set. Initially set S ← ϕ and start at predicate node.
Add dependents of the node to S, and move current node
to its parent. Repeat until current node reaches to ROOT.
Pruning Algorithm 2: Same as the Algorithm 1 ex-
cept that added nodes are its grandchildren as well as its
dependents.
The pruning results are shown in Table 2. Since
we could not prune arguments in Japanese accu-
rately using the two algorithms, we pruned argument
candidates simply by POS.
</bodyText>
<table confidence="0.999188875">
algorithm coverage (%) reduction (%)
Catalan 1 100 69.1
Chinese 1 98.9 69.1
Czech 2 98.5 49.1
English 1 97.3 63.1
German 1 98.3 64.3
Japanese POS 99.9 41.0
Spanish 1 100 69.7
</table>
<tableCaption confidence="0.998963">
Table 2: Pruning results.
</tableCaption>
<sectionHeader confidence="0.999865" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999801142857143">
The submitted results on the test data are shown in
the upper part of Table 1. Due to a bug, we mistak-
enly used the gold lemmas in the dependency parser.
Corrected results are shown in the part marked with
*. The lower part shows the post evaluation results
with the gold lemmas and POSs.
For some of the 7 languages, since the global
model described in Section 3.2 degraded perfor-
mance compare to a model trained with only FL,
we did NOT use the model for all languages. We
used the global model for only three languages: Chi-
nese, English and Japanese. The remaining lan-
guages (Catalan, Czech, German and Spanish) used
a model trained with only FL.
</bodyText>
<subsectionHeader confidence="0.988387">
4.1 Dependency Parsing Results
</subsectionHeader>
<bodyText confidence="0.999966777777778">
The parser achieved relatively high accuracies for
Czech, English and Japanese, and for each language,
the difference between the performance with correct
POS and predicted POS is not so large. However, in
Catalan, Chinese German and Spanish, the parsing
accuracies was seriously degraded by replacing cor-
rect POSs with predicted POSs (6.3 - 11.2 %). This
is likely because these languages have relatively low
predicted POS accuracies (92.3 - 95.5 %) ; Chinese
</bodyText>
<page confidence="0.993371">
117
</page>
<table confidence="0.99964625">
FL FL+FG (∆P, ∆R)
Catalan 85.80 85.68 (+0.01, -0.26)
Chinese 86.58 87.39 (+0.24, +1.36)
Czech 89.63 89.05 (-0.87, -0.28)
English 85.66 85.74 (-0.87, +0.98)
German 80.82 77.30 (-7.27, +0.40)
Japanese 79.87 81.01 (+0.17, +1.88)
Spanish 84.38 83.89 (-0.42, -0.57)
</table>
<tableCaption confidence="0.979360333333333">
Table 3: Effect of global features (semantic labeled F1).
∆P and ∆R denote the differentials of labeled precision
and labeled recall between FL and FL+FG respectively.
</tableCaption>
<bodyText confidence="0.993949">
has especially low accuracy (92.3%). The POS ac-
curacy may affect the parsing performances.
</bodyText>
<subsectionHeader confidence="0.992672">
4.2 SRL Results
</subsectionHeader>
<bodyText confidence="0.9999710625">
In order to highlight the effect of the global fea-
tures, we compared two models. The first model
is trained with only the local factor FL. The sec-
ond model is trained with both the local factor FL
and the global factor FG. The results are shown in
Table 3. In the experiments, we used the develop-
ment data with gold parse trees. For Chinese and
Japanese, significant improvements are obtained us-
ing the global features (over +1.0% in labeled re-
call and the slightly better labeled precision). How-
ever, for Catalan, Czech, German and Spanish, the
global features degraded the performance in labeled
F1. Especially, in German, the precision is substan-
tially degraded (-7.27% in labeled F1). These results
indicate that it is necessary to introduce language de-
pendent features.
</bodyText>
<subsectionHeader confidence="0.9614595">
4.3 Training, Evaluation Time and Memory
Requirements
</subsectionHeader>
<bodyText confidence="0.999905153846154">
Table 4 and 5 shows the training/evaluation times
and the memory consumption of the second-order
dependency parsers and the global argument classi-
fiers respectively. The training times of the predi-
cate classifier were less than one day, and the testing
times were mere seconds.
As reported in (Carreras, 2007; Johansson and
Nugues, 2008), training and inference of the second-
order parser are very expensive. For Chinese, we
could only complete 2 iterations.
In terms of the argument classifier, since N-best
generation time account for a substantial proportion
of the training time (in this work N = 100), chang-
</bodyText>
<table confidence="0.99672575">
iter hrs./iter sent./min. mem.
Catalan 9 14.6 9.0 9.6 GB
Chinese 2 56.5 3.7 16.2 GB
Czech 8 14.6 20.5 12.6 GB
English 7 22.0 13.4 15.1 GB
German 4 12.3 59.1 13.1 GB
Japanese 7 11.2 21.8 13.0 GB
Spanish 7 19.5 7.3 17.9 GB
</table>
<tableCaption confidence="0.991119">
Table 4: Training, evaluation time and memory require-
ments of the second-order dependency parsers. The ’iter’
column denote the number of iterations of the model
used for the evaluations. Catalan, Czech and English
are trained on Xeon 3.0GHz, Chinese and Japanese are
trained on Xeon 2.66GHz, German and Spanish are
trained on Opteron 2.3GHz machines.
</tableCaption>
<table confidence="0.999385">
train (hrs.) sent./min. mem.
Chinese 6.5 453.7 2.0 GB
English 13.5 449.8 3.2 GB
Japanese 3.5 137.6 1.1 GB
</table>
<tableCaption confidence="0.963053333333333">
Table 5: Training, evaluation time and memory require-
ments of the global argument classifiers. The classifiers
are all trained on Opteron 2.3GHz machines.
</tableCaption>
<bodyText confidence="0.9997937">
ing N affects the training and evaluation times sig-
nificantly.
All modules of our system are implemented in
Java. The required memory spaces shown in Table
4 and 5 are calculated by subtracting free memory
size from the total memory size of the Java VM.
Note that we observed that the value fluctuated dras-
tically while measuring memory usage, so the value
may not indicate precise memory requirements of
our system.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99987625">
In this paper, we have described our system for syn-
tactic and semantic dependency analysis in multilin-
gual. Although our system is not a joint approach
but a pipeline approach, the system is comparable to
the top system for some of the 7 languages.
A further research direction we are investigating
is the application of various types of global features.
We believe that there is still room for improvements
since we used only two types of global features for
the argument classifier.
Another research direction is investigating joint
approaches. To the best of our knowledge, three
</bodyText>
<page confidence="0.995448">
118
</page>
<bodyText confidence="0.999961545454546">
types of joint approaches have been proposed:
N-best based approach (Johansson and Nugues,
2008), synchronous joint approach (Henderson et
al., 2008), and a joint approach where parsing
and SRL are performed simultaneously (Llu´ıs and
M`arquez, 2008). We attempted to perform N-
best based joint approach, however, the expen-
sive computational cost of the 2nd-order projective
parser discouraged it. We would like to investigate
syntactic-semantic joint approaches with reasonable
time complexities.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999887166666667">
We would like to thank Richard Johansson for his
advice on parser implementation, and the CoNLL-
2009 organizers (Hajiˇc et al., 2009; Taul´e et al.,
2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Sur-
deanu et al., 2008; Burchardt et al., 2006; Kawahara
et al., 2002; Taul´e et al., 2008).
</bodyText>
<sectionHeader confidence="0.998151" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776740740741">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proc. of LREC-2006, Genoa,
Italy.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL 2007.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7:551–585.
Hal Daum´e III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA, August.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing. In Proc. of ICCL 1996.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proc. of ACL 2005.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proc. of CoNLL-2009,
Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proc. of CoNLL 2008.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proc. of CoNLL
2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis, Lund
University.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proc. of LREC-2002, pages 2008–2013,
Las Palmas, Canary Islands.
Jun’Ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proc. of EMNLP-CoNLL 2007.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In Proc. of
ACL-COLING 2006.
Xavier Llu´ıs and Llu´ıs M`arquez. 2008. A joint model for
parsing syntactic and semantic dependencies. In Proc.
of CoNLL 2008.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL 2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of CoNLL-2008.
Mariona Taul´e, Maria Ant`onia Mart´ı, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proc. of LREC-2008, Mar-
rakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proc. of ACL 2005.
Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara,
and Yuji Matsumoto. 2008. A pipeline approach for
syntactic and semantic dependency parsing. In Proc.
of CoNLL 2008.
</reference>
<page confidence="0.999092">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649179">
<title confidence="0.998066">Multilingual Syntactic-Semantic Dependency Parsing with Approximate Max-Margin Linear Models</title>
<author confidence="0.98586">Yotaro Watanabe</author>
<author confidence="0.98586">Masayuki Asahara</author>
<author confidence="0.98586">Yuji</author>
<affiliation confidence="0.9996105">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.992472">8916-5 Takayama, Ikoma, Nara, Japan,</address>
<email confidence="0.994023">masayu-a,</email>
<abstract confidence="0.953966352941176">This paper describes a system for syntacticsemantic dependency parsing for multiple languages. The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing. For semantic dependency parsing, we explore use of global features. All components are trained with an approximate max-margin learning algorithm. In the closed challenge of the CoNLL-2009 Shared Task (Hajiˇc et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proc. of LREC-2006,</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proc. of LREC-2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL</booktitle>
<contexts>
<context position="1780" citStr="Carreras, 2007" startWordPosition="246" endWordPosition="247">actic-semantic structure among all possible structures in that syntactic dependencies and semantic dependencies are correlated. However, solving this problem is too difficult because the search space of the problem is extremely large. Therefore we focus on improving performance for each subproblem: dependency parsing and semantic role labeling. In the past few years, research investigating higher-order dependency parsing algorithms has found its superiority to first-order parsing algorithms. To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP application</context>
<context position="3426" citStr="Carreras, 2007" startWordPosition="497" endWordPosition="499">global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is defined as follows. s(y) = ∑ F(h, m, x) (1) (h,m)∈y where h and m denote the head and the dependent of the dependency edge in y, and F(h, m, x) is a Factor that specifies dependency edge scores. 114 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 114–119, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We used a second-order factorization as in (Carreras, 2007). The second-order factor F is defined as follows. F(h,m,x) = w·Φ(h,m,x)+w·Φ(h,m, ch, x) + w · Φ(h, m, cmz, x) + w · Φ(h, m, cmo, x) (2) where w is a parameter vector, Φ is a feature vector, ch is the child of h in the span [h...m] that is closest to m, cmz is the child of m in the span [h...m] that is farthest from m and cmo is the child of m outside the span [h...m] that is farthest from m. For more details of the second-order parsing algorithm, see (Carreras, 2007). For parser training, we use the Passive Aggressive Algorithm (Crammer et al., 2006), which is an approximate max-margin varian</context>
<context position="16011" citStr="Carreras, 2007" startWordPosition="2565" endWordPosition="2566">erman and Spanish, the global features degraded the performance in labeled F1. Especially, in German, the precision is substantially degraded (-7.27% in labeled F1). These results indicate that it is necessary to introduce language dependent features. 4.3 Training, Evaluation Time and Memory Requirements Table 4 and 5 shows the training/evaluation times and the memory consumption of the second-order dependency parsers and the global argument classifiers respectively. The training times of the predicate classifier were less than one day, and the testing times were mere seconds. As reported in (Carreras, 2007; Johansson and Nugues, 2008), training and inference of the secondorder parser are very expensive. For Chinese, we could only complete 2 iterations. In terms of the argument classifier, since N-best generation time account for a substantial proportion of the training time (in this work N = 100), changiter hrs./iter sent./min. mem. Catalan 9 14.6 9.0 9.6 GB Chinese 2 56.5 3.7 16.2 GB Czech 8 14.6 20.5 12.6 GB English 7 22.0 13.4 15.1 GB German 4 12.3 59.1 13.1 GB Japanese 7 11.2 21.8 13.0 GB Spanish 7 19.5 7.3 17.9 GB Table 4: Training, evaluation time and memory requirements of the second-ord</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. of EMNLPCoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--551</pages>
<contexts>
<context position="3983" citStr="Crammer et al., 2006" startWordPosition="606" endWordPosition="609">tics We used a second-order factorization as in (Carreras, 2007). The second-order factor F is defined as follows. F(h,m,x) = w·Φ(h,m,x)+w·Φ(h,m, ch, x) + w · Φ(h, m, cmz, x) + w · Φ(h, m, cmo, x) (2) where w is a parameter vector, Φ is a feature vector, ch is the child of h in the span [h...m] that is closest to m, cmz is the child of m in the span [h...m] that is farthest from m and cmo is the child of m outside the span [h...m] that is farthest from m. For more details of the second-order parsing algorithm, see (Carreras, 2007). For parser training, we use the Passive Aggressive Algorithm (Crammer et al., 2006), which is an approximate max-margin variant of the perceptron algorithm. Also, we apply an efficient parameter averaging technique (Daum´e III, 2006). The resulting learning algorithm is shown in Algorithm 1. Algorithm 1 A Passive Aggressive Algorithm with parameter averaging input Training set T = {xt, yt}Tt=1, Number of iterations N and Parameter C w + -0, v + -0, c + -1 for i +- 0 to N do for (xt, yt) E T do yˆ = arg maxy w · Φ(xt, y) + ρ(yt, ˆy) τt = min “C w·Φ(xt ,ˆy)-w·Φ(xt,yt)+ρ(yt,ˆy) 1 ||Φ(xt,yt)-Φ(xt,ˆy)||2 J w +- w + τt(Φ(xt, yt) − Φ(xt, ˆy)) v +- v + cτt(Φ(xt, yt) − Φ(xt, ˆy)) c +</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern</institution>
<location>California, Los Angeles, CA,</location>
<marker>Daum´e, 2006</marker>
<rawString>Hal Daum´e III. 2006. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. thesis, University of Southern California, Los Angeles, CA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing.</title>
<date>1996</date>
<booktitle>In Proc. of ICCL</booktitle>
<contexts>
<context position="1853" citStr="Eisner, 1996" startWordPosition="257" endWordPosition="258">pendencies and semantic dependencies are correlated. However, solving this problem is too difficult because the search space of the problem is extremely large. Therefore we focus on improving performance for each subproblem: dependency parsing and semantic role labeling. In the past few years, research investigating higher-order dependency parsing algorithms has found its superiority to first-order parsing algorithms. To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning,</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing. In Proc. of ICCL 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="2430" citStr="Finkel et al., 2005" startWordPosition="338" endWordPosition="341">pan-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classifiers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is defined as follows. s(y) = ∑ F(h, m, x) (1) (h,m)∈y where h an</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Jiˇr´ı Havelka</author>
<author>Marie Mikulov´a</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, Havelka, Mikulov´a, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proc. of CoNLL-2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proc. of CoNLL-2009, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<contexts>
<context position="18314" citStr="Henderson et al., 2008" startWordPosition="2946" endWordPosition="2949">ugh our system is not a joint approach but a pipeline approach, the system is comparable to the top system for some of the 7 languages. A further research direction we are investigating is the application of various types of global features. We believe that there is still room for improvements since we used only two types of global features for the argument classifier. Another research direction is investigating joint approaches. To the best of our knowledge, three 118 types of joint approaches have been proposed: N-best based approach (Johansson and Nugues, 2008), synchronous joint approach (Henderson et al., 2008), and a joint approach where parsing and SRL are performed simultaneously (Llu´ıs and M`arquez, 2008). We attempted to perform Nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it. We would like to investigate syntactic-semantic joint approaches with reasonable time complexities. Acknowledgments We would like to thank Richard Johansson for his advice on parser implementation, and the CoNLL2009 organizers (Hajiˇc et al., 2009; Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al.</context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>James Henderson, Paola Merlo, Gabriele Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proc. of CoNLL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic-semantic analysis with propbank and nombank.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<contexts>
<context position="2192" citStr="Johansson and Nugues, 2008" startWordPosition="302" endWordPosition="305">order dependency parsing algorithms has found its superiority to first-order parsing algorithms. To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argumen</context>
<context position="9808" citStr="Johansson and Nugues, 2008" startWordPosition="1574" endWordPosition="1577"> (xt, At) ∈ T do let Φ(xt, A) = Pa∈A ΦL(xt, a) + ΦG(xt, A) generate n-best assignments {An} using FL Aˆ = arg maxA∈{An} w · Φ(xt, A) + p(At, A) Tt = min “” li w·Φ(xt, ˆA)−w·Φ(xt,At)+ρ(At,ˆA) ||Φ(xt,At)−Φ(xt, ˆA)||2 w ← w + Tt(Φ(xt, At) − Φ(xt, ˆA)) v ← v + cTt(Φ(xt, At) − Φ(xt, ˆA)) c ← c + 1 end for end for return w − v/c We set the margin value ρ(A, ˆA) as the number of incorrect assignments plus S(A, ˆA), and C as 1.0. The delta function returns 1 if at least one assignment is different from the correct assignment and 0 otherwise. The model is similar to re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). However in contrast to re-ranking, we only have to prepare one model. The re-ranking approach requires other training datasets that are different from the data used in local model training. 3.2.2 Features for Argument Classification The local features used in our system are the same as our previous work (Watanabe et al., 2008) except for language dependent features. The global features that used in our system are based on (Johansson and Nugues, 2008) that used for re-ranking. Local Features Word features: Predicted lemma and predicted POS of the predicate, predicate’s head, argument candidat</context>
<context position="16040" citStr="Johansson and Nugues, 2008" startWordPosition="2567" endWordPosition="2570">h, the global features degraded the performance in labeled F1. Especially, in German, the precision is substantially degraded (-7.27% in labeled F1). These results indicate that it is necessary to introduce language dependent features. 4.3 Training, Evaluation Time and Memory Requirements Table 4 and 5 shows the training/evaluation times and the memory consumption of the second-order dependency parsers and the global argument classifiers respectively. The training times of the predicate classifier were less than one day, and the testing times were mere seconds. As reported in (Carreras, 2007; Johansson and Nugues, 2008), training and inference of the secondorder parser are very expensive. For Chinese, we could only complete 2 iterations. In terms of the argument classifier, since N-best generation time account for a substantial proportion of the training time (in this work N = 100), changiter hrs./iter sent./min. mem. Catalan 9 14.6 9.0 9.6 GB Chinese 2 56.5 3.7 16.2 GB Czech 8 14.6 20.5 12.6 GB English 7 22.0 13.4 15.1 GB German 4 12.3 59.1 13.1 GB Japanese 7 11.2 21.8 13.0 GB Spanish 7 19.5 7.3 17.9 GB Table 4: Training, evaluation time and memory requirements of the second-order dependency parsers. The ’i</context>
<context position="18261" citStr="Johansson and Nugues, 2008" startWordPosition="2939" endWordPosition="2942">c and semantic dependency analysis in multilingual. Although our system is not a joint approach but a pipeline approach, the system is comparable to the top system for some of the 7 languages. A further research direction we are investigating is the application of various types of global features. We believe that there is still room for improvements since we used only two types of global features for the argument classifier. Another research direction is investigating joint approaches. To the best of our knowledge, three 118 types of joint approaches have been proposed: N-best based approach (Johansson and Nugues, 2008), synchronous joint approach (Henderson et al., 2008), and a joint approach where parsing and SRL are performed simultaneously (Llu´ıs and M`arquez, 2008). We attempted to perform Nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it. We would like to investigate syntactic-semantic joint approaches with reasonable time complexities. Acknowledgments We would like to thank Richard Johansson for his advice on parser implementation, and the CoNLL2009 organizers (Hajiˇc et al., 2009; Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc </context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic-semantic analysis with propbank and nombank. In Proc. of CoNLL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Dependency-based Semantic Analysis of Natural-language Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Lund University.</institution>
<contexts>
<context position="5437" citStr="Johansson, 2008" startWordPosition="862" endWordPosition="863"> (13.94 %, 3.74 %, 25.79 % and 0.91 % respectively), therefore we need to deal with non-projectivity. In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005). However, growth of the number of dependency labels by pseudo-projective transformation increases the dependency parser training time, so we did not adopt transformations. Therefore, the parser ignores the presence of non-projective edges in the training and the testing phases. The features used for our dependency parser are based on those listed in (Johansson, 2008). In addition, distance features are used. We use shorthand notations in order to simplify the feature representations: ’h’, ’d’, ’c’, ’l’, ’p’, ’−1’ and ’+1’ correspond to head, dependent, head’s or dependent’s child, lemma, POS, left position and right position respectively. First-order Features Token features: hl, hp, hl+hp, dl, dp and dl+dp. Head-Dependent features: hp+dp, hl+dl, hl+dl, hl+hp+dl, hl+hp+dp, hl+dl+dp, hp+dl+dp and hl+hp+dl+dp. Context features: hp+hp+1+dp−1+dp, hp−1+hp+dp−1+dp, hp+hp+1+dp+dp+1 and hp−1+hp+dp+dp+1. Distance features: The number of tokens between the head and </context>
</contexts>
<marker>Johansson, 2008</marker>
<rawString>Richard Johansson. 2008. Dependency-based Semantic Analysis of Natural-language Text. Ph.D. thesis, Lund University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proc. of LREC-2002,</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands.</location>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proc. of LREC-2002, pages 2008–2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’Ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A new perceptron algorithm for sequence labeling with nonlocal features.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL</booktitle>
<contexts>
<context position="2486" citStr="Kazama and Torisawa, 2007" startWordPosition="346" endWordPosition="349">yntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classifiers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is defined as follows. s(y) = ∑ F(h, m, x) (1) (h,m)∈y where h and m denote the head and the dependent of the dependency </context>
<context position="8625" citStr="Kazama and Torisawa, 2007" startWordPosition="1346" endWordPosition="1349">A(p)) = w · ΦG(x, A(p)) (5) where ΦL, ΦG denote feature vectors for the local factor and the global factor respectively. FL scores a particular role assignment for each argument candidate individually, and FG treats global features that capture what structure the assignment A has. Resulting scoring function for the assignment A(p) is as follows. s(A(p)) = ∑ w·ΦL(x, a(p))+w·ΦG(x, A(p)) a(p)∈A(p) (6) Use of global features is problematic, because it becomes difficult to find the highest assignment efficiently. In order to deal with the problem, we use a simple approach, n-best relaxation as in (Kazama and Torisawa, 2007). At first we generate n-best assignments using only the local factor, and then add the global factor score for each n-best assignment, finally select the best scoring assignment from them. In order to generate n-best assignments, we used a beam-search algorithm. 3.2.1 Learning the Model As in dependency parser and predicate classifier, we train the model using the PA algorithm with parameter averaging. The learning algorithm is shown input Training set T = {xt, At}Tt=1, Number of iterations N and Parameter C w ← 0, v ← 0, c ← 1 for i ← 0 to N do for (xt, At) ∈ T do let Φ(xt, A) = Pa∈A ΦL(xt, </context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’Ichi Kazama and Kentaro Torisawa. 2007. A new perceptron algorithm for sequence labeling with nonlocal features. In Proc. of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An effective two-stage model for exploiting non-local dependencies in named entity recognition.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-COLING</booktitle>
<contexts>
<context position="2458" citStr="Krishnan and Manning, 2006" startWordPosition="342" endWordPosition="345">orithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classifiers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is defined as follows. s(y) = ∑ F(h, m, x) (1) (h,m)∈y where h and m denote the head and the </context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D. Manning. 2006. An effective two-stage model for exploiting non-local dependencies in named entity recognition. In Proc. of ACL-COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Llu´ıs</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A joint model for parsing syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<marker>Llu´ıs, M`arquez, 2008</marker>
<rawString>Xavier Llu´ıs and Llu´ıs M`arquez. 2008. A joint model for parsing syntactic and semantic dependencies. In Proc. of CoNLL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Multilingual dependency parsing using global features.</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<contexts>
<context position="2526" citStr="Nakagawa, 2007" startWordPosition="353" endWordPosition="354">ole labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classifiers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is defined as follows. s(y) = ∑ F(h, m, x) (1) (h,m)∈y where h and m denote the head and the dependent of the dependency edge in y, and F(h, m, x) is a Factor th</context>
</contexts>
<marker>Nakagawa, 2007</marker>
<rawString>Tetsuji Nakagawa. 2007. Multilingual dependency parsing using global features. In Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5067" citStr="Nivre and Nilsson, 2005" startWordPosition="805" endWordPosition="808">min “C w·Φ(xt ,ˆy)-w·Φ(xt,yt)+ρ(yt,ˆy) 1 ||Φ(xt,yt)-Φ(xt,ˆy)||2 J w +- w + τt(Φ(xt, yt) − Φ(xt, ˆy)) v +- v + cτt(Φ(xt, yt) − Φ(xt, ˆy)) c +- c + 1 end for end for return w − v/c We set p(yt, ˆy) as the number of incorrect head predictions in the ˆy, and C as 1.0. Among the 7 languages of the task, 4 languages (Czech, English, German and Japanese) contain non-projective edges (13.94 %, 3.74 %, 25.79 % and 0.91 % respectively), therefore we need to deal with non-projectivity. In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005). However, growth of the number of dependency labels by pseudo-projective transformation increases the dependency parser training time, so we did not adopt transformations. Therefore, the parser ignores the presence of non-projective edges in the training and the testing phases. The features used for our dependency parser are based on those listed in (Johansson, 2008). In addition, distance features are used. We use shorthand notations in order to simplify the feature representations: ’h’, ’d’, ’c’, ’l’, ’p’, ’−1’ and ’+1’ correspond to head, dependent, head’s or dependent’s child, lemma, POS,</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In Proc. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL-2008.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proc. of CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proc. of LREC-2008,</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Mart´ı, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Mart´ı, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proc. of LREC-2008, Marrakesh, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="2163" citStr="Toutanova et al., 2005" startWordPosition="298" endWordPosition="301">ch investigating higher-order dependency parsing algorithms has found its superiority to first-order parsing algorithms. To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin l</context>
<context position="9779" citStr="Toutanova et al., 2005" startWordPosition="1570" endWordPosition="1573"> 1 for i ← 0 to N do for (xt, At) ∈ T do let Φ(xt, A) = Pa∈A ΦL(xt, a) + ΦG(xt, A) generate n-best assignments {An} using FL Aˆ = arg maxA∈{An} w · Φ(xt, A) + p(At, A) Tt = min “” li w·Φ(xt, ˆA)−w·Φ(xt,At)+ρ(At,ˆA) ||Φ(xt,At)−Φ(xt, ˆA)||2 w ← w + Tt(Φ(xt, At) − Φ(xt, ˆA)) v ← v + cTt(Φ(xt, At) − Φ(xt, ˆA)) c ← c + 1 end for end for return w − v/c We set the margin value ρ(A, ˆA) as the number of incorrect assignments plus S(A, ˆA), and C as 1.0. The delta function returns 1 if at least one assignment is different from the correct assignment and 0 otherwise. The model is similar to re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). However in contrast to re-ranking, we only have to prepare one model. The re-ranking approach requires other training datasets that are different from the data used in local model training. 3.2.2 Features for Argument Classification The local features used in our system are the same as our previous work (Watanabe et al., 2008) except for language dependent features. The global features that used in our system are based on (Johansson and Nugues, 2008) that used for re-ranking. Local Features Word features: Predicted lemma and predicted POS of the predicate, predic</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proc. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yotaro Watanabe</author>
<author>Masakazu Iwatate</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A pipeline approach for syntactic and semantic dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<contexts>
<context position="10138" citStr="Watanabe et al., 2008" startWordPosition="1627" endWordPosition="1630"> margin value ρ(A, ˆA) as the number of incorrect assignments plus S(A, ˆA), and C as 1.0. The delta function returns 1 if at least one assignment is different from the correct assignment and 0 otherwise. The model is similar to re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). However in contrast to re-ranking, we only have to prepare one model. The re-ranking approach requires other training datasets that are different from the data used in local model training. 3.2.2 Features for Argument Classification The local features used in our system are the same as our previous work (Watanabe et al., 2008) except for language dependent features. The global features that used in our system are based on (Johansson and Nugues, 2008) that used for re-ranking. Local Features Word features: Predicted lemma and predicted POS of the predicate, predicate’s head, argument candidate, argument candidate’s head, leftmost/rightmost dependent and leftmost/rightmost sibling. Dependency label: The dependency label of predicate, argument candidate and argument candidate’s dependent. Family: The position of the argument candicate with respect to the predicate position in the dependency tree (e.g. child, sibling).</context>
</contexts>
<marker>Watanabe, Iwatate, Asahara, Matsumoto, 2008</marker>
<rawString>Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto. 2008. A pipeline approach for syntactic and semantic dependency parsing. In Proc. of CoNLL 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>