<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002416">
<title confidence="0.9940525">
Efficiency, Robustness and Accuracy
in Picky Chart Parsing*
</title>
<author confidence="0.998607">
David M. Magerman Carl Weir
</author>
<affiliation confidence="0.99623">
Stanford University Paramax Systems
</affiliation>
<address confidence="0.835468">
Stanford, CA 94305 Paoli, PA 19301
</address>
<email confidence="0.996536">
magerman@cs.stanford.edu weir@prc.unisys.com
</email>
<sectionHeader confidence="0.955481" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9998834">
This paper describes Picky, a probabilistic agenda-based
chart parsing algorithm which uses a technique called prob-
abilistic prediction to predict which grammar rules are likely
to lead to an acceptable parse of the input. Using a subopti-
mal search method, Picky significantly reduces the number of
edges produced by CKY-like chart parsing algorithms, while
maintaining the robustness of pure bottom-up parsers and
the accuracy of existing probabilistic parsers. Experiments
using Picky demonstrate how probabilistic modelling can im-
pact upon the efficiency, robustness and accuracy of a parser.
</bodyText>
<sectionHeader confidence="0.981372" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.991779410714286">
This paper addresses the question: Why should we use
probabilistic models in natural language understanding?
There are many answers to this question, only a few of
which are regularly addressed in the literature.
The first and most common answer concerns ambigu-
ity resolution. A probabilistic model provides a clearly
defined preference rule for selecting among grammati-
cal alternatives (i.e. the highest probability interpreta-
tion is selected). However, this use of probabilistic mod-
els assumes that we already have efficient methods for
generating the alternatives in the first place. While we
have 0(n3) algorithms for determining the grammatical-
ity of a sentence, parsing, as a component of a natural
language understanding tool, involves more than simply
determining all of the grammatical interpretations of an
input. In order for a natural language system to process
input efficiently and robustly, it must. process all intelligi-
ble sentences, grammatical or not, while not significantly
red uci ng th e system&apos;s efficiency.
This observation suggests two other answers to the cen-
tral question of this paper. Probabilistic models offer
a convenient scoring method for partial interpretations
in a. xell-formed substring table. High probability con-
stituents in the parser&apos;s chart can be used to interpret.
ungrammatical sentences. Probabilistic models can also
*Special thanks to Jerry Hobbs and Bob Moore at SRI for
providing access to their computers, and to Salim Roukos, Pe-
ter Brown, and Vincent and Steven Della Pietra at IBM for their
instructive lessons on probabilistic modelling of natural language.
be used for efficiency by providing a best-first search
heuristic to order the parsing agenda.
This paper proposes an agenda-based probabilistic chart
parsing algorithm which is both robust and efficient. The
algorithm, Pickyl, is considered robust because it will
potentially generate all constituents produced by a pure
bottom-up parser and rank these constituents by likeli-
hood. The efficiency of the algorithm is achieved through
a technique called probabilistic prediction, which helps
the algorithm avoid worst-case behavior. Probabilistic
prediction is a trainable technique for modelling where
edges are likely to occur in the chart-parsing process.2
Once the predicted edges are added to the chart using
probabilistic prediction, they are processed in a style
similar to agenda-based chart parsing algorithms. By
limiting the edges in the chart to those which are pre-
dicted by this model, the parser can process a sentence
while generating only the most likely constituents given
the input.
In this paper, we will present the Picky parsing al-
gorithm, describing both the original features of the
parser and those adapted from previous work. Then,
we will compare the implementation of Picky with exist-
ing probabilistic and non-probabilistic parsers. Finally,
we will report the results of experiments exploring how
Picky&apos;s algorithm copes with the tradeoffs of efficiency,
robustness, and accuracy.3
</bodyText>
<sectionHeader confidence="0.892266" genericHeader="method">
2. Probabilistic Models in &apos;Picky
</sectionHeader>
<bodyText confidence="0.9593525">
The probabilistic models used in the implementation of
Picky are independent of the algorithm. To facilitate the
comparison between the performance of Picky and its
predecessor, Pearl, the probabilistic model implemented
for Picky is similar to Pearl&apos;s scoring model, the context.-
&apos;Pearl probabilistic Earley-style parser (-Earl). Picky
</bodyText>
<footnote confidence="0.991392625">
probabilistic CKY-like parser (P-CKY).
2 Some familiarity with chart parsing terminology is assumed in
this paper. For terminological definitions, see [9], [10], [11], or [17].
3Sections 2 and 3, the descriptions of the probabilistic models
used in Picky and the Picky algorithm, are similar in content
to the corresponding sections of Magerman and Weir[131. The
experimental results and discussions which follow in sections 4-6
are original.
</footnote>
<page confidence="0.99811">
40
</page>
<bodyText confidence="0.999630285714286">
free grammar with context-sensitive probability (CFG
with CSP) model. This probabilistic model estimates
the probability of each parse T given the words in the
sentence S,P(TIS), by assuming that each non-terminal
and its immediate children are dependent on the non-
terminal&apos;s siblings and parent and on the part-of-speech
trigram centered at the beginning of that rule:
</bodyText>
<equation confidence="0.9159705">
P(TIS) P(A --รท 13A7, aoaia2) (1)
A ET
</equation>
<bodyText confidence="0.999643142857143">
where C is the non-terminal node which immediately
dominates A, al is the part-of-speech associated with the
leftmost word of constituent A, and a() and a2 are the
parts-of-speech of the words to the left and to the right
of al, respectively. See Magerman and Marcus 1991 [12]
for a more detailed description of the CFG with CSP
model.
</bodyText>
<sectionHeader confidence="0.964625" genericHeader="method">
3. The Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999906692307692">
A probabilistic language model, such as the aforemen-
tioned CFG with CSP model, provides a metric for eval-
uating the likelihood of a parse tree. However, while it
may suggest a method for evaluating partial parse trees,
a language model alone does not dictate the search strat-
egy for determining the most likely analysis of a.n input.
Since exhaustive search of the space of parse trees pro-
duced by a natural language grammar is generally not
feasible, a parsing model can best take advantage of a
probabilistic language model by incorporating it into a
parser which probabilistically models the parsing pro-
cess. Picky attempts to model the chart parsing process
for context-free grammars using probabilistic prediction.
Picky parses sentences in three phases: covered left-
corner phase (I), covered bidirectional phase (II), and
tree completion phase (III). Each phase uses a. differ-
ent method for proposing edges to be introduced to the
parse chart. The first phase, covered left-corner, uses
probabilistic prediction based on the left-corner word of
the left-most daughter of a. constituent to propose edges.
The covered bidirectional phase also uses probabilistic
prediction, but it allows prediction to occur from the
left-corner word of any daughter of a constituent, and
parses that constituent outward (bidirectionally) from
that daughter. These phases are referred to as &amp;quot;cov-
ered&amp;quot; because, during these phases, the parsing mech-
anism proposes only edges that have non-zero proba-
bility according to the prediction model, i.e. that have
been covered by the training process. The final phase,
tree completion, is essentially an exhaustive search of all
interpretations of the input according to the grammar.
However, the search proceeds in best-first order, accord-
ing to the measures provided by the language model.
This phase is used only when the probabilistic prediction
model fails to propose the edges necessary to complete
a parse of the sentence.
The following sections will present and motivate the pre-
diction techniques used by the algorithm, and will then
describe how they are implemented in each phase.
</bodyText>
<subsectionHeader confidence="0.986532">
3.1. Probabilistic Prediction
</subsectionHeader>
<bodyText confidence="0.924035">
Probabilistic prediction is a general method for using
probabilistic information extracted from a parsed corpus
to estimate the likelihood that predicting an edge at a
certain point in the chart will lead to a correct analysis
of the sentence. The Picky algorithm is not dependent
on the specific probabilistic prediction model used. The
model used in the implementation, which is similar to
the probabilistic language model, will be described.4
The prediction model used in the implementation of
Picky estimates the probability that an edge proposed
at a point in the chart will lead to a correct parse to be:
P(A -4 aBi3laciai a2), (2)
where al is the part-of-speech of the left-corner word of
B, ao is the part-of-speech of the word to the left of al
and a2 is the part-of-speech of the word to the right of
al.
To illustrate how this model is used, consider the sen-
tence
The cow raced past the barn. (3)
The word &amp;quot;cow&amp;quot; in the word sequence &amp;quot;the cow raced&amp;quot;
predicts NP -4 det n, but not NP det n PP,
since PP is unlikely to generate a verb, based on train-
ing materia1.5 Assuming the prediction model is well
trained, it will propose the interpretation of &amp;quot;raced&amp;quot;
as the beginning of a participial phrase modifying &amp;quot;the
cow,&amp;quot; as in
The cow raced past the barn mooed. (4)
However, the interpretation of &amp;quot;raced&amp;quot; as a past par-
ticiple will receive a low probability estimate relative to
the verb interpretation, since the prediction .model only
considers local context.
</bodyText>
<footnote confidence="0.991725090909091">
4It is not necessary for the prediction model to be the same as
the language model used to evaluate complete analyses. However,
it is helpful if this is the case, so that the probability estimates of
incomplete edges will be consistent with the probability estimates
of completed constituents.
5Throughout this discussion, we will describe the prediction
process using words as the predictors of edges. In the implementa-
tion, due to sparse data concerns, only parts-of-speech are used to
predict edges. Given more robust estimation techniques, a prob-
abilistic prediction model conditioned on word sequences is likely
to perform as well or better.
</footnote>
<page confidence="0.999205">
41
</page>
<bodyText confidence="0.999992">
The process of probabilistic prediction is analogous to
that of a human parser recognizing predictive lexical
items or sequences in a sentence and using these hints to
restrict the search for the correct analysis of the sentence.
For instance, a sentence beginning with a wh-word and
auxiliary inversion is very likely to be a question, and try-
ing to interpret it as an assertion is wasteful. If a verb is
generally ditransitive, one should look for two objects to
that verb instead of one or none. Using probabilistic pre-
diction, sentences whose interpretations are highly pre-
dictable based on the trained parsing model can be ana-
lyzed with little wasted effort, generating sometimes no
more than ten spurious constituents for sentences which
contain between 30 and 40 constituents! Also, in some
of these cases every predicted rule results in a completed
constituent, indicating that the model made no incorrect
predictions and was led astray only by genuine ambigu-
ities in parts of the sentence.
</bodyText>
<subsectionHeader confidence="0.876353">
3.2. Exhaustive Prediction
</subsectionHeader>
<bodyText confidence="0.9572855">
When probabilistic prediction fails to generate the edges
necessary to complete a parse of the sentence, exhaus-
tive prediction uses the edges which have been generated
in earlier phases to predict new edges which might com-
bine with them to produce a complete parse. Exhaus-
tive prediction is a combination of two existing types of
prediction, &amp;quot;over-the-top&amp;quot; prediction [11] and top-down
filtering.
Over-the-top prediction is applied to complete edges. A
completed edge A โ&gt; a will predict all edges of the form
B 0A-y.6
Top-down filtering is used to predict edges in order to
complete incomplete edges. An edge of the form A
aB0B1B2fl, where a B1 has been recognized, will predict
edges of the form Bo -4 7 before B1 and edges of the
form B2 -4 5 after B1.
</bodyText>
<subsectionHeader confidence="0.994058">
3.3. Bidirectional Parsing
</subsectionHeader>
<bodyText confidence="0.990815603174603">
The only difference between phases I and II is that phase
II allows bidirectional parsing. Bidirectional parsing is
a technique for initiating the parsing of a constituent
from any point in that constituent. Chart parsing algo-
rithms generally process constituents from left-to-right.
For instance, given a grammar rule
A -4 B1 B2 โข โข &apos; Bn, (5)
61n the implementation of Picky, over-the-top prediction for
A -รท a will only predict edges of the form B Ay. This limitation
on over-the-top prediction is due to the expensive bookkeeping
involved in bidirectional parsing. See the section on bidirectional
parsing for more details.
a parser generally would attempt to recognize a Bl, then
search for a B2 following it, and so on. Bidirectional
parsing recognizes an A by looking for any Bi . Once a
Bi has been parsed, a bidirectional parser looks for a
B1_1 to the left of the Bi, a Bi+i to the right, and so
on.
Bidirectional parsing is generally an inefficient tech-
nique, since it allows duplicate edges to be introduced
into the chart. As an example, consider a context-free
rule NP DET N, and assume that there is a deter-
miner followed by a noun in the sentence being parsed.
Using bidirectional parsing, this NP rule can be pre-
dicted both by the determiner and by the noun. The
edge predicted by the determiner will look to the right
for a noun, find one, and introduce a new edge consisting
of a completed NP. The edge predicted by the noun will
look to the left for a determiner, find one, and also intro-
duce a new edge consisting of a completed NP. Both of
these NPs represent identical parse trees, and are thus
redundant. If the algorithm permits both edges to be
inserted into the chart, then an edge XP a NP /3 will
be advanced by both NPs, creating two copies of every
XP edge. These duplicate XP edges can themselves be
used in other rules, and so on.
To avoid this propagation of redundant edges, the parser
must ensure that no duplicate edges are introduced into
the chart. Picky does this simply by verifying every time
an edge is added that the edge is not already in the chart.
Although eliminating redundant edges prevents exces-
sive inefficiency, bidirectional parsing may still perform
more work than traditional left-to-right parsing. In the
previous example, three edges are introduced into the
chart to parse the NP DET N edge. A left-to-right
parser would only introduce two edges, one when the
determiner is recognized, and another when the noun is
recognized.
The benefit of bidirectional parsing can be seen when
probabilistic prediction is introduced into the parser.
Frequently, the syntactic structure of a constituent is
not determined by its left-corner word. For instance,
in the sequence V NP PP, the prepositional phrase PP
can modify either the noun phrase NP or the entire verb
phrase V NP. These two interpretations require different
VP rules to be predicted, but the decision about which
rule to use depends on more than just the verb. The cor-
rect rule may best be predicted by knowing the preposi-
tion used in the PP. Using probabilistic prediction, the
decision is made by pursuing the rule which has the high-
est probability according to the prediction model. This
rule is then parsed bidirectionally. If this rule is in fact
the correct rule to analyze the constituent, then no other
</bodyText>
<page confidence="0.995812">
42
</page>
<bodyText confidence="0.999953333333333">
predictions will be made for that constituent, and there
will be no more edges produced than in left-to-right pars-
ing. Thus, the only case where bidirectional parsing is
less efficient than left-to-right parsing is when the pre-
diction model fails to capture the elements of context of
the sentence which determine its correct interpretation.
</bodyText>
<subsectionHeader confidence="0.992658">
3.4. The Three Phases of Picky
</subsectionHeader>
<bodyText confidence="0.9989164">
Covered Left-Corner The first phase uses probabilis-
tic prediction based on the part-of-speech sequences from
the input sentence to predict all grammar rules which
have a non-zero probability of being dominated by that
trigram (based on the training corpus), i.e.
</bodyText>
<equation confidence="0.99222">
P(A Bblaoaia2) &gt; 0 (6)
</equation>
<bodyText confidence="0.999938941176471">
where al is the part-of-speech of the left-corner word of
B. In this phase, the only exception to the probabilis-
tic prediction is that any rule which can immediately
dominate the preterminal category of any word in the
sentence is also predicted, regardless of its probability.
This type of prediction is referred to as exhaustive pre-
diction. All of the predicted rules are processed using a
standard best-first agenda processing algorithm, where
the highest scoring edge in the chart is advanced.
Covered Bidirectional If an S spanning the entire
word string is not recognized by the end of the first
phase, the covered bidirectional phase continues the
parsing process. Using the chart generated by the first
phase, rules are predicted not only by the trigram cen-
tered at the left-corner word of the rule, but by the
trigram centered at the left-corner word of any of the
children of that rule, i.e.
</bodyText>
<equation confidence="0.996966">
P(A โ+ aBblbobib2) &gt; 0. (7)
</equation>
<bodyText confidence="0.999514857142858">
where b1 is the part-of-speech associated with the left-
most word of constituent B. This phase introduces in-
complete theories into the chart which need to be ex-
panded to the left and to the right, as described in the
bidirectional parsing section above.
Tree Completion If the bidirectional processing fails
to produce a successful parse, then it is assumed that
there is some part of the input sentence which is not
covered well by the training material. In the final phase,
exhaustive prediction is performed on all complete the-
ories which were introduced in the previous phases but
which are not predicted by the trigra.ms beneath them
(i.e. P(rule I trigram) = 0).
In this phase, edges are only predicted by their left-
corner word. As mentioned previously, bidirectional
parsing can be inefficient when the prediction model is
inaccurate. Since all edges which the prediction model
assigns non-zero probability have already been predicted,
the model can no longer provide any information for
future predictions. Thus, bidirectional parsing in this
phase is very likely to be inefficient. Edges already in
the chart will be parsed bidirectionally, since they were
predicted by the model, but all new edges will be pre-
dicted by the left-corner word only.
Since it is already known that the prediction model will
assign a zero probability to these rules, these predictions
are instead scored based on the number of words spanned
by the subtree which predicted them. Thus, this phase
processes longer theories by introducing rules which can
advance them. Each new theory which is proposed by
the parsing process is exhaustively predicted for, using
the length-based scoring model.
The final phase is used only when a sentence is so far
outside of the scope of the training material that none
of the previous phases are able to process it. This phase
of the algorithm exhibits the worst-case exponential be-
havior that is found in chart parsers which do not use
node packing. Since the probabilistic model is no longer
useful in this phase, the parser is forced to propose an
enormous number of theories. The expectation (or hope)
is that one of the theories which spans most of the sen-
tence will be completed by this final process. Depending
on the size of the grammar used, it may be unfeasible
to allow the parser to exhaust all possible predicts be-
fore deciding an input is ungrammatical. The question
of when the parser should give up is an empirical issue
which will not be explored here.
Post-processing: Partial Parsing Once the final
phase has exhausted all predictions made by the gram-
mar, or more likely, once the probability of all edges
in the chart falls below a certain threshold, Picky deter-
mines the sentence to be ungrammatical. However, since
the chart produced by Picky contains all recognized con-
stituents, sorted by probability, the chart can be used to
extract partial parses. As implemented, Picky prints out
the most probable completed S constituent.
</bodyText>
<sectionHeader confidence="0.703387" genericHeader="method">
4. Why a New Algorithm?
</sectionHeader>
<bodyText confidence="0.998971">
Previous research efforts have produced a wide vari-
ety of parsing algorithms for probabilistic and non-
probabilistic grammars. One might question the need
for a. new algorithm to deal with context-sensitive prob-
abilistic models. However, these previous efforts have
generally failed to address both efficiency and robust-
ness effectively.
For non-probabilistic grammar models, the CKY algo-
ritlun [9] [17] provides efficiency and robustness in poly-
nomial time, 0(Gn3). CKY can be modified to han-
</bodyText>
<page confidence="0.999508">
43
</page>
<bodyText confidence="0.999975014492754">
die simple P-CFGs [2] without loss of efficiency. How-
ever, with the introduction of context-sensitive proba-
bility models, such as the history-based grammar[1] and
the CFG with CSP models[12], CKY cannot be mod-
ified to accommodate these models without exhibiting
exponential behavior in the grammar size G. The linear
behavior of CKY with respect to grammar size is depen-
dent upon being able to collapse the distinctions among
constituents of the same type which span the same part
of the sentence. However, when using a context-sensitive
probabilistic model, these distinctions are necessary. For
instance, in the CFG with CSP model, the part-of-
speech sequence generated by a constituent affects the
probability of constituents that dominate it. Thus, two
constituents which generate different part-of-speech se-
quences must be considered individually and cannot be
collapsed.
Earley&apos;s algorithm [6] is even more attractive than CKY
in terms of efficiency, but it suffers from the same expo-
nential behavior when applied to context-sensitive prob-
abilistic models. Still, Earley-style prediction improves
the average case performance of en exponential chart-
parsing algorithm by reducing the size of the search
space, as was shown in [12]. However, Earley-style pre-
diction has serious impacts on robust processing of un-
grammatical sentences. Once a sentence has been de-
termined to be ungrammatical, Earley-style prediction
prevents any new edges from being added to the parse
chart. This behavior seriously degrades the robustness
of a natural language system using this type of parser.
A few recent works on probabilistic parsing have pro-
posed algorithms and devices for efficient, robust chart
parsing. Bobrow[3] and Chitrao[4] introduce agenda-
based probabilistic parsing algorithms, although nei-
ther describe their algorithms in detail. Both algo-
rithms use a strictly best first search. As both Chitrao
and Magerman[12] observe, a best first search penalizes
longer and more complex constituents (i.e. constituents
which are composed of more edges), resulting in thrash-
ing and loss of efficiency. Chitrao proposes a heuristic
penalty based on constituent length to deal with this
problem. Magerman avoids thrashing by calculating the
score of a parse tree using the geometric mean of the
probabilities of the constituents contained in the tree.
Moore[14] discusses techniques for improving the effi-
ciency and robustness of chart parsers for unification
grammars, but the ideas are applicable to probabilistic
grammars as well. Some of the techniques proposed are
well-known ideas, such a.s compiling c-transitions (null
gaps) out of the grammar and heuristically controlling
the introduction of predictions.
The Picky parser incorporates what we deem to be the
most effective techniques of these previous works into
one parsing algorithm. New techniques, such as proba-
bilistic prediction and the multi-phase approach, are in-
troduced where the literature does not provide adequate
solutions. Picky combines the standard chart parsing
data structures with existing bottom-up and top-down
parsing operations, and includes a probabilistic version
of top-down filtering and over-the-top prediction. Picky
also incorporates a limited form of bi-directional pars-
ing in a way which avoids its computationally expensive
side-effects. It uses an agenda processing control mech-
anism with the scoring heuristics of Pearl.
With the exception of probabilistic prediction, most of
the ideas in this work individually are not original to the
parsing technology literature. However, the combination
of these ideas provides robustness without sacrificing ef-
ficiency, and efficiency without losing accuracy.
</bodyText>
<sectionHeader confidence="0.997472" genericHeader="evaluation">
5. Results of Experiments
</sectionHeader>
<bodyText confidence="0.999847541666667">
The Picky parser was tested on 3 sets of 100 sentences
which were held out from the rest of the corpus during
training. The training corpus consisted of 982 sentences
which were parsed using the same grammar that Picky
used. The training and test corpora are samples from the
MIT&apos;s Voyager direction-finding system.&apos; Using Picky&apos;s
grammar, these test sentences generate, on average, over
100 parses per sentence, with some sentences generated
over 1,000 parses.
The purpose of these experiments is to explore the im-
pact of varying of &apos;Picky&apos;s parsing algorithm on parsing
accuracy, efficiency, and robustness. For these exper-
iments, we varied three attributes of the parser: the
phases used by parser, the maximum number of edges
the parser can produce before failure, and the minimum
probability parse acceptable.
In the following analysis, the accuracy rate represents
the percentage of the test sentences for which the high-
est probability parse generated by the parser is identical
to the &amp;quot;correct&amp;quot; parse tree indicated in the parsed test.
corpus.8
Efficiency is measured by two ratios, the prediction ratio
and the completion ratio. The prediction ratio is defined
as the ratio of number of predictions made by the parser
</bodyText>
<footnote confidence="0.985151875">
7Special thanks to Victor Zue at MIT for the use of the speech
data from MIT&apos;s Voyager system.
8There are two exceptions to this accuracy measure. If the
parser generates a plausible parse for a sentences which has multi-
ple plausible interpretations, the parse is considered correct. Also,
if the parser generates a correct parse, but the parsed test corpus
contains an incorrect parse (i.e. if there is an error in the answer
key), the parse is considered correct..
</footnote>
<page confidence="0.999225">
44
</page>
<bodyText confidence="0.999789952380952">
during the parse of a sentence to the number of con-
stituents necessary for a correct parse. The completion
ratio is the ratio of the number of completed edges to
the number of predictions during the parse of sentence.
Robustness cannot be measured directly by these ex-
periments, since there are few ungrammatical sentences
and there is no implemented method for interpreting the
well-formed substring table when a parse fails. However,
for each configuration of the parser, we will explore the
expected behavior of the parser in the face of ungram-
matical input.
Since Picky has the power of a pure bottom-up parser,
it would be useful to compare its performance and effi-
ciency to that of a probabilistic bottom-up parser. How-
ever, an implementation of a probabilistic bottom-up
parser using the same grammar produces on average
over 1000 constituents for each sentence, generating over
15,000 edges without generating a parse at all! This
supports our claim that exhaustive CKY-like parsing al-
gorithms are not feasible when probabilistic models are
applied to them.
</bodyText>
<subsectionHeader confidence="0.945629">
5.1. Control Configuration
</subsectionHeader>
<bodyText confidence="0.9998842">
The control for our experiments is the configuration of
Picky with all three phases and with a maximum edge
count of 15,000. Using this configuration, Picky parsed
the 3 test sets with an 89.3% accuracy rate. This is
a slight improvement over Pearl&apos;s 87.5% accuracy rate
reported in [12].
Recall that we will measure the efficiency of a parser
configuration by its prediction ratio and completion ratio
on the test sentences. A perfect prediction ratio is 1:1,
i.e. every edge predicted is used in the eventual parse.
However, since there is ambiguity in the input sentences,
a 1:1 prediction ratio is not likely to be achieved. Picky&apos;s
prediction ratio is approximately than 4.3:1, and its ratio
of predicted edges to completed edges is nearly 1.3:1.
Thus, although the prediction ratio is not perfect, on
average for every edge that is predicted more than one
completed constituent results.
This is the most robust configuration of Picky which will
be attempted in our experiments, since it includes bidi-
rectional parsing (phase II) and allows so many edges to
be created. Although there was not a sufficient num-
ber or variety of ungrammatical sentences to explore
the robustness of this configuration further, one inter-
esting example did occur in the test sets. The sentence
How do I how do I get to MIT?
is an ungrammatical but interpretable sentence which
begins with a restart. The Pearl parser would have gen-
erated no analysis for the latter part of the sentence and
the corresponding sections of the chart would be empty.
Using bidirectional probabilistic prediction, Picky pro-
duced a correct partial interpretation of the last 6 words
of the sentence, &amp;quot;how do I get to MIT?&amp;quot; One sentence
does not make for conclusive evidence, but it repre-
sents the type of performance which is expected from
the Picky algorithm.
</bodyText>
<subsectionHeader confidence="0.970433">
5.2. Phases vs. Efficiency
</subsectionHeader>
<bodyText confidence="0.999886076923077">
Each of Picky&apos;s three phases has a distinct role in the
parsing process. Phase I tries to parse the sentences
which are most standard, i.e. most consistent with the
training material. Phase II uses bidirectional parsing to
try to complete the parses for sentences which are nearly
completely parsed by Phase I. Phase III uses a simplis-
tic heuristic to glue together constituents generated by
phases I and II. Phase III is obviously inefficient, since it
is by definition processing atypical sentences. Phase II
is also inefficient because of the bidirectional predictions
added in this phase. But phase II also amplifies the in-
efficiency of phase III, since the bidirectional predictions
added in phase II are processed further in phase III.
</bodyText>
<table confidence="0.999424444444445">
Phases Pred. Comp. Coverage %Error
Ratio Ratio
I 1.95 1.02 75.7% 2.3%
1,11 2.15 0.94 77.0% 2.3%
II 2.44 0.86 77.3% 2.0%
LIII 4.01 1.44 88.3% 11.7%
III 4.29 1.40 88.7% 11.3%
1,11,111 4.30 1.28 89.3% 10.7%
HMI 4.59 1.24 89.7% 10.3%
</table>
<tableCaption confidence="0.962014666666667">
Table 1: Prediction and Completion Ratios and accuracy
statistics for Picky configured with different subsets of
Picky&apos;s three phases.
</tableCaption>
<bodyText confidence="0.999882">
In Table 1, we see the efficiency and accuracy of Picky
using different subsets of the parser&apos;s phases. Using the
control parser (phases 1,11, and II), the parser has a 4.3:1
prediction ratio and a 1.3:1 completion ratio.
By omitting phase III, we eliminate nearly half of the
predictions and half the completed edges, resulting in
a 2.15:1 prediction ratio. But this efficiency comes at
the cost of coverage, which will be discussed in the next
section.
By omitting phase II, we observe a slight reduction in
predictions, but an increase in completed edges. This
behavior results from the elimination of the bidirectional
predictions, which tend to generate duplicate edges.
Note that this configuration, while slightly more efficient,
</bodyText>
<page confidence="0.997734">
45
</page>
<bodyText confidence="0.658736">
is less robust in processing ungrammatical input.
</bodyText>
<subsectionHeader confidence="0.555732">
5.3. Phases vs. Accuracy
</subsectionHeader>
<bodyText confidence="0.998997428571428">
For some natural language applications, such as a natu-
ral language interface to a nuclear reactor or to a com-
puter operating system, it is imperative for the user to
have confidence in the parses generated by the parser.
Picky has a relatively high parsing accuracy rate of
nearly 90%; however, 10% error is far too high for fault-
intolerant applications.
</bodyText>
<table confidence="0.99894825">
Phase No. Accuracy Coverage %Error
I + II 238 97% 77% 3%
III 62 60% 12% 40%
Overall 300 89.3% 89.3% 10.7%
</table>
<tableCaption confidence="0.990744">
Table 2: Picky&apos;s parsing accuracy, categorized by the
</tableCaption>
<bodyText confidence="0.998321142857143">
phase which the parser reached in processing the test
sentences.
Consider the data in Table 2. While the parser has an
overall accuracy rate of 89.3%, it is far more accurate on
sentences which are parsed by phases I and II, at 97%.
Note that 238 of the 300 sentences, or 79%, of the test
sentences are parsed in these two phases. Thus, by elimi-
nating phase III, the percent error can be reduced to 3%,
while maintaining 77% coverage. An alternative to elim-
inating phase III is to replace the length-based heuristic
of this phase with a secondary probabilistic model of the
difficult sentences in this domain. This secondary model
might be trained on a set of sentences which cannot be
parsed in phases I and II.
</bodyText>
<subsectionHeader confidence="0.65665">
5.4. Edge Count vs. Accuracy
</subsectionHeader>
<bodyText confidence="0.995271058823529">
In the original implementation of the Picky algorithm,
we intended to allow the parser to generate edges un-
til it found a complete interpretation or exhausted all
possible predictions. However, for some ungrammati-
cal sentences, the parser generates tens of thousands of
edges without terminating. To limit the processing time
for the experiments, we implemented a maximum edge
count which was sufficiently large so that all grammat-
ical sentences in the test corpus would be parsed. All
of the grammatical test sentences generated a parse be-
fore producing 15,000 edges. However, some sentences
produced thousands of edges only to generate an incor-
rect parse. In fact, it seemed likely that there might be
a correlation between very high edge counts and incor-
rect parses. We tested this hypothesis by varying the
maximum edge count.
En Table 3, we see an increase in efficiency and a decrease
</bodyText>
<table confidence="0.999643375">
Maximum Pred. Comp. Coverage %Error
Edge Count Ratio Ratio
15,000 4.30 1.35 89.3% 10.7%
1,000 3.69 0.93 83.3% 7.0%
500 3.08 0.82 80.3% 5.3%
300 2.50 0.86 79.3% 2.7%
150 1.95 0.92 66.0% 1.7%
100 1.60 0.84 43.7% 1.7%
</table>
<tableCaption confidence="0.970294">
Table 3: Prediction and Completion Ratios and accuracy
statistics for Picky configured with different maximum
edge count.
</tableCaption>
<bodyText confidence="0.9980526">
in accuracy as we reduce the maximum number of edges
the parser will generate before declaring a sentence un-
grammatical. By reducing the maximum edge count by
a factor of 50, from 15,000 to 300, we can nearly cut
in half the number of predicts and edges generated by
the parser. And while this causes the accuracy rate to
fall from 89.3% to 79.3%, it also results in a significant
decrease in error rate, down to 2.7%. By decreasing the
maximum edge count down to 150, the error rate can be
reduced to 1.7%.
</bodyText>
<subsectionHeader confidence="0.774094">
5.5. Probability vs. Accuracy
</subsectionHeader>
<bodyText confidence="0.999826684210526">
Since a probability represents the likelihood of an inter-
pretation, it is not unreasonable to expect the probabil-
ity of a parse tree to be correlated with the accuracy of
the parse. However, based on the probabilities associ-
ated with the &amp;quot;correct&amp;quot; parse trees of the test sentences,
there appears to be no such correlation. Many of the
test sentences had correct parses with very low probabil-
ities (10-10), while others had much higher probabilities
(10-2). And the probabilities associated with incorrect
parses were not distinguishable from the probabilities of
correct parses.
The failure to find a correlation between probability and
accuracy in this experiment does not prove conclusively
that no such correlation exists. Admittedly, the training
corpus used for all of these experiments is far smaller
than one would hope to estimate the CFG with CSP
model parameters. Thus, while the model is trained well
enough to steer the parsing search, it may not be suffi-
ciently trained to provide meaningful probability values.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.99939825">
There are many different applications of natural lan-
guage parsing, and each application has a different cost
threshold for efficiency, robustness, and accuracy. The
Picky algorithm introduces a framework for integrating
</bodyText>
<page confidence="0.997194">
46
</page>
<bodyText confidence="0.999946769230769">
these thresholds into the configuration of the parser in
order to maximize the effectiveness of the parser for the
task at hand. An application which requires a high de-
gree of accuracy would omit the Tree Completion phase
of the parser. A real-time application would limit the
number of edges generated by the parser, likely at the
cost of accuracy. An application which is robust to er-
rors but requires efficient processing of input would omit
the Covered Bidirectional phase.
The Picky parsing algorithm illustrates how probabilis-
tic modelling of natural language can be used to improve
the efficiency, robustness, and accuracy of natural lan-
guage understanding tools.
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999990706896552">
1. Black, E., Jelinek, F., Lafferty, J., Magerman, D. M.,
Mercer, R. and Roukos, S. 1992. Towards History-based
Grammars: Using Richer Models of Context in Prob-
abilistic Parsing. In Proceedings of the February 1992
DARPA Speech and Natural Language Workshop. Ar-
den House, NY.
2. Brown, P., Jelinek, F., and Mercer, R. 1991. Basic
Method of Probabilistic Context-free Grammars. IBM
Internal Report. Yorktown Heights, NY.
3. Bobrow, R. J. 1991. Statistical Agenda Parsing. In Pro-
ceedings of the February 1991 DARPA Speech and Nat-
ural Language Workshop. Asilomar, California.
4. Chitrao, M. and Grishman, R. 1990. Statistical Parsing
of Messages. In Proceedings of the June 1990 DARPA
Speech and Natural Language Workshop. Hidden Valley,
Pennsylvania.
5. Church, K. 1988. A Stochastic Parts Program and Noun
Phrase Parser for Unrestricted Text. In Proceedings of
the Second Conference on Applied Natural Language
Processing. Austin, Texas.
6. Earley, J. 1970. An Efficient Context-Free Parsing Algo-
rithm. Communications of the ACM Vol. 13, No. 2, pp.
94-102.
7. Gale, W. A. and Church, K. 1990. Poor Estimates of
Context are Worse than None. In Proceedings of the
June 1990 DARPA Speech and Natural Language Work-
shop. Hidden Valley, Pennsylvania.
8. Jelinek, F. 1985. Self-organizing Language Modeling for
Speech Recognition. IBM Report.
9. Kasami, T. 1965. An Efficient Recognition and Syn-
tax Algorithm for Context-Free Languages. Scientific
Report AFCRL-65-758, Air Force Cambridge Research
Laboratory. Bedford, Massachusetts.
10. Kay, M. 1980. Algorithm Schemata and Data Structures
in Syntactic Processing. CSL-80-12, October 1980.
11. Kimball, J. 1973. Principles of Surface Structure Parsing
in Natural Language. Cognition, 2.15-47.
12. Magerman, D. M. and Marcus, M. P. 1991. Pearl: A
Probabilistic Chart. Parser. In Proceedings of the Euro-
pean ACL Conference, March 1991. Berlin, Germany.
13. Magerman, D. M. and New, C. 1992. Probabilistic Pre-
diction and Picky Chart Parsing. In Proceedings of the
February 1992 DARPA Speech and Natural Language
Workshop. Arden House, NY.
14. Moore, R. and Dowding, J. 1991. Efficient Bottom-Up
Parsing. In Proceedings of the February 1991 DARPA
Speech and Natural Language Workshop. Asilomar, Cal-
ifornia.
15. Sharman, R. A., Jelinek, F., and Mercer, R. 1990. Gen-
erating a Grammar for Statistical Training. In Proceed-
ings of the June 1990 DARPA Speech and Natural Lan-
guage Workshop. Hidden Valley, Pennsylvania.
16. Seneff, Stephanie 1989. TINA. In Proceedings of the Au-
gust 1989 International Workshop in Parsing Technolo-
gies. Pittsburgh, Pennsylvania.
17. Younger, D. H. 1967. Recognition and Parsing of
Context-Free Languages in Time n3. Information and
Control Vol. 10, No. 2, pp. 189-208.
</reference>
<page confidence="0.999492">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955112">
<title confidence="0.9985175">Efficiency, Robustness and Accuracy in Picky Chart Parsing*</title>
<author confidence="0.99996">David M Magerman Carl Weir</author>
<affiliation confidence="0.998684">Stanford University Paramax Systems</affiliation>
<address confidence="0.995904">Stanford, CA 94305 Paoli, PA 19301</address>
<email confidence="0.989257">magerman@cs.stanford.eduweir@prc.unisys.com</email>
<abstract confidence="0.997493090909091">This paper describes Picky, a probabilistic agenda-based parsing algorithm which uses a technique called probprediction predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D M Magerman</author>
<author>R Mercer</author>
<author>S Roukos</author>
</authors>
<title>Towards History-based Grammars: Using Richer Models of Context in Probabilistic Parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the February</booktitle>
<publisher>Arden House, NY.</publisher>
<contexts>
<context position="20073" citStr="[1]" startWordPosition="3250" endWordPosition="3250">iety of parsing algorithms for probabilistic and nonprobabilistic grammars. One might question the need for a. new algorithm to deal with context-sensitive probabilistic models. However, these previous efforts have generally failed to address both efficiency and robustness effectively. For non-probabilistic grammar models, the CKY algoritlun [9] [17] provides efficiency and robustness in polynomial time, 0(Gn3). CKY can be modified to han43 die simple P-CFGs [2] without loss of efficiency. However, with the introduction of context-sensitive probability models, such as the history-based grammar[1] and the CFG with CSP models[12], CKY cannot be modified to accommodate these models without exhibiting exponential behavior in the grammar size G. The linear behavior of CKY with respect to grammar size is dependent upon being able to collapse the distinctions among constituents of the same type which span the same part of the sentence. However, when using a context-sensitive probabilistic model, these distinctions are necessary. For instance, in the CFG with CSP model, the part-ofspeech sequence generated by a constituent affects the probability of constituents that dominate it. Thus, two co</context>
</contexts>
<marker>1.</marker>
<rawString>Black, E., Jelinek, F., Lafferty, J., Magerman, D. M., Mercer, R. and Roukos, S. 1992. Towards History-based Grammars: Using Richer Models of Context in Probabilistic Parsing. In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop. Arden House, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Basic Method of Probabilistic Context-free Grammars. IBM Internal Report.</title>
<date>1991</date>
<location>Yorktown Heights, NY.</location>
<contexts>
<context position="19936" citStr="[2]" startWordPosition="3231" endWordPosition="3231">d, Picky prints out the most probable completed S constituent. 4. Why a New Algorithm? Previous research efforts have produced a wide variety of parsing algorithms for probabilistic and nonprobabilistic grammars. One might question the need for a. new algorithm to deal with context-sensitive probabilistic models. However, these previous efforts have generally failed to address both efficiency and robustness effectively. For non-probabilistic grammar models, the CKY algoritlun [9] [17] provides efficiency and robustness in polynomial time, 0(Gn3). CKY can be modified to han43 die simple P-CFGs [2] without loss of efficiency. However, with the introduction of context-sensitive probability models, such as the history-based grammar[1] and the CFG with CSP models[12], CKY cannot be modified to accommodate these models without exhibiting exponential behavior in the grammar size G. The linear behavior of CKY with respect to grammar size is dependent upon being able to collapse the distinctions among constituents of the same type which span the same part of the sentence. However, when using a context-sensitive probabilistic model, these distinctions are necessary. For instance, in the CFG wit</context>
</contexts>
<marker>2.</marker>
<rawString>Brown, P., Jelinek, F., and Mercer, R. 1991. Basic Method of Probabilistic Context-free Grammars. IBM Internal Report. Yorktown Heights, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
</authors>
<title>Statistical Agenda Parsing.</title>
<date>1991</date>
<booktitle>In Proceedings of the February 1991 DARPA Speech and Natural Language Workshop.</booktitle>
<location>Asilomar, California.</location>
<contexts>
<context position="21625" citStr="[3]" startWordPosition="3488" endWordPosition="3488">verage case performance of en exponential chartparsing algorithm by reducing the size of the search space, as was shown in [12]. However, Earley-style prediction has serious impacts on robust processing of ungrammatical sentences. Once a sentence has been determined to be ungrammatical, Earley-style prediction prevents any new edges from being added to the parse chart. This behavior seriously degrades the robustness of a natural language system using this type of parser. A few recent works on probabilistic parsing have proposed algorithms and devices for efficient, robust chart parsing. Bobrow[3] and Chitrao[4] introduce agendabased probabilistic parsing algorithms, although neither describe their algorithms in detail. Both algorithms use a strictly best first search. As both Chitrao and Magerman[12] observe, a best first search penalizes longer and more complex constituents (i.e. constituents which are composed of more edges), resulting in thrashing and loss of efficiency. Chitrao proposes a heuristic penalty based on constituent length to deal with this problem. Magerman avoids thrashing by calculating the score of a parse tree using the geometric mean of the probabilities of the co</context>
</contexts>
<marker>3.</marker>
<rawString>Bobrow, R. J. 1991. Statistical Agenda Parsing. In Proceedings of the February 1991 DARPA Speech and Natural Language Workshop. Asilomar, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chitrao</author>
<author>R Grishman</author>
</authors>
<title>Statistical Parsing of Messages.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<contexts>
<context position="21640" citStr="[4]" startWordPosition="3490" endWordPosition="3490">formance of en exponential chartparsing algorithm by reducing the size of the search space, as was shown in [12]. However, Earley-style prediction has serious impacts on robust processing of ungrammatical sentences. Once a sentence has been determined to be ungrammatical, Earley-style prediction prevents any new edges from being added to the parse chart. This behavior seriously degrades the robustness of a natural language system using this type of parser. A few recent works on probabilistic parsing have proposed algorithms and devices for efficient, robust chart parsing. Bobrow[3] and Chitrao[4] introduce agendabased probabilistic parsing algorithms, although neither describe their algorithms in detail. Both algorithms use a strictly best first search. As both Chitrao and Magerman[12] observe, a best first search penalizes longer and more complex constituents (i.e. constituents which are composed of more edges), resulting in thrashing and loss of efficiency. Chitrao proposes a heuristic penalty based on constituent length to deal with this problem. Magerman avoids thrashing by calculating the score of a parse tree using the geometric mean of the probabilities of the constituents cont</context>
</contexts>
<marker>4.</marker>
<rawString>Chitrao, M. and Grishman, R. 1990. Statistical Parsing of Messages. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing.</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="11853" citStr="(5)" startWordPosition="1869" endWordPosition="1869">down filtering is used to predict edges in order to complete incomplete edges. An edge of the form A aB0B1B2fl, where a B1 has been recognized, will predict edges of the form Bo -4 7 before B1 and edges of the form B2 -4 5 after B1. 3.3. Bidirectional Parsing The only difference between phases I and II is that phase II allows bidirectional parsing. Bidirectional parsing is a technique for initiating the parsing of a constituent from any point in that constituent. Chart parsing algorithms generally process constituents from left-to-right. For instance, given a grammar rule A -4 B1 B2 โข โข &apos; Bn, (5) 61n the implementation of Picky, over-the-top prediction for A -รท a will only predict edges of the form B Ay. This limitation on over-the-top prediction is due to the expensive bookkeeping involved in bidirectional parsing. See the section on bidirectional parsing for more details. a parser generally would attempt to recognize a Bl, then search for a B2 following it, and so on. Bidirectional parsing recognizes an A by looking for any Bi . Once a Bi has been parsed, a bidirectional parser looks for a B1_1 to the left of the Bi, a Bi+i to the right, and so on. Bidirectional parsing is generally</context>
</contexts>
<marker>5.</marker>
<rawString>Church, K. 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In Proceedings of the Second Conference on Applied Natural Language Processing. Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM</journal>
<volume>13</volume>
<pages>94--102</pages>
<contexts>
<context position="20813" citStr="[6]" startWordPosition="3363" endWordPosition="3363">ar size G. The linear behavior of CKY with respect to grammar size is dependent upon being able to collapse the distinctions among constituents of the same type which span the same part of the sentence. However, when using a context-sensitive probabilistic model, these distinctions are necessary. For instance, in the CFG with CSP model, the part-ofspeech sequence generated by a constituent affects the probability of constituents that dominate it. Thus, two constituents which generate different part-of-speech sequences must be considered individually and cannot be collapsed. Earley&apos;s algorithm [6] is even more attractive than CKY in terms of efficiency, but it suffers from the same exponential behavior when applied to context-sensitive probabilistic models. Still, Earley-style prediction improves the average case performance of en exponential chartparsing algorithm by reducing the size of the search space, as was shown in [12]. However, Earley-style prediction has serious impacts on robust processing of ungrammatical sentences. Once a sentence has been determined to be ungrammatical, Earley-style prediction prevents any new edges from being added to the parse chart. This behavior serio</context>
</contexts>
<marker>6.</marker>
<rawString>Earley, J. 1970. An Efficient Context-Free Parsing Algorithm. Communications of the ACM Vol. 13, No. 2, pp. 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K Church</author>
</authors>
<title>Poor Estimates of Context are Worse than None.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<contexts>
<context position="16423" citStr="(7)" startWordPosition="2651" endWordPosition="2651">tive prediction. All of the predicted rules are processed using a standard best-first agenda processing algorithm, where the highest scoring edge in the chart is advanced. Covered Bidirectional If an S spanning the entire word string is not recognized by the end of the first phase, the covered bidirectional phase continues the parsing process. Using the chart generated by the first phase, rules are predicted not only by the trigram centered at the left-corner word of the rule, but by the trigram centered at the left-corner word of any of the children of that rule, i.e. P(A โ+ aBblbobib2) &gt; 0. (7) where b1 is the part-of-speech associated with the leftmost word of constituent B. This phase introduces incomplete theories into the chart which need to be expanded to the left and to the right, as described in the bidirectional parsing section above. Tree Completion If the bidirectional processing fails to produce a successful parse, then it is assumed that there is some part of the input sentence which is not covered well by the training material. In the final phase, exhaustive prediction is performed on all complete theories which were introduced in the previous phases but which are not p</context>
</contexts>
<marker>7.</marker>
<rawString>Gale, W. A. and Church, K. 1990. Poor Estimates of Context are Worse than None. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organizing Language Modeling for Speech Recognition.</title>
<date>1985</date>
<tech>IBM Report.</tech>
<marker>8.</marker>
<rawString>Jelinek, F. 1985. Self-organizing Language Modeling for Speech Recognition. IBM Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An Efficient Recognition and Syntax Algorithm for Context-Free Languages. Scientific Report AFCRL-65-758, Air Force Cambridge Research Laboratory.</title>
<date>1965</date>
<location>Bedford, Massachusetts.</location>
<contexts>
<context position="4374" citStr="[9]" startWordPosition="644" endWordPosition="644">algorithm copes with the tradeoffs of efficiency, robustness, and accuracy.3 2. Probabilistic Models in &apos;Picky The probabilistic models used in the implementation of Picky are independent of the algorithm. To facilitate the comparison between the performance of Picky and its predecessor, Pearl, the probabilistic model implemented for Picky is similar to Pearl&apos;s scoring model, the context.- &apos;Pearl probabilistic Earley-style parser (-Earl). Picky probabilistic CKY-like parser (P-CKY). 2 Some familiarity with chart parsing terminology is assumed in this paper. For terminological definitions, see [9], [10], [11], or [17]. 3Sections 2 and 3, the descriptions of the probabilistic models used in Picky and the Picky algorithm, are similar in content to the corresponding sections of Magerman and Weir[131. The experimental results and discussions which follow in sections 4-6 are original. 40 free grammar with context-sensitive probability (CFG with CSP) model. This probabilistic model estimates the probability of each parse T given the words in the sentence S,P(TIS), by assuming that each non-terminal and its immediate children are dependent on the nonterminal&apos;s siblings and parent and on the p</context>
<context position="19817" citStr="[9]" startWordPosition="3210" endWordPosition="3210">ains all recognized constituents, sorted by probability, the chart can be used to extract partial parses. As implemented, Picky prints out the most probable completed S constituent. 4. Why a New Algorithm? Previous research efforts have produced a wide variety of parsing algorithms for probabilistic and nonprobabilistic grammars. One might question the need for a. new algorithm to deal with context-sensitive probabilistic models. However, these previous efforts have generally failed to address both efficiency and robustness effectively. For non-probabilistic grammar models, the CKY algoritlun [9] [17] provides efficiency and robustness in polynomial time, 0(Gn3). CKY can be modified to han43 die simple P-CFGs [2] without loss of efficiency. However, with the introduction of context-sensitive probability models, such as the history-based grammar[1] and the CFG with CSP models[12], CKY cannot be modified to accommodate these models without exhibiting exponential behavior in the grammar size G. The linear behavior of CKY with respect to grammar size is dependent upon being able to collapse the distinctions among constituents of the same type which span the same part of the sentence. Howe</context>
</contexts>
<marker>9.</marker>
<rawString>Kasami, T. 1965. An Efficient Recognition and Syntax Algorithm for Context-Free Languages. Scientific Report AFCRL-65-758, Air Force Cambridge Research Laboratory. Bedford, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<date>1980</date>
<booktitle>Algorithm Schemata and Data Structures in Syntactic Processing.</booktitle>
<pages>80--12</pages>
<contexts>
<context position="4380" citStr="[10]" startWordPosition="645" endWordPosition="645">ithm copes with the tradeoffs of efficiency, robustness, and accuracy.3 2. Probabilistic Models in &apos;Picky The probabilistic models used in the implementation of Picky are independent of the algorithm. To facilitate the comparison between the performance of Picky and its predecessor, Pearl, the probabilistic model implemented for Picky is similar to Pearl&apos;s scoring model, the context.- &apos;Pearl probabilistic Earley-style parser (-Earl). Picky probabilistic CKY-like parser (P-CKY). 2 Some familiarity with chart parsing terminology is assumed in this paper. For terminological definitions, see [9], [10], [11], or [17]. 3Sections 2 and 3, the descriptions of the probabilistic models used in Picky and the Picky algorithm, are similar in content to the corresponding sections of Magerman and Weir[131. The experimental results and discussions which follow in sections 4-6 are original. 40 free grammar with context-sensitive probability (CFG with CSP) model. This probabilistic model estimates the probability of each parse T given the words in the sentence S,P(TIS), by assuming that each non-terminal and its immediate children are dependent on the nonterminal&apos;s siblings and parent and on the part-of</context>
</contexts>
<marker>10.</marker>
<rawString>Kay, M. 1980. Algorithm Schemata and Data Structures in Syntactic Processing. CSL-80-12, October 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<date>1973</date>
<booktitle>Principles of Surface Structure Parsing in Natural Language. Cognition,</booktitle>
<pages>2--15</pages>
<contexts>
<context position="4386" citStr="[11]" startWordPosition="646" endWordPosition="646">opes with the tradeoffs of efficiency, robustness, and accuracy.3 2. Probabilistic Models in &apos;Picky The probabilistic models used in the implementation of Picky are independent of the algorithm. To facilitate the comparison between the performance of Picky and its predecessor, Pearl, the probabilistic model implemented for Picky is similar to Pearl&apos;s scoring model, the context.- &apos;Pearl probabilistic Earley-style parser (-Earl). Picky probabilistic CKY-like parser (P-CKY). 2 Some familiarity with chart parsing terminology is assumed in this paper. For terminological definitions, see [9], [10], [11], or [17]. 3Sections 2 and 3, the descriptions of the probabilistic models used in Picky and the Picky algorithm, are similar in content to the corresponding sections of Magerman and Weir[131. The experimental results and discussions which follow in sections 4-6 are original. 40 free grammar with context-sensitive probability (CFG with CSP) model. This probabilistic model estimates the probability of each parse T given the words in the sentence S,P(TIS), by assuming that each non-terminal and its immediate children are dependent on the nonterminal&apos;s siblings and parent and on the part-of-speec</context>
<context position="11099" citStr="[11]" startWordPosition="1737" endWordPosition="1737">ome of these cases every predicted rule results in a completed constituent, indicating that the model made no incorrect predictions and was led astray only by genuine ambiguities in parts of the sentence. 3.2. Exhaustive Prediction When probabilistic prediction fails to generate the edges necessary to complete a parse of the sentence, exhaustive prediction uses the edges which have been generated in earlier phases to predict new edges which might combine with them to produce a complete parse. Exhaustive prediction is a combination of two existing types of prediction, &amp;quot;over-the-top&amp;quot; prediction [11] and top-down filtering. Over-the-top prediction is applied to complete edges. A completed edge A โ&gt; a will predict all edges of the form B 0A-y.6 Top-down filtering is used to predict edges in order to complete incomplete edges. An edge of the form A aB0B1B2fl, where a B1 has been recognized, will predict edges of the form Bo -4 7 before B1 and edges of the form B2 -4 5 after B1. 3.3. Bidirectional Parsing The only difference between phases I and II is that phase II allows bidirectional parsing. Bidirectional parsing is a technique for initiating the parsing of a constituent from any point in</context>
</contexts>
<marker>11.</marker>
<rawString>Kimball, J. 1973. Principles of Surface Structure Parsing in Natural Language. Cognition, 2.15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
<author>M P Marcus</author>
</authors>
<title>Pearl: A Probabilistic Chart. Parser.</title>
<date>1991</date>
<booktitle>In Proceedings of the European ACL Conference,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="5350" citStr="[12]" startWordPosition="802" endWordPosition="802">bilistic model estimates the probability of each parse T given the words in the sentence S,P(TIS), by assuming that each non-terminal and its immediate children are dependent on the nonterminal&apos;s siblings and parent and on the part-of-speech trigram centered at the beginning of that rule: P(TIS) P(A --รท 13A7, aoaia2) (1) A ET where C is the non-terminal node which immediately dominates A, al is the part-of-speech associated with the leftmost word of constituent A, and a() and a2 are the parts-of-speech of the words to the left and to the right of al, respectively. See Magerman and Marcus 1991 [12] for a more detailed description of the CFG with CSP model. 3. The Parsing Algorithm A probabilistic language model, such as the aforementioned CFG with CSP model, provides a metric for evaluating the likelihood of a parse tree. However, while it may suggest a method for evaluating partial parse trees, a language model alone does not dictate the search strategy for determining the most likely analysis of a.n input. Since exhaustive search of the space of parse trees produced by a natural language grammar is generally not feasible, a parsing model can best take advantage of a probabilistic lang</context>
<context position="20105" citStr="[12]" startWordPosition="3256" endWordPosition="3256">probabilistic and nonprobabilistic grammars. One might question the need for a. new algorithm to deal with context-sensitive probabilistic models. However, these previous efforts have generally failed to address both efficiency and robustness effectively. For non-probabilistic grammar models, the CKY algoritlun [9] [17] provides efficiency and robustness in polynomial time, 0(Gn3). CKY can be modified to han43 die simple P-CFGs [2] without loss of efficiency. However, with the introduction of context-sensitive probability models, such as the history-based grammar[1] and the CFG with CSP models[12], CKY cannot be modified to accommodate these models without exhibiting exponential behavior in the grammar size G. The linear behavior of CKY with respect to grammar size is dependent upon being able to collapse the distinctions among constituents of the same type which span the same part of the sentence. However, when using a context-sensitive probabilistic model, these distinctions are necessary. For instance, in the CFG with CSP model, the part-ofspeech sequence generated by a constituent affects the probability of constituents that dominate it. Thus, two constituents which generate differ</context>
<context position="21833" citStr="[12]" startWordPosition="3518" endWordPosition="3518">ngrammatical sentences. Once a sentence has been determined to be ungrammatical, Earley-style prediction prevents any new edges from being added to the parse chart. This behavior seriously degrades the robustness of a natural language system using this type of parser. A few recent works on probabilistic parsing have proposed algorithms and devices for efficient, robust chart parsing. Bobrow[3] and Chitrao[4] introduce agendabased probabilistic parsing algorithms, although neither describe their algorithms in detail. Both algorithms use a strictly best first search. As both Chitrao and Magerman[12] observe, a best first search penalizes longer and more complex constituents (i.e. constituents which are composed of more edges), resulting in thrashing and loss of efficiency. Chitrao proposes a heuristic penalty based on constituent length to deal with this problem. Magerman avoids thrashing by calculating the score of a parse tree using the geometric mean of the probabilities of the constituents contained in the tree. Moore[14] discusses techniques for improving the efficiency and robustness of chart parsers for unification grammars, but the ideas are applicable to probabilistic grammars a</context>
<context position="26705" citStr="[12]" startWordPosition="4282" endWordPosition="4282">p parser using the same grammar produces on average over 1000 constituents for each sentence, generating over 15,000 edges without generating a parse at all! This supports our claim that exhaustive CKY-like parsing algorithms are not feasible when probabilistic models are applied to them. 5.1. Control Configuration The control for our experiments is the configuration of Picky with all three phases and with a maximum edge count of 15,000. Using this configuration, Picky parsed the 3 test sets with an 89.3% accuracy rate. This is a slight improvement over Pearl&apos;s 87.5% accuracy rate reported in [12]. Recall that we will measure the efficiency of a parser configuration by its prediction ratio and completion ratio on the test sentences. A perfect prediction ratio is 1:1, i.e. every edge predicted is used in the eventual parse. However, since there is ambiguity in the input sentences, a 1:1 prediction ratio is not likely to be achieved. Picky&apos;s prediction ratio is approximately than 4.3:1, and its ratio of predicted edges to completed edges is nearly 1.3:1. Thus, although the prediction ratio is not perfect, on average for every edge that is predicted more than one completed constituent res</context>
</contexts>
<marker>12.</marker>
<rawString>Magerman, D. M. and Marcus, M. P. 1991. Pearl: A Probabilistic Chart. Parser. In Proceedings of the European ACL Conference, March 1991. Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
<author>C New</author>
</authors>
<title>Probabilistic Prediction and Picky Chart Parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop.</booktitle>
<publisher>Arden House, NY.</publisher>
<marker>13.</marker>
<rawString>Magerman, D. M. and New, C. 1992. Probabilistic Prediction and Picky Chart Parsing. In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop. Arden House, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
</authors>
<title>Efficient Bottom-Up Parsing.</title>
<date>1991</date>
<booktitle>In Proceedings of the February 1991 DARPA Speech and Natural Language Workshop.</booktitle>
<location>Asilomar, California.</location>
<contexts>
<context position="22268" citStr="[14]" startWordPosition="3585" endWordPosition="3585">robabilistic parsing algorithms, although neither describe their algorithms in detail. Both algorithms use a strictly best first search. As both Chitrao and Magerman[12] observe, a best first search penalizes longer and more complex constituents (i.e. constituents which are composed of more edges), resulting in thrashing and loss of efficiency. Chitrao proposes a heuristic penalty based on constituent length to deal with this problem. Magerman avoids thrashing by calculating the score of a parse tree using the geometric mean of the probabilities of the constituents contained in the tree. Moore[14] discusses techniques for improving the efficiency and robustness of chart parsers for unification grammars, but the ideas are applicable to probabilistic grammars as well. Some of the techniques proposed are well-known ideas, such a.s compiling c-transitions (null gaps) out of the grammar and heuristically controlling the introduction of predictions. The Picky parser incorporates what we deem to be the most effective techniques of these previous works into one parsing algorithm. New techniques, such as probabilistic prediction and the multi-phase approach, are introduced where the literature </context>
</contexts>
<marker>14.</marker>
<rawString>Moore, R. and Dowding, J. 1991. Efficient Bottom-Up Parsing. In Proceedings of the February 1991 DARPA Speech and Natural Language Workshop. Asilomar, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Sharman</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Generating a Grammar for Statistical Training.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<marker>15.</marker>
<rawString>Sharman, R. A., Jelinek, F., and Mercer, R. 1990. Generating a Grammar for Statistical Training. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<date>1989</date>
<booktitle>In Proceedings of the August 1989 International Workshop in Parsing Technologies.</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<marker>16.</marker>
<rawString>Seneff, Stephanie 1989. TINA. In Proceedings of the August 1989 International Workshop in Parsing Technologies. Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and Parsing of Context-Free Languages in Time n3.</title>
<date>1967</date>
<journal>Information and Control</journal>
<volume>10</volume>
<pages>189--208</pages>
<contexts>
<context position="4395" citStr="[17]" startWordPosition="648" endWordPosition="648"> the tradeoffs of efficiency, robustness, and accuracy.3 2. Probabilistic Models in &apos;Picky The probabilistic models used in the implementation of Picky are independent of the algorithm. To facilitate the comparison between the performance of Picky and its predecessor, Pearl, the probabilistic model implemented for Picky is similar to Pearl&apos;s scoring model, the context.- &apos;Pearl probabilistic Earley-style parser (-Earl). Picky probabilistic CKY-like parser (P-CKY). 2 Some familiarity with chart parsing terminology is assumed in this paper. For terminological definitions, see [9], [10], [11], or [17]. 3Sections 2 and 3, the descriptions of the probabilistic models used in Picky and the Picky algorithm, are similar in content to the corresponding sections of Magerman and Weir[131. The experimental results and discussions which follow in sections 4-6 are original. 40 free grammar with context-sensitive probability (CFG with CSP) model. This probabilistic model estimates the probability of each parse T given the words in the sentence S,P(TIS), by assuming that each non-terminal and its immediate children are dependent on the nonterminal&apos;s siblings and parent and on the part-of-speech trigram</context>
<context position="19822" citStr="[17]" startWordPosition="3211" endWordPosition="3211"> all recognized constituents, sorted by probability, the chart can be used to extract partial parses. As implemented, Picky prints out the most probable completed S constituent. 4. Why a New Algorithm? Previous research efforts have produced a wide variety of parsing algorithms for probabilistic and nonprobabilistic grammars. One might question the need for a. new algorithm to deal with context-sensitive probabilistic models. However, these previous efforts have generally failed to address both efficiency and robustness effectively. For non-probabilistic grammar models, the CKY algoritlun [9] [17] provides efficiency and robustness in polynomial time, 0(Gn3). CKY can be modified to han43 die simple P-CFGs [2] without loss of efficiency. However, with the introduction of context-sensitive probability models, such as the history-based grammar[1] and the CFG with CSP models[12], CKY cannot be modified to accommodate these models without exhibiting exponential behavior in the grammar size G. The linear behavior of CKY with respect to grammar size is dependent upon being able to collapse the distinctions among constituents of the same type which span the same part of the sentence. However, </context>
</contexts>
<marker>17.</marker>
<rawString>Younger, D. H. 1967. Recognition and Parsing of Context-Free Languages in Time n3. Information and Control Vol. 10, No. 2, pp. 189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>