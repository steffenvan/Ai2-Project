<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002865">
<title confidence="0.977651">
Enforcing Subcategorization Constraints in a Parser Using Sub-parses
Recombining
</title>
<author confidence="0.620148">
Seyed Abolghasem Mirroshandel†,* Alexis Nasr† Benoit Sagot�
</author>
<affiliation confidence="0.7056556">
†Laboratoire d’Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universit´e Aix-Marseille, Marseille, France
*Alpage, INRIA &amp; Universit´e Paris-Diderot, Paris, France
*Computer Engineering Department, Faculty of Engineering,
University of Guilan, Rasht, Iran
</affiliation>
<email confidence="0.9802455">
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
benoit.sagot@inria.fr)
</email>
<sectionHeader confidence="0.99537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899285714286">
Treebanks are not large enough to adequately
model subcategorization frames of predica-
tive lexemes, which is an important source of
lexico-syntactic constraints for parsing. As
a consequence, parsers trained on such tree-
banks usually make mistakes when selecting
the arguments of predicative lexemes. In this
paper, we propose an original way to correct
subcategorization errors by combining sub-
parses of a sentence S that appear in the list
of the n-best parses of S. The subcatego-
rization information comes from three differ-
ent resources, the first one is extracted from
a treebank, the second one is computed on a
large corpora and the third one is an existing
syntactic lexicon. Experiments on the French
Treebank showed a 15.24% reduction of er-
roneous subcategorization frames (SF) selec-
tions for verbs as well as a relative decrease of
the error rate of 4% Labeled Accuracy Score
on the state of the art parser on this treebank.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952974358975">
Automatic syntactic parsing of natural languages
has witnessed many important changes in the last
fifteen years. Among these changes, two have mod-
ified the nature of the task itself. The first one is
the availability of treebanks such as the Penn Tree-
bank (Marcus et al., 1993) or the French Treebank
(Abeill´e et al., 2003), which have been used in the
parsing community to train stochastic parsers, such
as (Collins, 1997; Petrov and Klein, 2008). Such
work remained rooted in the classical language the-
oretic tradition of parsing, generally based on vari-
ants of generative context free grammars. The sec-
ond change occurred with the use of discriminative
machine learning techniques, first to rerank the out-
put of a stochastic parser (Collins, 2000; Charniak
and Johnson, 2005) and then in the parser itself (Rat-
naparkhi, 1999; Nivre et al., 2007; McDonald et al.,
2005a). Such parsers clearly depart from classical
parsers in the sense that they do not rely anymore on
a generative grammar: given a sentence S, all pos-
sible parses for S1 are considered as possible parses
of S. A parse tree is seen as a set of lexico-syntactic
features which are associated to weights. The score
of a parse is computed as the sum of the weights of
its features.
This new generation of parsers allows to reach
high accuracy but possess their own limitations. We
will focus in this paper on one kind of weakness
of such parser which is their inability to properly
take into account subcategorization frames (SF) of
predicative lexemes2, an important source of lexico-
syntactic constraints. The proper treatment of SF is
actually confronted to two kinds of problems: (1)
the acquisition of correct SF for verbs and (2) the
integration of such constraints in the parser.
The first problem is a consequence of the use of
treebanks for training parsers. Such treebanks are
composed of a few thousands sentences and only a
small subpart of acceptable SF for a verb actually
</bodyText>
<footnote confidence="0.733511285714286">
1Another important aspect of the new parsing paradigm is
the use of dependency trees as a means to represent syntactic
structure. In dependency syntax, the number of possible syn-
tactic trees associated to a sentence is bounded, and only de-
pends on the length of the sentence, which is not the case with
syntagmatic derivation trees.
2We will concentrate in this paper on verbal SF.
</footnote>
<page confidence="0.951263">
239
</page>
<note confidence="0.4759615">
Proceedings of NAACL-HLT 2013, pages 239–247,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.986082525423729">
occur in the treebank.
The second problem is a consequence of the pars-
ing models. For algorithmic complexity as well as
data sparseness reasons, the parser only considers
lexico-syntactic configurations of limited domain of
locality (in the parser used in the current work, this
domain of locality is limited to configurations made
of one or two dependencies). As described in more
details in section 2, SF often exceed in scope such
domains of locality and are therefore not easy to in-
tegrate in the parser. A popular method for intro-
ducing higher order constraints in a parser consist in
reranking the n best output of a parser as in (Collins,
2000; Charniak and Johnson, 2005). The reranker
search space is restricted by the output of the parser
and high order features can be used. One draw-
back of the reranking approach is that correct SF for
the predicates of a sentence can actually appear in
different parse trees. Selecting complete trees can
therefore lead to sub-optimal solutions. The method
proposed in this paper merges parts of different trees
that appear in an n best list in order to build a new
parse.
Taking into account SF in a parser has been a ma-
jor issue in the design of syntactic formalisms in the
eighties and nineties. Unification grammars, such
as Lexical Functional Grammars (Bresnan, 1982),
Generalized Phrase Structure Grammars (Gazdar et
al., 1985) and Head-driven Phrase Structure Gram-
mars (Pollard and Sag, 1994), made SF part of the
grammar. Tree Adjoining Grammars (Joshi et al.,
1975) proposed to extend the domain of locality of
Context Free Grammars partly in order to be able
to represent SF in a generative grammar. More
recently, (Collins, 1997) proposed a way to intro-
duce SF in a probabilistic context free grammar and
(Arun and Keller, 2005) used the same technique
for French. (Carroll et al., 1998), used subcate-
gorization probabilities for ranking trees generated
by unification-based phrasal grammar and (Zeman,
2002) showed that using frame frequency in a de-
pendency parser can lead to a significant improve-
ment of the performance of the parser.
The main novelties of the work presented here is
(1) the way a new parse is built by combining sub-
parses that appear in the n best parse list and (2)
the use of three very different resources that list the
possible SF for verbs.
The organization of the paper is the following: in
section 2, we will briefly describe the parsing model
that we will be using for this work and give accuracy
results on a French corpus. Section 3 will describe
three different resources that we have been using to
correct SF errors made by the parser and give cov-
erage results for these resources on a development
corpus. Section 4 will propose three different ways
to take into account, in the parser, the resources de-
scribed in section 3 and give accuracy results. Sec-
tion 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.924831" genericHeader="method">
2 The Parser
</sectionHeader>
<bodyText confidence="0.999676625">
The parser used in this work is the second order
graph based parser (McDonald et al., 2005b) imple-
mentation of (Bohnet, 2010). The parser was trained
on the French Treebank (Abeill´e et al., 2003) which
was transformed into dependency trees by (Candito
et al., 2009). The size of the treebank and its de-
composition into train, development and test sets are
represented in table 1.
</bodyText>
<table confidence="0.89329975">
nb of sentences nb of tokens
TRAIN 9 881 278 083
DEV 1 239 36 508
TEST 1 235 36 340
</table>
<tableCaption confidence="0.998639">
Table 1: Size and decomposition of the French Treebank
</tableCaption>
<bodyText confidence="0.997317">
The parser gave state of the art results for parsing
of French, reported in table 2. Table 2 reports the
standard Labeled Accuracy Score (LAS) and Unla-
beled Accuracy Score (UAS) which is the ratio of
correct labeled (for LAS) or unlabeled (for UAS) de-
pendencies in a sentence. We also defined a more
specific measure: the SF Accuracy Score (SAS)
which is the ratio of verb occurrences that have been
paired with the correct SF by the parser. We have
introduced this quantity in order to measure more
accurately the impact of the methods described in
this paper on the selection of a SF for the verbs of a
sentence.
</bodyText>
<table confidence="0.963147">
TEST DEV
SAS 80.84 79.88
LAS 88.88 88.53
UAS 90.71 90.37
</table>
<tableCaption confidence="0.9725355">
Table 2: Subcategorization Frame Accuracy, Labeled and
Unlabeled Accuracy Score on TEST and DEV.
</tableCaption>
<page confidence="0.996271">
240
</page>
<bodyText confidence="0.9989215">
We have chosen a second order graph parser in
this work for two reasons. The first is that it is the
parsing model that obtained the best results on the
French Treebank. The second is that it allows us
to impose structural constraints in the solution of
the parser, as described in (Mirroshandel and Nasr,
2011), a feature that will reveal itself precious when
enforcing SF in the parser output.
</bodyText>
<sectionHeader confidence="0.983126" genericHeader="method">
3 The Resources
</sectionHeader>
<bodyText confidence="0.9999086875">
Three resources have been used in this work in order
to correct SF errors. The first one has been extracted
from a treebank, the second has been extracted from
an automatically parsed corpus that is several order
of magnitude bigger than the treebank. The third one
has been extracted from an existing lexico-syntactic
resource. The three resources are respectively de-
scribed in sections 3.2, 3.3 and 3.4. Before describ-
ing the resources, we describe in details, in section
3.1 our definition of SF. In section 3.5, we evalu-
ate the coverage of these resources on the DEV cor-
pus. Coverage is an important characteristic of a re-
source: in case of an SF error made by the parser, if
the correct SF that should be associated to a verb, in
a sentence, does not appear in the resource, it will be
impossible to correct the error.
</bodyText>
<subsectionHeader confidence="0.998163">
3.1 Subcat Frames Description
</subsectionHeader>
<bodyText confidence="0.999959842105263">
In this work, a SF is defined as a couple (G, L)
where G is the part of speech tag of the element that
licenses the SF. This part of speech tag can either
be a verb in infinitive form (VINF), a past participle
(VPP), a finite tense verb (V) or a present participle
(VPR). L is a set of couples (f, c) where f is a syn-
tactic function tag chosen among a set F and c is
a part of speech tag chosen among the set C. Cou-
ple (f, c) indicates that function f can be realized as
part of speech tag c. Sets F and C are respectively
displayed in top and bottom tables of figure 1. An
anchored SF (ASF) is a couple (v, 5) where v is a
verb lemma and 5 is a SF, as described above.
A resource is defined as a collection of ASF
(v, 5), each associated to a count c, to represent the
fact that verb v has been seen with SF 5 c times. In
the case of the resource extracted form an existing
lexicon (section 3.4), the notion of count is not ap-
plicable and we will consider that it is always equal
</bodyText>
<table confidence="0.957513357142857">
SUJ subject
OBJ object
A OBJ indirect object introduced by the preposition a`
DE OBJ indirect object introduced by the preposition de
P OBJ indirect object introduced by another preposition
ATS attribute of the subject
ATO attribute of the direct object
ADJ adjective
CS subordinating conjunction
N noun
V verb finite tense
VINF verb infinitive form
VPP verb past participle
VPR verb present participle
</table>
<figureCaption confidence="0.997409333333333">
Figure 1: Syntactic functions of the arguments of the SF
(top table). Part of speech tags of the arguments of the SF
(bottom table)
</figureCaption>
<bodyText confidence="0.991431580645161">
to one.
Below is an example of three ASF for the french
verb donner (to give). The first one is a transitive SF
where both the subject and the object are realized as
nouns as in Jean donne un livre (Jean gives a book.).
The second one is ditransitive, it has both a direct
object and an indirect one introduced by the prepo-
sition a` as in Jean donne un livre a` Marie. (Jean
gives a book to Marie). The third one corresponds
to a passive form as in le livre est donn´e a` Marie par
Jean (The book is given to Marie by Jean).
(donner,(V,(suj,N),(obj,N)))
(donner,(V,(suj,N),(obj,N),(a_obj,N)))
(donner,(VPP,(suj,N),(aux_pass,V),
(a_obj,N),(p_obj,N)))
One can note that when an argument corresponds
to an indirect dependent of the verb (introduced ei-
ther by a preposition or a subordinating conjunc-
tion), we do not represent in the SF, the category
of the element that introduces the argument, but the
category of the argument itself, a noun or a verb.
Two important choices have to be made when
defining SF. The first one concerns the dependents
of the predicative element that are in the SF (argu-
ment/adjunct distinction) and the second is the level
of abstraction at which SF are defined.
In our case, the first choice is constrained by the
treebank annotation guidelines. The FTB distin-
guishes seven syntactic functions which can be con-
sidered as arguments of a verb. They are listed in
the top table of figure 1. Most of them are straight-
</bodyText>
<page confidence="0.986341">
241
</page>
<bodyText confidence="0.999992514285714">
forward and do not deserve an explanation. Some-
thing has to be said though on the syntactic function
P OBJ which is used to model arguments of the verb
introduced by a preposition that is neither a` nor de,
such as the agent in passive form, which is intro-
duced by the preposition par.
We have added in the SF two elements that do not
correspond to arguments of the verb: the reflexive
pronoun, and the passive auxiliary. The reason for
adding these elements to the SF is that their pres-
ence influences the presence or absence of some ar-
guments of the verb, and therefore the SF.
The second important choice that must be made
when defining SF is the level of abstraction, or, in
other words, how much the SF abstracts away from
its realization in the sentence. In our case, we have
used two ways to abstract away from the surface re-
alization of the SF. The first one is factoring sev-
eral part of speech tags. We have factored pronouns,
common nouns and proper nouns into a single cat-
egory N. We have not gathered verbs in different
modes into one category since the mode of the verb
influences its syntactic behavior and hence its SF.
The second means of abstraction we have used is
the absence of linear order between the arguments.
Taking into account argument order increases the
number of SF and, hence, data sparseness, without
adding much information for selecting the correct
SF, this is why we have decided to to ignore it. In
our second example above, each of the three argu-
ments can be realized as one out of eight parts of
speech that correspond to the part of speech tag N
and the 24 possible orderings are represented as one
canonical ordering. This SF therefore corresponds
to 12 288 possible realizations.
</bodyText>
<subsectionHeader confidence="0.999764">
3.2 Treebank Extracted Subcat Frames
</subsectionHeader>
<bodyText confidence="0.999464">
This resource has been extracted from the TRAIN
corpus. At a first glance, it may seen strange to ex-
tract data from the corpus that have been used for
training our parser. The reason is that, as seen in
section 1, SF are not directly modeled by the parser,
which only takes into account subtrees made of, at
most, two dependencies.
The extraction procedure of SF from the treebank
is straightforward : the tree of every sentence is vis-
ited and, for every verb of the sentence, its daughters
are visited, and, depending whether they are consid-
ered as arguments of the verb (with respect to the
conventions or section 3.1), they are added to the SF.
The number of different verbs extracted, as well as
the number of different SF and the average number
of SF per verb are displayed in table 3. Column T
(for Train) is the one that we are interested in here.
</bodyText>
<table confidence="0.9943145">
T L A0 A5 A10
nb of verbs 2058 7824 23915 4871 3923
nb of diff SF 666 1469 12122 2064 1355
avg. nb of SF 4.83 52.09 14.26 16.16 13.45
</table>
<tableCaption confidence="0.999737">
Table 3: Resources statistics
</tableCaption>
<bodyText confidence="0.9999518">
The extracted resource can directly be compared
with the TREELEX resource (Kupsc and Abeill´e,
2008), which has been extracted from the same tree-
bank. The result that we obtain is different, due to
the fact that (Kupsc and Abeill´e, 2008) have a more
abstract definition of SF. As a consequence, they de-
fine a smaller number of SF: 58 instead of 666 in
our case. The smaller number of SF yields a smaller
average number of SF per verb: 1.72 instead of 4.83
in our case.
</bodyText>
<subsectionHeader confidence="0.998991">
3.3 Automatically computed Subcat Frames
</subsectionHeader>
<bodyText confidence="0.999728086956522">
The extraction procedure described above has been
used to extract ASF from an automatically parsed
corpus. The corpus is actually a collection of three
corpora of slightly different genres. The first one
is a collection of news reports of the French press
agency Agence France Presse, the second is a col-
lection of newspaper articles from a local French
newspaper : l’Est R´epublicain. The third one is
a collection of articles from the French Wikipedia.
The size of the different corpora are detailed in ta-
ble 4.
The corpus was first POS tagged with the MELT
tagger (Denis and Sagot, 2010), lemmatized with the
MACAON tool suite (Nasr et al., 2011) and parsed
in order to get the best parse for every sentence.
Then the ASF have been extracted.
The number of verbs, number of SF and average
number of SF per verb are represented in table 3,
in column AO (A stands for Automatic). As one
can see, the number of verbs and SF are unrealis-
tic. This is due to the fact that the data that we ex-
tract SF from is noisy: it consists of automatically
produced syntactic trees which contain errors (recall
</bodyText>
<page confidence="0.97913">
242
</page>
<table confidence="0.9981514">
CORPUS Sent. nb. Tokens nb.
AFP 2 041 146 59 914 238
EST REP 2 998 261 53 913 288
WIKI 1 592 035 33 821 460
TOTAL 5 198 642 147 648 986
</table>
<tableCaption confidence="0.999861">
Table 4: sizes of the corpora used to collect SF
</tableCaption>
<bodyText confidence="0.999600956521739">
that the LAS on the DEV corpus is 88,02%). There
are two main sources of errors in the parsed data: the
pre-processing chain (tokenization, part of speech
tagging and lemmatization) which can consider as
a verb a word that is not, and, of course, parsing
errors, which tend to create crazy SF. In order to
fight against noise, we have used a simple thresh-
olding: we only collect ASF that occur more than a
threshold i. The result of the thresholding appears
in columns A5 and A10 , where the subscript is the
value of the threshold. As expected both the number
of verbs and SF decrease sharply when increasing
the value of the threshold.
Extracting SF for verbs from raw data has been
an active direction of research for a long time, dat-
ing back at least to the work of (Brent, 1991) and
(Manning, 1993). More recently (Messiant et al.,
2008) proposed such a system for French verbs. The
method we use for extracting SF is not novel with
respect to such work. Our aim was not to devise
new extraction techniques but merely to evaluate the
resource produced by such techniques for statistical
parsing.
</bodyText>
<subsectionHeader confidence="0.996588">
3.4 Using an existing resource
</subsectionHeader>
<bodyText confidence="0.999972423076923">
The third resource that we have used is the Lefff
(Lexique des formes fl´echies du franc¸ais — Lexicon
of French inflected form), a large-coverage syntac-
tic lexicon for French (Sagot, 2010). The Lefff was
developed in a semi-automatic way: automatic tools
were used together with manual work. The latest
version of the Lefff contains 10,618 verbal entries
for 7,835 distinct verbal lemmas (the Lefff covers all
categories, but only verbal entries are used in this
work).
A sub-categorization frame consists in a list of
syntactic functions, using an inventory slightly more
fine-grained than in the French Treebank, and for
each of them a list of possible realizations (e.g.,
noun phrase, infinitive clause, or null-realization if
the syntactic function is optional).
For each verbal lemma, we extracted all sub-
categorization frames for each of the four verbal
part-of-speech tags (V, VINF, VPR, VPP), thus cre-
ating an inventory of SFs in the same sense and for-
mat as described in Section 3.1. Note that such SFs
do not contain alternatives concerning the way each
syntactic argument is realized or not: this extraction
process includes a de-factorization step. Its output,
hereafter L, contains 801,246 distinct (lemma, SF)
pairs.
</bodyText>
<subsectionHeader confidence="0.887417">
3.5 Coverage
</subsectionHeader>
<bodyText confidence="0.95507209375">
In order to be able to correct SF errors, the three
resources described above must possess two impor-
tant characteristics: high coverage and high accu-
racy. Coverage measures the presence, in the re-
source, of the correct SF of a verb, in a given sen-
tence. Accuracy measures the ability of a resource
to select the correct SF for a verb in a given context
when several ones are possible.
We will give in this section coverage result, com-
puted on the DEV corpus. Accuracy will be de-
scribed and computed in section 4. The reason why
the two measures are not described together is due
to the fact that coverage can be computed on a ref-
erence corpus while accuracy must be computed on
the output of a parser, since it is the parser that will
propose different SF for a verb in a given context.
Given a reference corpus C and a resource R,
two coverage measures have been computed, lexi-
cal coverage, which measures the ratio of verbs of C
that appear in R and syntactic coverage, which mea-
sures the ratio of ASF of C that appear in R. Two
variants of each measures are computed: on types
and on occurrences. The values of these measures
computed on the DEV corpus are summarized in ta-
ble 5.
T L A0 A5 A10
Lex. types 89.56 99.52 99.52 98.56 98.08
cov.
occ 96.98 99.85 99.85 99.62 99.50
Synt. types 62.24 78.15 95.78 91.08 88.84
cov.
occ 73.54 80.35 97.13 93.96 92.39
</bodyText>
<tableCaption confidence="0.9651">
Table 5: Lexical and syntactic coverage of the three re-
sources on DEV
</tableCaption>
<bodyText confidence="0.9891955">
The figures of table 5 show that lexical cover-
age of the three resources is quite high, ranging
</bodyText>
<page confidence="0.996946">
243
</page>
<bodyText confidence="0.999497315789474">
from 89.56 to 99.52 when computed on types and
from 96.98 to 99.85 when computed on occurrences.
The lowest coverage is obtained by the T resource,
which does not come as a surprise since it is com-
puted on a rather small number of sentences. It
is also interesting to note that lexical coverage of
A does not decrease much when augmenting the
threshold, while the size of the resource decreases
dramatically (as shown in table 3). This validates
the hypothesis that the resource is very noisy and
that a simple threshold on the occurrences of ASF is
a reasonable means to fight against noise.
Syntactic coverage is, as expected, lower than lex-
ical coverage. The best results are obtained by A0:
95.78 on types and 97.13 on occurrences. Thresh-
olding on the occurrences of anchored SF has a big-
ger impact on syntactic coverage than it had on lexi-
cal coverage. A threshold of 10 yields a coverage of
88.84 on types and 92.39 on occurrences.
</bodyText>
<sectionHeader confidence="0.992856" genericHeader="method">
4 Integrating Subcat Frames in the Parser
</sectionHeader>
<bodyText confidence="0.99997896875">
As already mentioned in section 1, SF usually ex-
ceed the domain of locality of the structures that are
directly modeled by the parser. It is therefore dif-
ficult to integrate directly SF in the model of the
parser. In order to circumvent the problem, we have
decided to work on the n-best output of the parser:
we consider that a verb v, in a given sentence 5,
can be associated to any of the SF that v licenses in
one of the n-best trees. The main weakness of this
method is that an SF error can be corrected only if
the right SF appears at least in one of the n-best parse
trees.
In order to estimate an upper bound of the SAS
that such methods can reach (how many SF errors
can actually be corrected), we have computed the
oracle SAS on the 100 best trees of the DEV corpus
DEV (for how many verbs the correct SF appears
in at least one of the n-best parse trees). The oracle
score is equal to 95.16, which means that for 95.16%
of the verb occurrences of the DEV, the correct SF
appears somewhere in the 100-best trees. 95.16 is
therefore the best SAS that we can reach. Recall
that the baseline SAS is equal to 79.88% the room
for progress is therefore equal to 15.28% absolute.
Three experiments are described below. In the
first one, section 4.1, a simple technique, called Post
Processing is used. Section 4.2 describes a second
technique, called Double Parsing, which is a is a
refinement of Post Processing. Both sections 4.1
and 4.2 are based on single resources. Section 4.3
proposes a simple way to combine the different re-
sources.
</bodyText>
<subsectionHeader confidence="0.997856">
4.1 Post Processing
</subsectionHeader>
<bodyText confidence="0.995800636363636">
The post processing method (PP) is the simplest one
that we have tested. It takes as input the different
ASF that occur in the n-best output of the parser as
well as a resource R. Given a sentence, let’s note
T1 ... Tn the trees that appear in the n-best output
of the parser, in decreasing order of their score. For
every verb v of the sentence, we note S(v) the set
of all the SF associated to v that appear in the trees
T1 ... Tn.
Given a verb v and a SF s, we define the following
functions:
</bodyText>
<equation confidence="0.9454555">
C(v, s) is the number of occurrences of the ASF
(v, s) in the trees T1 ... Tn.
</equation>
<bodyText confidence="0.963600904761905">
F(v) is the SF associated to v in T1
CR(v, s) the number of occurrences of the ASF
(v, s) in the resource R.
We define a selection function as a function that
selects a SF for a given verb in a given sentence.
A selection function has to take into account the in-
formation given by the resource (whether an SF is
acceptable/frequent for a given verb) as well as the
information given by the parser (whether the parser
has a strong preference to associate a given SF to a
given verb).
In our experiments, we have tested two simple
selection functions. cOR which selects the first SF
s E S(v), such that CR(v, s) &gt; 0 when traversing
the trees T1 ... Tn in the decreasing order of score
(best tree first).
The second function, OR(v) compares the most
frequent SF for v in the resource R with the SF of the
first parse. If the ratio of the number of occurrences
in the n-best of the former and the latter is above a
threshold α, the former is selected. More formally:
</bodyText>
<equation confidence="0.997063666666667">
s� = arg maxsES(v) CR(v, s)
if C(v,�s) &gt; α
C(v,F(v))
F(v)
otherwise
OR(v) = {
</equation>
<page confidence="0.9957">
244
</page>
<bodyText confidence="0.999931523809524">
The coefficient α has been optimized on DEV cor-
pus. Its value is equal to 2.5 for the Automatic re-
source, 2 for the Train resource and 1.5 for the Lefff.
The construction of the new solution proceeds as
follows: for every verb v of the sentence, a SF is se-
lected with the selection function. It is important to
note, at this point, that the SF selected for different
verbs of the sentence can pertain to different parse
trees. The new solution is built based on tree T1. For
every verb v, its arguments are potentially modified
in agreement with the SF selected by the selection
function. There is no guarantee at this point that the
solution is well formed. We will return to this prob-
lem in section 4.2.
We have evaluated the PP method with different
selection functions on the TEST corpus. The results
of applying function OR were more successful. As
a result we just report the results of this function in
table 6. Different levels of thresholding for resource
A gave almost the same results, we therefore used
A10 which is the smallest one.
</bodyText>
<table confidence="0.99201425">
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.14 89.03 89.03
UAS 90.71 90.91 90.81 90.82
</table>
<tableCaption confidence="0.998115">
Table 6: LAS and UAS on TEST using PP
</tableCaption>
<bodyText confidence="0.999647416666667">
The results of table 6 show two interesting facts.
First, the SAS is improved, it jumps from 80.84 to
83.11. PP therefore corrects some SF errors made
by the parser. It must be noted however that this im-
provement is much lower than the oracle score. The
second interesting fact is the very moderate increase
of both LAS and UAS. This is due to the fact that
the number of dependencies modified is small with
respect to the total number of dependencies. The
impact on LAS and UAS is therefore weak.
The best results are obtained with resource T. Al-
though the coverage of T is low, the resource is very
close to the train data, this fact probably explains the
good results obtained with this resource.
It is interesting, at this point, to compare our
method with a reranking approach. In order to do so,
we have compared the upper bound of the number of
SF errors that can be corrected when using rerank-
ing and our approach. The results of the comparison
computed on a list of 100 best trees is reported in
table 7 which shows the ratio of subcat frame errors
that could be corrected with a reranking approach
and the ratio of errors sub-parse recombining could
reach.
</bodyText>
<table confidence="0.994113">
DEV TEST
reranking 53.9% 58.5%
sub-parse recombining 75.5% 76%
</table>
<tableCaption confidence="0.9146878">
Table 7: Correction rate for subcat frames errors with dif-
ferent methods
Table 7 shows that combining sub-parses can, in
theory, correct a much larger number of wrong SF
assignments than reranking.
</tableCaption>
<subsectionHeader confidence="0.993692">
4.2 Double Parsing
</subsectionHeader>
<bodyText confidence="0.999980269230769">
The post processing method shows some improve-
ment over the baseline. But it has an important draw-
back: it can create inconsistent parses. Recall that
the parser we are using is based on a second order
model. In other words, the score of a dependency
depends on some neighboring dependencies. When
building a new solution, the post processing method
modifies some dependencies independently of their
context, which may give birth to very unlikely con-
figurations.
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies. The new solution
is therefore the optimal solution that preserves the
dependencies modified by the PP method.
The double parsing (DP) method is therefore a
three stage method. First, sentence 5 is parsed, pro-
ducing the n-best parses. Then, the post processing
method is used, modifying the first best parse. Let’s
note D the set of dependencies that were changed in
this process. In the last stage, a new parse is pro-
duced, that preserves D.
</bodyText>
<table confidence="0.98901975">
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.30 89.25 89.31
UAS 90.71 91.07 91.05 91.08
</table>
<tableCaption confidence="0.999219">
Table 8: LAS and UAS on TEST using DP
</tableCaption>
<page confidence="0.997586">
245
</page>
<bodyText confidence="0.999802125">
The results of DP on TEST are reported in table
8. SAS did not change with respect to PP, because
DP keeps the SF selected by PP. As expected DP
does increase LAS and UAS. Recomputing an op-
timal solution therefore increases the quality of the
parses. Table 8 also shows that the three resources
get almost the same LAS and UAS although SAS is
better for resource T.
</bodyText>
<subsectionHeader confidence="0.999396">
4.3 Combining Resources
</subsectionHeader>
<bodyText confidence="0.999787590909091">
Due to the different generation techniques of our
three resources, another direction of research is
combining them. We did different experiments con-
cerning all possible combination of resources: A and
L (AL), T and L (TL), T and A (TA), and all tree
(TAL) resources. The results of these combinations
for PP and DP methods are shown in tables 9 and
10, respectively.
The resource are combined in a back-off schema:
we search for a candidate ASF in a first resource. If
it is found, the search stops. Otherwise, the next re-
source(s) are probed. One question that arises is:
which sequence is the optimal one for combining
the resources. To answer this question, we did sev-
eral experiments on DEV set. Our experiments have
shown that it is better to search T resource, then
A, and, eventually, L. The results of this combining
method, using PP are reported in table 9. The best
results are obtained for the TL combination. The
SAS jumps from 83.11 to 83.76. As it was the case
with single resources, the LAS and UAS increase is
moderate.
</bodyText>
<table confidence="0.99535175">
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.03 89.22 89.19 89.19
UAS 90.71 90.79 90.98 90.95 90.95
</table>
<tableCaption confidence="0.9811545">
Table 9: LAS and UAS on TEST using PP with resource
combination
</tableCaption>
<bodyText confidence="0.999850555555555">
With DP (table 9), the order of resource combina-
tion is exactly the same as with PP. As was the case
with single resources, DP has a positive, but moder-
ate, impact on LAS and UAS.
The results of tables 9 and 10 do not show con-
siderable improvement over single resources. This
might be due to the large intersection between our
resources. In other words, they do not have comple-
mentary information, and their combination will not
</bodyText>
<table confidence="0.7737365">
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.22 89.31 89.34 89.34
UAS 90.71 91.02 91.05 91.08 91.09
</table>
<tableCaption confidence="0.9797285">
Table 10: LAS and UAS on TEST using DP with resource
combination
</tableCaption>
<bodyText confidence="0.999893">
introduce much information. Another possible rea-
son for this result is the combination technique used.
More sophisticated techniques might yield better re-
sults.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999971705882353">
Subcategorization frames for verbs constitute a rich
source of lexico-syntactic information which is hard
to integrate in graph based parsers. In this paper, we
have used three different resources for subcatego-
rization frames. These resources are from different
origins with various characteristics. We have pro-
posed two different methods to introduce the useful
information from these resources in a second order
model parser. We have conducted different exper-
iments on French Treebank that showed a 15.24%
reduction of erroneous SF selections for verbs. Al-
though encouraging, there is still plenty of room
for better results since the oracle score for 100 best
parses is equal to 95.16% SAS and we reached
83.76%. Future work will concentrate on more elab-
orate selection functions as well as more sophisti-
cated ways to combine the different resources.
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996863">
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
EDYLEX (ANR-08-CORD-009).
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999781363636364">
A. Abeill´e, L. Cl´ement, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeill´e, editor, Tree-
banks. Kluwer, Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of french. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 306–313.
Association for Computational Linguistics.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89–97.
</reference>
<page confidence="0.978561">
246
</page>
<reference confidence="0.999685091954023">
Michael Brent. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceedings
of ACL.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press.
M. Candito, B. Crabb´e, P. Denis, and F. Gu´erin. 2009.
Analyse syntaxique du franc¸ais : des constituants aux
d´ependances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
J. Carroll, G. Minnen, and T. Briscoe. 1998. Can sub-
categorisation probabilities help a statistical parser?
Arxiv preprint cmp-lg/9806013.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
P. Denis and B. Sagot. 2010. Exploitation d’une
ressource lexicale pour la construction d’un ´etiqueteur
morphosyntaxique ´etat-de-l’art du franc¸ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan Sag. 1985. Generalized Phrase Structure Gram-
mar. Harvard University Press.
Aravind Joshi, Leon Levy, and M Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10:136–163.
Anna Kupsc and Anne Abeill´e. 2008. Treelex: A subcat-
egorisation lexicon for french verbs. In Proceedings of
the First International Conference on Global Interop-
erability for Language Resources.
Christopher Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of ACL.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313–330.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 91–98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523–530.
C. Messiant, A. Korhonen, T. Poibeau, et al. 2008.
Lexschem: A large subcategorization lexicon for
french verbs. In Proceedings of the Language Re-
sources and Evaluation Conference.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
A. Nasr, F. B´echet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Slav Petrov and Dan Klein. 2008. Discriminative Log-
Linear Grammars with Latent Variables. In J.C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 20 (NIPS),
pages 1153–1160, Cambridge, MA. MIT Press.
Carl Pollard and Ivan Sag. 1994. Head-driven Phrase
Structure Grammmar. CSLI Series. University of
Chicago Press.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1):151–175.
Benoit Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for french. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), pages 2744–2751, Valletta, Malta.
D. Zeman. 2002. Can subcategorization help a statistical
dependency parser? In Proceedings of the 19th in-
ternational conference on Computational linguistics-
Volume 1, pages 1–7. Association for Computational
Linguistics.
</reference>
<page confidence="0.997954">
247
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.675373">
<title confidence="0.9986745">Enforcing Subcategorization Constraints in a Parser Using Sub-parses Recombining</title>
<author confidence="0.978103">Abolghasem Alexis</author>
<affiliation confidence="0.9613362">d’Informatique Fondamentale de Marseille- CNRS - UMR Universit´e Aix-Marseille, Marseille, INRIA &amp; Universit´e Paris-Diderot, Paris, Engineering Department, Faculty of University of Guilan, Rasht, Iran</affiliation>
<email confidence="0.908396">(ghasem.mirroshandel@lif.univ-mrs.fr,benoit.sagot@inria.fr)</email>
<abstract confidence="0.999387045454545">Treebanks are not large enough to adequately model subcategorization frames of predicative lexemes, which is an important source of lexico-syntactic constraints for parsing. As a consequence, parsers trained on such treebanks usually make mistakes when selecting the arguments of predicative lexemes. In this paper, we propose an original way to correct subcategorization errors by combining subof a sentence appear in the list the parses of The subcategorization information comes from three different resources, the first one is extracted from a treebank, the second one is computed on a large corpora and the third one is an existing syntactic lexicon. Experiments on the French showed a of erroneous subcategorization frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeill´e</author>
<author>L Cl´ement</author>
<author>F Toussenel</author>
</authors>
<title>Building a treebank for french.</title>
<date>2003</date>
<editor>In Anne Abeill´e, editor, Treebanks.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>A. Abeill´e, L. Cl´ement, and F. Toussenel. 2003. Building a treebank for french. In Anne Abeill´e, editor, Treebanks. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>F Keller</author>
</authors>
<title>Lexicalization in crosslinguistic probabilistic parsing: The case of french.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>306--313</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5710" citStr="Arun and Keller, 2005" startWordPosition="917" endWordPosition="920">a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources that list the possible SF for verbs. The organization of the paper is the following: in section 2,</context>
</contexts>
<marker>Arun, Keller, 2005</marker>
<rawString>A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of french. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 306–313. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="6935" citStr="Bohnet, 2010" startWordPosition="1134" endWordPosition="1135">efly describe the parsing model that we will be using for this work and give accuracy results on a French corpus. Section 3 will describe three different resources that we have been using to correct SF errors made by the parser and give coverage results for these resources on a development corpus. Section 4 will propose three different ways to take into account, in the parser, the resources described in section 3 and give accuracy results. Section 5 concludes the paper. 2 The Parser The parser used in this work is the second order graph based parser (McDonald et al., 2005b) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency trees by (Candito et al., 2009). The size of the treebank and its decomposition into train, development and test sets are represented in table 1. nb of sentences nb of tokens TRAIN 9 881 278 083 DEV 1 239 36 508 TEST 1 235 36 340 Table 1: Size and decomposition of the French Treebank The parser gave state of the art results for parsing of French, reported in table 2. Table 2 reports the standard Labeled Accuracy Score (LAS) and Unlabeled Accuracy Score (UAS) which is the ratio of corre</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>B. Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of ACL, pages 89–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>Automatic acquisition of subcategorization frames from untagged text.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17686" citStr="Brent, 1991" startWordPosition="3095" endWordPosition="3096">atization) which can consider as a verb a word that is not, and, of course, parsing errors, which tend to create crazy SF. In order to fight against noise, we have used a simple thresholding: we only collect ASF that occur more than a threshold i. The result of the thresholding appears in columns A5 and A10 , where the subscript is the value of the threshold. As expected both the number of verbs and SF decrease sharply when increasing the value of the threshold. Extracting SF for verbs from raw data has been an active direction of research for a long time, dating back at least to the work of (Brent, 1991) and (Manning, 1993). More recently (Messiant et al., 2008) proposed such a system for French verbs. The method we use for extracting SF is not novel with respect to such work. Our aim was not to devise new extraction techniques but merely to evaluate the resource produced by such techniques for statistical parsing. 3.4 Using an existing resource The third resource that we have used is the Lefff (Lexique des formes fl´echies du franc¸ais — Lexicon of French inflected form), a large-coverage syntactic lexicon for French (Sagot, 2010). The Lefff was developed in a semi-automatic way: automatic t</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>Michael Brent. 1991. Automatic acquisition of subcategorization frames from untagged text. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<editor>Joan Bresnan, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1982</marker>
<rawString>Joan Bresnan, editor. 1982. The Mental Representation of Grammatical Relations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
<author>P Denis</author>
<author>F Gu´erin</author>
</authors>
<title>Analyse syntaxique du franc¸ais : des constituants aux d´ependances.</title>
<date>2009</date>
<booktitle>In Proceedings of Traitement Automatique des Langues Naturelles.</booktitle>
<marker>Candito, Crabb´e, Denis, Gu´erin, 2009</marker>
<rawString>M. Candito, B. Crabb´e, P. Denis, and F. Gu´erin. 2009. Analyse syntaxique du franc¸ais : des constituants aux d´ependances. In Proceedings of Traitement Automatique des Langues Naturelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>T Briscoe</author>
</authors>
<title>Can subcategorisation probabilities help a statistical parser? Arxiv preprint cmp-lg/9806013.</title>
<date>1998</date>
<contexts>
<context position="5769" citStr="Carroll et al., 1998" startWordPosition="927" endWordPosition="930">ighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources that list the possible SF for verbs. The organization of the paper is the following: in section 2, we will briefly describe the parsing model that we will be</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1998</marker>
<rawString>J. Carroll, G. Minnen, and T. Briscoe. 1998. Can subcategorisation probabilities help a statistical parser? Arxiv preprint cmp-lg/9806013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2235" citStr="Charniak and Johnson, 2005" startWordPosition="330" endWordPosition="333">the nature of the task itself. The first one is the availability of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We will focus in this paper on one</context>
<context position="4603" citStr="Charniak and Johnson, 2005" startWordPosition="730" endWordPosition="733">quence of the parsing models. For algorithmic complexity as well as data sparseness reasons, the parser only considers lexico-syntactic configurations of limited domain of locality (in the parser used in the current work, this domain of locality is limited to configurations made of one or two dependencies). As described in more details in section 2, SF often exceed in scope such domains of locality and are therefore not easy to integrate in the parser. A popular method for introducing higher order constraints in a parser consist in reranking the n best output of a parser as in (Collins, 2000; Charniak and Johnson, 2005). The reranker search space is restricted by the output of the parser and high order features can be used. One drawback of the reranking approach is that correct SF for the predicates of a sentence can actually appear in different parse trees. Selecting complete trees can therefore lead to sub-optimal solutions. The method proposed in this paper merges parts of different trees that appear in an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lex</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1879" citStr="Collins, 1997" startWordPosition="276" endWordPosition="277">categorization frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank. 1 Introduction Automatic syntactic parsing of natural languages has witnessed many important changes in the last fifteen years. Among these changes, two have modified the nature of the task itself. The first one is the availability of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possi</context>
<context position="5611" citStr="Collins, 1997" startWordPosition="901" endWordPosition="902"> an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings of the 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2206" citStr="Collins, 2000" startWordPosition="328" endWordPosition="329"> have modified the nature of the task itself. The first one is the availability of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We wi</context>
<context position="4574" citStr="Collins, 2000" startWordPosition="728" endWordPosition="729">blem is a consequence of the parsing models. For algorithmic complexity as well as data sparseness reasons, the parser only considers lexico-syntactic configurations of limited domain of locality (in the parser used in the current work, this domain of locality is limited to configurations made of one or two dependencies). As described in more details in section 2, SF often exceed in scope such domains of locality and are therefore not easy to integrate in the parser. A popular method for introducing higher order constraints in a parser consist in reranking the n best output of a parser as in (Collins, 2000; Charniak and Johnson, 2005). The reranker search space is restricted by the output of the parser and high order features can be used. One drawback of the reranking approach is that correct SF for the predicates of a sentence can actually appear in different parse trees. Selecting complete trees can therefore lead to sub-optimal solutions. The method proposed in this paper merges parts of different trees that appear in an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unif</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>B Sagot</author>
</authors>
<title>Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais.</title>
<date>2010</date>
<booktitle>In Proceedings of Traitement Automatique des Langues Naturelles.</booktitle>
<contexts>
<context position="16208" citStr="Denis and Sagot, 2010" startWordPosition="2809" endWordPosition="2812">atically computed Subcat Frames The extraction procedure described above has been used to extract ASF from an automatically parsed corpus. The corpus is actually a collection of three corpora of slightly different genres. The first one is a collection of news reports of the French press agency Agence France Presse, the second is a collection of newspaper articles from a local French newspaper : l’Est R´epublicain. The third one is a collection of articles from the French Wikipedia. The size of the different corpora are detailed in table 4. The corpus was first POS tagged with the MELT tagger (Denis and Sagot, 2010), lemmatized with the MACAON tool suite (Nasr et al., 2011) and parsed in order to get the best parse for every sentence. Then the ASF have been extracted. The number of verbs, number of SF and average number of SF per verb are represented in table 3, in column AO (A stands for Automatic). As one can see, the number of verbs and SF are unrealistic. This is due to the fact that the data that we extract SF from is noisy: it consists of automatically produced syntactic trees which contain errors (recall 242 CORPUS Sent. nb. Tokens nb. AFP 2 041 146 59 914 238 EST REP 2 998 261 53 913 288 WIKI 1 5</context>
</contexts>
<marker>Denis, Sagot, 2010</marker>
<rawString>P. Denis and B. Sagot. 2010. Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais. In Proceedings of Traitement Automatique des Langues Naturelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="5304" citStr="Gazdar et al., 1985" startWordPosition="848" endWordPosition="851">der features can be used. One drawback of the reranking approach is that correct SF for the predicates of a sentence can actually appear in different parse trees. Selecting complete trees can therefore lead to sub-optimal solutions. The method proposed in this paper merges parts of different trees that appear in an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that u</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Leon Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>10--136</pages>
<contexts>
<context position="5445" citStr="Joshi et al., 1975" startWordPosition="871" endWordPosition="874">fferent parse trees. Selecting complete trees can therefore lead to sub-optimal solutions. The method proposed in this paper merges parts of different trees that appear in an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind Joshi, Leon Levy, and M Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10:136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kupsc</author>
<author>Anne Abeill´e</author>
</authors>
<title>Treelex: A subcategorisation lexicon for french verbs.</title>
<date>2008</date>
<booktitle>In Proceedings of the First International Conference on Global Interoperability for Language Resources.</booktitle>
<marker>Kupsc, Abeill´e, 2008</marker>
<rawString>Anna Kupsc and Anne Abeill´e. 2008. Treelex: A subcategorisation lexicon for french verbs. In Proceedings of the First International Conference on Global Interoperability for Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17706" citStr="Manning, 1993" startWordPosition="3098" endWordPosition="3099">an consider as a verb a word that is not, and, of course, parsing errors, which tend to create crazy SF. In order to fight against noise, we have used a simple thresholding: we only collect ASF that occur more than a threshold i. The result of the thresholding appears in columns A5 and A10 , where the subscript is the value of the threshold. As expected both the number of verbs and SF decrease sharply when increasing the value of the threshold. Extracting SF for verbs from raw data has been an active direction of research for a long time, dating back at least to the work of (Brent, 1991) and (Manning, 1993). More recently (Messiant et al., 2008) proposed such a system for French verbs. The method we use for extracting SF is not novel with respect to such work. Our aim was not to devise new extraction techniques but merely to evaluate the resource produced by such techniques for statistical parsing. 3.4 Using an existing resource The third resource that we have used is the Lefff (Lexique des formes fl´echies du franc¸ais — Lexicon of French inflected form), a large-coverage syntactic lexicon for French (Sagot, 2010). The Lefff was developed in a semi-automatic way: automatic tools were used toget</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Christopher Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="1733" citStr="Marcus et al., 1993" startWordPosition="250" endWordPosition="253">uted on a large corpora and the third one is an existing syntactic lexicon. Experiments on the French Treebank showed a 15.24% reduction of erroneous subcategorization frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank. 1 Introduction Automatic syntactic parsing of natural languages has witnessed many important changes in the last fifteen years. Among these changes, two have modified the nature of the task itself. The first one is the availability of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Su</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="2327" citStr="McDonald et al., 2005" startWordPosition="347" endWordPosition="350">ebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We will focus in this paper on one kind of weakness of such parser which is their inability to properly take into account subc</context>
<context position="6900" citStr="McDonald et al., 2005" startWordPosition="1127" endWordPosition="1130">is the following: in section 2, we will briefly describe the parsing model that we will be using for this work and give accuracy results on a French corpus. Section 3 will describe three different resources that we have been using to correct SF errors made by the parser and give coverage results for these resources on a development corpus. Section 4 will propose three different ways to take into account, in the parser, the resources described in section 3 and give accuracy results. Section 5 concludes the paper. 2 The Parser The parser used in this work is the second order graph based parser (McDonald et al., 2005b) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency trees by (Candito et al., 2009). The size of the treebank and its decomposition into train, development and test sets are represented in table 1. nb of sentences nb of tokens TRAIN 9 881 278 083 DEV 1 239 36 508 TEST 1 235 36 340 Table 1: Size and decomposition of the French Treebank The parser gave state of the art results for parsing of French, reported in table 2. Table 2 reports the standard Labeled Accuracy Score (LAS) and Unlabeled Accuracy Scor</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Messiant</author>
<author>A Korhonen</author>
<author>T Poibeau</author>
</authors>
<title>Lexschem: A large subcategorization lexicon for french verbs.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="17745" citStr="Messiant et al., 2008" startWordPosition="3102" endWordPosition="3105">at is not, and, of course, parsing errors, which tend to create crazy SF. In order to fight against noise, we have used a simple thresholding: we only collect ASF that occur more than a threshold i. The result of the thresholding appears in columns A5 and A10 , where the subscript is the value of the threshold. As expected both the number of verbs and SF decrease sharply when increasing the value of the threshold. Extracting SF for verbs from raw data has been an active direction of research for a long time, dating back at least to the work of (Brent, 1991) and (Manning, 1993). More recently (Messiant et al., 2008) proposed such a system for French verbs. The method we use for extracting SF is not novel with respect to such work. Our aim was not to devise new extraction techniques but merely to evaluate the resource produced by such techniques for statistical parsing. 3.4 Using an existing resource The third resource that we have used is the Lefff (Lexique des formes fl´echies du franc¸ais — Lexicon of French inflected form), a large-coverage syntactic lexicon for French (Sagot, 2010). The Lefff was developed in a semi-automatic way: automatic tools were used together with manual work. The latest versio</context>
</contexts>
<marker>Messiant, Korhonen, Poibeau, 2008</marker>
<rawString>C. Messiant, A. Korhonen, T. Poibeau, et al. 2008. Lexschem: A large subcategorization lexicon for french verbs. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Mirroshandel</author>
<author>A Nasr</author>
</authors>
<title>Active learning for dependency parsing using partially annotated sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="8414" citStr="Mirroshandel and Nasr, 2011" startWordPosition="1398" endWordPosition="1401">uced this quantity in order to measure more accurately the impact of the methods described in this paper on the selection of a SF for the verbs of a sentence. TEST DEV SAS 80.84 79.88 LAS 88.88 88.53 UAS 90.71 90.37 Table 2: Subcategorization Frame Accuracy, Labeled and Unlabeled Accuracy Score on TEST and DEV. 240 We have chosen a second order graph parser in this work for two reasons. The first is that it is the parsing model that obtained the best results on the French Treebank. The second is that it allows us to impose structural constraints in the solution of the parser, as described in (Mirroshandel and Nasr, 2011), a feature that will reveal itself precious when enforcing SF in the parser output. 3 The Resources Three resources have been used in this work in order to correct SF errors. The first one has been extracted from a treebank, the second has been extracted from an automatically parsed corpus that is several order of magnitude bigger than the treebank. The third one has been extracted from an existing lexico-syntactic resource. The three resources are respectively described in sections 3.2, 3.3 and 3.4. Before describing the resources, we describe in details, in section 3.1 our definition of SF.</context>
<context position="28210" citStr="Mirroshandel and Nasr, 2011" startWordPosition="4990" endWordPosition="4993">uble Parsing The post processing method shows some improvement over the baseline. But it has an important drawback: it can create inconsistent parses. Recall that the parser we are using is based on a second order model. In other words, the score of a dependency depends on some neighboring dependencies. When building a new solution, the post processing method modifies some dependencies independently of their context, which may give birth to very unlikely configurations. In order to compute a new optimal parse tree that preserves the modified dependencies, we have used a technique proposed in (Mirroshandel and Nasr, 2011) that modifies the scoring function of the parser in such a way that the dependencies that we want to keep in the parser output get better scores than all competing dependencies. The new solution is therefore the optimal solution that preserves the dependencies modified by the PP method. The double parsing (DP) method is therefore a three stage method. First, sentence 5 is parsed, producing the n-best parses. Then, the post processing method is used, modifying the first best parse. Let’s note D the set of dependencies that were changed in this process. In the last stage, a new parse is produce</context>
</contexts>
<marker>Mirroshandel, Nasr, 2011</marker>
<rawString>S.A. Mirroshandel and A. Nasr. 2011. Active learning for dependency parsing using partially annotated sentences. In Proceedings of International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nasr</author>
<author>F B´echet</author>
<author>J-F Rey</author>
<author>B Favre</author>
<author>J Le Roux</author>
</authors>
<title>MACAON: An NLP tool suite for processing word lattices.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Nasr, B´echet, Rey, Favre, Le Roux, 2011</marker>
<rawString>A. Nasr, F. B´echet, J-F. Rey, B. Favre, and Le Roux J. 2011. MACAON: An NLP tool suite for processing word lattices. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S Kbler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>Maltparser: A language-independent system for data-driven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="2304" citStr="Nivre et al., 2007" startWordPosition="343" endWordPosition="346">such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We will focus in this paper on one kind of weakness of such parser which is their inability to properly</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, Kbler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative LogLinear Grammars with Latent Variables.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20 (NIPS),</booktitle>
<pages>1153--1160</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1904" citStr="Petrov and Klein, 2008" startWordPosition="278" endWordPosition="281">frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank. 1 Introduction Automatic syntactic parsing of natural languages has witnessed many important changes in the last fifteen years. Among these changes, two have modified the nature of the task itself. The first one is the availability of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S1 are con</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Discriminative LogLinear Grammars with Latent Variables. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS), pages 1153–1160, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammmar. CSLI Series.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="5370" citStr="Pollard and Sag, 1994" startWordPosition="858" endWordPosition="861"> is that correct SF for the predicates of a sentence can actually appear in different parse trees. Selecting complete trees can therefore lead to sub-optimal solutions. The method proposed in this paper merges parts of different trees that appear in an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a signific</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-driven Phrase Structure Grammmar. CSLI Series. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2284" citStr="Ratnaparkhi, 1999" startWordPosition="340" endWordPosition="342">ility of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We will focus in this paper on one kind of weakness of such parser which is their i</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine learning, 34(1):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
</authors>
<title>The Lefff, a freely available and large-coverage morphological and syntactic lexicon for french.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>2744--2751</pages>
<location>Valletta,</location>
<contexts>
<context position="16208" citStr="Sagot, 2010" startWordPosition="2811" endWordPosition="2812">omputed Subcat Frames The extraction procedure described above has been used to extract ASF from an automatically parsed corpus. The corpus is actually a collection of three corpora of slightly different genres. The first one is a collection of news reports of the French press agency Agence France Presse, the second is a collection of newspaper articles from a local French newspaper : l’Est R´epublicain. The third one is a collection of articles from the French Wikipedia. The size of the different corpora are detailed in table 4. The corpus was first POS tagged with the MELT tagger (Denis and Sagot, 2010), lemmatized with the MACAON tool suite (Nasr et al., 2011) and parsed in order to get the best parse for every sentence. Then the ASF have been extracted. The number of verbs, number of SF and average number of SF per verb are represented in table 3, in column AO (A stands for Automatic). As one can see, the number of verbs and SF are unrealistic. This is due to the fact that the data that we extract SF from is noisy: it consists of automatically produced syntactic trees which contain errors (recall 242 CORPUS Sent. nb. Tokens nb. AFP 2 041 146 59 914 238 EST REP 2 998 261 53 913 288 WIKI 1 5</context>
<context position="18224" citStr="Sagot, 2010" startWordPosition="3184" endWordPosition="3185">esearch for a long time, dating back at least to the work of (Brent, 1991) and (Manning, 1993). More recently (Messiant et al., 2008) proposed such a system for French verbs. The method we use for extracting SF is not novel with respect to such work. Our aim was not to devise new extraction techniques but merely to evaluate the resource produced by such techniques for statistical parsing. 3.4 Using an existing resource The third resource that we have used is the Lefff (Lexique des formes fl´echies du franc¸ais — Lexicon of French inflected form), a large-coverage syntactic lexicon for French (Sagot, 2010). The Lefff was developed in a semi-automatic way: automatic tools were used together with manual work. The latest version of the Lefff contains 10,618 verbal entries for 7,835 distinct verbal lemmas (the Lefff covers all categories, but only verbal entries are used in this work). A sub-categorization frame consists in a list of syntactic functions, using an inventory slightly more fine-grained than in the French Treebank, and for each of them a list of possible realizations (e.g., noun phrase, infinitive clause, or null-realization if the syntactic function is optional). For each verbal lemma</context>
</contexts>
<marker>Sagot, 2010</marker>
<rawString>Benoit Sagot. 2010. The Lefff, a freely available and large-coverage morphological and syntactic lexicon for french. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 2744–2751, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zeman</author>
</authors>
<title>Can subcategorization help a statistical dependency parser?</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguisticsVolume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5890" citStr="Zeman, 2002" startWordPosition="944" endWordPosition="945">mars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources that list the possible SF for verbs. The organization of the paper is the following: in section 2, we will briefly describe the parsing model that we will be using for this work and give accuracy results on a French corpus. Section 3 will describe three different resources that</context>
</contexts>
<marker>Zeman, 2002</marker>
<rawString>D. Zeman. 2002. Can subcategorization help a statistical dependency parser? In Proceedings of the 19th international conference on Computational linguisticsVolume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>