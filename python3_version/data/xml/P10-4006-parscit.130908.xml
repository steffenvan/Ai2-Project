<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000272">
<title confidence="0.973462">
The S-Space Package: An Open Source Package for Word Space Models
</title>
<author confidence="0.998306">
David Jurgens
</author>
<affiliation confidence="0.999318">
University of California, Los Angeles,
</affiliation>
<address confidence="0.7928495">
4732 Boelter Hall
Los Angeles, CA 90095
</address>
<email confidence="0.999413">
jurgens@cs.ucla.edu
</email>
<author confidence="0.995079">
Keith Stevens
</author>
<affiliation confidence="0.998733">
University of California, Los Angeles,
</affiliation>
<address confidence="0.792657">
4732 Boelter Hall
Los Angeles, CA 90095
</address>
<email confidence="0.999498">
kstevens@cs.ucla.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972214285714">
We present the S-Space Package, an open
source framework for developing and eval-
uating word space algorithms. The pack-
age implements well-known word space
algorithms, such as LSA, and provides a
comprehensive set of matrix utilities and
data structures for extending new or ex-
isting models. The package also includes
word space benchmarks for evaluation.
Both algorithms and libraries are designed
for high concurrency and scalability. We
demonstrate the efficiency of the reference
implementations and also provide their re-
sults on six benchmarks.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999748428571429">
Word similarity is an essential part of understand-
ing natural language. Similarity enables meaning-
ful comparisons, entailments, and is a bridge to
building and extending rich ontologies for evaluat-
ing word semantics. Word space algorithms have
been proposed as an automated approach for de-
veloping meaningfully comparable semantic rep-
resentations based on word distributions in text.
Many of the well known algorithms, such as
Latent Semantic Analysis (Landauer and Dumais,
1997) and Hyperspace Analogue to Language
(Burgess and Lund, 1997), have been shown to
approximate human judgements of word similar-
ity in addition to providing computational mod-
els for other psychological and linguistic phenom-
ena. More recent approaches have extended this
approach to model phenomena such as child lan-
guage acquisition (Baroni et al., 2007) or seman-
tic priming (Jones et al., 2006). In addition, these
models have provided insight in fields outside of
linguistics, such as information retrieval, natu-
ral language processing and cognitive psychology.
For a recent survey of word space approaches and
applications, see (Turney and Pantel, 2010).
The parallel development of word space models
in different fields has often resulted in duplicated
work. The pace of development presents a need
for a reliable method for accurate comparisons be-
tween new and existing approaches. Furthermore,
given the frequent similarity of approaches, we
argue that the research community would greatly
benefit from a common library and evaluation util-
ities for word spaces. Therefore, we introduce the
S-Space Package, an open source framework with
four main contributions:
</bodyText>
<listItem confidence="0.9668657">
1. reference implementations of frequently
cited algorithms
2. a comprehensive, highly concurrent library of
tools for building new models
3. an evaluation framework for testing mod-
els on standard benchmarks, e.g. the TOEFL
Synonym Test (Landauer et al., 1998)
4. a standardized interface for interacting with
all word space models, which facilitates word
space based applications.
</listItem>
<bodyText confidence="0.999887421052632">
The package is written in Java and defines a
standardized Java interface for word space algo-
rithms. While other word space frameworks ex-
ist, e.g. (Widdows and Ferraro, 2008), the focus
of this framework is to ease the development of
new algorithms and the comparison against exist-
ing models. Compared to existing frameworks,
the S-Space Package supports a much wider vari-
ety of algorithms and provides significantly more
reusable developer utilities for word spaces, such
as tokenizing and filtering, sparse vectors and
matrices, specialized data structures, and seam-
less integration with external programs for di-
mensionality reduction and clustering. We hope
that the release of this framework will greatly fa-
cilitate other researchers in their efforts to de-
velop and validate new word space models. The
toolkit is available at http://code.google.com/
p/airhead-research/, which includes a wiki
</bodyText>
<page confidence="0.983975">
30
</page>
<note confidence="0.601665">
Proceedings of the ACL 2010 System Demonstrations, pages 30–35,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9976265">
containing detailed information on the algorithms,
code documentation and mailing list archives.
</bodyText>
<sectionHeader confidence="0.871017" genericHeader="method">
2 Word Space Models
</sectionHeader>
<bodyText confidence="0.999846361702128">
Word space models are based on the contextual
distribution in which a word occurs. This ap-
proach has a long history in linguistics, starting
with Firth (1957) and Harris (1968), the latter
of whom defined this approach as the Distribu-
tional Hypothesis: for two words, their similarity
in meaning is predicted by the similarity of the
distributions of their co-occurring words. Later
models have expanded the notion of co-occurrence
but retain the premise that distributional similarity
can be used to extract meaningful relationships be-
tween words.
Word space algorithms consist of the same core
algorithmic steps: word features are extracted
from a corpus and the distribution of these features
is used as a basis for semantic similarity. Figure 1
illustrates the shared algorithmic structure of all
the approaches, which is divided into four compo-
nents: corpus processing, context selection, fea-
ture extraction and global vector space operations.
Corpus processing normalizes the input to cre-
ate a more uniform set of features on which the al-
gorithm can work. Corpus processing techniques
frequently include stemming and filtering of stop
words or low-frequency words. For web-gathered
corpora, these steps also include removal of non
linguistic tokens, such as html markup, or restrict-
ing documents to a single language.
Context selection determines which tokens in a
document may be considered for features. Com-
mon approaches use a lexical distance, syntac-
tic relation, or document co-occurrence to define
the context. The various decisions for selecting
the context accounts for many differences between
otherwise similar approaches.
Feature extraction determines the dimensions of
the vector space by selecting which tokens in the
context will count as features. Features are com-
monly word co-occurrences, but more advanced
models may perform a statistical analysis to se-
lect only those features that best distinguish word
meanings. Other models approximate the full set
of features to enable better scalability.
Global vector space operations are applied to
the entire space once the initial word features have
been computed. Common operations include al-
tering feature weights and dimensionality reduc-
</bodyText>
<figure confidence="0.290542166666667">
Document-Based Models
LSA (Landauer and Dumais, 1997)
ESA (Gabrilovich and Markovitch, 2007)
Vector Space Model (Salton et al., 1975)
Co-occurrence Models
HAL (Burgess and Lund, 1997)
COALS (Rohde et al., 2009)
Approximation Models
Random Indexing (Sahlgren et al., 2008)
Reflective Random Indexing (Cohen et al., 2009)
TRI (Jurgens and Stevens, 2009)
BEAGLE (Jones et al., 2006)
</figure>
<subsectionHeader confidence="0.270783">
Incremental Semantic Analysis (Baroni et al., 2007)
Word Sense Induction Models
</subsectionHeader>
<bodyText confidence="0.3030205">
Purandare and Pedersen (Purandare and Pedersen, 2004)
HERMIT (Jurgens and Stevens, 2010)
</bodyText>
<tableCaption confidence="0.998979">
Table 1: Algorithms in the S-Space Package
</tableCaption>
<bodyText confidence="0.999035666666667">
tion. These operations are designed to improve
word similarity by changing the feature space it-
self.
</bodyText>
<sectionHeader confidence="0.951295" genericHeader="method">
3 The S-Space Framework
</sectionHeader>
<bodyText confidence="0.999978857142857">
The S-Space framework is designed to be extensi-
ble, simple to use, and scalable. We achieve these
goals through the use of Java interfaces, reusable
word space related data structures, and support for
multi-threading. Each word space algorithm is de-
signed to run as a stand alone program and also to
be used as a library class.
</bodyText>
<subsectionHeader confidence="0.997521">
3.1 Reference Algorithms
</subsectionHeader>
<bodyText confidence="0.999990476190476">
The package provides reference implementations
for twelve word space algorithms, which are listed
in Table 1. Each algorithm is implemented in its
own Java package, and all commonalities have
been factored out into reusable library classes.
The algorithms implement the same Java interface,
which provides a consistent abstraction of the four
processing stages.
We divide the algorithms into four categories
based on their structural similarity: document-
based, co-occurrence, approximation, and Word
Sense Induction (WSI) models. Document-based
models divide a corpus into discrete documents
and construct the vector space from word fre-
quencies in the documents. The documents are
defined independently of the words that appear
in them. Co-occurrence models build the vector
space using the distribution of co-occurring words
in a context, which is typically defined as a re-
gion around a word or paths rooted in a parse
tree. The third category of models approximate
</bodyText>
<page confidence="0.999691">
31
</page>
<figure confidence="0.995162066666667">
Corpus Processing Context Selection Feature Extraction Global Operations
Corpus
Token Filtering
Stemming
Bigramming
Lexical Distance
In Same Document
Syntactic Link
Word Co-occurence
Joint Probabilitiy
Approximation
Dimensionality Reduction
Feature Selection
Matrix Transforms
Vector Space
</figure>
<figureCaption confidence="0.999961">
Figure 1: A high-level depiction of common algorithmic steps that convert a corpus into a word space
</figureCaption>
<bodyText confidence="0.999944571428571">
co-occurrence data rather than model it explic-
itly in order to achieve better scalability for larger
data sets. WSI models also use co-occurrence but
also attempt to discover distinct word senses while
building the vector space. For example, these al-
gorithms might represent “earth” with two vectors
based on its meanings “planet” and “dirt.”
</bodyText>
<subsectionHeader confidence="0.999925">
3.2 Data Structures and Utilities
</subsectionHeader>
<bodyText confidence="0.999984090909091">
The S-Space Package provides efficient imple-
mentations for matrices, vectors, and specialized
data structures such as multi-maps and tries. Im-
plementations are modeled after the java.util li-
brary and offer concurrent implementations when
multi-threading is required. In addition, the li-
braries provide support for converting between
multiple matrix formats, enabling interaction with
external matrix-based programs. The package also
provides support for parsing different corpora for-
mats, such as XML or email threads.
</bodyText>
<subsectionHeader confidence="0.994582">
3.3 Global Operation Utilities
</subsectionHeader>
<bodyText confidence="0.998902652173913">
Many algorithms incorporate dimensionality re-
duction to smooth their feature data, e.g. (Lan-
dauer and Dumais, 1997; Rohde et al., 2009),
or to improve efficiency, e.g. (Sahlgren et al.,
2008; Jones et al., 2006). The S-Space Pack-
age supports two common techniques: the Sin-
gular Value Decomposition (SVD) and random-
ized projections. All matrix data structures are de-
signed to seamlessly integrate with six SVD im-
plementations for maximum portability, including
SVDLIBJ1 , a Java port of SVDLIBC2, a scalable
sparse SVD library. The package also provides
a comprehensive library for randomized projec-
tions, which project high-dimensional feature data
into a lower dimensional space. The library sup-
ports both integer-based projections (Kanerva et
al., 2000) and Gaussian-based (Jones et al., 2006).
The package supports common matrix trans-
formations that have been applied to word
spaces: point wise mutual information (Dekang,
1998), term frequency-inverse document fre-
quency (Salton and Buckley, 1988), and log en-
tropy (Landauer and Dumais, 1997).
</bodyText>
<subsectionHeader confidence="0.887195">
3.4 Measurements
</subsectionHeader>
<bodyText confidence="0.999482615384616">
The choice of similarity function for the vector
space is the least standardized across approaches.
Typically the function is empirically chosen based
on a performance benchmark and different func-
tions have been shown to provide application spe-
cific benefits (Weeds et al., 2004). To facili-
tate exploration of the similarity function param-
eter space, the S-Space Package provides sup-
port for multiple similarity functions: cosine sim-
ilarity, Euclidean distance, KL divergence, Jac-
card Index, Pearson product-moment correlation,
Spearman’s rank correlation, and Lin Similarity
(Dekang, 1998)
</bodyText>
<subsectionHeader confidence="0.916243">
3.5 Clustering
</subsectionHeader>
<bodyText confidence="0.9999141">
Clustering serves as a tool for building and refin-
ing word spaces. WSI algorithms, e.g. (Puran-
dare and Pedersen, 2004), use clustering to dis-
cover the different meanings of a word in a cor-
pus. The S-Space Package provides bindings for
using the CLUTO clustering package3. In addi-
tion, the package provides Java implementations
of Hierarchical Agglomerative Clustering, Spec-
tral Clustering (Kannan et al., 2004), and the Gap
Statistic (Tibshirani et al., 2000).
</bodyText>
<sectionHeader confidence="0.997702" genericHeader="method">
4 Benchmarks
</sectionHeader>
<bodyText confidence="0.999487888888889">
Word space benchmarks assess the semantic con-
tent of the space through analyzing the geomet-
ric properties of the space itself. Currently used
benchmarks assess the semantics by inspecting the
representational similarity of word pairs. Two
types of benchmarks are commonly used: word
choice tests and association tests. The S-Space
Package supports six tests, and has an easily ex-
tensible model for adding new tests.
</bodyText>
<footnote confidence="0.9998555">
1http://bender.unibe.ch/svn/codemap/Archive/svdlibj/
2http://tedlab.mit.edu/˜dr/SVDLIBC/ 3http://glaros.dtc.umn.edu/gkhome/views/cluto
</footnote>
<page confidence="0.97985">
32
</page>
<table confidence="0.999842933333333">
Algorithm Corpus Word Choice Word Association
TOEFL ESL RDWP R-G WordSim353 Deese
BEAGLE TASA 46.03 35.56 46.99 0.431 0.342 0.235
COALS TASA 65.33 60.42 93.02 0.572 0.478 0.388
HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318
HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042
ISA TASA 41.33 18.75 33.72 0.245 0.150 0.286
LSA TASA 56.00a 50.00 45.83 0.652 0.519 0.349
LSA Wiki 60.76 54.17 59.20 0.681 0.614 0.206
P&amp;P TASA 34.67 20.83 31.39 0.088 -0.036 0.216
RI TASA 42.67 27.08 34.88 0.224 0.201 0.211
RI Wiki 68.35 31.25 40.80 0.226 0.315 0.090
RI + Perm.b TASA 52.00 33.33 31.39 0.137 0.260 0.268
RRI TASA 36.00 22.92 34.88 0.088 0.138 0.109
VSM TASA 61.33 52.08 84.88 0.496 0.396 0.200
</table>
<tableCaption confidence="0.971299666666667">
a Landauer et al. (1997) report a score of 64.4 for this test, while Rohde et al. (2009) report a score of 53.4.
b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al., 2008)
Table 2: A comparison of the implemented algorithms on common evaluation benchmarks
</tableCaption>
<subsectionHeader confidence="0.993357">
4.1 Word Choice
</subsectionHeader>
<bodyText confidence="0.999897611111111">
Word choice tests provide a target word and a list
of options, one of which has the desired relation to
the target. Word space models solve these tests by
selecting the option whose representation is most
similar. Three word choice benchmarks that mea-
sure synonymy are supported.
The first test is the widely-reported Test of En-
glish as a Foreign Language (TOEFL) synonym
test from (Landauer et al., 1998), which consists
of 80 multiple-choice questions with four options.
The second test comes from the English as a Sec-
ond Language (ESL) exam and consists of 50
question with four choices (Turney, 2001). The
third consists of 200 questions from the Canadian
Reader’s Digest Word Power (RDWP) (Jarmasz
and Szpakowicz, 2003), which unlike the previ-
ous two tests, allows the target and options to be
multi-word phrases.
</bodyText>
<subsectionHeader confidence="0.978809">
4.2 Word Association
</subsectionHeader>
<bodyText confidence="0.993961133333333">
Word association tests measure the semantic re-
latedness of two words by comparing word space
similarity with human judgements. Frequently,
these tests measure synonymy; however, other
types of word relations such as antonymy (“hot”
and “cold”) or functional relatedness (“doctor”
and “hospital”) are also possible. The S-Space
Package supports three association tests.
The first test uses data gathered by Rubenstein
and Goodneough (1965). To measure word simi-
larity, word similarity scores of 51 human review-
ers were gathered a set of 65 noun pairs, scored on
a scale of 0 to 4. The ratings are then correlated
with word space similarity scores.
Finkelstein et al. (2002) test for relatedness. 353
word pairs were rated by either 13 or 16 subjects
on a 0 to 10 scale for how related the words are.
This test is notably more challenging for word
space models because human ratings are not tied
to a specific semantic relation.
The third benchmark considers the antonym as-
sociation. Deese (1964) introduced 39 antonym
pairs that Greffenstette (1992) used to assess
whether a word space modeled the antonymy rela-
tionship. We quantify this relationship by measur-
ing the similarity rank of each word in an antonym
pair, w1, w2, i.e. w2 is the kth most-similar word
to w1 in the vector space. The antonym score is
calculated as rank l (w2)+rank,, 2 (wl) &apos; The score
,,
</bodyText>
<page confidence="0.389136">
2
</page>
<bodyText confidence="0.998386666666667">
ranges from [0, 1], where 1 indicates that the most
similar neighbors in the space are antonyms. We
report the mean score for all 39 antonyms.
</bodyText>
<sectionHeader confidence="0.985611" genericHeader="method">
5 Algorithm Analysis
</sectionHeader>
<bodyText confidence="0.9999265">
The content of a word space is fundamentally
dependent upon the corpus used to construct it.
Moreover, algorithms which use operations such
as the SVD have a limit to the corpora sizes they
</bodyText>
<page confidence="0.99574">
33
</page>
<figure confidence="0.7491415">
Tokens in Documents (in millions)
Number of documents
</figure>
<figureCaption confidence="0.9902654">
Figure 2: Processing time across different corpus
sizes for a word space with the 100,000 most fre-
quent words
Figure 3: Run time improvement as a factor of in-
creasing the number of threads
</figureCaption>
<bodyText confidence="0.999815972222222">
can process. We therefore highlight the differ-
ences in performance using two corpora. TASA
is a collection of 44,486 topical essays introduced
in (Landauer and Dumais, 1997). The second cor-
pus is built from a Nov. 11, 2009 Wikipedia snap-
shot, and filtered to contain only articles with more
than 1000 words. The resulting corpus consists of
387,082 documents and 917 million tokens.
Table 2 reports the scores of reference algo-
rithms on the six benchmarks using cosine simi-
larity. The variation in scoring illustrates that dif-
ferent algorithms are more effective at capturing
certain semantic relations. We note that scores are
likely to change for different parameter configura-
tions of the same algorithm, e.g. token filtering or
changing the number of dimensions.
As a second analysis, we report the efficiency
of reference implementations by varying the cor-
pus size and number of threads. Figure 2 reports
the total amount of time each algorithm needs for
processing increasingly larger segments of a web-
gathered corpus when using 8 threads. In all cases,
only the top 100,000 words were counted as fea-
tures. Figure 3 reports run time improvements due
to multi-threading on the TASA corpus.
Algorithm efficiency is determined by three fac-
tors: contention on global statistics, contention on
disk I/O, and memory limitations. Multi-threading
benefits increase proportionally to the amount of
work done per context. Memory limitations ac-
count for the largest efficiency constraint, espe-
cially as the corpus size and number of features
grow. Several algorithms lack data points for
larger corpora and show a sharp increase in run-
ning time in Figure 2, reflecting the point at which
the models no longer fit into 8GB of memory.
</bodyText>
<sectionHeader confidence="0.99256" genericHeader="discussions">
6 Future Work and Conclusion
</sectionHeader>
<bodyText confidence="0.99998125">
We have described a framework for developing
and evaluating word space algorithms. Many well
known algorithms are already provided as part of
the framework as reference implementations for
researches in distributional semantics. We have
shown that the provided algorithms and libraries
scale appropriately. Last, we motivate further re-
search by illustrating the significant performance
differences of the algorithms on six benchmarks.
Future work will be focused on providing sup-
port for syntactic features, including dependency
parsing as described by (Pad´o and Lapata, 2007),
reference implementations of algorithms that use
this information, non-linear dimensionality reduc-
tion techniques, and more advanced clustering al-
gorithms.
</bodyText>
<sectionHeader confidence="0.990787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.743818882352941">
Marco Baroni, Alessandro Lenci, and Luca Onnis.
2007. Isa meets lara: A fully incremental word
space model for cognitively plausible simulations of
semantic learning. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics.
Curt Burgess and Kevin Lund. 1997. Modeling pars-
ing constraints with high-dimensional context space.
Language and Cognitive Processes, 12:177210.
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2009. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43.
J. Deese. 1964. The associative structure of some com-
mon english adjectives. Journal of Verbal Learning
and Verbal Behavior, 3(5):347–357.
</reference>
<figure confidence="0.998875323529412">
63.5M 125M 173M 228M 267M 296M
100000 200000 300000 400000 500000 600000
Seconds
25000
20000
15000
10000
5000
0
LSA
VSM
COALS
BEAGLE
HAL
RI
RRI
BEAGLE
COALS
LSA
HAL
RI
VSM
500
400
300
200
100
0
2 3 4 5 6 7 8
Number of threads
Percentage improvement
800
700
600
</figure>
<page confidence="0.987403">
34
</page>
<reference confidence="0.999521666666667">
Lin Dekang. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the Joint An-
nual Meeting of the Association for Computational
Linguistics and International Conference on Com-
putational Linguistics, pages 768–774.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Z. S.
Rivlin, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions of Information Systems, 20(1):116–
131.
J. R. Firth, 1957. A synopsis of linguistic theory 1930-
1955. Oxford: Philological Society. Reprinted in
F. R. Palmer (Ed.), (1968). Selected papers of J. R.
Firth 1952-1959, London: Longman.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI’07: Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, pages 1606–1611.
Gregory Grefenstette. 1992. Finding semantic similar-
ity in raw text: The Deese antonyms. In Working
notes of the AAAI Fall Symposium on Probabilis-
tic Approaches to Natural Language, pages 61–65.
AAAI Press.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
thesaurus and semantic similarity. In Conference on
Recent Advances in Natural Language Processing,
pages 212–219.
Michael N. Jones, Walter Kintsch, and Doughlas J. K.
Mewhort. 2006. High-dimensional semantic space
accounts of priming. Journal of Memory and Lan-
guage, 55:534–552.
David Jurgens and Keith Stevens. 2009. Event detec-
tion in blogs using temporal random indexing. In
Proceedings of RANLP 2009: Events in Emerging
Text Types Workshop.
David Jurgens and Keith Stevens. 2010. HERMIT:
Flexible Clustering for the SemEval-2 WSI Task. In
Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2010). Association
of Computational Linguistics.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In L. R. Gleitman and A. K. Josh, editors,
Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497–515.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato’s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211–240.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to Latent Semantic Analysis. Discourse
Processes, (25):259–284.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161–199.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In HLT-NAACL 2004 Work-
shop: Eighth Conference on Computational Natu-
ral Language Learning (CoNLL-2004), pages 41–
48. Association for Computational Linguistics.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2009. An improved model of
semantic similarity based on lexical co-occurrence.
Cognitive Science. sumitted.
H. Rubenstein and J. B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627–633.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society (CogSci’08).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing &amp; Management, 24:513–523.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18(11):613–620.
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in a
dataset via the gap statistic. Journal Royal Statistics
Society B, 63:411–423.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal ofArtificial Intelligence Research,
37:141–188.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491–502.
Julie Weeds, David Weir, and Diana McCarty. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
COLING’04, pages 1015–1021.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic vectors: a scalable open source package and
online technology management application. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC’08).
</reference>
<page confidence="0.999321">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975433">
<title confidence="0.999809">The S-Space Package: An Open Source Package for Word Space Models</title>
<author confidence="0.99997">David Jurgens</author>
<affiliation confidence="0.999908">University of California, Los Angeles,</affiliation>
<address confidence="0.9996545">4732 Boelter Hall Los Angeles, CA 90095</address>
<email confidence="0.999646">jurgens@cs.ucla.edu</email>
<author confidence="0.998378">Keith Stevens</author>
<affiliation confidence="0.999915">University of California, Los Angeles,</affiliation>
<address confidence="0.9995505">4732 Boelter Hall Los Angeles, CA 90095</address>
<email confidence="0.999715">kstevens@cs.ucla.edu</email>
<abstract confidence="0.998626266666667">We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
<author>Luca Onnis</author>
</authors>
<title>Isa meets lara: A fully incremental word space model for cognitively plausible simulations of semantic learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1697" citStr="Baroni et al., 2007" startWordPosition="248" endWordPosition="251">tics. Word space algorithms have been proposed as an automated approach for developing meaningfully comparable semantic representations based on word distributions in text. Many of the well known algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997) and Hyperspace Analogue to Language (Burgess and Lund, 1997), have been shown to approximate human judgements of word similarity in addition to providing computational models for other psychological and linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). The parallel development of word space models in different fields has often resulted in duplicated work. The pace of development presents a need for a reliable method for accurate comparisons between new and existing approaches. Furthermore, given the frequent similarity of approaches, we argue </context>
<context position="6692" citStr="Baroni et al., 2007" startWordPosition="999" endWordPosition="1002">Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space algorithm is designed to run as a stand alone program and also to be used as a l</context>
</contexts>
<marker>Baroni, Lenci, Onnis, 2007</marker>
<rawString>Marco Baroni, Alessandro Lenci, and Luca Onnis. 2007. Isa meets lara: A fully incremental word space model for cognitively plausible simulations of semantic learning. In Proceedings of the 45th Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
<author>Kevin Lund</author>
</authors>
<title>Modeling parsing constraints with high-dimensional context space.</title>
<date>1997</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>12--177210</pages>
<contexts>
<context position="1406" citStr="Burgess and Lund, 1997" startWordPosition="203" endWordPosition="206">e implementations and also provide their results on six benchmarks. 1 Introduction Word similarity is an essential part of understanding natural language. Similarity enables meaningful comparisons, entailments, and is a bridge to building and extending rich ontologies for evaluating word semantics. Word space algorithms have been proposed as an automated approach for developing meaningfully comparable semantic representations based on word distributions in text. Many of the well known algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997) and Hyperspace Analogue to Language (Burgess and Lund, 1997), have been shown to approximate human judgements of word similarity in addition to providing computational models for other psychological and linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). The p</context>
<context position="6444" citStr="Burgess and Lund, 1997" startWordPosition="962" endWordPosition="965"> Features are commonly word co-occurrences, but more advanced models may perform a statistical analysis to select only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use,</context>
</contexts>
<marker>Burgess, Lund, 1997</marker>
<rawString>Curt Burgess and Kevin Lund. 1997. Modeling parsing constraints with high-dimensional context space. Language and Cognitive Processes, 12:177210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohen</author>
<author>Roger Schvaneveldt</author>
<author>Dominic Widdows</author>
</authors>
<title>Reflective random indexing and indirect inference: A scalable method for discovery of implicit connections.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>43</volume>
<contexts>
<context position="6580" citStr="Cohen et al., 2009" startWordPosition="982" endWordPosition="985">est distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for m</context>
</contexts>
<marker>Cohen, Schvaneveldt, Widdows, 2009</marker>
<rawString>Trevor Cohen, Roger Schvaneveldt, and Dominic Widdows. 2009. Reflective random indexing and indirect inference: A scalable method for discovery of implicit connections. Journal of Biomedical Informatics, 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Deese</author>
</authors>
<title>The associative structure of some common english adjectives.</title>
<date>1964</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>3</volume>
<issue>5</issue>
<contexts>
<context position="15154" citStr="Deese (1964)" startWordPosition="2303" endWordPosition="2304">t uses data gathered by Rubenstein and Goodneough (1965). To measure word similarity, word similarity scores of 51 human reviewers were gathered a set of 65 noun pairs, scored on a scale of 0 to 4. The ratings are then correlated with word space similarity scores. Finkelstein et al. (2002) test for relatedness. 353 word pairs were rated by either 13 or 16 subjects on a 0 to 10 scale for how related the words are. This test is notably more challenging for word space models because human ratings are not tied to a specific semantic relation. The third benchmark considers the antonym association. Deese (1964) introduced 39 antonym pairs that Greffenstette (1992) used to assess whether a word space modeled the antonymy relationship. We quantify this relationship by measuring the similarity rank of each word in an antonym pair, w1, w2, i.e. w2 is the kth most-similar word to w1 in the vector space. The antonym score is calculated as rank l (w2)+rank,, 2 (wl) &apos; The score ,, 2 ranges from [0, 1], where 1 indicates that the most similar neighbors in the space are antonyms. We report the mean score for all 39 antonyms. 5 Algorithm Analysis The content of a word space is fundamentally dependent upon the </context>
</contexts>
<marker>Deese, 1964</marker>
<rawString>J. Deese. 1964. The associative structure of some common english adjectives. Journal of Verbal Learning and Verbal Behavior, 3(5):347–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Dekang</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the Joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="10550" citStr="Dekang, 1998" startWordPosition="1577" endWordPosition="1578"> projections. All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer-based projections (Kanerva et al., 2000) and Gaussian-based (Jones et al., 2006). The package supports common matrix transformations that have been applied to word spaces: point wise mutual information (Dekang, 1998), term frequency-inverse document frequency (Salton and Buckley, 1988), and log entropy (Landauer and Dumais, 1997). 3.4 Measurements The choice of similarity function for the vector space is the least standardized across approaches. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al., 2004). To facilitate exploration of the similarity function parameter space, the S-Space Package provides support for multiple similarity functions: cosine similarity, Euclidean distance, KL d</context>
</contexts>
<marker>Dekang, 1998</marker>
<rawString>Lin Dekang. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the Joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Z S Rivlin</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions of Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>131</pages>
<contexts>
<context position="14832" citStr="Finkelstein et al. (2002)" startWordPosition="2244" endWordPosition="2247">atedness of two words by comparing word space similarity with human judgements. Frequently, these tests measure synonymy; however, other types of word relations such as antonymy (“hot” and “cold”) or functional relatedness (“doctor” and “hospital”) are also possible. The S-Space Package supports three association tests. The first test uses data gathered by Rubenstein and Goodneough (1965). To measure word similarity, word similarity scores of 51 human reviewers were gathered a set of 65 noun pairs, scored on a scale of 0 to 4. The ratings are then correlated with word space similarity scores. Finkelstein et al. (2002) test for relatedness. 353 word pairs were rated by either 13 or 16 subjects on a 0 to 10 scale for how related the words are. This test is notably more challenging for word space models because human ratings are not tied to a specific semantic relation. The third benchmark considers the antonym association. Deese (1964) introduced 39 antonym pairs that Greffenstette (1992) used to assess whether a word space modeled the antonymy relationship. We quantify this relationship by measuring the similarity rank of each word in an antonym pair, w1, w2, i.e. w2 is the kth most-similar word to w1 in th</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Z. S. Rivlin, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions of Information Systems, 20(1):116– 131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Oxford: Philological Society.</title>
<date>1957</date>
<publisher>Longman.</publisher>
<location>London:</location>
<note>Reprinted in</note>
<contexts>
<context position="4211" citStr="Firth (1957)" startWordPosition="628" endWordPosition="629">y facilitate other researchers in their efforts to develop and validate new word space models. The toolkit is available at http://code.google.com/ p/airhead-research/, which includes a wiki 30 Proceedings of the ACL 2010 System Demonstrations, pages 30–35, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics containing detailed information on the algorithms, code documentation and mailing list archives. 2 Word Space Models Word space models are based on the contextual distribution in which a word occurs. This approach has a long history in linguistics, starting with Firth (1957) and Harris (1968), the latter of whom defined this approach as the Distributional Hypothesis: for two words, their similarity in meaning is predicted by the similarity of the distributions of their co-occurring words. Later models have expanded the notion of co-occurrence but retain the premise that distributional similarity can be used to extract meaningful relationships between words. Word space algorithms consist of the same core algorithmic steps: word features are extracted from a corpus and the distribution of these features is used as a basis for semantic similarity. Figure 1 illustrat</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth, 1957. A synopsis of linguistic theory 1930-1955. Oxford: Philological Society. Reprinted in F. R. Palmer (Ed.), (1968). Selected papers of J. R. Firth 1952-1959, London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="6353" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="948" endWordPosition="951">s the dimensions of the vector space by selecting which tokens in the context will count as features. Features are commonly word co-occurrences, but more advanced models may perform a statistical analysis to select only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself.</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Finding semantic similarity in raw text: The Deese antonyms.</title>
<date>1992</date>
<booktitle>In Working notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<pages>61--65</pages>
<publisher>AAAI Press.</publisher>
<marker>Grefenstette, 1992</marker>
<rawString>Gregory Grefenstette. 1992. Finding semantic similarity in raw text: The Deese antonyms. In Working notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 61–65. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="4229" citStr="Harris (1968)" startWordPosition="631" endWordPosition="632">r researchers in their efforts to develop and validate new word space models. The toolkit is available at http://code.google.com/ p/airhead-research/, which includes a wiki 30 Proceedings of the ACL 2010 System Demonstrations, pages 30–35, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics containing detailed information on the algorithms, code documentation and mailing list archives. 2 Word Space Models Word space models are based on the contextual distribution in which a word occurs. This approach has a long history in linguistics, starting with Firth (1957) and Harris (1968), the latter of whom defined this approach as the Distributional Hypothesis: for two words, their similarity in meaning is predicted by the similarity of the distributions of their co-occurring words. Later models have expanded the notion of co-occurrence but retain the premise that distributional similarity can be used to extract meaningful relationships between words. Word space algorithms consist of the same core algorithmic steps: word features are extracted from a corpus and the distribution of these features is used as a basis for semantic similarity. Figure 1 illustrates the shared algo</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>212--219</pages>
<contexts>
<context position="14044" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="2119" endWordPosition="2122">ation to the target. Word space models solve these tests by selecting the option whose representation is most similar. Three word choice benchmarks that measure synonymy are supported. The first test is the widely-reported Test of English as a Foreign Language (TOEFL) synonym test from (Landauer et al., 1998), which consists of 80 multiple-choice questions with four options. The second test comes from the English as a Second Language (ESL) exam and consists of 50 question with four choices (Turney, 2001). The third consists of 200 questions from the Canadian Reader’s Digest Word Power (RDWP) (Jarmasz and Szpakowicz, 2003), which unlike the previous two tests, allows the target and options to be multi-word phrases. 4.2 Word Association Word association tests measure the semantic relatedness of two words by comparing word space similarity with human judgements. Frequently, these tests measure synonymy; however, other types of word relations such as antonymy (“hot” and “cold”) or functional relatedness (“doctor” and “hospital”) are also possible. The S-Space Package supports three association tests. The first test uses data gathered by Rubenstein and Goodneough (1965). To measure word similarity, word similarity </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Conference on Recent Advances in Natural Language Processing, pages 212–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael N Jones</author>
<author>Walter Kintsch</author>
<author>Doughlas J K Mewhort</author>
</authors>
<title>High-dimensional semantic space accounts of priming.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<pages>55--534</pages>
<contexts>
<context position="1738" citStr="Jones et al., 2006" startWordPosition="256" endWordPosition="259">osed as an automated approach for developing meaningfully comparable semantic representations based on word distributions in text. Many of the well known algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997) and Hyperspace Analogue to Language (Burgess and Lund, 1997), have been shown to approximate human judgements of word similarity in addition to providing computational models for other psychological and linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). The parallel development of word space models in different fields has often resulted in duplicated work. The pace of development presents a need for a reliable method for accurate comparisons between new and existing approaches. Furthermore, given the frequent similarity of approaches, we argue that the research community would greatly</context>
<context position="6640" citStr="Jones et al., 2006" startWordPosition="992" endWordPosition="995">full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space algorithm is designed to run</context>
<context position="9830" citStr="Jones et al., 2006" startWordPosition="1468" endWordPosition="1471">plementations are modeled after the java.util library and offer concurrent implementations when multi-threading is required. In addition, the libraries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. The package also provides support for parsing different corpora formats, such as XML or email threads. 3.3 Global Operation Utilities Many algorithms incorporate dimensionality reduction to smooth their feature data, e.g. (Landauer and Dumais, 1997; Rohde et al., 2009), or to improve efficiency, e.g. (Sahlgren et al., 2008; Jones et al., 2006). The S-Space Package supports two common techniques: the Singular Value Decomposition (SVD) and randomized projections. All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer-based projections (Kanerva et al., 2000) and Gaussian-based (Jones et al., 2006). The package su</context>
</contexts>
<marker>Jones, Kintsch, Mewhort, 2006</marker>
<rawString>Michael N. Jones, Walter Kintsch, and Doughlas J. K. Mewhort. 2006. High-dimensional semantic space accounts of priming. Journal of Memory and Language, 55:534–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>Event detection in blogs using temporal random indexing.</title>
<date>2009</date>
<booktitle>In Proceedings of RANLP 2009: Events in Emerging Text Types Workshop.</booktitle>
<contexts>
<context position="6612" citStr="Jurgens and Stevens, 2009" startWordPosition="987" endWordPosition="990">ings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space </context>
</contexts>
<marker>Jurgens, Stevens, 2009</marker>
<rawString>David Jurgens and Keith Stevens. 2009. Event detection in blogs using temporal random indexing. In Proceedings of RANLP 2009: Events in Emerging Text Types Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>HERMIT: Flexible Clustering for the SemEval-2 WSI Task.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010). Association of Computational Linguistics.</booktitle>
<contexts>
<context position="6809" citStr="Jurgens and Stevens, 2010" startWordPosition="1015" endWordPosition="1018">d. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space algorithm is designed to run as a stand alone program and also to be used as a library class. 3.1 Reference Algorithms The package provides reference implementations for twelve word space algorithm</context>
</contexts>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. HERMIT: Flexible Clustering for the SemEval-2 WSI Task. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010). Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kanerva</author>
<author>J Kristoferson</author>
<author>A Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1036</pages>
<editor>In L. R. Gleitman and A. K. Josh, editors,</editor>
<contexts>
<context position="10374" citStr="Kanerva et al., 2000" startWordPosition="1549" endWordPosition="1552">), or to improve efficiency, e.g. (Sahlgren et al., 2008; Jones et al., 2006). The S-Space Package supports two common techniques: the Singular Value Decomposition (SVD) and randomized projections. All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer-based projections (Kanerva et al., 2000) and Gaussian-based (Jones et al., 2006). The package supports common matrix transformations that have been applied to word spaces: point wise mutual information (Dekang, 1998), term frequency-inverse document frequency (Salton and Buckley, 1988), and log entropy (Landauer and Dumais, 1997). 3.4 Measurements The choice of similarity function for the vector space is the least standardized across approaches. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al., 2004). To facili</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>P. Kanerva, J. Kristoferson, and A. Holst. 2000. Random indexing of text samples for latent semantic analysis. In L. R. Gleitman and A. K. Josh, editors, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, page 1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kannan</author>
<author>Santosh Vempala</author>
<author>Adrian Vetta</author>
</authors>
<title>On clusterings: Good, bad and spectral.</title>
<date>2004</date>
<journal>Journal of the ACM,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="11700" citStr="Kannan et al., 2004" startWordPosition="1746" endWordPosition="1749">iple similarity functions: cosine similarity, Euclidean distance, KL divergence, Jaccard Index, Pearson product-moment correlation, Spearman’s rank correlation, and Lin Similarity (Dekang, 1998) 3.5 Clustering Clustering serves as a tool for building and refining word spaces. WSI algorithms, e.g. (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. The S-Space Package provides bindings for using the CLUTO clustering package3. In addition, the package provides Java implementations of Hierarchical Agglomerative Clustering, Spectral Clustering (Kannan et al., 2004), and the Gap Statistic (Tibshirani et al., 2000). 4 Benchmarks Word space benchmarks assess the semantic content of the space through analyzing the geometric properties of the space itself. Currently used benchmarks assess the semantics by inspecting the representational similarity of word pairs. Two types of benchmarks are commonly used: word choice tests and association tests. The S-Space Package supports six tests, and has an easily extensible model for adding new tests. 1http://bender.unibe.ch/svn/codemap/Archive/svdlibj/ 2http://tedlab.mit.edu/˜dr/SVDLIBC/ 3http://glaros.dtc.umn.edu/gkho</context>
</contexts>
<marker>Kannan, Vempala, Vetta, 2004</marker>
<rawString>Ravi Kannan, Santosh Vempala, and Adrian Vetta. 2004. On clusterings: Good, bad and spectral. Journal of the ACM, 51(3):497–515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="1345" citStr="Landauer and Dumais, 1997" startWordPosition="194" endWordPosition="197">y and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks. 1 Introduction Word similarity is an essential part of understanding natural language. Similarity enables meaningful comparisons, entailments, and is a bridge to building and extending rich ontologies for evaluating word semantics. Word space algorithms have been proposed as an automated approach for developing meaningfully comparable semantic representations based on word distributions in text. Many of the well known algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997) and Hyperspace Analogue to Language (Burgess and Lund, 1997), have been shown to approximate human judgements of word similarity in addition to providing computational models for other psychological and linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space appr</context>
<context position="6314" citStr="Landauer and Dumais, 1997" startWordPosition="943" endWordPosition="946">es. Feature extraction determines the dimensions of the vector space by selecting which tokens in the context will count as features. Features are commonly word co-occurrences, but more advanced models may perform a statistical analysis to select only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarit</context>
<context position="9733" citStr="Landauer and Dumais, 1997" startWordPosition="1450" endWordPosition="1454">implementations for matrices, vectors, and specialized data structures such as multi-maps and tries. Implementations are modeled after the java.util library and offer concurrent implementations when multi-threading is required. In addition, the libraries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. The package also provides support for parsing different corpora formats, such as XML or email threads. 3.3 Global Operation Utilities Many algorithms incorporate dimensionality reduction to smooth their feature data, e.g. (Landauer and Dumais, 1997; Rohde et al., 2009), or to improve efficiency, e.g. (Sahlgren et al., 2008; Jones et al., 2006). The S-Space Package supports two common techniques: the Singular Value Decomposition (SVD) and randomized projections. All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer</context>
<context position="16299" citStr="Landauer and Dumais, 1997" startWordPosition="2500" endWordPosition="2503">gorithm Analysis The content of a word space is fundamentally dependent upon the corpus used to construct it. Moreover, algorithms which use operations such as the SVD have a limit to the corpora sizes they 33 Tokens in Documents (in millions) Number of documents Figure 2: Processing time across different corpus sizes for a word space with the 100,000 most frequent words Figure 3: Run time improvement as a factor of increasing the number of threads can process. We therefore highlight the differences in performance using two corpora. TASA is a collection of 44,486 topical essays introduced in (Landauer and Dumais, 1997). The second corpus is built from a Nov. 11, 2009 Wikipedia snapshot, and filtered to contain only articles with more than 1000 words. The resulting corpus consists of 387,082 documents and 917 million tokens. Table 2 reports the scores of reference algorithms on the six benchmarks using cosine similarity. The variation in scoring illustrates that different algorithms are more effective at capturing certain semantic relations. We note that scores are likely to change for different parameter configurations of the same algorithm, e.g. token filtering or changing the number of dimensions. As a se</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="2771" citStr="Landauer et al., 1998" startWordPosition="410" endWordPosition="413"> a reliable method for accurate comparisons between new and existing approaches. Furthermore, given the frequent similarity of approaches, we argue that the research community would greatly benefit from a common library and evaluation utilities for word spaces. Therefore, we introduce the S-Space Package, an open source framework with four main contributions: 1. reference implementations of frequently cited algorithms 2. a comprehensive, highly concurrent library of tools for building new models 3. an evaluation framework for testing models on standard benchmarks, e.g. the TOEFL Synonym Test (Landauer et al., 1998) 4. a standardized interface for interacting with all word space models, which facilitates word space based applications. The package is written in Java and defines a standardized Java interface for word space algorithms. While other word space frameworks exist, e.g. (Widdows and Ferraro, 2008), the focus of this framework is to ease the development of new algorithms and the comparison against existing models. Compared to existing frameworks, the S-Space Package supports a much wider variety of algorithms and provides significantly more reusable developer utilities for word spaces, such as tok</context>
<context position="13725" citStr="Landauer et al., 1998" startWordPosition="2068" endWordPosition="2071">f 53.4. b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al., 2008) Table 2: A comparison of the implemented algorithms on common evaluation benchmarks 4.1 Word Choice Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. Word space models solve these tests by selecting the option whose representation is most similar. Three word choice benchmarks that measure synonymy are supported. The first test is the widely-reported Test of English as a Foreign Language (TOEFL) synonym test from (Landauer et al., 1998), which consists of 80 multiple-choice questions with four options. The second test comes from the English as a Second Language (ESL) exam and consists of 50 question with four choices (Turney, 2001). The third consists of 200 questions from the Canadian Reader’s Digest Word Power (RDWP) (Jarmasz and Szpakowicz, 2003), which unlike the previous two tests, allows the target and options to be multi-word phrases. 4.2 Word Association Word association tests measure the semantic relatedness of two words by comparing word space similarity with human judgements. Frequently, these tests measure synony</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes, (25):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-Based Construction of Semantic Space Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-Based Construction of Semantic Space Models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Ted Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<pages>pages</pages>
<contexts>
<context position="6774" citStr="Purandare and Pedersen, 2004" startWordPosition="1010" endWordPosition="1013">nitial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space algorithm is designed to run as a stand alone program and also to be used as a library class. 3.1 Reference Algorithms The package provides reference implementati</context>
<context position="11408" citStr="Purandare and Pedersen, 2004" startWordPosition="1700" endWordPosition="1704">hes. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al., 2004). To facilitate exploration of the similarity function parameter space, the S-Space Package provides support for multiple similarity functions: cosine similarity, Euclidean distance, KL divergence, Jaccard Index, Pearson product-moment correlation, Spearman’s rank correlation, and Lin Similarity (Dekang, 1998) 3.5 Clustering Clustering serves as a tool for building and refining word spaces. WSI algorithms, e.g. (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. The S-Space Package provides bindings for using the CLUTO clustering package3. In addition, the package provides Java implementations of Hierarchical Agglomerative Clustering, Spectral Clustering (Kannan et al., 2004), and the Gap Statistic (Tibshirani et al., 2000). 4 Benchmarks Word space benchmarks assess the semantic content of the space through analyzing the geometric properties of the space itself. Currently used benchmarks assess the semantics by inspecting the representational similarity of word pairs. Two types</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 41– 48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L T Rohde</author>
<author>Laura M Gonnerman</author>
<author>David C Plaut</author>
</authors>
<title>An improved model of semantic similarity based on lexical co-occurrence. Cognitive Science.</title>
<date>2009</date>
<tech>sumitted.</tech>
<contexts>
<context position="6471" citStr="Rohde et al., 2009" startWordPosition="967" endWordPosition="970">occurrences, but more advanced models may perform a statistical analysis to select only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve t</context>
<context position="9754" citStr="Rohde et al., 2009" startWordPosition="1455" endWordPosition="1458">s, vectors, and specialized data structures such as multi-maps and tries. Implementations are modeled after the java.util library and offer concurrent implementations when multi-threading is required. In addition, the libraries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. The package also provides support for parsing different corpora formats, such as XML or email threads. 3.3 Global Operation Utilities Many algorithms incorporate dimensionality reduction to smooth their feature data, e.g. (Landauer and Dumais, 1997; Rohde et al., 2009), or to improve efficiency, e.g. (Sahlgren et al., 2008; Jones et al., 2006). The S-Space Package supports two common techniques: the Singular Value Decomposition (SVD) and randomized projections. All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer-based projections (K</context>
<context position="13086" citStr="Rohde et al. (2009)" startWordPosition="1961" endWordPosition="1964">.42 93.02 0.572 0.478 0.388 HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318 HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042 ISA TASA 41.33 18.75 33.72 0.245 0.150 0.286 LSA TASA 56.00a 50.00 45.83 0.652 0.519 0.349 LSA Wiki 60.76 54.17 59.20 0.681 0.614 0.206 P&amp;P TASA 34.67 20.83 31.39 0.088 -0.036 0.216 RI TASA 42.67 27.08 34.88 0.224 0.201 0.211 RI Wiki 68.35 31.25 40.80 0.226 0.315 0.090 RI + Perm.b TASA 52.00 33.33 31.39 0.137 0.260 0.268 RRI TASA 36.00 22.92 34.88 0.088 0.138 0.109 VSM TASA 61.33 52.08 84.88 0.496 0.396 0.200 a Landauer et al. (1997) report a score of 64.4 for this test, while Rohde et al. (2009) report a score of 53.4. b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al., 2008) Table 2: A comparison of the implemented algorithms on common evaluation benchmarks 4.1 Word Choice Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. Word space models solve these tests by selecting the option whose representation is most similar. Three word choice benchmarks that measure synonymy are supported. The first test is the widely-reported Test of English as a Foreign Language (TOEFL) sy</context>
</contexts>
<marker>Rohde, Gonnerman, Plaut, 2009</marker>
<rawString>Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. 2009. An improved model of semantic similarity based on lexical co-occurrence. Cognitive Science. sumitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<pages>8--627</pages>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
<author>A Holst</author>
<author>P Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08).</booktitle>
<contexts>
<context position="6532" citStr="Sahlgren et al., 2008" startWordPosition="975" endWordPosition="978">tical analysis to select only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word </context>
<context position="9809" citStr="Sahlgren et al., 2008" startWordPosition="1464" endWordPosition="1467">ulti-maps and tries. Implementations are modeled after the java.util library and offer concurrent implementations when multi-threading is required. In addition, the libraries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. The package also provides support for parsing different corpora formats, such as XML or email threads. 3.3 Global Operation Utilities Many algorithms incorporate dimensionality reduction to smooth their feature data, e.g. (Landauer and Dumais, 1997; Rohde et al., 2009), or to improve efficiency, e.g. (Sahlgren et al., 2008; Jones et al., 2006). The S-Space Package supports two common techniques: the Singular Value Decomposition (SVD) and randomized projections. All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer-based projections (Kanerva et al., 2000) and Gaussian-based (Jones et al., </context>
<context position="13219" citStr="Sahlgren et al., 2008" startWordPosition="1984" endWordPosition="1987">33 18.75 33.72 0.245 0.150 0.286 LSA TASA 56.00a 50.00 45.83 0.652 0.519 0.349 LSA Wiki 60.76 54.17 59.20 0.681 0.614 0.206 P&amp;P TASA 34.67 20.83 31.39 0.088 -0.036 0.216 RI TASA 42.67 27.08 34.88 0.224 0.201 0.211 RI Wiki 68.35 31.25 40.80 0.226 0.315 0.090 RI + Perm.b TASA 52.00 33.33 31.39 0.137 0.260 0.268 RRI TASA 36.00 22.92 34.88 0.088 0.138 0.109 VSM TASA 61.33 52.08 84.88 0.496 0.396 0.200 a Landauer et al. (1997) report a score of 64.4 for this test, while Rohde et al. (2009) report a score of 53.4. b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al., 2008) Table 2: A comparison of the implemented algorithms on common evaluation benchmarks 4.1 Word Choice Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. Word space models solve these tests by selecting the option whose representation is most similar. Three word choice benchmarks that measure synonymy are supported. The first test is the widely-reported Test of English as a Foreign Language (TOEFL) synonym test from (Landauer et al., 1998), which consists of 80 multiple-choice questions with four options. The second test comes from</context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permutations as a means to encode order in word space. In Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>24--513</pages>
<contexts>
<context position="10620" citStr="Salton and Buckley, 1988" startWordPosition="1584" endWordPosition="1587">eamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. The library supports both integer-based projections (Kanerva et al., 2000) and Gaussian-based (Jones et al., 2006). The package supports common matrix transformations that have been applied to word spaces: point wise mutual information (Dekang, 1998), term frequency-inverse document frequency (Salton and Buckley, 1988), and log entropy (Landauer and Dumais, 1997). 3.4 Measurements The choice of similarity function for the vector space is the least standardized across approaches. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al., 2004). To facilitate exploration of the similarity function parameter space, the S-Space Package provides support for multiple similarity functions: cosine similarity, Euclidean distance, KL divergence, Jaccard Index, Pearson product-moment correlation, Spearman</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing &amp; Management, 24:513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="6394" citStr="Salton et al., 1975" startWordPosition="955" endWordPosition="958">h tokens in the context will count as features. Features are commonly word co-occurrences, but more advanced models may perform a statistical analysis to select only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al., 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al., 2009) Approximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space fram</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
<author>Guenther Walther</author>
<author>Trevor Hastie</author>
</authors>
<title>Estimating the number of clusters in a dataset via the gap statistic.</title>
<date>2000</date>
<journal>Journal Royal Statistics Society B,</journal>
<pages>63--411</pages>
<contexts>
<context position="11749" citStr="Tibshirani et al., 2000" startWordPosition="1754" endWordPosition="1757"> Euclidean distance, KL divergence, Jaccard Index, Pearson product-moment correlation, Spearman’s rank correlation, and Lin Similarity (Dekang, 1998) 3.5 Clustering Clustering serves as a tool for building and refining word spaces. WSI algorithms, e.g. (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. The S-Space Package provides bindings for using the CLUTO clustering package3. In addition, the package provides Java implementations of Hierarchical Agglomerative Clustering, Spectral Clustering (Kannan et al., 2004), and the Gap Statistic (Tibshirani et al., 2000). 4 Benchmarks Word space benchmarks assess the semantic content of the space through analyzing the geometric properties of the space itself. Currently used benchmarks assess the semantics by inspecting the representational similarity of word pairs. Two types of benchmarks are commonly used: word choice tests and association tests. The S-Space Package supports six tests, and has an easily extensible model for adding new tests. 1http://bender.unibe.ch/svn/codemap/Archive/svdlibj/ 2http://tedlab.mit.edu/˜dr/SVDLIBC/ 3http://glaros.dtc.umn.edu/gkhome/views/cluto 32 Algorithm Corpus Word Choice Wo</context>
</contexts>
<marker>Tibshirani, Walther, Hastie, 2000</marker>
<rawString>Robert Tibshirani, Guenther Walther, and Trevor Hastie. 2000. Estimating the number of clusters in a dataset via the gap statistic. Journal Royal Statistics Society B, 63:411–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1999" citStr="Turney and Pantel, 2010" startWordPosition="294" endWordPosition="297">nguage (Burgess and Lund, 1997), have been shown to approximate human judgements of word similarity in addition to providing computational models for other psychological and linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). The parallel development of word space models in different fields has often resulted in duplicated work. The pace of development presents a need for a reliable method for accurate comparisons between new and existing approaches. Furthermore, given the frequent similarity of approaches, we argue that the research community would greatly benefit from a common library and evaluation utilities for word spaces. Therefore, we introduce the S-Space Package, an open source framework with four main contributions: 1. reference implementations of frequently cited algorithms 2. a comprehensive, highly c</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal ofArtificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<contexts>
<context position="13924" citStr="Turney, 2001" startWordPosition="2103" endWordPosition="2104">d Choice Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. Word space models solve these tests by selecting the option whose representation is most similar. Three word choice benchmarks that measure synonymy are supported. The first test is the widely-reported Test of English as a Foreign Language (TOEFL) synonym test from (Landauer et al., 1998), which consists of 80 multiple-choice questions with four options. The second test comes from the English as a Second Language (ESL) exam and consists of 50 question with four choices (Turney, 2001). The third consists of 200 questions from the Canadian Reader’s Digest Word Power (RDWP) (Jarmasz and Szpakowicz, 2003), which unlike the previous two tests, allows the target and options to be multi-word phrases. 4.2 Word Association Word association tests measure the semantic relatedness of two words by comparing word space similarity with human judgements. Frequently, these tests measure synonymy; however, other types of word relations such as antonymy (“hot” and “cold”) or functional relatedness (“doctor” and “hospital”) are also possible. The S-Space Package supports three association te</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarty</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics COLING’04,</booktitle>
<pages>1015--1021</pages>
<contexts>
<context position="10963" citStr="Weeds et al., 2004" startWordPosition="1637" endWordPosition="1640">ections (Kanerva et al., 2000) and Gaussian-based (Jones et al., 2006). The package supports common matrix transformations that have been applied to word spaces: point wise mutual information (Dekang, 1998), term frequency-inverse document frequency (Salton and Buckley, 1988), and log entropy (Landauer and Dumais, 1997). 3.4 Measurements The choice of similarity function for the vector space is the least standardized across approaches. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al., 2004). To facilitate exploration of the similarity function parameter space, the S-Space Package provides support for multiple similarity functions: cosine similarity, Euclidean distance, KL divergence, Jaccard Index, Pearson product-moment correlation, Spearman’s rank correlation, and Lin Similarity (Dekang, 1998) 3.5 Clustering Clustering serves as a tool for building and refining word spaces. WSI algorithms, e.g. (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. The S-Space Package provides bindings for using the CLUTO clustering package3. I</context>
</contexts>
<marker>Weeds, Weir, McCarty, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarty. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics COLING’04, pages 1015–1021.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Kathleen Ferraro</author>
</authors>
<title>Semantic vectors: a scalable open source package and online technology management application.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08).</booktitle>
<contexts>
<context position="3066" citStr="Widdows and Ferraro, 2008" startWordPosition="456" endWordPosition="459">-Space Package, an open source framework with four main contributions: 1. reference implementations of frequently cited algorithms 2. a comprehensive, highly concurrent library of tools for building new models 3. an evaluation framework for testing models on standard benchmarks, e.g. the TOEFL Synonym Test (Landauer et al., 1998) 4. a standardized interface for interacting with all word space models, which facilitates word space based applications. The package is written in Java and defines a standardized Java interface for word space algorithms. While other word space frameworks exist, e.g. (Widdows and Ferraro, 2008), the focus of this framework is to ease the development of new algorithms and the comparison against existing models. Compared to existing frameworks, the S-Space Package supports a much wider variety of algorithms and provides significantly more reusable developer utilities for word spaces, such as tokenizing and filtering, sparse vectors and matrices, specialized data structures, and seamless integration with external programs for dimensionality reduction and clustering. We hope that the release of this framework will greatly facilitate other researchers in their efforts to develop and vali</context>
</contexts>
<marker>Widdows, Ferraro, 2008</marker>
<rawString>Dominic Widdows and Kathleen Ferraro. 2008. Semantic vectors: a scalable open source package and online technology management application. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>