<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.98764">
Phrase Chunking using Entropy Guided Transformation Learning
</title>
<author confidence="0.758422">
Ruy L. Milidi´u
</author>
<affiliation confidence="0.62092">
Departamento de Inform´atica
</affiliation>
<note confidence="0.5921342">
PUC-Rio
Rio de Janeiro, Brazil
Cicero Nogueira dos Santos
Departamento de Inform´atica
PUC-Rio
</note>
<email confidence="0.985201">
nogueira@inf.puc-rio.br
</email>
<note confidence="0.994354">
Julio C. Duarte
Centro Tecnol´ogico do Ex´ercito
Rio de Janeiro, Brazil
</note>
<email confidence="0.978385">
jduarte@ctex.eb.br
milidiu@inf.puc-rio.br
</email>
<sectionHeader confidence="0.995394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99824835">
Entropy Guided Transformation Learning
(ETL) is a new machine learning strategy
that combines the advantages of decision
trees (DT) and Transformation Based Learn-
ing (TBL). In this work, we apply the ETL
framework to four phrase chunking tasks: Por-
tuguese noun phrase chunking, English base
noun phrase chunking, English text chunking
and Hindi text chunking. In all four tasks,
ETL shows better results than Decision Trees
and also than TBL with hand-crafted tem-
plates. ETL provides a new training strat-
egy that accelerates transformation learning.
For the English text chunking task this corre-
sponds to a factor of five speedup. For Por-
tuguese noun phrase chunking, ETL shows the
best reported results for the task. For the other
three linguistic tasks, ETL shows state-of-the-
art competitive results and maintains the ad-
vantages of using a rule based system.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865288888889">
Phrase Chunking is a Natural Language Processing
(NLP) task that consists in dividing a text into syn-
tactically correlated parts of words. Theses phrases
are non-overlapping, i.e., a word can only be a mem-
ber of one chunk (Sang and Buchholz, 2000). It pro-
vides a key feature that helps on more elaborated
NLP tasks such as parsing and information extrac-
tion.
Since the last decade, many high-performance
chunking systems were proposed, such as, SVM-
based (Kudo and Matsumoto, 2001; Wu et al.,
2006), Winnow (Zhang et al., 2002), voted-
perceptrons (Carreras and M`arquez, 2003),
Transformation-Based Learning (TBL) (Ramshaw
and Marcus, 1999; Megyesi, 2002) and Hidden
Markov Model (HMM) (Molina and Pla, 2002),
Memory-based (Sang, 2002). State-of-the-art
systems for English base noun phrase chunking and
text chunking are based in statistical techniques
(Kudo and Matsumoto, 2001; Wu et al., 2006;
Zhang et al., 2002).
TBL is one of the most accurate rule-based tech-
niques for phrase chunking tasks (Ramshaw and
Marcus, 1999; Ngai and Florian, 2001; Megyesi,
2002). On the other hand, TBL rules must follow
patterns, called templates, that are meant to cap-
ture the relevant feature combinations. The process
of generating good templates is highly expensive.
It strongly depends on the problem expert skills to
build them. Even when a template set is available
for a given task, it may not be effective when we
change from a language to another (dos Santos and
Oliveira, 2005).
In this work, we apply Entropy Guided Transfor-
mation Learning (ETL) for phrase chunking. ETL is
a new machine learning strategy that combines the
advantages of Decision Trees (DT) and TBL (dos
Santos and Milidi´u, 2007a). The ETL key idea is to
use decision tree induction to obtain feature com-
binations (templates) and then use the TBL algo-
rithm to generate transformation rules. ETL pro-
duces transformation rules that are more effective
than decision trees and also eliminates the need of
a problem domain expert to build TBL templates.
We evaluate the performance of ETL over four
</bodyText>
<page confidence="0.978268">
647
</page>
<note confidence="0.714668">
Proceedings of ACL-08: HLT, pages 647–655,
</note>
<page confidence="0.538166">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9999486">
phrase chunking tasks: (1) English Base Noun
Phrase (NP) chunking; (2) Portuguese NP chunk-
ing; (3) English Text Chunking; and (4) Hindi Text
Chunking. Base NP chunking consists in recogniz-
ing non-overlapping text segments that contain NPs.
Text chunking consists in dividing a text into syn-
tactically correlated parts of words. For these four
tasks, ETL shows state-of-the-art competitive results
and maintains the advantages of using a rule based
system.
The remainder of the paper is organized as fol-
lows. In section 2, the ETL strategy is described.
In section 3, the experimental design and the corre-
sponding results are reported. Finally, in section 4,
we present our concluding remarks.
</bodyText>
<sectionHeader confidence="0.740708" genericHeader="method">
2 Entropy Guided Transformation
Learning
</sectionHeader>
<bodyText confidence="0.964249222222222">
Entropy Guided Transformation Learning (ETL)
is a new machine learning strategy that com-
bines the advantages of Decision Trees (DT) and
Transformation-Based Learning (TBL) (dos Santos
and Milidi´u, 2007a). The key idea of ETL is to use
decision tree induction to obtain templates. Next,
the TBL strategy is used to generate transformation
rules. The proposed method is illustrated in the Fig.
1.
</bodyText>
<figureCaption confidence="0.9940045">
Figure 1: ETL - Entropy Guided Transformation Learn-
ing.
</figureCaption>
<bodyText confidence="0.999910153846154">
A combination of DT and TBL is presented in
(Corston-Oliver and Gamon, 2003). The main dif-
ference between Corston-Oliver &amp; Gamon work and
the ETL strategy is that they extract candidate rules
directly from the DT, and then use the TBL strategy
to select the appropriate rules. Another difference is
that they use a binary DT, whereas ETL uses a DT
that is not necessarily binary.
An evolutionary approach based on Genetic Al-
gorithms (GA) to automatically generate TBL tem-
plates is presented in (Milidi´u et al., 2007). Us-
ing a simple genetic coding, the generated template
sets have efficacy near to the handcrafted templates
for the tasks: English Base Noun Phrase Identifica-
tion, Text Chunking and Portuguese Named Entities
Recognition. The main drawback of this strategy is
that the GA step is computationally expensive. If we
need to consider a large context window or a large
number of features, it can be infeasible.
The remainder of this section is organized as fol-
lows. In section 2.1, we describe the DT learning
algorithm. In section 2.2, the TBL algorithm is de-
picted. In section 2.3, we depict the process of ob-
taining templates from a decision tree decomposi-
tion. Finally, in section 2.4, we present a template
evolution scheme that speeds up the TBL step.
</bodyText>
<subsectionHeader confidence="0.983203">
2.1 Decision Trees
</subsectionHeader>
<bodyText confidence="0.9989735625">
Decision tree learning is one of the most widely used
machine learning algorithms. It performs a parti-
tioning of the training set using principles of Infor-
mation Theory. The learning algorithm executes a
general to specific search of a feature space. The
most informative feature is added to a tree structure
at each step of the search. Information Gain Ratio,
which is based on the data Entropy, is normally used
as the informativeness measure. The objective is to
construct a tree, using a minimal set of features, that
efficiently partitions the training set into classes of
observations. After the tree is grown, a pruning step
is carried out in order to avoid overfitting.
One of the most used algorithms for induction of
a DT is the C4.5 (Quinlan, 1993). We use Quinlan’s
C4.5 system throughout this work.
</bodyText>
<subsectionHeader confidence="0.991042">
2.2 Transformation-Based Learning
</subsectionHeader>
<bodyText confidence="0.999486571428571">
Transformation Based error-driven Learning (TBL)
is a successful machine learning algorithm intro-
duced by Eric Brill (Brill, 1995). It has since been
used for several Natural Language Processing tasks,
such as part-of-speech (POS) tagging (Brill, 1995),
English text chunking (Ramshaw and Marcus, 1999;
dos Santos and Milidi´u, 2007b), spelling correc-
</bodyText>
<page confidence="0.997594">
648
</page>
<bodyText confidence="0.998894909090909">
tion (Mangu and Brill, 1997), Portuguese appos-
itive extraction (Freitas et al., 2006), Portuguese
named entity extraction (Milidi´u et al., 2006) and
Portuguese noun-phrase chunking (dos Santos and
Oliveira, 2005), achieving state-of-the-art perfor-
mance in many of them.
TBL uses an error correcting strategy. Its main
scheme is to generate an ordered list of rules that
correct classification mistakes in the training set,
which have been produced by an initial classifier.
The requirements of the algorithm are:
</bodyText>
<listItem confidence="0.9732155625">
• two instances of the training set, one that has
been correctly labeled, and another that re-
mains unlabeled;
• an initial classifier, the baseline system, which
classifies the unlabeled training set by trying
to apply the correct class for each sample. In
general, the baseline system is based on simple
statistics of the labeled training set; and
• a set of rule templates, which are meant to
capture the relevant feature combinations that
would determine the sample’s classification.
Concrete rules are acquired by instantiation of
this predefined set of rule templates.
• a threshold value, that is used as a stopping cri-
teria for the algorithm and is needed to avoid
overfitting to the training data.
</listItem>
<bodyText confidence="0.99813275">
The learning method is a mistake-driven greedy
procedure that iteratively acquires a set of transfor-
mation rules. The TBL algorithm can be depicted as
follows:
</bodyText>
<listItem confidence="0.99990735">
1. Starts applying the baseline system, in order to
guess an initial classification for the unlabeled
version of the training set;
2. Compares the resulting classification with the
correct one and, whenever a classification error
is found, all the rules that can correct it are gen-
erated by instantiating the templates. This tem-
plate instantiation is done by capturing some
contextual data of the sample being corrected.
Usually, a new rule will correct some errors, but
will also generate some other errors by chang-
ing correctly classified samples;
3. Computes the rules’ scores (errors repaired - er-
rors created). If there is not a rule with a score
above an arbitrary threshold, the learning pro-
cess is stopped;
4. Selects the best scoring rule, stores it in the set
of learned rules and applies it to the training
set;
5. Returns to step 2.
</listItem>
<bodyText confidence="0.975932">
When classifying a new sample item, the resulting
sequence of rules is applied according to its genera-
tion order.
</bodyText>
<subsectionHeader confidence="0.995535">
2.3 DT Template Extraction
</subsectionHeader>
<bodyText confidence="0.992158484848485">
There are many ways to extract feature combinations
from decision trees. In an path from the root to the
leaves, more informative features appear first. Since
we want to generate the most promising templates
only, we just combine the more informative ones.
The process we use to extract templates from a
DT includes a depth-first traversal of the DT. For
each visited node, we create a new template that
combines its parent node template with the feature
used to split the data at that node. This is a very
simple decomposition scheme. Nevertheless, it re-
sults into extremely effective templates. We also use
pruned trees in all experiments shown in section 3.
Fig. 2 shows an excerpt of a DT generated for the
English text chunking task1. Using the described
method to extract templates from the DT shown in
Fig. 2, we obtain the template set listed in the left
side of Table 1. In order to generate more feature
combinations, without largely increasing the num-
ber of templates, we extend the template set by in-
cluding templates that do not have the root node fea-
ture. The extended template set for the DT shown in
Fig. 2 is listed in the right side of the Table 1.
We have also tried some other strategies that ex-
tract a larger number of templates from a DT. How-
ever, the efficacy of the learned rules is quite similar
to the one generated by the first method. This rein-
forces the conjecture that a DT generates informa-
tive feature combinations.
1CK[0] = Chunk tag of the current word (initial classifier
result); CK[–1] = previous word Chunk tag; CK[1] = next word
Chunk tag; POS[0] = current word POS tag; WRD[0] = current
word.
</bodyText>
<page confidence="0.999449">
649
</page>
<tableCaption confidence="0.999713">
Table 1: Text chunking DT Template set example
</tableCaption>
<figure confidence="0.885236571428571">
Template set Extended template set
CK[0] CK[0] CK[1]
CK[0] CK[1] CK[0] CK[1] CK[1] WRD[0]
CK[0] CK[1] WRD[0] CK[0] CK[1] WRD[0] CK[1] WRD[0] CK[–1]
CK[0] CK[1] WRD[0] CK[–1] CK[0] CK[1] WRD[0] CK[–1] CK[1] POS[0]
CK[0] CK[1] POS[0] CK[0] CK[1] POS[0] CK[–1]
CK[0] CK[–1] CK[0] CK[–1]
</figure>
<figureCaption confidence="0.998066">
Figure 2: Text chunking decision tree excerpt.
</figureCaption>
<subsectionHeader confidence="0.98959">
2.4 Template Evolution Speedup
</subsectionHeader>
<bodyText confidence="0.996811">
TBL training time is highly sensitive to the number
and complexity of the applied templates. In (Cur-
ran and Wong, 2000), it is argued that we can bet-
ter tune the training time vs. templates complex-
ity trade-off by using an evolutionary template ap-
proach. The main idea is to apply only a small num-
ber of templates that evolve throughout the training.
When training starts, templates are short, consisting
of few feature combinations. As training proceeds,
templates evolve to more complex ones that contain
more feature combinations. In this way, only a few
templates are considered at any point in time. Nev-
ertheless, the descriptive power is not significantly
reduced.
The template evolution approach can be easily im-
plemented by using template sets extracted from a
DT. We implement this idea by successively training
TBL models. Each model uses only the templates
that contain feature combinations up to a given tree
level. For instance, using the tree shown in Fig. 2,
we have the following template sets for the three first
training rounds2:
</bodyText>
<listItem confidence="0.4986276">
1. CK[0] CK[1];
CK[0] CK[–1]
2. CK[0] CK[1] WRD[0];
CK[0] CK[1] POS[0]
3. CK[0] CK[1] WRD[0] CK[–1]
</listItem>
<bodyText confidence="0.999812666666667">
Using the template evolution strategy, the training
time is decreased by a factor of five for the English
text chunking task. This is a remarkable reduction,
since we use an implementation of the fastTBL algo-
rithm (Ngai and Florian, 2001) that is already a very
fast TBL version. The efficacy of the rules gener-
ated by the sequential training is quite similar to the
one obtained by training with all the templates at the
same time.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999918769230769">
This section presents the experimental setup and re-
sults of the application of ETL to four phrase chunk-
ing tasks. ETL results are compared with the results
of DT and TBL using hand-crafted templates.
In the TBL step, for each one of the four chunking
tasks, the initial classifier assigns to each word the
chunk tag that was most frequently associated with
the part-of-speech of that word in the training set.
The DT learning works as a feature selector and
is not affected by irrelevant features. We have tried
several context window sizes when training the clas-
sifiers. Some of the tested window sizes would be
very hard to be explored by a domain expert using
</bodyText>
<footnote confidence="0.962406">
2We ignore templates composed of only one feature test.
</footnote>
<page confidence="0.994781">
650
</page>
<bodyText confidence="0.999918517241379">
TBL alone. The corresponding huge number of pos-
sible templates would be very difficult to be man-
aged by a template designer.
For the four tasks, the following experimental
setup provided us our best results.
ETL in the ETL learning, we use the features word,
POS and chunk. In order to overcome the spar-
sity problem, we only use the 200 most fre-
quent words to induce the DT. In the DT learn-
ing, the chunk tag of the word is the one applied
by the initial classifier. On the other hand, the
chunk tag of neighbor words are the true ones.
We report results for ETL trained with all the
templates at the same time as well as using tem-
plate evolution.
TBL the results for the TBL approach refers to TBL
trained with the set of templates proposed in
(Ramshaw and Marcus, 1999).
DT the best result for the DT classifier is shown.
The features word, POS and chunk are used to
generate the DT classifier. The chunk tag of a
word and its neighbors are the ones guessed by
the initial classifier. Using only the 100 most
frequent words gives our best results.
In all experiments, the term WS=X subscript
means that a window of size X was used for the
given model. For instance, ETLWS=3 corresponds
to ETL trained with window of size three, that is,
the current token, the previous and the next one.
</bodyText>
<subsectionHeader confidence="0.998093">
3.1 Portuguese noun phrase chunking
</subsectionHeader>
<bodyText confidence="0.999727928571429">
For this task, we use the SNR-CLIC corpus de-
scribed in (Freitas et al., 2005). This corpus is
tagged with both POS and NP tags. The NP tags
are: I, for in NP; O, for out of NP; and B for the
leftmost word of an NP beginning immediately af-
ter another NP. We divided the corpus into 3514-
sentence (83346 tokens) training set and a 878-
sentence (20798 tokens) test set.
In Table 2 we compare the results3 of ETL with
DT and TBL. We can see that ETL, even with a
small window size, produces better results than DT
and TBL. The Fβ=1 of the ETLWS=7 classifier is
1.8% higher than the one of TBL and 2.6% higher
than the one of the DT classifier.
</bodyText>
<equation confidence="0.394672">
3#T = Number of templates.
</equation>
<tableCaption confidence="0.995277">
Table 2: Portuguese noun phrase chunking.
</tableCaption>
<table confidence="0.997592555555556">
Acc. Prec. Rec. Fβ=1 # T
(%) (%) (%) (%)
BLS 96.57 62.69 74.45 68.06 –
DTWS=13 97.35 83.96 87.27 85.58 –
TBL 97.45 85.48 87.32 86.39 100
ETLWS=3 97.61 86.12 87.24 86.67 21
ETLWS=5 97.68 86.85 87.49 87.17 35
ETLWS=7 97.82 88.15 88.20 88.18 34
ETLWS=9 97.82 88.02 88.34 88.18 40
</table>
<bodyText confidence="0.990406833333333">
Table 3 shows the results4 of ETL using template
evolution. As we can see, for the task of Portuguese
noun phrase chunking, the template evolution strat-
egy reduces the average training time in approxi-
mately 35%. On the other hand, there is a decrease
of the classifier efficacy in some cases.
</bodyText>
<tableCaption confidence="0.952555">
Table 3: Portuguese noun phrase chunking using ETL
with template evolution.
</tableCaption>
<table confidence="0.9789905">
Acc. Prec. Rec. Fβ=1 TTR
(%) (%) (%) (%) (%)
ETLWS=3 97.61 86.22 87.27 86.74 20.7
ETLWS=5 97.56 86.39 87.10 86.74 38.2
ETLWS=7 97.69 87.35 87.89 87.62 37.0
ETLWS=9 97.76 87.55 88.14 87.85 41.9
</table>
<bodyText confidence="0.999748058823529">
In (dos Santos and Oliveira, 2005), a special set
of six templates is shown. These templates are
designed to reduce classification errors of prepo-
sition within the task of Portuguese noun phrase
chunking. These templates use very specific do-
main knowledge and are difficult to DT and TBL
to extract. Table 4 shows the results of an experi-
ment where we include these six templates into the
Ramshaw&amp;Marcus template set and also into the
template sets generated by ETL. Again, ETL pro-
duces better results than TBL.
Table 5 shows the results of using a committee
composed by the three best ETL classifiers. The
classification is done by selecting the most popular
tag among all the three committee members. The
achieved Fβ=1, 89.14% is the best one ever reported
for the SNR-CLIC corpus.
</bodyText>
<footnote confidence="0.795018">
4TTR = Training time reduction.
</footnote>
<page confidence="0.998684">
651
</page>
<tableCaption confidence="0.976556">
Table 4: Portuguese noun phrase chunking using six ad-
ditional hand-crafted templates.
</tableCaption>
<table confidence="0.99988675">
Acc. Prec. Rec. Fβ=1 # T
(%) (%) (%) (%)
BLS 96.57 62.69 74.45 68.06 –
TBL 97.60 86.79 88.12 87.45 106
ETLWS=3 97.73 86.95 88.40 87.67 27
ETLWS=5 97.87 88.35 89.02 88.68 41
ETLWS=7 97.91 88.12 89.22 88.67 40
ETLWS=9 97.93 88.53 89.11 88.82 46
</table>
<tableCaption confidence="0.900739">
Table 6: Base NP chunking.
</tableCaption>
<table confidence="0.999914">
Acc. Prec. Rec. Fβ=1 # T
(%) (%) (%) (%)
BLS 94.48 78.20 81.87 79.99 –
DTWS=11 97.03 89.92 91.16 90.53 –
TBL 97.42 91.68 92.26 91.97 100
ETLWS=3 97.54 91.93 92.78 92.35 68
ETLWS=5 97.55 92.43 92.77 92.60 85
ETLWS=7 97.52 92.49 92.70 92.59 106
ETLWS=9 97.63 92.62 93.05 92.84 122
</table>
<tableCaption confidence="0.998214">
Table 5: Committee with the classifiers ETLWS=5,
</tableCaption>
<table confidence="0.987104">
ETLWS=7 and ETLWS=9, shown in Table 4.
Results (%)
Accuracy 97.97
Precision 88.62
Recall 89.67
Fβ=1 89.14
</table>
<subsectionHeader confidence="0.990488">
3.2 English base noun phrase chunking
</subsectionHeader>
<bodyText confidence="0.999173">
The data used in the base NP chunking experiments
is the one by Ramshaw &amp; Marcus (Ramshaw and
Marcus, 1999). This corpus contains sections 15-
18 and section 20 of the Penn Treebank, and is pre-
divided into 8936-sentence (211727 tokens) training
set and a 2012-sentence (47377 tokens) test. This
corpus is tagged with both POS and chunk tags.
Table 6 compares the results of ETL with DT
and TBL for the base NP chunking. We can see
that ETL, even using a small window size, produces
better results than DT and TBL. The Fβ=1 of the
ETLWS=9 classifier is 0.87% higher than the one of
TBL and 2.31% higher than the one of the DT clas-
sifier.
Table 7 shows the results of ETL using template
evolution. The template evolution strategy reduces
the average training time in approximately 62%.
Differently from the Portuguese NP chunking, we
observe an increase of the classifier efficacy in al-
most all the cases.
Table 8 shows the results of using a committee
composed by the eight ETL classifiers reported in
this section. Table 8 also shows the results for a
committee of SVM models presented in (Kudo and
Matsumoto, 2001). SVM’s results are the state-of-
</bodyText>
<tableCaption confidence="0.983435">
Table 7: Base NP chunking using ETL with template evo-
lution.
</tableCaption>
<table confidence="0.9983605">
Acc. Prec. Rec. Fβ=1 TTR
(%) (%) (%) (%) (%)
ETLWS=3 97.58 92.07 92.74 92.41 53.9
ETLWS=5 97.63 92.66 93.16 92.91 57.9
ETLWS=7 97.61 92.56 93.04 92.80 65.1
ETLWS=9 97.59 92.50 93.01 92.76 69.4
</table>
<bodyText confidence="0.992562">
the-art for the Base NP chunking task. On the other
hand, using a committee of ETL classifiers, we pro-
duce very competitive results and maintain the ad-
vantages of using a rule based system.
</bodyText>
<tableCaption confidence="0.9826675">
Table 8: Base NP chunking using a committee of eight
ETL classifiers.
</tableCaption>
<table confidence="0.99491725">
Accuracy Precision Recall Fβ=1
(%) (%) (%) (%)
ETL 97.72 92.87 93.34 93.11
SVM – 94.15 94.29 94.22
</table>
<subsectionHeader confidence="0.991503">
3.3 English text chunking
</subsectionHeader>
<bodyText confidence="0.99964775">
The data used in the English text chunking exper-
iments is the CoNLL-2000 corpus, which is de-
scribed in (Sang and Buchholz, 2000). It is com-
posed by the same texts as the Ramshaw &amp; Marcus
(Ramshaw and Marcus, 1999) corpus.
Table 9 compares the results of ETL with DTs and
TBL for English text chunking. ETL, even using a
small window size, produces better results than DTs
and TBL. The Fβ=1 of the ETLWS=3 classifier is
0.28% higher than the one of TBL and 2.17% higher
than the one of the DT classifier. It is an interesting
linguistic finding that the use of a window of size 3
</bodyText>
<page confidence="0.996894">
652
</page>
<bodyText confidence="0.9924885">
(the current token, the previous token and the next
token) provides the current best results for this task.
type, of using a committee composed by the eight
ETL classifiers reported in this section.
</bodyText>
<tableCaption confidence="0.996208">
Table 9: English text Chunking.
</tableCaption>
<table confidence="0.999649888888889">
Acc. Prec. Rec. F0=1 # T
(%) (%) (%) (%)
BLS 77.29 72.58 82.14 77.07 –
DTWS=9 94.29 89.55 91.00 90.27 –
TBL 95.12 92.05 92.28 92.16 100
ETLWS=3 95.24 92.32 92.56 92.44 105
ETLWS=5 95.12 92.19 92.27 92.23 167
ETLWS=7 95.13 92.24 92.32 92.28 183
ETLWS=9 95.07 92.10 92.27 92.19 205
</table>
<bodyText confidence="0.6806832">
Table 10 shows the results of ETL using template
evolution. The template evolution strategy reduces
the average training time by approximately 81%. On
the other hand, there is a small decrease of the clas-
sifier efficacy in all cases.
</bodyText>
<tableCaption confidence="0.982486">
Table 10: English text chunking using ETL with template
evolution.
</tableCaption>
<table confidence="0.998388333333333">
Acc. Prec. Rec. F0=1 TTR
(%) (%) (%) (%) (%)
ETLWS=3 95.21 92.14 92.53 92.34 77.2
ETLWS=5 94.98 91.84 92.25 92.04 80.8
ETLWS=7 95.03 91.89 92.28 92.09 83.0
ETLWS=9 95.01 91.87 92.21 92.04 84.5
</table>
<bodyText confidence="0.971054888888889">
Table 11 shows the results of using a committee
composed by the eight ETL classifiers reported in
this section. Table 11 also shows the results for a
SVM model presented in (Wu et al., 2006). SVM’s
results are the state-of-the-art for the Text chunking
task. On the other hand, using a committee of ETL
classifiers, we produce very competitive results and
maintain the advantages of using a rule based sys-
tem.
</bodyText>
<tableCaption confidence="0.935326">
Table 11: English text Chunking using a committee of
eight ETL classifiers.
</tableCaption>
<table confidence="0.999863">
Accuracy Precision Recall F0=1
(%) (%) (%) (%)
ETL 95.50 92.63 92.96 92.79
SVM – 94.12 94.13 94.12
</table>
<tableCaption confidence="0.990990333333333">
Table 12 shows the results, broken down by chunk
Table 12: English text chunking results, broken down by
chunk type, for the ETL committee.
</tableCaption>
<table confidence="0.999905769230769">
Precision Recall F0=1
(%) (%) (%)
ADJP 75.59 72.83 74.19
ADVP 82.02 79.56 80.77
CONJP 35.71 55.56 43.48
INTJ 00.00 00.00 00.00
LST 00.00 00.00 00.00
NP 92.90 93.08 92.99
PP 96.53 97.63 97.08
PRT 66.93 80.19 72.96
SBAR 86.50 85.05 85.77
VP 92.84 93.58 93.21
Overall 92.63 92.96 92.79
</table>
<subsectionHeader confidence="0.973589">
3.4 Hindi text chunking
</subsectionHeader>
<bodyText confidence="0.999825807692308">
The data used in the Hindi text chunking exper-
iments is the SPSAL-2007 corpus, which is de-
scribed in (Bharati and Mannem, 2007). This cor-
pus is pre-divided into a 20000-tokens training set, a
5000-tokens development set and a 5000-tokens test
set. This corpus is tagged with both POS and chunk
tags.
To fairly compare our approach with the ones
presented in the SPSAL-2007, the POS tags of the
test corpus were replaced by the ones predicted by
an ETL-based Hindi POS Tagger. The description
of our ETL pos tagger is beyond the scope of this
work. Since the amount of training data is very small
(20000 tokens), the accuracy of the ETL Hindi POS
tagger is low, 77.50% for the test set.
The results are reported in terms of chunking ac-
curacy, the same performance measure used in the
SPSAL-2007. Table 13 compares the results of ETL
with DT and TBL for Hindi text chunking. ETL pro-
duces better results than DT and achieves the same
performance of TBL using 60% less templates. We
believe that ETL performance is not as good as in
the other tasks mainly because of the small amount
of training data, which increases the sparsity prob-
lem.
We do not use template evolution for Hindi text
</bodyText>
<page confidence="0.998035">
653
</page>
<bodyText confidence="0.9980854">
chunking. Since the training corpus is very small,
the training time reduction is not significant.
three linguistic tasks, ETL shows competitive results
and maintains the advantages of using a rule based
system.
</bodyText>
<tableCaption confidence="0.992407">
Table 13: Hindi text Chunking.
</tableCaption>
<table confidence="0.99483">
Accuracy # Templates
(%)
BLS 70.05 –
DTWS=5 78.20 –
TBL 78.53 100
ETLWS=5 78.53 30
</table>
<tableCaption confidence="0.639802">
Table 14 compares the results of ETL with the two
</tableCaption>
<bodyText confidence="0.995472545454545">
best Hindi text chunkers at SPSAL-2007 (Bharati
and Mannem, 2007). The first one is a combination
of Hidden Markov Models (HMM) and Conditional
Random Fields (CRF) (PVS and Gali, 2007). The
second is based in Maximum Entropy Models (Max-
Ent) (Dandapat, 2007). ETL performs better than
MaxEnt and worst than HMM+CRF. It is important
to note that the accuracy of the POS tagger used by
(PVS and Gali, 2007) (78.66%) is better than ours
(77.50%). The POS tagging quality directly affects
the chunking accuracy.
</bodyText>
<tableCaption confidence="0.999272">
Table 14: Comparison with best systems of SPSAL-2007
</tableCaption>
<table confidence="0.706328">
Accuracy
(%)
HMM + CRF 80.97
ETLWS=5 78.53
MaxEnt 74.92
</table>
<sectionHeader confidence="0.997915" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999956615384615">
In this paper, we approach the phrase chunking
task using Entropy Guided Transformation Learning
(ETL). We carry out experiments with four phrase
chunking tasks: Portuguese noun phrase chunking,
English base noun phrase chunking, English text
chunking and Hindi text chunking. In all four tasks,
ETL shows better results than Decision Trees and
also than TBL with hand-crafted templates. ETL
provides a new training strategy that accelerates
transformation learning. For the English text chunk-
ing task this corresponds to a factor of five speedup.
For Portuguese noun phrase chunking, ETL shows
the best reported results for the task. For the other
</bodyText>
<sectionHeader confidence="0.988341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995184739130434">
Akshar Bharati and Prashanth R. Mannem. 2007. In-
troduction to shallow parsing contest on south asian
languages. In Proceedings of the IJCAI and the Work-
shop On Shallow Parsing for South Asian Languages
(SPSAL), pages 1–8.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Comput. Linguistics,
21(4):543–565.
Xavier Carreras and Llu´ıs M`arquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003, Borovets, Bulgaria.
Simon Corston-Oliver and Michael Gamon. 2003. Com-
bining decision trees and transformation-based learn-
ing to correct transferred linguistic representations. In
Proceedings of the Ninth Machine Tranlsation Sum-
mit, pages 55–62, New Orleans, USA. Association for
Machine Translation in the Americas.
J. R. Curran and R. K. Wong. 2000. Formalisation
of transformation-based learning. In Proceedings of
the Australian Computer Science Conference - ACSC,
pages 51–57, Canberra, Australia.
Sandipan Dandapat. 2007. Part of speech tagging and
chunking with maximum entropy model. In Proceed-
ings of the IJCAI and the Workshop On Shallow Pars-
ing for South Asian Languages (SPSAL), pages 29–32.
C´ıcero N. dos Santos and Ruy L. Milidi´u. 2007a. En-
tropy guided transformation learning. Technical Re-
port 29/07, Departamento de Informtica, PUC-Rio.
C´ıcero N. dos Santos and Ruy L. Milidi´u. 2007b. Prob-
abilistic classifications with tbl. In Proceedings of
Eighth International Conference on Intelligent Text
Processing and Computational Linguistics – CICLing,
pages 196–207, Mexico City, Mexico, February.
C´ıcero N. dos Santos and Claudia Oliveira. 2005. Con-
strained atomic term: Widening the reach of rule tem-
plates in transformation based learning. In EPIA,
pages 622–633.
M. C. Freitas, M. Garrao, C. Oliveira, C. N. dos Santos,
and M. Silveira. 2005. A anotac¸˜ao de um corpus para
o aprendizado supervisionado de um modelo de sn. In
Proceedings of the III TIL / XXV Congresso da SBC,
S˜ao Leopoldo - RS - Brasil.
M. C. Freitas, J. C. Duarte, C. N. dos Santos, R. L. Mi-
lidi´u, R. P. Renteria, and V. Quental. 2006. A ma-
chine learning approach to the identification of appos-
</reference>
<page confidence="0.990305">
654
</page>
<reference confidence="0.99983101754386">
itives. In Proceedings of Ibero-American AI Confer-
ence, Ribeir˜ao Preto, Brazil, October.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of the NAACL-
2001.
Lidia Mangu and Eric Brill. 1997. Automatic rule ac-
quisition for spelling correction. In Proceedings of
The Fourteenth International Conference on Machine
Learning, ICML 97. Morgan Kaufmann.
Be´ata Megyesi. 2002. Shallow parsing with pos taggers
and linguistic features. Journal of Machine Learning
Research, 2:639–668.
Ruy L. Milidi´u, Julio C. Duarte, and Roberto Cavalcante.
2006. Machine learning algorithms for portuguese
named entity recognition. In Proceedings of Fourth
Workshop in Information and Human Language Tech-
nology (TIL’06), Ribeir˜ao Preto, Brazil.
Ruy L. Milidi´u, Julio C. Duarte, and C´ıcero N. dos San-
tos. 2007. Tbl template selection: An evolutionary
approach. In Proceedings of Conference of the Span-
ish Association for Artificial Intelligence - CAEPIA,
Salamanca, Spain.
Antonio Molina and Ferran Pla. 2002. Shallow parsing
using specialized hmms. J. Mach. Learn. Res., 2:595–
613.
Grace Ngai and Radu Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings of
North Americal ACL, pages 40–47, June.
Avinesh PVS and Karthik Gali. 2007. Part-of-speech
tagging and chunking using conditional random fields
and transformation based learning. In Proceedings of
the IJCAI and the Workshop On Shallow Parsing for
South Asian Languages (SPSAL), pages 21–24.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Lance Ramshaw and Mitch Marcus. 1999. Text chunk-
ing using transformation-based learning. In S. Arm-
strong, K.W. Church, P. Isabelle, S. Manzi, E. Tzouk-
ermann, and D. Yarowsky, editors, Natural Language
Processing Using Very Large Corpora. Kluwer.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunking.
In Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th CONLL, pages 127–132,
Morristown, NJ, USA. Association for Computational
Linguistics.
Erik F. Tjong Kim Sang. 2002. Memory-based shallow
parsing. J. Mach. Learn. Res., 2:559–594.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of 7th In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, pages 144–155.
Tong Zhang, Fred Damerau, and David Johnson. 2002.
Text chunking based on a generalization of winnow. J.
Mach. Learn. Res., 2:615–637.
</reference>
<page confidence="0.998881">
655
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.254145">
<title confidence="0.999656">Phrase Chunking using Entropy Guided Transformation Learning</title>
<author confidence="0.999741">Ruy L Milidi´u</author>
<affiliation confidence="0.960404">Departamento de Inform´atica</affiliation>
<email confidence="0.791777">PUC-Rio</email>
<author confidence="0.847796">Rio de_Janeiro</author>
<author confidence="0.847796">Brazil</author>
<affiliation confidence="0.923297">Cicero Nogueira dos Santos Departamento de Inform´atica</affiliation>
<email confidence="0.7370675">PUC-Rionogueira@inf.puc-rio.br</email>
<author confidence="0.992607">Julio C Duarte</author>
<affiliation confidence="0.979529">Centro Tecnol´ogico do Ex´ercito</affiliation>
<address confidence="0.894891">Rio de Janeiro, Brazil</address>
<email confidence="0.907385">jduarte@ctex.eb.brmilidiu@inf.puc-rio.br</email>
<abstract confidence="0.998902047619048">Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of decision trees (DT) and Transformation Based Learning (TBL). In this work, we apply the ETL framework to four phrase chunking tasks: Portuguese noun phrase chunking, English base noun phrase chunking, English text chunking and Hindi text chunking. In all four tasks, ETL shows better results than Decision Trees and also than TBL with hand-crafted templates. ETL provides a new training strategy that accelerates transformation learning. For the English text chunking task this corresponds to a factor of five speedup. For Portuguese noun phrase chunking, ETL shows the best reported results for the task. For the other three linguistic tasks, ETL shows state-of-theart competitive results and maintains the advantages of using a rule based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Prashanth R Mannem</author>
</authors>
<title>Introduction to shallow parsing contest on south asian languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="22873" citStr="Bharati and Mannem, 2007" startWordPosition="3874" endWordPosition="3877">TL 95.50 92.63 92.96 92.79 SVM – 94.12 94.13 94.12 Table 12 shows the results, broken down by chunk Table 12: English text chunking results, broken down by chunk type, for the ETL committee. Precision Recall F0=1 (%) (%) (%) ADJP 75.59 72.83 74.19 ADVP 82.02 79.56 80.77 CONJP 35.71 55.56 43.48 INTJ 00.00 00.00 00.00 LST 00.00 00.00 00.00 NP 92.90 93.08 92.99 PP 96.53 97.63 97.08 PRT 66.93 80.19 72.96 SBAR 86.50 85.05 85.77 VP 92.84 93.58 93.21 Overall 92.63 92.96 92.79 3.4 Hindi text chunking The data used in the Hindi text chunking experiments is the SPSAL-2007 corpus, which is described in (Bharati and Mannem, 2007). This corpus is pre-divided into a 20000-tokens training set, a 5000-tokens development set and a 5000-tokens test set. This corpus is tagged with both POS and chunk tags. To fairly compare our approach with the ones presented in the SPSAL-2007, the POS tags of the test corpus were replaced by the ones predicted by an ETL-based Hindi POS Tagger. The description of our ETL pos tagger is beyond the scope of this work. Since the amount of training data is very small (20000 tokens), the accuracy of the ETL Hindi POS tagger is low, 77.50% for the test set. The results are reported in terms of chun</context>
<context position="24375" citStr="Bharati and Mannem, 2007" startWordPosition="4137" endWordPosition="4140">nce is not as good as in the other tasks mainly because of the small amount of training data, which increases the sparsity problem. We do not use template evolution for Hindi text 653 chunking. Since the training corpus is very small, the training time reduction is not significant. three linguistic tasks, ETL shows competitive results and maintains the advantages of using a rule based system. Table 13: Hindi text Chunking. Accuracy # Templates (%) BLS 70.05 – DTWS=5 78.20 – TBL 78.53 100 ETLWS=5 78.53 30 Table 14 compares the results of ETL with the two best Hindi text chunkers at SPSAL-2007 (Bharati and Mannem, 2007). The first one is a combination of Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (PVS and Gali, 2007). The second is based in Maximum Entropy Models (MaxEnt) (Dandapat, 2007). ETL performs better than MaxEnt and worst than HMM+CRF. It is important to note that the accuracy of the POS tagger used by (PVS and Gali, 2007) (78.66%) is better than ours (77.50%). The POS tagging quality directly affects the chunking accuracy. Table 14: Comparison with best systems of SPSAL-2007 Accuracy (%) HMM + CRF 80.97 ETLWS=5 78.53 MaxEnt 74.92 4 Conclusions In this paper, we approach the phra</context>
</contexts>
<marker>Bharati, Mannem, 2007</marker>
<rawString>Akshar Bharati and Prashanth R. Mannem. 2007. Introduction to shallow parsing contest on south asian languages. In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Comput. Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="6862" citStr="Brill, 1995" startWordPosition="1093" endWordPosition="1094">ch is based on the data Entropy, is normally used as the informativeness measure. The objective is to construct a tree, using a minimal set of features, that efficiently partitions the training set into classes of observations. After the tree is grown, a pruning step is carried out in order to avoid overfitting. One of the most used algorithms for induction of a DT is the C4.5 (Quinlan, 1993). We use Quinlan’s C4.5 system throughout this work. 2.2 Transformation-Based Learning Transformation Based error-driven Learning (TBL) is a successful machine learning algorithm introduced by Eric Brill (Brill, 1995). It has since been used for several Natural Language Processing tasks, such as part-of-speech (POS) tagging (Brill, 1995), English text chunking (Ramshaw and Marcus, 1999; dos Santos and Milidi´u, 2007b), spelling correc648 tion (Mangu and Brill, 1997), Portuguese appositive extraction (Freitas et al., 2006), Portuguese named entity extraction (Milidi´u et al., 2006) and Portuguese noun-phrase chunking (dos Santos and Oliveira, 2005), achieving state-of-the-art performance in many of them. TBL uses an error correcting strategy. Its main scheme is to generate an ordered list of rules that corr</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Comput. Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Phrase recognition by filtering and ranking with perceptrons.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-2003, Borovets,</booktitle>
<marker>Carreras, M`arquez, 2003</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2003. Phrase recognition by filtering and ranking with perceptrons. In Proceedings of RANLP-2003, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
</authors>
<title>Combining decision trees and transformation-based learning to correct transferred linguistic representations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Tranlsation Summit,</booktitle>
<pages>55--62</pages>
<publisher>Association</publisher>
<location>New Orleans, USA.</location>
<contexts>
<context position="4673" citStr="Corston-Oliver and Gamon, 2003" startWordPosition="728" endWordPosition="731">ed. Finally, in section 4, we present our concluding remarks. 2 Entropy Guided Transformation Learning Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of Decision Trees (DT) and Transformation-Based Learning (TBL) (dos Santos and Milidi´u, 2007a). The key idea of ETL is to use decision tree induction to obtain templates. Next, the TBL strategy is used to generate transformation rules. The proposed method is illustrated in the Fig. 1. Figure 1: ETL - Entropy Guided Transformation Learning. A combination of DT and TBL is presented in (Corston-Oliver and Gamon, 2003). The main difference between Corston-Oliver &amp; Gamon work and the ETL strategy is that they extract candidate rules directly from the DT, and then use the TBL strategy to select the appropriate rules. Another difference is that they use a binary DT, whereas ETL uses a DT that is not necessarily binary. An evolutionary approach based on Genetic Algorithms (GA) to automatically generate TBL templates is presented in (Milidi´u et al., 2007). Using a simple genetic coding, the generated template sets have efficacy near to the handcrafted templates for the tasks: English Base Noun Phrase Identifica</context>
</contexts>
<marker>Corston-Oliver, Gamon, 2003</marker>
<rawString>Simon Corston-Oliver and Michael Gamon. 2003. Combining decision trees and transformation-based learning to correct transferred linguistic representations. In Proceedings of the Ninth Machine Tranlsation Summit, pages 55–62, New Orleans, USA. Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>R K Wong</author>
</authors>
<title>Formalisation of transformation-based learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the Australian Computer Science Conference - ACSC,</booktitle>
<pages>51--57</pages>
<location>Canberra, Australia.</location>
<contexts>
<context position="11618" citStr="Curran and Wong, 2000" startWordPosition="1885" endWordPosition="1889">k tag; CK[1] = next word Chunk tag; POS[0] = current word POS tag; WRD[0] = current word. 649 Table 1: Text chunking DT Template set example Template set Extended template set CK[0] CK[0] CK[1] CK[0] CK[1] CK[0] CK[1] CK[1] WRD[0] CK[0] CK[1] WRD[0] CK[0] CK[1] WRD[0] CK[1] WRD[0] CK[–1] CK[0] CK[1] WRD[0] CK[–1] CK[0] CK[1] WRD[0] CK[–1] CK[1] POS[0] CK[0] CK[1] POS[0] CK[0] CK[1] POS[0] CK[–1] CK[0] CK[–1] CK[0] CK[–1] Figure 2: Text chunking decision tree excerpt. 2.4 Template Evolution Speedup TBL training time is highly sensitive to the number and complexity of the applied templates. In (Curran and Wong, 2000), it is argued that we can better tune the training time vs. templates complexity trade-off by using an evolutionary template approach. The main idea is to apply only a small number of templates that evolve throughout the training. When training starts, templates are short, consisting of few feature combinations. As training proceeds, templates evolve to more complex ones that contain more feature combinations. In this way, only a few templates are considered at any point in time. Nevertheless, the descriptive power is not significantly reduced. The template evolution approach can be easily im</context>
</contexts>
<marker>Curran, Wong, 2000</marker>
<rawString>J. R. Curran and R. K. Wong. 2000. Formalisation of transformation-based learning. In Proceedings of the Australian Computer Science Conference - ACSC, pages 51–57, Canberra, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandipan Dandapat</author>
</authors>
<title>Part of speech tagging and chunking with maximum entropy model.</title>
<date>2007</date>
<booktitle>In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL),</booktitle>
<pages>29--32</pages>
<contexts>
<context position="24567" citStr="Dandapat, 2007" startWordPosition="4171" endWordPosition="4172">e training corpus is very small, the training time reduction is not significant. three linguistic tasks, ETL shows competitive results and maintains the advantages of using a rule based system. Table 13: Hindi text Chunking. Accuracy # Templates (%) BLS 70.05 – DTWS=5 78.20 – TBL 78.53 100 ETLWS=5 78.53 30 Table 14 compares the results of ETL with the two best Hindi text chunkers at SPSAL-2007 (Bharati and Mannem, 2007). The first one is a combination of Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (PVS and Gali, 2007). The second is based in Maximum Entropy Models (MaxEnt) (Dandapat, 2007). ETL performs better than MaxEnt and worst than HMM+CRF. It is important to note that the accuracy of the POS tagger used by (PVS and Gali, 2007) (78.66%) is better than ours (77.50%). The POS tagging quality directly affects the chunking accuracy. Table 14: Comparison with best systems of SPSAL-2007 Accuracy (%) HMM + CRF 80.97 ETLWS=5 78.53 MaxEnt 74.92 4 Conclusions In this paper, we approach the phrase chunking task using Entropy Guided Transformation Learning (ETL). We carry out experiments with four phrase chunking tasks: Portuguese noun phrase chunking, English base noun phrase chunkin</context>
</contexts>
<marker>Dandapat, 2007</marker>
<rawString>Sandipan Dandapat. 2007. Part of speech tagging and chunking with maximum entropy model. In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL), pages 29–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero N dos Santos</author>
<author>Ruy L Milidi´u</author>
</authors>
<title>Entropy guided transformation learning.</title>
<date>2007</date>
<tech>Technical Report 29/07, Departamento de Informtica, PUC-Rio.</tech>
<marker>Santos, Milidi´u, 2007</marker>
<rawString>C´ıcero N. dos Santos and Ruy L. Milidi´u. 2007a. Entropy guided transformation learning. Technical Report 29/07, Departamento de Informtica, PUC-Rio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero N dos Santos</author>
<author>Ruy L Milidi´u</author>
</authors>
<title>Probabilistic classifications with tbl.</title>
<date>2007</date>
<booktitle>In Proceedings of Eighth International Conference on Intelligent Text Processing and Computational Linguistics – CICLing,</booktitle>
<pages>196--207</pages>
<location>Mexico City, Mexico,</location>
<marker>Santos, Milidi´u, 2007</marker>
<rawString>C´ıcero N. dos Santos and Ruy L. Milidi´u. 2007b. Probabilistic classifications with tbl. In Proceedings of Eighth International Conference on Intelligent Text Processing and Computational Linguistics – CICLing, pages 196–207, Mexico City, Mexico, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero N dos Santos</author>
<author>Claudia Oliveira</author>
</authors>
<title>Constrained atomic term: Widening the reach of rule templates in transformation based learning.</title>
<date>2005</date>
<booktitle>In EPIA,</booktitle>
<pages>622--633</pages>
<contexts>
<context position="2697" citStr="Santos and Oliveira, 2005" startWordPosition="415" endWordPosition="418">ques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations. The process of generating good templates is highly expensive. It strongly depends on the problem expert skills to build them. Even when a template set is available for a given task, it may not be effective when we change from a language to another (dos Santos and Oliveira, 2005). In this work, we apply Entropy Guided Transformation Learning (ETL) for phrase chunking. ETL is a new machine learning strategy that combines the advantages of Decision Trees (DT) and TBL (dos Santos and Milidi´u, 2007a). The ETL key idea is to use decision tree induction to obtain feature combinations (templates) and then use the TBL algorithm to generate transformation rules. ETL produces transformation rules that are more effective than decision trees and also eliminates the need of a problem domain expert to build TBL templates. We evaluate the performance of ETL over four 647 Proceeding</context>
<context position="7300" citStr="Santos and Oliveira, 2005" startWordPosition="1154" endWordPosition="1157">m throughout this work. 2.2 Transformation-Based Learning Transformation Based error-driven Learning (TBL) is a successful machine learning algorithm introduced by Eric Brill (Brill, 1995). It has since been used for several Natural Language Processing tasks, such as part-of-speech (POS) tagging (Brill, 1995), English text chunking (Ramshaw and Marcus, 1999; dos Santos and Milidi´u, 2007b), spelling correc648 tion (Mangu and Brill, 1997), Portuguese appositive extraction (Freitas et al., 2006), Portuguese named entity extraction (Milidi´u et al., 2006) and Portuguese noun-phrase chunking (dos Santos and Oliveira, 2005), achieving state-of-the-art performance in many of them. TBL uses an error correcting strategy. Its main scheme is to generate an ordered list of rules that correct classification mistakes in the training set, which have been produced by an initial classifier. The requirements of the algorithm are: • two instances of the training set, one that has been correctly labeled, and another that remains unlabeled; • an initial classifier, the baseline system, which classifies the unlabeled training set by trying to apply the correct class for each sample. In general, the baseline system is based on s</context>
<context position="16726" citStr="Santos and Oliveira, 2005" startWordPosition="2796" endWordPosition="2799">=9 97.82 88.02 88.34 88.18 40 Table 3 shows the results4 of ETL using template evolution. As we can see, for the task of Portuguese noun phrase chunking, the template evolution strategy reduces the average training time in approximately 35%. On the other hand, there is a decrease of the classifier efficacy in some cases. Table 3: Portuguese noun phrase chunking using ETL with template evolution. Acc. Prec. Rec. Fβ=1 TTR (%) (%) (%) (%) (%) ETLWS=3 97.61 86.22 87.27 86.74 20.7 ETLWS=5 97.56 86.39 87.10 86.74 38.2 ETLWS=7 97.69 87.35 87.89 87.62 37.0 ETLWS=9 97.76 87.55 88.14 87.85 41.9 In (dos Santos and Oliveira, 2005), a special set of six templates is shown. These templates are designed to reduce classification errors of preposition within the task of Portuguese noun phrase chunking. These templates use very specific domain knowledge and are difficult to DT and TBL to extract. Table 4 shows the results of an experiment where we include these six templates into the Ramshaw&amp;Marcus template set and also into the template sets generated by ETL. Again, ETL produces better results than TBL. Table 5 shows the results of using a committee composed by the three best ETL classifiers. The classification is done by s</context>
</contexts>
<marker>Santos, Oliveira, 2005</marker>
<rawString>C´ıcero N. dos Santos and Claudia Oliveira. 2005. Constrained atomic term: Widening the reach of rule templates in transformation based learning. In EPIA, pages 622–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Freitas</author>
<author>M Garrao</author>
<author>C Oliveira</author>
<author>C N dos Santos</author>
<author>M Silveira</author>
</authors>
<title>A anotac¸˜ao de um corpus para o aprendizado supervisionado de um modelo de sn.</title>
<date>2005</date>
<booktitle>In Proceedings of the III TIL / XXV Congresso da SBC, S˜ao Leopoldo - RS -</booktitle>
<publisher>Brasil.</publisher>
<contexts>
<context position="15219" citStr="Freitas et al., 2005" startWordPosition="2519" endWordPosition="2522">est result for the DT classifier is shown. The features word, POS and chunk are used to generate the DT classifier. The chunk tag of a word and its neighbors are the ones guessed by the initial classifier. Using only the 100 most frequent words gives our best results. In all experiments, the term WS=X subscript means that a window of size X was used for the given model. For instance, ETLWS=3 corresponds to ETL trained with window of size three, that is, the current token, the previous and the next one. 3.1 Portuguese noun phrase chunking For this task, we use the SNR-CLIC corpus described in (Freitas et al., 2005). This corpus is tagged with both POS and NP tags. The NP tags are: I, for in NP; O, for out of NP; and B for the leftmost word of an NP beginning immediately after another NP. We divided the corpus into 3514- sentence (83346 tokens) training set and a 878- sentence (20798 tokens) test set. In Table 2 we compare the results3 of ETL with DT and TBL. We can see that ETL, even with a small window size, produces better results than DT and TBL. The Fβ=1 of the ETLWS=7 classifier is 1.8% higher than the one of TBL and 2.6% higher than the one of the DT classifier. 3#T = Number of templates. Table 2:</context>
</contexts>
<marker>Freitas, Garrao, Oliveira, Santos, Silveira, 2005</marker>
<rawString>M. C. Freitas, M. Garrao, C. Oliveira, C. N. dos Santos, and M. Silveira. 2005. A anotac¸˜ao de um corpus para o aprendizado supervisionado de um modelo de sn. In Proceedings of the III TIL / XXV Congresso da SBC, S˜ao Leopoldo - RS - Brasil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Freitas</author>
<author>J C Duarte</author>
<author>C N dos Santos</author>
<author>R L Milidi´u</author>
<author>R P Renteria</author>
<author>V Quental</author>
</authors>
<title>A machine learning approach to the identification of appositives.</title>
<date>2006</date>
<booktitle>In Proceedings of Ibero-American AI Conference, Ribeir˜ao Preto,</booktitle>
<location>Brazil,</location>
<marker>Freitas, Duarte, Santos, Milidi´u, Renteria, Quental, 2006</marker>
<rawString>M. C. Freitas, J. C. Duarte, C. N. dos Santos, R. L. Milidi´u, R. P. Renteria, and V. Quental. 2006. A machine learning approach to the identification of appositives. In Proceedings of Ibero-American AI Conference, Ribeir˜ao Preto, Brazil, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL2001.</booktitle>
<contexts>
<context position="1703" citStr="Kudo and Matsumoto, 2001" startWordPosition="257" endWordPosition="260"> three linguistic tasks, ETL shows state-of-theart competitive results and maintains the advantages of using a rule based system. 1 Introduction Phrase Chunking is a Natural Language Processing (NLP) task that consists in dividing a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other ha</context>
<context position="19455" citStr="Kudo and Matsumoto, 2001" startWordPosition="3271" endWordPosition="3274">esults than DT and TBL. The Fβ=1 of the ETLWS=9 classifier is 0.87% higher than the one of TBL and 2.31% higher than the one of the DT classifier. Table 7 shows the results of ETL using template evolution. The template evolution strategy reduces the average training time in approximately 62%. Differently from the Portuguese NP chunking, we observe an increase of the classifier efficacy in almost all the cases. Table 8 shows the results of using a committee composed by the eight ETL classifiers reported in this section. Table 8 also shows the results for a committee of SVM models presented in (Kudo and Matsumoto, 2001). SVM’s results are the state-ofTable 7: Base NP chunking using ETL with template evolution. Acc. Prec. Rec. Fβ=1 TTR (%) (%) (%) (%) (%) ETLWS=3 97.58 92.07 92.74 92.41 53.9 ETLWS=5 97.63 92.66 93.16 92.91 57.9 ETLWS=7 97.61 92.56 93.04 92.80 65.1 ETLWS=9 97.59 92.50 93.01 92.76 69.4 the-art for the Base NP chunking task. On the other hand, using a committee of ETL classifiers, we produce very competitive results and maintain the advantages of using a rule based system. Table 8: Base NP chunking using a committee of eight ETL classifiers. Accuracy Precision Recall Fβ=1 (%) (%) (%) (%) ETL 97.</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In Proceedings of the NAACL2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceedings of The Fourteenth International Conference on Machine Learning, ICML 97.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7115" citStr="Mangu and Brill, 1997" startWordPosition="1129" endWordPosition="1132">is grown, a pruning step is carried out in order to avoid overfitting. One of the most used algorithms for induction of a DT is the C4.5 (Quinlan, 1993). We use Quinlan’s C4.5 system throughout this work. 2.2 Transformation-Based Learning Transformation Based error-driven Learning (TBL) is a successful machine learning algorithm introduced by Eric Brill (Brill, 1995). It has since been used for several Natural Language Processing tasks, such as part-of-speech (POS) tagging (Brill, 1995), English text chunking (Ramshaw and Marcus, 1999; dos Santos and Milidi´u, 2007b), spelling correc648 tion (Mangu and Brill, 1997), Portuguese appositive extraction (Freitas et al., 2006), Portuguese named entity extraction (Milidi´u et al., 2006) and Portuguese noun-phrase chunking (dos Santos and Oliveira, 2005), achieving state-of-the-art performance in many of them. TBL uses an error correcting strategy. Its main scheme is to generate an ordered list of rules that correct classification mistakes in the training set, which have been produced by an initial classifier. The requirements of the algorithm are: • two instances of the training set, one that has been correctly labeled, and another that remains unlabeled; • an</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>Lidia Mangu and Eric Brill. 1997. Automatic rule acquisition for spelling correction. In Proceedings of The Fourteenth International Conference on Machine Learning, ICML 97. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be´ata Megyesi</author>
</authors>
<title>Shallow parsing with pos taggers and linguistic features.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--639</pages>
<contexts>
<context position="1877" citStr="Megyesi, 2002" startWordPosition="283" endWordPosition="284">rocessing (NLP) task that consists in dividing a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations. The process of generating good templates is highly expensiv</context>
</contexts>
<marker>Megyesi, 2002</marker>
<rawString>Be´ata Megyesi. 2002. Shallow parsing with pos taggers and linguistic features. Journal of Machine Learning Research, 2:639–668.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruy L Milidi´u</author>
<author>Julio C Duarte</author>
<author>Roberto Cavalcante</author>
</authors>
<title>Machine learning algorithms for portuguese named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of Fourth Workshop in Information and Human Language Technology (TIL’06), Ribeir˜ao Preto,</booktitle>
<marker>Milidi´u, Duarte, Cavalcante, 2006</marker>
<rawString>Ruy L. Milidi´u, Julio C. Duarte, and Roberto Cavalcante. 2006. Machine learning algorithms for portuguese named entity recognition. In Proceedings of Fourth Workshop in Information and Human Language Technology (TIL’06), Ribeir˜ao Preto, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruy L Milidi´u</author>
<author>Julio C Duarte</author>
<author>C´ıcero N dos Santos</author>
</authors>
<title>Tbl template selection: An evolutionary approach.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference of the Spanish Association for Artificial Intelligence - CAEPIA,</booktitle>
<location>Salamanca,</location>
<marker>Milidi´u, Duarte, Santos, 2007</marker>
<rawString>Ruy L. Milidi´u, Julio C. Duarte, and C´ıcero N. dos Santos. 2007. Tbl template selection: An evolutionary approach. In Proceedings of Conference of the Spanish Association for Artificial Intelligence - CAEPIA, Salamanca, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Molina</author>
<author>Ferran Pla</author>
</authors>
<title>Shallow parsing using specialized hmms.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>2</volume>
<pages>613</pages>
<contexts>
<context position="1930" citStr="Molina and Pla, 2002" startWordPosition="290" endWordPosition="293"> a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations. The process of generating good templates is highly expensive. It strongly depends on the problem expert skills t</context>
</contexts>
<marker>Molina, Pla, 2002</marker>
<rawString>Antonio Molina and Ferran Pla. 2002. Shallow parsing using specialized hmms. J. Mach. Learn. Res., 2:595– 613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformationbased learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings of North Americal ACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="2270" citStr="Ngai and Florian, 2001" startWordPosition="343" endWordPosition="346">e proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations. The process of generating good templates is highly expensive. It strongly depends on the problem expert skills to build them. Even when a template set is available for a given task, it may not be effective when we change from a language to another (dos Santos and Oliveira, 2005). In this work, we apply Entropy Guided Transformation Learning (ETL) for phrase chunking. ETL is a new machine learning strategy that combines the advantages of Decision Tr</context>
<context position="12885" citStr="Ngai and Florian, 2001" startWordPosition="2096" endWordPosition="2099">rom a DT. We implement this idea by successively training TBL models. Each model uses only the templates that contain feature combinations up to a given tree level. For instance, using the tree shown in Fig. 2, we have the following template sets for the three first training rounds2: 1. CK[0] CK[1]; CK[0] CK[–1] 2. CK[0] CK[1] WRD[0]; CK[0] CK[1] POS[0] 3. CK[0] CK[1] WRD[0] CK[–1] Using the template evolution strategy, the training time is decreased by a factor of five for the English text chunking task. This is a remarkable reduction, since we use an implementation of the fastTBL algorithm (Ngai and Florian, 2001) that is already a very fast TBL version. The efficacy of the rules generated by the sequential training is quite similar to the one obtained by training with all the templates at the same time. 3 Experiments This section presents the experimental setup and results of the application of ETL to four phrase chunking tasks. ETL results are compared with the results of DT and TBL using hand-crafted templates. In the TBL step, for each one of the four chunking tasks, the initial classifier assigns to each word the chunk tag that was most frequently associated with the part-of-speech of that word in</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformationbased learning in the fast lane. In Proceedings of North Americal ACL, pages 40–47, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avinesh PVS</author>
<author>Karthik Gali</author>
</authors>
<title>Part-of-speech tagging and chunking using conditional random fields and transformation based learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL),</booktitle>
<pages>21--24</pages>
<contexts>
<context position="24494" citStr="PVS and Gali, 2007" startWordPosition="4157" endWordPosition="4160">oblem. We do not use template evolution for Hindi text 653 chunking. Since the training corpus is very small, the training time reduction is not significant. three linguistic tasks, ETL shows competitive results and maintains the advantages of using a rule based system. Table 13: Hindi text Chunking. Accuracy # Templates (%) BLS 70.05 – DTWS=5 78.20 – TBL 78.53 100 ETLWS=5 78.53 30 Table 14 compares the results of ETL with the two best Hindi text chunkers at SPSAL-2007 (Bharati and Mannem, 2007). The first one is a combination of Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (PVS and Gali, 2007). The second is based in Maximum Entropy Models (MaxEnt) (Dandapat, 2007). ETL performs better than MaxEnt and worst than HMM+CRF. It is important to note that the accuracy of the POS tagger used by (PVS and Gali, 2007) (78.66%) is better than ours (77.50%). The POS tagging quality directly affects the chunking accuracy. Table 14: Comparison with best systems of SPSAL-2007 Accuracy (%) HMM + CRF 80.97 ETLWS=5 78.53 MaxEnt 74.92 4 Conclusions In this paper, we approach the phrase chunking task using Entropy Guided Transformation Learning (ETL). We carry out experiments with four phrase chunking</context>
</contexts>
<marker>PVS, Gali, 2007</marker>
<rawString>Avinesh PVS and Karthik Gali. 2007. Part-of-speech tagging and chunking using conditional random fields and transformation based learning. In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL), pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6645" citStr="Quinlan, 1993" startWordPosition="1064" endWordPosition="1065">s of Information Theory. The learning algorithm executes a general to specific search of a feature space. The most informative feature is added to a tree structure at each step of the search. Information Gain Ratio, which is based on the data Entropy, is normally used as the informativeness measure. The objective is to construct a tree, using a minimal set of features, that efficiently partitions the training set into classes of observations. After the tree is grown, a pruning step is carried out in order to avoid overfitting. One of the most used algorithms for induction of a DT is the C4.5 (Quinlan, 1993). We use Quinlan’s C4.5 system throughout this work. 2.2 Transformation-Based Learning Transformation Based error-driven Learning (TBL) is a successful machine learning algorithm introduced by Eric Brill (Brill, 1995). It has since been used for several Natural Language Processing tasks, such as part-of-speech (POS) tagging (Brill, 1995), English text chunking (Ramshaw and Marcus, 1999; dos Santos and Milidi´u, 2007b), spelling correc648 tion (Mangu and Brill, 1997), Portuguese appositive extraction (Freitas et al., 2006), Portuguese named entity extraction (Milidi´u et al., 2006) and Portugue</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1999</date>
<booktitle>Natural Language Processing Using Very Large Corpora.</booktitle>
<editor>In S. Armstrong, K.W. Church, P. Isabelle, S. Manzi, E. Tzoukermann, and D. Yarowsky, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="1861" citStr="Ramshaw and Marcus, 1999" startWordPosition="279" endWordPosition="282">ng is a Natural Language Processing (NLP) task that consists in dividing a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations. The process of generating good templates is</context>
<context position="7033" citStr="Ramshaw and Marcus, 1999" startWordPosition="1116" endWordPosition="1119">fficiently partitions the training set into classes of observations. After the tree is grown, a pruning step is carried out in order to avoid overfitting. One of the most used algorithms for induction of a DT is the C4.5 (Quinlan, 1993). We use Quinlan’s C4.5 system throughout this work. 2.2 Transformation-Based Learning Transformation Based error-driven Learning (TBL) is a successful machine learning algorithm introduced by Eric Brill (Brill, 1995). It has since been used for several Natural Language Processing tasks, such as part-of-speech (POS) tagging (Brill, 1995), English text chunking (Ramshaw and Marcus, 1999; dos Santos and Milidi´u, 2007b), spelling correc648 tion (Mangu and Brill, 1997), Portuguese appositive extraction (Freitas et al., 2006), Portuguese named entity extraction (Milidi´u et al., 2006) and Portuguese noun-phrase chunking (dos Santos and Oliveira, 2005), achieving state-of-the-art performance in many of them. TBL uses an error correcting strategy. Its main scheme is to generate an ordered list of rules that correct classification mistakes in the training set, which have been produced by an initial classifier. The requirements of the algorithm are: • two instances of the training </context>
<context position="14588" citStr="Ramshaw and Marcus, 1999" startWordPosition="2405" endWordPosition="2408">he following experimental setup provided us our best results. ETL in the ETL learning, we use the features word, POS and chunk. In order to overcome the sparsity problem, we only use the 200 most frequent words to induce the DT. In the DT learning, the chunk tag of the word is the one applied by the initial classifier. On the other hand, the chunk tag of neighbor words are the true ones. We report results for ETL trained with all the templates at the same time as well as using template evolution. TBL the results for the TBL approach refers to TBL trained with the set of templates proposed in (Ramshaw and Marcus, 1999). DT the best result for the DT classifier is shown. The features word, POS and chunk are used to generate the DT classifier. The chunk tag of a word and its neighbors are the ones guessed by the initial classifier. Using only the 100 most frequent words gives our best results. In all experiments, the term WS=X subscript means that a window of size X was used for the given model. For instance, ETLWS=3 corresponds to ETL trained with window of size three, that is, the current token, the previous and the next one. 3.1 Portuguese noun phrase chunking For this task, we use the SNR-CLIC corpus desc</context>
<context position="18447" citStr="Ramshaw and Marcus, 1999" startWordPosition="3094" endWordPosition="3097">.53 89.11 88.82 46 Table 6: Base NP chunking. Acc. Prec. Rec. Fβ=1 # T (%) (%) (%) (%) BLS 94.48 78.20 81.87 79.99 – DTWS=11 97.03 89.92 91.16 90.53 – TBL 97.42 91.68 92.26 91.97 100 ETLWS=3 97.54 91.93 92.78 92.35 68 ETLWS=5 97.55 92.43 92.77 92.60 85 ETLWS=7 97.52 92.49 92.70 92.59 106 ETLWS=9 97.63 92.62 93.05 92.84 122 Table 5: Committee with the classifiers ETLWS=5, ETLWS=7 and ETLWS=9, shown in Table 4. Results (%) Accuracy 97.97 Precision 88.62 Recall 89.67 Fβ=1 89.14 3.2 English base noun phrase chunking The data used in the base NP chunking experiments is the one by Ramshaw &amp; Marcus (Ramshaw and Marcus, 1999). This corpus contains sections 15- 18 and section 20 of the Penn Treebank, and is predivided into 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test. This corpus is tagged with both POS and chunk tags. Table 6 compares the results of ETL with DT and TBL for the base NP chunking. We can see that ETL, even using a small window size, produces better results than DT and TBL. The Fβ=1 of the ETLWS=9 classifier is 0.87% higher than the one of TBL and 2.31% higher than the one of the DT classifier. Table 7 shows the results of ETL using template evolution. The templat</context>
<context position="20339" citStr="Ramshaw and Marcus, 1999" startWordPosition="3429" endWordPosition="3432">.59 92.50 93.01 92.76 69.4 the-art for the Base NP chunking task. On the other hand, using a committee of ETL classifiers, we produce very competitive results and maintain the advantages of using a rule based system. Table 8: Base NP chunking using a committee of eight ETL classifiers. Accuracy Precision Recall Fβ=1 (%) (%) (%) (%) ETL 97.72 92.87 93.34 93.11 SVM – 94.15 94.29 94.22 3.3 English text chunking The data used in the English text chunking experiments is the CoNLL-2000 corpus, which is described in (Sang and Buchholz, 2000). It is composed by the same texts as the Ramshaw &amp; Marcus (Ramshaw and Marcus, 1999) corpus. Table 9 compares the results of ETL with DTs and TBL for English text chunking. ETL, even using a small window size, produces better results than DTs and TBL. The Fβ=1 of the ETLWS=3 classifier is 0.28% higher than the one of TBL and 2.17% higher than the one of the DT classifier. It is an interesting linguistic finding that the use of a window of size 3 652 (the current token, the previous token and the next token) provides the current best results for this task. type, of using a committee composed by the eight ETL classifiers reported in this section. Table 9: English text Chunking.</context>
</contexts>
<marker>Ramshaw, Marcus, 1999</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1999. Text chunking using transformation-based learning. In S. Armstrong, K.W. Church, P. Isabelle, S. Manzi, E. Tzoukermann, and D. Yarowsky, editors, Natural Language Processing Using Very Large Corpora. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd workshop on Learning language in logic and the 4th CONLL,</booktitle>
<pages>127--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1471" citStr="Sang and Buchholz, 2000" startWordPosition="219" endWordPosition="222">strategy that accelerates transformation learning. For the English text chunking task this corresponds to a factor of five speedup. For Portuguese noun phrase chunking, ETL shows the best reported results for the task. For the other three linguistic tasks, ETL shows state-of-theart competitive results and maintains the advantages of using a rule based system. 1 Introduction Phrase Chunking is a Natural Language Processing (NLP) task that consists in dividing a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techni</context>
<context position="20254" citStr="Sang and Buchholz, 2000" startWordPosition="3412" endWordPosition="3415">ETLWS=5 97.63 92.66 93.16 92.91 57.9 ETLWS=7 97.61 92.56 93.04 92.80 65.1 ETLWS=9 97.59 92.50 93.01 92.76 69.4 the-art for the Base NP chunking task. On the other hand, using a committee of ETL classifiers, we produce very competitive results and maintain the advantages of using a rule based system. Table 8: Base NP chunking using a committee of eight ETL classifiers. Accuracy Precision Recall Fβ=1 (%) (%) (%) (%) ETL 97.72 92.87 93.34 93.11 SVM – 94.15 94.29 94.22 3.3 English text chunking The data used in the English text chunking experiments is the CoNLL-2000 corpus, which is described in (Sang and Buchholz, 2000). It is composed by the same texts as the Ramshaw &amp; Marcus (Ramshaw and Marcus, 1999) corpus. Table 9 compares the results of ETL with DTs and TBL for English text chunking. ETL, even using a small window size, produces better results than DTs and TBL. The Fβ=1 of the ETLWS=3 classifier is 0.28% higher than the one of TBL and 2.17% higher than the one of the DT classifier. It is an interesting linguistic finding that the use of a window of size 3 652 (the current token, the previous token and the next token) provides the current best results for this task. type, of using a committee composed b</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: chunking. In Proceedings of the 2nd workshop on Learning language in logic and the 4th CONLL, pages 127–132, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--559</pages>
<contexts>
<context position="1957" citStr="Sang, 2002" startWordPosition="295" endWordPosition="296"> parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called templates, that are meant to capture the relevant feature combinations. The process of generating good templates is highly expensive. It strongly depends on the problem expert skills to build them. Even when a t</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Memory-based shallow parsing. J. Mach. Learn. Res., 2:559–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Chia-Hui Chang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>A general and multi-lingual phrase chunking model based on masking method.</title>
<date>2006</date>
<booktitle>In Proceedings of 7th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="1721" citStr="Wu et al., 2006" startWordPosition="261" endWordPosition="264">TL shows state-of-theart competitive results and maintains the advantages of using a rule based system. 1 Introduction Phrase Chunking is a Natural Language Processing (NLP) task that consists in dividing a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must</context>
<context position="21904" citStr="Wu et al., 2006" startWordPosition="3707" endWordPosition="3710">late evolution strategy reduces the average training time by approximately 81%. On the other hand, there is a small decrease of the classifier efficacy in all cases. Table 10: English text chunking using ETL with template evolution. Acc. Prec. Rec. F0=1 TTR (%) (%) (%) (%) (%) ETLWS=3 95.21 92.14 92.53 92.34 77.2 ETLWS=5 94.98 91.84 92.25 92.04 80.8 ETLWS=7 95.03 91.89 92.28 92.09 83.0 ETLWS=9 95.01 91.87 92.21 92.04 84.5 Table 11 shows the results of using a committee composed by the eight ETL classifiers reported in this section. Table 11 also shows the results for a SVM model presented in (Wu et al., 2006). SVM’s results are the state-of-the-art for the Text chunking task. On the other hand, using a committee of ETL classifiers, we produce very competitive results and maintain the advantages of using a rule based system. Table 11: English text Chunking using a committee of eight ETL classifiers. Accuracy Precision Recall F0=1 (%) (%) (%) (%) ETL 95.50 92.63 92.96 92.79 SVM – 94.12 94.13 94.12 Table 12 shows the results, broken down by chunk Table 12: English text chunking results, broken down by chunk type, for the ETL committee. Precision Recall F0=1 (%) (%) (%) ADJP 75.59 72.83 74.19 ADVP 82.</context>
</contexts>
<marker>Wu, Chang, Lee, 2006</marker>
<rawString>Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006. A general and multi-lingual phrase chunking model based on masking method. In Proceedings of 7th International Conference on Intelligent Text Processing and Computational Linguistics, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David Johnson</author>
</authors>
<title>Text chunking based on a generalization of winnow.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--615</pages>
<contexts>
<context position="1750" citStr="Zhang et al., 2002" startWordPosition="266" endWordPosition="269">ompetitive results and maintains the advantages of using a rule based system. 1 Introduction Phrase Chunking is a Natural Language Processing (NLP) task that consists in dividing a text into syntactically correlated parts of words. Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk (Sang and Buchholz, 2000). It provides a key feature that helps on more elaborated NLP tasks such as parsing and information extraction. Since the last decade, many high-performance chunking systems were proposed, such as, SVMbased (Kudo and Matsumoto, 2001; Wu et al., 2006), Winnow (Zhang et al., 2002), votedperceptrons (Carreras and M`arquez, 2003), Transformation-Based Learning (TBL) (Ramshaw and Marcus, 1999; Megyesi, 2002) and Hidden Markov Model (HMM) (Molina and Pla, 2002), Memory-based (Sang, 2002). State-of-the-art systems for English base noun phrase chunking and text chunking are based in statistical techniques (Kudo and Matsumoto, 2001; Wu et al., 2006; Zhang et al., 2002). TBL is one of the most accurate rule-based techniques for phrase chunking tasks (Ramshaw and Marcus, 1999; Ngai and Florian, 2001; Megyesi, 2002). On the other hand, TBL rules must follow patterns, called temp</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David Johnson. 2002. Text chunking based on a generalization of winnow. J. Mach. Learn. Res., 2:615–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>