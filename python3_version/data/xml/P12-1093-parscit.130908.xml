<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000213">
<title confidence="0.9982865">
Exploiting Social Information in Grounded Language Learning via
Grammatical Reductions
</title>
<author confidence="0.998487">
Mark Johnson
</author>
<affiliation confidence="0.881642666666667">
Department of Computing
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.978299">
Mark.Johnson@MQ.edu.au
</email>
<author confidence="0.988004">
Katherine Demuth
</author>
<affiliation confidence="0.881795333333333">
Department of Linguistics
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.97773">
Katherine.Demuth@MQ.edu.au
</email>
<author confidence="0.991831">
Michael Frank
</author>
<affiliation confidence="0.990182">
Department of Psychology
Stanford University
</affiliation>
<address confidence="0.626878">
Stanford, California
</address>
<email confidence="0.998564">
mcfrank@Stanford.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998853173913044">
This paper uses an unsupervised model of
grounded language acquisition to study the
role that social cues play in language acqui-
sition. The input to the model consists of (or-
thographically transcribed) child-directed ut-
terances accompanied by the set of objects
present in the non-linguistic context. Each
object is annotated by social cues, indicating
e.g., whether the caregiver is looking at or
touching the object. We show how to model
the task of inferring which objects are be-
ing talked about (and which words refer to
which objects) as standard grammatical in-
ference, and describe PCFG-based unigram
models and adaptor grammar-based colloca-
tion models for the task. Exploiting social
cues improves the performance of all mod-
els. Our models learn the relative importance
of each social cue jointly with word-object
mappings and collocation structure, consis-
tent with the idea that children could discover
the importance of particular social informa-
tion sources during word learning.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977448979592">
From learning sounds to learning the meanings of
words, social interactions are extremely important
for children’s early language acquisition (Baldwin,
1993; Kuhl et al., 2003). For example, children who
engage in more joint attention (e.g. looking at par-
ticular objects together) with caregivers tend to learn
words faster (Carpenter et al., 1998). Yet compu-
tational or formal models of social interaction are
rare, and those that exist have rarely gone beyond
the stage of cue-weighting models. In order to study
the role that social cues play in language acquisition,
this paper presents a structured statistical model of
grounded learning that learns a mapping between
words and objects from a corpus of child-directed
utterances in a completely unsupervised fashion. It
exploits five different social cues, which indicate
which object (if any) the child is looking at, which
object the child is touching, etc. Our models learn
the salience of each social cue in establishing refer-
ence, relative to their co-occurrence with objects that
are not being referred to. Thus, this work is consis-
tent with a view of language acquisition in which
children learn to learn, discovering organizing prin-
ciples for how language is organized and used so-
cially (Baldwin, 1993; Hollich et al., 2000; Smith et
al., 2002).
We reduce the grounded learning task to a gram-
matical inference problem (Johnson et al., 2010;
B¨orschinger et al., 2011). The strings presented to
our grammatical learner contain a prefix which en-
codes the objects and their social cues for each ut-
terance, and the rules of the grammar encode rela-
tionships between these objects and specific words.
These rules permit every object to map to every
word (including function words; i.e., there is no
“stop word” list), and the learning process decides
which of these rules will have a non-trivial proba-
bility (these encode the object-word mappings the
system has learned).
This reduction of grounded learning to grammat-
ical inference allows us to use standard grammati-
cal inference procedures to learn our models. Here
we use the adaptor grammar package described in
Johnson et al. (2007) and Johnson and Goldwater
(2009) with “out of the box” default settings; no
parameter tuning whatsoever was done. Adaptor
grammars are a framework for specifying hierarchi-
cal non-parametric models that has been previously
used to model language acquisition (Johnson, 2008).
</bodyText>
<page confidence="0.986475">
883
</page>
<note confidence="0.988563">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883–891,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.416684">
Social cue Value
</subsectionHeader>
<bodyText confidence="0.8100642">
child.eyes objects child is looking at
child.hands objects child is touching
mom.eyes objects care-giver is looking at
mom.hands objects care-giver is touching
mom.point objects care-giver is pointing to
</bodyText>
<figureCaption confidence="0.995277">
Figure 1: The 5 social cues in the Frank et al. (to appear)
corpus. The value of a social cue for an utterance is a
subset of the available topics (i.e., the objects in the non-
linguistic context) of that utterance.
</figureCaption>
<bodyText confidence="0.999992107142857">
A semanticist might argue that our view of refer-
ential mapping is flawed: full noun phrases (e.g., the
dog), rather than nouns, refer to specific objects, and
nouns denote properties (e.g., dog denotes the prop-
erty of being a dog). Learning that a noun, e.g., dog,
is part of a phrase used to refer to a specific dog (say,
Fido) does not suffice to determine the noun’s mean-
ing: the noun could denote a specific breed of dog,
or animals in general. But learning word-object rela-
tionships is a plausible first step for any learner: it is
often only the contrast between learned relationships
and novel relationships that allows children to in-
duce super- or sub-ordinate mappings (Clark, 1987).
Nevertheless, in deference to such objections, we
call the object that a phrase containing a given noun
refers to the topic of that noun. (This is also appro-
priate, given that our models are specialisations of
topic models).
Our models are intended as an “ideal learner” ap-
proach to early social language learning, attempt-
ing to weight the importance of social and structural
factors in the acquisition of word-object correspon-
dences. From this perspective, the primary goal is
to investigate the relationships between acquisition
tasks (Johnson, 2008; Johnson et al., 2010), looking
for synergies (areas of acquisition where attempting
two learning tasks jointly can provide gains in both)
as well as areas where information overlaps.
</bodyText>
<subsectionHeader confidence="0.999797">
1.1 A training corpus for social cues
</subsectionHeader>
<bodyText confidence="0.9858032">
Our work here uses a corpus of child-directed
speech annotated with social cues, described in
Frank et al. (to appear). The corpus consists
of 4,763 orthographically-transcribed utterances of
caregivers to their pre-linguistic children (ages 6, 12,
and 18 months) during home visits where children
played with a consistent set of toys. The sessions
were video-taped, and each utterance was annotated
with the five social cues described in Figure 1.
Each utterance in the corpus contains the follow-
</bodyText>
<listItem confidence="0.985784625">
ing information:
• the sequence of orthographic words uttered by
the care-giver,
• a set of available topics (i.e., objects in the non-
linguistic objects),
• the values of the social cues, and
• a set of intended topics, which the care-giver
refers to.
</listItem>
<bodyText confidence="0.990554">
Figure 2 presents this information for an example ut-
terance. All of these but the intended topics are pro-
vided to our learning algorithms; the intended top-
ics are used to evaluate the output produced by our
learners.
Generally the intended topics consist of zero or
one elements from the available topics, but not al-
ways: it is possible for the caregiver to refer to two
objects in a single utterance, or to refer to an object
not in the current non-linguistic context (e.g., to a
toy that has been put away). There is a considerable
amount of anaphora in this corpus, which our mod-
els currently ignore.
Frank et al. (to appear) give extensive details on
the corpus, including inter-annotator reliability in-
formation for all annotations, and provide detailed
statistical analyses of the relationships between the
various social cues, the available topics and the in-
tended topics. That paper also gives instructions on
obtaining the corpus.
</bodyText>
<subsectionHeader confidence="0.774179">
1.2 Previous work
</subsectionHeader>
<bodyText confidence="0.999953238095238">
There is a growing body of work on the role of social
cues in language acquisition. The language acqui-
sition research community has long recognized the
importance of social cues for child language acqui-
sition (Baldwin, 1991; Carpenter et al., 1998; Kuhl
et al., 2003).
Siskind (1996) describes one of the first exam-
ples of a model that learns the relationship between
words and topics, albeit in a non-statistical frame-
work. Yu and Ballard (2007) describe an associative
learner that associates words with topics and that
exploits prosodic as well as social cues. The rela-
tive importance of the various social cues are spec-
ified a priori in their model (rather than learned, as
they are here), and unfortunately their training cor-
pus is not available. Frank et al. (2008) describes a
Bayesian model that learns the relationship between
words and topics, but the version of their model that
included social cues presented a number of chal-
lenges for inference. The unigram model we de-
scribe below corresponds most closely to the Frank
</bodyText>
<page confidence="0.9961">
884
</page>
<figure confidence="0.814734">
.dog # .pig child.eyes mom.eyes mom.hands # ## wheres the piggie
</figure>
<figureCaption confidence="0.9885377">
Figure 2: The photograph indicates non-linguistic context containing a (toy) pig and dog for the utterance Where’s the
piggie?. Below that, we show the representation of this utterance that serves as the input to our models. The prefix (the
portion of the string before the “##”) lists the available topics (i.e., the objects in the non-linguistic context) and their
associated social cues (the cues for the pig are child.eyes, mom.eyes and mom.hands, while the dog is not associated
with any social cues). The intended topic is the pig. The learner’s goals are to identify the utterance’s intended topic,
and which words in the utterance are associated with which topic.
Figure 3: Sample parse generated by the Unigram PCFG. Nodes coloured red show how the “pig” topic is propagated
from the prefix (before the “##” separator) into the utterance. The social cues associated with each object are generated
either from a “Topical” or a “NotTopical” nonterminal, depending on whether the corresponding object is topical or
not.
</figureCaption>
<figure confidence="0.99969671875">
#
#
##
the
.pig
.dog
piggie
wheres
child.eyes
mom.hands
Topic.pig
Words.pig
Sentence
T.None
Word.None
Words.pig
NotTopical.child.eyes
NotTopical.child.hands
NotTopical.mom.eyes
NotTopical.mom.hands
NotTopical.mom.point
Topical.child.eyes
Topical.child.hands
Topical.mom.eyes
Topical.mom.hands
Topic.None
T.pig
Topical.mom.point
Topic.pig
Word.None
Words.pig
Word.pig
</figure>
<page confidence="0.99365">
885
</page>
<bodyText confidence="0.999978833333333">
et al. model. Johnson et al. (2010) reduces grounded
learning to grammatical inference for adaptor gram-
mars and shows how it can be used to perform word
segmentation as well as learning word-topic rela-
tionships, but their model does not take social cues
into account.
</bodyText>
<sectionHeader confidence="0.3973565" genericHeader="method">
2 Reducing grounded learning with social
cues to grammatical inference
</sectionHeader>
<bodyText confidence="0.999983586206897">
This section explains how we reduce ground learn-
ing problems with social cues to grammatical in-
ference problems, which lets us apply a wide vari-
ety of grammatical inference algorithms to grounded
learning problems. An advantage of reducing
grounded learning to grammatical inference is that
it suggests new ways to generalise grounded learn-
ing models; we explore three such generalisations
here. The main challenge in this reduction is finding
a way of expressing the non-linguistic information
as part of the strings that serve as the grammatical in-
ference procedure’s input. Here we encode the non-
linguistic information in a “prefix” to each utterance
as shown in Figure 2, and devise a grammar such
that inference for the grammar corresponds to learn-
ing the word-topic relationships and the salience of
the social cues for grounded learning.
All our models associate each utterance with zero
or one topics (this means we cannot correctly anal-
yse utterances with more than one intended topic).
We analyse an utterance associated with zero topics
as having the special topic None, so we can assume
that every utterance has exactly one topic. All our
grammars generate strings of the form shown in Fig-
ure 2, and they do so by parsing the prefix and the
words of the utterance separately; the top-level rules
of the grammar force the same topic to be associated
with both the prefix and the words of the utterance
(see Figure 3).
</bodyText>
<subsectionHeader confidence="0.998517">
2.1 Topic models and the unigram PCFG
</subsectionHeader>
<bodyText confidence="0.999915222222222">
As Johnson et al. (2010) observe, this kind of
grounded learning can be viewed as a specialised
kind of topic inference in a topic model, where the
utterance topic is constrained by the available ob-
jects (possible topics). We exploit this observation
here using a reduction based on the reduction of
LDA topic models to PCFGs proposed by Johnson
(2010). This leads to our first model, the unigram
grammar, which is a PCFG.1
</bodyText>
<figure confidence="0.977102615384615">
Sentence —* Topict Wordst dt E T1
TopicNone —* ##
Topict —* Tt TopicNone dt E T1
Topict —* TNone Topict dt E T
Tt —* t Topicalc1 dt E T
Topicalci —* (ci) Topicalci+1 i = �, . . . , ` � 1
Topicalc` —* (ct) #
TNone —* t NotTopicalc1 dt E T
NotTopicalci —* (ci) NotTopicalci+1 i = �, . . . , ` � 1
NotTopicalc` —* (ct) #
Wordst —* WordNone (Wordst) dt E T1
Wordst —* Wordt (Wordst) dt E T
Wordt —* w dt E T1, w E W
</figure>
<figureCaption confidence="0.981926529411765">
Figure 4: The rule schema that generate the unigram
PCFG. Here (cl, ... , cj) is an ordered list of the so-
cial cues, T is the set of all non-None available topics,
T0 = T U {None}, and W is the set of words appearing
in the utterances. Parentheses indicate optionality.
Figure 4 presents the rules of the unigram gram-
mar. This grammar has two major parts. The rules
expanding the Topict nonterminals ensure that the
social cues for the available topic t are parsed un-
der the Topical nonterminals. All other available
topics are parsed under TNone nonterminals, so their
social cues are parsed under NotTopical nontermi-
nals. The rules expanding these non-terminals are
specifically designed so that the generation of the so-
cial cues corresponds to a series of binary decisions
about each social cue. For example, the probability
of the rule
</figureCaption>
<bodyText confidence="0.965631769230769">
Topicalchild.eyes —* .child.eyes Topicalchild.hands
is the probability of an object that is an utterance
topic occuring with the child.eyes social cue. By es-
timating the probabilities of these rules, the model
effectively learns the probability of each social cue
being associated with a Topical or a NotTopical
available topic, respectively.
The nonterminals Wordst expand to a sequence
of Wordt and WordNone nonterminals, each of
which can expand to any word whatsoever. In prac-
tice Wordt will expand to those words most strongly
associated with topic t, while WordNone will expand
to those words not associated with any topic.
</bodyText>
<footnote confidence="0.99166775">
1In fact, the unigram grammar is equivalent to a HMM,
but the PCFG parameterisation makes clear the relationship
between grounded learning and estimation of grammar rule
weights.
</footnote>
<page confidence="0.987552">
886
</page>
<table confidence="0.65376175">
Sentence —* Topict Collocst dt E T&apos;
Collocst —* Colloct (Collocst) dt E T&apos;
Collocst —* CollocNone (Collocst) dt E T
Colloct —* Wordst dt E T&apos;
Wordst —* Wordt (Wordst) dt E T&apos;
Wordst —* WordNone (Wordst) dt E T
Wordt —* Word dt E T&apos;
Word—*w dw E W
</table>
<figureCaption confidence="0.991491166666667">
Figure 5: The rule schema that generate the collocation
adaptor grammar. Adapted nonterminals are indicated via
underlining. Here T is the set of all non-None available
topics, T&apos; = T U {None}, and W is the set of words ap-
pearing in the utterances. The rules expanding the Topict
nonterminals are exactly as in unigram PCFG.
</figureCaption>
<subsectionHeader confidence="0.993386">
2.2 Adaptor grammars
</subsectionHeader>
<bodyText confidence="0.999983793103448">
Our other grounded learning models are based on
reductions of grounded learning to adaptor gram-
mar inference problems. Adaptor grammars are a
framework for stating a variety of Bayesian non-
parametric models defined in terms of a hierarchy of
Pitman-Yor Processes: see Johnson et al. (2007) for
a formal description. Informally, an adaptor gram-
mar is specified by a set of rules just as in a PCFG,
plus a set of adapted nonterminals. The set of
trees generated by an adaptor grammar is the same
as the set of trees generated by a PCFG with the
same rules, but the generative process differs. Non-
adapted nonterminals in an adaptor grammar expand
just as they do in a PCFG: the probability of choos-
ing a rule is specified by its probability. However,
the expansion of an adapted nonterminal depends on
how it expanded in previous derivations. An adapted
nonterminal can directly expand to a subtree with
probability proportional to the number of times that
subtree has been previously generated; it can also
“back off” to expand using a grammar rule, just as
in a PCFG, with probability proportional to a con-
stant.2
Thus an adaptor grammar can be viewed as
caching each tree generated by each adapted non-
terminal, and regenerating it with probability pro-
portional to the number of times it was previously
generated (with some probability mass reserved to
generate “new” trees). This enables adaptor gram-
</bodyText>
<footnote confidence="0.9585428">
2This is a description of Chinese Restaurant Processes,
which are the predictive distributions for Dirichlet Processes.
Our adaptor grammars are actually based on the more general
Pitman-Yor Processes, as described in Johnson and Goldwater
(2009).
</footnote>
<note confidence="0.650556">
Sentence
</note>
<figureCaption confidence="0.689392">
Figure 6: Sample parse generated by the collocation
adaptor grammar. The adapted nonterminals Colloct and
Wordt are shown underlined; the subtrees they dominate
are “cached” by the adaptor grammar. The prefix (not
shown here) is parsed exactly as in the Unigram PCFG.
</figureCaption>
<bodyText confidence="0.999912096774194">
mars to generalise over subtrees of arbitrary size.
Generic software is available for adaptor grammar
inference, based either on Variational Bayes (Cohen
et al., 2010) or Markov Chain Monte Carlo (Johnson
and Goldwater, 2009). We used the latter software
because it is capable of performing hyper-parameter
inference for the PCFG rule probabilities and the
Pitman-Yor Process parameters. We used the “out-
of-the-box” settings for this software, i.e., uniform
priors on all PCFG rule parameters, a Beta(2,1)
prior on the Pitman-Yor a parameters and a “vague”
Gamma(100, 0.01) prior on the Pitman-Yor b pa-
rameters. (Presumably performance could be im-
proved if the priors were tuned, but we did not ex-
plore this here).
Here we explore a simple “collocation” extension
to the unigram PCFG which associates multiword
collocations, rather than individual words, with top-
ics. Hardisty et al. (2010) showed that this signifi-
cantly improved performance in a sentiment analy-
sis task.
The collocation adaptor grammar in Figure 5 gen-
erates the words of the utterance as a sequence of
collocations, each of which is a sequence of words.
Each collocation is either associated with the sen-
tence topic or with the None topic, just like words in
the unigram model. Figure 6 shows a sample parse
generated by the collocation adaptor grammar.
We also experimented with a variant of the uni-
gram and collocation grammars in which the topic-
specific word distributions Wordt for each t E T
</bodyText>
<figure confidence="0.998306705882353">
Topic.pig
Collocs.pig
Colloc.None
Words.None
Word.None
Collocs.pig
Colloc.pig
Words.pig
Word
... wheres
Word.None
Word
the
Words.pig
Word.pig
Word
piggie
</figure>
<page confidence="0.985834">
887
</page>
<table confidence="0.9991795">
Model Social acc. Utterance topic rec. Word topic rec. f-score Lexicon rec.
cues f-score prec. f-score prec. prec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.7381
unigram&apos; none 0.3261 0.3767 0.3054 0.4914 0.1893 0.1131 0.5811 0.1167 0.06583 0.5122
unigram&apos; all 0.5117 0.6106 0.4986 0.7875 0.2846 0.1693 0.891 0.1684 0.09402 0.8049
colloc&apos; none 0.5238 0.3419 0.3844 0.3078 0.2551 0.1732 0.4843 0.2162 0.1495 0.3902
colloc&apos; all 0.6492 0.6034 0.6664 0.5514 0.3981 0.2613 0.8354 0.3375 0.2269 0.6585
</table>
<figureCaption confidence="0.997190833333333">
Figure 7: Utterance topic, word topic and lexicon results for all models, on data with and without social cues. The
results for the variant models, in which Wordt nonterminals expand via WordNone, are shown under unigram&apos; and
colloc&apos;. Utterance topic shows how well the model discovered the intended topics at the utterance level, word topic
shows how well the model associates word tokens with topics, and lexicon shows how well the topic most frequently
associated with a word type matches an external word-topic dictionary. In this figure and below, “colloc” abbreviates
“collocation”, “acc.” abbreviates “accuracy”, “prec.” abbreviates “precision” and “rec.” abbreviates “recall”.
</figureCaption>
<bodyText confidence="0.88536125">
(the set of non-None available topics) expand via
WordNone non-terminals. That is, in the variant
grammars topical words are generated with the fol-
lowing rule schema:
</bodyText>
<equation confidence="0.742844333333333">
Wordt -* WordNone Vt E T
WordNone -* Word
Word-*w Vw E W
</equation>
<bodyText confidence="0.999934166666667">
In these variant grammars, the WordNone nontermi-
nal generates all the words of the language, so it de-
fines a generic “background” distribution over all the
words, rather than just the nontopical words. An ef-
fect of this is that the variant grammars tend to iden-
tify fewer words as topical.
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="method">
3 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.985754543859649">
We performed grammatical inference using the
adaptor grammar software described in Johnson and
Goldwater (2009).3 All experiments involved 4 runs
of 5,000 samples each, of which the first 2,500 were
discarded for “burn-in”.4 From these samples we
extracted the modal (i.e., most frequent) analysis,
3Because adaptor grammars are a generalisation of PCFGs,
we could use the adaptor grammar software to estimate the un-
igram model.
4We made no effort to optimise the computation, but it
seems the samplers actually stabilised after around a hundred
iterations, so it was probably not necessary to sample so exten-
sively. We estimated the error in our results by running our most
complex model (the colloc&apos; model with all social cues) 20 times
(i.e., 20×8 chains for 5,000 iterations) so we could compute the
variance of each of the evaluation scores (it is reasonable to as-
sume that the simpler models will have smaller variance). The
standard deviation of all utterance topic and word topic mea-
sures is between 0.005 and 0.01; the standard deviation for lex-
icon f-score is 0.02, lexicon precision is 0.01 and lexicon recall
is 0.03. The adaptor grammar software uses a sentence-wise
which we evaluated as described below. The results
of evaluating each model on the corpus with social
cues, and on another corpus identical except that the
social cues have been removed, are presented in Fig-
ure 7.
Each model was evaluated on each corpus as fol-
lows. First, we extracted the utterance’s topic from
the modal parse (this can be read off the Topict
nodes), and compared this to the intended topics an-
notated in the corpus. The frequency with which
the models’ predicted topics exactly matches the
intended topics is given under “utterance topic ac-
curacy”; the f-score, precision and recall of each
model’s topic predictions are also given in the table.
Because our models all associate word tokens
with topics, we can also evaluate the accuracy with
which word tokens are associated with topics. We
constructed a small dictionary which identifies the
words that can be used as the head of a phrase to
refer to the topical objects (e.g., the dictionary in-
dicates that dog, doggie and puppy name the topi-
cal object DOG). Our dictionary is relatively conser-
vative; between one and eight words are associated
with each topic. We scored the topic label on each
word token in our corpus as follows. A topic label is
scored as correct if it is given in our dictionary and
the topic is one of the intended topics for the utter-
ance. The “word topic” entries in Figure 7 give the
results of this evaluation.
blocked sampler, so it requires fewer iterations than a point-
wise sampler. We used 5,000 iterations because this is the soft-
ware’s default setting; evaluating the trace output suggests it
only takes several hundred iterations to “burn in”. However, we
ran 8 chains for 25,000 iterations of the colloc&apos; model; as ex-
pected the results of this run are within two standard deviations
of the results reported above.
</bodyText>
<page confidence="0.994606">
888
</page>
<table confidence="0.999557571428571">
Model Social acc. Utterance topic rec. Word topic rec. f-score Lexicon rec.
cues f-score prec. f-score prec. prec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram +child.eyes 0.4573 0.5725 0.4559 0.7694 0.2891 0.1724 0.8951 0.1362 0.07415 0.8333
unigram +child.hands 0.3399 0.4011 0.3246 0.5247 0.2008 0.121 0.5892 0.09705 0.05324 0.5476
unigram +mom.eyes 0.338 0.4023 0.3234 0.5322 0.1992 0.1198 0.5908 0.09664 0.053 0.5476
unigram +mom.hands 0.3563 0.4279 0.3437 0.5667 0.1984 0.1191 0.5948 0.09959 0.05455 0.5714
unigram +mom.point 0.3063 0.3548 0.285 0.4698 0.1806 0.1086 0.5359 0.09224 0.05057 0.5238
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc +child.eyes 0.5159 0.5006 0.4652 0.542 0.351 0.2309 0.7312 0.1432 0.07989 0.6905
colloc +child.hands 0.4827 0.4275 0.3999 0.4592 0.2897 0.1913 0.5964 0.1192 0.06686 0.5476
colloc +mom.eyes 0.4697 0.4171 0.3869 0.4525 0.2708 0.1781 0.5642 0.1013 0.05666 0.4762
colloc +mom.hands 0.4747 0.4251 0.3942 0.4612 0.274 0.1806 0.5666 0.09548 0.05337 0.4524
colloc +mom.point 0.4228 0.3378 0.3151 0.3639 0.2575 0.1716 0.5157 0.09278 0.05202 0.4286
</table>
<figureCaption confidence="0.8956105">
Figure 8: Effect of using just one social cue on the experimental results for the unigram and collocation models. The
“importance” of a social cue can be quantified by the degree to which the model’s evaluation score improves when
using a corpus containing that social cue relative to its evaluation score when using a corpus without any social cues.
The most important social cue is the one which causes performance to improve the most.
</figureCaption>
<bodyText confidence="0.999992961538461">
Finally, we extracted a lexicon from the parsed
corpus produced by each model. We counted how
often each word type was associated with each topic
in our sampler’s output (including the None topic),
and assigned the word to its most frequent topic.
The “lexicon” entries in Figure 7 show how well
the entries in these lexicons match the entries in the
manually-constructed dictionary discussed above.
There are 10 different evaluation scores, and no
model dominates in all of them. However, the top-
scoring result in every evaluation is always for a
model trained using social cues, demonstrating the
importance of these social cues. The variant colloca-
tion model (trained on data with social cues) was the
top-scoring model on four evaluation scores, which
is more than any other model.
One striking thing about this evaluation is that the
recall scores are all much higher than the precision
scores, for each evaluation. This indicates that all
of the models, especially the unigram model, are la-
belling too many words as topical. This is perhaps
not too surprising: because our models completely
lack any notion of syntactic structure and simply
model the association between words and topics,
they label many non-nouns with topics (e.g., woof
is typically labelled with the topic DOG).
</bodyText>
<subsectionHeader confidence="0.999672">
3.1 Evaluating the importance of social cues
</subsectionHeader>
<bodyText confidence="0.9982359">
It is scientifically interesting to be able to evalu-
ate the importance of each of the social cues to
grounded learning. One way to do this is to study
the effect of adding or removing social cues from
the corpus on the ability of our models to perform
grounded learning. An important social cue should
have a large impact on our models’ performance; an
unimportant cue should have little or no impact.
Figure 8 compares the performance of the uni-
gram and collocation models on corpora containing
a single social cue to their performance on the cor-
pus without any social cues, while Figure 9 com-
pares the performance of these models on corpora
containing all but one social cue to the corpus con-
taining all of the social cues. In both of these evalua-
tions, with respect to all 10 evaluation measures, the
child.eyes social cue had the most impact on model
performance.
Why would the child’s own gaze be more impor-
tant than the caregiver’s? Perhaps caregivers are fol-
lowing in, i.e., talking about objects that their chil-
dren are interested in (Baldwin, 1991). However, an-
other possible explanation is that this result is due to
the general continuity of conversational topics over
time. Frank et al. (to appear) show that for the cur-
rent corpus, the topic of the preceding utterance is
very likely to be the topic of the current one also.
Thus, the child’s eyes might be a good predictor be-
cause they reflect the fact that the child’s attention
has been drawn to an object by previous utterances.
Notice that these two possible explanations of the
importance of the child.eyes cue are diametrically
opposed; the first explanation claims that the cue is
important because the child is driving the discourse,
while the second explanation claims that the cue is
important because the child’s gaze follows the topic
of the caregiver’s previous utterance. This sort of
question about causal relationships in conversations
may be very difficult to answer using standard de-
scriptive techniques, but it may be an interesting av-
</bodyText>
<page confidence="0.995504">
889
</page>
<table confidence="0.999291071428572">
Model Social acc. Utterance topic rec. Word topic rec. f-score Lexicon rec.
cues f-score prec. f-score prec. prec.
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
unigram −child.eyes 0.3836 0.4659 0.3738 0.6184 0.2149 0.1286 0.6546 0.1111 0.06089 0.6341
unigram −child.hands 0.4907 0.6063 0.4863 0.8051 0.296 0.1769 0.9056 0.1525 0.08353 0.878
unigram −mom.eyes 0.4799 0.5974 0.4768 0.7996 0.2898 0.1727 0.9007 0.1551 0.08486 0.9024
unigram −mom.hands 0.4871 0.5996 0.4815 0.7945 0.2925 0.1746 0.8991 0.1561 0.08545 0.9024
unigram −mom.point 0.4875 0.6033 0.4841 0.8004 0.2934 0.1752 0.9007 0.1558 0.08525 0.9024
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.738
colloc −child.eyes 0.5604 0.5746 0.529 0.6286 0.39 0.2561 0.8176 0.1534 0.08642 0.6829
colloc −child.hands 0.5849 0.6 0.5609 0.6451 0.4145 0.273 0.8612 0.1662 0.09375 0.7317
colloc −mom.eyes 0.5709 0.5829 0.5457 0.6255 0.4036 0.2655 0.8418 0.1662 0.09375 0.7317
colloc −mom.hands 0.5795 0.5935 0.5571 0.6349 0.4038 0.2653 0.8442 0.1788 0.1009 0.7805
colloc −mom.point 0.5851 0.6006 0.5607 0.6467 0.4097 0.2685 0.8644 0.1742 0.09841 0.7561
</table>
<figureCaption confidence="0.938807">
Figure 9: Effect of using all but one social cue on the experimental results for the unigram and collocation models.
The “importance” of a social cue can be quantified by the degree to which the model’s evaluation score degrades when
that just social cue is removed from the corpus, relative to its evaluation score when using a corpus without all social
cues. The most important social cue is the one which causes performance to degrade the most.
</figureCaption>
<bodyText confidence="0.9982655">
enue for future investigation using more structured
models such as those proposed here.5
</bodyText>
<sectionHeader confidence="0.996214" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999657587301588">
This paper presented four different grounded learn-
ing models that exploit social cues. These models
are all expressed via reductions to grammatical in-
ference problems, so standard “off the shelf” gram-
matical inference tools can be used to learn them.
Here we used the same adaptor grammar software
tools to learn all these models, so we can be rel-
atively certain that any differences we observe are
due to differences in the models, rather than quirks
in the software.
Because the adaptor grammar software performs
full Bayesian inference, including for model param-
eters, an unusual feature of our models is that we
did not need to perform any parameter tuning what-
soever. This feature is particularly interesting with
respect to the parameters on social cues. Psycholog-
ical proposals have suggested that children may dis-
cover that particular social cues help in establishing
reference (Baldwin, 1993; Hollich et al., 2000), but
prior modeling work has often assumed that cues,
cue weights, or both are prespecified. In contrast, the
models described here could in principle discover a
wide range of different social conventions.
5A reviewer suggested that we can test whether child.eyes
effectively provides the same information as the previous topic
by adding the previous topic as a (pseudo-) social cue. We tried
this, and child.eyes and previous.topic do in fact seem to convey
very similar information: e.g., the model with previous.topic
and without child.eyes scores essentially the same as the model
with all social cues.
Our work instantiates the strategy of investigating
the structure of children’s learning environment us-
ing “ideal learner” models. We used our models to
investigate scientific questions about the role of so-
cial cues in grounded language learning. Because
the performance of all four models studied in this
paper improve dramatically when provided with so-
cial cues in all ten evaluation metrics, this paper pro-
vides strong support for the view that social cues are
a crucial information source for grounded language
learning.
We also showed that the importance of the differ-
ent social cues in grounded language learning can
be evaluated using “add one cue” and “subtract one
cue” methodologies. According to both of these, the
child.eyes cue is the most important of the five so-
cial cues studied here. There are at least two pos-
sible reasons for this: the caregiver’s topic could
be determined by the child’s gaze, or the child.eyes
cue could be providing our models with information
about the topic of the previous utterance.
Incorporating topic continuity and anaphoric de-
pendencies into our models would be likely to im-
prove performance. This improvement might also
help us distinguish the two hypotheses about the
child.eyes cue. If the child.eyes cue is just provid-
ing indirect information about topic continuity, then
the importance of the child.eyes cue should decrease
when we incorporate topic continuity into our mod-
els. But if the child’s gaze is in fact determining the
care-giver’s topic, then child.eyes should remain a
strong cue even when anaphoric dependencies and
topic continuity are incorporated into our models.
</bodyText>
<page confidence="0.995739">
890
</page>
<sectionHeader confidence="0.997985" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.927179">
This research was supported under the Australian
Research Council’s Discovery Projects funding
scheme (project number DP110102506).
</bodyText>
<sectionHeader confidence="0.996032" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999962431578947">
Dare A. Baldwin. 1991. Infants’ contribution to the
achievement of joint reference. Child Development,
62(5):874–890.
Dare A. Baldwin. 1993. Infants’ ability to consult the
speaker for clues to word reference. Journal of Child
Language, 20:395–395.
Benjamin B¨orschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416–1425, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
M. Carpenter, K. Nagell, M. Tomasello, G. Butterworth,
and C. Moore. 1998. Social cognition, joint attention,
and communicative competence from 9 to 15 months
of age. Monographs of the society for research in child
development.
E.V. Clark. 1987. The principle of contrast: A constraint
on language acquisition. Mechanisms of language ac-
quisition, 1:33.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564–
572, Los Angeles, California, June. Association for
Computational Linguistics.
Michael Frank, Noah Goodman, and Joshua Tenenbaum.
2008. A Bayesian framework for cross-situational
word-learning. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 457–464, Cambridge,
MA. MIT Press.
Michael C. Frank, Joshua Tenenbaum, and Anne Fernald.
to appear. Social and discourse contributions to the
determination of reference in cross-situational word
learning. Language, Learning, and Development.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 284–
292, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
G.J. Hollich, K. Hirsh-Pasek, and R. Golinkoff. 2000.
Breaking the language barrier: An emergentist coali-
tion model for the origins of word learning. Mono-
graphs of the Society for Research in Child Develop-
ment.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the NorthAmeri-
can Chapter of the Association for Computational Lin-
guistics, pages 317–325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641–648. MIT Press, Cambridge, MA.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018–1026.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, pages 398–406, Columbus, Ohio. Association for
Computational Linguistics.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148–1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu.
2003. Foreign-language experience in infancy: Effects
of short-term exposure and social interaction on pho-
netic learning. Proceedings of the National Academy
of Sciences USA, 100(15):9096–9101.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61(1-2):39–91.
L.B. Smith, S.S. Jones, B. Landau, L. Gershkoff-Stowe,
and L. Samuelson. 2002. Object name learning pro-
vides on-the-job training for attention. Psychological
Science, 13(1):13.
Chen Yu and Dana H Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149–2165.
</reference>
<page confidence="0.998605">
891
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.029423">
<title confidence="0.9997775">Exploiting Social Information in Grounded Language Learning Grammatical Reductions</title>
<author confidence="0.998936">Mark</author>
<affiliation confidence="0.7869245">Department of Macquarie</affiliation>
<address confidence="0.371055">Sydney,</address>
<email confidence="0.946365">Mark.Johnson@MQ.edu.au</email>
<author confidence="0.566807">Katherine</author>
<affiliation confidence="0.697227">Department of Macquarie</affiliation>
<address confidence="0.430697">Sydney,</address>
<email confidence="0.955327">Katherine.Demuth@MQ.edu.au</email>
<author confidence="0.947005">Michael</author>
<affiliation confidence="0.796113333333333">Department of Stanford Stanford,</affiliation>
<email confidence="0.99994">mcfrank@Stanford.edu</email>
<abstract confidence="0.998683208333333">This paper uses an unsupervised model of grounded language acquisition to study the role that social cues play in language acquisition. The input to the model consists of (orthographically transcribed) child-directed utterances accompanied by the set of objects present in the non-linguistic context. Each is annotated by indicating e.g., whether the caregiver is looking at or touching the object. We show how to model the task of inferring which objects are being talked about (and which words refer to which objects) as standard grammatical inand describe PCFG-based and adaptor grammar-based collocafor the task. Exploiting social cues improves the performance of all models. Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consistent with the idea that children could discover the importance of particular social information sources during word learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dare A Baldwin</author>
</authors>
<title>Infants’ contribution to the achievement of joint reference.</title>
<date>1991</date>
<journal>Child Development,</journal>
<volume>62</volume>
<issue>5</issue>
<contexts>
<context position="7859" citStr="Baldwin, 1991" startWordPosition="1238" endWordPosition="1239"> which our models currently ignore. Frank et al. (to appear) give extensive details on the corpus, including inter-annotator reliability information for all annotations, and provide detailed statistical analyses of the relationships between the various social cues, the available topics and the intended topics. That paper also gives instructions on obtaining the corpus. 1.2 Previous work There is a growing body of work on the role of social cues in language acquisition. The language acquisition research community has long recognized the importance of social cues for child language acquisition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003). Siskind (1996) describes one of the first examples of a model that learns the relationship between words and topics, albeit in a non-statistical framework. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The relative importance of the various social cues are specified a priori in their model (rather than learned, as they are here), and unfortunately their training corpus is not available. Frank et al. (2008) describes a Bayesian model that learns the relation</context>
<context position="27589" citStr="Baldwin, 1991" startWordPosition="4465" endWordPosition="4466">unigram and collocation models on corpora containing a single social cue to their performance on the corpus without any social cues, while Figure 9 compares the performance of these models on corpora containing all but one social cue to the corpus containing all of the social cues. In both of these evaluations, with respect to all 10 evaluation measures, the child.eyes social cue had the most impact on model performance. Why would the child’s own gaze be more important than the caregiver’s? Perhaps caregivers are following in, i.e., talking about objects that their children are interested in (Baldwin, 1991). However, another possible explanation is that this result is due to the general continuity of conversational topics over time. Frank et al. (to appear) show that for the current corpus, the topic of the preceding utterance is very likely to be the topic of the current one also. Thus, the child’s eyes might be a good predictor because they reflect the fact that the child’s attention has been drawn to an object by previous utterances. Notice that these two possible explanations of the importance of the child.eyes cue are diametrically opposed; the first explanation claims that the cue is impor</context>
</contexts>
<marker>Baldwin, 1991</marker>
<rawString>Dare A. Baldwin. 1991. Infants’ contribution to the achievement of joint reference. Child Development, 62(5):874–890.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dare A Baldwin</author>
</authors>
<title>Infants’ ability to consult the speaker for clues to word reference.</title>
<date>1993</date>
<journal>Journal of Child Language,</journal>
<pages>20--395</pages>
<contexts>
<context position="1565" citStr="Baldwin, 1993" startWordPosition="216" endWordPosition="217">dard grammatical inference, and describe PCFG-based unigram models and adaptor grammar-based collocation models for the task. Exploiting social cues improves the performance of all models. Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consistent with the idea that children could discover the importance of particular social information sources during word learning. 1 Introduction From learning sounds to learning the meanings of words, social interactions are extremely important for children’s early language acquisition (Baldwin, 1993; Kuhl et al., 2003). For example, children who engage in more joint attention (e.g. looking at particular objects together) with caregivers tend to learn words faster (Carpenter et al., 1998). Yet computational or formal models of social interaction are rare, and those that exist have rarely gone beyond the stage of cue-weighting models. In order to study the role that social cues play in language acquisition, this paper presents a structured statistical model of grounded learning that learns a mapping between words and objects from a corpus of child-directed utterances in a completely unsupe</context>
<context position="31185" citStr="Baldwin, 1993" startWordPosition="5027" endWordPosition="5028">r software tools to learn all these models, so we can be relatively certain that any differences we observe are due to differences in the models, rather than quirks in the software. Because the adaptor grammar software performs full Bayesian inference, including for model parameters, an unusual feature of our models is that we did not need to perform any parameter tuning whatsoever. This feature is particularly interesting with respect to the parameters on social cues. Psychological proposals have suggested that children may discover that particular social cues help in establishing reference (Baldwin, 1993; Hollich et al., 2000), but prior modeling work has often assumed that cues, cue weights, or both are prespecified. In contrast, the models described here could in principle discover a wide range of different social conventions. 5A reviewer suggested that we can test whether child.eyes effectively provides the same information as the previous topic by adding the previous topic as a (pseudo-) social cue. We tried this, and child.eyes and previous.topic do in fact seem to convey very similar information: e.g., the model with previous.topic and without child.eyes scores essentially the same as t</context>
</contexts>
<marker>Baldwin, 1993</marker>
<rawString>Dare A. Baldwin. 1993. Infants’ ability to consult the speaker for clues to word reference. Journal of Child Language, 20:395–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Bevan K Jones</author>
<author>Mark Johnson</author>
</authors>
<title>Reducing grounded learning tasks to grammatical inference.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1416--1425</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>B¨orschinger, Jones, Johnson, 2011</marker>
<rawString>Benjamin B¨orschinger, Bevan K. Jones, and Mark Johnson. 2011. Reducing grounded learning tasks to grammatical inference. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416–1425, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpenter</author>
<author>K Nagell</author>
<author>M Tomasello</author>
<author>G Butterworth</author>
<author>C Moore</author>
</authors>
<title>Social cognition, joint attention, and communicative competence from 9 to 15 months of age. Monographs of the society for research in child development.</title>
<date>1998</date>
<contexts>
<context position="1757" citStr="Carpenter et al., 1998" startWordPosition="245" endWordPosition="248">odels. Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consistent with the idea that children could discover the importance of particular social information sources during word learning. 1 Introduction From learning sounds to learning the meanings of words, social interactions are extremely important for children’s early language acquisition (Baldwin, 1993; Kuhl et al., 2003). For example, children who engage in more joint attention (e.g. looking at particular objects together) with caregivers tend to learn words faster (Carpenter et al., 1998). Yet computational or formal models of social interaction are rare, and those that exist have rarely gone beyond the stage of cue-weighting models. In order to study the role that social cues play in language acquisition, this paper presents a structured statistical model of grounded learning that learns a mapping between words and objects from a corpus of child-directed utterances in a completely unsupervised fashion. It exploits five different social cues, which indicate which object (if any) the child is looking at, which object the child is touching, etc. Our models learn the salience of </context>
<context position="7883" citStr="Carpenter et al., 1998" startWordPosition="1240" endWordPosition="1243">ls currently ignore. Frank et al. (to appear) give extensive details on the corpus, including inter-annotator reliability information for all annotations, and provide detailed statistical analyses of the relationships between the various social cues, the available topics and the intended topics. That paper also gives instructions on obtaining the corpus. 1.2 Previous work There is a growing body of work on the role of social cues in language acquisition. The language acquisition research community has long recognized the importance of social cues for child language acquisition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003). Siskind (1996) describes one of the first examples of a model that learns the relationship between words and topics, albeit in a non-statistical framework. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The relative importance of the various social cues are specified a priori in their model (rather than learned, as they are here), and unfortunately their training corpus is not available. Frank et al. (2008) describes a Bayesian model that learns the relationship between words and t</context>
</contexts>
<marker>Carpenter, Nagell, Tomasello, Butterworth, Moore, 1998</marker>
<rawString>M. Carpenter, K. Nagell, M. Tomasello, G. Butterworth, and C. Moore. 1998. Social cognition, joint attention, and communicative competence from 9 to 15 months of age. Monographs of the society for research in child development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E V Clark</author>
</authors>
<title>The principle of contrast: A constraint on language acquisition. Mechanisms of language acquisition,</title>
<date>1987</date>
<pages>1--33</pages>
<contexts>
<context position="5153" citStr="Clark, 1987" startWordPosition="798" endWordPosition="799">noun phrases (e.g., the dog), rather than nouns, refer to specific objects, and nouns denote properties (e.g., dog denotes the property of being a dog). Learning that a noun, e.g., dog, is part of a phrase used to refer to a specific dog (say, Fido) does not suffice to determine the noun’s meaning: the noun could denote a specific breed of dog, or animals in general. But learning word-object relationships is a plausible first step for any learner: it is often only the contrast between learned relationships and novel relationships that allows children to induce super- or sub-ordinate mappings (Clark, 1987). Nevertheless, in deference to such objections, we call the object that a phrase containing a given noun refers to the topic of that noun. (This is also appropriate, given that our models are specialisations of topic models). Our models are intended as an “ideal learner” approach to early social language learning, attempting to weight the importance of social and structural factors in the acquisition of word-object correspondences. From this perspective, the primary goal is to investigate the relationships between acquisition tasks (Johnson, 2008; Johnson et al., 2010), looking for synergies </context>
</contexts>
<marker>Clark, 1987</marker>
<rawString>E.V. Clark. 1987. The principle of contrast: A constraint on language acquisition. Mechanisms of language acquisition, 1:33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>David M Blei</author>
<author>Noah A Smith</author>
</authors>
<title>Variational inference for adaptor grammars. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>564--572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="17115" citStr="Cohen et al., 2010" startWordPosition="2774" endWordPosition="2777">dictive distributions for Dirichlet Processes. Our adaptor grammars are actually based on the more general Pitman-Yor Processes, as described in Johnson and Goldwater (2009). Sentence Figure 6: Sample parse generated by the collocation adaptor grammar. The adapted nonterminals Colloct and Wordt are shown underlined; the subtrees they dominate are “cached” by the adaptor grammar. The prefix (not shown here) is parsed exactly as in the Unigram PCFG. mars to generalise over subtrees of arbitrary size. Generic software is available for adaptor grammar inference, based either on Variational Bayes (Cohen et al., 2010) or Markov Chain Monte Carlo (Johnson and Goldwater, 2009). We used the latter software because it is capable of performing hyper-parameter inference for the PCFG rule probabilities and the Pitman-Yor Process parameters. We used the “outof-the-box” settings for this software, i.e., uniform priors on all PCFG rule parameters, a Beta(2,1) prior on the Pitman-Yor a parameters and a “vague” Gamma(100, 0.01) prior on the Pitman-Yor b parameters. (Presumably performance could be improved if the priors were tuned, but we did not explore this here). Here we explore a simple “collocation” extension to </context>
</contexts>
<marker>Cohen, Blei, Smith, 2010</marker>
<rawString>Shay B. Cohen, David M. Blei, and Noah A. Smith. 2010. Variational inference for adaptor grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 564– 572, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Frank</author>
<author>Noah Goodman</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>A Bayesian framework for cross-situational word-learning.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>457--464</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8407" citStr="Frank et al. (2008)" startWordPosition="1329" endWordPosition="1332">importance of social cues for child language acquisition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003). Siskind (1996) describes one of the first examples of a model that learns the relationship between words and topics, albeit in a non-statistical framework. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The relative importance of the various social cues are specified a priori in their model (rather than learned, as they are here), and unfortunately their training corpus is not available. Frank et al. (2008) describes a Bayesian model that learns the relationship between words and topics, but the version of their model that included social cues presented a number of challenges for inference. The unigram model we describe below corresponds most closely to the Frank 884 .dog # .pig child.eyes mom.eyes mom.hands # ## wheres the piggie Figure 2: The photograph indicates non-linguistic context containing a (toy) pig and dog for the utterance Where’s the piggie?. Below that, we show the representation of this utterance that serves as the input to our models. The prefix (the portion of the string before</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2008</marker>
<rawString>Michael Frank, Noah Goodman, and Joshua Tenenbaum. 2008. A Bayesian framework for cross-situational word-learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 457–464, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael C Frank</author>
<author>Joshua Tenenbaum</author>
<author>Anne Fernald</author>
</authors>
<title>to appear. Social and discourse contributions to the determination of reference</title>
<booktitle>in cross-situational word learning. Language, Learning, and Development.</booktitle>
<marker>Frank, Tenenbaum, Fernald, </marker>
<rawString>Michael C. Frank, Joshua Tenenbaum, and Anne Fernald. to appear. Social and discourse contributions to the determination of reference in cross-situational word learning. Language, Learning, and Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric A Hardisty</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Modeling perspective using adaptor grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284--292</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17838" citStr="Hardisty et al. (2010)" startWordPosition="2888" endWordPosition="2891"> capable of performing hyper-parameter inference for the PCFG rule probabilities and the Pitman-Yor Process parameters. We used the “outof-the-box” settings for this software, i.e., uniform priors on all PCFG rule parameters, a Beta(2,1) prior on the Pitman-Yor a parameters and a “vague” Gamma(100, 0.01) prior on the Pitman-Yor b parameters. (Presumably performance could be improved if the priors were tuned, but we did not explore this here). Here we explore a simple “collocation” extension to the unigram PCFG which associates multiword collocations, rather than individual words, with topics. Hardisty et al. (2010) showed that this significantly improved performance in a sentiment analysis task. The collocation adaptor grammar in Figure 5 generates the words of the utterance as a sequence of collocations, each of which is a sequence of words. Each collocation is either associated with the sentence topic or with the None topic, just like words in the unigram model. Figure 6 shows a sample parse generated by the collocation adaptor grammar. We also experimented with a variant of the unigram and collocation grammars in which the topicspecific word distributions Wordt for each t E T Topic.pig Collocs.pig Co</context>
</contexts>
<marker>Hardisty, Boyd-Graber, Resnik, 2010</marker>
<rawString>Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik. 2010. Modeling perspective using adaptor grammars. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284– 292, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Hollich</author>
<author>K Hirsh-Pasek</author>
<author>R Golinkoff</author>
</authors>
<title>Breaking the language barrier: An emergentist coalition model for the origins of word learning.</title>
<date>2000</date>
<journal>Monographs of the Society for Research in Child Development.</journal>
<contexts>
<context position="2695" citStr="Hollich et al., 2000" startWordPosition="397" endWordPosition="400">etween words and objects from a corpus of child-directed utterances in a completely unsupervised fashion. It exploits five different social cues, which indicate which object (if any) the child is looking at, which object the child is touching, etc. Our models learn the salience of each social cue in establishing reference, relative to their co-occurrence with objects that are not being referred to. Thus, this work is consistent with a view of language acquisition in which children learn to learn, discovering organizing principles for how language is organized and used socially (Baldwin, 1993; Hollich et al., 2000; Smith et al., 2002). We reduce the grounded learning task to a grammatical inference problem (Johnson et al., 2010; B¨orschinger et al., 2011). The strings presented to our grammatical learner contain a prefix which encodes the objects and their social cues for each utterance, and the rules of the grammar encode relationships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no “stop word” list), and the learning process decides which of these rules will have a non-trivial probability (these encode the obj</context>
<context position="31208" citStr="Hollich et al., 2000" startWordPosition="5029" endWordPosition="5032">s to learn all these models, so we can be relatively certain that any differences we observe are due to differences in the models, rather than quirks in the software. Because the adaptor grammar software performs full Bayesian inference, including for model parameters, an unusual feature of our models is that we did not need to perform any parameter tuning whatsoever. This feature is particularly interesting with respect to the parameters on social cues. Psychological proposals have suggested that children may discover that particular social cues help in establishing reference (Baldwin, 1993; Hollich et al., 2000), but prior modeling work has often assumed that cues, cue weights, or both are prespecified. In contrast, the models described here could in principle discover a wide range of different social conventions. 5A reviewer suggested that we can test whether child.eyes effectively provides the same information as the previous topic by adding the previous topic as a (pseudo-) social cue. We tried this, and child.eyes and previous.topic do in fact seem to convey very similar information: e.g., the model with previous.topic and without child.eyes scores essentially the same as the model with all socia</context>
</contexts>
<marker>Hollich, Hirsh-Pasek, Golinkoff, 2000</marker>
<rawString>G.J. Hollich, K. Hirsh-Pasek, and R. Golinkoff. 2000. Breaking the language barrier: An emergentist coalition model for the origins of word learning. Monographs of the Society for Research in Child Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3586" citStr="Johnson and Goldwater (2009)" startWordPosition="543" endWordPosition="546"> each utterance, and the rules of the grammar encode relationships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no “stop word” list), and the learning process decides which of these rules will have a non-trivial probability (these encode the object-word mappings the system has learned). This reduction of grounded learning to grammatical inference allows us to use standard grammatical inference procedures to learn our models. Here we use the adaptor grammar package described in Johnson et al. (2007) and Johnson and Goldwater (2009) with “out of the box” default settings; no parameter tuning whatsoever was done. Adaptor grammars are a framework for specifying hierarchical non-parametric models that has been previously used to model language acquisition (Johnson, 2008). 883 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883–891, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Social cue Value child.eyes objects child is looking at child.hands objects child is touching mom.eyes objects care-giver is looking at mom.hands objects care-g</context>
<context position="16669" citStr="Johnson and Goldwater (2009)" startWordPosition="2706" endWordPosition="2709"> expand using a grammar rule, just as in a PCFG, with probability proportional to a constant.2 Thus an adaptor grammar can be viewed as caching each tree generated by each adapted nonterminal, and regenerating it with probability proportional to the number of times it was previously generated (with some probability mass reserved to generate “new” trees). This enables adaptor gram2This is a description of Chinese Restaurant Processes, which are the predictive distributions for Dirichlet Processes. Our adaptor grammars are actually based on the more general Pitman-Yor Processes, as described in Johnson and Goldwater (2009). Sentence Figure 6: Sample parse generated by the collocation adaptor grammar. The adapted nonterminals Colloct and Wordt are shown underlined; the subtrees they dominate are “cached” by the adaptor grammar. The prefix (not shown here) is parsed exactly as in the Unigram PCFG. mars to generalise over subtrees of arbitrary size. Generic software is available for adaptor grammar inference, based either on Variational Bayes (Cohen et al., 2010) or Markov Chain Monte Carlo (Johnson and Goldwater, 2009). We used the latter software because it is capable of performing hyper-parameter inference for </context>
<context position="20686" citStr="Johnson and Goldwater (2009)" startWordPosition="3331" endWordPosition="3334">ailable topics) expand via WordNone non-terminals. That is, in the variant grammars topical words are generated with the following rule schema: Wordt -* WordNone Vt E T WordNone -* Word Word-*w Vw E W In these variant grammars, the WordNone nonterminal generates all the words of the language, so it defines a generic “background” distribution over all the words, rather than just the nontopical words. An effect of this is that the variant grammars tend to identify fewer words as topical. 3 Experimental evaluation We performed grammatical inference using the adaptor grammar software described in Johnson and Goldwater (2009).3 All experiments involved 4 runs of 5,000 samples each, of which the first 2,500 were discarded for “burn-in”.4 From these samples we extracted the modal (i.e., most frequent) analysis, 3Because adaptor grammars are a generalisation of PCFGs, we could use the adaptor grammar software to estimate the unigram model. 4We made no effort to optimise the computation, but it seems the samplers actually stabilised after around a hundred iterations, so it was probably not necessary to sample so extensively. We estimated the error in our results by running our most complex model (the colloc&apos; model wit</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3553" citStr="Johnson et al. (2007)" startWordPosition="538" endWordPosition="541"> and their social cues for each utterance, and the rules of the grammar encode relationships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no “stop word” list), and the learning process decides which of these rules will have a non-trivial probability (these encode the object-word mappings the system has learned). This reduction of grounded learning to grammatical inference allows us to use standard grammatical inference procedures to learn our models. Here we use the adaptor grammar package described in Johnson et al. (2007) and Johnson and Goldwater (2009) with “out of the box” default settings; no parameter tuning whatsoever was done. Adaptor grammars are a framework for specifying hierarchical non-parametric models that has been previously used to model language acquisition (Johnson, 2008). 883 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883–891, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Social cue Value child.eyes objects child is looking at child.hands objects child is touching mom.eyes objects care-giver is lo</context>
<context position="15312" citStr="Johnson et al. (2007)" startWordPosition="2480" endWordPosition="2483">rule schema that generate the collocation adaptor grammar. Adapted nonterminals are indicated via underlining. Here T is the set of all non-None available topics, T&apos; = T U {None}, and W is the set of words appearing in the utterances. The rules expanding the Topict nonterminals are exactly as in unigram PCFG. 2.2 Adaptor grammars Our other grounded learning models are based on reductions of grounded learning to adaptor grammar inference problems. Adaptor grammars are a framework for stating a variety of Bayesian nonparametric models defined in terms of a hierarchy of Pitman-Yor Processes: see Johnson et al. (2007) for a formal description. Informally, an adaptor grammar is specified by a set of rules just as in a PCFG, plus a set of adapted nonterminals. The set of trees generated by an adaptor grammar is the same as the set of trees generated by a PCFG with the same rules, but the generative process differs. Nonadapted nonterminals in an adaptor grammar expand just as they do in a PCFG: the probability of choosing a rule is specified by its probability. However, the expansion of an adapted nonterminal depends on how it expanded in previous derivations. An adapted nonterminal can directly expand to a s</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Katherine Demuth</author>
<author>Michael Frank</author>
<author>Bevan Jones</author>
</authors>
<title>Synergies in learning words and their referents.</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems 23,</booktitle>
<pages>1018--1026</pages>
<editor>In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,</editor>
<contexts>
<context position="2811" citStr="Johnson et al., 2010" startWordPosition="417" endWordPosition="420">s five different social cues, which indicate which object (if any) the child is looking at, which object the child is touching, etc. Our models learn the salience of each social cue in establishing reference, relative to their co-occurrence with objects that are not being referred to. Thus, this work is consistent with a view of language acquisition in which children learn to learn, discovering organizing principles for how language is organized and used socially (Baldwin, 1993; Hollich et al., 2000; Smith et al., 2002). We reduce the grounded learning task to a grammatical inference problem (Johnson et al., 2010; B¨orschinger et al., 2011). The strings presented to our grammatical learner contain a prefix which encodes the objects and their social cues for each utterance, and the rules of the grammar encode relationships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no “stop word” list), and the learning process decides which of these rules will have a non-trivial probability (these encode the object-word mappings the system has learned). This reduction of grounded learning to grammatical inference allows us to</context>
<context position="5729" citStr="Johnson et al., 2010" startWordPosition="887" endWordPosition="890">super- or sub-ordinate mappings (Clark, 1987). Nevertheless, in deference to such objections, we call the object that a phrase containing a given noun refers to the topic of that noun. (This is also appropriate, given that our models are specialisations of topic models). Our models are intended as an “ideal learner” approach to early social language learning, attempting to weight the importance of social and structural factors in the acquisition of word-object correspondences. From this perspective, the primary goal is to investigate the relationships between acquisition tasks (Johnson, 2008; Johnson et al., 2010), looking for synergies (areas of acquisition where attempting two learning tasks jointly can provide gains in both) as well as areas where information overlaps. 1.1 A training corpus for social cues Our work here uses a corpus of child-directed speech annotated with social cues, described in Frank et al. (to appear). The corpus consists of 4,763 orthographically-transcribed utterances of caregivers to their pre-linguistic children (ages 6, 12, and 18 months) during home visits where children played with a consistent set of toys. The sessions were video-taped, and each utterance was annotated </context>
<context position="10170" citStr="Johnson et al. (2010)" startWordPosition="1589" endWordPosition="1592"> “##” separator) into the utterance. The social cues associated with each object are generated either from a “Topical” or a “NotTopical” nonterminal, depending on whether the corresponding object is topical or not. # # ## the .pig .dog piggie wheres child.eyes mom.hands Topic.pig Words.pig Sentence T.None Word.None Words.pig NotTopical.child.eyes NotTopical.child.hands NotTopical.mom.eyes NotTopical.mom.hands NotTopical.mom.point Topical.child.eyes Topical.child.hands Topical.mom.eyes Topical.mom.hands Topic.None T.pig Topical.mom.point Topic.pig Word.None Words.pig Word.pig 885 et al. model. Johnson et al. (2010) reduces grounded learning to grammatical inference for adaptor grammars and shows how it can be used to perform word segmentation as well as learning word-topic relationships, but their model does not take social cues into account. 2 Reducing grounded learning with social cues to grammatical inference This section explains how we reduce ground learning problems with social cues to grammatical inference problems, which lets us apply a wide variety of grammatical inference algorithms to grounded learning problems. An advantage of reducing grounded learning to grammatical inference is that it su</context>
<context position="11965" citStr="Johnson et al. (2010)" startWordPosition="1888" endWordPosition="1891">each utterance with zero or one topics (this means we cannot correctly analyse utterances with more than one intended topic). We analyse an utterance associated with zero topics as having the special topic None, so we can assume that every utterance has exactly one topic. All our grammars generate strings of the form shown in Figure 2, and they do so by parsing the prefix and the words of the utterance separately; the top-level rules of the grammar force the same topic to be associated with both the prefix and the words of the utterance (see Figure 3). 2.1 Topic models and the unigram PCFG As Johnson et al. (2010) observe, this kind of grounded learning can be viewed as a specialised kind of topic inference in a topic model, where the utterance topic is constrained by the available objects (possible topics). We exploit this observation here using a reduction based on the reduction of LDA topic models to PCFGs proposed by Johnson (2010). This leads to our first model, the unigram grammar, which is a PCFG.1 Sentence —* Topict Wordst dt E T1 TopicNone —* ## Topict —* Tt TopicNone dt E T1 Topict —* TNone Topict dt E T Tt —* t Topicalc1 dt E T Topicalci —* (ci) Topicalci+1 i = �, . . . , ` � 1 Topicalc` —* </context>
</contexts>
<marker>Johnson, Demuth, Frank, Jones, 2010</marker>
<rawString>Mark Johnson, Katherine Demuth, Michael Frank, and Bevan Jones. 2010. Synergies in learning words and their referents. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1018–1026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>398--406</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3826" citStr="Johnson, 2008" startWordPosition="580" endWordPosition="581">ides which of these rules will have a non-trivial probability (these encode the object-word mappings the system has learned). This reduction of grounded learning to grammatical inference allows us to use standard grammatical inference procedures to learn our models. Here we use the adaptor grammar package described in Johnson et al. (2007) and Johnson and Goldwater (2009) with “out of the box” default settings; no parameter tuning whatsoever was done. Adaptor grammars are a framework for specifying hierarchical non-parametric models that has been previously used to model language acquisition (Johnson, 2008). 883 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883–891, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Social cue Value child.eyes objects child is looking at child.hands objects child is touching mom.eyes objects care-giver is looking at mom.hands objects care-giver is touching mom.point objects care-giver is pointing to Figure 1: The 5 social cues in the Frank et al. (to appear) corpus. The value of a social cue for an utterance is a subset of the available topics (i.e., the objects in the nonlin</context>
<context position="5706" citStr="Johnson, 2008" startWordPosition="885" endWordPosition="886">dren to induce super- or sub-ordinate mappings (Clark, 1987). Nevertheless, in deference to such objections, we call the object that a phrase containing a given noun refers to the topic of that noun. (This is also appropriate, given that our models are specialisations of topic models). Our models are intended as an “ideal learner” approach to early social language learning, attempting to weight the importance of social and structural factors in the acquisition of word-object correspondences. From this perspective, the primary goal is to investigate the relationships between acquisition tasks (Johnson, 2008; Johnson et al., 2010), looking for synergies (areas of acquisition where attempting two learning tasks jointly can provide gains in both) as well as areas where information overlaps. 1.1 A training corpus for social cues Our work here uses a corpus of child-directed speech annotated with social cues, described in Frank et al. (to appear). The corpus consists of 4,763 orthographically-transcribed utterances of caregivers to their pre-linguistic children (ages 6, 12, and 18 months) during home visits where children played with a consistent set of toys. The sessions were video-taped, and each u</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, pages 398–406, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1148--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12293" citStr="Johnson (2010)" startWordPosition="1945" endWordPosition="1946">2, and they do so by parsing the prefix and the words of the utterance separately; the top-level rules of the grammar force the same topic to be associated with both the prefix and the words of the utterance (see Figure 3). 2.1 Topic models and the unigram PCFG As Johnson et al. (2010) observe, this kind of grounded learning can be viewed as a specialised kind of topic inference in a topic model, where the utterance topic is constrained by the available objects (possible topics). We exploit this observation here using a reduction based on the reduction of LDA topic models to PCFGs proposed by Johnson (2010). This leads to our first model, the unigram grammar, which is a PCFG.1 Sentence —* Topict Wordst dt E T1 TopicNone —* ## Topict —* Tt TopicNone dt E T1 Topict —* TNone Topict dt E T Tt —* t Topicalc1 dt E T Topicalci —* (ci) Topicalci+1 i = �, . . . , ` � 1 Topicalc` —* (ct) # TNone —* t NotTopicalc1 dt E T NotTopicalci —* (ci) NotTopicalci+1 i = �, . . . , ` � 1 NotTopicalc` —* (ct) # Wordst —* WordNone (Wordst) dt E T1 Wordst —* Wordt (Wordst) dt E T Wordt —* w dt E T1, w E W Figure 4: The rule schema that generate the unigram PCFG. Here (cl, ... , cj) is an ordered list of the social cues,</context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patricia K Kuhl</author>
<author>Feng-Ming Tsao</author>
<author>Huei-Mei Liu</author>
</authors>
<title>Foreign-language experience in infancy: Effects of short-term exposure and social interaction on phonetic learning.</title>
<date>2003</date>
<booktitle>Proceedings of the National Academy of Sciences USA,</booktitle>
<pages>100--15</pages>
<contexts>
<context position="1585" citStr="Kuhl et al., 2003" startWordPosition="218" endWordPosition="221">l inference, and describe PCFG-based unigram models and adaptor grammar-based collocation models for the task. Exploiting social cues improves the performance of all models. Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consistent with the idea that children could discover the importance of particular social information sources during word learning. 1 Introduction From learning sounds to learning the meanings of words, social interactions are extremely important for children’s early language acquisition (Baldwin, 1993; Kuhl et al., 2003). For example, children who engage in more joint attention (e.g. looking at particular objects together) with caregivers tend to learn words faster (Carpenter et al., 1998). Yet computational or formal models of social interaction are rare, and those that exist have rarely gone beyond the stage of cue-weighting models. In order to study the role that social cues play in language acquisition, this paper presents a structured statistical model of grounded learning that learns a mapping between words and objects from a corpus of child-directed utterances in a completely unsupervised fashion. It e</context>
<context position="7903" citStr="Kuhl et al., 2003" startWordPosition="1244" endWordPosition="1247">nk et al. (to appear) give extensive details on the corpus, including inter-annotator reliability information for all annotations, and provide detailed statistical analyses of the relationships between the various social cues, the available topics and the intended topics. That paper also gives instructions on obtaining the corpus. 1.2 Previous work There is a growing body of work on the role of social cues in language acquisition. The language acquisition research community has long recognized the importance of social cues for child language acquisition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003). Siskind (1996) describes one of the first examples of a model that learns the relationship between words and topics, albeit in a non-statistical framework. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The relative importance of the various social cues are specified a priori in their model (rather than learned, as they are here), and unfortunately their training corpus is not available. Frank et al. (2008) describes a Bayesian model that learns the relationship between words and topics, but the versi</context>
</contexts>
<marker>Kuhl, Tsao, Liu, 2003</marker>
<rawString>Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu. 2003. Foreign-language experience in infancy: Effects of short-term exposure and social interaction on phonetic learning. Proceedings of the National Academy of Sciences USA, 100(15):9096–9101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Siskind</author>
</authors>
<title>A computational study of crosssituational techniques for learning word-to-meaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="7919" citStr="Siskind (1996)" startWordPosition="1248" endWordPosition="1249">) give extensive details on the corpus, including inter-annotator reliability information for all annotations, and provide detailed statistical analyses of the relationships between the various social cues, the available topics and the intended topics. That paper also gives instructions on obtaining the corpus. 1.2 Previous work There is a growing body of work on the role of social cues in language acquisition. The language acquisition research community has long recognized the importance of social cues for child language acquisition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003). Siskind (1996) describes one of the first examples of a model that learns the relationship between words and topics, albeit in a non-statistical framework. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The relative importance of the various social cues are specified a priori in their model (rather than learned, as they are here), and unfortunately their training corpus is not available. Frank et al. (2008) describes a Bayesian model that learns the relationship between words and topics, but the version of their mode</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey Siskind. 1996. A computational study of crosssituational techniques for learning word-to-meaning mappings. Cognition, 61(1-2):39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L B Smith</author>
<author>S S Jones</author>
<author>B Landau</author>
<author>L Gershkoff-Stowe</author>
<author>L Samuelson</author>
</authors>
<title>Object name learning provides on-the-job training for attention.</title>
<date>2002</date>
<journal>Psychological Science,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="2716" citStr="Smith et al., 2002" startWordPosition="401" endWordPosition="404">ts from a corpus of child-directed utterances in a completely unsupervised fashion. It exploits five different social cues, which indicate which object (if any) the child is looking at, which object the child is touching, etc. Our models learn the salience of each social cue in establishing reference, relative to their co-occurrence with objects that are not being referred to. Thus, this work is consistent with a view of language acquisition in which children learn to learn, discovering organizing principles for how language is organized and used socially (Baldwin, 1993; Hollich et al., 2000; Smith et al., 2002). We reduce the grounded learning task to a grammatical inference problem (Johnson et al., 2010; B¨orschinger et al., 2011). The strings presented to our grammatical learner contain a prefix which encodes the objects and their social cues for each utterance, and the rules of the grammar encode relationships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no “stop word” list), and the learning process decides which of these rules will have a non-trivial probability (these encode the object-word mappings the</context>
</contexts>
<marker>Smith, Jones, Landau, Gershkoff-Stowe, Samuelson, 2002</marker>
<rawString>L.B. Smith, S.S. Jones, B. Landau, L. Gershkoff-Stowe, and L. Samuelson. 2002. Object name learning provides on-the-job training for attention. Psychological Science, 13(1):13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>A unified model of early word learning: Integrating statistical and social cues.</title>
<date>2007</date>
<journal>Neurocomputing,</journal>
<pages>70--13</pages>
<contexts>
<context position="8082" citStr="Yu and Ballard (2007)" startWordPosition="1274" endWordPosition="1277">the relationships between the various social cues, the available topics and the intended topics. That paper also gives instructions on obtaining the corpus. 1.2 Previous work There is a growing body of work on the role of social cues in language acquisition. The language acquisition research community has long recognized the importance of social cues for child language acquisition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003). Siskind (1996) describes one of the first examples of a model that learns the relationship between words and topics, albeit in a non-statistical framework. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The relative importance of the various social cues are specified a priori in their model (rather than learned, as they are here), and unfortunately their training corpus is not available. Frank et al. (2008) describes a Bayesian model that learns the relationship between words and topics, but the version of their model that included social cues presented a number of challenges for inference. The unigram model we describe below corresponds most closely to the Frank 884 .dog # .p</context>
</contexts>
<marker>Yu, Ballard, 2007</marker>
<rawString>Chen Yu and Dana H Ballard. 2007. A unified model of early word learning: Integrating statistical and social cues. Neurocomputing, 70(13-15):2149–2165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>