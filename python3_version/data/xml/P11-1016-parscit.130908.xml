<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000070">
<title confidence="0.98062">
Target-dependent Twitter Sentiment Classification
</title>
<author confidence="0.999332">
Long Jiang1 Mo Yu2 Ming Zhou1 Xiaohua Liu1 Tiejun Zhao2
</author>
<affiliation confidence="0.832789">
1 Microsoft Research Asia 2 School of Computer Science &amp; Technology
Beijing, China Harbin Institute of Technology
</affiliation>
<address confidence="0.894126">
Harbin, China
</address>
<email confidence="0.998439">
{longj,mingzhou,xiaoliu}@microsoft.com {yumo,tjzhao}@mtlab.hit.edu.cn
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999664333333333">
Sentiment analysis on Twitter data has attract-
ed much attention recently. In this paper, we
focus on target-dependent Twitter sentiment
classification; namely, given a query, we clas-
sify the sentiments of the tweets as positive,
negative or neutral according to whether they
contain positive, negative or neutral senti-
ments about that query. Here the query serves
as the target of the sentiments. The state-of-
the-art approaches for solving this problem
always adopt the target-independent strategy,
which may assign irrelevant sentiments to the
given target. Moreover, the state-of-the-art
approaches only take the tweet to be classified
into consideration when classifying the senti-
ment; they ignore its context (i.e., related
tweets). However, because tweets are usually
short and more ambiguous, sometimes it is not
enough to consider only the current tweet for
sentiment classification. In this paper, we pro-
pose to improve target-dependent Twitter sen-
timent classification by 1) incorporating
target-dependent features; and 2) taking relat-
ed tweets into consideration. According to the
experimental results, our approach greatly im-
proves the performance of target-dependent
sentiment classification.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955444444444">
Twitter, as a micro-blogging system, allows users
to publish tweets of up to 140 characters in length
to tell others what they are doing, what they are
thinking, or what is happening around them. Over
the past few years, Twitter has become very popu-
lar. According to the latest Twitter entry in Wik-
ipedia, the number of Twitter users has climbed to
190 million and the number of tweets published on
Twitter every day is over 65 million1.
As a result of the rapidly increasing number of
tweets, mining people’s sentiments expressed in
tweets has attracted more and more attention. In
fact, there are already many web sites built on the
Internet providing a Twitter sentiment search ser-
vice, such as Tweetfeel2, Twendz3, and Twitter
Sentiment4. In those web sites, the user can input a
sentiment target as a query, and search for tweets
containing positive or negative sentiments towards
the target. The problem needing to be addressed
can be formally named as Target-dependent Sen-
timent Classification of Tweets; namely, given a
query, classifying the sentiments of the tweets as
positive, negative or neutral according to whether
they contain positive, negative or neutral senti-
ments about that query. Here the query serves as
the target of the sentiments.
The state-of-the-art approaches for solving this
problem, such as (Go et al., 20095; Barbosa and
Feng, 2010), basically follow (Pang et al., 2002),
who utilize machine learning based classifiers for
the sentiment classification of texts. However, their
classifiers actually work in a target-independent
way: all the features used in the classifiers are in-
dependent of the target, so the sentiment is decided
no matter what the target is. Since (Pang et al.,
2002) (or later research on sentiment classification
</bodyText>
<footnote confidence="0.9895902">
1 http://en.wikipedia.org/wiki/Twitter
2 http://www.tweetfeel.com/
3 http://twendz.waggeneredstrom.com/
4 http://twittersentiment.appspot.com/
5 The algorithm used in Twitter Sentiment
</footnote>
<page confidence="0.926138">
151
</page>
<note confidence="0.980094">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 151–160,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998765">
of product reviews) aim to classify the polarities of
movie (or product) reviews and each movie (or
product) review is assumed to express sentiments
only about the target movie (or product), it is rea-
sonable for them to adopt the target-independent
approach. However, for target-dependent sentiment
classification of tweets, it is not suitable to exactly
adopt that approach. Because people may mention
multiple targets in one tweet or comment on a tar-
get in a tweet while saying many other unrelated
things in the same tweet, target-independent ap-
proaches are likely to yield unsatisfactory results:
</bodyText>
<listItem confidence="0.974794263157895">
1. Tweets that do not express any sentiments
to the given target but express sentiments
to other things will be considered as being
opinionated about the target. For example,
the following tweet expresses no sentiment
to Bill Gates but is very likely to be classi-
fied as positive about Bill Gates by target-
independent approaches.
&amp;quot;People everywhere love Windows &amp; vista.
Bill Gates&amp;quot;
2. The polarities of some tweets towards the
given target are misclassified because of
the interference from sentiments towards
other targets in the tweets. For example,
the following tweet expresses a positive
sentiment to Windows 7 and a negative
sentiment to Vista. However, with target-
independent sentiment classification, both
of the targets would get positive polarity.
</listItem>
<figureCaption confidence="0.467574">
“Windows 7 is much better than Vista!”
</figureCaption>
<bodyText confidence="0.999928928571429">
In fact, it is easy to find many such cases by
looking at the output of Twitter Sentiment or other
Twitter sentiment analysis web sites. Based on our
manual evaluation of Twitter Sentiment output,
about 40% of errors are because of this (see Sec-
tion 6.1 for more details).
In addition, tweets are usually shorter and more
ambiguous than other sentiment data commonly
used for sentiment analysis, such as reviews and
blogs. Consequently, it is more difficult to classify
the sentiment of a tweet only based on its content.
For instance, for the following tweet, which con-
tains only three words, it is difficult for any exist-
ing approaches to classify its sentiment correctly.
</bodyText>
<subsubsectionHeader confidence="0.450849">
“First game: Lakers!”
</subsubsectionHeader>
<bodyText confidence="0.999978488372093">
However, relations between individual tweets
are more common than those in other sentiment
data. We can easily find many related tweets of a
given tweet, such as the tweets published by the
same person, the tweets replying to or replied by
the given tweet, and retweets of the given tweet.
These related tweets provide rich information
about what the given tweet expresses and should
definitely be taken into consideration for classify-
ing the sentiment of the given tweet.
In this paper, we propose to improve target-
dependent sentiment classification of tweets by
using both target-dependent and context-aware
approaches. Specifically, the target-dependent ap-
proach refers to incorporating syntactic features
generated using words syntactically connected
with the given target in the tweet to decide whether
or not the sentiment is about the given target. For
instance, in the second example, using syntactic
parsing, we know that “Windows 7” is connected
to “better” by a copula, while “Vista” is connected
to “better” by a preposition. By learning from
training data, we can probably predict that “Win-
dows 7” should get a positive sentiment and
“Vista” should get a negative sentiment.
In addition, we also propose to incorporate the
contexts of tweets into classification, which we call
a context-aware approach. By considering the sen-
timent labels of the related tweets, we can further
boost the performance of the sentiment classifica-
tion, especially for very short and ambiguous
tweets. For example, in the third example we men-
tioned above, if we find that the previous and fol-
lowing tweets published by the same person are
both positive about the Lakers, we can confidently
classify this tweet as positive.
The remainder of this paper is structured as fol-
lows. In Section 2, we briefly summarize related
work. Section 3 gives an overview of our approach.
We explain the target-dependent and context-
aware approaches in detail in Sections 4 and 5 re-
spectively. Experimental results are reported in
Section 6 and Section 7 concludes our work.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999732">
In recent years, sentiment analysis (SA) has be-
come a hot topic in the NLP research community.
A lot of papers have been published on this topic.
</bodyText>
<page confidence="0.996968">
152
</page>
<subsectionHeader confidence="0.882992">
2.1 Target-independent SA
</subsectionHeader>
<bodyText confidence="0.999938952380952">
Specifically, Turney (2002) proposes an unsuper-
vised method for classifying product or movie re-
views as positive or negative. In this method,
sentimental phrases are first selected from the re-
views according to predefined part-of-speech pat-
terns. Then the semantic orientation score of each
phrase is calculated according to the mutual infor-
mation values between the phrase and two prede-
fined seed words. Finally, a review is classified
based on the average semantic orientation of the
sentimental phrases in the review.
In contrast, (Pang et al., 2002) treat the senti-
ment classification of movie reviews simply as a
special case of a topic-based text categorization
problem and investigate three classification algo-
rithms: Naive Bayes, Maximum Entropy, and Sup-
port Vector Machines. According to the
experimental results, machine learning based clas-
sifiers outperform the unsupervised approach,
where the best performance is achieved by the
SVM classifier with unigram presences as features.
</bodyText>
<subsectionHeader confidence="0.99414">
2.2 Target-dependent SA
</subsectionHeader>
<bodyText confidence="0.99981912">
Besides the above mentioned work for target-
independent sentiment classification, there are also
several approaches proposed for target-dependent
classification, such as (Nasukawa and Yi, 2003;
Hu and Liu, 2004; Ding and Liu, 2007). (Nasuka-
wa and Yi, 2003) adopt a rule based approach,
where rules are created by humans for adjectives,
verbs, nouns, and so on. Given a sentiment target
and its context, part-of-speech tagging and de-
pendency parsing are first performed on the con-
text. Then predefined rules are matched in the
context to determine the sentiment about the target.
In (Hu and Liu, 2004), opinions are extracted from
product reviews, where the features of the product
are considered opinion targets. The sentiment
about each target in each sentence of the review is
determined based on the dominant orientation of
the opinion words appearing in the sentence.
As mentioned in Section 1, target-dependent
sentiment classification of review sentences is
quite different from that of tweets. In reviews, if
any sentiment is expressed in a sentence containing
a feature, it is very likely that the sentiment is
about the feature. However, the assumption does
not hold in tweets.
</bodyText>
<subsectionHeader confidence="0.997254">
2.3 SA of Tweets
</subsectionHeader>
<bodyText confidence="0.999980555555556">
As Twitter becomes more popular, sentiment anal-
ysis on Twitter data becomes more attractive. (Go
et al., 2009; Parikh and Movassate, 2009; Barbosa
and Feng, 2010; Davidiv et al., 2010) all follow the
machine learning based approach for sentiment
classification of tweets. Specifically, (Davidiv et
al., 2010) propose to classify tweets into multiple
sentiment types using hashtags and smileys as la-
bels. In their approach, a supervised KNN-like
classifier is used. In contrast, (Barbosa and Feng,
2010) propose a two-step approach to classify the
sentiments of tweets using SVM classifiers with
abstract features. The training data is collected
from the outputs of three existing Twitter senti-
ment classification web sites. As mentioned above,
these approaches work in a target-independent way,
and so need to be adapted for target-dependent sen-
timent classification.
</bodyText>
<sectionHeader confidence="0.967975" genericHeader="method">
3 Approach Overview
</sectionHeader>
<bodyText confidence="0.998548">
The problem we address in this paper is target-
dependent sentiment classification of tweets. So
the input of our task is a collection of tweets con-
taining the target and the output is labels assigned
to each of the tweets. Inspired by (Barbosa and
Feng, 2010; Pang and Lee, 2004), we design a
three-step approach in this paper:
</bodyText>
<listItem confidence="0.9301751">
1. Subjectivity classification as the first step
to decide if the tweet is subjective or neu-
tral about the target;
2. Polarity classification as the second step to
decide if the tweet is positive or negative
about the target if it is classified as subjec-
tive in Step 1;
3. Graph-based optimization as the third step
to further boost the performance by taking
the related tweets into consideration.
</listItem>
<bodyText confidence="0.999539">
In each of the first two steps, a binary SVM
classifier is built to perform the classification. To
train the classifiers, we use SVM-Light6 with a
linear kernel; the default setting is adopted in all
experiments.
</bodyText>
<footnote confidence="0.928374">
6 http://svmlight.joachims.org/
</footnote>
<page confidence="0.997036">
153
</page>
<subsectionHeader confidence="0.998037">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999973">
In our approach, rich feature representations are
used to distinguish between sentiments expressed
towards different targets. In order to generate such
features, much NLP work has to be done before-
hand, such as tweet normalization, POS tagging,
word stemming, and syntactic parsing.
In our experiments, POS tagging is performed
by the OpenNLP POS tagger7. Word stemming is
performed by using a word stem mapping table
consisting of about 20,000 entries. We also built a
simple rule-based model for tweet normalization
which can correct simple spelling errors and varia-
tions into normal form, such as “gooood” to
“good” and “luve” to “love”. For syntactic parsing
we use a Maximum Spanning Tree dependency
parser (McDonald et al., 2005).
</bodyText>
<subsectionHeader confidence="0.996799">
3.2 Target-independent Features
</subsectionHeader>
<bodyText confidence="0.999966272727273">
Previous work (Barbosa and Feng, 2010; Davidiv
et al., 2010) has discovered many effective features
for sentiment analysis of tweets, such as emoticons,
punctuation, prior subjectivity and polarity of a
word. In our classifiers, most of these features are
also used. Since these features are all generated
without considering the target, we call them target-
independent features. In both the subjectivity clas-
sifier and polarity classifier, the same target-
independent feature set is used. Specifically, we
use two kinds of target-independent features:
</bodyText>
<listItem confidence="0.7454076">
1. Content features, including words, punctu-
ation, emoticons, and hashtags (hashtags
are provided by the author to indicate the
topic of the tweet).
2. Sentiment lexicon features, indicating how
many positive or negative words are in-
cluded in the tweet according to a prede-
fined lexicon. In our experiments, we use
the lexicon downloaded from General In-
quirer8.
</listItem>
<sectionHeader confidence="0.978003" genericHeader="method">
4 Target-dependent Sentiment Classifica-
tion
</sectionHeader>
<bodyText confidence="0.848141">
Besides target-independent features, we also incor-
porate target-dependent features in both the subjec-
</bodyText>
<footnote confidence="0.988304">
7 http://opennlp.sourceforge.net/projects.html
8 http://www.wjh.harvard.edu/~inquirer/
</footnote>
<bodyText confidence="0.871525">
tivity classifier and polarity classifier. We will ex-
plain them in detail below.
</bodyText>
<subsectionHeader confidence="0.994339">
4.1 Extended Targets
</subsectionHeader>
<bodyText confidence="0.965299955555555">
It is quite common that people express their senti-
ments about a target by commenting not on the
target itself but on some related things of the target.
For example, one may express a sentiment about a
company by commenting on its products or tech-
nologies. To express a sentiment about a product,
one may choose to comment on the features or
functionalities of the product. It is assumed that
readers or audiences can clearly infer the sentiment
about the target based on those sentiments about
the related things. As shown in the tweet below,
the author expresses a positive sentiment about
“Microsoft” by expressing a positive sentiment
directly about “Microsoft technologies”.
“I am passionate about Microsoft technologies
especially Silverlight.”
In this paper, we define those aforementioned
related things as Extended Targets. Tweets ex-
pressing positive or negative sentiments towards
the extended targets are also regarded as positive
or negative about the target. Therefore, for target-
dependent sentiment classification of tweets, the
first thing is identifying all extended targets in the
input tweet collection.
In this paper, we first regard all noun phrases,
including the target, as extended targets for sim-
plicity. However, it would be interesting to know
under what circumstances the sentiment towards
the target is truly consistent with that towards its
extended targets. For example, a sentiment about
someone’s behavior usually means a sentiment
about the person, while a sentiment about some-
one’s colleague usually has nothing to do with the
person. This could be a future work direction for
target-dependent sentiment classification.
In addition to the noun phrases including the
target, we further expand the extended target set
with the following three methods:
1. Adding mentions co-referring to the target
as new extended targets. It is common that
people use definite or demonstrative noun
phrases or pronouns referring to the target
in a tweet and express sentiments directly
on them. For instance, in “Oh, Jon Stewart.
How I love you so.”, the author expresses
</bodyText>
<page confidence="0.999013">
154
</page>
<bodyText confidence="0.928688677419355">
a positive sentiment to “you” which actual-
ly refers to “Jon Stewart”. By using a sim-
ple co-reference resolution tool adapted
from (Soon et al., 2001), we add all the
mentions referring to the target into the ex-
tended target set.
2. Identifying the top K nouns and noun
phrases which have the strongest associa-
tion with the target. Here, we use
Pointwise Mutual Information (PMI) to
measure the association.
Where p(w,t), p(w), and p(t) are probabili-
ties of w and t co-occurring, w appearing,
and t appearing in a tweet respectively. In
the experiments, we estimate them on a
tweet corpus containing 20 million tweets.
We set K = 20 in the experiments based on
empirical observations.
3. Extracting head nouns of all extended tar-
gets, whose PMI values with the target are
above some predefined threshold, as new
extended targets. For instance, suppose we
have found “Microsoft Technologies” as
the extended target, we will further add
“technologies” into the extended target set
if the PMI value for “technologies” and
“Microsoft” is above the threshold. Simi-
larly, we can find “price” as the extended
targets for “iPhone” from “the price of
iPhone” and “LoveGame” for “Lady Ga-
ga” from “LoveGame by Lady Gaga”.
</bodyText>
<subsectionHeader confidence="0.958418">
4.2 Target-dependent Features
</subsectionHeader>
<bodyText confidence="0.989064133333333">
Target-dependent sentiment classification needs to
distinguish the expressions describing the target
from other expressions. In this paper, we rely on
the syntactic parse tree to satisfy this need. Specif-
ically, for any word stem wi in a tweet which has
one of the following relations with the given target
T or any from the extended target set, we generate
corresponding target-dependent features with the
following rules:
• wi is a transitive verb and T (or any of the
extended target) is its object; we generate a
feature wi _arg2. “arg” is short for “argu-
ment”. For example, for the target iPhone
in “I love iPhone”, we generate
“love_arg2” as a feature.
</bodyText>
<listItem confidence="0.994591090909091">
• wi is a transitive verb and T (or any of the
extended target) is its subject; we generate
a feature wi_arg1 similar to Rule 1.
• wi is a intransitive verb and T (or any of the
extended target) is its subject; we generate
a feature wi_it_arg1.
• wi is an adjective or noun and T (or any of
the extended target) is its head; we gener-
ate a feature wi_arg1.
• wi is an adjective or noun and it (or its
head) is connected by a copula with T (or
any of the extended target); we generate a
feature wi_cp_arg1.
• wi is an adjective or intransitive verb ap-
pearing alone as a sentence and T (or any
of the extended target) appears in the pre-
vious sentence; we generate a feature
wi_arg. For example, in “John did that.
Great!”, “Great” appears alone as a sen-
tence, so we generate “great_arg” for the
target “John”.
• wi is an adverb, and the verb it modifies
</listItem>
<bodyText confidence="0.991844571428572">
has T (or any of the extended target) as its
subject; we generate a feature arg1_v_wi.
For example, for the target iPhone in the
tweet “iPhone works better with the Cell-
Band”, we will generate the feature
“arg1_v_well”.
Moreover, if any word included in the generated
target-dependent features is modified by a nega-
tion9, then we will add a prefix “neg-” to it in the
generated features. For example, for the target iPh-
one in the tweet “iPhone does not work better with
the CellBand”, we will generate the features
“arg1_v_neg-well” and “neg-work_it_arg1”.
To overcome the sparsity of target-dependent
features mentioned above, we design a special bi-
nary feature indicating whether or not the tweet
contains at least one of the above target-dependent
features. Target-dependent features are binary fea-
tures, each of which corresponds to the presence of
the feature in the tweet. If the feature is present, the
entry will be 1; otherwise it will be 0.
</bodyText>
<footnote confidence="0.3307345">
9 Seven negations are used in the experiments: not, no, never,
n’t, neither, seldom, hardly.
</footnote>
<page confidence="0.999211">
155
</page>
<sectionHeader confidence="0.983991" genericHeader="method">
5 Graph-based Sentiment Optimization
</sectionHeader>
<bodyText confidence="0.9998635">
As we mentioned in Section 1, since tweets are
usually shorter and more ambiguous, it would be
useful to take their contexts into consideration
when classifying the sentiments. In this paper, we
regard the following three kinds of related tweets
as context for a tweet.
</bodyText>
<listItem confidence="0.990439230769231">
1. Retweets. Retweeting in Twitter is essen-
tially the forwarding of a previous message.
People usually do not change the content
of the original tweet when retweeting. So
retweets usually have the same sentiment
as the original tweets.
2. Tweets containing the target and published
by the same person. Intuitively, the tweets
published by the same person within a
short timeframe should have a consistent
sentiment about the same target.
3. Tweets replying to or replied by the tweet
to be classified.
</listItem>
<bodyText confidence="0.999341">
Based on these three kinds of relations, we can
construct a graph using the input tweet collection
of a given target. As illustrated in Figure 1, each
circle in the graph indicates a tweet. The three
kinds of edges indicate being published by the
same person (solid line), retweeting (dash line),
and replying relations (round dotted line) respec-
tively.
</bodyText>
<figureCaption confidence="0.995729">
Figure 1. An example graph of tweets about a target
</figureCaption>
<bodyText confidence="0.999982285714286">
If we consider that the sentiment of a tweet only
depends on its content and immediate neighbors,
we can leverage a graph-based method for senti-
ment classification of tweets. Specifically, the
probability of a tweet belonging to a specific sen-
timent class can be computed with the following
formula:
Where c is the sentiment label of a tweet which
belongs to {positive, negative, neutral}, G is the
tweet graph, N(d) is a specific assignment of sen-
timent labels to all immediate neighbors of the
tweet, and r is the content of the tweet.
We can convert the output scores of a tweet by
the subjectivity and polarity classifiers into proba-
bilistic form and use them to approximate p(c |r).
Then a relaxation labeling algorithm described in
(Angelova and Weikum, 2006) can be used on the
graph to iteratively estimate p(c|r,G) for all tweets.
After the iteration ends, for any tweet in the graph,
the sentiment label that has the maximum p(c |r,G)
is considered the final label.
</bodyText>
<sectionHeader confidence="0.9997" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99996008">
Because there is no annotated tweet corpus public-
ly available for evaluation of target-dependent
Twitter sentiment classification, we have to create
our own. Since people are most interested in sen-
timents towards celebrities, companies and prod-
ucts, we selected 5 popular queries of these kinds:
{Obama, Google, iPad, Lakers, Lady Gaga}. For
each of those queries, we downloaded 400 English
tweets10 containing the query using the Twitter API.
We manually classify each tweet as positive,
negative or neutral towards the query with which it
is downloaded. After removing duplicate tweets,
we finally obtain 459 positive, 268 negative and
1,212 neutral tweets.
Among the tweets, 100 are labeled by two hu-
man annotators for inter-annotator study. The re-
sults show that for 86% of them, both annotators
gave identical labels. Among the 14 tweets which
the two annotators disagree on, only 1 case is a
positive-negative disagreement (one annotator con-
siders it positive while the other negative), and the
other 13 are all neutral-subjective disagreement.
This probably indicates that it is harder for humans
to decide if a tweet is neutral or subjective than to
decide if it is positive or negative.
</bodyText>
<footnote confidence="0.983473">
10 In this paper, we use sentiment classification of English
tweets as a case study; however, our approach is applicable to
other languages as well.
</footnote>
<page confidence="0.994531">
156
</page>
<subsectionHeader confidence="0.7694705">
6.1 Error Analysis of Twitter Sentiment Out-
put
</subsectionHeader>
<bodyText confidence="0.998925228571429">
We first analyze the output of Twitter Sentiment
(TS) using the five test queries. For each query, we
randomly select 20 tweets labeled as positive or
negative by TS. We also manually classify each
tweet as positive, negative or neutral about the cor-
responding query. Then, we analyze those tweets
that get different labels from TS and humans. Fi-
nally we find two major types of error: 1) Tweets
which are totally neutral (for any target) are classi-
fied as subjective by TS; 2) sentiments in some
tweets are classified correctly but the sentiments
are not truly about the query. The two types take
up about 35% and 40% of the total errors, respec-
tively.
The second type is actually what we want to re-
solve in this paper. After further checking those
tweets of the second type, we found that most of
them are actually neutral for the target, which
means that the dominant error in Twitter Sentiment
is classifying neutral tweets as subjective. Below
are several examples of the second type where the
bolded words are the targets.
“No debate needed, heat can&apos;t beat lakers or
celtics” (negative by TS but positive by human)
“why am i getting spams from weird people ask-
ing me if i want to chat with lady gaga” (positive
by TS but neutral by human)
“Bringing iPhone and iPad apps into cars?
http://www.speakwithme.com/ will be out soon and
alpha is awesome in my car.” (positive by TS but
neutral by human)
“Here&apos;s a great article about Monte Veronese
cheese. It&apos;s in Italian so just put the url into Google
translate and enjoy http://ow.ly/3oQ77” (positive
by TS but neutral by human)
</bodyText>
<subsectionHeader confidence="0.999928">
6.2 Evaluation of Subjectivity Classification
</subsectionHeader>
<bodyText confidence="0.999295571428571">
We conduct several experiments to evaluate sub-
jectivity classifiers using different features. In the
experiments, we consider the positive and negative
tweets annotated by humans as subjective tweets
(i.e., positive instances in the SVM classifiers),
which amount to 727 tweets. Following (Pang et
al., 2002), we balance the evaluation data set by
randomly selecting 727 tweets from all neutral
tweets annotated by humans and consider them as
objective tweets (i.e., negative instances in the
classifiers). We perform 10-fold cross-validations
on the selected data. Following (Go et al., 2009;
Pang et al., 2002), we use accuracy as a metric in
our experiments. The results are listed below.
</bodyText>
<table confidence="0.9977165">
Features Accuracy (%)
Content features 61.1
+ Sentiment lexicon features 63.8
+ Target-dependent features 68.2
Re-implementation of (Bar- 60.3
bosa and Feng, 2010)
</table>
<tableCaption confidence="0.999966">
Table 1. Evaluation of subjectivity classifiers.
</tableCaption>
<bodyText confidence="0.999933666666667">
As shown in Table 1, the classifier using only
the content features achieves an accuracy of 61.1%.
Adding sentiment lexicon features improves the
accuracy to 63.8%. Finally, the best performance
(68.2%) is achieved by combining target-
dependent features and other features (t-test: p &lt;
0.005). This clearly shows that target-dependent
features do help remove many sentiments not truly
about the target. We also re-implemented the
method proposed in (Barbosa and Feng, 2010) for
comparison. From Table 1, we can see that all our
systems perform better than (Barbosa and Feng,
2010) on our data set. One possible reason is that
(Barbosa and Feng, 2010) use only abstract fea-
tures while our systems use more lexical features.
To further evaluate the contribution of target ex-
tension, we compare the system using the exact
target and all extended targets with that using only
the exact target. We also eliminate the extended
targets generated by each of the three target exten-
sion methods and reevaluate the performances.
</bodyText>
<table confidence="0.997995">
Target Accuracy (%)
Exact target 65.6
+ all extended targets 68.2
- co-references 68.0
- targets found by PMI 67.8
- head nouns 67.3
</table>
<tableCaption confidence="0.999937">
Table 2. Evaluation of target extension methods.
</tableCaption>
<bodyText confidence="0.9998955">
As shown in Table 2, without extended targets,
the accuracy is 65.6%, which is still higher than
those using only target-independent features. After
adding all extended targets, the accuracy is im-
proved significantly to 68.2% (p &lt; 0.005), which
suggests that target extension does help find indi-
</bodyText>
<page confidence="0.995131">
157
</page>
<bodyText confidence="0.9997188">
rectly expressed sentiments about the target. In
addition, all of the three methods contribute to the
overall improvement, with the head noun method
contributing most. However, the other two meth-
ods do not contribute significantly.
</bodyText>
<subsectionHeader confidence="0.999514">
6.3 Evaluation of Polarity Classification
</subsectionHeader>
<bodyText confidence="0.9996948">
Similarly, we conduct several experiments on posi-
tive and negative tweets to compare the polarity
classifiers with different features, where we use
268 negative and 268 randomly selected positive
tweets. The results are listed below.
</bodyText>
<table confidence="0.997564833333333">
Features Accuracy (%)
Content features 78.8
+ Sentiment lexicon features 84.2
+ Target-dependent features 85.6
Re-implementation of (Bar- 83.9
bosa and Feng, 2010)
</table>
<tableCaption confidence="0.999914">
Table 3. Evaluation of polarity classifiers.
</tableCaption>
<bodyText confidence="0.999981351351352">
From Table 3, we can see that the classifier us-
ing only the content features achieves the worst
accuracy (78.8%). Sentiment lexicon features are
shown to be very helpful for improving the per-
formance. Similarly, we re-implemented the meth-
od proposed by (Barbosa and Feng, 2010) in this
experiment. The results show that our system using
both content features and sentiment lexicon fea-
tures performs slightly better than (Barbosa and
Feng, 2010). The reason may be same as that we
explained above.
Again, the classifier using all features achieves
the best performance. Both the classifiers with all
features and with the combination of content and
sentiment lexicon features are significantly better
than that with only the content features (p &lt; 0.01).
However, the classifier with all features does not
significantly outperform that using the combina-
tion of content and sentiment lexicon features. We
also note that the improvement by target-dependent
features here is not as large as that in subjectivity
classification. Both of these indicate that target-
dependent features are more useful for improving
subjectivity classification than for polarity classifi-
cation. This is consistent with our observation in
Subsection 6.2 that most errors caused by incorrect
target association are made in subjectivity classifi-
cation. We also note that all numbers in Table 3
are much bigger than those in Table 1, which sug-
gests that subjectivity classification of tweets is
more difficult than polarity classification.
Similarly, we evaluated the contribution of tar-
get extension for polarity classification. According
to the results, adding all extended targets improves
the accuracy by about 1 point. However, the con-
tributions from the three individual methods are
not statistically significant.
</bodyText>
<subsectionHeader confidence="0.997928">
6.4 Evaluation of Graph-based Optimization
</subsectionHeader>
<bodyText confidence="0.999018142857143">
As seen in Figure 1, there are several tweets which
are not connected with any other tweets. For these
tweets, our graph-based optimization approach will
have no effect. The following table shows the per-
centages of the tweets in our evaluation data set
which have at least one related tweet according to
various relation types.
</bodyText>
<table confidence="0.9848066">
Relation type Percentage
Published by the same person11 41.6
Retweet 23.0
Reply 21.0
All 66.2
</table>
<tableCaption confidence="0.9611435">
Table 4. Percentages of tweets having at least one relat-
ed tweet according to various relation types.
</tableCaption>
<bodyText confidence="0.999440428571429">
According to Table 4, for 66.2% of the tweets
concerning the test queries, we can find at least one
related tweet. That means our context-aware ap-
proach is potentially useful for most of the tweets.
To evaluate the effectiveness of our context-
aware approach, we compared the systems with
and without considering the context.
</bodyText>
<table confidence="0.999590666666667">
System Accuracy F1-score (%)
pos neu neg
Target-dependent 66.0 57.5 70.1 66.1
sentiment classifier
+Graph-based op- 68.3 63.5 71.0 68.5
timization
</table>
<tableCaption confidence="0.99969">
Table 5. Effectiveness of the context-aware approach.
</tableCaption>
<bodyText confidence="0.9990846">
As shown in Table 5, the overall accuracy of the
target-dependent classifiers over three classes is
66.0%. The graph-based optimization improves the
performance by over 2 points (p &lt; 0.005), which
clearly shows that the context information is very
</bodyText>
<footnote confidence="0.5705575">
11 We limit the time frame from one week before to one week
after the post time of the current tweet.
</footnote>
<page confidence="0.994805">
158
</page>
<bodyText confidence="0.9998365">
useful for classifying the sentiments of tweets.
From the detailed improvement for each sentiment
class, we find that the context-aware approach is
especially helpful for positive and negative classes.
</bodyText>
<table confidence="0.72844625">
Relation type Accuracy (%)
Published by the same person 67.8
Retweet 66.0
Reply 67.0
</table>
<tableCaption confidence="0.995557">
Table 6. Contribution comparison between relations.
</tableCaption>
<bodyText confidence="0.999982272727273">
We further compared the three types of relations
for context-aware sentiment classification; the re-
sults are reported in Table 6. Clearly, being pub-
lished by the same person is the most useful
relation for sentiment classification, which is con-
sistent with the percentage distribution of the
tweets over relation types; using retweet only does
not help. One possible reason for this is that the
retweets and their original tweets are nearly the
same, so it is very likely that they have already got
the same labels in previous classifications.
</bodyText>
<sectionHeader confidence="0.996849" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999938791666667">
Twitter sentiment analysis has attracted much at-
tention recently. In this paper, we address target-
dependent sentiment classification of tweets. Dif-
ferent from previous work using target-
independent classification, we propose to incorpo-
rate syntactic features to distinguish texts used for
expressing sentiments towards different targets in a
tweet. According to the experimental results, the
classifiers incorporating target-dependent features
significantly outperform the previous target-
independent classifiers.
In addition, different from previous work using
only information on the current tweet for sentiment
classification, we propose to take the related tweets
of the current tweet into consideration by utilizing
graph-based optimization. According to the exper-
imental results, the graph-based optimization sig-
nificantly improves the performance.
As mentioned in Section 4.1, in future we would
like to explore the relations between a target and
any of its extended targets. We are also interested
in exploring relations between Twitter accounts for
classifying the sentiments of the tweets published
by them.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999994428571429">
We would like to thank Matt Callcut for refining
the language of this paper, and thank Yuki Arase
and the anonymous reviewers for many valuable
comments and helpful suggestions. We would also
thank Furu Wei and Xiaolong Wang for their help
with some of the experiments and the preparation
of the camera-ready version of the paper.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849684210526">
Ralitsa Angelova, Gerhard Weikum. 2006. Graph-based
text classification: learn from your neighbors. SIGIR
2006: 485-492
Luciano Barbosa and Junlan Feng. 2010. Robust Senti-
ment Detection on Twitter from Biased and Noisy
Data. Coling 2010.
Christopher Burges. 1998. A Tutorial on Support Vector
Machines for Pattern Recognition. Data Mining and
Knowledge Discovery, 2(2):121-167.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan S. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proc. of the 2005 Human Language Tech-
nology Conf. and Conf. on Empirical Methods in
Natural Language Processing (HLT/EMNLP 2005).
pp. 355-362
Dmitry Davidiv, Oren Tsur and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hash-
tags and Smileys. Coling 2010.
Xiaowen Ding and Bing Liu. 2007. The Utility of Lin-
guistic Rules in Opinion Mining. SIGIR-2007 (poster
paper), 23-27 July 2007, Amsterdam.
Alec Go, Richa Bhayani, Lei Huang. 2009. Twitter Sen-
timent Classification using Distant Supervision.
Vasileios Hatzivassiloglou and Kathleen.R. McKeown.
2002. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th ACL and the 8th
Conference of the European Chapter of the ACL.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM
SIGKDD International Conference on Knowledge
Discovery &amp; Data Mining (KDD-2004, full paper),
Seattle, Washington, USA, Aug 22-25, 2004.
Thorsten Joachims. Making Large-scale Support Vector
Machine Learning Practical. In B. SchÄolkopf, C. J.
C. Burges, and A. J. Smola, editors, Advances in
kernel methods: support vector learning, pages 169-
184. MIT Press, Cambridge, MA, USA, 1999.
</reference>
<page confidence="0.986283">
159
</page>
<reference confidence="0.999652242424243">
Soo-Min Kim and Eduard Hovy 2006. Extracting opi-
nions, opinion holders, and topics expressed in online
news media text, In Proc. of ACL Workshop on Sen-
timent and Subjectivity in Text, pp.1-8, Sydney, Aus-
tralia.
Ryan McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. HLT/EMNLP.
Tetsuya Nasukawa, Jeonghee Yi. 2003. Sentiment anal-
ysis: capturing favorability using natural language
processing. In Proceedings of K-CAP.
Bo Pang, Lillian Lee. 2004. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of
ACL 2004.
Bo Pang, Lillian Lee, Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment Classification using Machine
Learning Techniques.
Ravi Parikh and Matin Movassate. 2009. Sentiment
Analysis of User-Generated Twitter Updates using
Various Classification Techniques.
Wee. M. Soon, Hwee. T. Ng, and Danial. C. Y. Lim.
2001. A Machine Learning Approach to Coreference
Resolution of Noun Phrases. Computational Linguis-
tics, 27(4):521–544.
Peter D. Turney. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. In proceedings of ACL 2002.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI-2000.
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005.
Recognizing Contextual Polarity in Phrase-Level
Sentiment Analysis. In Proceedings of NAACL 2005.
</reference>
<page confidence="0.997482">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862236">
<title confidence="0.999731">Target-dependent Twitter Sentiment Classification</title>
<author confidence="0.999421">Mo Ming Xiaohua Tiejun</author>
<affiliation confidence="0.99204">1 Microsoft Research Asia 2 School of Computer Science &amp; Technology Beijing, China Harbin Institute of Technology</affiliation>
<address confidence="0.938258">Harbin, China</address>
<email confidence="0.965886">longj@mtlab.hit.edu.cn</email>
<email confidence="0.965886">mingzhou@mtlab.hit.edu.cn</email>
<email confidence="0.965886">xiaoliu}@microsoft.com{yumo@mtlab.hit.edu.cn</email>
<email confidence="0.965886">tjzhao@mtlab.hit.edu.cn</email>
<abstract confidence="0.998740321428571">Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ralitsa Angelova</author>
<author>Gerhard Weikum</author>
</authors>
<title>Graph-based text classification: learn from your neighbors. SIGIR</title>
<date>2006</date>
<pages>485--492</pages>
<contexts>
<context position="21998" citStr="Angelova and Weikum, 2006" startWordPosition="3497" endWordPosition="3500">sentiment classification of tweets. Specifically, the probability of a tweet belonging to a specific sentiment class can be computed with the following formula: Where c is the sentiment label of a tweet which belongs to {positive, negative, neutral}, G is the tweet graph, N(d) is a specific assignment of sentiment labels to all immediate neighbors of the tweet, and r is the content of the tweet. We can convert the output scores of a tweet by the subjectivity and polarity classifiers into probabilistic form and use them to approximate p(c |r). Then a relaxation labeling algorithm described in (Angelova and Weikum, 2006) can be used on the graph to iteratively estimate p(c|r,G) for all tweets. After the iteration ends, for any tweet in the graph, the sentiment label that has the maximum p(c |r,G) is considered the final label. 6 Experiments Because there is no annotated tweet corpus publicly available for evaluation of target-dependent Twitter sentiment classification, we have to create our own. Since people are most interested in sentiments towards celebrities, companies and products, we selected 5 popular queries of these kinds: {Obama, Google, iPad, Lakers, Lady Gaga}. For each of those queries, we downloa</context>
</contexts>
<marker>Angelova, Weikum, 2006</marker>
<rawString>Ralitsa Angelova, Gerhard Weikum. 2006. Graph-based text classification: learn from your neighbors. SIGIR 2006: 485-492</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust Sentiment Detection on Twitter from Biased and Noisy Data. Coling</title>
<date>2010</date>
<contexts>
<context position="2898" citStr="Barbosa and Feng, 2010" startWordPosition="433" endWordPosition="436"> In those web sites, the user can input a sentiment target as a query, and search for tweets containing positive or negative sentiments towards the target. The problem needing to be addressed can be formally named as Target-dependent Sentiment Classification of Tweets; namely, given a query, classifying the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem, such as (Go et al., 20095; Barbosa and Feng, 2010), basically follow (Pang et al., 2002), who utilize machine learning based classifiers for the sentiment classification of texts. However, their classifiers actually work in a target-independent way: all the features used in the classifiers are independent of the target, so the sentiment is decided no matter what the target is. Since (Pang et al., 2002) (or later research on sentiment classification 1 http://en.wikipedia.org/wiki/Twitter 2 http://www.tweetfeel.com/ 3 http://twendz.waggeneredstrom.com/ 4 http://twittersentiment.appspot.com/ 5 The algorithm used in Twitter Sentiment 151 Proceedi</context>
<context position="10394" citStr="Barbosa and Feng, 2010" startWordPosition="1607" endWordPosition="1610">t in each sentence of the review is determined based on the dominant orientation of the opinion words appearing in the sentence. As mentioned in Section 1, target-dependent sentiment classification of review sentences is quite different from that of tweets. In reviews, if any sentiment is expressed in a sentence containing a feature, it is very likely that the sentiment is about the feature. However, the assumption does not hold in tweets. 2.3 SA of Tweets As Twitter becomes more popular, sentiment analysis on Twitter data becomes more attractive. (Go et al., 2009; Parikh and Movassate, 2009; Barbosa and Feng, 2010; Davidiv et al., 2010) all follow the machine learning based approach for sentiment classification of tweets. Specifically, (Davidiv et al., 2010) propose to classify tweets into multiple sentiment types using hashtags and smileys as labels. In their approach, a supervised KNN-like classifier is used. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned above, these approaches </context>
<context position="12919" citStr="Barbosa and Feng, 2010" startWordPosition="2007" endWordPosition="2010">e done beforehand, such as tweet normalization, POS tagging, word stemming, and syntactic parsing. In our experiments, POS tagging is performed by the OpenNLP POS tagger7. Word stemming is performed by using a word stem mapping table consisting of about 20,000 entries. We also built a simple rule-based model for tweet normalization which can correct simple spelling errors and variations into normal form, such as “gooood” to “good” and “luve” to “love”. For syntactic parsing we use a Maximum Spanning Tree dependency parser (McDonald et al., 2005). 3.2 Target-independent Features Previous work (Barbosa and Feng, 2010; Davidiv et al., 2010) has discovered many effective features for sentiment analysis of tweets, such as emoticons, punctuation, prior subjectivity and polarity of a word. In our classifiers, most of these features are also used. Since these features are all generated without considering the target, we call them targetindependent features. In both the subjectivity classifier and polarity classifier, the same targetindependent feature set is used. Specifically, we use two kinds of target-independent features: 1. Content features, including words, punctuation, emoticons, and hashtags (hashtags a</context>
<context position="26625" citStr="Barbosa and Feng, 2010" startWordPosition="4250" endWordPosition="4253">n features 63.8 + Target-dependent features 68.2 Re-implementation of (Bar- 60.3 bosa and Feng, 2010) Table 1. Evaluation of subjectivity classifiers. As shown in Table 1, the classifier using only the content features achieves an accuracy of 61.1%. Adding sentiment lexicon features improves the accuracy to 63.8%. Finally, the best performance (68.2%) is achieved by combining targetdependent features and other features (t-test: p &lt; 0.005). This clearly shows that target-dependent features do help remove many sentiments not truly about the target. We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. To further evaluate the contribution of target extension, we compare the system using the exact target and all extended targets with that using only the exact target. We also eliminate the extended targets generated by each of the three target extension methods and reevaluate the performances. Target Accuracy (%) Exact target 65.6 + all extended ta</context>
<context position="28645" citStr="Barbosa and Feng, 2010" startWordPosition="4571" endWordPosition="4574"> classifiers with different features, where we use 268 negative and 268 randomly selected positive tweets. The results are listed below. Features Accuracy (%) Content features 78.8 + Sentiment lexicon features 84.2 + Target-dependent features 85.6 Re-implementation of (Bar- 83.9 bosa and Feng, 2010) Table 3. Evaluation of polarity classifiers. From Table 3, we can see that the classifier using only the content features achieves the worst accuracy (78.8%). Sentiment lexicon features are shown to be very helpful for improving the performance. Similarly, we re-implemented the method proposed by (Barbosa and Feng, 2010) in this experiment. The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). The reason may be same as that we explained above. Again, the classifier using all features achieves the best performance. Both the classifiers with all features and with the combination of content and sentiment lexicon features are significantly better than that with only the content features (p &lt; 0.01). However, the classifier with all features does not significantly outperform that using the combination of content and sentim</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust Sentiment Detection on Twitter from Biased and Noisy Data. Coling 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Burges</author>
</authors>
<title>A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery,</title>
<date>1998</date>
<pages>2--2</pages>
<marker>Burges, 1998</marker>
<rawString>Christopher Burges. 1998. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2(2):121-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan S</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proc. of the 2005 Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<pages>355--362</pages>
<marker>Choi, Cardie, Riloff, S, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan S. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proc. of the 2005 Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005). pp. 355-362</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidiv</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. Coling</title>
<date>2010</date>
<contexts>
<context position="10417" citStr="Davidiv et al., 2010" startWordPosition="1611" endWordPosition="1614">e review is determined based on the dominant orientation of the opinion words appearing in the sentence. As mentioned in Section 1, target-dependent sentiment classification of review sentences is quite different from that of tweets. In reviews, if any sentiment is expressed in a sentence containing a feature, it is very likely that the sentiment is about the feature. However, the assumption does not hold in tweets. 2.3 SA of Tweets As Twitter becomes more popular, sentiment analysis on Twitter data becomes more attractive. (Go et al., 2009; Parikh and Movassate, 2009; Barbosa and Feng, 2010; Davidiv et al., 2010) all follow the machine learning based approach for sentiment classification of tweets. Specifically, (Davidiv et al., 2010) propose to classify tweets into multiple sentiment types using hashtags and smileys as labels. In their approach, a supervised KNN-like classifier is used. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned above, these approaches work in a target-indepe</context>
<context position="12942" citStr="Davidiv et al., 2010" startWordPosition="2011" endWordPosition="2014">as tweet normalization, POS tagging, word stemming, and syntactic parsing. In our experiments, POS tagging is performed by the OpenNLP POS tagger7. Word stemming is performed by using a word stem mapping table consisting of about 20,000 entries. We also built a simple rule-based model for tweet normalization which can correct simple spelling errors and variations into normal form, such as “gooood” to “good” and “luve” to “love”. For syntactic parsing we use a Maximum Spanning Tree dependency parser (McDonald et al., 2005). 3.2 Target-independent Features Previous work (Barbosa and Feng, 2010; Davidiv et al., 2010) has discovered many effective features for sentiment analysis of tweets, such as emoticons, punctuation, prior subjectivity and polarity of a word. In our classifiers, most of these features are also used. Since these features are all generated without considering the target, we call them targetindependent features. In both the subjectivity classifier and polarity classifier, the same targetindependent feature set is used. Specifically, we use two kinds of target-independent features: 1. Content features, including words, punctuation, emoticons, and hashtags (hashtags are provided by the auth</context>
</contexts>
<marker>Davidiv, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidiv, Oren Tsur and Ari Rappoport. 2010. Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. Coling 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
</authors>
<date>2007</date>
<booktitle>The Utility of Linguistic Rules in Opinion Mining. SIGIR-2007 (poster paper),</booktitle>
<pages>23--27</pages>
<location>Amsterdam.</location>
<contexts>
<context position="9260" citStr="Ding and Liu, 2007" startWordPosition="1422" endWordPosition="1425">se of a topic-based text categorization problem and investigate three classification algorithms: Naive Bayes, Maximum Entropy, and Support Vector Machines. According to the experimental results, machine learning based classifiers outperform the unsupervised approach, where the best performance is achieved by the SVM classifier with unigram presences as features. 2.2 Target-dependent SA Besides the above mentioned work for targetindependent sentiment classification, there are also several approaches proposed for target-dependent classification, such as (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). (Nasukawa and Yi, 2003) adopt a rule based approach, where rules are created by humans for adjectives, verbs, nouns, and so on. Given a sentiment target and its context, part-of-speech tagging and dependency parsing are first performed on the context. Then predefined rules are matched in the context to determine the sentiment about the target. In (Hu and Liu, 2004), opinions are extracted from product reviews, where the features of the product are considered opinion targets. The sentiment about each target in each sentence of the review is determined based on the dominant orientation of the </context>
</contexts>
<marker>Ding, Liu, 2007</marker>
<rawString>Xiaowen Ding and Bing Liu. 2007. The Utility of Linguistic Rules in Opinion Mining. SIGIR-2007 (poster paper), 23-27 July 2007, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter Sentiment Classification using Distant Supervision.</title>
<date>2009</date>
<contexts>
<context position="2872" citStr="Go et al., 2009" startWordPosition="429" endWordPosition="432">witter Sentiment4. In those web sites, the user can input a sentiment target as a query, and search for tweets containing positive or negative sentiments towards the target. The problem needing to be addressed can be formally named as Target-dependent Sentiment Classification of Tweets; namely, given a query, classifying the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem, such as (Go et al., 20095; Barbosa and Feng, 2010), basically follow (Pang et al., 2002), who utilize machine learning based classifiers for the sentiment classification of texts. However, their classifiers actually work in a target-independent way: all the features used in the classifiers are independent of the target, so the sentiment is decided no matter what the target is. Since (Pang et al., 2002) (or later research on sentiment classification 1 http://en.wikipedia.org/wiki/Twitter 2 http://www.tweetfeel.com/ 3 http://twendz.waggeneredstrom.com/ 4 http://twittersentiment.appspot.com/ 5 The algorithm used in Twit</context>
<context position="10342" citStr="Go et al., 2009" startWordPosition="1599" endWordPosition="1602">inion targets. The sentiment about each target in each sentence of the review is determined based on the dominant orientation of the opinion words appearing in the sentence. As mentioned in Section 1, target-dependent sentiment classification of review sentences is quite different from that of tweets. In reviews, if any sentiment is expressed in a sentence containing a feature, it is very likely that the sentiment is about the feature. However, the assumption does not hold in tweets. 2.3 SA of Tweets As Twitter becomes more popular, sentiment analysis on Twitter data becomes more attractive. (Go et al., 2009; Parikh and Movassate, 2009; Barbosa and Feng, 2010; Davidiv et al., 2010) all follow the machine learning based approach for sentiment classification of tweets. Specifically, (Davidiv et al., 2010) propose to classify tweets into multiple sentiment types using hashtags and smileys as labels. In their approach, a supervised KNN-like classifier is used. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classificat</context>
<context position="25840" citStr="Go et al., 2009" startWordPosition="4131" endWordPosition="4134">ctivity Classification We conduct several experiments to evaluate subjectivity classifiers using different features. In the experiments, we consider the positive and negative tweets annotated by humans as subjective tweets (i.e., positive instances in the SVM classifiers), which amount to 727 tweets. Following (Pang et al., 2002), we balance the evaluation data set by randomly selecting 727 tweets from all neutral tweets annotated by humans and consider them as objective tweets (i.e., negative instances in the classifiers). We perform 10-fold cross-validations on the selected data. Following (Go et al., 2009; Pang et al., 2002), we use accuracy as a metric in our experiments. The results are listed below. Features Accuracy (%) Content features 61.1 + Sentiment lexicon features 63.8 + Target-dependent features 68.2 Re-implementation of (Bar- 60.3 bosa and Feng, 2010) Table 1. Evaluation of subjectivity classifiers. As shown in Table 1, the classifier using only the content features achieves an accuracy of 61.1%. Adding sentiment lexicon features improves the accuracy to 63.8%. Finally, the best performance (68.2%) is achieved by combining targetdependent features and other features (t-test: p &lt; 0.</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, Lei Huang. 2009. Twitter Sentiment Classification using Distant Supervision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>2002</date>
<booktitle>In Proceedings of the 35th ACL and the 8th Conference of the European Chapter of the ACL.</booktitle>
<marker>Hatzivassiloglou, McKeown, 2002</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen.R. McKeown. 2002. Predicting the semantic orientation of adjectives. In Proceedings of the 35th ACL and the 8th Conference of the European Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004, full paper),</booktitle>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9239" citStr="Hu and Liu, 2004" startWordPosition="1418" endWordPosition="1421">ly as a special case of a topic-based text categorization problem and investigate three classification algorithms: Naive Bayes, Maximum Entropy, and Support Vector Machines. According to the experimental results, machine learning based classifiers outperform the unsupervised approach, where the best performance is achieved by the SVM classifier with unigram presences as features. 2.2 Target-dependent SA Besides the above mentioned work for targetindependent sentiment classification, there are also several approaches proposed for target-dependent classification, such as (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). (Nasukawa and Yi, 2003) adopt a rule based approach, where rules are created by humans for adjectives, verbs, nouns, and so on. Given a sentiment target and its context, part-of-speech tagging and dependency parsing are first performed on the context. Then predefined rules are matched in the context to determine the sentiment about the target. In (Hu and Liu, 2004), opinions are extracted from product reviews, where the features of the product are considered opinion targets. The sentiment about each target in each sentence of the review is determined based on the dominan</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004, full paper), Seattle, Washington, USA, Aug 22-25, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-scale Support Vector Machine Learning Practical. In</title>
<date>1999</date>
<booktitle>Advances in kernel methods: support vector learning,</booktitle>
<pages>169--184</pages>
<editor>B. SchÄolkopf, C. J. C. Burges, and A. J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA,</location>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. Making Large-scale Support Vector Machine Learning Practical. In B. SchÄolkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in kernel methods: support vector learning, pages 169-184. MIT Press, Cambridge, MA, USA, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text,</title>
<date>2006</date>
<booktitle>In Proc. of ACL Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia.</location>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy 2006. Extracting opinions, opinion holders, and topics expressed in online news media text, In Proc. of ACL Workshop on Sentiment and Subjectivity in Text, pp.1-8, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of K-CAP.</booktitle>
<contexts>
<context position="9221" citStr="Nasukawa and Yi, 2003" startWordPosition="1414" endWordPosition="1417">n of movie reviews simply as a special case of a topic-based text categorization problem and investigate three classification algorithms: Naive Bayes, Maximum Entropy, and Support Vector Machines. According to the experimental results, machine learning based classifiers outperform the unsupervised approach, where the best performance is achieved by the SVM classifier with unigram presences as features. 2.2 Target-dependent SA Besides the above mentioned work for targetindependent sentiment classification, there are also several approaches proposed for target-dependent classification, such as (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). (Nasukawa and Yi, 2003) adopt a rule based approach, where rules are created by humans for adjectives, verbs, nouns, and so on. Given a sentiment target and its context, part-of-speech tagging and dependency parsing are first performed on the context. Then predefined rules are matched in the context to determine the sentiment about the target. In (Hu and Liu, 2004), opinions are extracted from product reviews, where the features of the product are considered opinion targets. The sentiment about each target in each sentence of the review is determined ba</context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa, Jeonghee Yi. 2003. Sentiment analysis: capturing favorability using natural language processing. In Proceedings of K-CAP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="11399" citStr="Pang and Lee, 2004" startWordPosition="1765" endWordPosition="1768">sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned above, these approaches work in a target-independent way, and so need to be adapted for target-dependent sentiment classification. 3 Approach Overview The problem we address in this paper is targetdependent sentiment classification of tweets. So the input of our task is a collection of tweets containing the target and the output is labels assigned to each of the tweets. Inspired by (Barbosa and Feng, 2010; Pang and Lee, 2004), we design a three-step approach in this paper: 1. Subjectivity classification as the first step to decide if the tweet is subjective or neutral about the target; 2. Polarity classification as the second step to decide if the tweet is positive or negative about the target if it is classified as subjective in Step 1; 3. Graph-based optimization as the third step to further boost the performance by taking the related tweets into consideration. In each of the first two steps, a binary SVM classifier is built to perform the classification. To train the classifiers, we use SVM-Light6 with a linear</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang, Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<contexts>
<context position="2936" citStr="Pang et al., 2002" startWordPosition="439" endWordPosition="442">entiment target as a query, and search for tweets containing positive or negative sentiments towards the target. The problem needing to be addressed can be formally named as Target-dependent Sentiment Classification of Tweets; namely, given a query, classifying the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem, such as (Go et al., 20095; Barbosa and Feng, 2010), basically follow (Pang et al., 2002), who utilize machine learning based classifiers for the sentiment classification of texts. However, their classifiers actually work in a target-independent way: all the features used in the classifiers are independent of the target, so the sentiment is decided no matter what the target is. Since (Pang et al., 2002) (or later research on sentiment classification 1 http://en.wikipedia.org/wiki/Twitter 2 http://www.tweetfeel.com/ 3 http://twendz.waggeneredstrom.com/ 4 http://twittersentiment.appspot.com/ 5 The algorithm used in Twitter Sentiment 151 Proceedings of the 49th Annual Meeting of the </context>
<context position="8566" citStr="Pang et al., 2002" startWordPosition="1323" endWordPosition="1326">have been published on this topic. 152 2.1 Target-independent SA Specifically, Turney (2002) proposes an unsupervised method for classifying product or movie reviews as positive or negative. In this method, sentimental phrases are first selected from the reviews according to predefined part-of-speech patterns. Then the semantic orientation score of each phrase is calculated according to the mutual information values between the phrase and two predefined seed words. Finally, a review is classified based on the average semantic orientation of the sentimental phrases in the review. In contrast, (Pang et al., 2002) treat the sentiment classification of movie reviews simply as a special case of a topic-based text categorization problem and investigate three classification algorithms: Naive Bayes, Maximum Entropy, and Support Vector Machines. According to the experimental results, machine learning based classifiers outperform the unsupervised approach, where the best performance is achieved by the SVM classifier with unigram presences as features. 2.2 Target-dependent SA Besides the above mentioned work for targetindependent sentiment classification, there are also several approaches proposed for target-d</context>
<context position="25556" citStr="Pang et al., 2002" startWordPosition="4088" endWordPosition="4091">l be out soon and alpha is awesome in my car.” (positive by TS but neutral by human) “Here&apos;s a great article about Monte Veronese cheese. It&apos;s in Italian so just put the url into Google translate and enjoy http://ow.ly/3oQ77” (positive by TS but neutral by human) 6.2 Evaluation of Subjectivity Classification We conduct several experiments to evaluate subjectivity classifiers using different features. In the experiments, we consider the positive and negative tweets annotated by humans as subjective tweets (i.e., positive instances in the SVM classifiers), which amount to 727 tweets. Following (Pang et al., 2002), we balance the evaluation data set by randomly selecting 727 tweets from all neutral tweets annotated by humans and consider them as objective tweets (i.e., negative instances in the classifiers). We perform 10-fold cross-validations on the selected data. Following (Go et al., 2009; Pang et al., 2002), we use accuracy as a metric in our experiments. The results are listed below. Features Accuracy (%) Content features 61.1 + Sentiment lexicon features 63.8 + Target-dependent features 68.2 Re-implementation of (Bar- 60.3 bosa and Feng, 2010) Table 1. Evaluation of subjectivity classifiers. As </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Parikh</author>
<author>Matin Movassate</author>
</authors>
<title>Sentiment Analysis of User-Generated Twitter Updates using Various Classification Techniques.</title>
<date>2009</date>
<contexts>
<context position="10370" citStr="Parikh and Movassate, 2009" startWordPosition="1603" endWordPosition="1606">e sentiment about each target in each sentence of the review is determined based on the dominant orientation of the opinion words appearing in the sentence. As mentioned in Section 1, target-dependent sentiment classification of review sentences is quite different from that of tweets. In reviews, if any sentiment is expressed in a sentence containing a feature, it is very likely that the sentiment is about the feature. However, the assumption does not hold in tweets. 2.3 SA of Tweets As Twitter becomes more popular, sentiment analysis on Twitter data becomes more attractive. (Go et al., 2009; Parikh and Movassate, 2009; Barbosa and Feng, 2010; Davidiv et al., 2010) all follow the machine learning based approach for sentiment classification of tweets. Specifically, (Davidiv et al., 2010) propose to classify tweets into multiple sentiment types using hashtags and smileys as labels. In their approach, a supervised KNN-like classifier is used. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned </context>
</contexts>
<marker>Parikh, Movassate, 2009</marker>
<rawString>Ravi Parikh and Matin Movassate. 2009. Sentiment Analysis of User-Generated Twitter Updates using Various Classification Techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Soon</author>
<author>Hwee T Ng</author>
<author>Danial C Y Lim</author>
</authors>
<title>A Machine Learning Approach to Coreference Resolution of Noun Phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="16369" citStr="Soon et al., 2001" startWordPosition="2534" endWordPosition="2537">dent sentiment classification. In addition to the noun phrases including the target, we further expand the extended target set with the following three methods: 1. Adding mentions co-referring to the target as new extended targets. It is common that people use definite or demonstrative noun phrases or pronouns referring to the target in a tweet and express sentiments directly on them. For instance, in “Oh, Jon Stewart. How I love you so.”, the author expresses 154 a positive sentiment to “you” which actually refers to “Jon Stewart”. By using a simple co-reference resolution tool adapted from (Soon et al., 2001), we add all the mentions referring to the target into the extended target set. 2. Identifying the top K nouns and noun phrases which have the strongest association with the target. Here, we use Pointwise Mutual Information (PMI) to measure the association. Where p(w,t), p(w), and p(t) are probabilities of w and t co-occurring, w appearing, and t appearing in a tweet respectively. In the experiments, we estimate them on a tweet corpus containing 20 million tweets. We set K = 20 in the experiments based on empirical observations. 3. Extracting head nouns of all extended targets, whose PMI value</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee. M. Soon, Hwee. T. Ng, and Danial. C. Y. Lim. 2001. A Machine Learning Approach to Coreference Resolution of Noun Phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In proceedings of ACL</booktitle>
<contexts>
<context position="8040" citStr="Turney (2002)" startWordPosition="1241" endWordPosition="1242">e Lakers, we can confidently classify this tweet as positive. The remainder of this paper is structured as follows. In Section 2, we briefly summarize related work. Section 3 gives an overview of our approach. We explain the target-dependent and contextaware approaches in detail in Sections 4 and 5 respectively. Experimental results are reported in Section 6 and Section 7 concludes our work. 2 Related Work In recent years, sentiment analysis (SA) has become a hot topic in the NLP research community. A lot of papers have been published on this topic. 152 2.1 Target-independent SA Specifically, Turney (2002) proposes an unsupervised method for classifying product or movie reviews as positive or negative. In this method, sentimental phrases are first selected from the reviews according to predefined part-of-speech patterns. Then the semantic orientation score of each phrase is calculated according to the mutual information values between the phrase and two predefined seed words. Finally, a review is classified based on the average semantic orientation of the sentimental phrases in the review. In contrast, (Pang et al., 2002) treat the sentiment classification of movie reviews simply as a special c</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI-2000.</booktitle>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of AAAI-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. In Proceedings of NAACL 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>