<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079544">
<title confidence="0.982515">
Context-Enhanced Citation Sentiment Detection
</title>
<author confidence="0.976227">
Awais Athar
</author>
<affiliation confidence="0.975448">
University of Cambridge
Computer Laboratory
</affiliation>
<address confidence="0.989931">
15 JJ Thomson Avenue
Cambridge, CB3 0FD, U.K.
</address>
<email confidence="0.999079">
awais.athar@cl.cam.ac.uk
</email>
<author confidence="0.996987">
Simone Teufel
</author>
<affiliation confidence="0.9768705">
University of Cambridge
Computer Laboratory
</affiliation>
<address confidence="0.9899505">
15 JJ Thomson Avenue
Cambridge, CB3 0FD, U.K.
</address>
<email confidence="0.999158">
simone.teufel@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998757789473684">
Sentiment analysis of citations in scientific pa-
pers and articles is a new and interesting prob-
lem which can open up many exciting new ap-
plications in bibliographic search and biblio-
metrics. Current work on citation sentiment
detection focuses on only the citation sen-
tence. In this paper, we address the problem
of context-enhanced citation sentiment detec-
tion. We present a new citation sentiment cor-
pus which has been annotated to take the dom-
inant sentiment in the entire citation context
into account. We believe that this gold stan-
dard is closer to the truth than annotation that
looks only at the citation sentence itself. We
then explore the effect of context windows of
different lengths on the performance of a state-
of-the-art citation sentiment detection system
when using this context-enhanced gold stan-
dard definition.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997266115384616">
Sentiment analysis of citations in scientific papers
and articles is a new and interesting problem. It can
open up many exciting new applications in biblio-
graphic search and in bibliometrics, i.e., the auto-
matic evaluation of the influence and impact of in-
dividuals and journals via citations. Automatic de-
tection of citation sentiment can also be used as a
first step to scientific summarisation (Abu-Jbara and
Radev, 2011). Alternatively, it can help researchers
during search, e.g., by identifying problems with a
particular approach, or by helping to recognise un-
addressed issues and possible gaps in the current re-
search.
However, there is a problem with the expression
of sentiment in scientific text. Conventionally, the
writing style in scientific writing is meant to be ob-
jective. Any personal bias by authors has to be
hedged (Hyland, 1995). Negative sentiment is po-
litically particularly dangerous (Ziman, 1968), and
some authors have documented the strategy of pref-
acing the intended criticism by slightly disingenuous
praise (MacRoberts and MacRoberts, 1984). This
makes the problem of identifying such opinions par-
ticularly challenging. This non-local expression of
sentiment has been observed in other genres as well
(Wilson et al., 2009; Polanyi and Zaenen, 2006).
</bodyText>
<figureCaption confidence="0.999291">
Figure 1: Example of anaphora in citations
</figureCaption>
<bodyText confidence="0.999189">
A typical case is illustrated in Figure 1. While the
first sentence praises some aspects of the cited pa-
per, the remaining sentences list its shortcomings. It
is clear that criticism is the intended sentiment, but
</bodyText>
<page confidence="0.901386333333333">
597
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999993257142857">
if we define our gold standard only by looking at
the citation sentence, we lose a significant amount
of sentiment hidden in the text. Given that most ci-
tations are neutral (Spiegel-Rosing, 1977; Teufel et
al., 2006), this makes it ever more important to re-
cover what explicit sentiment there is from the con-
text of the citation.
However, the dominant assumption in current ci-
tation identification methods (Ritchie et al., 2008;
Radev et al., 2009) is that the sentiment present in
the citation sentence represents the true sentiment
of the author towards the cited paper. This is due
to the difficulty of determining the relevant context,
whereas it is substantially easier to identify the cita-
tion sentence. In our example above, however, such
an approach would lead to the wrong prediction of
praise or neutral sentiment.
In this paper, we address the problem of context-
enhanced citation sentiment detection. We present
a new citation sentiment corpus where each citation
has been annotated according to the dominant sen-
timent in the corresponding citation context. We
claim that this corpus is closer to the truth than an-
notation that considers only the citation sentence it-
self. We show that it increases citation sentiment
coverage, particularly for negative sentiment. Using
this gold standard, we explore the effect of assum-
ing context windows of different but fixed lengths
on the performance of a state-of-the-art citation sen-
timent detection system where the sentiment of ci-
tation is considered in the entire context of the ci-
tation and more than one single sentiment can be
assigned. Previous approaches neither detect cita-
tion sentiment and context simultaneously nor use
as large a corpus as we do.
</bodyText>
<sectionHeader confidence="0.948651" genericHeader="introduction">
2 Corpus Construction
</sectionHeader>
<bodyText confidence="0.999776777777778">
We chose the dataset used by Athar (2011) compris-
ing 310 papers taken from the ACL Anthology (Bird
et al., 2008). The citation summary data from the
ACL Anthology Network1 (Radev et al., 2009) was
used. This dataset is rather large (8736 citations) and
since manual annotation of context for each citation
is a time consuming task, a subset of 20 papers were
selected corresponding to approximately 20% of the
original dataset.
</bodyText>
<footnote confidence="0.962934">
1http://www.aclweb.org
</footnote>
<bodyText confidence="0.999982666666667">
We selected a four-class scheme for annotation.
Every sentence that is in a window of 4 sentences
of the citation and does not contain any direct or in-
direct mention of the citation was labelled as being
excluded (x). The window length was motivated by
recent research (Qazvinian and Radev, 2010) which
shows the best score for a four-sentence boundary
when detecting non-explicit citation. The rest of the
sentences were marked either positive (p), negative
(n) or objective/neutral (o).
A total of 1,741 citations were annotated. Al-
though this annotation was performed by the first
author only, we know from previous work that simi-
lar styles of annotation can achieve acceptable inter-
annotator agreement (Teufel et al., 2006). An exam-
ple annotation for Smadja (1993) is given in Figure
2, where the first column shows the line number and
the second one shows the class label.
</bodyText>
<figureCaption confidence="0.995311">
Figure 2: Example annotation of a citation context.
</figureCaption>
<bodyText confidence="0.999973">
To compare our work with Athar (2011), we also
applied a three-class annotation scheme. In this
method of annotation, we merge the citation context
into a single sentence. Since the context introduces
more than one sentiment per citation, we marked the
citation sentiment with the last sentiment mentioned
in the context window as this is pragmatically most
likely to be the real intention (MacRoberts and Mac-
Roberts, 1984).
As is evident from Table 1, including the 4 sen-
tence window around the citation more than dou-
bles the instances of subjective sentiment, and in the
case of negative sentiment, this proportion rises to 3.
In light of the overall sparsity of detectable citation
sentiment in a paper, and of the envisaged applica-
</bodyText>
<page confidence="0.991803">
598
</page>
<bodyText confidence="0.999692">
tions, this is a very positive result. The reason for
this effect is most likely “sweetened criticism” – au-
thors’ strategic behaviour of softening the effect of
criticism among their peers (Hornsey et al., 2008).
</bodyText>
<table confidence="0.977547">
Without Context With Context
87% 73%
5% 17%
8% 11%
</table>
<tableCaption confidence="0.995448">
Table 1: Distribution of classes.
</tableCaption>
<sectionHeader confidence="0.990323" genericHeader="background">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999625">
We represent each citation as a feature set in a Sup-
port Vector Machine (SVM) (Cortes and Vapnik,
1995) framework and use n-grams of length 1 to 3
as well as dependency triplets as features. The de-
pendency triplets are constructed by merging the re-
lation, governor and dependent in a single string, for
instance, the relation nsubj(failed, method) is rep-
resented as nsubj failed method. This setup
has been shown to produce good results earlier as
well (Pang et al., 2002; Athar, 2011).
The first set of experiments focuses on simulta-
neous detection of sentiment and context sentences.
For this purpose, we use the four-class annotated
corpus described earlier. While the original anno-
tations were performed for a window of length 4,
we also experiment with asymmetrical windows of l
sentences preceding the citation and r sentences suc-
ceeding it. The detailed results are given in Table 2.
</bodyText>
<table confidence="0.9916124">
l r x o n p Fmacro Fmicro
0 0 - 1509 86 146 0.768 0.932
1 1 2823 1982 216 200 0.737 0.820
2 2 5984 2214 273 218 0.709 0.851
3 3 9170 2425 318 234 0.672 0.875
4 4 12385 2605 352 252 0.680 0.892
0 4 5963 2171 322 215 0.712 0.853
0 3 4380 2070 293 201 0.702 0.832
0 2 2817 1945 258 193 0.701 0.801
0 1 1280 1812 206 182 0.717 0.777
</table>
<tableCaption confidence="0.864624">
Table 2: Results for joint context and sentiment de-
tection.
</tableCaption>
<bodyText confidence="0.999557380952381">
Because of the skewed class distribution, we use
both the Fmacro and Fmicro scores with 10-fold
cross-validation. The baseline score, shown in bold,
is obtained with no context window and is compara-
ble to the results reported by Athar (2011). However,
we can observe that the F scores decrease as more
context is introduced. This may be attributed to the
increase in the vocabulary size of the n-grams and a
consequent reduction in the discriminating power of
the decision boundaries. These results show that the
task of jointly detecting sentiment and context is a
hard problem.
For our second set of experiments, we use the
three-class annotation scheme. We merge the text
of the sentences in the context windows as well as
their dependency triplets to obtain the features. The
results are reported in Table 3 with best results in
bold. Although these results are not better than the
context-less baseline, the reason might be data spar-
sity since existing work on citation sentiment analy-
sis uses more data (Athar, 2011).
</bodyText>
<figure confidence="0.708078777777778">
l r Fmacro Fmicro
1 1 0.638 0.827
2 2 0.620 0.793
3 3 0.629 0.786
4 4 0.628 0.771
0 4 0.643 0.796
0 3 0.658 0.816
0 2 0.642 0.824
0 1 0.731 0.871
</figure>
<tableCaption confidence="0.997553">
Table 3: Results using different context windows.
</tableCaption>
<sectionHeader confidence="0.999305" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999733785714286">
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-Rosing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), the only recent work on
citation sentiment detection using a relatively large
corpus is by Athar (2011). However, this work does
not handle citation context. Piao et al. (2007) pro-
posed a system to attach sentiment information to
the citation links between biomedical papers by us-
ing existing semantic lexical resources.
A common approach for sentiment detection is to
use a labelled lexicon to score sentences (Hatzivas-
siloglou and McKeown, 1997; Turney, 2002; Yu and
Hatzivassiloglou, 2003). However, such approaches
</bodyText>
<figure confidence="0.342803">
o
n
p
</figure>
<page confidence="0.990938">
599
</page>
<bodyText confidence="0.9998936">
have been found to be highly topic dependent (En-
gstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al.,
2007).
Teufel et al. (2006) worked on a 2,829 sentence ci-
tation corpus using a 12-class classification scheme.
Although they used context in their annotation, their
focus was on determining the author’s reason for cit-
ing a given paper. This task differs from citation sen-
timent, which is in a sense a “lower level” of analy-
sis.
For implicit citation extraction, Kaplan et al.
(2009) explore co-reference chains for citation ex-
traction using a combination of co-reference reso-
lution techniques. However, their corpus consists
of only 94 sentences of citations to 4 papers which
is likely to be too small to be representative. The
most relevant work is by Qazvinian and Radev
(2010) who extract only the non-explicit citations
for a given paper. They model each sentence as a
node in a graph and experiment with various win-
dow boundaries to create edges between neighbour-
ing nodes. However, their dataset consists of only 10
papers and their annotation scheme differs from our
four-class annotation as they do not deal with any
sentiment.
</bodyText>
<sectionHeader confidence="0.994528" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967235294118">
In this paper, we focus on automatic detection of
citation sentiment using the citation context. We
present a new corpus and show that ignoring the cita-
tion context would result in loss of a lot of sentiment,
specially criticism towards the cited paper. We also
report the results of the state-of-the-art citation sen-
timent detection systems on this corpus when using
this context-enhanced gold standard definition.
Future work directions may include improving
the detection algorithms by filtering the context sen-
tences more intelligently. For this purpose, exist-
ing work on coreference resolution (Lee et al., 2011)
may prove to be useful. Context features may also
be used for first filtering citations which have been
mentioned only in passing, and then applying con-
text based sentiment classification to the remaining
significant citations.
</bodyText>
<sectionHeader confidence="0.988656" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99828846">
A. Abu-Jbara and D. Radev. 2011. Coherent citation-
based summarization of scientific papers. In Proc. of
ACL.
A. Athar. 2011. Sentiment analysis of citations using
sentence structure-based features. In Proc of ACL,
page 81.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. of LREC.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In Proc. of ACL,
number 1.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273–297.
C. Engstr¨om. 2004. Topic dependence in sentiment clas-
sification. University of Cambridge.
M. Gamon and A. Aue. 2005. Automatic identification
of sentiment vocabulary: exploiting low association
with known sentiment terms. In Proc. of the ACL.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc. of
ACL, page 181.
M.J. Hornsey, E. Robson, J. Smith, S. Esposo, and R.M.
Sutton. 2008. Sugaring the pill: Assessing rhetori-
cal strategies designed to minimize defensive reactions
to group criticism. Human Communication Research,
34(1):70–98.
K. Hyland. 1995. The Author in the Text: Hedging Sci-
entific Writing. Hong Kong papers in linguistics and
language teaching, 18:11.
D. Kaplan, R. Iida, and T. Tokunaga. 2009. Automatic
extraction of citation contexts for research paper sum-
marization: A coreference-chain based approach. In
Proc. of the 2009 Workshop on Text and Citation Anal-
ysis for Scholarly Digital Libraries.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford’s multi-pass
sieve coreference resolution system at the conll-2011
shared task. ACL HLT 2011.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91–94.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926–931. Citeseer.
</reference>
<page confidence="0.960624">
600
</page>
<reference confidence="0.999665375">
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proc. of EMNLP.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS). Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1–10.
V. Qazvinian and D.R. Radev. 2010. Identifying non-
explicit citing sentences for citation-based summariza-
tion. In Proc. of ACL.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Soc. for Info. Sci. and Tech.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Com-
paring citation contexts for information retrieval. In
Proc. of ACM conference on Information and knowl-
edge management, pages 213–222. ACM.
I. Spiegel-Rosing. 1977. Science studies: Bibliometric
and content analysis. Social Studies of Science.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proc. of
EMNLP, pages 103–110.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of ACL.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: an exploration of fea-
tures for phrase-level sentiment analysis. Comp. Ling.,
35(3):399–433.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc.
of EMNLP, page 136.
J.M. Ziman. 1968. Public Knowledge: An essay con-
cerning the social dimension of science. Cambridge
Univ. Press, College Station, Texas.
</reference>
<page confidence="0.998226">
601
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.344376">
<title confidence="0.986312">Context-Enhanced Citation Sentiment Detection</title>
<author confidence="0.518051">Awais</author>
<affiliation confidence="0.9886">University of Computer</affiliation>
<address confidence="0.9593195">15 JJ Thomson Cambridge, CB3 0FD,</address>
<email confidence="0.997986">awais.athar@cl.cam.ac.uk</email>
<author confidence="0.978039">Simone</author>
<affiliation confidence="0.995306">University of Computer</affiliation>
<address confidence="0.959483">15 JJ Thomson Cambridge, CB3 0FD,</address>
<email confidence="0.999447">simone.teufel@cl.cam.ac.uk</email>
<abstract confidence="0.9910195">Sentiment analysis of citations in scientific papers and articles is a new and interesting problem which can open up many exciting new applications in bibliographic search and bibliometrics. Current work on citation sentiment detection focuses on only the citation sentence. In this paper, we address the problem of context-enhanced citation sentiment detection. We present a new citation sentiment corpus which has been annotated to take the dominant sentiment in the entire citation context into account. We believe that this gold standard is closer to the truth than annotation that looks only at the citation sentence itself. We then explore the effect of context windows of different lengths on the performance of a stateof-the-art citation sentiment detection system when using this context-enhanced gold standard definition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abu-Jbara</author>
<author>D Radev</author>
</authors>
<title>Coherent citationbased summarization of scientific papers.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1584" citStr="Abu-Jbara and Radev, 2011" startWordPosition="238" endWordPosition="241"> the effect of context windows of different lengths on the performance of a stateof-the-art citation sentiment detection system when using this context-enhanced gold standard definition. 1 Introduction Sentiment analysis of citations in scientific papers and articles is a new and interesting problem. It can open up many exciting new applications in bibliographic search and in bibliometrics, i.e., the automatic evaluation of the influence and impact of individuals and journals via citations. Automatic detection of citation sentiment can also be used as a first step to scientific summarisation (Abu-Jbara and Radev, 2011). Alternatively, it can help researchers during search, e.g., by identifying problems with a particular approach, or by helping to recognise unaddressed issues and possible gaps in the current research. However, there is a problem with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly dising</context>
</contexts>
<marker>Abu-Jbara, Radev, 2011</marker>
<rawString>A. Abu-Jbara and D. Radev. 2011. Coherent citationbased summarization of scientific papers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Athar</author>
</authors>
<title>Sentiment analysis of citations using sentence structure-based features.</title>
<date>2011</date>
<booktitle>In Proc of ACL,</booktitle>
<pages>81</pages>
<contexts>
<context position="4702" citStr="Athar (2011)" startWordPosition="733" endWordPosition="734">on sentence itself. We show that it increases citation sentiment coverage, particularly for negative sentiment. Using this gold standard, we explore the effect of assuming context windows of different but fixed lengths on the performance of a state-of-the-art citation sentiment detection system where the sentiment of citation is considered in the entire context of the citation and more than one single sentiment can be assigned. Previous approaches neither detect citation sentiment and context simultaneously nor use as large a corpus as we do. 2 Corpus Construction We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008). The citation summary data from the ACL Anthology Network1 (Radev et al., 2009) was used. This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset. 1http://www.aclweb.org We selected a four-class scheme for annotation. Every sentence that is in a window of 4 sentences of the citation and does not contain any direct or indirect mention of the citation was labe</context>
<context position="6079" citStr="Athar (2011)" startWordPosition="959" endWordPosition="960">en detecting non-explicit citation. The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o). A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006). An example annotation for Smadja (1993) is given in Figure 2, where the first column shows the line number and the second one shows the class label. Figure 2: Example annotation of a citation context. To compare our work with Athar (2011), we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984). As is evident from Table 1, including the 4 sentence window around the citation more than doubles the instances of subjective sentiment, and in the case of negative sentiment, this proportion rises to 3. In light</context>
<context position="7590" citStr="Athar, 2011" startWordPosition="1213" endWordPosition="1214"> et al., 2008). Without Context With Context 87% 73% 5% 17% 8% 11% Table 1: Distribution of classes. 3 Experiments and Results We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method. This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011). The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it. The detailed results are given in Table 2. l r x o n p Fmacro Fmicro 0 0 - 1509 86 146 0.768 0.932 1 1 2823 1982 216 200 0.737 0.820 2 2 5984 2214 273 218 0.709 0.851 3 3 9170 2425 318 234 0.672 0.875 4 4 12385 2605 352 252 0.680 0.892 0</context>
<context position="9407" citStr="Athar, 2011" startWordPosition="1539" endWordPosition="1540">nsequent reduction in the discriminating power of the decision boundaries. These results show that the task of jointly detecting sentiment and context is a hard problem. For our second set of experiments, we use the three-class annotation scheme. We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011). l r Fmacro Fmicro 1 1 0.638 0.827 2 2 0.620 0.793 3 3 0.629 0.786 4 4 0.628 0.771 0 4 0.643 0.796 0 3 0.658 0.816 0 2 0.642 0.824 0 1 0.731 0.871 Table 3: Results using different context windows. 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment i</context>
</contexts>
<marker>Athar, 2011</marker>
<rawString>A. Athar. 2011. Sentiment analysis of citations using sentence structure-based features. In Proc of ACL, page 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>R Dale</author>
<author>B J Dorr</author>
<author>B Gibson</author>
<author>M T Joseph</author>
<author>M Y Kan</author>
<author>D Lee</author>
<author>B Powley</author>
<author>D R Radev</author>
<author>Y F Tan</author>
</authors>
<title>The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.</title>
<date>2008</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="4773" citStr="Bird et al., 2008" startWordPosition="744" endWordPosition="747">coverage, particularly for negative sentiment. Using this gold standard, we explore the effect of assuming context windows of different but fixed lengths on the performance of a state-of-the-art citation sentiment detection system where the sentiment of citation is considered in the entire context of the citation and more than one single sentiment can be assigned. Previous approaches neither detect citation sentiment and context simultaneously nor use as large a corpus as we do. 2 Corpus Construction We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008). The citation summary data from the ACL Anthology Network1 (Radev et al., 2009) was used. This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset. 1http://www.aclweb.org We selected a four-class scheme for annotation. Every sentence that is in a window of 4 sentences of the citation and does not contain any direct or indirect mention of the citation was labelled as being excluded (x). The window length was motivated by recent r</context>
</contexts>
<marker>Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, Tan, 2008</marker>
<rawString>S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan. 2008. The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL, number 1.</booktitle>
<contexts>
<context position="10425" citStr="Blitzer et al., 2007" startWordPosition="1709" endWordPosition="1712">nt work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. of ACL, number 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="7208" citStr="Cortes and Vapnik, 1995" startWordPosition="1146" endWordPosition="1149">bjective sentiment, and in the case of negative sentiment, this proportion rises to 3. In light of the overall sparsity of detectable citation sentiment in a paper, and of the envisaged applica598 tions, this is a very positive result. The reason for this effect is most likely “sweetened criticism” – authors’ strategic behaviour of softening the effect of criticism among their peers (Hornsey et al., 2008). Without Context With Context 87% 73% 5% 17% 8% 11% Table 1: Distribution of classes. 3 Experiments and Results We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method. This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011). The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were perfo</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Engstr¨om</author>
</authors>
<title>Topic dependence in sentiment classification.</title>
<date>2004</date>
<institution>University of Cambridge.</institution>
<marker>Engstr¨om, 2004</marker>
<rawString>C. Engstr¨om. 2004. Topic dependence in sentiment classification. University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>A Aue</author>
</authors>
<title>Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="10402" citStr="Gamon and Aue, 2005" startWordPosition="1705" endWordPosition="1708"> 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is li</context>
</contexts>
<marker>Gamon, Aue, 2005</marker>
<rawString>M. Gamon and A. Aue. 2005. Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garzone</author>
<author>R Mercer</author>
</authors>
<title>Towards an automated citation classifier.</title>
<date>2000</date>
<booktitle>Advances in Artificial Intelligence.</booktitle>
<contexts>
<context position="9789" citStr="Garzone and Mercer, 2000" startWordPosition="1606" endWordPosition="1609"> results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011). l r Fmacro Fmicro 1 1 0.638 0.827 2 2 0.620 0.793 3 3 0.629 0.786 4 4 0.628 0.771 0 4 0.643 0.796 0 3 0.658 0.816 0 2 0.642 0.824 0 1 0.731 0.871 Table 3: Results using different context windows. 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon </context>
</contexts>
<marker>Garzone, Mercer, 2000</marker>
<rawString>M. Garzone and R. Mercer. 2000. Towards an automated citation classifier. Advances in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>181</pages>
<contexts>
<context position="10237" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="1676" endWordPosition="1680">. 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for ci</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K.R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of ACL, page 181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Hornsey</author>
<author>E Robson</author>
<author>J Smith</author>
<author>S Esposo</author>
<author>R M Sutton</author>
</authors>
<title>Sugaring the pill: Assessing rhetorical strategies designed to minimize defensive reactions to group criticism.</title>
<date>2008</date>
<journal>Human Communication Research,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="6992" citStr="Hornsey et al., 2008" startWordPosition="1108" endWordPosition="1111">s this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984). As is evident from Table 1, including the 4 sentence window around the citation more than doubles the instances of subjective sentiment, and in the case of negative sentiment, this proportion rises to 3. In light of the overall sparsity of detectable citation sentiment in a paper, and of the envisaged applica598 tions, this is a very positive result. The reason for this effect is most likely “sweetened criticism” – authors’ strategic behaviour of softening the effect of criticism among their peers (Hornsey et al., 2008). Without Context With Context 87% 73% 5% 17% 8% 11% Table 1: Distribution of classes. 3 Experiments and Results We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method. This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011). </context>
</contexts>
<marker>Hornsey, Robson, Smith, Esposo, Sutton, 2008</marker>
<rawString>M.J. Hornsey, E. Robson, J. Smith, S. Esposo, and R.M. Sutton. 2008. Sugaring the pill: Assessing rhetorical strategies designed to minimize defensive reactions to group criticism. Human Communication Research, 34(1):70–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hyland</author>
</authors>
<title>The Author in the Text: Hedging Scientific Writing. Hong Kong papers in linguistics and language teaching,</title>
<date>1995</date>
<pages>18--11</pages>
<contexts>
<context position="2010" citStr="Hyland, 1995" startWordPosition="308" endWordPosition="309">ce and impact of individuals and journals via citations. Automatic detection of citation sentiment can also be used as a first step to scientific summarisation (Abu-Jbara and Radev, 2011). Alternatively, it can help researchers during search, e.g., by identifying problems with a particular approach, or by helping to recognise unaddressed issues and possible gaps in the current research. However, there is a problem with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006). Figure 1: Example of anaphora in citations A typical case is illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sen</context>
</contexts>
<marker>Hyland, 1995</marker>
<rawString>K. Hyland. 1995. The Author in the Text: Hedging Scientific Writing. Hong Kong papers in linguistics and language teaching, 18:11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kaplan</author>
<author>R Iida</author>
<author>T Tokunaga</author>
</authors>
<title>Automatic extraction of citation contexts for research paper summarization: A coreference-chain based approach.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries.</booktitle>
<contexts>
<context position="10802" citStr="Kaplan et al. (2009)" startWordPosition="1773" endWordPosition="1776"> to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. The most relevant work is by Qazvinian and Radev (2010) who extract only the non-explicit citations for a given paper. They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes. However, their dataset consists of only 10 papers and their annotation scheme differs from our four-cl</context>
</contexts>
<marker>Kaplan, Iida, Tokunaga, 2009</marker>
<rawString>D. Kaplan, R. Iida, and T. Tokunaga. 2009. Automatic extraction of citation contexts for research paper summarization: A coreference-chain based approach. In Proc. of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lee</author>
<author>Y Peirsman</author>
<author>A Chang</author>
<author>N Chambers</author>
<author>M Surdeanu</author>
<author>D Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<journal>ACL HLT</journal>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and D. Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. ACL HLT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H MacRoberts</author>
<author>B R MacRoberts</author>
</authors>
<title>The negational reference: Or the art of dissembling.</title>
<date>1984</date>
<journal>Social Studies of Science,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="2231" citStr="MacRoberts and MacRoberts, 1984" startWordPosition="336" endWordPosition="339">, it can help researchers during search, e.g., by identifying problems with a particular approach, or by helping to recognise unaddressed issues and possible gaps in the current research. However, there is a problem with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006). Figure 1: Example of anaphora in citations A typical case is illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, page</context>
<context position="6465" citStr="MacRoberts and MacRoberts, 1984" startWordPosition="1017" endWordPosition="1021"> 2006). An example annotation for Smadja (1993) is given in Figure 2, where the first column shows the line number and the second one shows the class label. Figure 2: Example annotation of a citation context. To compare our work with Athar (2011), we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984). As is evident from Table 1, including the 4 sentence window around the citation more than doubles the instances of subjective sentiment, and in the case of negative sentiment, this proportion rises to 3. In light of the overall sparsity of detectable citation sentiment in a paper, and of the envisaged applica598 tions, this is a very positive result. The reason for this effect is most likely “sweetened criticism” – authors’ strategic behaviour of softening the effect of criticism among their peers (Hornsey et al., 2008). Without Context With Context 87% 73% 5% 17% 8% 11% Table 1: Distributio</context>
</contexts>
<marker>MacRoberts, MacRoberts, 1984</marker>
<rawString>M.H. MacRoberts and B.R. MacRoberts. 1984. The negational reference: Or the art of dissembling. Social Studies of Science, 14(1):91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nanba</author>
<author>M Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In IJCAI,</booktitle>
<volume>16</volume>
<pages>926--931</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="9762" citStr="Nanba and Okumura, 1999" startWordPosition="1602" endWordPosition="1605"> obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011). l r Fmacro Fmicro 1 1 0.638 0.827 2 2 0.620 0.793 3 3 0.629 0.786 4 4 0.628 0.771 0 4 0.643 0.796 0 3 0.658 0.816 0 2 0.642 0.824 0 1 0.731 0.871 Table 3: Results using different context windows. 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic depende</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>H. Nanba and M. Okumura. 1999. Towards multi-paper summarization using reference information. In IJCAI, volume 16, pages 926–931. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7576" citStr="Pang et al., 2002" startWordPosition="1209" endWordPosition="1212">heir peers (Hornsey et al., 2008). Without Context With Context 87% 73% 5% 17% 8% 11% Table 1: Distribution of classes. 3 Experiments and Results We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method. This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011). The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it. The detailed results are given in Table 2. l r x o n p Fmacro Fmicro 0 0 - 1509 86 146 0.768 0.932 1 1 2823 1982 216 200 0.737 0.820 2 2 5984 2214 273 218 0.709 0.851 3 3 9170 2425 318 234 0.672 0.875 4 4 12385 2605 352 252</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Piao</author>
<author>S Ananiadou</author>
<author>Y Tsuruoka</author>
<author>Y Sasaki</author>
<author>J McNaught</author>
</authors>
<title>Mining opinion polarity relations of citations.</title>
<date>2007</date>
<booktitle>In International Workshop on Computational Semantics (IWCS).</booktitle>
<publisher>Citeseer.</publisher>
<contexts>
<context position="9967" citStr="Piao et al. (2007)" startWordPosition="1635" endWordPosition="1638">n citation sentiment analysis uses more data (Athar, 2011). l r Fmacro Fmicro 1 1 0.638 0.827 2 2 0.620 0.793 3 3 0.629 0.786 4 4 0.628 0.771 0 4 0.643 0.796 0 3 0.658 0.816 0 2 0.642 0.824 0 1 0.731 0.871 Table 3: Results using different context windows. 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their </context>
</contexts>
<marker>Piao, Ananiadou, Tsuruoka, Sasaki, McNaught, 2007</marker>
<rawString>S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. McNaught. 2007. Mining opinion polarity relations of citations. In International Workshop on Computational Semantics (IWCS). Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual valence shifters. Computing attitude and affect in text: Theory and applications,</title>
<date>2006</date>
<pages>1--10</pages>
<contexts>
<context position="2439" citStr="Polanyi and Zaenen, 2006" startWordPosition="368" endWordPosition="371"> with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006). Figure 1: Example of anaphora in citations A typical case is illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a significant amount of sentimen</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>L. Polanyi and A. Zaenen. 2006. Contextual valence shifters. Computing attitude and affect in text: Theory and applications, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Qazvinian</author>
<author>D R Radev</author>
</authors>
<title>Identifying nonexplicit citing sentences for citation-based summarization.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5408" citStr="Qazvinian and Radev, 2010" startWordPosition="847" endWordPosition="850">ation summary data from the ACL Anthology Network1 (Radev et al., 2009) was used. This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset. 1http://www.aclweb.org We selected a four-class scheme for annotation. Every sentence that is in a window of 4 sentences of the citation and does not contain any direct or indirect mention of the citation was labelled as being excluded (x). The window length was motivated by recent research (Qazvinian and Radev, 2010) which shows the best score for a four-sentence boundary when detecting non-explicit citation. The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o). A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006). An example annotation for Smadja (1993) is given in Figure 2, where the first column shows the line number and the second one shows the class label. Figure 2: Example </context>
<context position="11100" citStr="Qazvinian and Radev (2010)" startWordPosition="1821" endWordPosition="1824"> citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. The most relevant work is by Qazvinian and Radev (2010) who extract only the non-explicit citations for a given paper. They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes. However, their dataset consists of only 10 papers and their annotation scheme differs from our four-class annotation as they do not deal with any sentiment. 5 Conclusion In this paper, we focus on automatic detection of citation sentiment using the citation context. We present a new corpus and show that ignoring the citation context would result in loss of a lot of sentiment, specially criticism t</context>
</contexts>
<marker>Qazvinian, Radev, 2010</marker>
<rawString>V. Qazvinian and D.R. Radev. 2010. Identifying nonexplicit citing sentences for citation-based summarization. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>M T Joseph</author>
<author>B Gibson</author>
<author>P Muthukrishnan</author>
</authors>
<title>A Bibliometric and Network Analysis of the field of Computational Linguistics.</title>
<date>2009</date>
<journal>Journal of the American Soc. for Info. Sci. and Tech.</journal>
<contexts>
<context position="3374" citStr="Radev et al., 2009" startWordPosition="516" endWordPosition="519">Association for Computational Linguistics: Human Language Technologies, pages 597–601, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a significant amount of sentiment hidden in the text. Given that most citations are neutral (Spiegel-Rosing, 1977; Teufel et al., 2006), this makes it ever more important to recover what explicit sentiment there is from the context of the citation. However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. In our example above, however, such an approach would lead to the wrong prediction of praise or neutral sentiment. In this paper, we address the problem of contextenhanced citation sentiment detection. We present a new citation sentiment corpus where each citation has been annotated according to the dominant sentiment in the correspondi</context>
<context position="4853" citStr="Radev et al., 2009" startWordPosition="757" endWordPosition="760">lore the effect of assuming context windows of different but fixed lengths on the performance of a state-of-the-art citation sentiment detection system where the sentiment of citation is considered in the entire context of the citation and more than one single sentiment can be assigned. Previous approaches neither detect citation sentiment and context simultaneously nor use as large a corpus as we do. 2 Corpus Construction We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008). The citation summary data from the ACL Anthology Network1 (Radev et al., 2009) was used. This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset. 1http://www.aclweb.org We selected a four-class scheme for annotation. Every sentence that is in a window of 4 sentences of the citation and does not contain any direct or indirect mention of the citation was labelled as being excluded (x). The window length was motivated by recent research (Qazvinian and Radev, 2010) which shows the best score for a four-senten</context>
</contexts>
<marker>Radev, Joseph, Gibson, Muthukrishnan, 2009</marker>
<rawString>D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrishnan. 2009. A Bibliometric and Network Analysis of the field of Computational Linguistics. Journal of the American Soc. for Info. Sci. and Tech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritchie</author>
<author>S Robertson</author>
<author>S Teufel</author>
</authors>
<title>Comparing citation contexts for information retrieval.</title>
<date>2008</date>
<booktitle>In Proc. of ACM conference on Information and knowledge management,</booktitle>
<pages>213--222</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3353" citStr="Ritchie et al., 2008" startWordPosition="512" endWordPosition="515">erican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a significant amount of sentiment hidden in the text. Given that most citations are neutral (Spiegel-Rosing, 1977; Teufel et al., 2006), this makes it ever more important to recover what explicit sentiment there is from the context of the citation. However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. In our example above, however, such an approach would lead to the wrong prediction of praise or neutral sentiment. In this paper, we address the problem of contextenhanced citation sentiment detection. We present a new citation sentiment corpus where each citation has been annotated according to the dominant sentime</context>
</contexts>
<marker>Ritchie, Robertson, Teufel, 2008</marker>
<rawString>A. Ritchie, S. Robertson, and S. Teufel. 2008. Comparing citation contexts for information retrieval. In Proc. of ACM conference on Information and knowledge management, pages 213–222. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Spiegel-Rosing</author>
</authors>
<title>Science studies: Bibliometric and content analysis.</title>
<date>1977</date>
<journal>Social Studies of Science.</journal>
<contexts>
<context position="3120" citStr="Spiegel-Rosing, 1977" startWordPosition="475" endWordPosition="476">s illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a significant amount of sentiment hidden in the text. Given that most citations are neutral (Spiegel-Rosing, 1977; Teufel et al., 2006), this makes it ever more important to recover what explicit sentiment there is from the context of the citation. However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. In our example above, however, such an approach would lead to the wrong prediction o</context>
<context position="9737" citStr="Spiegel-Rosing, 1977" startWordPosition="1600" endWordPosition="1601">dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011). l r Fmacro Fmicro 1 1 0.638 0.827 2 2 0.620 0.793 3 3 0.629 0.786 4 4 0.628 0.771 0 4 0.643 0.796 0 3 0.658 0.816 0 2 0.642 0.824 0 1 0.731 0.871 Table 3: Results using different context windows. 4 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found t</context>
</contexts>
<marker>Spiegel-Rosing, 1977</marker>
<rawString>I. Spiegel-Rosing. 1977. Science studies: Bibliometric and content analysis. Social Studies of Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>D Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>103--110</pages>
<contexts>
<context position="3142" citStr="Teufel et al., 2006" startWordPosition="477" endWordPosition="480">e 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a significant amount of sentiment hidden in the text. Given that most citations are neutral (Spiegel-Rosing, 1977; Teufel et al., 2006), this makes it ever more important to recover what explicit sentiment there is from the context of the citation. However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. In our example above, however, such an approach would lead to the wrong prediction of praise or neutral se</context>
<context position="5839" citStr="Teufel et al., 2006" startWordPosition="914" endWordPosition="917">tation and does not contain any direct or indirect mention of the citation was labelled as being excluded (x). The window length was motivated by recent research (Qazvinian and Radev, 2010) which shows the best score for a four-sentence boundary when detecting non-explicit citation. The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o). A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006). An example annotation for Smadja (1993) is given in Figure 2, where the first column shows the line number and the second one shows the class label. Figure 2: Example annotation of a citation context. To compare our work with Athar (2011), we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRob</context>
<context position="10447" citStr="Teufel et al. (2006)" startWordPosition="1713" endWordPosition="1716">timent detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. Th</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Automatic classification of citation function. In Proc. of EMNLP, pages 103–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10251" citStr="Turney, 2002" startWordPosition="1681" endWordPosition="1682">emes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extract</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P.D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Comp. Ling.,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="2412" citStr="Wilson et al., 2009" startWordPosition="364" endWordPosition="367">r, there is a problem with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006). Figure 1: Example of anaphora in citations A typical case is illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended sentiment, but 597 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597–601, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics if we define our gold standard only by looking at the citation sentence, we lose a sig</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis. Comp. Ling., 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>136</pages>
<contexts>
<context position="10283" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="1683" endWordPosition="1686"> proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011). However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, such approaches o n p 599 have been found to be highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author’s reason for citing a given paper. This task differs from citation sentiment, which is in a sense a “lower level” of analysis. For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-re</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proc. of EMNLP, page 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ziman</author>
</authors>
<title>Public Knowledge: An essay concerning the social dimension of science.</title>
<date>1968</date>
<publisher>Cambridge Univ. Press,</publisher>
<location>College Station, Texas.</location>
<contexts>
<context position="2082" citStr="Ziman, 1968" startWordPosition="317" endWordPosition="318">ion of citation sentiment can also be used as a first step to scientific summarisation (Abu-Jbara and Radev, 2011). Alternatively, it can help researchers during search, e.g., by identifying problems with a particular approach, or by helping to recognise unaddressed issues and possible gaps in the current research. However, there is a problem with the expression of sentiment in scientific text. Conventionally, the writing style in scientific writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006). Figure 1: Example of anaphora in citations A typical case is illustrated in Figure 1. While the first sentence praises some aspects of the cited paper, the remaining sentences list its shortcomings. It is clear that criticism is the intended</context>
</contexts>
<marker>Ziman, 1968</marker>
<rawString>J.M. Ziman. 1968. Public Knowledge: An essay concerning the social dimension of science. Cambridge Univ. Press, College Station, Texas.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>