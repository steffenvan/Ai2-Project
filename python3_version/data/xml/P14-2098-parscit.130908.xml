<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000820">
<title confidence="0.972241">
Improved Correction Detection in Revised ESL Sentences
</title>
<author confidence="0.990961">
Huichao Xue and Rebecca Hwa
</author>
<affiliation confidence="0.9989135">
Department of Computer Science,
University of Pittsburgh,
</affiliation>
<address confidence="0.495121">
210 S Bouquet St, Pittsburgh, PA 15260, USA
</address>
<email confidence="0.999301">
{hux10,hwa}@cs.pitt.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978583333333">
This work explores methods of automat-
ically detecting corrections of individual
mistakes in sentence revisions for ESL
students. We have trained a classifier
that specializes in determining whether
consecutive basic-edits (word insertions,
deletions, substitutions) address the same
mistake. Experimental result shows that
the proposed system achieves an Fl-score
of 81% on correction detection and 66%
for the overall system, out-performing the
baseline by a large margin.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996401754386">
Quality feedback from language tutors can
help English-as-a-Second-Language (ESL) stu-
dents improve their writing skills. One of the tu-
tors’ tasks is to isolate writing mistakes within
sentences, and point out (1) why each case is
considered a mistake, and (2) how each mistake
should be corrected. Because this is time consum-
ing, tutors often just rewrite the sentences with-
out giving any explanations (Fregeau, 1999). Due
to the effort involved in comparing revisions with
the original texts, students often fail to learn from
these revisions (Williams, 2003).
Computer aided language learning tools offer
a solution for providing more detailed feedback.
Programs can be developed to compare the stu-
dent’s original sentences with the tutor-revised
sentences. Swanson and Yamangil (2012) have
proposed a promising framework for this purpose.
Their approach has two components: one to de-
tect individual corrections within a revision, which
they termed correction detection; another to deter-
mine what the correction fixes, which they termed
error type selection. Although they reported a
high accuracy for the error type selection classifier
alone, the bottleneck of their system is the other
component – correction detection. An analysis of
their system shows that approximately 70% of the
system’s mistakes are caused by mis-detections
in the first place. Their correction detection al-
gorithm relies on a set of heuristics developed
from one single data collection (the FCE corpus
(Yannakoudakis et al., 2011)). When determining
whether a set of basic-edits (word insertions, dele-
tions, substitutions) contributes to the same cor-
rection, these heuristics lack the flexibility to adapt
to a specific context. Furthermore, it is not clear if
the heuristics will work as well for tutors trained
to mark up revisions under different guidelines.
We propose to improve upon the correction de-
tection component by training a classifier that de-
termines which edits in a revised sentence address
the same error in the original sentence. The classi-
fier can make more accurate decisions adjusted to
contexts. Because the classifier were trained on re-
visions where corrections are explicitly marked by
English experts, it is also possible to build systems
adjusted to different annotation standards.
The contributions of this paper are: (1) We show
empirically that a major challenge in correction
detection is to determine the number of edits that
address the same error. (2) We have developed a
merging model that reduces mis-detection by 1/3,
leading to significant improvement in the accu-
racies of combined correction detection and er-
ror type selection. (3) We have conducted experi-
ments across multiple corpora, indicating that the
proposed merging model is generalizable.
</bodyText>
<sectionHeader confidence="0.960673" genericHeader="method">
2 Correction Detection
</sectionHeader>
<bodyText confidence="0.997909857142857">
Comparing a student-written sentence with its re-
vision, we observe that each correction can be de-
composed into a set of more basic edits such as
word insertions, word deletions and word substi-
tutions. In the example shown in Figure 1, the
correction “to change ⇒ changing” is composed
of a deletion of to and a substitution from change
</bodyText>
<page confidence="0.980858">
599
</page>
<note confidence="0.603285">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 599–604,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.961308166666667">
Figure 1: Detecting corrections from revisions. Our system detects individual corrections by comparing the original sentence
with its revision, so that each correction addresses one error. Each polygon corresponds to one correction; the labels are codes
of the error types. The codes follow the annotation standard in FCE corpus (Nicholls, 2003). In this example, W is incorrect
Word order; UT is Unnecessary preposiTion; FV is wrong Verb Form; RN is Nnoun needs to be Replaced; ID is IDiom error.
Figure 2: A portion of the example from Figure 1 undergoing the two-step correction detection process. The basic edits are
indicated by black polygons. The corrections are shown in red polygons.
Figure 3: Basic edits extracted by the edit-distance algo-
rithm (Levenshtein, 1966) do not necessarily match our lin-
guistic intuition. The ideal basic-edits are shown in Figure
3a, but since the algorithm only cares about minimizing the
number of edits, it may end up extracting basic-edits shown
in Figure 3b.
</figureCaption>
<bodyText confidence="0.99964575">
to changing; the correction “moment ⇒ minute”
is itself a single word substitution. Thus, we can
build systems to detect corrections which operates
in two steps: (1) detecting the basic edits that took
place during the revision, and (2) merging those
basic edits that address the same error. Figure 2 il-
lustrates the process for a fragment of the example
sentence from Figure 1.
In practice, however, this two-step approach
may result in mis-detections due to ambiguities.
Mis-detections may be introduced from either
steps. While detecting basic edits, Figures 3 gives
an example of problems that might arise. Because
the Levenshtein algorithm only tries to minimize
the number of edits, it does not care whether the
edits make any linguistic sense. For merging basic
edits, Swanson and Yamangil applied a distance
heuristic – basic-edits that are close to each other
(e.g. basic edits with at most one word lying in
between) are merged. Figure 4 shows cases for
which the heuristic results in the wrong scope.
These errors caused their system to mis-detect
30% of the corrections. Since mis-detected cor-
rections cannot be analyzed down the pipeline,
</bodyText>
<listItem confidence="0.622295166666667">
(a) The basic edits are addressing the same problem. But
these basic edits are non-adjacent, and therefore not merged by
S&amp;Y’s algorithm.
(b) The basic edits in the above two cases address different
problems though they are adjacent. S&amp;Y’s merging algorithm
incorrectly merges them.
</listItem>
<figureCaption confidence="0.996456666666667">
Figure 4: Merging mistakes by the algorithm proposed in
Swanson and Yamangil (2012) (S&amp;Y), which merges adja-
cent basic edits.
</figureCaption>
<bodyText confidence="0.9989015">
the correction detection component became the
bottle-neck of their overall system. Out of the
42% corrections that are incorrectly analyzed1,
30%/42%≈70% are caused by mis-detections in
the first place. An improvement in correction de-
tection may increase the system accuracy overall.
We conducted an error analysis to attribute er-
rors to either step when the system detects a wrong
set of corrections for a sentence. We examine
the first step’s output. If the resulting basic ed-
its do not match with those that compose the ac-
tual corrections, we attribute the error to the first
step. Otherwise, we attribute the error to the sec-
ond step. Our analysis confirms that the merging
step is the bottleneck in the current correction de-
tection system – it accounts for 75% of the mis-
detections. Therefore, to effectively reduce the
algorithm’s mis-detection errors, we propose to
</bodyText>
<footnote confidence="0.787669">
1Swanson and Yamangil reported an overall system with
58% F-score.
</footnote>
<figure confidence="0.93348">
(a) (b)
</figure>
<page confidence="0.965178">
600
</page>
<bodyText confidence="0.999634857142857">
build a classifier to merge with better accuracies.
Other previous tasks also involve comparing
two sentences. Unlike evaluating grammar er-
ror correction systems (Dahlmeier and Ng, 2012),
correction detection cannot refer to a gold stan-
dard. Our error analysis above also highlights our
task’s difference with previous work that identify
corresponding phrases between two sentences, in-
cluding phrase extraction (Koehn et al., 2003) and
paraphrase extraction (Cohn et al., 2008). They
are fundamentally different in that the granularity
of the extracted phrase pairs is a major concern
in our work – we need to guarantee each detected
phrase pair to address exactly one writing prob-
lem. In comparison, phrase extraction systems
aim to improve the end-to-end MT or paraphrasing
systems. A bigger concern is to guarantee the ex-
tracted phrase pairs are indeed translations or para-
phrases. Recent work therefore focuses on identi-
fying the alignment/edits between two sentences
(Snover et al., 2009; Heilman and Smith, 2010).
</bodyText>
<sectionHeader confidence="0.968956" genericHeader="method">
3 A Classifier for Merging Basic-Edits
</sectionHeader>
<bodyText confidence="0.999672714285714">
Figures 4 highlights the problems with indiscrimi-
nantly merging basic-edits that are adjacent. Intu-
itively, it seems that the decision should be more
context dependent. Certain patterns may indicate
that two adjacent basic-edits are a part of the same
correction while others may indicate that they each
address a different problem. For example, in Fig-
ure 5a, when the insertion of one word is followed
by the deletion of the same word, the insertion
and deletion are likely addressing one single error.
This is because these two edits would combine to-
gether as a word-order change. On the other hand,
in Figure 5b, if one edit includes a substitution be-
tween words with the same POS’s, then it is likely
fixing a word choice error by itself. In this case, it
should not be merged with other edits.
To predict whether two basic-edits address the
same writing problem more discriminatively, we
train a Maximum Entropy binary classifier based
on features extracted from relevant contexts for
the basic edits. We use features in Table 1 in the
proposed classifier. We design the features to in-
dicate: (A) whether merging the two basic-edits
matches the pattern for a common correction. (B)
whether one basic-edit addresses one single error.
We train the classifier using samples extracted
from revisions where individual corrections are
explicitly annotated. We first extract the basic-
</bodyText>
<figure confidence="0.678916">
(b) The pattern indicates that
the two edits do not address
the same problem
</figure>
<figureCaption confidence="0.9914537">
Figure 5: Patterns indicating whether two edits address the
same writing mistake.
Figure 6: Extracting training instances for the merger. Our
goal is to train classifiers to tell if two basic edits should
be merged (True or False). We break each correction (outer
polygons, also colored in red) in the training corpus into a set
of basic edits (black polygons). We construct an instance for
each consecutive pair of basic edits. If two basic edits were
extracted from the same correction, we will mark the outcome
as True, otherwise we will mark the outcome as False.
</figureCaption>
<bodyText confidence="0.999714">
edits that compose each correction. We then create
a training instance for each pair of two consecutive
basic edits: if two consecutive basic edits need to
be merged, we will mark the outcome as True, oth-
erwise it is False. We illustrate this in Figure 6.
</bodyText>
<sectionHeader confidence="0.999306" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999676">
We combine Levenshtein algorithm with different
merging algorithms for correction detection.
</bodyText>
<subsectionHeader confidence="0.899543">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999703764705882">
An ideal data resource would be a real-world col-
lection of student essays and their revisions (Tajiri
et al., 2012). However, existing revision corpora
do not have the fine-grained annotations necessary
for our experimental gold standard. We instead
use error annotated data, in which the corrections
were provided by human experts. We simulate the
revisions by applying corrections onto the original
sentence. The teachers’ annotations are treated as
gold standard for the detailed corrections.
We considered four corpora with different ESL
populations and annotation standards, including
FCE corpus (Yannakoudakis et al., 2011), NU-
CLE corpus (Dahlmeier et al., 2013), UIUC cor-
pus2 (Rozovskaya and Roth, 2010) and HOO2011
corpus (Dale and Kilgarriff, 2011). These corpora
all provide experts’ corrections along with error
</bodyText>
<footnote confidence="0.968642">
2UIUC corpus contains annotations of essays collected
from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003).
</footnote>
<figure confidence="0.924884333333333">
(a) The pattern indicates that
the two edits address the
same problem
</figure>
<page confidence="0.926423">
601
</page>
<table confidence="0.939725866666667">
Type name description
gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits’
original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a
A special case that only considers if the basic-edits have zero gap in both sentences.
tense-change We detect patterns such as: if the original-revision pair matches the pattern “V-ing⇒to V”.
word-order-error Whether the basic-edits’ original word set and the revised word set are the same (one or zero).
same-word-set If the original sentence and the revised sentence have the same word set, then it’s likely that all
the edits are fixing the word order error.
revised-to The phrase comprised of the two revised words.
B editdistance=1 If one basic-edit is a substitution, and the original/revised word only has 1 edit distance, it
not-in-dict indicates that the basic-edit is fixing a misspelling error.
If the original word does not have a valid dictionary entry, then it indicates a misspelling error.
word-choice If the original and the revised words have the same POS, then it is likely fixing a word choice
error.
preposition-error Whether the original and the revised words are both prepositions.
</table>
<tableCaption confidence="0.968041">
Table 1: Features used in our proposed classifier.
</tableCaption>
<table confidence="0.973436666666667">
corpus sentences sentences with ≥ 2 corrections
revised sentences
FCE 33,900 53.45%
NUCLE 61,625 48.74%
UIUC 883 61.32%
HOO2011 966 42.05%
</table>
<tableCaption confidence="0.999607">
Table 2: Basic statistics of the corpora that we consider.
</tableCaption>
<bodyText confidence="0.999090166666667">
type mark-ups. The basic statistics of the corpora
are shown in Table 2. In these corpora, around half
of revised sentences contains multiple corrections.
We have split each corpus into 11 equal parts. One
part is used as the development dataset; the rest are
used for 10-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.962513">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9141125">
In addition to evaluating the merging algorithms
on the stand-alone task of correction detection, we
have also plugged in the merging algorithms into
an end-to-end system in which every automati-
cally detected correction is further classified into
an error type. We replicated the error type selector
described in Swanson and Yamangil (2012). The
error type selector’s accuracies are shown in Table
33 . We compare two merging algorithms, com-
bined with Levenshtein algorithm:
S&amp;Y The merging heuristic proposed by Swan-
son and Yamangil, which merges the adjacent ba-
sic edits into single corrections.
MaxEntMerger We use the Maximum Entropy
classifier to predict whether we should merge the
two edits, as described in Section 34.
We evaluate extrinsically the merging compo-
nents’ effect on overall system performance by
3Our replication has a slightly lower error type selection
accuracy on FCE (80.02%) than the figure reported by Swan-
son and Yamangil (82.5%). This small difference on error
type selection does not affect our conclusions about correc-
</bodyText>
<table confidence="0.99891">
Corpus Error Types Accuracy
FCE 73 80.02%
NUCLE 27 67.36%
UIUC 8 80.23%
HOO2011 38 64.88%
</table>
<tableCaption confidence="0.815157">
Table 3: Error type selection accuracies on different cor-
pora. We use a Maximum Entropy classifier along with fea-
tures suggested by Swanson and Yamangil for this task. The
reported figures come from 10-fold cross validations on dif-
ferent corpora.
</tableCaption>
<bodyText confidence="0.9991546">
comparing the boundaries of system’s detected
corrections with the gold standard. We evaluate
both (1) the F-score in detecting corrections (2)
the F-score in correctly detecting both the correc-
tions’ and the error types they address.
</bodyText>
<sectionHeader confidence="0.999705" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99944">
We design experiments to answer two questions:
</bodyText>
<listItem confidence="0.991175333333333">
1. Do the additional contextual information
about correction patterns help guide the merging
decisions? How much does a classifier trained for
this task improve the system’s overall accuracy?
2. How well does our method generalize over re-
visions from different sources?
</listItem>
<bodyText confidence="0.9998645">
Our major experimental results are presented in
Table 4 and Table 6. Table 4 compares the over-
all educational system’s accuracies with different
merging algorithms. Table 6 shows the system’s
F1 score when trained and tested on different cor-
pora. We make the following observations:
First, Table 4 shows that by incorporating cor-
rection patterns into the merging algorithm, the
</bodyText>
<footnote confidence="0.97507125">
tion detection.
4We use the implementation at http://homepages.
inf.ed.ac.uk/lzhang10/maxent_toolkit.
html.
</footnote>
<page confidence="0.996325">
602
</page>
<bodyText confidence="0.99985052">
errors in correction detection step were reduced.
This led to a significant improvement on the over-
all system’s F1-score on all corpora. The improve-
ment is most noticeable on FCE corpus, where
the error in correction detection step was reduced
by 9%. That is, one third of the correction mis-
detections were eliminated. Table 5 shows that the
number of merging errors are significantly reduced
by the new merging algorithm. In particular, the
number of false positives (system proposes merges
when it should not) is significantly reduced.
Second, our proposed model is able to gener-
alize over different corpora. As shown in Table
6. The models built on corpora can generally im-
prove the correction detection accuracy5. Mod-
els built on the same corpus generally perform
the best. Also, as suggested by the experimental
result, among the four corpora, FCE corpus is a
comparably good resource for training correction
detection models with our current feature set. One
reason is that FCE corpus has many more training
instances, which benefits model training. We tried
varying the training dataset size, and test it on dif-
ferent corpora. Figure 7 suggests that the model’s
accuracies increase with the training corpus size.
</bodyText>
<sectionHeader confidence="0.995657" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99998625">
A revision often contains multiple corrections that
address different writing mistakes. We explore
building computer programs to accurately detect
individual corrections in one single revision. One
major challenge lies in determining whether con-
secutive basic-edits address the same mistake. We
propose a classifier specialized in this task. Our
experiments suggest that: (1) the proposed classi-
fier reduces correction mis-detections in previous
systems by 1/3, leading to significant overall sys-
tem performance. (2) our method is generalizable
over different data collections.
</bodyText>
<sectionHeader confidence="0.962685" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.971807333333333">
This work is supported by U.S. National Sci-
ence Foundation Grant IIS-0745914. We thank
the anonymous reviewers for their suggestions;
we also thank Homa Hashemi, Wencan Luo, Fan
Zhang, Lingjia Deng, Wenting Xiong and Yafei
Wei for helpful discussions.
</bodyText>
<footnote confidence="0.978191333333333">
5We currently do not evaluate the end-to-end system over
different corpora. This is because different corpora employ
different error type categorization standards.
</footnote>
<table confidence="0.9987703">
Method Corpus Correction Overall
Detection F1 F1-score
S&amp;Y FCE 70.40% 57.10%
MaxEntMerger FCE 80.96% 66.36%
S&amp;Y NUCLE 61.18% 39.32%
MaxEntMerger NUCLE 63.88% 41.00%
S&amp;Y UIUC 76.57% 65.08%
MaxEntMerger UIUC 82.81% 70.55%
S&amp;Y HOO2011 68.73% 50.95%
MaxEntMerger HOO2011 75.71% 56.14%
</table>
<tableCaption confidence="0.990967666666667">
Table 4: Extrinsic evaluation, where we plugged the two
merging models into an end-to-end feedback detection sys-
tem by Swanson and Yamangil.
</tableCaption>
<table confidence="0.998891666666667">
Merging algorithm TP FP FN TN
S&amp;Y 33.73% 13.46% 5.71% 47.10%
MaxEntMerger 36.04% 3.26% 3.41% 57.30%
</table>
<tableCaption confidence="0.6772374">
Table 5: Intrinsic evaluation, where we evaluate the pro-
posed merging model’s prediction accuracy on FCE corpus.
This table shows a breakdown of true-positives (TP), false-
positives (FP), false-negatives (FN) and true-negatives (TN)
for the system built on FCE corpus.
</tableCaption>
<table confidence="0.999761857142857">
testing FCE NUCLE UIUC HOO2011
training
S&amp;Y 70.44 61.18% 76.57% 68.73%
FCE 80.96% 61.26% 83.07% 75.43%
NUCLE 74.53% 63.88% 78.57% 74.73%
UIUC 77.25% 58.21% 82.81% 70.83%
HOO2011 71.94% 54.99% 71.19% 75.71%
</table>
<tableCaption confidence="0.9718842">
Table 6: Correction detection experiments by building the
model on one corpus, and applying it onto another. We
evaluate the correction detection performance with F1 score.
When training and testing on the same corpus, we run a 10-
fold cross validation.
</tableCaption>
<figure confidence="0.9983795">
0.80
0.75
0.70
0.65
0.60
HOO2011
UIUC
FCE
NUCLE
0.40
101 102 103 104 105
Number of sentences in the training corpus
</figure>
<figureCaption confidence="0.996633333333333">
Figure 7: We illustrate the performance of correction detec-
tion systems trained on subsets of FCE corpus. Each curve in
this figure represents the F1-scores for correction detection
of the model trained on a subset of FCE and tested on differ-
ent corpora. When testing on FCE, we used 111 of the FCE
corpus, which we kept as development data.
</figureCaption>
<figure confidence="0.9904345">
F, score
0.55
0.50
0.45
</figure>
<page confidence="0.995391">
603
</page>
<note confidence="0.551367909090909">
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL errors: Challenges and rewards. In Proceed-
ings of the NAACL HLT 2010 fifth workshop on inno-
vative use of NLP for building educational applica-
tions, pages 28–36. Association for Computational
Linguistics.
References
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4):597–614.
</note>
<reference confidence="0.994408333333333">
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568–572, Montr´eal, Canada, June. Association for
Computational Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS corpus of learner english. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22–31.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, pages 242–249. Association for
Computational Linguistics.
Laureen A Fregeau. 1999. Preparing ESL students
for college writing: Two case studies. The Internet
TESL Journal, 5(10).
Sylviane Granger. 2003. The International Corpus of
Learner English: a new resource for foreign lan-
guage learning and teaching and second language
acquisition research. Tesol Quarterly, 37(3):538–
546.
Shicun Gui and Huizhong Yang. 2003. Zhong-
guo xuexizhe yingyu yuliaohu.(chinese learner en-
glish corpus). Shanghai: Shanghai Waiyu Jiaoyu
Chubanshe.
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011–1019.
Association for Computational Linguistics.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 48–54.
Association for Computational Linguistics.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707710.
D. Nicholls. 2003. The Cambridge Learner Corpus:
Error coding and analysis for lexicography and ELT.
In Proceedings of the Corpus Linguistics 2003 con-
ference, pages 572–581.
Matthew G Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117–127.
Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an ESL educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357–361, Montr´eal, Canada, June.
Association for Computational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction for
ESL learners using global context. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers-Volume 2,
pages 198–202. Association for Computational Lin-
guistics.
Jason Gordon Williams. 2003. Providing feedback
on ESL students written assignments. The Internet
TESL Journal, 4(10).
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 180–189. Association for Computational
Linguistics.
</reference>
<page confidence="0.998713">
604
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951669">
<title confidence="0.997137">Improved Correction Detection in Revised ESL Sentences</title>
<author confidence="0.976468">Xue</author>
<affiliation confidence="0.999632">Department of Computer University of</affiliation>
<address confidence="0.993573">210 S Bouquet St, Pittsburgh, PA 15260,</address>
<abstract confidence="0.998618769230769">This work explores methods of automatically detecting corrections of individual mistakes in sentence revisions for ESL students. We have trained a classifier that specializes in determining whether consecutive basic-edits (word insertions, deletions, substitutions) address the same mistake. Experimental result shows that proposed system achieves an of 81% on correction detection and 66% for the overall system, out-performing the baseline by a large margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>568--572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7748" citStr="Dahlmeier and Ng, 2012" startWordPosition="1204" endWordPosition="1207">ose the actual corrections, we attribute the error to the first step. Otherwise, we attribute the error to the second step. Our analysis confirms that the merging step is the bottleneck in the current correction detection system – it accounts for 75% of the misdetections. Therefore, to effectively reduce the algorithm’s mis-detection errors, we propose to 1Swanson and Yamangil reported an overall system with 58% F-score. (a) (b) 600 build a classifier to merge with better accuracies. Other previous tasks also involve comparing two sentences. Unlike evaluating grammar error correction systems (Dahlmeier and Ng, 2012), correction detection cannot refer to a gold standard. Our error analysis above also highlights our task’s difference with previous work that identify corresponding phrases between two sentences, including phrase extraction (Koehn et al., 2003) and paraphrase extraction (Cohn et al., 2008). They are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work – we need to guarantee each detected phrase pair to address exactly one writing problem. In comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems. A</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568–572, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a large annotated corpus of learner english: The NUS corpus of learner english.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>22--31</pages>
<contexts>
<context position="11698" citStr="Dahlmeier et al., 2013" startWordPosition="1837" endWordPosition="1840">collection of student essays and their revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). (a) The pattern indicates that the two edits address the same problem 601 Type name description gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a A special case that only considers </context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner english: The NUS corpus of learner english. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping our own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>242--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11787" citStr="Dale and Kilgarriff, 2011" startWordPosition="1851" endWordPosition="1854">ing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). (a) The pattern indicates that the two edits address the same problem 601 Type name description gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a A special case that only considers if the basic-edits have zero gap in both sentences. tense-change We detect patterns such </context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 242–249. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laureen A Fregeau</author>
</authors>
<title>Preparing ESL students for college writing: Two case studies.</title>
<date>1999</date>
<journal>The Internet TESL Journal,</journal>
<volume>5</volume>
<issue>10</issue>
<contexts>
<context position="1124" citStr="Fregeau, 1999" startWordPosition="161" endWordPosition="162">Experimental result shows that the proposed system achieves an Fl-score of 81% on correction detection and 66% for the overall system, out-performing the baseline by a large margin. 1 Introduction Quality feedback from language tutors can help English-as-a-Second-Language (ESL) students improve their writing skills. One of the tutors’ tasks is to isolate writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected. Because this is time consuming, tutors often just rewrite the sentences without giving any explanations (Fregeau, 1999). Due to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions (Williams, 2003). Computer aided language learning tools offer a solution for providing more detailed feedback. Programs can be developed to compare the student’s original sentences with the tutor-revised sentences. Swanson and Yamangil (2012) have proposed a promising framework for this purpose. Their approach has two components: one to detect individual corrections within a revision, which they termed correction detection; another to determine what the correction fix</context>
</contexts>
<marker>Fregeau, 1999</marker>
<rawString>Laureen A Fregeau. 1999. Preparing ESL students for college writing: Two case studies. The Internet TESL Journal, 5(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>The International Corpus of Learner English: a new resource for foreign language learning and teaching and second language acquisition research.</title>
<date>2003</date>
<journal>Tesol Quarterly,</journal>
<volume>37</volume>
<issue>3</issue>
<pages>546</pages>
<contexts>
<context position="11932" citStr="Granger, 2003" startWordPosition="1873" endWordPosition="1874"> corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). (a) The pattern indicates that the two edits address the same problem 601 Type name description gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a A special case that only considers if the basic-edits have zero gap in both sentences. tense-change We detect patterns such as: if the original-revision pair matches the pattern “V-ing⇒to V”. word-order-error Whether the basic-edits’ original word set and the revised w</context>
</contexts>
<marker>Granger, 2003</marker>
<rawString>Sylviane Granger. 2003. The International Corpus of Learner English: a new resource for foreign language learning and teaching and second language acquisition research. Tesol Quarterly, 37(3):538– 546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shicun Gui</author>
<author>Huizhong Yang</author>
</authors>
<title>Zhongguo xuexizhe yingyu yuliaohu.(chinese learner english corpus). Shanghai: Shanghai Waiyu Jiaoyu Chubanshe.</title>
<date>2003</date>
<contexts>
<context position="11962" citStr="Gui and Yang, 2003" startWordPosition="1877" endWordPosition="1880">d by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). (a) The pattern indicates that the two edits address the same problem 601 Type name description gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a A special case that only considers if the basic-edits have zero gap in both sentences. tense-change We detect patterns such as: if the original-revision pair matches the pattern “V-ing⇒to V”. word-order-error Whether the basic-edits’ original word set and the revised word set are the same (one or z</context>
</contexts>
<marker>Gui, Yang, 2003</marker>
<rawString>Shicun Gui and Huizhong Yang. 2003. Zhongguo xuexizhe yingyu yuliaohu.(chinese learner english corpus). Shanghai: Shanghai Waiyu Jiaoyu Chubanshe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8580" citStr="Heilman and Smith, 2010" startWordPosition="1334" endWordPosition="1337">hrase extraction (Koehn et al., 2003) and paraphrase extraction (Cohn et al., 2008). They are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work – we need to guarantee each detected phrase pair to address exactly one writing problem. In comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems. A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases. Recent work therefore focuses on identifying the alignment/edits between two sentences (Snover et al., 2009; Heilman and Smith, 2010). 3 A Classifier for Merging Basic-Edits Figures 4 highlights the problems with indiscriminantly merging basic-edits that are adjacent. Intuitively, it seems that the decision should be more context dependent. Certain patterns may indicate that two adjacent basic-edits are a part of the same correction while others may indicate that they each address a different problem. For example, in Figure 5a, when the insertion of one word is followed by the deletion of the same word, the insertion and deletion are likely addressing one single error. This is because these two edits would combine together </context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1011–1019. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7993" citStr="Koehn et al., 2003" startWordPosition="1240" endWordPosition="1243">% of the misdetections. Therefore, to effectively reduce the algorithm’s mis-detection errors, we propose to 1Swanson and Yamangil reported an overall system with 58% F-score. (a) (b) 600 build a classifier to merge with better accuracies. Other previous tasks also involve comparing two sentences. Unlike evaluating grammar error correction systems (Dahlmeier and Ng, 2012), correction detection cannot refer to a gold standard. Our error analysis above also highlights our task’s difference with previous work that identify corresponding phrases between two sentences, including phrase extraction (Koehn et al., 2003) and paraphrase extraction (Cohn et al., 2008). They are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work – we need to guarantee each detected phrase pair to address exactly one writing problem. In comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems. A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases. Recent work therefore focuses on identifying the alignment/edits between two sentences (Snover et al., 2009; Heilman and Smith, 2010). 3 A Classif</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="4821" citStr="Levenshtein, 1966" startWordPosition="734" endWordPosition="735">correction addresses one error. Each polygon corresponds to one correction; the labels are codes of the error types. The codes follow the annotation standard in FCE corpus (Nicholls, 2003). In this example, W is incorrect Word order; UT is Unnecessary preposiTion; FV is wrong Verb Form; RN is Nnoun needs to be Replaced; ID is IDiom error. Figure 2: A portion of the example from Figure 1 undergoing the two-step correction detection process. The basic edits are indicated by black polygons. The corrections are shown in red polygons. Figure 3: Basic edits extracted by the edit-distance algorithm (Levenshtein, 1966) do not necessarily match our linguistic intuition. The ideal basic-edits are shown in Figure 3a, but since the algorithm only cares about minimizing the number of edits, it may end up extracting basic-edits shown in Figure 3b. to changing; the correction “moment ⇒ minute” is itself a single word substitution. Thus, we can build systems to detect corrections which operates in two steps: (1) detecting the basic edits that took place during the revision, and (2) merging those basic edits that address the same error. Figure 2 illustrates the process for a fragment of the example sentence from Fig</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nicholls</author>
</authors>
<title>The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT.</title>
<date>2003</date>
<booktitle>In Proceedings of the Corpus Linguistics 2003 conference,</booktitle>
<pages>572--581</pages>
<contexts>
<context position="4391" citStr="Nicholls, 2003" startWordPosition="663" endWordPosition="664">sed of a deletion of to and a substitution from change 599 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 599–604, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Figure 1: Detecting corrections from revisions. Our system detects individual corrections by comparing the original sentence with its revision, so that each correction addresses one error. Each polygon corresponds to one correction; the labels are codes of the error types. The codes follow the annotation standard in FCE corpus (Nicholls, 2003). In this example, W is incorrect Word order; UT is Unnecessary preposiTion; FV is wrong Verb Form; RN is Nnoun needs to be Replaced; ID is IDiom error. Figure 2: A portion of the example from Figure 1 undergoing the two-step correction detection process. The basic edits are indicated by black polygons. The corrections are shown in red polygons. Figure 3: Basic edits extracted by the edit-distance algorithm (Levenshtein, 1966) do not necessarily match our linguistic intuition. The ideal basic-edits are shown in Figure 3a, but since the algorithm only cares about minimizing the number of edits,</context>
</contexts>
<marker>Nicholls, 2003</marker>
<rawString>D. Nicholls. 2003. The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT. In Proceedings of the Corpus Linguistics 2003 conference, pages 572–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TER-Plus: paraphrase, semantic, and alignment enhancements to translation edit rate.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="8554" citStr="Snover et al., 2009" startWordPosition="1330" endWordPosition="1333">entences, including phrase extraction (Koehn et al., 2003) and paraphrase extraction (Cohn et al., 2008). They are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work – we need to guarantee each detected phrase pair to address exactly one writing problem. In comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems. A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases. Recent work therefore focuses on identifying the alignment/edits between two sentences (Snover et al., 2009; Heilman and Smith, 2010). 3 A Classifier for Merging Basic-Edits Figures 4 highlights the problems with indiscriminantly merging basic-edits that are adjacent. Intuitively, it seems that the decision should be more context dependent. Certain patterns may indicate that two adjacent basic-edits are a part of the same correction while others may indicate that they each address a different problem. For example, in Figure 5a, when the insertion of one word is followed by the deletion of the same word, the insertion and deletion are likely addressing one single error. This is because these two edi</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew G Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. TER-Plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation, 23(2-3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Elif Yamangil</author>
</authors>
<title>Correction detection and error type selection as an ESL educational aid.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>357--361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1494" citStr="Swanson and Yamangil (2012)" startWordPosition="213" endWordPosition="216">writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected. Because this is time consuming, tutors often just rewrite the sentences without giving any explanations (Fregeau, 1999). Due to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions (Williams, 2003). Computer aided language learning tools offer a solution for providing more detailed feedback. Programs can be developed to compare the student’s original sentences with the tutor-revised sentences. Swanson and Yamangil (2012) have proposed a promising framework for this purpose. Their approach has two components: one to detect individual corrections within a revision, which they termed correction detection; another to determine what the correction fixes, which they termed error type selection. Although they reported a high accuracy for the error type selection classifier alone, the bottleneck of their system is the other component – correction detection. An analysis of their system shows that approximately 70% of the system’s mistakes are caused by mis-detections in the first place. Their correction detection algo</context>
<context position="6567" citStr="Swanson and Yamangil (2012)" startWordPosition="1014" endWordPosition="1017"> at most one word lying in between) are merged. Figure 4 shows cases for which the heuristic results in the wrong scope. These errors caused their system to mis-detect 30% of the corrections. Since mis-detected corrections cannot be analyzed down the pipeline, (a) The basic edits are addressing the same problem. But these basic edits are non-adjacent, and therefore not merged by S&amp;Y’s algorithm. (b) The basic edits in the above two cases address different problems though they are adjacent. S&amp;Y’s merging algorithm incorrectly merges them. Figure 4: Merging mistakes by the algorithm proposed in Swanson and Yamangil (2012) (S&amp;Y), which merges adjacent basic edits. the correction detection component became the bottle-neck of their overall system. Out of the 42% corrections that are incorrectly analyzed1, 30%/42%≈70% are caused by mis-detections in the first place. An improvement in correction detection may increase the system accuracy overall. We conducted an error analysis to attribute errors to either step when the system detects a wrong set of corrections for a sentence. We examine the first step’s output. If the resulting basic edits do not match with those that compose the actual corrections, we attribute t</context>
<context position="14175" citStr="Swanson and Yamangil (2012)" startWordPosition="2231" endWordPosition="2234">tics of the corpora are shown in Table 2. In these corpora, around half of revised sentences contains multiple corrections. We have split each corpus into 11 equal parts. One part is used as the development dataset; the rest are used for 10-fold cross validation. 4.2 Evaluation Metrics In addition to evaluating the merging algorithms on the stand-alone task of correction detection, we have also plugged in the merging algorithms into an end-to-end system in which every automatically detected correction is further classified into an error type. We replicated the error type selector described in Swanson and Yamangil (2012). The error type selector’s accuracies are shown in Table 33 . We compare two merging algorithms, combined with Levenshtein algorithm: S&amp;Y The merging heuristic proposed by Swanson and Yamangil, which merges the adjacent basic edits into single corrections. MaxEntMerger We use the Maximum Entropy classifier to predict whether we should merge the two edits, as described in Section 34. We evaluate extrinsically the merging components’ effect on overall system performance by 3Our replication has a slightly lower error type selection accuracy on FCE (80.02%) than the figure reported by Swanson and</context>
</contexts>
<marker>Swanson, Yamangil, 2012</marker>
<rawString>Ben Swanson and Elif Yamangil. 2012. Correction detection and error type selection as an ESL educational aid. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 357–361, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshikazu Tajiri</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Tense and aspect error correction for ESL learners using global context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>198--202</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11145" citStr="Tajiri et al., 2012" startWordPosition="1758" endWordPosition="1761">ts were extracted from the same correction, we will mark the outcome as True, otherwise we will mark the outcome as False. edits that compose each correction. We then create a training instance for each pair of two consecutive basic edits: if two consecutive basic edits need to be merged, we will mark the outcome as True, otherwise it is False. We illustrate this in Figure 6. 4 Experimental Setup We combine Levenshtein algorithm with different merging algorithms for correction detection. 4.1 Dataset An ideal data resource would be a real-world collection of student essays and their revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and </context>
</contexts>
<marker>Tajiri, Komachi, Matsumoto, 2012</marker>
<rawString>Toshikazu Tajiri, Mamoru Komachi, and Yuji Matsumoto. 2012. Tense and aspect error correction for ESL learners using global context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 198–202. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Gordon Williams</author>
</authors>
<title>Providing feedback on ESL students written assignments.</title>
<date>2003</date>
<journal>The Internet TESL Journal,</journal>
<volume>4</volume>
<issue>10</issue>
<contexts>
<context position="1267" citStr="Williams, 2003" startWordPosition="183" endWordPosition="184">forming the baseline by a large margin. 1 Introduction Quality feedback from language tutors can help English-as-a-Second-Language (ESL) students improve their writing skills. One of the tutors’ tasks is to isolate writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected. Because this is time consuming, tutors often just rewrite the sentences without giving any explanations (Fregeau, 1999). Due to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions (Williams, 2003). Computer aided language learning tools offer a solution for providing more detailed feedback. Programs can be developed to compare the student’s original sentences with the tutor-revised sentences. Swanson and Yamangil (2012) have proposed a promising framework for this purpose. Their approach has two components: one to detect individual corrections within a revision, which they termed correction detection; another to determine what the correction fixes, which they termed error type selection. Although they reported a high accuracy for the error type selection classifier alone, the bottlenec</context>
</contexts>
<marker>Williams, 2003</marker>
<rawString>Jason Gordon Williams. 2003. Providing feedback on ESL students written assignments. The Internet TESL Journal, 4(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A new dataset and method for automatically grading esol texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>180--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2216" citStr="Yannakoudakis et al., 2011" startWordPosition="324" endWordPosition="327">o detect individual corrections within a revision, which they termed correction detection; another to determine what the correction fixes, which they termed error type selection. Although they reported a high accuracy for the error type selection classifier alone, the bottleneck of their system is the other component – correction detection. An analysis of their system shows that approximately 70% of the system’s mistakes are caused by mis-detections in the first place. Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al., 2011)). When determining whether a set of basic-edits (word insertions, deletions, substitutions) contributes to the same correction, these heuristics lack the flexibility to adapt to a specific context. Furthermore, it is not clear if the heuristics will work as well for tutors trained to mark up revisions under different guidelines. We propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence. The classifier can make more accurate decisions adjusted to contexts. Because the </context>
<context position="11659" citStr="Yannakoudakis et al., 2011" startWordPosition="1830" endWordPosition="1833"> ideal data resource would be a real-world collection of student essays and their revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). (a) The pattern indicates that the two edits address the same problem 601 Type name description gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach i</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading esol texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 180–189. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>