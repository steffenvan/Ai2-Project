<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.998367">
Pseudo-Projective Dependency Parsing
</title>
<author confidence="0.992415">
Joakim Nivre and Jens Nilsson
</author>
<affiliation confidence="0.990367">
School of Mathematics and Systems Engineering
V¨axj¨o University
</affiliation>
<address confidence="0.926855">
SE-35195 V¨axj¨o, Sweden
</address>
<email confidence="0.995148">
{nivre,jni}@msi.vxu.se
</email>
<sectionHeader confidence="0.994948" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963375">
In order to realize the full potential of
dependency-based syntactic parsing, it is
desirable to allow non-projective depen-
dency structures. We show how a data-
driven deterministic dependency parser,
in itself restricted to projective structures,
can be combined with graph transforma-
tion techniques to produce non-projective
structures. Experiments using data from
the Prague Dependency Treebank show
that the combined system can handle non-
projective constructions with a precision
sufficient to yield a significant improve-
ment in overall parsing accuracy. This
leads to the best reported performance for
robust non-projective parsing of Czech.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976319148937">
It is sometimes claimed that one of the advantages
of dependency grammar over approaches based on
constituency is that it allows a more adequate treat-
ment of languages with variable word order, where
discontinuous syntactic constructions are more com-
mon than in languages like English (Mel’ˇcuk,
1988; Covington, 1990). However, this argument
is only plausible if the formal framework allows
non-projective dependency structures, i.e. structures
where a head and its dependents may correspond
to a discontinuous constituent. From the point of
view of computational implementation this can be
problematic, since the inclusion of non-projective
structures makes the parsing problem more com-
plex and therefore compromises efficiency and in
practice also accuracy and robustness. Thus, most
broad-coverage parsers based on dependency gram-
mar have been restricted to projective structures.
This is true of the widely used link grammar parser
for English (Sleator and Temperley, 1993), which
uses a dependency grammar of sorts, the probabilis-
tic dependency parser of Eisner (1996), and more
recently proposed deterministic dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al., 2004).
It is also true of the adaptation of the Collins parser
for Czech (Collins et al., 1999) and the finite-state
dependency parser for Turkish by Oflazer (2003).
This is in contrast to dependency treebanks, e.g.
Prague Dependency Treebank (Hajiˇc et al., 2001b),
Danish Dependency Treebank (Kromann, 2003),
and the METU Treebank of Turkish (Oflazer et al.,
2003), which generally allow annotations with non-
projective dependency structures. The fact that pro-
jective dependency parsers can never exactly repro-
duce the analyses found in non-projective treebanks
is often neglected because of the relative scarcity of
problematic constructions. While the proportion of
sentences containing non-projective dependencies is
often 15–25%, the total proportion of non-projective
arcs is normally only 1–2%. As long as the main
evaluation metric is dependency accuracy per word,
with state-of-the-art accuracy mostly below 90%,
the penalty for not handling non-projective construc-
tions is almost negligible. Still, from a theoretical
point of view, projective parsing of non-projective
structures has the drawback that it rules out perfect
accuracy even as an asymptotic goal.
</bodyText>
<note confidence="0.733616">
99
Proceedings of the 43rd Annual Meeting of the ACL, pages 99–106,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
AuxZ
(“Only one of them concerns quality.”)
</note>
<figureCaption confidence="0.997634">
Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1
</figureCaption>
<figure confidence="0.991629133333333">
☎
VB
je
is
T
jen
only
P
nich
them
✞ ☎
AuxP
AuxP
Atr
✞ ☎
❄
R
Z
(Out-of
Sb
✞ ☎
AuxZ
✞ ☎
❄
C
jedna
one-FEM-SG
N4
kvalitu
quality
R
na
to
Adv
✞ ☎
❄ ❄
Z:
✞
❄
.
.)
✞
☎
❄
❄
</figure>
<bodyText confidence="0.998028224489796">
There exist a few robust broad-coverage parsers
that produce non-projective dependency structures,
notably Tapanainen and J¨arvinen (1997) and Wang
and Harper (2004) for English, Foth et al. (2004)
for German, and Holan (2004) for Czech. In addi-
tion, there are several approaches to non-projective
dependency parsing that are still to be evaluated in
the large (Covington, 1990; Kahane et al., 1998;
Duchier and Debusmann, 2001; Holan et al., 2001;
Hellwig, 2003). Finally, since non-projective con-
structions often involve long-distance dependencies,
the problem is closely related to the recovery of
empty categories and non-local dependencies in
constituency-based parsing (Johnson, 2002; Dienes
and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill
et al., 2004; Levy and Manning, 2004; Campbell,
2004).
In this paper, we show how non-projective depen-
dency parsing can be achieved by combining a data-
driven projective parser with special graph transfor-
mation techniques. First, the training data for the
parser is projectivized by applying a minimal num-
ber of lifting operations (Kahane et al., 1998) and
encoding information about these lifts in arc labels.
When the parser is trained on the transformed data,
it will ideally learn not only to construct projective
dependency structures but also to assign arc labels
that encode information about lifts. By applying an
inverse transformation to the output of the parser,
arcs with non-standard labels can be lowered to their
proper place in the dependency graph, giving rise
1The dependency graph has been modified to make the final
period a dependent of the main verb instead of being a depen-
dent of a special root node for the sentence.
to non-projective structures. We call this pseudo-
projective dependency parsing, since it is based on a
notion of pseudo-projectivity (Kahane et al., 1998).
The rest of the paper is structured as follows.
In section 2 we introduce the graph transformation
techniques used to projectivize and deprojectivize
dependency graphs, and in section 3 we describe the
data-driven dependency parser that is the core of our
system. We then evaluate the approach in two steps.
First, in section 4, we evaluate the graph transfor-
mation techniques in themselves, with data from the
Prague Dependency Treebank and the Danish De-
pendency Treebank. In section 5, we then evaluate
the entire parsing system by training and evaluating
on data from the Prague Dependency Treebank.
</bodyText>
<sectionHeader confidence="0.980122" genericHeader="method">
2 Dependency Graph Transformations
</sectionHeader>
<bodyText confidence="0.9996">
We assume that the goal in dependency parsing is to
construct a labeled dependency graph of the kind de-
picted in Figure 1. Formally, we define dependency
graphs as follows:
</bodyText>
<listItem confidence="0.9695125">
1. Let R = Irl, ... , rm} be the set of permissible
dependency types (arc labels).
2. A dependency graph for a string of words
W = w1· · ·w,,, is a labeled directed graph
D = (W, A), where
(a) W is the set of nodes, i.e. word tokens in
the input string, ordered by a linear prece-
dence relation &lt;,
(b) A is a set of labeled arcs (wi, r, wj), where
wi,wj EW,rER,
(c) for every wj E W, there is at most one arc
(wi, r, wj) E A.
</listItem>
<figure confidence="0.99926732">
100
AuxZ
✞ AuxP Adv ❄
Sb ☎
❄
☎ ✞ ☎
AuxZ
✞ ☎ ✞ ☎
❄ ❄ ❄
VB T C R N4 Z:
je jen jedna na kvalitu .
is only one-FEM-SG to quality .)
✞ ☎
AuxP
✞ ☎
❄
Atr
R
Z
(Out-of
P
nich
them
✞
❄
</figure>
<figureCaption confidence="0.7761785">
(“Only one of them concerns quality.”)
Figure 2: Projectivized dependency graph for Czech sentence
</figureCaption>
<bodyText confidence="0.965045666666667">
3. A graph D = (W, A) is well-formed iff it is
acyclic and connected.
If (wi, r, wj) E A, we say that wi is the head of wj
and wj a dependent of wi. In the following, we use
the notation wi wj to mean that (wi, r, wj) E A;
r
we also use wi wj to denote an arc with unspeci-
fied label and wi —*∗ wj for the reflexive and transi-
tive closure of the (unlabeled) arc relation.
The dependency graph in Figure 1 satisfies all the
defining conditions above, but it fails to satisfy the
condition ofprojectivity (Kahane et al., 1998):
</bodyText>
<listItem confidence="0.913774">
1. An arc wi —*wk is projective iff, for every word
wj occurring between wi and wk in the string
(wi &lt;wj &lt;wk or wi &gt;wj &gt;wk), wi —*∗ wj.
2. A dependency graph D = (W, A) is projective
iff every arc in A is projective.
</listItem>
<bodyText confidence="0.999961272727273">
The arc connecting the head jedna (one) to the de-
pendent Z (out-of) spans the token je (is), which is
not dominated by jedna.
As observed by Kahane et al. (1998), any (non-
projective) dependency graph can be transformed
into a projective one by a lifting operation, which
replaces each non-projective arc wj wk by a pro-
jective arc wi —* wk such that wi —*∗ wj holds in
the original graph. Here we use a slightly different
notion of lift, applying to individual arcs and moving
their head upwards one step at a time:
</bodyText>
<equation confidence="0.8554868">
�
LIFT(wj —* wk)
wi —* wk if wi —* wj
=
undefined otherwise
</equation>
<bodyText confidence="0.999885823529412">
Intuitively, lifting an arc makes the word wk depen-
dent on the head wi of its original head wj (which is
unique in a well-formed dependency graph), unless
wj is a root in which case the operation is undefined
(but then wj —* wk is necessarily projective if the
dependency graph is well-formed).
Projectivizing a dependency graph by lifting non-
projective arcs is a nondeterministic operation in the
general case. However, since we want to preserve
as much of the original structure as possible, we
are interested in finding a transformation that in-
volves a minimal number of lifts. Even this may
be nondeterministic, in case the graph contains sev-
eral non-projective arcs whose lifts interact, but we
use the following algorithm to construct a minimal
projective transformation D0 = (W, A0) of a (non-
projective) dependency graph D = (W, A):
</bodyText>
<equation confidence="0.888001166666667">
PROJECTIVIZE(W, A)
1 A0 +— A
2 while (W, A0) is non-projective
3 a +— SMALLEST-NONP-ARC(A0)
4 A0 +— (A0 — {a1) U {LIFT(a)1
5 return (W, A0)
</equation>
<bodyText confidence="0.994759384615385">
The function SMALLEST-NONP-ARC returns the
non-projective arc with the shortest distance from
head to dependent (breaking ties from left to right).
Applying the function PROJECTIVIZE to the graph
in Figure 1 yields the graph in Figure 2, where the
problematic arc pointing to Z has been lifted from
the original head jedna to the ancestor je. Using
the terminology of Kahane et al. (1998), we say that
jedna is the syntactic head of Z, while je is its linear
head in the projectivized representation.
Unlike Kahane et al. (1998), we do not regard a
projectivized representation as the final target of the
parsing process. Instead, we want to apply an in-
</bodyText>
<table confidence="0.954682166666667">
101
Lifted arc label Path labels Number of labels
Baseline d p n
Head d↑h p n(n + 1)
Head+Path d↑h p↓ 2n(n + 1)
Path d↑ p↓ 4n
</table>
<tableCaption confidence="0.999943">
Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types)
</tableCaption>
<bodyText confidence="0.9974272">
verse transformation to recover the underlying (non-
projective) dependency graph. In order to facilitate
this task, we extend the set of arc labels to encode
information about lifting operations. In principle, it
would be possible to encode the exact position of the
syntactic head in the label of the arc from the linear
head, but this would give a potentially infinite set of
arc labels and would make the training of the parser
very hard. In practice, we can therefore expect a
trade-off such that increasing the amount of infor-
mation encoded in arc labels will cause an increase
in the accuracy of the inverse transformation but a
decrease in the accuracy with which the parser can
construct the labeled representations. To explore this
tradeoff, we have performed experiments with three
different encoding schemes (plus a baseline), which
are described schematically in Table 1.
The baseline simply retains the original labels for
all arcs, regardless of whether they have been lifted
or not, and the number of distinct labels is therefore
simply the number n of distinct dependency types.2
In the first encoding scheme, called Head, we use
a new label d↑h for each lifted arc, where d is the
dependency relation between the syntactic head and
the dependent in the non-projective representation,
and h is the dependency relation that the syntactic
head has to its own head in the underlying structure.
Using this encoding scheme, the arc from je to Z
in Figure 2 would be assigned the label AuxP↑Sb
(signifying an AuxP that has been lifted from a Sb).
In the second scheme, Head+Path, we in addition
modify the label of every arc along the lifting path
from the syntactic to the linear head so that if the
original label is p the new label is p↓. Thus, the arc
from je to jedna will be labeled 5b↓ (to indicate that
there is a syntactic head below it). In the third and
final scheme, denoted Path, we keep the extra infor-
2Note that this is a baseline for the parsing experiment only
(Experiment 2). For Experiment 1 it is meaningless as a base-
line, since it would result in 0% accuracy.
mation on path labels but drop the information about
the syntactic head of the lifted arc, using the label d↑
instead of d↑h (AuxP↑ instead of AuxP↑Sb).
As can be seen from the last column in Table 1,
both Head and Head+Path may theoretically lead
to a quadratic increase in the number of distinct arc
labels (Head+Path being worse than Head only by
a constant factor), while the increase is only linear in
the case of Path. On the other hand, we can expect
Head+Path to be the most useful representation for
reconstructing the underlying non-projective depen-
dency graph. In approaching this problem, a vari-
ety of different methods are conceivable, including
a more or less sophisticated use of machine learn-
ing. In the present study, we limit ourselves to an
algorithmic approach, using a deterministic breadth-
first search. The details of the transformation proce-
dure are slightly different depending on the encod-
ing schemes:
d↑h
</bodyText>
<listItem confidence="0.97774075">
• Head: For every arc of the form wi −→ wn,
we search the graph top-down, left-to-right,
breadth-first starting at the head node wi. If we
find an arc wl −→ wm, called a target arc, we
</listItem>
<equation confidence="0.827705142857143">
h
d↑h
replace wi −→ wn by wm −→ wn; otherwise
d
d↑h
we replace wi −→ wn by wi −→ wn (i.e. we
d
</equation>
<bodyText confidence="0.943391">
let the linear head be the syntactic head).
</bodyText>
<listItem confidence="0.999025">
• Head+Path: Same as Head, but the search
</listItem>
<equation confidence="0.528796">
p↓
only follows arcs of the form wj −→ wk and a
h↓
</equation>
<bodyText confidence="0.9193565">
target arc must have the form wl −→ wm; if no
target arc is found, Head is used as backoff.
</bodyText>
<listItem confidence="0.999526">
• Path: Same as Head+Path, but a target arc
</listItem>
<equation confidence="0.435809">
p↓
</equation>
<bodyText confidence="0.9993148">
must have the form wl −→ wm and no out-
going arcs of the form wm p&apos;↓ −→ wo; no backoff.
In section 4 we evaluate these transformations with
respect to projectivized dependency treebanks, and
in section 5 they are applied to parser output. Before
</bodyText>
<figure confidence="0.931940285714286">
102
Feature type Top−1 Top Next Next+1 Next+2 Next+3
Word form + + + +
Part-of-speech + + + + + +
Dep type of head +
leftmost dep + +
rightmost dep +
</figure>
<tableCaption confidence="0.97844">
Table 2: Features used in predicting the next parser action
</tableCaption>
<bodyText confidence="0.995268666666667">
we turn to the evaluation, however, we need to intro-
duce the data-driven dependency parser used in the
latter experiments.
</bodyText>
<sectionHeader confidence="0.970424" genericHeader="method">
3 Memory-Based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.998086454545455">
In the experiments below, we employ a data-driven
deterministic dependency parser producing labeled
projective dependency graphs,3 previously tested on
Swedish (Nivre et al., 2004) and English (Nivre and
Scholz, 2004). The parser builds dependency graphs
by traversing the input from left to right, using a
stack to store tokens that are not yet complete with
respect to their dependents. At each point during the
derivation, the parser has a choice between pushing
the next input token onto the stack – with or with-
out adding an arc from the token on top of the stack
to the token pushed – and popping a token from the
stack – with or without adding an arc from the next
input token to the token popped. More details on the
parsing algorithm can be found in Nivre (2003).
The choice between different actions is in general
nondeterministic, and the parser relies on a memory-
based classifier, trained on treebank data, to pre-
dict the next action based on features of the cur-
rent parser configuration. Table 2 shows the features
used in the current version of the parser. At each
point during the derivation, the prediction is based
on six word tokens, the two topmost tokens on the
stack, and the next four input tokens. For each to-
ken, three types of features may be taken into ac-
count: the word form; the part-of-speech assigned
by an automatic tagger; and labels on previously as-
signed dependency arcs involving the token – the arc
from its head and the arcs to its leftmost and right-
most dependent, respectively. Except for the left-
3The graphs satisfy all the well-formedness conditions given
in section 2 except (possibly) connectedness. For robustness
reasons, the parser may output a set of dependency trees instead
of a single tree.
most dependent of the next input token, dependency
type features are limited to tokens on the stack.
The prediction based on these features is a k-
nearest neighbor classification, using the IB1 algo-
rithm and k = 5, the modified value difference met-
ric (MVDM) and class voting with inverse distance
weighting, as implemented in the TiMBL software
package (Daelemans et al., 2003). More details on
the memory-based prediction can be found in Nivre
et al. (2004) and Nivre and Scholz (2004).
</bodyText>
<sectionHeader confidence="0.989967" genericHeader="method">
4 Experiment 1: Treebank Transformation
</sectionHeader>
<bodyText confidence="0.999970608695652">
The first experiment uses data from two dependency
treebanks. The Prague Dependency Treebank (PDT)
consists of more than 1M words of newspaper text,
annotated on three levels, the morphological, ana-
lytical and tectogrammatical levels (Hajiˇc, 1998).
Our experiments all concern the analytical annota-
tion, and the first experiment is based only on the
training part. The Danish Dependency Treebank
(DDT) comprises about 100K words of text selected
from the Danish PAROLE corpus, with annotation
of primary and secondary dependencies (Kromann,
2003). The entire treebank is used in the experiment,
but only primary dependencies are considered.4 In
all experiments, punctuation tokens are included in
the data but omitted in evaluation scores.
In the first part of the experiment, dependency
graphs from the treebanks were projectivized using
the algorithm described in section 2. As shown in
Table 3, the proportion of sentences containing some
non-projective dependency ranges from about 15%
in DDT to almost 25% in PDT. However, the over-
all percentage of non-projective arcs is less than 2%
in PDT and less than 1% in DDT. The last four
</bodyText>
<table confidence="0.82172575">
4If secondary dependencies had been included, the depen-
dency graphs would not have satisfied the well-formedness con-
ditions formulated in section 2.
103
# Lifts in projectivization
Data set # Sentences % NonP # Tokens % NonP 1 2 3 &gt;3
PDT training 73,088 23.15 1,255,333 1.81 93.79 5.60 0.51 0.11
DDT total 5,512 15.48 100,238 0.94 79.49 13.28 4.36 2.87
</table>
<tableCaption confidence="0.983741">
Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective)
</tableCaption>
<table confidence="0.999876">
Data set Head H+P Path
PDT training (28 labels) 92.3 (230) 99.3 (314) 97.3 (84)
DDT total (54 labels) 92.3 (123) 99.8 (147) 98.3 (99)
</table>
<tableCaption confidence="0.999754">
Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)
</tableCaption>
<bodyText confidence="0.999894791666667">
columns in Table 3 show the distribution of non-
projective arcs with respect to the number of lifts
required. It is worth noting that, although non-
projective constructions are less frequent in DDT
than in PDT, they seem to be more deeply nested,
since only about 80% can be projectivized with a
single lift, while almost 95% of the non-projective
arcs in PDT only require a single lift.
In the second part of the experiment, we applied
the inverse transformation based on breadth-first
search under the three different encoding schemes.
The results are given in Table 4. As expected, the
most informative encoding, Head+Path, gives the
highest accuracy with over 99% of all non-projective
arcs being recovered correctly in both data sets.
However, it can be noted that the results for the least
informative encoding, Path, are almost comparable,
while the third encoding, Head, gives substantially
worse results for both data sets. We also see that
the increase in the size of the label sets for Head
and Head+Path is far below the theoretical upper
bounds given in Table 1. The increase is gener-
ally higher for PDT than for DDT, which indicates a
greater diversity in non-projective constructions.
</bodyText>
<sectionHeader confidence="0.932825" genericHeader="evaluation">
5 Experiment 2: Memory-Based Parsing
</sectionHeader>
<bodyText confidence="0.999949">
The second experiment is limited to data from PDT.5
The training part of the treebank was projectivized
under different encoding schemes and used to train
memory-based dependency parsers, which were run
on the test part of the treebank, consisting of 7,507
</bodyText>
<footnote confidence="0.58597">
5Preliminary experiments using data from DDT indicated
that the limited size of the treebank creates a severe sparse data
problem with respect to non-projective constructions.
</footnote>
<bodyText confidence="0.9997181875">
sentences and 125,713 tokens.6 The inverse trans-
formation was applied to the output of the parsers
and the result compared to the gold standard test set.
Table 5 shows the overall parsing accuracy at-
tained with the three different encoding schemes,
compared to the baseline (no special arc labels) and
to training directly on non-projective dependency
graphs. Evaluation metrics used are Attachment
Score (AS), i.e. the proportion of tokens that are at-
tached to the correct head, and Exact Match (EM),
i.e. the proportion of sentences for which the depen-
dency graph exactly matches the gold standard. In
the labeled version of these metrics (L) both heads
and arc labels must be correct, while the unlabeled
version (U) only considers heads.
The first thing to note is that projectivizing helps
in itself, even if no encoding is used, as seen from
the fact that the projective baseline outperforms the
non-projective training condition by more than half
a percentage point on attachment score, although the
gain is much smaller with respect to exact match.
The second main result is that the pseudo-projective
approach to parsing (using special arc labels to guide
an inverse transformation) gives a further improve-
ment of about one percentage point on attachment
score. With respect to exact match, the improvement
is even more noticeable, which shows quite clearly
that even if non-projective dependencies are rare on
the token level, they are nevertheless important for
getting the global syntactic structure correct.
All improvements over the baseline are statisti-
cally significant beyond the 0.01 level (McNemar’s
</bodyText>
<footnote confidence="0.64732">
6The part-of-speech tagging used in both training and testing
</footnote>
<table confidence="0.880259111111111">
was the uncorrected output of an HMM tagger distributed with
the treebank; cf. Hajiˇc et al. (2001a).
104
Encoding UAS LAS UEM LEM
Non-projective 78.5 71.3 28.9 20.6
Baseline 79.1 72.0 29.2 20.7
Head 80.1 72.8 31.6 22.2
Head+Path 80.0 72.8 31.8 22.4
Path 80.0 72.7 31.6 22.0
</table>
<tableCaption confidence="0.991641">
Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled)
</tableCaption>
<table confidence="0.9996482">
Unlabeled Labeled
Encoding P R F P R F
Head 61.3 54.1 57.5 55.2 49.8 52.4
Head+Path 63.9 54.9 59.0 57.9 50.6 54.0
Path 58.2 49.5 53.4 51.0 45.7 48.2
</table>
<tableCaption confidence="0.997027">
Table 6: Precision, recall and F-measure for non-projective arcs
</tableCaption>
<bodyText confidence="0.99992141509434">
test). By contrast, when we turn to a comparison
of the three encoding schemes it is hard to find any
significant differences, and the overall impression is
that it makes little or no difference which encoding
scheme is used, as long as there is some indication
of which words are assigned their linear head instead
of their syntactic head by the projective parser. This
may seem surprising, given the experiments reported
in section 4, but the explanation is probably that the
non-projective dependencies that can be recovered at
all are of the simple kind that only requires a single
lift, where the encoding of path information is often
redundant. It is likely that the more complex cases,
where path information could make a difference, are
beyond the reach of the parser in most cases.
However, if we consider precision, recall and F-
measure on non-projective dependencies only, as
shown in Table 6, some differences begin to emerge.
The most informative scheme, Head+Path, gives
the highest scores, although with respect to Head
the difference is not statistically significant, while
the least informative scheme, Path – with almost the
same performance on treebank transformation – is
significantly lower (p &lt; 0.01). On the other hand,
given that all schemes have similar parsing accuracy
overall, this means that the Path scheme is the least
likely to introduce errors on projective arcs.
The overall parsing accuracy obtained with the
pseudo-projective approach is still lower than for the
best projective parsers. Although the best published
results for the Collins parser is 80% UAS (Collins,
1999), this parser reaches 82% when trained on the
entire training data set, and an adapted version of
Charniak’s parser (Charniak, 2000) performs at 84%
(Jan Hajiˇc, pers. comm.). However, the accuracy is
considerably higher than previously reported results
for robust non-projective parsing of Czech, with a
best performance of 73% UAS (Holan, 2004).
Compared to related work on the recovery of
long-distance dependencies in constituency-based
parsing, our approach is similar to that of Dienes
and Dubey (2003) in that the processing of non-local
dependencies is partly integrated in the parsing pro-
cess, via an extension of the set of syntactic cate-
gories, whereas most other approaches rely on post-
processing only. However, while Dienes and Dubey
recognize empty categories in a pre-processing step
and only let the parser find their antecedents, we use
the parser both to detect dislocated dependents and
to predict either the type or the location of their syn-
tactic head (or both) and use post-processing only to
transform the graph in accordance with the parser’s
analysis.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999932">
We have presented a new method for non-projective
dependency parsing, based on a combination of
data-driven projective dependency parsing and
graph transformation techniques. The main result is
that the combined system can recover non-projective
dependencies with a precision sufficient to give a
significant improvement in overall parsing accuracy,
</bodyText>
<page confidence="0.871875">
105
</page>
<bodyText confidence="0.999791666666667">
especially with respect to the exact match criterion,
leading to the best reported performance for robust
non-projective parsing of Czech.
</bodyText>
<sectionHeader confidence="0.995575" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999888111111111">
This work was supported in part by the Swedish
Research Council (621-2002-4207). Memory-based
classifiers for the experiments were created using
TiMBL (Daelemans et al., 2003). Special thanks to
Jan Hajiˇc and Matthias Trautner Kromann for assis-
tance with the Czech and Danish data, respectively,
and to Jan Hajiˇc, Tom´aˇs Holan, Dan Zeman and
three anonymous reviewers for valuable comments
on a preliminary version of the paper.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999633698795181">
Cahill, A., Burke, M., O’Donovan, R., Van Genabith, J. and
Way, A. 2004. Long-distance dependency resolution in
automatically acquired wide-coverage PCFG-based LFG ap-
proximations. In Proceedings ofACL.
Campbell, R. 2004. Using linguistic principles to recover
empty categories. In Proceedings ofACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proceedings ofNAACL.
Collins, M., Hajiˇc, J., Brill, E., Ramshaw, L. and Tillmann, C.
1999. A statistical parser for Czech. In Proceedings ofACL.
Collins, M. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
Covington, M. A. 1990. Parsing discontinuous constituents in
dependency grammar. Computational Linguistics, 16:234–
236.
Daelemans, W., Zavrel, J., van der Sloot, K. and van den Bosch,
A. 2003. TiMBL: Tilburg Memory Based Learner, version
5.0, Reference Guide. Technical Report ILK 03-10, Tilburg
University, ILK.
Dienes, P. and Dubey, A. 2003. Deep syntactic processing by
combining shallow methods. In Proceedings ofACL.
Duchier, D. and Debusmann, R. 2001. Topological dependency
trees: A constraint-based account of linear precedence. In
Proceedings ofACL.
Eisner, J. M. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proceedings of COLING.
Foth, K., Daum, M. and Menzel, W. 2004. A broad-coverage
parser for German based on defeasible constraints. In Pro-
ceedings ofKONVENS.
Hajiˇc, J., Krbec, P., Oliva, K., Kveton, P. and Petkevic, V. 2001.
Serial combination of rules and statistics: A case study in
Czech tagging. In Proceedings ofACL.
Hajiˇc, J., Vidova Hladka, B., Panevov´a, J., Hajiˇcov´a, E., Sgall,
P. and Pajas, P. 2001. Prague Dependency Treebank 1.0.
LDC, 2001T10.
Hajiˇc, J. 1998. Building a syntactically annotated corpus:
The Prague Dependency Treebank. In Issues of Valency and
Meaning, pages 106–132. Karolinum.
Hellwig, P. 2003. Dependency unification grammar. In Depen-
dency and Valency, pages 593–635. Walter de Gruyter.
Holan, T., Kuboˇn, V. and Pl´atek, M. 2001. Word-order re-
laxations and restrictions within a dependency grammar. In
Proceedings ofIWPT.
Holan, T. 2004. Tvorba zavislostniho syntaktickeho analyza-
toru. In Proceedings ofMIS’2004.
Jijkoun, V. and de Rijke, M. 2004. Enriching the output of
a parser using memory-based learning. In Proceedings of
ACL.
Johnson, M. 2002. A simple pattern-matching algorithm for re-
covering empty nodes and their antecedents. In Proceedings
ofACL.
Kahane, S., Nasr, A. and Rambow, O. 1998. Pseudo-
projectivity: A polynomially parsable non-projective depen-
dency grammar. In Proceedings ofACL-COLING.
Kromann, M. T. 2003. The Danish Dependency Treebank and
the DTAG treebank tool. In Proceedings of TLT 2003.
Levy, R. and Manning, C. 2004. Deep dependencies from
context-free statistical parsers: Correcting the surface depen-
dency approximation. In Proceedings ofACL.
Mel’ˇcuk, I. 1988. Dependency Syntax: Theory and Practice.
State University of New York Press.
Nivre, J. and Scholz, M. 2004. Deterministic dependency pars-
ing of English text. In Proceedings of COLING.
Nivre, J., Hall, J. and Nilsson, J. 2004. Memory-based depen-
dency parsing. In Proceedings of CoNLL.
Nivre, J. 2003. An efficient algorithm for projective depen-
dency parsing. In Proceedings ofIWPT.
Oflazer, K., Say, B., Hakkani-T¨ur, D. Z. and T¨ur, G. 2003.
Building a Turkish treebank. In Treebanks: Building and
Using Parsed Corpora, pages 261–277. Kluwer Academic
Publishers.
Oflazer, K. 2003. Dependency parsing with an extended finite-
state approach. Computational Linguistics, 29:515–544.
Sleator, D. and Temperley, D. 1993. Parsing English with a
link grammar. In Proceedings ofIWPT.
Tapanainen, P. and J¨arvinen, T. 1997. A non-projective depen-
dency parser. In Proceedings ofANLP.
Wang, W. and Harper, M. P. 2004. A statistical constraint
dependency grammar (CDG) parser. In Proceedings of the
Workshop in Incremental Parsing (ACL).
Yamada, H. and Matsumoto, Y. 2003. Statistical dependency
analysis with support vector machines. In Proceedings of
IWPT.
</reference>
<page confidence="0.888444">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951831">
<title confidence="0.999861">Pseudo-Projective Dependency Parsing</title>
<author confidence="0.987376">Nivre Nilsson</author>
<affiliation confidence="0.9996945">School of Mathematics and Systems Engineering V¨axj¨o University</affiliation>
<address confidence="0.994084">SE-35195 V¨axj¨o, Sweden</address>
<abstract confidence="0.997757294117647">In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R O’Donovan</author>
<author>J Van Genabith</author>
<author>A Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Cahill, Burke, O’Donovan, Van Genabith, Way, 2004</marker>
<rawString>Cahill, A., Burke, M., O’Donovan, R., Van Genabith, J. and Way, A. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Campbell</author>
</authors>
<title>Using linguistic principles to recover empty categories.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4434" citStr="Campbell, 2004" startWordPosition="667" endWordPosition="668">l. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. By applying an inverse transformation to the ou</context>
</contexts>
<marker>Campbell, 2004</marker>
<rawString>Campbell, R. 2004. Using linguistic principles to recover empty categories. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="24104" citStr="Charniak, 2000" startWordPosition="4073" endWordPosition="4074"> with almost the same performance on treebank transformation – is significantly lower (p &lt; 0.01). On the other hand, given that all schemes have similar parsing accuracy overall, this means that the Path scheme is the least likely to introduce errors on projective arcs. The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers. Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniak’s parser (Charniak, 2000) performs at 84% (Jan Hajiˇc, pers. comm.). However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004). Compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process, via an extension of the set of syntactic categories, whereas most other approaches rely on postprocessing only. However, while Di</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Hajiˇc</author>
<author>E Brill</author>
<author>L Ramshaw</author>
<author>C Tillmann</author>
</authors>
<title>A statistical parser for Czech.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Collins, Hajiˇc, Brill, Ramshaw, Tillmann, 1999</marker>
<rawString>Collins, M., Hajiˇc, J., Brill, E., Ramshaw, L. and Tillmann, C. 1999. A statistical parser for Czech. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="23972" citStr="Collins, 1999" startWordPosition="4052" endWordPosition="4053">t scores, although with respect to Head the difference is not statistically significant, while the least informative scheme, Path – with almost the same performance on treebank transformation – is significantly lower (p &lt; 0.01). On the other hand, given that all schemes have similar parsing accuracy overall, this means that the Path scheme is the least likely to introduce errors on projective arcs. The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers. Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniak’s parser (Charniak, 2000) performs at 84% (Jan Hajiˇc, pers. comm.). However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004). Compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing proces</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Covington</author>
</authors>
<title>Parsing discontinuous constituents in dependency grammar.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<pages>236</pages>
<contexts>
<context position="1167" citStr="Covington, 1990" startWordPosition="162" endWordPosition="163">om the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech. 1 Introduction It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order, where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk, 1988; Covington, 1990). However, this argument is only plausible if the formal framework allows non-projective dependency structures, i.e. structures where a head and its dependents may correspond to a discontinuous constituent. From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used l</context>
<context position="4008" citStr="Covington, 1990" startWordPosition="606" endWordPosition="607">ependency graph for Czech sentence from the Prague Dependency Treebank1 ☎ VB je is T jen only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First</context>
</contexts>
<marker>Covington, 1990</marker>
<rawString>Covington, M. A. 1990. Parsing discontinuous constituents in dependency grammar. Computational Linguistics, 16:234– 236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 5.0, Reference Guide.</title>
<date>2003</date>
<tech>Technical Report ILK 03-10,</tech>
<institution>Tilburg University, ILK.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>Daelemans, W., Zavrel, J., van der Sloot, K. and van den Bosch, A. 2003. TiMBL: Tilburg Memory Based Learner, version 5.0, Reference Guide. Technical Report ILK 03-10, Tilburg University, ILK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dienes</author>
<author>A Dubey</author>
</authors>
<title>Deep syntactic processing by combining shallow methods.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4344" citStr="Dienes and Dubey, 2003" startWordPosition="650" endWordPosition="653">ctures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc l</context>
<context position="24480" citStr="Dienes and Dubey (2003)" startWordPosition="4127" endWordPosition="4130"> the best projective parsers. Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniak’s parser (Charniak, 2000) performs at 84% (Jan Hajiˇc, pers. comm.). However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004). Compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process, via an extension of the set of syntactic categories, whereas most other approaches rely on postprocessing only. However, while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents, we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis. 6 Conclusion We have presented </context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Dienes, P. and Dubey, A. 2003. Deep syntactic processing by combining shallow methods. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>R Debusmann</author>
</authors>
<title>Topological dependency trees: A constraint-based account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4058" citStr="Duchier and Debusmann, 2001" startWordPosition="612" endWordPosition="615">m the Prague Dependency Treebank1 ☎ VB je is T jen only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivize</context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>Duchier, D. and Debusmann, R. 2001. Topological dependency trees: A constraint-based account of linear precedence. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1923" citStr="Eisner (1996)" startWordPosition="272" endWordPosition="273">d its dependents may correspond to a discontinuous constituent. From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures. The fact that projective dependency parsers can nev</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Eisner, J. M. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
<author>M Daum</author>
<author>W Menzel</author>
</authors>
<title>A broad-coverage parser for German based on defeasible constraints.</title>
<date>2004</date>
<booktitle>In Proceedings ofKONVENS.</booktitle>
<contexts>
<context position="3828" citStr="Foth et al. (2004)" startWordPosition="575" endWordPosition="578">ngs of the 43rd Annual Meeting of the ACL, pages 99–106, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics AuxZ (“Only one of them concerns quality.”) Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1 ☎ VB je is T jen only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell,</context>
</contexts>
<marker>Foth, Daum, Menzel, 2004</marker>
<rawString>Foth, K., Daum, M. and Menzel, W. 2004. A broad-coverage parser for German based on defeasible constraints. In Proceedings ofKONVENS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>P Krbec</author>
<author>K Oliva</author>
<author>P Kveton</author>
<author>V Petkevic</author>
</authors>
<title>Serial combination of rules and statistics: A case study in Czech tagging.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Hajiˇc, Krbec, Oliva, Kveton, Petkevic, 2001</marker>
<rawString>Hajiˇc, J., Krbec, P., Oliva, K., Kveton, P. and Petkevic, V. 2001. Serial combination of rules and statistics: A case study in Czech tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>Vidova Hladka</author>
<author>B Panevov´a</author>
<author>J Hajiˇcov´a</author>
<author>E Sgall</author>
<author>P</author>
<author>P Pajas</author>
</authors>
<title>Prague Dependency Treebank 1.0. LDC,</title>
<date>2001</date>
<marker>Hajiˇc, Hladka, Panevov´a, Hajiˇcov´a, Sgall, P, Pajas, 2001</marker>
<rawString>Hajiˇc, J., Vidova Hladka, B., Panevov´a, J., Hajiˇcov´a, E., Sgall, P. and Pajas, P. 2001. Prague Dependency Treebank 1.0. LDC, 2001T10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>In Issues of Valency and Meaning,</booktitle>
<pages>106--132</pages>
<publisher>Karolinum.</publisher>
<marker>Hajiˇc, 1998</marker>
<rawString>Hajiˇc, J. 1998. Building a syntactically annotated corpus: The Prague Dependency Treebank. In Issues of Valency and Meaning, pages 106–132. Karolinum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hellwig</author>
</authors>
<title>Dependency unification grammar.</title>
<date>2003</date>
<booktitle>In Dependency and Valency,</booktitle>
<pages>593--635</pages>
<note>Walter de Gruyter.</note>
<contexts>
<context position="4094" citStr="Hellwig, 2003" startWordPosition="620" endWordPosition="621">n only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of li</context>
</contexts>
<marker>Hellwig, 2003</marker>
<rawString>Hellwig, P. 2003. Dependency unification grammar. In Dependency and Valency, pages 593–635. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Holan</author>
<author>V Kuboˇn</author>
<author>M Pl´atek</author>
</authors>
<title>Word-order relaxations and restrictions within a dependency grammar.</title>
<date>2001</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<marker>Holan, Kuboˇn, Pl´atek, 2001</marker>
<rawString>Holan, T., Kuboˇn, V. and Pl´atek, M. 2001. Word-order relaxations and restrictions within a dependency grammar. In Proceedings ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Holan</author>
</authors>
<title>Tvorba zavislostniho syntaktickeho analyzatoru.</title>
<date>2004</date>
<booktitle>In Proceedings ofMIS’2004.</booktitle>
<contexts>
<context position="3857" citStr="Holan (2004)" startWordPosition="582" endWordPosition="583">he ACL, pages 99–106, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics AuxZ (“Only one of them concerns quality.”) Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1 ☎ VB je is T jen only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we sho</context>
<context position="24318" citStr="Holan, 2004" startWordPosition="4105" endWordPosition="4106"> least likely to introduce errors on projective arcs. The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers. Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniak’s parser (Charniak, 2000) performs at 84% (Jan Hajiˇc, pers. comm.). However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004). Compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process, via an extension of the set of syntactic categories, whereas most other approaches rely on postprocessing only. However, while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents, we use the parser both to detect dislocated dependents and to predict either the type or the locati</context>
</contexts>
<marker>Holan, 2004</marker>
<rawString>Holan, T. 2004. Tvorba zavislostniho syntaktickeho analyzatoru. In Proceedings ofMIS’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Enriching the output of a parser using memory-based learning.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Jijkoun, de Rijke, 2004</marker>
<rawString>Jijkoun, V. and de Rijke, M. 2004. Enriching the output of a parser using memory-based learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4320" citStr="Johnson, 2002" startWordPosition="648" endWordPosition="649">dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures </context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Johnson, M. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudoprojectivity: A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proceedings ofACL-COLING.</booktitle>
<contexts>
<context position="4029" citStr="Kahane et al., 1998" startWordPosition="608" endWordPosition="611">or Czech sentence from the Prague Dependency Treebank1 ☎ VB je is T jen only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data f</context>
<context position="5476" citStr="Kahane et al., 1998" startWordPosition="835" endWordPosition="838">ly learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures. We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998). The rest of the paper is structured as follows. In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs, and in section 3 we describe the data-driven dependency parser that is the core of our system. We then evaluate the approach in two steps. First, in section 4, we evaluate the graph transformation techniques in themselves, with data from the Prague Dependency Treebank and the Danish Dependency Treebank. In section 5, we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treeb</context>
<context position="7511" citStr="Kahane et al., 1998" startWordPosition="1234" endWordPosition="1237">Only one of them concerns quality.”) Figure 2: Projectivized dependency graph for Czech sentence 3. A graph D = (W, A) is well-formed iff it is acyclic and connected. If (wi, r, wj) E A, we say that wi is the head of wj and wj a dependent of wi. In the following, we use the notation wi wj to mean that (wi, r, wj) E A; r we also use wi wj to denote an arc with unspecified label and wi —*∗ wj for the reflexive and transitive closure of the (unlabeled) arc relation. The dependency graph in Figure 1 satisfies all the defining conditions above, but it fails to satisfy the condition ofprojectivity (Kahane et al., 1998): 1. An arc wi —*wk is projective iff, for every word wj occurring between wi and wk in the string (wi &lt;wj &lt;wk or wi &gt;wj &gt;wk), wi —*∗ wj. 2. A dependency graph D = (W, A) is projective iff every arc in A is projective. The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is), which is not dominated by jedna. As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original </context>
<context position="9673" citStr="Kahane et al. (1998)" startWordPosition="1624" endWordPosition="1627">al projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): PROJECTIVIZE(W, A) 1 A0 +— A 2 while (W, A0) is non-projective 3 a +— SMALLEST-NONP-ARC(A0) 4 A0 +— (A0 — {a1) U {LIFT(a)1 5 return (W, A0) The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right). Applying the function PROJECTIVIZE to the graph in Figure 1 yields the graph in Figure 2, where the problematic arc pointing to Z has been lifted from the original head jedna to the ancestor je. Using the terminology of Kahane et al. (1998), we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation. Unlike Kahane et al. (1998), we do not regard a projectivized representation as the final target of the parsing process. Instead, we want to apply an in101 Lifted arc label Path labels Number of labels Baseline d p n Head d↑h p n(n + 1) Head+Path d↑h p↓ 2n(n + 1) Path d↑ p↓ 4n Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types) verse transformation to recover the underlying (nonprojective) dependency graph. In order to facilitate t</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Kahane, S., Nasr, A. and Rambow, O. 1998. Pseudoprojectivity: A polynomially parsable non-projective dependency grammar. In Proceedings ofACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Kromann</author>
</authors>
<title>The Danish Dependency Treebank and the DTAG treebank tool.</title>
<date>2003</date>
<booktitle>In Proceedings of TLT</booktitle>
<contexts>
<context position="2337" citStr="Kromann, 2003" startWordPosition="334" endWordPosition="335">tructures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions. While the proportion of sentences containing non-projective dependencies is often 15–25%, the total proportion of non-projective arcs is normally only 1–2%. As long as the main evaluation metric is dependency accuracy per word, with state-of-the-art accuracy mostl</context>
<context position="17174" citStr="Kromann, 2003" startWordPosition="2942" endWordPosition="2943">d Nivre and Scholz (2004). 4 Experiment 1: Treebank Transformation The first experiment uses data from two dependency treebanks. The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text, annotated on three levels, the morphological, analytical and tectogrammatical levels (Hajiˇc, 1998). Our experiments all concern the analytical annotation, and the first experiment is based only on the training part. The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus, with annotation of primary and secondary dependencies (Kromann, 2003). The entire treebank is used in the experiment, but only primary dependencies are considered.4 In all experiments, punctuation tokens are included in the data but omitted in evaluation scores. In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2. As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT. However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT. The last four 4If second</context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>Kromann, M. T. 2003. The Danish Dependency Treebank and the DTAG treebank tool. In Proceedings of TLT 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>C Manning</author>
</authors>
<title>Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4417" citStr="Levy and Manning, 2004" startWordPosition="663" endWordPosition="666">) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. By applying an inverse transfo</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Levy, R. and Manning, C. 2004. Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Mel’ˇcuk, I. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="14575" citStr="Nivre and Scholz, 2004" startWordPosition="2503" endWordPosition="2506">y are applied to parser output. Before 102 Feature type Top−1 Top Next Next+1 Next+2 Next+3 Word form + + + + Part-of-speech + + + + + + Dep type of head + leftmost dep + + rightmost dep + Table 2: Features used in predicting the next parser action we turn to the evaluation, however, we need to introduce the data-driven dependency parser used in the latter experiments. 3 Memory-Based Dependency Parsing In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004). The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents. At each point during the derivation, the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped. More details on the parsing algorithm can be found in Nivre (2003). The choice between different actions is in g</context>
<context position="16585" citStr="Nivre and Scholz (2004)" startWordPosition="2854" endWordPosition="2857">ion 2 except (possibly) connectedness. For robustness reasons, the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token, dependency type features are limited to tokens on the stack. The prediction based on these features is a knearest neighbor classification, using the IB1 algorithm and k = 5, the modified value difference metric (MVDM) and class voting with inverse distance weighting, as implemented in the TiMBL software package (Daelemans et al., 2003). More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004). 4 Experiment 1: Treebank Transformation The first experiment uses data from two dependency treebanks. The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text, annotated on three levels, the morphological, analytical and tectogrammatical levels (Hajiˇc, 1998). Our experiments all concern the analytical annotation, and the first experiment is based only on the training part. The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus, with annotation of primary and secondary dependencies (Kromann, 2003). The entir</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Nivre, J. and Scholz, M. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2033" citStr="Nivre et al., 2004" startWordPosition="285" endWordPosition="288">implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative</context>
<context position="14538" citStr="Nivre et al., 2004" startWordPosition="2497" endWordPosition="2500">y treebanks, and in section 5 they are applied to parser output. Before 102 Feature type Top−1 Top Next Next+1 Next+2 Next+3 Word form + + + + Part-of-speech + + + + + + Dep type of head + leftmost dep + + rightmost dep + Table 2: Features used in predicting the next parser action we turn to the evaluation, however, we need to introduce the data-driven dependency parser used in the latter experiments. 3 Memory-Based Dependency Parsing In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004). The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents. At each point during the derivation, the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped. More details on the parsing algorithm can be found in Nivre (2003). The cho</context>
<context position="16557" citStr="Nivre et al. (2004)" startWordPosition="2849" endWordPosition="2852">conditions given in section 2 except (possibly) connectedness. For robustness reasons, the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token, dependency type features are limited to tokens on the stack. The prediction based on these features is a knearest neighbor classification, using the IB1 algorithm and k = 5, the modified value difference metric (MVDM) and class voting with inverse distance weighting, as implemented in the TiMBL software package (Daelemans et al., 2003). More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004). 4 Experiment 1: Treebank Transformation The first experiment uses data from two dependency treebanks. The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text, annotated on three levels, the morphological, analytical and tectogrammatical levels (Hajiˇc, 1998). Our experiments all concern the analytical annotation, and the first experiment is based only on the training part. The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus, with annotation of primary and secondary dependencie</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Nivre, J., Hall, J. and Nilsson, J. 2004. Memory-based dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<contexts>
<context position="15129" citStr="Nivre (2003)" startWordPosition="2610" endWordPosition="2611"> (Nivre et al., 2004) and English (Nivre and Scholz, 2004). The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents. At each point during the derivation, the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped. More details on the parsing algorithm can be found in Nivre (2003). The choice between different actions is in general nondeterministic, and the parser relies on a memorybased classifier, trained on treebank data, to predict the next action based on features of the current parser configuration. Table 2 shows the features used in the current version of the parser. At each point during the derivation, the prediction is based on six word tokens, the two topmost tokens on the stack, and the next four input tokens. For each token, three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on pr</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Nivre, J. 2003. An efficient algorithm for projective dependency parsing. In Proceedings ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>B Say</author>
<author>D Z Hakkani-T¨ur</author>
<author>G T¨ur</author>
</authors>
<title>Building a Turkish treebank. In Treebanks: Building and Using Parsed Corpora,</title>
<date>2003</date>
<pages>261--277</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Oflazer, Say, Hakkani-T¨ur, T¨ur, 2003</marker>
<rawString>Oflazer, K., Say, B., Hakkani-T¨ur, D. Z. and T¨ur, G. 2003. Building a Turkish treebank. In Treebanks: Building and Using Parsed Corpora, pages 261–277. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
</authors>
<title>Dependency parsing with an extended finitestate approach.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--515</pages>
<contexts>
<context position="2192" citStr="Oflazer (2003)" startWordPosition="314" endWordPosition="315"> and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions. While the proportion of sentences containing non-projective dependencies is often 15–25%, the total proportion of non-p</context>
</contexts>
<marker>Oflazer, 2003</marker>
<rawString>Oflazer, K. 2003. Dependency parsing with an extended finitestate approach. Computational Linguistics, 29:515–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1993</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<contexts>
<context position="1827" citStr="Sleator and Temperley, 1993" startWordPosition="255" endWordPosition="258"> plausible if the formal framework allows non-projective dependency structures, i.e. structures where a head and its dependents may correspond to a discontinuous constituent. From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotatio</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Sleator, D. and Temperley, D. 1993. Parsing English with a link grammar. In Proceedings ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>T J¨arvinen</author>
</authors>
<title>A non-projective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP.</booktitle>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Tapanainen, P. and J¨arvinen, T. 1997. A non-projective dependency parser. In Proceedings ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>M P Harper</author>
</authors>
<title>A statistical constraint dependency grammar (CDG) parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop in Incremental Parsing (ACL).</booktitle>
<contexts>
<context position="3796" citStr="Wang and Harper (2004)" startWordPosition="569" endWordPosition="572">n as an asymptotic goal. 99 Proceedings of the 43rd Annual Meeting of the ACL, pages 99–106, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics AuxZ (“Only one of them concerns quality.”) Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1 ☎ VB je is T jen only P nich them ✞ ☎ AuxP AuxP Atr ✞ ☎ ❄ R Z (Out-of Sb ✞ ☎ AuxZ ✞ ☎ ❄ C jedna one-FEM-SG N4 kvalitu quality R na to Adv ✞ ☎ ❄ ❄ Z: ✞ ❄ . .) ✞ ☎ ❄ ❄ There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; L</context>
</contexts>
<marker>Wang, Harper, 2004</marker>
<rawString>Wang, W. and Harper, M. P. 2004. A statistical constraint dependency grammar (CDG) parser. In Proceedings of the Workshop in Incremental Parsing (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="2012" citStr="Yamada and Matsumoto, 2003" startWordPosition="281" endWordPosition="284">nt of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected be</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, H. and Matsumoto, Y. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>