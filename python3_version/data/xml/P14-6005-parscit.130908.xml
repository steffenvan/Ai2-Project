<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.209570">
<title confidence="0.995688">
New Directions in Vector Space Models of Meaning
</title>
<author confidence="0.806634">
Phil Blunsom, Edward Grefenstette Georgiana Dinu
</author>
<affiliation confidence="0.766642">
and Karl Moritz Hermann∗ Center for Mind/Brain Sciences
University of Oxford University of Trento
</affiliation>
<email confidence="0.992586">
first.last@cs.ox.ac.uk georgiana.dinu@unitn.it
</email>
<sectionHeader confidence="0.996837" genericHeader="abstract">
1 Abstract
</sectionHeader>
<bodyText confidence="0.986507102564103">
Symbolic approaches have dominated NLP as a
means to model syntactic and semantic aspects of
natural language. While powerful inferential tools
exist for such models, they suffer from an inabil-
ity to capture correlation between words and to
provide a continuous model for word, phrase, and
document similarity. Distributed representations
are one mechanism to overcome these constraints.
This tutorial will supply NLP researchers with
the mathematical and conceptual background to
make use of vector-based models of meaning in
their own research. We will begin by motivating
the need for a transition from symbolic represen-
tations to distributed ones. We will briefly cover
how collocational (distributional) vectors can be
used and manipulated to model word meaning. We
will discuss the progress from distributional to dis-
tributed representations, and how neural networks
allow us to learn word vectors and condition them
on metadata such as parallel texts, topic labels, or
sentiment labels. Finally, we will present various
forms of semantic vector composition, and discuss
their relative strengths and weaknesses, and their
application to problems such as language mod-
elling, paraphrasing, machine translation and doc-
ument classification.
This tutorial aims to bring researchers up to
speed with recent developments in this fast-
moving field. It aims to strike a balance be-
tween providing a general introduction to vector-
based models of meaning, an analysis of diverg-
ing strands of research in the field, and also being
a hands-on tutorial to equip NLP researchers with
the necessary tools and background knowledge to
start working on such models. Attendees should
be comfortable with basic probability, linear alge-
bra, and continuous mathematics. No substantial
knowledge of machine learning is required.
∗Instructors listed in alphabetical order. 8
</bodyText>
<sectionHeader confidence="0.915588" genericHeader="keywords">
2 Outline
</sectionHeader>
<listItem confidence="0.999739916666667">
1. Motivation: Meaning in space
2. Learning distributional models for words
3. Neural language modelling and distributed
representations
(a) Neural language model fundamentals
(b) Recurrent neural language models
(c) Conditional neural language models
4. Semantic composition in vector spaces
(a) Algebraic and tensor-based composition
(b) The role of non-linearities
(c) Learning recursive neural models
(d) Convolutional maps and composition
</listItem>
<sectionHeader confidence="0.995054" genericHeader="introduction">
3 Instructors
</sectionHeader>
<bodyText confidence="0.9922404">
Phil Blunsom is an Associate Professor at the
University of Oxford’s Department of Computer
Science. His research centres on the probabilistic
modelling of natural languages, with a particular
interest in automating the discovery of structure
and meaning in text.
Georgiana Dinu is a postdoctoral researcher
at the University of Trento. Her research re-
volves around distributional semantics with a fo-
cus on compositionality within the distributional
paradigm.
Edward Grefenstette is a postdoctoral researcher
at Oxford’s Department of Computer Science. He
works on the relation between vector represen-
tations of language meaning and structured logi-
cal reasoning. His work in this area was recently
recognised by a best paper award at *SEM 2013.
Karl Moritz Hermann is a final-year DPhil stu-
dent at the Department of Computer Science in
Oxford. His research studies distributed and com-
positional semantics, with a particular emphasis
on mechanisms to reduce task-specific and mono-
lingual syntactic bias in such representations.
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 8,
Baltimore, Maryland, USA, 22 June 2014. c�2014 Association for Computational Linguistics
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.373780">
<title confidence="0.999628">New Directions in Vector Space Models of Meaning</title>
<author confidence="0.99196">Phil Blunsom</author>
<author confidence="0.99196">Edward Grefenstette Georgiana Dinu</author>
<affiliation confidence="0.870696">Moritz for Mind/Brain Sciences University of Oxford University of Trento</affiliation>
<abstract confidence="0.975221903225806">first.last@cs.ox.ac.uk georgiana.dinu@unitn.it 1 Abstract Symbolic approaches have dominated NLP as a means to model syntactic and semantic aspects of natural language. While powerful inferential tools exist for such models, they suffer from an inability to capture correlation between words and to provide a continuous model for word, phrase, and document similarity. Distributed representations are one mechanism to overcome these constraints. This tutorial will supply NLP researchers with the mathematical and conceptual background to make use of vector-based models of meaning in their own research. We will begin by motivating the need for a transition from symbolic representations to distributed ones. We will briefly cover how collocational (distributional) vectors can be used and manipulated to model word meaning. We will discuss the progress from distributional to distributed representations, and how neural networks allow us to learn word vectors and condition them on metadata such as parallel texts, topic labels, or sentiment labels. Finally, we will present various forms of semantic vector composition, and discuss their relative strengths and weaknesses, and their application to problems such as language modelling, paraphrasing, machine translation and document classification. This tutorial aims to bring researchers up to speed with recent developments in this fastmoving field. It aims to strike a balance be-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>