<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000352">
<title confidence="0.996193">
Adaptation of Reordering Models for Statistical Machine Translation
</title>
<author confidence="0.991774">
Boxing Chen, George Foster and Roland Kuhn
</author>
<affiliation confidence="0.727044">
National Research Council Canada
first.last@nrc-cnrc.gc.ca
</affiliation>
<sectionHeader confidence="0.996202" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998474576923077">
Previous research on domain adaptation (DA)
for statistical machine translation (SMT) has
mainly focused on the translation model (TM)
and the language model (LM). To the best of
our knowledge, there is no previous work on
reordering model (RM) adaptation for phrase-
based SMT. In this paper, we demonstrate
that mixture model adaptation of a lexical-
ized RM can significantly improve SMT per-
formance, even when the system already con-
tains a domain-adapted TM and LM. We find
that, surprisingly, different training corpora
can vary widely in their reordering character-
istics for particular phrase pairs. Furthermore,
particular training corpora may be highly suit-
able for training the TM or the LM, but unsuit-
able for training the RM, or vice versa, so mix-
ture weights for these models should be esti-
mated separately. An additional contribution
of the paper is to propose two improvements
to mixture model adaptation: smoothing the
in-domain sample, and weighting instances
by document frequency. Applied to mixture
RMs in our experiments, these techniques (es-
pecially smoothing) yield significant perfor-
mance improvements.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952125">
A phrase-based statistical machine translation
(SMT) system typically has three main components:
a translation model (TM) that contains information
about how to translate word sequences (phrases)
from the source language to the target language,
a language model (LM) that contains information
about probable word sequences in the target lan-
guage, and a reordering model (RM) that indicates
how the order of words in the source sentence is
likely to influence the order of words in the target
sentence. The TM and the RM are trained on parallel
data, and the LM is trained on target-language data.
Usage of language and therefore the best translation
practice differs widely across genres, topics, and di-
alects, and even depends on a particular author’s or
publication’s style; the word “domain” is often used
to indicate a particular combination of all these fac-
tors. Unless there is a perfect match between the
training data domain and the (test) domain in which
the SMT system will be used, one can often get bet-
ter performance by adapting the system to the test
domain.
In offline domain adaptation, the system is pro-
vided with a sample of translated sentences from
the test domain prior to deployment. In a popular
variant of offline adaptation, linear mixture model
adaptation, each training corpus is used to gener-
ate a separate model component that forms part of
a linear combination, and the sample is used to as-
sign a weight to each component (Foster and Kuhn,
2007). If the sample resembles some of the corpora
more than others, those corpora will receive higher
weights in the combination.
Previous research on domain adaptation for SMT
has focused on the TM and the LM. Such research
is easily motivated: translations across domains are
unreliable. For example, the Chinese translation
of the English word “mouse” would most likely be
“laoshu 老鼠” if the topic is the animal; if the topic
is computer hardware, its translation would most
</bodyText>
<page confidence="0.967244">
938
</page>
<note confidence="0.472241">
Proceedings of NAACL-HLT 2013, pages 938–946,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999956214285714">
likely be “shubiao 鼠标”. However, when the trans-
lation is for people in Taiwan, even when the topic
is computer hardware, its translation would more
likely be “huashu 滑鼠”. It is intuitively obvious
why TM and LM adaptation would be helpful here.
By contrast, it is not at all obvious that RM model
adaptation will improve SMT performace. One
would expect reordering behaviour to be characteris-
tic of a particular language pair, but not of particular
domains. At most, one might think that reordering
is lexicalized—perhaps, (for instance) in translating
from Chinese to English, or from Arabic to English,
there are certain words whose English translations
tend to undergo long-distance movement from their
original positions, while others stay close to their
original positions. However, one would not expect
a particular Chinese adverb or a particular Arabic
noun to undergo long-distance movement when be-
ing translated into English in one domain, but not in
others. Nevertheless, that is what we observe: see
section 5 below.
This paper shows that RM adaptation improves
the performance of our phrase-based SMT system.
In our implementation, the RM is adapted by means
of a linear mixture model, but it is likely that other
forms of RM adaptation would also work. We ob-
tain even more effective RM adaptation by smooth-
ing the in-domain sample and by weighting orienta-
tion counts by the document frequency of the phrase
pair. Both improvements could be applied to the TM
or the LM as well, though we have not done so.
Finally, the paper analyzes reordering to see why
RM adaptation works. There seem to be two fac-
tors at work. First, the reordering behaviour of
words and phrases often differs dramatically from
one bilingual corpus to another. Second, there are
corpora (for instance, comparable corpora and bilin-
gual lexicons) which may contain very valuable in-
formation for the TM, but which are poor sources
of RM information; RM adaptation downweights in-
formation from these corpora significantly, and thus
improves the overall quality of the RM.
</bodyText>
<sectionHeader confidence="0.997489" genericHeader="method">
2 Reordering Model
</sectionHeader>
<bodyText confidence="0.993983274509804">
In early SMT systems, such as (Koehn, 2004),
changes in word order when a sentence is trans-
lated were modeled by means of a penalty that is in-
curred when the decoder chooses, as the next source
phrase to be translated, a phrase that does not imme-
diately follow the previously translated source sen-
tence. Thus, the system penalizes deviations from
monotone order, with the magnitude of the penalty
being proportional to distance in the source sentence
between the end of the previously translated source
phrase and the start of the newly chosen source
phrase.
Many SMT systems, including our own, still use
this distance-based penalty as a feature. However,
starting with (Tillmann and Zhang, 2005; Koehn
et al., 2005), a more sophisticated type of reorder-
ing model has often been adopted as well, and has
yielded consistent performance gains. This type of
RM typically identifies three possible orientations
for a newly chosen source phrase: monotone (M),
swap (S), and discontinuous (D). The M orientation
occurs when the newly chosen phrase is immedi-
ately to the right of the previously translated phrase
in the source sentence, the S orientation occurs when
the new phrase is immediately to the left of the pre-
vious phrase, and the D orientation covers all other
cases.&apos; This type of RM is lexicalized: the estimated
probabilities of M, S and D depend on the source-
language and target-language words in both the pre-
vious phrase pair and the newly chosen one.
Galley and Manning (2008) proposed a “hierar-
chical” lexicalized RM in which the orientation (M,
S, or D) is determined not by individual phrase pairs,
but by blocks. A block is the largest contiguous se-
quence of phrase pairs that satisfies the phrase pair
consistency requirement of having no external links.
Thus, classification of the orientation of a newly
chosen phrase as M, S, or D is carried out as if the
decoder always chose the longest possible source
phrase in the past, and will choose the longest pos-
sible source phrase in the future.
The RM used in this paper is hierarchical and lex-
icalized. For a given phrase pair (f, e), we estimate
the probabilities that it will be in an M, S, or D ori-
entation o with respect to the previous phrase pair
and the following phrase pair (two separate distri-
butions). Orientation counts c(o, f, e) are obtained
from a word-aligned corpus using the method de-
&apos;Some researchers have distinguished between left and right
versions of the D orientation, but this 4-orientation scheme has
not yielded significant gains over the 3-orientation one.
</bodyText>
<page confidence="0.953653">
939
</page>
<equation confidence="0.916091">
c(·) + αu
</equation>
<bodyText confidence="0.999726444444444">
where p(o|e) is defined analogously to p(o|f), and
the four smoothing parameters αe, αf, αg, and αu
are set to values that minimize the perplexity of the
resulting model on held-out data.
During decoding, orientations with respect to the
previous context are obtained from a shift-reduce
parser, and orientations with respect to following
context are approximated using the coverage vector
(Cherry et al., 2012).
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="method">
3 RM Adaptation
</sectionHeader>
<subsectionHeader confidence="0.999741">
3.1 Linear mixture model
</subsectionHeader>
<bodyText confidence="0.99998075">
Following previous work (Foster and Kuhn, 2007;
Foster et al., 2010), we adopt the linear mixture
model technique for RM adaptation. This technique
trains separate models for each training corpus, then
learns weights for each of the models and combines
the weighted component models into a single model.
If we have N sub-corpora, the global reordering
model probabilities p(o|f, e) are computed as in (2):
</bodyText>
<equation confidence="0.991217333333334">
N
p(o|f, e) = αi pi(o|f, e) (2)
i=1
</equation>
<bodyText confidence="0.962075833333333">
where pi(o|f, e) is the reordering model trained on
sub-corpus i, and αi is its weight.
Following (Foster et al., 2010), we use the EM
algorithm to learn the weights that maximize the
probability of phrase-pair orientations in the devel-
opment set (in-domain data):
</bodyText>
<equation confidence="0.920697666666667">
αˆ = argmax � ˜p(o, f, e) log N αi pi(o|f, e)
α o,f,e i=1
(3)
</equation>
<bodyText confidence="0.9998452">
where ˜p(o, f, e) is the empirical distribution of
counts in the dev set (proportional to c(o, f, e)). Two
separate sets of mixing weights are learned: one for
the distribution with respect to the previous phrase
pair, and one for the next phrase pair.
</bodyText>
<subsectionHeader confidence="0.999518">
3.2 Development set smoothing
</subsectionHeader>
<bodyText confidence="0.999954375">
In Equation 3, ˜p(o, f, e) is extracted from the in-
domain development set. Since dev sets for SMT
systems are typically small (1,000-3,000 sentences),
we apply smoothing to this RM. We first obtain
a smoothed conditional distribution p(o|f, e) using
the MAP technique described above, then multiply
by the empirical marginal ˜p(e, f) to obtain a final
smoothed joint distribution p(o, f, e).
There is nothing about this idea that limits it to
the RM: smoothing could be applied to the statistics
in the dev that are used to estimate a mixture TM
or LM, in order to mitigate over-fitting. However,
we note that, compared to the TM, the over-fitting
problem is likely to be more acute for the RM, since
it splits counts for each phrase pair into three cate-
gories.
</bodyText>
<subsectionHeader confidence="0.996609">
3.3 Document-frequency weighting
</subsectionHeader>
<bodyText confidence="0.999821166666667">
Mixture models, like the RM in this paper, depend
on the existence of multiple training corpora, with
each sub-corpus nominally representing a domain.
A recent paper suggests that some phrase pairs be-
long to general language, while others are domain-
specific (Foster et al., 2010). If a phrase pair exists
in all training corpora, it probably belongs to general
language; on the other hand, if it appears in only
one or two training corpora, it is more likely to be
domain-specific.
We were interested in seeing whether information
about domain-specificity could improve the estima-
tion of mixture RM weights. The intuition is that
phrase pairs that belong to general language should
contribute more to determining sub-corpus weights,
since they are the ones whose reordering behaviour
is most likely to shift with domain. To capture this
intuition, we multiplied the empirical distribution in
</bodyText>
<listItem confidence="0.5151325">
(3) by the following factor, inspired by the standard
document-frequency formula:
</listItem>
<equation confidence="0.927597">
D(f, e) = log(DF(f, e) + K), (4)
</equation>
<bodyText confidence="0.753925166666667">
where DF(f, e) is the number of sub-corpora
that (f, e) appears in, and K is an empirically-
determined smoothing term.
scribed in (Cherry et al., 2012), and corresponding
probabilities p(o|f, e) are estimated using recursive
MAP smoothing:
</bodyText>
<equation confidence="0.9994965">
c(o, f, e) + αf p(o|f) + αe p(o|e)
p(o|f, e) = c(f, e) + αf + αe
c(o, f) + αg p(o)
p(o|f) =
c(f) + αg
c(o) + αu/3
p(o) =
, (1)
</equation>
<page confidence="0.987807">
940
</page>
<table confidence="0.99882495">
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 financial
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 Hansard
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&amp;ne 1.3M 2.0M 0.7 lexicon
others nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bn ng
NIST08 1,357 164K nw wl
</table>
<tableCaption confidence="0.9850055">
Table 1: NIST Chinese-English data. In the gen-
res column: nw=newswire, bc=broadcast conversa-
tion, bn=broadcast news, wl=weblog, ng=newsgroup,
un=United Nations proceedings.
</tableCaption>
<sectionHeader confidence="0.998579" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998164">
4.1 Data setting
</subsectionHeader>
<bodyText confidence="0.999965294117647">
We carried out experiments in two different settings,
both involving data from NIST Open MT 2012.2
The first setting uses data from the Chinese to En-
glish constrained track, comprising 283M English
tokens. We manually identified 14 sub-corpora on
the basis of genres and origins. Table 1 summarizes
the statistics and genres of all the training corpora
and the development and test sets; for the training
corpora, we show their size in number of words as
a percentage of all training data. Most training cor-
pora consist of parallel sentence pairs. The isi and
lex&amp;ne corpora are exceptions: the former is ex-
tracted from comparable data, while the latter is a
lexicon that includes many named entities. The de-
velopment set (tune) was taken from the NIST 2005
evaluation set, augmented with some web-genre ma-
terial reserved from other NIST corpora.
</bodyText>
<footnote confidence="0.949626">
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
</footnote>
<table confidence="0.998849">
corpus # segs # en toks % genres
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nw wl
NIST08 1,360 205K nw wl
NIST09 1,313 187K nw wl
</table>
<tableCaption confidence="0.975941333333333">
Table 2: NIST Arabic-English data. In the gen-
res column: nw=newswire, bc=broadcast conversation,
bn=broadcase news, ng=newsgroup, wl=weblog.
</tableCaption>
<bodyText confidence="0.999888833333333">
The second setting uses NIST 2012 Arabic to En-
glish data, but excluding the UN data. There are
about 47.8 million English running words in these
training data. We manually grouped the training data
into 7 groups according to genre and origin. Ta-
ble 2 summarizes the statistics and genres of all the
training corpora and the development and test sets.
Note that for this language pair, the comparable isi
data represent a large proportion of the training data:
72% of the English words. We use the evaluation
sets from NIST 2006, 2008, and 2009 as our devel-
opment set and two test sets, respectively.
</bodyText>
<subsectionHeader confidence="0.965107">
4.2 System
</subsectionHeader>
<bodyText confidence="0.999772333333333">
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et al.,
2007). The corpus was word-aligned using IBM2,
HMM, and IBM4 models, and the phrase table was
the union of phrase pairs extracted from these sepa-
rate alignments, with a length limit of 7. The trans-
lation model was smoothed in both directions with
KN smoothing (Chen et al., 2011). The DF smooth-
ing term K in equation 4 was set to 0.1 using held-
out optimization. We use the hierarchical lexical-
ized RM described above, with a distortion limit of
7. Other features include lexical weighting in both
directions, word count, a distance-based RM, a 4-
gram LM trained on the target side of the parallel
data, and a 6-gram English Gigaword LM. The sys-
</bodyText>
<page confidence="0.983052">
941
</page>
<table confidence="0.999710142857143">
system Chinese Arabic
baseline 31.7 46.8
baseline+loglin 29.6 45.9
RMA 31.8 47.7**
RMA+DF 32.2* 47.9**
RMA+dev smoothing 32.3* 48.3**
RMA+dev smoothing+DF 32.8** 48.2**
</table>
<tableCaption confidence="0.927179">
Table 3: Results for variants of RM adaptation.
</tableCaption>
<table confidence="0.999704">
system Chinese Arabic
LM+TM adaptation 33.2 47.7
+RMA+dev-smoothing+DF 33.5 48.4**
</table>
<tableCaption confidence="0.9754615">
Table 4: RM adaptation improves over a baseline con-
taining adapted LMs and TMs.
</tableCaption>
<bodyText confidence="0.9221665">
tem was tuned with batch lattice MIRA (Cherry and
Foster, 2012).
</bodyText>
<subsectionHeader confidence="0.860605">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999938518518519">
For our main baseline, we simply concatenate all
training data. We also tried augmenting this with
separate log-linear features corresponding to sub-
corpus-specific RMs. Our metric is case-insensitvie
IBM BLEU-4 (Papineni et al., 2002); we report
BLEU scores averaged across both test sets. Follow-
ing (Koehn, 2004), we use the bootstrap-resampling
test to do significance testing. In tables 3 to 5, *
and ** denote significant gains over the baseline at
p &lt; 0.05 and p &lt; 0.01 levels, respectively.
Table 3 shows that reordering model adaptation
helps in both data settings. Adding either document-
frequency weighting (equation 4) or dev-set smooth-
ing makes the improvement significant in both set-
tings. Using both techniques together yields highly
significant improvements.
Our second experiment measures the improve-
ment from RM adaptation over a baseline that
includes adapted LMs and TMs. We use the
same technique—linear mixtures with EM-tuned
weights—to adapt these models. Table 4 shows that
adapting the RM gives gains over this strong base-
line for both language pairs; improvements are sig-
nificant in the case of Arabic to English.
The third experiment breaks down the gains in the
last line of table 4 by individual adapted model. As
shown in table 5, RM adaptation yielded the largest
</bodyText>
<table confidence="0.999315">
system Chinese Arabic
baseline 31.7 46.8
LM adaptation 32.1* 47.0
TM adaptation 33.0** 47.5**
RM adaptation 32.8** 48.2**
</table>
<tableCaption confidence="0.99993">
Table 5: Comparison of LM, TM, and RM adaptation.
</tableCaption>
<bodyText confidence="0.9814174">
improvement on Arabic, while TM adaptation did
best on Chinese. Surprisingly, both methods sig-
nificantly outperformed LM adaptation, which only
achieved significant gains over the baseline for Chi-
nese.
</bodyText>
<sectionHeader confidence="0.997553" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999916714285714">
Why does RM adaptation work? Intuitively, one
would think that reordering behaviour for a given
phrase pair should not be much affected by domain,
making RM adaptation pointless. That is probably
why (as far as we know) no-one has tried it before.
In this section, we describe three factors that account
for at least part of the observed gains.
</bodyText>
<subsectionHeader confidence="0.999565">
5.1 Weighting by corpus quality
</subsectionHeader>
<bodyText confidence="0.999991608695652">
One answer to the above question is that some cor-
pora are better for training RMs than others. Fur-
thermore, corpora that are good for training the LM
or TM are not necessarily good for training the RM,
and vice versa. Tables 6 and 7 illustrate this. These
list the weights assigned to various sub-corpora for
LM, TM, and RM mixture models.
The weights assigned to the isi sub-corpus in par-
ticular exhibit a striking pattern. These are high in
the LM mixtures, moderate in the TM mixtures, and
very low in the RM mixtures. When one considers
that isi contains 72.6% of the English words in the
Arabic training data (see table 2), its weight of 0.01
in the RM mixture is remarkable.
On reflection, it makes sense that EM would as-
sign weights in the order it does. The isi corpus
consists of comparable data: sentence pairs whose
source- and target-language sides are similar, but of-
ten not mutual translations. These are a valuable
source of in-domain n-grams for the LM; a some-
what noisy source of in-domain phrase pairs for the
TM; and an unreliable source of re-ordering patterns
for the RM. Figure 1 shows this. Although the two
</bodyText>
<page confidence="0.994524">
942
</page>
<table confidence="0.9725674">
LM TM RM
isi (0.23) un (0.29) un (0.21)
gale nw (0.11) fbis (0.15) gale nw (0.13)
un (0.11) hkh (0.10) lex&amp;ne (0.12)
sino. (0.09) gale nw (0.09) hkh (0.08)
fbis (0.08) gale bn (0.07) fbis (0.08)
fin. (0.07) oth nw (0.06) gale bn (0.08)
oth nw (0.07) sino. (0.06) gale wl (0.06)
gale bn (0.07) isi (0.05) gale bc (0.06)
gale wl (0.06) hkn (0.04) hkn (0.04)
hkh (0.06) fin. (0.04) fin. (0.04)
hkn (0.03) gale bc (0.03) oth nw (0.03)
gale bc (0.02) gale wl (0.02) hkl (0.03)
lex&amp;ne (0.00) lex&amp;ne (0.00) isi (0.01)
hkl (0.00) hkl (0.00) sino. (0.01)
</table>
<tableCaption confidence="0.9322665">
Table 6: Chinese-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
</tableCaption>
<table confidence="0.9295515">
LM TM RM
isi (0.41) isi (0.35) gale bc (0.21)
oth nw (0.19) oth nw (0.29) gale ng (0.20)
gale ng (0.15) gale bc (0.10) gale nw (0.20)
gale wl (0.09) gale ng (0.08) oth nw (0.13)
gale nw (0.07) gale bn (0.07) gale ng (0.12)
gale bc (0.05) gale nw (0.07) gale wb (0.11)
gale bn (0.02) gale wl (0.05) isi (0.01)
</table>
<tableCaption confidence="0.9934645">
Table 7: Arabic-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
</tableCaption>
<bodyText confidence="0.977078">
sides of the comparable data are similar, they give
the misleading impression that the phrases labeled
1, 2, 3 in the Chinese source should be reordered as
2, 3, 1 in English. We show a reference translation
of the Chinese source (not found in the comparable
data) that reorders the phrases as 1, 3, 2.
Thus, RM adaptation allows the RM to learn that
certain corpora whose reordering information is of
lower quality corpora should have lower weights.
The optimal weights for corpora inside an RM may
be different from the optimal weights inside a TM or
LM.
</bodyText>
<subsectionHeader confidence="0.999546">
5.2 Weighting by domain match
</subsectionHeader>
<bodyText confidence="0.689075583333334">
So is this all that RM adaptation does: downweight
poor-quality data? We believe there is more to
RM adaptation than that. Specifically, even if one
REF: The American list of goods that would incur
tariffs in retaliation would certainly not be
accepted by the Chinese government.
SRC: �®(1) M 411 i**! *®(2) axe mut-f T,
�� M(3)�
TGT: And the Chinese(2) side would certainly not
accept(3) the unreasonable demands put
forward by the Americans(1) concerning the
protection of intellectual property rights .
</bodyText>
<figureCaption confidence="0.975173">
Figure 1: Example of sentence pair from comparable
data; underlined words with the same number are trans-
lations of each other
</figureCaption>
<table confidence="0.999276125">
Corpus M S D Count
fbis 0.50 0.07 0.43 685
financial 0.32 0.28 0.41 65
gale bc 0.60 0.10 0.31 50
gale bn 0.47 0.15 0.37 109
gale nw 0.51 0.05 0.44 326
gale wl 0.42 0.26 0.32 52
hkh 0.29 0.23 0.48 130
hkl 0.28 0.16 0.56 263
hkn 0.30 0.27 0.43 241
isi 0.24 0.16 0.60 240
lex&amp;ne 0.94 0.03 0.02 1
others nw 0.29 0.16 0.55 23
sinorama 0.44 0.07 0.49 110
un 0.37 0.10 0.53 15
dev 0.46 0.24 0.31 11
</table>
<tableCaption confidence="0.979271">
Table 8: Orientation frequencies for the phrase pair “立
即 immediately”, with respect to the previous phrase.
</tableCaption>
<bodyText confidence="0.999829769230769">
considers only high-quality data for training RMs
(ignoring comparable data, etc.) one sees differ-
ences in reordering behaviour between different do-
mains. This isn’t just because of differences in word
frequencies between domains, because we observe
domain-dependent differences in reordering for the
same phrase pair. Two examples are given below:
one Chinese-English, one Arabic-English.
Table 8 shows reordering data for the phrase
pair “立即 immediately” in various corpora. No-
tice the strong difference in behaviour between the
three Hong Kong corpora—hkh, hkl and hkn—and
some of the other corpora, for instance fbis. In the
</bodyText>
<page confidence="0.997142">
943
</page>
<figure confidence="0.881415888888889">
Corpus
gale bc
gale bn
gale ng
gale nw
gale wl
isi
other nw
dev
</figure>
<table confidence="0.993006444444445">
M S D Count
0.50 0.27 0.22 233
0.56 0.21 0.23 226
0.51 0.13 0.37 295
0.47 0.20 0.33 167
0.56 0.18 0.26 127
0.50 0.06 0.44 5502
0.50 0.16 0.34 1450
0.75 0.12 0.13 52
</table>
<figureCaption confidence="0.7099845">
Figure 2: An example of different word ordering in Man-
darin from different area.
</figureCaption>
<tableCaption confidence="0.8977055">
Table 9: Orientation frequencies for the phrase pair
“work AlEml” with respect to the previous phrase.
</tableCaption>
<bodyText confidence="0.99951825">
Hong Kong corpora, immediately is much less likely
(probability of around 0.3) to be associated with a
monotone (M) orientation than it is in fbis (proba-
bility of 0.5). This phrase pair is relatively frequent
in both corpora, so this disparity seems too great to
be due to chance.
Table 9 shows reordering behaviour for the phrase
pair “work AlEml”3 across different sub-corpora.
As in the Chinese example, there appear to be sig-
nificant differences in reordering patterns for cer-
tain corpora. For instance, gale bc swaps this well-
attested phrase pair twice as often (probability of
0.27) as gale ng (probability of 0.13).
For Chinese, it is possible that dialect plays a role
in reordering behaviour. In theory, Mandarin Chi-
nese is a single language which is quite different,
especially in spoken form, from other languages of
China such as Cantonese, Hokkien, Shanghainese,
and so on. In practice, many speakers of Mandarin
may be unconsciously influenced by other languages
that they speak, or by other languages that they don’t
speak but that have an influence over people they in-
teract with frequently. Word order can be affected
by this: the Mandarin of Mainland China, Hong
Kong and Taiwan sometimes has slightly different
word order. Hong Kong Mandarin can be somewhat
influenced by Cantonese, and Taiwan Mandarin by
Hokkien. For instance, if a verb is modified by an
adverb in Mandarin, the standard word order is “ad-
verb verb”. However, since in Cantonese, “verb ad-
verb” is a more common word order, speakers and
writers of Mandarin in Hong Kong may adopt the
</bodyText>
<footnote confidence="0.8617685">
3We represent the Arabic word AlEml in its Buckwalter
transliteration.
</footnote>
<bodyText confidence="0.99984575">
“verb adverb” order in that language as well. Figure
2 shows how a different word order in the Mandarin
source affects reordering when translating into En-
glish. Perhaps in situations where different training
corpora represent different dialects, RM adaptation
involves an element of dialect adaptation. We are ea-
ger to test this hypothesis for Arabic—different di-
alects of Arabic are much more different from each
other than dialects of Mandarin, and reordering is
often one of the differences—but we do not have ac-
cess to Arabic training, dev, and test data in which
the dialects are clearly separated.
It is possible that RM adaptation also has an el-
ement of genre adaptation. We have not yet been
able to confirm or refute this hypothesis. However,
whatever is causing the corpus-dependent reorder-
ing patterns for particular phrase pairs shown in the
two tables above, it is clear that they may explain
the performance improvements we observe for RM
adaptation in our experiments.
</bodyText>
<subsectionHeader confidence="0.998388">
5.3 Penalizing highly-specific phrase pairs
</subsectionHeader>
<bodyText confidence="0.999931533333333">
In section 3.3 we described our strategy for giving
general (high document-frequency) phrase pairs that
occur in the dev set more influence in determining
mixing weights. An artifact of our implementation
applies a similar strategy to the probability estimates
for all phrase pairs in the model. This is that 0 prob-
abilities are assigned to all orientations whenever a
phrase pair is absent from a particular sub-corpus.
Thus, for example, a pair (f, e) that occurs only
in sub-corpus i will receive a probability p(o|f, e) _
αi pi(o|f, e) in the mixture model (equation 2).
Since αi G 1, this amounts to a penalty on pairs
that occur in few sub-corpora, especially ones with
low mixture weights.
The resulting mixture model is deficient (non-
</bodyText>
<page confidence="0.995216">
944
</page>
<bodyText confidence="0.999960363636364">
normalized), but easy to fix by backing off to a
global distribution such as p(o) in equation 1. How-
ever, we found that this “fix” caused large drops in
performance, for instance from the Arabic BLEU
score of 48.3 reported in table 3 to 46.0. We there-
fore retained the original strategy, which can be seen
as a form of instance weighting. Moreover, it is one
that is particularly effective in the RM, since, com-
pared to a similar strategy in the TM (which we also
employ), it applies to whole phrase pairs and results
in much larger penalties.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999914509803922">
Domain adaptation is an active topic in the NLP re-
search community. Its application to SMT systems
has recently received considerable attention. Previ-
ous work on SMT adaptation has mainly focused
on translation model (TM) and language model
(LM) adaptation. Approaches that have been tried
for SMT model adaptation include mixture models,
transductive learning, data selection, data weighting,
and phrase sense disambiguation.
Research on mixture models has considered both
linear and log-linear mixtures. Both were studied
in (Foster and Kuhn, 2007), which concluded that
the best approach was to combine sub-models of
the same type (for instance, several different TMs
or several different LMs) linearly, while combining
models of different types (for instance, a mixture
TM with a mixture LM) log-linearly. (Koehn and
Schroeder, 2007), instead, opted for combining the
sub-models directly in the SMT log-linear frame-
work.
In transductive learning, an MT system trained on
general domain data is used to translate in-domain
monolingual data. The resulting bilingual sentence
pairs are then used as additional training data (Ueff-
ing et al., 2007; Chen et al., 2008; Schwenk, 2008;
Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al., 2004; L¨u
et al., 2007; Moore and Lewis, 2010; Axelrod et
al., 2011) search for bilingual sentence pairs that are
similar to the in-domain “dev” data, then add them
to the training data. The selection criteria are typi-
cally related to the TM, though the newly found data
will be used for training not only the TM but also the
LM and RM.
Data weighting approaches (Matsoukas et al.,
2009; Foster et al., 2010; Huang and Xiang, 2010;
Phillips and Brown, 2011; Sennrich, 2012) use a
rich feature set to decide on weights for the train-
ing data, at the sentence or phrase pair level. For
instance, a sentence from a corpus whose domain is
far from that of the dev set would typically receive
a low weight, but sentences in this corpus that ap-
pear to be of a general nature might receive higher
weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 4 proposed phrase sense disambiguation
(PSD) for translation model adaptation. In this ap-
proach, the context of a phrase helps the system to
find the appropriate translation.
All of the above work focuses on either TM or
LM domain adaptation.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999940391304348">
In this paper, we adapt the lexicalized reordering
model (RM) of an SMT system to the domain in
which the system will operate using a mixture model
approach. Domain adaptation of translation mod-
els (TMs) and language models (LMs) has become
common for SMT systems, but to our knowledge
this is the first attempt in the literature to adapt the
RM. Our experiments demonstrate that RM adap-
tation can significantly improve translation quality,
even when the system already has TM and LM adap-
tation. We also experimented with two modifica-
tions to linear mixture model adaptation: dev set
smoothing and weighting orientation counts with
document frequency of phrase pairs. Both ideas
are potentially applicable to TM and LM adaptation.
Dev set smoothing, in particular, seems to improve
the performance of RM adaptation significantly. Fi-
nally, we investigate why RM adaptation helps SMT
performance. Three factors seem to be important:
downweighting information from corpora that are
less suitable for modeling reordering (such as com-
parable corpora), dialect/genre effects, and implicit
instance weighting.
</bodyText>
<footnote confidence="0.990799">
4http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
</footnote>
<page confidence="0.996825">
945
</page>
<sectionHeader confidence="0.996282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764320388349">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
Workshop on Statistical Machine Translation, Athens,
March. WMT.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and Howard
Johnson. 2011. Unpacking and transforming feature
functions: New ways to smooth phrase tables. In MT
Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In WMT 2012.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the ACL Work-
shop on Statistical Machine Translation, Prague, June.
WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model. In
EMNLP 2008, pages 848–856, Hawaii, October.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for SMT. In COLING 2010.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224–227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Edin-
burgh system description for the 2005 NIST MT eval-
uation. In Proceedings of Machine Translation Evalu-
ation Workshop.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL 2007, Demon-
stration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the 6th Conference of the As-
sociation for Machine Translation in the Americas,
Georgetown University, Washington D.C., October.
Springer-Verlag.
Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Prague, Czech
Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311–318, Philadel-
phia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Training
machine translation with a second-order taylor approx-
imation of weighted translation instances. In MT Sum-
mit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT 2008.
Rico Sennrich. 2012. Mixture-modeling with unsuper-
vised clusters for domain adaptation in statistical ma-
chine translation. In EACL 2012.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Ann Ar-
bor, Michigan, July. ACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Prague, Czech Republic, June. ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 2004, Geneva, August.
</reference>
<page confidence="0.99878">
946
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902300">
<title confidence="0.997506">Adaptation of Reordering Models for Statistical Machine Translation</title>
<author confidence="0.935257">Boxing Chen</author>
<author confidence="0.935257">George Foster</author>
<author confidence="0.935257">Roland</author>
<affiliation confidence="0.975745">National Research Council</affiliation>
<email confidence="0.991055">first.last@nrc-cnrc.gc.ca</email>
<abstract confidence="0.999263481481481">Previous research on domain adaptation (DA) for statistical machine translation (SMT) has mainly focused on the translation model (TM) and the language model (LM). To the best of our knowledge, there is no previous work on reordering model (RM) adaptation for phrasebased SMT. In this paper, we demonstrate that mixture model adaptation of a lexicalized RM can significantly improve SMT performance, even when the system already contains a domain-adapted TM and LM. We find that, surprisingly, different training corpora can vary widely in their reordering characteristics for particular phrase pairs. Furthermore, particular training corpora may be highly suitable for training the TM or the LM, but unsuitable for training the RM, or vice versa, so mixture weights for these models should be estimated separately. An additional contribution of the paper is to propose two improvements to mixture model adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield significant performance improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="27879" citStr="Axelrod et al., 2011" startWordPosition="4683" endWordPosition="4686">nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation,</booktitle>
<publisher>WMT.</publisher>
<location>Athens,</location>
<contexts>
<context position="27769" citStr="Bertoldi and Federico, 2009" startWordPosition="4664" endWordPosition="4667">approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence o</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the 4th Workshop on Statistical Machine Translation, Athens, March. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Exploiting n-best hypotheses for smt selfenhancement.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="27724" citStr="Chen et al., 2008" startWordPosition="4658" endWordPosition="4661">7), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on wei</context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008. Exploiting n-best hypotheses for smt selfenhancement. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
<author>Howard Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="14751" citStr="Chen et al., 2011" startWordPosition="2468" endWordPosition="2471">language pair, the comparable isi data represent a large proportion of the training data: 72% of the English words. We use the evaluation sets from NIST 2006, 2008, and 2009 as our development set and two test sets, respectively. 4.2 System Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). The corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with KN smoothing (Chen et al., 2011). The DF smoothing term K in equation 4 was set to 0.1 using heldout optimization. We use the hierarchical lexicalized RM described above, with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4- gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The sys941 system Chinese Arabic baseline 31.7 46.8 baseline+loglin 29.6 45.9 RMA 31.8 47.7** RMA+DF 32.2* 47.9** RMA+dev smoothing 32.3* 48.3** RMA+dev smoothing+DF 32.8** 48.2** Table 3: Results for variants of RM adaptation. system Chines</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In MT Summit 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="15564" citStr="Cherry and Foster, 2012" startWordPosition="2601" endWordPosition="2604">de lexical weighting in both directions, word count, a distance-based RM, a 4- gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The sys941 system Chinese Arabic baseline 31.7 46.8 baseline+loglin 29.6 45.9 RMA 31.8 47.7** RMA+DF 32.2* 47.9** RMA+dev smoothing 32.3* 48.3** RMA+dev smoothing+DF 32.8** 48.2** Table 3: Results for variants of RM adaptation. system Chinese Arabic LM+TM adaptation 33.2 47.7 +RMA+dev-smoothing+DF 33.5 48.4** Table 4: RM adaptation improves over a baseline containing adapted LMs and TMs. tem was tuned with batch lattice MIRA (Cherry and Foster, 2012). 4.3 Results For our main baseline, we simply concatenate all training data. We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs. Our metric is case-insensitvie IBM BLEU-4 (Papineni et al., 2002); we report BLEU scores averaged across both test sets. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p &lt; 0.05 and p &lt; 0.01 levels, respectively. Table 3 shows that reordering model adaptation helps in both data settings. Adding either</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In NAACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>On hierarchical re-ordering and permutation parsing for phrase-based decoding.</title>
<date>2012</date>
<booktitle>In WMT</booktitle>
<contexts>
<context position="8445" citStr="Cherry et al., 2012" startWordPosition="1375" endWordPosition="1378">he method de&apos;Some researchers have distinguished between left and right versions of the D orientation, but this 4-orientation scheme has not yielded significant gains over the 3-orientation one. 939 c(·) + αu where p(o|e) is defined analogously to p(o|f), and the four smoothing parameters αe, αf, αg, and αu are set to values that minimize the perplexity of the resulting model on held-out data. During decoding, orientations with respect to the previous context are obtained from a shift-reduce parser, and orientations with respect to following context are approximated using the coverage vector (Cherry et al., 2012). 3 RM Adaptation 3.1 Linear mixture model Following previous work (Foster and Kuhn, 2007; Foster et al., 2010), we adopt the linear mixture model technique for RM adaptation. This technique trains separate models for each training corpus, then learns weights for each of the models and combines the weighted component models into a single model. If we have N sub-corpora, the global reordering model probabilities p(o|f, e) are computed as in (2): N p(o|f, e) = αi pi(o|f, e) (2) i=1 where pi(o|f, e) is the reordering model trained on sub-corpus i, and αi is its weight. Following (Foster et al., 2</context>
<context position="11492" citStr="Cherry et al., 2012" startWordPosition="1889" endWordPosition="1892">domain-specificity could improve the estimation of mixture RM weights. The intuition is that phrase pairs that belong to general language should contribute more to determining sub-corpus weights, since they are the ones whose reordering behaviour is most likely to shift with domain. To capture this intuition, we multiplied the empirical distribution in (3) by the following factor, inspired by the standard document-frequency formula: D(f, e) = log(DF(f, e) + K), (4) where DF(f, e) is the number of sub-corpora that (f, e) appears in, and K is an empiricallydetermined smoothing term. scribed in (Cherry et al., 2012), and corresponding probabilities p(o|f, e) are estimated using recursive MAP smoothing: c(o, f, e) + αf p(o|f) + αe p(o|e) p(o|f, e) = c(f, e) + αf + αe c(o, f) + αg p(o) p(o|f) = c(f) + αg c(o) + αu/3 p(o) = , (1) 940 corpus # segs # en tok % genres fbis 250K 10.5M 3.7 nw financial 90K 2.5M 0.9 financial gale bc 79K 1.3M 0.5 bc gale bn 75K 1.8M 0.6 bn ng gale nw 25K 696K 0.2 nw gale wl 24K 596K 0.2 wl hkh 1.3M 39.5M 14.0 Hansard hkl 400K 9.3M 3.3 legal hkn 702K 16.6M 5.9 nw isi 558K 18.0M 6.4 nw lex&amp;ne 1.3M 2.0M 0.7 lexicon others nw 146K 5.2M 1.8 nw sinorama 282K 10.0M 3.5 nw un 5.0M 164M 5</context>
</contexts>
<marker>Cherry, Moore, Quirk, 2012</marker>
<rawString>Colin Cherry, Robert C. Moore, and Chris Quirk. 2012. On hierarchical re-ordering and permutation parsing for phrase-based decoding. In WMT 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<publisher>WMT.</publisher>
<location>Prague,</location>
<contexts>
<context position="2791" citStr="Foster and Kuhn, 2007" startWordPosition="440" endWordPosition="443">these factors. Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. In offline domain adaptation, the system is provided with a sample of translated sentences from the test domain prior to deployment. In a popular variant of offline adaptation, linear mixture model adaptation, each training corpus is used to generate a separate model component that forms part of a linear combination, and the sample is used to assign a weight to each component (Foster and Kuhn, 2007). If the sample resembles some of the corpora more than others, those corpora will receive higher weights in the combination. Previous research on domain adaptation for SMT has focused on the TM and the LM. Such research is easily motivated: translations across domains are unreliable. For example, the Chinese translation of the English word “mouse” would most likely be “laoshu 老鼠” if the topic is the animal; if the topic is computer hardware, its translation would most 938 Proceedings of NAACL-HLT 2013, pages 938–946, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Lingu</context>
<context position="8534" citStr="Foster and Kuhn, 2007" startWordPosition="1389" endWordPosition="1392">D orientation, but this 4-orientation scheme has not yielded significant gains over the 3-orientation one. 939 c(·) + αu where p(o|e) is defined analogously to p(o|f), and the four smoothing parameters αe, αf, αg, and αu are set to values that minimize the perplexity of the resulting model on held-out data. During decoding, orientations with respect to the previous context are obtained from a shift-reduce parser, and orientations with respect to following context are approximated using the coverage vector (Cherry et al., 2012). 3 RM Adaptation 3.1 Linear mixture model Following previous work (Foster and Kuhn, 2007; Foster et al., 2010), we adopt the linear mixture model technique for RM adaptation. This technique trains separate models for each training corpus, then learns weights for each of the models and combines the weighted component models into a single model. If we have N sub-corpora, the global reordering model probabilities p(o|f, e) are computed as in (2): N p(o|f, e) = αi pi(o|f, e) (2) i=1 where pi(o|f, e) is the reordering model trained on sub-corpus i, and αi is its weight. Following (Foster et al., 2010), we use the EM algorithm to learn the weights that maximize the probability of phras</context>
<context position="27109" citStr="Foster and Kuhn, 2007" startWordPosition="4562" endWordPosition="4565">whole phrase pairs and results in much larger penalties. 6 Related work Domain adaptation is an active topic in the NLP research community. Its application to SMT systems has recently received considerable attention. Previous work on SMT adaptation has mainly focused on translation model (TM) and language model (LM) adaptation. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and phrase sense disambiguation. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Ch</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the ACL Workshop on Statistical Machine Translation, Prague, June. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Boston.</location>
<contexts>
<context position="8556" citStr="Foster et al., 2010" startWordPosition="1393" endWordPosition="1396"> 4-orientation scheme has not yielded significant gains over the 3-orientation one. 939 c(·) + αu where p(o|e) is defined analogously to p(o|f), and the four smoothing parameters αe, αf, αg, and αu are set to values that minimize the perplexity of the resulting model on held-out data. During decoding, orientations with respect to the previous context are obtained from a shift-reduce parser, and orientations with respect to following context are approximated using the coverage vector (Cherry et al., 2012). 3 RM Adaptation 3.1 Linear mixture model Following previous work (Foster and Kuhn, 2007; Foster et al., 2010), we adopt the linear mixture model technique for RM adaptation. This technique trains separate models for each training corpus, then learns weights for each of the models and combines the weighted component models into a single model. If we have N sub-corpora, the global reordering model probabilities p(o|f, e) are computed as in (2): N p(o|f, e) = αi pi(o|f, e) (2) i=1 where pi(o|f, e) is the reordering model trained on sub-corpus i, and αi is its weight. Following (Foster et al., 2010), we use the EM algorithm to learn the weights that maximize the probability of phrase-pair orientations in</context>
<context position="10614" citStr="Foster et al., 2010" startWordPosition="1746" endWordPosition="1749">othing could be applied to the statistics in the dev that are used to estimate a mixture TM or LM, in order to mitigate over-fitting. However, we note that, compared to the TM, the over-fitting problem is likely to be more acute for the RM, since it splits counts for each phrase pair into three categories. 3.3 Document-frequency weighting Mixture models, like the RM in this paper, depend on the existence of multiple training corpora, with each sub-corpus nominally representing a domain. A recent paper suggests that some phrase pairs belong to general language, while others are domainspecific (Foster et al., 2010). If a phrase pair exists in all training corpora, it probably belongs to general language; on the other hand, if it appears in only one or two training corpora, it is more likely to be domain-specific. We were interested in seeing whether information about domain-specificity could improve the estimation of mixture RM weights. The intuition is that phrase pairs that belong to general language should contribute more to determining sub-corpus weights, since they are the ones whose reordering behaviour is most likely to shift with domain. To capture this intuition, we multiplied the empirical dis</context>
<context position="28218" citStr="Foster et al., 2010" startWordPosition="4744" endWordPosition="4747">ta. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropria</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP 2008,</booktitle>
<pages>848--856</pages>
<location>Hawaii,</location>
<contexts>
<context position="6956" citStr="Galley and Manning (2008)" startWordPosition="1124" endWordPosition="1127">ically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.&apos; This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP 2008, pages 848–856, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Bing Xiang</author>
</authors>
<title>Feature-rich discriminative phrase rescoring for SMT.</title>
<date>2010</date>
<booktitle>In COLING</booktitle>
<contexts>
<context position="28241" citStr="Huang and Xiang, 2010" startWordPosition="4748" endWordPosition="4751">ingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of </context>
</contexts>
<marker>Huang, Xiang, 2010</marker>
<rawString>Fei Huang and Bing Xiang. 2010. Feature-rich discriminative phrase rescoring for SMT. In COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="27396" citStr="Koehn and Schroeder, 2007" startWordPosition="4606" endWordPosition="4609">model (TM) and language model (LM) adaptation. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and phrase sense disambiguation. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A B Mayne</author>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>D Talbot</author>
<author>M White</author>
</authors>
<title>Edinburgh system description for the 2005 NIST MT evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="6186" citStr="Koehn et al., 2005" startWordPosition="994" endWordPosition="997">ere modeled by means of a penalty that is incurred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a feature. However, starting with (Tillmann and Zhang, 2005; Koehn et al., 2005), a more sophisticated type of reordering model has often been adopted as well, and has yielded consistent performance gains. This type of RM typically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.&apos; This type of RM is lexicalized: the estimat</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, White, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch, M. Osborne, D. Talbot, and M. White. 2005. Edinburgh system description for the 2005 NIST MT evaluation. In Proceedings of Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL 2007, Demonstration Session.</booktitle>
<contexts>
<context position="14477" citStr="Koehn et al., 2007" startWordPosition="2420" endWordPosition="2423">about 47.8 million English running words in these training data. We manually grouped the training data into 7 groups according to genre and origin. Table 2 summarizes the statistics and genres of all the training corpora and the development and test sets. Note that for this language pair, the comparable isi data represent a large proportion of the training data: 72% of the English words. We use the evaluation sets from NIST 2006, 2008, and 2009 as our development set and two test sets, respectively. 4.2 System Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). The corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with KN smoothing (Chen et al., 2011). The DF smoothing term K in equation 4 was set to 0.1 using heldout optimization. We use the hierarchical lexicalized RM described above, with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4- gram LM trained on the target side of the parallel data, a</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>Georgetown University, Washington D.C.,</location>
<contexts>
<context position="5512" citStr="Koehn, 2004" startWordPosition="884" endWordPosition="885">ve not done so. Finally, the paper analyzes reordering to see why RM adaptation works. There seem to be two factors at work. First, the reordering behaviour of words and phrases often differs dramatically from one bilingual corpus to another. Second, there are corpora (for instance, comparable corpora and bilingual lexicons) which may contain very valuable information for the TM, but which are poor sources of RM information; RM adaptation downweights information from these corpora significantly, and thus improves the overall quality of the RM. 2 Reordering Model In early SMT systems, such as (Koehn, 2004), changes in word order when a sentence is translated were modeled by means of a penalty that is incurred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a feat</context>
<context position="15891" citStr="Koehn, 2004" startWordPosition="2652" endWordPosition="2653">8** 48.2** Table 3: Results for variants of RM adaptation. system Chinese Arabic LM+TM adaptation 33.2 47.7 +RMA+dev-smoothing+DF 33.5 48.4** Table 4: RM adaptation improves over a baseline containing adapted LMs and TMs. tem was tuned with batch lattice MIRA (Cherry and Foster, 2012). 4.3 Results For our main baseline, we simply concatenate all training data. We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs. Our metric is case-insensitvie IBM BLEU-4 (Papineni et al., 2002); we report BLEU scores averaged across both test sets. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p &lt; 0.05 and p &lt; 0.01 levels, respectively. Table 3 shows that reordering model adaptation helps in both data settings. Adding either documentfrequency weighting (equation 4) or dev-set smoothing makes the improvement significant in both settings. Using both techniques together yields highly significant improvements. Our second experiment measures the improvement from RM adaptation over a baseline that includes adapted LMs and TMs. We use the same techniqu</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas, Georgetown University, Washington D.C., October. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan L¨u</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving Statistical Machine Translation Performance by Training Data Selection and Optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Prague, Czech Republic.</location>
<marker>L¨u, Huang, Liu, 2007</marker>
<rawString>Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving Statistical Machine Translation Performance by Training Data Selection and Optimization. In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="28197" citStr="Matsoukas et al., 2009" startWordPosition="4740" endWordPosition="4743">in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system </context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data. In</title>
<date>2010</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="27856" citStr="Moore and Lewis, 2010" startWordPosition="4679" endWordPosition="4682">veral different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from th</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia,</location>
<contexts>
<context position="15812" citStr="Papineni et al., 2002" startWordPosition="2637" endWordPosition="2640">A 31.8 47.7** RMA+DF 32.2* 47.9** RMA+dev smoothing 32.3* 48.3** RMA+dev smoothing+DF 32.8** 48.2** Table 3: Results for variants of RM adaptation. system Chinese Arabic LM+TM adaptation 33.2 47.7 +RMA+dev-smoothing+DF 33.5 48.4** Table 4: RM adaptation improves over a baseline containing adapted LMs and TMs. tem was tuned with batch lattice MIRA (Cherry and Foster, 2012). 4.3 Results For our main baseline, we simply concatenate all training data. We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs. Our metric is case-insensitvie IBM BLEU-4 (Papineni et al., 2002); we report BLEU scores averaged across both test sets. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p &lt; 0.05 and p &lt; 0.01 levels, respectively. Table 3 shows that reordering model adaptation helps in both data settings. Adding either documentfrequency weighting (equation 4) or dev-set smoothing makes the improvement significant in both settings. Using both techniques together yields highly significant improvements. Our second experiment measures the improvement from RM adaptat</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron B Phillips</author>
<author>Ralf D Brown</author>
</authors>
<title>Training machine translation with a second-order taylor approximation of weighted translation instances.</title>
<date>2011</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="28267" citStr="Phillips and Brown, 2011" startWordPosition="4752" endWordPosition="4755">re then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on </context>
</contexts>
<marker>Phillips, Brown, 2011</marker>
<rawString>Aaron B. Phillips and Ralf D. Brown. 2011. Training machine translation with a second-order taylor approximation of weighted translation instances. In MT Summit 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on largescale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In IWSLT</booktitle>
<contexts>
<context position="27739" citStr="Schwenk, 2008" startWordPosition="4662" endWordPosition="4663"> that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the tr</context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In EACL</booktitle>
<contexts>
<context position="28284" citStr="Sennrich, 2012" startWordPosition="4756" endWordPosition="4757"> training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on either TM or LM d</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine translation. In EACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A localized prediction model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<publisher>ACL.</publisher>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="6165" citStr="Tillmann and Zhang, 2005" startWordPosition="990" endWordPosition="993">a sentence is translated were modeled by means of a penalty that is incurred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a feature. However, starting with (Tillmann and Zhang, 2005; Koehn et al., 2005), a more sophisticated type of reordering model has often been adopted as well, and has yielded consistent performance gains. This type of RM typically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.&apos; This type of RM is lex</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A localized prediction model for statistical machine translation. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL), Ann Arbor, Michigan, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="27705" citStr="Ueffing et al., 2007" startWordPosition="4653" endWordPosition="4657"> (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature s</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING)</booktitle>
<location>Geneva,</location>
<contexts>
<context position="27815" citStr="Zhao et al., 2004" startWordPosition="4671" endWordPosition="4674">instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence </context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the International Conference on Computational Linguistics (COLING) 2004, Geneva, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>