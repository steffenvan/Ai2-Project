<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003586">
<title confidence="0.988681">
ISTI@SemEval-2 Task #8:
Boosting-Based Multiway Relation Classification
</title>
<author confidence="0.949816">
Andrea Esuli, Diego Marcheggiani, Fabrizio Sebastiani
</author>
<affiliation confidence="0.583757">
Istituto di Scienza e Tecnologie dell’Informazione
</affiliation>
<address confidence="0.437167">
Consiglio Nazionale delle Ricerche
56124 Pisa, Italy
</address>
<email confidence="0.994975">
firstname.lastname@isti.cnr.it
</email>
<sectionHeader confidence="0.993766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907285714286">
We describe a boosting-based supervised
learning approach to the “Multi-Way Clas-
sification of Semantic Relations between
Pairs of Nominals” task #8 of SemEval-
2. Participants were asked to determine
which relation, from a set of nine relations
plus “Other”, exists between two nomi-
nals, and also to determine the roles of the
two nominals in the relation.
Our participation has focused, rather than
on the choice of a rich set of features,
on the classification model adopted to de-
termine the correct assignment of relation
and roles.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999558">
The “Multi-Way Classification of Semantic Rela-
tions between Pairs of Nominals” (Hendrickx et
al., 2010) we faced can be seen as the composition
of two sub-tasks:
</bodyText>
<listItem confidence="0.884298833333333">
1. Determining which relation r, from a set of
relations R (see Table 1), exists between two
entities e1 and e2.
2. Determining the direction of the relation, i.e.,
determining which of r(e1, e2) or r(e2, e1)
holds.
</listItem>
<bodyText confidence="0.9998317">
The set R is composed by nine “semantically
determined” relations, plus a special Other rela-
tion which includes all the pairs which do not be-
long to any of the nine previously mentioned rela-
tions.
The two novel aspects of this task with respect to
the similar task # 4 of SemEval-2007 (Girju et al.,
2007) (“Classification of Semantic Relations be-
tween Nominals”) are (i) the definition of the task
as a “single-label” classification task and (ii) the
</bodyText>
<figure confidence="0.991038666666667">
1 Cause-Effect
2 Instrument-Agency
3 Product-Producer
4 Content-Container
5 Entity-Origin
6 Entity-Destination
7 Component-Whole
8 Member-Collection
9 Message-Topic
</figure>
<tableCaption confidence="0.985426">
Table 1: The nine relations defined for the task.
</tableCaption>
<bodyText confidence="0.99919225">
need of determining the direction of the relation
(i.e., Item 2 above).
The classification task described can be formal-
ized as a single-label (aka “multiclass”) text clas-
sification (SLTC) task, i.e., as one in which exactly
one class must be picked for a given object out of
a set of m available classes.
Given a set of objects D (ordered pairs of nom-
inals, in our case) and a predefined set of classes
(aka labels, or categories) C = {c1, ... , cr.},
SLTC can be defined as the task of estimating
an unknown target function Φ : D → C, that
describes how objects ought to be classified, by
means of a function Φˆ : D → C called the classi-
fier1.
In the relation classification task which is the
object of this evaluation, the set C of classes is
composed of 19 elements, i.e., the nine relations
of Table 1, each one considered twice because it
may take two possible directions, plus Other.
</bodyText>
<sectionHeader confidence="0.849878" genericHeader="method">
2 The learner
</sectionHeader>
<bodyText confidence="0.999242142857143">
As the learner for our experiments we have used a
boosting-based learner called MP-BOOST (Esuli
et al., 2006). Boosting is among the classes of su-
pervised learning devices that have obtained the
best performance in several learning tasks and,
at the same time, have strong justifications from
computational learning theory. MP-BOOST is a
</bodyText>
<footnote confidence="0.9986505">
1Consistently with most mathematical literature we use
the caret symbol (ˆ) to indicate estimation.
</footnote>
<page confidence="0.892364">
218
</page>
<bodyText confidence="0.976161435897436">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 218–221,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
variant of ADABOOST.MH (Schapire and Singer,
2000), which has been shown in (Esuli et al.,
2006) to obtain considerable effectiveness im-
provements with respect to ADABOOST.MH.
MP-BOOST works by iteratively generating, for
each class cj, a sequence ˆΦj1, ... , ˆΦjS of classifiers
(called weak hypotheses). A weak hypothesis is a
function ˆΦjs : D —* R, where D is the set of doc-
uments and R is the set of real numbers. The sign
of ˆΦjs(di) (denoted by sgn(ˆΦj s(di))) represents the
binary decision of ˆΦj s on whether di belongs to cj,
i.e. sgn(ˆΦj s(di)) = +1 (resp., −1) means that di is
believed to belong (resp., not to belong) to cj. The
absolute value of ˆΦj s(di) (denoted by |ˆΦjs(di)|)
represents instead the confidence that ˆΦjs has in
this decision, with higher values indicating higher
confidence.
At each iteration s MP-BOOST tests the effec-
tiveness of the most recently generated weak hy-
pothesis ˆΦjs on the training set, and uses the results
to update a distribution Djs of weights on the train-
ing examples. The initial distribution Dj1 is uni-
form by default. At each iteration s all the weights
Djs(di) are updated, yielding Djs+1(di), so that the
weight assigned to an example correctly (resp., in-
correctly) classified by ˆΦj s is decreased (resp., in-
creased). The weight Djs+1(di) is thus meant to
capture how ineffective ˆΦj1, ... ,ˆΦjs have been in
guessing the correct cj-assignment of di (denoted
by Φj(di)), i.e., in guessing whether training doc-
ument di belongs to class cj or not. By using this
distribution, MP-BOOST generates a new weak
hypothesisˆΦjs+1 that concentrates on the exam-
ples with the highest weights, i.e. those that had
proven harder to classify for the previous weak hy-
potheses.
The overall prediction on whether di belongs to
</bodyText>
<equation confidence="0.445525333333333">
”&lt;e1&gt;People&lt;/e1&gt; have been moving back into
&lt;e2&gt;downtown&lt;/e2&gt;.”
Entity-Destination(e1,e2)
</equation>
<tableCaption confidence="0.869135">
Table 2: A training sentence and the features ex-
tracted from it.
</tableCaption>
<bodyText confidence="0.9983304">
all. In order to obtain a single-label classifier,
we compare the outcome of the |C |binary clas-
sifiers, and the class which has obtained the high-
est ˆΦj(di) value is assigned to di, i.e., ˆΦ(di) =
arg maxjˆΦj(di).
</bodyText>
<sectionHeader confidence="0.992804" genericHeader="method">
3 Vectorial representation
</sectionHeader>
<bodyText confidence="0.998920466666667">
We have generated the vectorial representations of
the training and test objects by extracting a number
of contextual features from the text surrounding
the two nominals whose relation is to be identified.
An important choice we have made is to “nor-
malize” the representation of the two nominals
with respect to the order in which they appear in
the relation, and not in the sentence. Thus, if e2
appears in a relation r(e2, e1), then e2 is consid-
ered to be the first (F) entity in the feature genera-
tion process and e1 is the second (S) entity.
We have generated a number of features for
each term denoting an entity and also for the three
terms preceding each nominal (P1, P2, P3) and for
the three terms following it (S1, S2, S3):
</bodyText>
<table confidence="0.942498666666667">
F People FS Peopl FH group FP NNP
FS1 have FS1S have FS1H have FS1P VBP
FS2 been FS2S been FS2H be FS2P VBN
FP3 moving FP3S move FP3H travel FP3P VBG
SP3 moving SP3S move SP3H travel SP3P VBG
SP2 back SP2S back SP2H O SP2P RB
SP1 into SP1S into SP1H O SP1P IN
S downtown SS downtown SH city district SP NN
SS1 . SS1S . SS1H O SS1P .
</table>
<bodyText confidence="0.981459416666667">
cj is obtained as a sum ˆΦj(di) = ESs=1 ˆΦj s(di) of T : the term itself;
the predictions made by the weak hypotheses. The
final classifier ˆΦj is thus a committee of 5 clas-
sifiers, a committee whose 5 members each cast
a weighted vote (the vote being the binary deci-
sion sgn(ˆΦjs(di)), the weight being the confidence
|ˆΦj s(di)|) on whether di belongs to cj. For the final
classifier ˆΦj too, sgn(ˆΦj(di)) represents the bi-
nary decision as to whether di belongs to cj, while
|ˆΦj(di) |represents the confidence in this decision.
MP-BOOST produces a multi-label classifier,
i.e., a classifier which independently classifies a
document against each class, possibly assigning
a document to multiple classes or no class at
S : the stemmed version of the term, obtained
using a Porter stemmer;
P : the POS of the term, obtained using the Brill
Tagger;
H : the hypernym of the term, taken from Word-
Net (“O” if not available).
Features are prefixed with a proper composition
of the above labels in order to identify their role
in the sentence. Table 2 illustrates a sentence from
the training set and its extracted features.
</bodyText>
<page confidence="0.995202">
219
</page>
<bodyText confidence="0.999040333333333">
If an entity is composed by k &gt; 1 terms, entity-
specific features are generated for all the term n-
grams contained in the entity, for all n E [1, ..., k].
E.g., for “phone call” features are generated for
the n-grams: “phone”, “call”, “phone call”.
In all the experiments described in this paper,
MP-BOOST has been run for S = 1000 iterations.
No feature weighting has been performed, since
MP-BOOST requires binary input only.
</bodyText>
<sectionHeader confidence="0.997687" genericHeader="method">
4 Classification model
</sectionHeader>
<bodyText confidence="0.999964">
The classification model we adopted in our exper-
iments splits the two tasks of recognizing the rela-
tion type and the one of determining the direction
of the relation in two well distinct phases.
</bodyText>
<subsectionHeader confidence="0.986878">
4.1 Relation type determination
</subsectionHeader>
<bodyText confidence="0.99694475">
Given the training set Tr of all the sentences for
which the classifier outcome is known, vectorial
representations (see Section 3) are built in a way
that “normalizes” the direction of the relation, i.e.:
</bodyText>
<listItem confidence="0.964419">
• if the training object belongs to one of the
nine relevant relations, the features extracted
from the documents are given proper identi-
fiers in order to mark their role in the relation,
not the order of appearance in the sentence;
• if the training object belongs to Other the
two distinct vectorial representations are gen-
erated, one for relation Other(e1, e2) and one
for Other(e2, e1).
</listItem>
<bodyText confidence="0.999929">
The produced training set has thus a larger num-
ber of examples than the one actually provided.
The training set provided for the task yielded 9410
training examples from the original 8000 sen-
tences. A 10-way classifier is then trained on the
vectorial representation.
</bodyText>
<subsectionHeader confidence="0.98068">
4.2 Relation direction determination
</subsectionHeader>
<bodyText confidence="0.998063076923077">
The 10-way classifier is thus able to assign a rela-
tion, or the Other relation, to a sentence, but not to
return the direction of the relation. The direction
of the relation is determined at test time, by classi-
fying two instances of each test sentence, and then
combining the outcome of the two classifications
in order to produce the final classification result.
More formally, given a test sentence d belong-
ing to an unknown relation r, two vectorial repre-
sentations are built: one, d1,2, under the hypoth-
esis that r(e1, e2) holds, and one, d2,1, under the
hypothesis that r(e2, e1) holds.
Both d1,2 and d2,1 are classified by ˆΦ:
</bodyText>
<listItem confidence="0.913579769230769">
• if both classifications return Other, then d is
assigned to Other;
• if one classification returns Other and the
other returns a relation r, then r, with the
proper direction determined by which vec-
torial representation determined the assign-
ment, is assigned to d;
• if the two classifications return two relations
r1,2 and r2,1 different from Other (of the
same or of different relation type), then the
one that obtains the highest Φˆ value deter-
mines the relation and the direction to be as-
signed to d.
</listItem>
<sectionHeader confidence="0.997404" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999975">
We have produced two official runs.
The ISTI-2 run uses the learner, vectorial rep-
resentation, and classification model described in
the previous sections.
The ISTI-1 run uses the same configuration of
ISTI-2, with the only difference being how the
initial distribution Dj1 of the boosting method is
defined. Concerning this, we followed the ob-
servations of (Schapire et al., 1998, Section 3.2)
on boosting with general utility functions; the ini-
tial distribution in the ISTI-1 run is thus set to be
equidistributed between the portion Tr+j of pos-
itive examples of the training set and the portion
Tr−j of negative examples, for each class j, i.e.,
</bodyText>
<equation confidence="0.9915135">
1
Dj 1(di) = + iff di E Tri (1)
2|Trj
1
Dj 1(di) = iff di E Tri (2)
2|Tr−j
</equation>
<bodyText confidence="0.999945615384615">
This choice of initial distribution, which gives
more relevance to the less frequent type of ele-
ments of the training set (namely, the positive ex-
amples), is meant to improve the performance on
highly imbalanced classes, thus improving effec-
tiveness at the the macro-averaged level.
We have also defined a third method for an addi-
tional run, ISTI-3; unfortunately we were not able
to produce it in time, and there is thus no offi-
cial evaluation for this run on the test data. The
method upon which the ISTI-3 run is based re-
lies on a more “traditional” approach to the clas-
sification task, i.e., a single-label classifier trained
</bodyText>
<page confidence="0.989753">
220
</page>
<table confidence="0.999407285714286">
Run 7rµ pµ Fµ 7rM pM FM
1 1
Official results ISTI-1 72.01% 67.08% 69.46% 71.12% 66.24% 68.42%
ISTI-2 73.55% 63.54% 68.18% 72.38% 62.34% 66.65%
ISTI-1 73.60% 69.34% 71.41% 72.44% 68.17% 69.95%
10-fold cross-validation ISTI-2 75.34% 65.92% 70.32% 73.96% 64.65% 68.52%
ISTI-3 68.52% 61.58% 64.86% 66.19% 59.75% 62.31%
</table>
<tableCaption confidence="0.909499666666667">
Table 3: Official results (upper part), and results of the three relation classification methods when used in
a 10-fold cross-validation experiment on training data (lower part). Precision, recall, and F1 are reported
as percentages for more convenience.
</tableCaption>
<bodyText confidence="0.999959090909091">
on the nine relations plus Other, not considering
the direction, coupled with nine binary classifiers
trained to determined the direction of each rela-
tion. We consider this configuration as a reason-
able baseline to evaluate the impact of the original
classification model adopted in the other two runs.
Table 3 summarizes the experimental results.
The upper part of the table reports the official re-
sults for the two official runs. The lower part
reports the results obtained by the three rela-
tion classification methods when used in a 10-
fold cross-validation experiment on the training
data. The evaluation measures are precison (7r),
recall (p), and the F1 score, computed both in
a microaveraged (*µ) and a macroaveraged
(*M) way (Yang, 1999).
The results for ISTI-1 and ISTI-2 in the 10-fold
validation experiment are similar both in trend and
in absolute value to the official results, allowing
us to consider the ISTI-3 results in the 10-fold
validation experiment as a good prediction of the
efficacy of the ISTI-3 method on the test data.
The classification model of ISTI-2, which uses
an initial uniform distribution for the MP-BOOST
learner as ISTI-3, improves F1M over ISTI-3 by
9.97%, and F1µ by 8.42%.
The use of a F1-customized distribution in ISTI-
1 results in a F1 improvement with respect to
ISTI-2 (F1M improves by 2.66% in official re-
sults, 2.09% in 10-fold validation results), which
is mainly due to a relevant improvement in recall.
Comparing ISTI-1 with ISTI-3 the total im-
provement is 12.26% for F1M and 10.10% for F1µ .
</bodyText>
<sectionHeader confidence="0.996424" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999979846153846">
The original relation classification model we have
adopted has produced a relevant improvement in
efficacy with respect to a “traditional” approach.
We have not focused on the development of a
rich set of features. In the future we would like to
apply our classification model to the vectorial rep-
resentations generated by the other participants, in
order to evaluate the distinct contributions of the
feature set and the classification model.
The use of a F1-customized initial distribution
for the MP-BOOST learner has also produced a
relevant improvement, and it will be further inves-
tigated on more traditional text classification tasks.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999820303030303">
Andrea Esuli, Tiziano Fagni, and Fabrizio Sebastiani.
2006. MP-Boost: A multiple-pivot boosting al-
gorithm and its application to text categorization.
In Proceedings of the 13th International Sympo-
sium on String Processing and Information Retrieval
(SPIRE’06), pages 1–12, Glasgow, UK.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 13–18, Prague, CZ.
Association for Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, Upp-
sala, Sweden.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135–168.
Robert E. Schapire, Yoram Singer, and Amit Singhal.
1998. Boosting and rocchio applied to text filtering.
In SIGIR ’98: Proceedings of the 21st annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 215–
223, New York, NY, USA. ACM.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. Information Re-
trieval, 1(1/2):69–90.
</reference>
<page confidence="0.998331">
221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591168">
<title confidence="0.9166295">ISTI@SemEval-2 Task #8: Boosting-Based Multiway Relation Classification</title>
<author confidence="0.996326">Andrea Esuli</author>
<author confidence="0.996326">Diego Marcheggiani</author>
<author confidence="0.996326">Fabrizio Sebastiani</author>
<affiliation confidence="0.8211815">Istituto di Scienza e Tecnologie dell’Informazione Consiglio Nazionale delle Ricerche</affiliation>
<address confidence="0.99971">56124 Pisa, Italy</address>
<email confidence="0.999382">firstname.lastname@isti.cnr.it</email>
<abstract confidence="0.999140466666667">We describe a boosting-based supervised learning approach to the “Multi-Way Classification of Semantic Relations between Pairs of Nominals” task #8 of SemEval- 2. Participants were asked to determine which relation, from a set of nine relations exists between two nominals, and also to determine the roles of the two nominals in the relation. Our participation has focused, rather than on the choice of a rich set of features, on the classification model adopted to determine the correct assignment of relation and roles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Tiziano Fagni</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>MP-Boost: A multiple-pivot boosting algorithm and its application to text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 13th International Symposium on String Processing and Information Retrieval (SPIRE’06),</booktitle>
<pages>1--12</pages>
<location>Glasgow, UK.</location>
<contexts>
<context position="2877" citStr="Esuli et al., 2006" startWordPosition="468" endWordPosition="471"> classes (aka labels, or categories) C = {c1, ... , cr.}, SLTC can be defined as the task of estimating an unknown target function Φ : D → C, that describes how objects ought to be classified, by means of a function Φˆ : D → C called the classifier1. In the relation classification task which is the object of this evaluation, the set C of classes is composed of 19 elements, i.e., the nine relations of Table 1, each one considered twice because it may take two possible directions, plus Other. 2 The learner As the learner for our experiments we have used a boosting-based learner called MP-BOOST (Esuli et al., 2006). Boosting is among the classes of supervised learning devices that have obtained the best performance in several learning tasks and, at the same time, have strong justifications from computational learning theory. MP-BOOST is a 1Consistently with most mathematical literature we use the caret symbol (ˆ) to indicate estimation. 218 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 218–221, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics variant of ADABOOST.MH (Schapire and Singer, 2000), which has been shown in (Esuli et al.</context>
</contexts>
<marker>Esuli, Fagni, Sebastiani, 2006</marker>
<rawString>Andrea Esuli, Tiziano Fagni, and Fabrizio Sebastiani. 2006. MP-Boost: A multiple-pivot boosting algorithm and its application to text categorization. In Proceedings of the 13th International Symposium on String Processing and Information Retrieval (SPIRE’06), pages 1–12, Glasgow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, CZ.</location>
<contexts>
<context position="1502" citStr="Girju et al., 2007" startWordPosition="233" endWordPosition="236">s of Nominals” (Hendrickx et al., 2010) we faced can be seen as the composition of two sub-tasks: 1. Determining which relation r, from a set of relations R (see Table 1), exists between two entities e1 and e2. 2. Determining the direction of the relation, i.e., determining which of r(e1, e2) or r(e2, e1) holds. The set R is composed by nine “semantically determined” relations, plus a special Other relation which includes all the pairs which do not belong to any of the nine previously mentioned relations. The two novel aspects of this task with respect to the similar task # 4 of SemEval-2007 (Girju et al., 2007) (“Classification of Semantic Relations between Nominals”) are (i) the definition of the task as a “single-label” classification task and (ii) the 1 Cause-Effect 2 Instrument-Agency 3 Product-Producer 4 Content-Container 5 Entity-Origin 6 Entity-Destination 7 Component-Whole 8 Member-Collection 9 Message-Topic Table 1: The nine relations defined for the task. need of determining the direction of the relation (i.e., Item 2 above). The classification task described can be formalized as a single-label (aka “multiclass”) text classification (SLTC) task, i.e., as one in which exactly one class must</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 13–18, Prague, CZ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation,</booktitle>
<location>Uppsala,</location>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="3438" citStr="Schapire and Singer, 2000" startWordPosition="548" endWordPosition="551">sed a boosting-based learner called MP-BOOST (Esuli et al., 2006). Boosting is among the classes of supervised learning devices that have obtained the best performance in several learning tasks and, at the same time, have strong justifications from computational learning theory. MP-BOOST is a 1Consistently with most mathematical literature we use the caret symbol (ˆ) to indicate estimation. 218 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 218–221, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics variant of ADABOOST.MH (Schapire and Singer, 2000), which has been shown in (Esuli et al., 2006) to obtain considerable effectiveness improvements with respect to ADABOOST.MH. MP-BOOST works by iteratively generating, for each class cj, a sequence ˆΦj1, ... , ˆΦjS of classifiers (called weak hypotheses). A weak hypothesis is a function ˆΦjs : D —* R, where D is the set of documents and R is the set of real numbers. The sign of ˆΦjs(di) (denoted by sgn(ˆΦj s(di))) represents the binary decision of ˆΦj s on whether di belongs to cj, i.e. sgn(ˆΦj s(di)) = +1 (resp., −1) means that di is believed to belong (resp., not to belong) to cj. The absolu</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
<author>Amit Singhal</author>
</authors>
<title>Boosting and rocchio applied to text filtering.</title>
<date>1998</date>
<booktitle>In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>215--223</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10859" citStr="Schapire et al., 1998" startWordPosition="1830" endWordPosition="1833">classifications return two relations r1,2 and r2,1 different from Other (of the same or of different relation type), then the one that obtains the highest Φˆ value determines the relation and the direction to be assigned to d. 5 Experiments We have produced two official runs. The ISTI-2 run uses the learner, vectorial representation, and classification model described in the previous sections. The ISTI-1 run uses the same configuration of ISTI-2, with the only difference being how the initial distribution Dj1 of the boosting method is defined. Concerning this, we followed the observations of (Schapire et al., 1998, Section 3.2) on boosting with general utility functions; the initial distribution in the ISTI-1 run is thus set to be equidistributed between the portion Tr+j of positive examples of the training set and the portion Tr−j of negative examples, for each class j, i.e., 1 Dj 1(di) = + iff di E Tri (1) 2|Trj 1 Dj 1(di) = iff di E Tri (2) 2|Tr−j This choice of initial distribution, which gives more relevance to the less frequent type of elements of the training set (namely, the positive examples), is meant to improve the performance on highly imbalanced classes, thus improving effectiveness at the</context>
</contexts>
<marker>Schapire, Singer, Singhal, 1998</marker>
<rawString>Robert E. Schapire, Yoram Singer, and Amit Singhal. 1998. Boosting and rocchio applied to text filtering. In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 215– 223, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization. Information Retrieval,</title>
<date>1999</date>
<pages>1--1</pages>
<contexts>
<context position="13154" citStr="Yang, 1999" startWordPosition="2215" endWordPosition="2216">n of each relation. We consider this configuration as a reasonable baseline to evaluate the impact of the original classification model adopted in the other two runs. Table 3 summarizes the experimental results. The upper part of the table reports the official results for the two official runs. The lower part reports the results obtained by the three relation classification methods when used in a 10- fold cross-validation experiment on the training data. The evaluation measures are precison (7r), recall (p), and the F1 score, computed both in a microaveraged (*µ) and a macroaveraged (*M) way (Yang, 1999). The results for ISTI-1 and ISTI-2 in the 10-fold validation experiment are similar both in trend and in absolute value to the official results, allowing us to consider the ISTI-3 results in the 10-fold validation experiment as a good prediction of the efficacy of the ISTI-3 method on the test data. The classification model of ISTI-2, which uses an initial uniform distribution for the MP-BOOST learner as ISTI-3, improves F1M over ISTI-3 by 9.97%, and F1µ by 8.42%. The use of a F1-customized distribution in ISTI1 results in a F1 improvement with respect to ISTI-2 (F1M improves by 2.66% in offi</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yiming Yang. 1999. An evaluation of statistical approaches to text categorization. Information Retrieval, 1(1/2):69–90.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>