<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.9988015">
Introduction to the Special Issue on
Natural Language Generation
</title>
<author confidence="0.999948">
Robert Dale* Barbara Di Eugeniot
</author>
<affiliation confidence="0.99974">
Macquarie University University of Pittsburgh
</affiliation>
<author confidence="0.982478">
Donia Scottt
</author>
<affiliation confidence="0.997778">
University of Brighton
</affiliation>
<sectionHeader confidence="0.993056" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9999764">
There are two sides to natural language processing. On the one hand, work in natural
language understanding is concerned with the mapping from some surface represen-
tation of linguistic material—expressed as speech or text—to an underlying repre-
sentation of the meaning carried by that surface representation. But there is also the
question of how one maps from some underlying representation of meaning into text
or speech: this is the domain of natural language generation.
Whether our end-goal is the construction of artifacts that use natural languages
intelligently, the formal characterization of phenomena in human languages, or the
computational modeling of the human language processing mechanism, we cannot
ignore the fact that language is both spoken (or written) and heard (or read). Both are
equally large and important problems, but the literature contains much less work on
natural language generation (NLG) than it does on natural language understanding
(NLU). There are many reasons why this might be so, although clearly an important
one is that researchers in natural language understanding in some sense start out with
a more well-defined task: the input is known, and there is a lot of it around. This is not
the case in natural language generation: there, it is the desired output that is known,
but the input is an unknown; and while the world is awash with text waiting to be
processed, there are fewer instances of what we might consider appropriate inputs for
the process of natural language generation. For researchers in the field, this highlights
the fundamental question that always has to be asked: What do we generate from?
Despite this problem, the natural language generation community is a thriving
one, with a research base that has been developing steadily—although perhaps at a
slower pace because of the smaller size of the community—for just as long as work
in natural language understanding. It should not be forgotten that much of NLP has
its origins in the early work on machine translation in the 1950s; and that to carry out
machine translation, one has to not only analyze existing texts but also to generate
new ones. The early machine translation experiments, however, did not recognize the
problems that give modern work in NLG its particular character. The first significant
pieces of work in the field appeared during the 1970s; in particular, Goldman&apos;s work
on the problem of lexicalizing underlying conceptual material (Goldman 1974) and
</bodyText>
<affiliation confidence="0.668192">
* School of Mathematics, Physics, Computing and Electronics, Sydney NSW 2109, Australia
f Learning Research and Development Center, 3939 O&apos;Hara Street, Pittsburgh, PA 15260, U.S.A.
$ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK
</affiliation>
<note confidence="0.8722525">
C) 1998 Association for Computational Linguistics
Computational Linguistics Volume 24, Number 3
</note>
<bodyText confidence="0.999793263157895">
Davey&apos;s work on the generation of paragraph-long descriptions of tic-tac-toe games
(Davey 1979) were among the first to focus on issues unique to NLG. The field really
took off, however, in the 1980s; for those working in NLG, the decade began with a
bang, and the Ph.D. theses of McDonald (1980), Appelt (1981), and McKeown (1982)
have had a lasting impact on the shape of the field.&apos;
But what has happened in the last fifteen years since those major pieces of work
first appeared? Although one does find articles on NLG in the pages of Computational
Linguistics and other journals in the field, and papers on generation do appear at the
major NLP conferences, the quantity and range of work being carried out in NLG
tends to be underrepresented in these forums. Instead, the community has tended
to present its results at the two biennial series of workshops—one European and one
international—that have sprung up in the last ten years. Many of these workshops have
led to books: see Kempen (1987); McDonald and Bolc (1988); Zock and Sabah (1988);
Dale, Mellish, and Zock (1990); Paris, Swartout, and Mann (1991); Dale et al. (1992);
Horacek and Zock (1993); and Adorni and Zock (1996). This special issue of Computa-
tional Linguistics was inspired by discussions at the International Workshop on Natural
Language Generation held in Herstmonceux in 1996; the aim of the volume you are
reading is to show the wider computational linguistics community something of the
range of activities in NLG.
</bodyText>
<sectionHeader confidence="0.80082" genericHeader="method">
2. Some Perspectives on Natural Language Generation
</sectionHeader>
<bodyText confidence="0.9804091875">
What is natural language generation about? A definition offered by McDonald (1987,
983) over ten years ago has stood the test of time:
Natural language generation is the process of deliberately constructing a natural
language text in order to meet specified communicative goals.
A more recent definition with a slightly different emphasis can be found in Reiter and
Dale (1997, 57):
Natural language generation is the subfield of artificial intelligence and
computational linguistics that is concerned with the construction of computer
systems that can produce understandable texts in ... human languages from
some underlying non-linguistic representation of information.
Both definitions pick out some of the foci of interest that give work in NLG its distinc-
tive flavor. From the first we note the emphasis on deliberate choice as the fundamental
operation that underlies much work in the area, and on the generation of texts as
opposed to single sentences; from the second, we note the emphasis on underlying
representations of information that may be nonlinguistic in nature. Each of these points
bears some elaboration:
</bodyText>
<listItem confidence="0.752466166666667">
• In work in NLG, a major concern is that of choosing between different
ways of doing things, as the same content can often be expressed in
many different ways. Although some of these choices may indeed be
arbitrary, there is a view in NLG that a great many are not, and the
choices between different ways to say things—different ways to structure
a text, different ways to refer to objects, the use of different syntactic
</listItem>
<footnote confidence="0.582165">
1 The latter two works are more widely available in revised form as Appelt (1985) and McKeown (1985).
</footnote>
<page confidence="0.997825">
346
</page>
<note confidence="0.705303">
Dale, Di Eugenio, and Scott Introduction
</note>
<bodyText confidence="0.968910333333333">
constructions, and of different words to realize underlying
concepts—need to be motivated in some way. Much research in NLG is
oriented towards uncovering those motivations.
</bodyText>
<listItem confidence="0.888492222222222">
• There is a sense in which work in NLU tends to start with the sentence
as the principal focus of inquiry However, for much work in NLG, the
primary focus is the text or discourse: although there are many important
issues involved in the generation of sentential forms, those working in
NLG research have long accepted that discourse-level issues are just as
important, and probably more so. This relates to the previous point: it is
often only by considering the context within which a sentence is being
generated that the appropriate choice of surface form can be made.
• The input representation provided to an NLG system may be symbolic
(for example, an expert system knowledge base) or numeric (for
example, a database containing stock market prices) but it is generally
nonlinguistic in nature. Early work in the field relied on the use of
hand-crafted knowledge sources, which sometimes meant that the
representations used embodied unspoken assumptions. More recent
work has been able to take advantage of representations created for other
purposes; using these as the input to the generation process reinforces
the realization that the elements of the underlying representation may
not correspond in a straightforward way to words and sentences.
</listItem>
<bodyText confidence="0.996396785714286">
Much work in NLG thus concerns itself with pragmatics and discourse-level consider-
ations. Interestingly, these too have been somewhat underrepresented in the standard
computational linguistics forums, where the bulk of the work carried out is often in
the area of well-specified and rigorous formal treatments of sentential phenomena.
There is an important point here that bears emphasizing: natural language gener-
ation is not the inverse of the process of parsing.&apos; Those working in NLG generally
break down the process of generating a text into a number of stages, and it is only
the last of these—generally referred to as surface realization—that corresponds to any-
thing like the inverse of parsing. If we want to seek the mirror-image of work in NLG
within research in natural language understanding, we have to consider the entire
analysis process, all the way through to plan recognition in multisentential discourses
or dialogues.
This is perhaps an appropriate place to review what the task of NLG is now
commonly seen to involve:
</bodyText>
<listItem confidence="0.955923333333333">
• First, there is the question of content determination: deciding what to
say. This impacts at both macro and micro levels. At the macro level,
researchers in NLG are concerned with how the content of a
</listItem>
<bodyText confidence="0.810707875">
multisentential text, or of a turn in a dialogue, can be determined. At a
micro level, researchers are concerned with how the content of
appropriate referring expressions can be worked out. In each case the
problem is how to select the right information from that which is
available; it is rarely appropriate to say everything we could say.
2 It should be noted, however, that there is a body of work that looks at the use of bidirectional
grammars, where a common declarative representation of grammatical knowledge is used both for
parsing and for realization; see, for example, Shieber et al. (1990).
</bodyText>
<page confidence="0.987581">
347
</page>
<note confidence="0.282786">
Computational Linguistics Volume 24, Number 3
</note>
<listItem confidence="0.998095142857143">
• There is also the question of text structure: texts are not just random
collections of sentences; they exhibit a structure that plays a key role in
conveying their meaning. Researchers in NLG are concerned with
elucidating mechanisms for determining the most appropriate structures
to use in particular circumstances, and with working out how the
information to be conveyed can best be packaged into paragraph- and
sentence-sized chunks.
• Closer to the kinds of issues that concern those working in parsing, there
are the problems of surface realization and lexicalization: once the
content of individual sentences has been determined, this still has to be
mapped into morphologically and grammatically well-formed words and
sentences. Where the underlying representation expresses informational
elements at a granularity that does not map easily into words, decisions
about how to lexicalize the conceptual material have to be taken.
</listItem>
<bodyText confidence="0.999981333333333">
These are the kinds of issues that have driven much research in NLG over the
last 15 years. Our understanding of the issues has come a long way in that time. This
issue of Computational Linguistics contains what is no more than a snapshot of work in
the field at the current time; it should be read against the background of the broader
picture we have attempted to sketch here, albeit briefly. In the next section, we provide
short summaries of the papers collected together in this special issue.
</bodyText>
<sectionHeader confidence="0.517053" genericHeader="method">
3. An Overview of the Issue
</sectionHeader>
<bodyText confidence="0.9370495">
From the 25 papers originally submitted to the special issue, our reviewers helped
us eventually select five. There were many more papers that, given space, we would
have included; we hope that some of these will appear in subsequent regular issues
of Computational Linguistics.
</bodyText>
<subsectionHeader confidence="0.999516">
3.1 Chu-Carroll and Carberry
</subsectionHeader>
<bodyText confidence="0.999994">
As we mentioned earlier, &amp;quot;deciding what to say&amp;quot; is a key issue in NLG. Chu-Carroll
and Carberry&apos;s paper focuses on strategies for selecting the content of responses in
collaborative planning dialogues. The authors concentrate on situations in which the
system and the user have different beliefs that they attempt to reconcile, namely: cases
when the system needs to gather further information in order to decide whether to
accept a proposal from the user, and cases when the system must negotiate with the
user to resolve a detected conflict in beliefs. In both cases, the implemented algorithms
identify the subset of beliefs that the system believes will most effectively help solve
either its uncertainty or the conflict in beliefs; further, the system chooses an appro-
priate strategy and produces a response that initiates a subdialogue addressing the
impasse in conversation.
Two other points deserve special note. First, the computational model is based on
a small but convincing corpus study. Second, the authors conducted a formal, even
if limited, evaluation of their prototype implementation. The evaluation consists of
human raters grading the system&apos;s actual response and some distractors, obtained
by selectively altering the system&apos;s response generation strategies. The evaluation is
suggestive that the proposed strategies and their implementation are effective.
</bodyText>
<page confidence="0.991604">
348
</page>
<note confidence="0.756971">
Dale, Di Eugenio, and Scott Introduction
</note>
<subsectionHeader confidence="0.99966">
3.2 Stede
</subsectionHeader>
<bodyText confidence="0.999982733333333">
Addressing the issue of &amp;quot;deciding how to say it,&amp;quot; Stede focuses on the role of the
lexicon and of lexical choice within an NLG system. More specifically, Stede describes
how his approach can generate verbal alternations that change the aspectual category
of the verb. One such alternation is the causative, as in The mechanic drained the oil from
the engine, the causative form of The oil drained from the engine. Stede takes the lexicon to
be the central device for mapping between domain representations and intermediate
semantic representations. Alternations are generated by applying one or more rules
in a predetermined order to a basic lexical form; the choice of a specific alternation is
determined by parameters such as salience. The intermediate semantic representation
of a sentence is the input for the surface generator—in this case, Penman (Penman
group 1989).
Interesting aspects of Stede&apos;s approach are the capacity of his system to generate
fine-grained distinctions of meaning, and the attention paid to both linguistic con-
straints and computational concerns. Although only English is discussed in this paper,
Stede&apos;s system uses the same mechanism to generate alternations in German as well.
</bodyText>
<subsectionHeader confidence="0.990278">
3.3 Mittal, Moore, Carenini, and Roth
</subsectionHeader>
<bodyText confidence="0.9999904375">
Generation technology is now increasingly finding a place in applied systems. One
such application is described by Mittal, Moore, Carenini, and Roth, who have devel-
oped a system to generate captions to accompany the graphical presentations produced
by SAGE (Roth et al. 1994). It is well known that the interpretation of even simple, con-
ventional, graphics can be difficult without accompanying textual pointers (e.g., keys,
labels of axes, and the like). SAGE is innovative in its ability to produce novel graphics
for highly abstract and complex data. The comprehension of these presentations is
often heavily reliant on captions: extended textual descriptions of the relation of the
presentation to the data it depicts.
Mittal et al. show how a SAGE graphic, together with information about the per-
ceptual complexity of its elements and the structure of its underlying data, can be used
to generate an effective multisentential caption. This is demonstrated through exam-
ples in the domain of housing sales; however, with the exception of the lexicon, the
caption generator is fully domain independent. Although the system has not yet been
formally evaluated, we are told that users of SAGE report that the generated captions
contribute positively to their understanding of complex graphical presentations.
</bodyText>
<subsectionHeader confidence="0.998338">
3.4 Radev and McKeown
</subsectionHeader>
<bodyText confidence="0.999640428571429">
Automated text summarization is a practical problem of increasing interest, especially
with the ever-widening dissemination of the World Wide Web; this is an obvious area
where NLG can contribute. Radev and McKeown describe an application of NLG
techniques towards the end of producing summaries of a kind that are, as the au-
thors argue, beyond the scope of current statistical summarizers. There are two main
elements to their approach. The first is the use of &amp;quot;summarization operators&amp;quot; that com-
pare data structures containing information derived from different sources and thus
allow the system to produce summaries of several input messages; the second is the
use of a technique for identifying proper names and related descriptions from on-line
text so that these can be used to extend the descriptions provided in summaries.
The data structures used as input to generate the summary texts are filled MUC-
style templates; the task of identifying key information in source texts and extracting
it has already been carried out. The paper thus provides an excellent application of
established technologies, with new mechanisms being developed to complete the pic-
</bodyText>
<page confidence="0.993214">
349
</page>
<note confidence="0.630666">
Computational Linguistics Volume 24, Number 3
</note>
<bodyText confidence="0.9961585">
ture; the work shows well how NLG techniques can make a real difference to the
important task area of summarization.
</bodyText>
<subsectionHeader confidence="0.948919">
3.5 Oberlander
</subsectionHeader>
<bodyText confidence="0.99996424">
Texts are generated to be read, and while generators can provide a range of texts for a
given context, the question of what expression is most appropriate remains nontrivial.
Typically nowadays, the designers of NLG systems tune their generators to produce
expressions compatible with those found in a corpus of &amp;quot;good&amp;quot; exemplars from the
domain in question (see, e.g., Scott and Power [1994] and Paris et al. [1995]). But this
approach is not always possible (e.g., for novel domains), and even in cases where
there is an available corpus, judgments of quality can often only be made by appeal-
ing to convention. Psycholinguistic studies suggest themselves as a useful source of
guidance but this too can be problematic: what speakers or writers typically produce
often conflicts with their preferences as perceivers, as shown, for example, with re-
gard to referring expressions by Stevenson and her colleagues (Stevenson, Crawley,
and Kleinman 1994; Stevenson and Urbanowicz 1995).
Oberlander discusses this apparent paradox with reference to the generation of
referring expressions—in particular, the suggestion by Dale and Reiter (1995) that
generation algorithms for definite noun phrases should be based on observations about
human language production rather than on a strict observation of the Gricean maxims
(Grice 1975). Oberlander calls this the Spike Lee maxim: Do the right thing—where
&amp;quot;right&amp;quot; is that which is human and simple. He shows that, when generating referring
expressions, we can&apos;t always tell whether the right thing is to mimic the preferences
of language producers or language perceivers, since these preferences often conflict.
He argues that until we develop a more sophisticated view of the expectations of
speakers and hearers, developers of NLG systems should probably stick to the Spike
Lee maxim: even with its known limitations it produces more natural results than are
achieved by following a strict interpretation of the Gricean maxims—and we would
all agree that even that is better than the Cole Porter maxim.&apos;
</bodyText>
<sectionHeader confidence="0.902769" genericHeader="method">
4. Future Directions for Research in Natural Language Generation
</sectionHeader>
<bodyText confidence="0.999987666666667">
We said earlier that NLG research has come a long way since its beginnings, but there
is still a long way to go. What does the future hold? Crystal-ball gazing is always a
risky business, but on the basis of our experience and some of the issues that arise
both in the work presented here and in other submissions to the special issue, we
would suggest the following aspects of NLG will be seen as important areas in the
next five years.
Microplanning. Ever since Thompson (1977), there has been a tendency to see NLG
as involving two problems, which Thompson characterized as being concerned with
decisions of strategy and tactics: in short, questions about what to say and questions
about how to say it. In the field, this translated into work in the two areas of text
planning and linguistic realization, with researchers often declaring themselves as
working on one or the other. In more recent years, there has been the realization that
something is required in the middle; this was most notably expressed in Meteer&apos;s work
on what she called &amp;quot;the generation gap&amp;quot; (Meteer 1990). This has given rise to a body
of work that explores questions of what is often referred to as microplanning: once a
</bodyText>
<page confidence="0.778651">
3 This being: Anything goes.
350
</page>
<note confidence="0.845494">
Dale, Di Eugenio, and Scott Introduction
</note>
<bodyText confidence="0.999943187500001">
text planning process has worked out the overall structure of a text and the content
to be conveyed, how is this information packaged into sentences? Serious work here
has only just begun: there are a great many unresolved issues, and in many cases the
questions themselves are unclear.
Multimodal generation. Real text is not disembodied. It always appears in context, and
in particular within some medium—for example, on a page, on a screen, or in a
speech stream. As soon as we begin to consider the generation of text in context,
we immediately have to countenance issues of typography and orthography (for the
written form) and prosody (for the spoken form). These questions can rarely be dealt
with as afterthoughts. This is perhaps most obvious in the case of systems that generate
both text and graphics and attempt to combine these in sensible ways. We predict that
the World Wide Web will be a major factor in forcing some of the issues here: if systems
are to automatically generate the text on Web pages (see, for example, Milosavljevic
and Dale [1996]), then they also need to consider other elements of that container.
Reusable resources. It may be an indication of a maturing of some subareas of NLG
research that we are now in a position where there are reusable components for par-
ticular tasks. Specifically, three linguistic realization packages, FUF /SURGE (Elhadad
1993a, 1993b; Elhadad and Robin 1996), PENMAN! NIGEL (Penman group 1989), and
its descendant KPML / NIGEL (Bateman 1997), are widely used in the field. For any-
thing other than simple applications, it is now questionable whether it makes sense to
build a linguistic realization component from scratch. We may expect other kinds of
reusable components to be developed within the research community within the next
5-10 years; it is developments of this kind that signal significant progress, since being
able to reuse the work of others obviously has the potential to increase research pro-
ductivity. In related developments, there is a growing interest within the community
in defining a reference architecture for NLG; if successful, this is likely to stimulate
further research and development in NLG through the provision of a modular baseline
for development, comparison, and evaluation.
Evaluation. Although there have been attempts at the evaluation of NLG techniques
and systems in the past, formal evaluation has only recently come to the fore. For
example, systems have been evaluated by using human judges to assess the quality
of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue);
by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997);
by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &amp;quot;task
efficacy&amp;quot; measures (Young 1997). The major stumbling block for progress is determin-
ing what metrics and methods should be used: for example, how can the quality of
an output text be measured? Because of the different nature of the task, it is unlikely
that methods that have been used in NLU, such as the evaluation process adopted
in the Message Understanding Conferences, can be carried over to generation. Dale
and Mellish (1998) suggest that the NLG community could make progress by devis-
ing specific evaluation methods for NLG subtasks such as content determination, text
structuring, and realization; this &amp;quot;glass box&amp;quot; approach is likely to result in a clearer
understanding of how to evaluate NLG systems as a whole.
The particular foci we have just outlined are specific to work in NLG. However,
just as corpus-based methods have become very important in NLU research, we may
expect this to happen increasingly in work on NLG too. Raw or coded text has been
used by researchers to investigate strategies in a number of different areas of NLG, as
demonstrated in the papers by Radev and McKeown and by Chu-Carroll and Carberry
</bodyText>
<page confidence="0.995114">
351
</page>
<note confidence="0.638985">
Computational Linguistics Volume 24, Number 3
</note>
<bodyText confidence="0.999932470588235">
in this issue. Given the emphasis within NLG research on text-level issues, a major
bottleneck for work here is the encoding of corpora with semantic and discourse
structural features; see Di Eugenio, Moore, and Paolucci (1997). These are needed to
uncover plausible text-structuring and microplanning strategies, but annotating cor-
pora for such features will remain a laborious manual task at least for the foreseeable
future. This effort may be alleviated if sharable corpora become available through
the Discourse Resource Initiative (http: / / www.georgetown.edu / luperfoy / Discourse-
Treebank / dri-home.html).
With so many rich seams to mine, natural language generation has a promising
future. We mentioned at the outset that researchers in NLG face the unique problem of
deciding what to generate from: Yorick Wilks is credited with pointing out that, while
the problem of natural language understanding is somewhat like counting from one
to infinity, researchers in natural language generation face the problem of counting
from infinity to one. In order to make progress, researchers in NLG pick a reasonably
high number and get to work; as researchers in NLU climb the numerical ladder from
the other end, we can expect that some of the big numbers discovered in NLG will
prove to be of use in NLU too.
</bodyText>
<sectionHeader confidence="0.907705" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999856777777778">
We offer our grateful thanks to the body of
reviewers who did so much work in
helping us put this issue together. We also
acknowledge the many fruitful discussions
we have had with our colleagues, including
especially Giuseppe Carenini for sharing his
notes on evaluation in NLG and Ehud
Reiter for his observations on the state of
the field.
</bodyText>
<sectionHeader confidence="0.792467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.950102246153846">
Adorni, G. and M. Zock, editors. 1996.
Trends in Natural Language Generation.
Lecture Notes in Artificial Intelligence.
Springer-Verlag, Berlin.
Appelt, Douglas E. 1981. Planning Natural
Language Utterances to Satisfy Multiple
Goals. Ph.D. thesis, Stanford University
Stanford, CA. Available as SRI Technical
Note 259.
Appelt, Douglas E. 1985. Planning English
Sentences. Cambridge University Press,
Cambridge.
Bateman, John A. 1997. Enabling technology
for multilingual natural language
generation: The KPML development
environment. Natural Language
Engineering, 3:15-55.
Dale, Robert, Eduard H. Hovy, Dietmar
Rosner, and Oliviero Stock, editors. 1992.
Aspects of Automated Natural Language
Generation. Lecture Notes in Artificial
Intelligence. Springer-Verlag, Berlin.
Dale, Robert and Christopher S. Mellish.
1998. Towards evaluation in natural
language generation. In Proceedings of the
First International Conference on Language
Resources and Evaluation, Granada, Spain,
May 28-30.
Dale, Robert, Chris Mellish, and Michael
Zock, editors. 1990. Current Research in
Natural Language Generation. Academic
Press, New York.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233-263.
Davey, Anthony C. 1979. Discourse
Production. Edinburgh University Press,
Edinburgh.
Di Eugenio, Barbara, Johanna D. Moore,
and Massimo Paolucci. 1997. Learning
features that predict cue usage. In
Proceedings of the 35th Annual Meeting,
pages 80-87, Madrid, Spain. Association
for Computational Linguistics.
Elhadad, Michael. 1993a. FUF: The
universal unifier—user manual version
5.2. Technical Report CUCS-038-91,
Columbia University.
Elhadad, Michael. 1993b. Using
Argumentation to Control Lexical Choice: A
Unification-based Implementation. Ph.D.
thesis, Computer Science Department,
Columbia University.
Elhadad, Michael and Jacques Robin. 1996.
An overview of SURGE: A reusable
comprehensive syntactic realisation
component. In Proceedings of the 8th
International Workshop on Natural Language
Generation (Demos and Posters),
Herstmonceux, Sussex, UK, June.
Goldman, Neil M. 1974. Computer Generation
of Natural Language from a Deep Conceptual
Base. Ph.D. thesis, Stanford University.
</reference>
<page confidence="0.995316">
352
</page>
<note confidence="0.66080125">
Dale, Di Eugenio, and Scott Introduction
Available as Stanford AT Laboratory
Memo AIM-247 or CS Technical Report
CS-74-461.
</note>
<reference confidence="0.998750375">
Grice, H. P. 1975. Logic and Conversation.
In P. Cole and J. L. Morgan, editors,
Syntax and Semantics 3: Speech Acts.
Academic Press.
Horacek, H. and M. Zock, editors. 1993.
New Concepts in Natural Language
Generation. Pinter, London.
Kempen, Gerard, editor. 1987. Natural
Language Generation: New Results in
Artificial Intelligence, Psychology and
Linguistics. NATO ASI Series No. 135.
Martinus Nijhoff Publishers, Boston,
Dordrecht.
Lester, James C. and Bruce W. Porter. 1997.
Developing and empirically evaluating
robust explanation generators: The
KNIGHT experiments. Computational
Linguistics, 23(1):65-102.
McDonald, David D. 1980. Natural Language
Production as a Process of Decision Making
under Constraint. Ph.D. thesis, MIT,
Cambridge, MA.
McDonald, David D. 1987. Natural
language generation. In Stuart C. Shapiro,
editor, Encyclopedia of Artificial Intelligence.
John Wiley and Sons, pages 642-655.
McDonald, David D. and Leonard Bolc.
1988. Natural Language Generation Systems.
Springer-Verlag, New York, NY.
McKeown, Kathleen R. 1982. Generating
Natural Language Text in Response to
Questions About Database Structure. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA, May. Available as
Technical Report MS-CIS-82-05.
McKeown, Kathleen R. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Cambridge University Press,
Cambridge.
Meteer, Marie. 1990. The Generation Gap: The
Problem of Expressibility in Text Planning.
Ph.D. thesis, University of Massachusetts.
Milosavljevic, Maria and Robert Dale. 1996.
Strategies for comparison in encyclopedia
descriptions. In Proceedings of the Eighth
International Natural Language Generation
Workshop, pages 161-170, Herstmonceux,
Sussex, UK, June.
Paris, Cecile, Keith Vander Linden, Markus
Fischer, Anthony Hartley, Lyn Pemberton,
Richard Power, and Donia Scott. 1995. A
support tool for writing multilingual
instructions. In Proceedings of the Fourteenth
International Joint Conference on Artificial
Intelligence, pages 1398-1404.
Paris, Cecile L., William R. Swartout, and
William C. Mann, editors. 1991. Natural
Language Generation in Artificial Intelligence
and Computational Linguistics. Kluwer
Academic Publishers, Boston.
Penman group. 1989. Documentation of the
Penman Sentence Generation System.
USC Information Sciences Institute,
Marina del Rey, CA.
Reiter, Ehud and Robert Dale. 1997.
Building applied natural language
generation systems. Natural Language
Engineering, 3:57-87.
Robin, Jacques and Kathleen McKeown.
1996. Empirically designing and
evaluating a new revision-based model
for summary generation. Artificial
Intelligence, 85:135-179.
Roth, Steven F., John Kolojejchick, Joe
Mattis, and Jade Goldstein. 1994.
Interactive graphic design using
automatic presentation knowledge. In
Proceedings of CHI94: Human Factors in
Computing Systems, pages 193-200.
Scott, Donia and Richard Power, editors.
1994. Characteristics of administrative
forms in English, German and Italian.
Deliverable EV-1, GIST project LRE
062-09. http:/ / ecate.itc.it:1024 /projects /
gist/ gist-bibliography.html.
Shieber, Stuart, Gertjan van Noord,
Fernando Pereira, and Robert Moore.
1990. Semantic head-driven generation.
Computational Linguistics, 16(1):30-42.
Stevenson, Rosemary, Rosalind Crawley,
and David Kleinman. 1994. Thematic
roles, focus and the representation of
events. Language and Cognitive Processes,
9:519-548.
Stevenson, Rosemary and Agnieszka
Urbanowicz. 1995. Structural focusing,
thematic role focusing and the
comprehension of pronouns. In
Proceedings of the Seventeenth Annual
Conference of the Cognive Science Society,
pages 328-332.
Thompson, Henry S. 1977. Strategy and
tactics: A model for language production.
In W. A. Beach, S. E. Fox, and
S. Philosoph, editors, Papers from the 13th
Regional Meeting of the Chicago Linguistics
Society, pages 651-668, Chicago, IL.
Yeh, Ching-Long and Chris Mellish. 1997.
An empirical study of the generation of
anaphora in Chinese. Computational
Linguistics, 23(1):169-190.
Young, R. Michael. 1997. Generating
Descriptions of Complex Activities. Ph.D.
thesis, Intelligent Systems Program,
University of Pittsburgh.
Zock, Michael and Gerard Sabah, editors.
1988. Advances in Natural Language
Generation: An Interdisciplinary Perspective.
Ablex Publishing Corp., Norwood, NJ.
</reference>
<page confidence="0.999354">
353
</page>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Adorni</author>
<author>M Zock</author>
<author>editors</author>
</authors>
<date>1996</date>
<booktitle>Trends in Natural Language Generation. Lecture Notes in Artificial Intelligence.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Adorni, Zock, editors, 1996</marker>
<rawString>Adorni, G. and M. Zock, editors. 1996. Trends in Natural Language Generation. Lecture Notes in Artificial Intelligence. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning Natural Language Utterances to Satisfy Multiple Goals.</title>
<date>1981</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University Stanford, CA.</institution>
<note>Available as SRI Technical Note 259.</note>
<contexts>
<context position="3352" citStr="Appelt (1981)" startWordPosition="525" endWordPosition="526">ronics, Sydney NSW 2109, Australia f Learning Research and Development Center, 3939 O&apos;Hara Street, Pittsburgh, PA 15260, U.S.A. $ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 3 Davey&apos;s work on the generation of paragraph-long descriptions of tic-tac-toe games (Davey 1979) were among the first to focus on issues unique to NLG. The field really took off, however, in the 1980s; for those working in NLG, the decade began with a bang, and the Ph.D. theses of McDonald (1980), Appelt (1981), and McKeown (1982) have had a lasting impact on the shape of the field.&apos; But what has happened in the last fifteen years since those major pieces of work first appeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work being carried out in NLG tends to be underrepresented in these forums. Instead, the community has tended to present its results at the two biennial series of workshops—one European and one international—that have sprun</context>
</contexts>
<marker>Appelt, 1981</marker>
<rawString>Appelt, Douglas E. 1981. Planning Natural Language Utterances to Satisfy Multiple Goals. Ph.D. thesis, Stanford University Stanford, CA. Available as SRI Technical Note 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="6218" citStr="Appelt (1985)" startWordPosition="994" endWordPosition="995">esentations of information that may be nonlinguistic in nature. Each of these points bears some elaboration: • In work in NLG, a major concern is that of choosing between different ways of doing things, as the same content can often be expressed in many different ways. Although some of these choices may indeed be arbitrary, there is a view in NLG that a great many are not, and the choices between different ways to say things—different ways to structure a text, different ways to refer to objects, the use of different syntactic 1 The latter two works are more widely available in revised form as Appelt (1985) and McKeown (1985). 346 Dale, Di Eugenio, and Scott Introduction constructions, and of different words to realize underlying concepts—need to be motivated in some way. Much research in NLG is oriented towards uncovering those motivations. • There is a sense in which work in NLU tends to start with the sentence as the principal focus of inquiry However, for much work in NLG, the primary focus is the text or discourse: although there are many important issues involved in the generation of sentential forms, those working in NLG research have long accepted that discourse-level issues are just as </context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, Douglas E. 1985. Planning English Sentences. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bateman</author>
</authors>
<title>Enabling technology for multilingual natural language generation: The KPML development environment.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<pages>3--15</pages>
<contexts>
<context position="21628" citStr="Bateman 1997" startWordPosition="3459" endWordPosition="3460">b will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF /SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN! NIGEL (Penman group 1989), and its descendant KPML / NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In related developments, there is a growing interest within the community in defining a reference architecture for NLG; if</context>
</contexts>
<marker>Bateman, 1997</marker>
<rawString>Bateman, John A. 1997. Enabling technology for multilingual natural language generation: The KPML development environment. Natural Language Engineering, 3:15-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Eduard H Hovy</author>
</authors>
<title>Dietmar Rosner,</title>
<date>1992</date>
<booktitle>Aspects of Automated Natural Language Generation. Lecture Notes in Artificial Intelligence.</booktitle>
<editor>and Oliviero Stock, editors.</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Dale, Hovy, 1992</marker>
<rawString>Dale, Robert, Eduard H. Hovy, Dietmar Rosner, and Oliviero Stock, editors. 1992. Aspects of Automated Natural Language Generation. Lecture Notes in Artificial Intelligence. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Christopher S Mellish</author>
</authors>
<title>Towards evaluation in natural language generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="23324" citStr="Dale and Mellish (1998)" startWordPosition="3726" endWordPosition="3729">l and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &amp;quot;task efficacy&amp;quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to generation. Dale and Mellish (1998) suggest that the NLG community could make progress by devising specific evaluation methods for NLG subtasks such as content determination, text structuring, and realization; this &amp;quot;glass box&amp;quot; approach is likely to result in a clearer understanding of how to evaluate NLG systems as a whole. The particular foci we have just outlined are specific to work in NLG. However, just as corpus-based methods have become very important in NLU research, we may expect this to happen increasingly in work on NLG too. Raw or coded text has been used by researchers to investigate strategies in a number of differ</context>
</contexts>
<marker>Dale, Mellish, 1998</marker>
<rawString>Dale, Robert and Christopher S. Mellish. 1998. Towards evaluation in natural language generation. In Proceedings of the First International Conference on Language Resources and Evaluation, Granada, Spain, May 28-30.</rawString>
</citation>
<citation valid="true">
<date>1990</date>
<booktitle>Current Research in Natural Language Generation.</booktitle>
<editor>Dale, Robert, Chris Mellish, and Michael Zock, editors.</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="4121" citStr="(1990)" startWordPosition="657" endWordPosition="657">ppeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work being carried out in NLG tends to be underrepresented in these forums. Instead, the community has tended to present its results at the two biennial series of workshops—one European and one international—that have sprung up in the last ten years. Many of these workshops have led to books: see Kempen (1987); McDonald and Bolc (1988); Zock and Sabah (1988); Dale, Mellish, and Zock (1990); Paris, Swartout, and Mann (1991); Dale et al. (1992); Horacek and Zock (1993); and Adorni and Zock (1996). This special issue of Computational Linguistics was inspired by discussions at the International Workshop on Natural Language Generation held in Herstmonceux in 1996; the aim of the volume you are reading is to show the wider computational linguistics community something of the range of activities in NLG. 2. Some Perspectives on Natural Language Generation What is natural language generation about? A definition offered by McDonald (1987, 983) over ten years ago has stood the test of tim</context>
<context position="9557" citStr="(1990)" startWordPosition="1537" endWordPosition="1537">f a multisentential text, or of a turn in a dialogue, can be determined. At a micro level, researchers are concerned with how the content of appropriate referring expressions can be worked out. In each case the problem is how to select the right information from that which is available; it is rarely appropriate to say everything we could say. 2 It should be noted, however, that there is a body of work that looks at the use of bidirectional grammars, where a common declarative representation of grammatical knowledge is used both for parsing and for realization; see, for example, Shieber et al. (1990). 347 Computational Linguistics Volume 24, Number 3 • There is also the question of text structure: texts are not just random collections of sentences; they exhibit a structure that plays a key role in conveying their meaning. Researchers in NLG are concerned with elucidating mechanisms for determining the most appropriate structures to use in particular circumstances, and with working out how the information to be conveyed can best be packaged into paragraph- and sentence-sized chunks. • Closer to the kinds of issues that concern those working in parsing, there are the problems of surface rea</context>
</contexts>
<marker>1990</marker>
<rawString>Dale, Robert, Chris Mellish, and Michael Zock, editors. 1990. Current Research in Natural Language Generation. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>18--233</pages>
<contexts>
<context position="17877" citStr="Dale and Reiter (1995)" startWordPosition="2833" endWordPosition="2836"> is an available corpus, judgments of quality can often only be made by appealing to convention. Psycholinguistic studies suggest themselves as a useful source of guidance but this too can be problematic: what speakers or writers typically produce often conflicts with their preferences as perceivers, as shown, for example, with regard to referring expressions by Stevenson and her colleagues (Stevenson, Crawley, and Kleinman 1994; Stevenson and Urbanowicz 1995). Oberlander discusses this apparent paradox with reference to the generation of referring expressions—in particular, the suggestion by Dale and Reiter (1995) that generation algorithms for definite noun phrases should be based on observations about human language production rather than on a strict observation of the Gricean maxims (Grice 1975). Oberlander calls this the Spike Lee maxim: Do the right thing—where &amp;quot;right&amp;quot; is that which is human and simple. He shows that, when generating referring expressions, we can&apos;t always tell whether the right thing is to mimic the preferences of language producers or language perceivers, since these preferences often conflict. He argues that until we develop a more sophisticated view of the expectations of speak</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, Robert and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 18:233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony C Davey</author>
</authors>
<title>Discourse Production.</title>
<date>1979</date>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh.</location>
<contexts>
<context position="3136" citStr="Davey 1979" startWordPosition="485" endWordPosition="486">s of work in the field appeared during the 1970s; in particular, Goldman&apos;s work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and * School of Mathematics, Physics, Computing and Electronics, Sydney NSW 2109, Australia f Learning Research and Development Center, 3939 O&apos;Hara Street, Pittsburgh, PA 15260, U.S.A. $ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 3 Davey&apos;s work on the generation of paragraph-long descriptions of tic-tac-toe games (Davey 1979) were among the first to focus on issues unique to NLG. The field really took off, however, in the 1980s; for those working in NLG, the decade began with a bang, and the Ph.D. theses of McDonald (1980), Appelt (1981), and McKeown (1982) have had a lasting impact on the shape of the field.&apos; But what has happened in the last fifteen years since those major pieces of work first appeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work be</context>
</contexts>
<marker>Davey, 1979</marker>
<rawString>Davey, Anthony C. 1979. Discourse Production. Edinburgh University Press, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Johanna D Moore</author>
<author>Massimo Paolucci</author>
</authors>
<title>Learning features that predict cue usage.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting,</booktitle>
<pages>80--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid,</location>
<marker>Di Eugenio, Moore, Paolucci, 1997</marker>
<rawString>Di Eugenio, Barbara, Johanna D. Moore, and Massimo Paolucci. 1997. Learning features that predict cue usage. In Proceedings of the 35th Annual Meeting, pages 80-87, Madrid, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>FUF: The universal unifier—user manual version 5.2.</title>
<date>1993</date>
<tech>Technical Report CUCS-038-91,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="21512" citStr="Elhadad 1993" startWordPosition="3441" endWordPosition="3442">at generate both text and graphics and attempt to combine these in sensible ways. We predict that the World Wide Web will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF /SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN! NIGEL (Penman group 1989), and its descendant KPML / NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In rel</context>
</contexts>
<marker>Elhadad, 1993</marker>
<rawString>Elhadad, Michael. 1993a. FUF: The universal unifier—user manual version 5.2. Technical Report CUCS-038-91, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>Using Argumentation to Control Lexical Choice: A Unification-based Implementation.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, Columbia University.</institution>
<contexts>
<context position="21512" citStr="Elhadad 1993" startWordPosition="3441" endWordPosition="3442">at generate both text and graphics and attempt to combine these in sensible ways. We predict that the World Wide Web will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF /SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN! NIGEL (Penman group 1989), and its descendant KPML / NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In rel</context>
</contexts>
<marker>Elhadad, 1993</marker>
<rawString>Elhadad, Michael. 1993b. Using Argumentation to Control Lexical Choice: A Unification-based Implementation. Ph.D. thesis, Computer Science Department, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
<author>Jacques Robin</author>
</authors>
<title>An overview of SURGE: A reusable comprehensive syntactic realisation component.</title>
<date>1996</date>
<booktitle>In Proceedings of the 8th International Workshop on Natural Language Generation (Demos and Posters),</booktitle>
<location>Herstmonceux, Sussex, UK,</location>
<contexts>
<context position="21545" citStr="Elhadad and Robin 1996" startWordPosition="3444" endWordPosition="3447">and graphics and attempt to combine these in sensible ways. We predict that the World Wide Web will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF /SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN! NIGEL (Penman group 1989), and its descendant KPML / NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In related developments, there is a gro</context>
</contexts>
<marker>Elhadad, Robin, 1996</marker>
<rawString>Elhadad, Michael and Jacques Robin. 1996. An overview of SURGE: A reusable comprehensive syntactic realisation component. In Proceedings of the 8th International Workshop on Natural Language Generation (Demos and Posters), Herstmonceux, Sussex, UK, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil M Goldman</author>
</authors>
<title>Computer Generation of Natural Language from a Deep Conceptual Base.</title>
<date>1974</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="2681" citStr="Goldman 1974" startWordPosition="423" endWordPosition="424">for just as long as work in natural language understanding. It should not be forgotten that much of NLP has its origins in the early work on machine translation in the 1950s; and that to carry out machine translation, one has to not only analyze existing texts but also to generate new ones. The early machine translation experiments, however, did not recognize the problems that give modern work in NLG its particular character. The first significant pieces of work in the field appeared during the 1970s; in particular, Goldman&apos;s work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and * School of Mathematics, Physics, Computing and Electronics, Sydney NSW 2109, Australia f Learning Research and Development Center, 3939 O&apos;Hara Street, Pittsburgh, PA 15260, U.S.A. $ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 3 Davey&apos;s work on the generation of paragraph-long descriptions of tic-tac-toe games (Davey 1979) were among the first to focus on issues unique to NLG. The field really took off, however, in the 1980s; for those working in NLG, the decade be</context>
</contexts>
<marker>Goldman, 1974</marker>
<rawString>Goldman, Neil M. 1974. Computer Generation of Natural Language from a Deep Conceptual Base. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<editor>In P. Cole and J. L. Morgan, editors,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="18065" citStr="Grice 1975" startWordPosition="2863" endWordPosition="2864">atic: what speakers or writers typically produce often conflicts with their preferences as perceivers, as shown, for example, with regard to referring expressions by Stevenson and her colleagues (Stevenson, Crawley, and Kleinman 1994; Stevenson and Urbanowicz 1995). Oberlander discusses this apparent paradox with reference to the generation of referring expressions—in particular, the suggestion by Dale and Reiter (1995) that generation algorithms for definite noun phrases should be based on observations about human language production rather than on a strict observation of the Gricean maxims (Grice 1975). Oberlander calls this the Spike Lee maxim: Do the right thing—where &amp;quot;right&amp;quot; is that which is human and simple. He shows that, when generating referring expressions, we can&apos;t always tell whether the right thing is to mimic the preferences of language producers or language perceivers, since these preferences often conflict. He argues that until we develop a more sophisticated view of the expectations of speakers and hearers, developers of NLG systems should probably stick to the Spike Lee maxim: even with its known limitations it produces more natural results than are achieved by following a s</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H. P. 1975. Logic and Conversation. In P. Cole and J. L. Morgan, editors, Syntax and Semantics 3: Speech Acts. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Horacek</author>
<author>M Zock</author>
<author>editors</author>
</authors>
<date>1993</date>
<booktitle>New Concepts in Natural Language Generation.</booktitle>
<location>Pinter, London.</location>
<marker>Horacek, Zock, editors, 1993</marker>
<rawString>Horacek, H. and M. Zock, editors. 1993. New Concepts in Natural Language Generation. Pinter, London.</rawString>
</citation>
<citation valid="true">
<title>Natural Language Generation: New Results</title>
<date>1987</date>
<booktitle>in Artificial Intelligence, Psychology and Linguistics. NATO ASI Series No. 135. Martinus Nijhoff Publishers,</booktitle>
<editor>Kempen, Gerard, editor.</editor>
<location>Boston, Dordrecht.</location>
<contexts>
<context position="4040" citStr="(1987)" startWordPosition="644" endWordPosition="644">t has happened in the last fifteen years since those major pieces of work first appeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work being carried out in NLG tends to be underrepresented in these forums. Instead, the community has tended to present its results at the two biennial series of workshops—one European and one international—that have sprung up in the last ten years. Many of these workshops have led to books: see Kempen (1987); McDonald and Bolc (1988); Zock and Sabah (1988); Dale, Mellish, and Zock (1990); Paris, Swartout, and Mann (1991); Dale et al. (1992); Horacek and Zock (1993); and Adorni and Zock (1996). This special issue of Computational Linguistics was inspired by discussions at the International Workshop on Natural Language Generation held in Herstmonceux in 1996; the aim of the volume you are reading is to show the wider computational linguistics community something of the range of activities in NLG. 2. Some Perspectives on Natural Language Generation What is natural language generation about? A defini</context>
</contexts>
<marker>1987</marker>
<rawString>Kempen, Gerard, editor. 1987. Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics. NATO ASI Series No. 135. Martinus Nijhoff Publishers, Boston, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James C Lester</author>
<author>Bruce W Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The KNIGHT experiments.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="22689" citStr="Lester and Porter 1997" startWordPosition="3625" endWordPosition="3628">ial to increase research productivity. In related developments, there is a growing interest within the community in defining a reference architecture for NLG; if successful, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &amp;quot;task efficacy&amp;quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to </context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>Lester, James C. and Bruce W. Porter. 1997. Developing and empirically evaluating robust explanation generators: The KNIGHT experiments. Computational Linguistics, 23(1):65-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Natural Language Production as a Process of Decision Making under Constraint.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="3337" citStr="McDonald (1980)" startWordPosition="523" endWordPosition="524">mputing and Electronics, Sydney NSW 2109, Australia f Learning Research and Development Center, 3939 O&apos;Hara Street, Pittsburgh, PA 15260, U.S.A. $ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 3 Davey&apos;s work on the generation of paragraph-long descriptions of tic-tac-toe games (Davey 1979) were among the first to focus on issues unique to NLG. The field really took off, however, in the 1980s; for those working in NLG, the decade began with a bang, and the Ph.D. theses of McDonald (1980), Appelt (1981), and McKeown (1982) have had a lasting impact on the shape of the field.&apos; But what has happened in the last fifteen years since those major pieces of work first appeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work being carried out in NLG tends to be underrepresented in these forums. Instead, the community has tended to present its results at the two biennial series of workshops—one European and one international—</context>
</contexts>
<marker>McDonald, 1980</marker>
<rawString>McDonald, David D. 1980. Natural Language Production as a Process of Decision Making under Constraint. Ph.D. thesis, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Natural language generation. In</title>
<date>1987</date>
<booktitle>Encyclopedia of Artificial Intelligence.</booktitle>
<pages>642--655</pages>
<editor>Stuart C. Shapiro, editor,</editor>
<publisher>John Wiley and Sons,</publisher>
<contexts>
<context position="4670" citStr="McDonald (1987" startWordPosition="741" endWordPosition="742">olc (1988); Zock and Sabah (1988); Dale, Mellish, and Zock (1990); Paris, Swartout, and Mann (1991); Dale et al. (1992); Horacek and Zock (1993); and Adorni and Zock (1996). This special issue of Computational Linguistics was inspired by discussions at the International Workshop on Natural Language Generation held in Herstmonceux in 1996; the aim of the volume you are reading is to show the wider computational linguistics community something of the range of activities in NLG. 2. Some Perspectives on Natural Language Generation What is natural language generation about? A definition offered by McDonald (1987, 983) over ten years ago has stood the test of time: Natural language generation is the process of deliberately constructing a natural language text in order to meet specified communicative goals. A more recent definition with a slightly different emphasis can be found in Reiter and Dale (1997, 57): Natural language generation is the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in ... human languages from some underlying non-linguistic representation of information. Both defi</context>
</contexts>
<marker>McDonald, 1987</marker>
<rawString>McDonald, David D. 1987. Natural language generation. In Stuart C. Shapiro, editor, Encyclopedia of Artificial Intelligence. John Wiley and Sons, pages 642-655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
<author>Leonard Bolc</author>
</authors>
<title>Natural Language Generation Systems.</title>
<date>1988</date>
<publisher>Springer-Verlag,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="4066" citStr="McDonald and Bolc (1988)" startWordPosition="645" endWordPosition="648">ppened in the last fifteen years since those major pieces of work first appeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work being carried out in NLG tends to be underrepresented in these forums. Instead, the community has tended to present its results at the two biennial series of workshops—one European and one international—that have sprung up in the last ten years. Many of these workshops have led to books: see Kempen (1987); McDonald and Bolc (1988); Zock and Sabah (1988); Dale, Mellish, and Zock (1990); Paris, Swartout, and Mann (1991); Dale et al. (1992); Horacek and Zock (1993); and Adorni and Zock (1996). This special issue of Computational Linguistics was inspired by discussions at the International Workshop on Natural Language Generation held in Herstmonceux in 1996; the aim of the volume you are reading is to show the wider computational linguistics community something of the range of activities in NLG. 2. Some Perspectives on Natural Language Generation What is natural language generation about? A definition offered by McDonald (</context>
</contexts>
<marker>McDonald, Bolc, 1988</marker>
<rawString>McDonald, David D. and Leonard Bolc. 1988. Natural Language Generation Systems. Springer-Verlag, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions About Database Structure.</title>
<date>1982</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="3372" citStr="McKeown (1982)" startWordPosition="528" endWordPosition="529">2109, Australia f Learning Research and Development Center, 3939 O&apos;Hara Street, Pittsburgh, PA 15260, U.S.A. $ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 3 Davey&apos;s work on the generation of paragraph-long descriptions of tic-tac-toe games (Davey 1979) were among the first to focus on issues unique to NLG. The field really took off, however, in the 1980s; for those working in NLG, the decade began with a bang, and the Ph.D. theses of McDonald (1980), Appelt (1981), and McKeown (1982) have had a lasting impact on the shape of the field.&apos; But what has happened in the last fifteen years since those major pieces of work first appeared? Although one does find articles on NLG in the pages of Computational Linguistics and other journals in the field, and papers on generation do appear at the major NLP conferences, the quantity and range of work being carried out in NLG tends to be underrepresented in these forums. Instead, the community has tended to present its results at the two biennial series of workshops—one European and one international—that have sprung up in the last ten</context>
</contexts>
<marker>McKeown, 1982</marker>
<rawString>McKeown, Kathleen R. 1982. Generating Natural Language Text in Response to Questions About Database Structure. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, May. Available as Technical Report MS-CIS-82-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="6237" citStr="McKeown (1985)" startWordPosition="997" endWordPosition="998">ormation that may be nonlinguistic in nature. Each of these points bears some elaboration: • In work in NLG, a major concern is that of choosing between different ways of doing things, as the same content can often be expressed in many different ways. Although some of these choices may indeed be arbitrary, there is a view in NLG that a great many are not, and the choices between different ways to say things—different ways to structure a text, different ways to refer to objects, the use of different syntactic 1 The latter two works are more widely available in revised form as Appelt (1985) and McKeown (1985). 346 Dale, Di Eugenio, and Scott Introduction constructions, and of different words to realize underlying concepts—need to be motivated in some way. Much research in NLG is oriented towards uncovering those motivations. • There is a sense in which work in NLU tends to start with the sentence as the principal focus of inquiry However, for much work in NLG, the primary focus is the text or discourse: although there are many important issues involved in the generation of sentential forms, those working in NLG research have long accepted that discourse-level issues are just as important, and prob</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, Kathleen R. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
</authors>
<title>The Generation Gap: The Problem of Expressibility in Text Planning.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="19931" citStr="Meteer 1990" startWordPosition="3174" endWordPosition="3175">son (1977), there has been a tendency to see NLG as involving two problems, which Thompson characterized as being concerned with decisions of strategy and tactics: in short, questions about what to say and questions about how to say it. In the field, this translated into work in the two areas of text planning and linguistic realization, with researchers often declaring themselves as working on one or the other. In more recent years, there has been the realization that something is required in the middle; this was most notably expressed in Meteer&apos;s work on what she called &amp;quot;the generation gap&amp;quot; (Meteer 1990). This has given rise to a body of work that explores questions of what is often referred to as microplanning: once a 3 This being: Anything goes. 350 Dale, Di Eugenio, and Scott Introduction text planning process has worked out the overall structure of a text and the content to be conveyed, how is this information packaged into sentences? Serious work here has only just begun: there are a great many unresolved issues, and in many cases the questions themselves are unclear. Multimodal generation. Real text is not disembodied. It always appears in context, and in particular within some medium—f</context>
</contexts>
<marker>Meteer, 1990</marker>
<rawString>Meteer, Marie. 1990. The Generation Gap: The Problem of Expressibility in Text Planning. Ph.D. thesis, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Milosavljevic</author>
<author>Robert Dale</author>
</authors>
<title>Strategies for comparison in encyclopedia descriptions.</title>
<date>1996</date>
<booktitle>In Proceedings of the Eighth International Natural Language Generation Workshop,</booktitle>
<pages>161--170</pages>
<location>Herstmonceux, Sussex, UK,</location>
<marker>Milosavljevic, Dale, 1996</marker>
<rawString>Milosavljevic, Maria and Robert Dale. 1996. Strategies for comparison in encyclopedia descriptions. In Proceedings of the Eighth International Natural Language Generation Workshop, pages 161-170, Herstmonceux, Sussex, UK, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile Paris</author>
<author>Keith Vander Linden</author>
<author>Markus Fischer</author>
<author>Anthony Hartley</author>
<author>Lyn Pemberton</author>
<author>Richard Power</author>
<author>Donia Scott</author>
</authors>
<title>A support tool for writing multilingual instructions.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1398--1404</pages>
<marker>Paris, Linden, Fischer, Hartley, Pemberton, Power, Scott, 1995</marker>
<rawString>Paris, Cecile, Keith Vander Linden, Markus Fischer, Anthony Hartley, Lyn Pemberton, Richard Power, and Donia Scott. 1995. A support tool for writing multilingual instructions. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 1398-1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile L Paris</author>
<author>William R Swartout</author>
<author>William C Mann</author>
<author>editors</author>
</authors>
<date>1991</date>
<booktitle>Natural Language Generation in Artificial Intelligence and Computational Linguistics.</booktitle>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<marker>Paris, Swartout, Mann, editors, 1991</marker>
<rawString>Paris, Cecile L., William R. Swartout, and William C. Mann, editors. 1991. Natural Language Generation in Artificial Intelligence and Computational Linguistics. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penman group</author>
</authors>
<title>Documentation of the Penman Sentence Generation System. USC Information Sciences Institute,</title>
<date>1989</date>
<location>Marina del Rey, CA.</location>
<contexts>
<context position="13689" citStr="group 1989" startWordPosition="2184" endWordPosition="2185">verb. One such alternation is the causative, as in The mechanic drained the oil from the engine, the causative form of The oil drained from the engine. Stede takes the lexicon to be the central device for mapping between domain representations and intermediate semantic representations. Alternations are generated by applying one or more rules in a predetermined order to a basic lexical form; the choice of a specific alternation is determined by parameters such as salience. The intermediate semantic representation of a sentence is the input for the surface generator—in this case, Penman (Penman group 1989). Interesting aspects of Stede&apos;s approach are the capacity of his system to generate fine-grained distinctions of meaning, and the attention paid to both linguistic constraints and computational concerns. Although only English is discussed in this paper, Stede&apos;s system uses the same mechanism to generate alternations in German as well. 3.3 Mittal, Moore, Carenini, and Roth Generation technology is now increasingly finding a place in applied systems. One such application is described by Mittal, Moore, Carenini, and Roth, who have developed a system to generate captions to accompany the graphica</context>
<context position="21580" citStr="group 1989" startWordPosition="3451" endWordPosition="3452">nsible ways. We predict that the World Wide Web will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF /SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN! NIGEL (Penman group 1989), and its descendant KPML / NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In related developments, there is a growing interest within the community </context>
</contexts>
<marker>group, 1989</marker>
<rawString>Penman group. 1989. Documentation of the Penman Sentence Generation System. USC Information Sciences Institute, Marina del Rey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<pages>3--57</pages>
<contexts>
<context position="4965" citStr="Reiter and Dale (1997" startWordPosition="787" endWordPosition="790">al Language Generation held in Herstmonceux in 1996; the aim of the volume you are reading is to show the wider computational linguistics community something of the range of activities in NLG. 2. Some Perspectives on Natural Language Generation What is natural language generation about? A definition offered by McDonald (1987, 983) over ten years ago has stood the test of time: Natural language generation is the process of deliberately constructing a natural language text in order to meet specified communicative goals. A more recent definition with a slightly different emphasis can be found in Reiter and Dale (1997, 57): Natural language generation is the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in ... human languages from some underlying non-linguistic representation of information. Both definitions pick out some of the foci of interest that give work in NLG its distinctive flavor. From the first we note the emphasis on deliberate choice as the fundamental operation that underlies much work in the area, and on the generation of texts as opposed to single sentences; from the second,</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Reiter, Ehud and Robert Dale. 1997. Building applied natural language generation systems. Natural Language Engineering, 3:57-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
<author>Kathleen McKeown</author>
</authors>
<title>Empirically designing and evaluating a new revision-based model for summary generation.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>85--135</pages>
<contexts>
<context position="22861" citStr="Robin and McKeown 1996" startWordPosition="3650" endWordPosition="3653">l, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &amp;quot;task efficacy&amp;quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to generation. Dale and Mellish (1998) suggest that the NLG community could make progress by devising specific evaluation methods for NLG subtasks such as content determinatio</context>
</contexts>
<marker>Robin, McKeown, 1996</marker>
<rawString>Robin, Jacques and Kathleen McKeown. 1996. Empirically designing and evaluating a new revision-based model for summary generation. Artificial Intelligence, 85:135-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven F Roth</author>
<author>John Kolojejchick</author>
<author>Joe Mattis</author>
<author>Jade Goldstein</author>
</authors>
<title>Interactive graphic design using automatic presentation knowledge.</title>
<date>1994</date>
<booktitle>In Proceedings of CHI94: Human Factors in Computing Systems,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="14340" citStr="Roth et al. 1994" startWordPosition="2281" endWordPosition="2284">pproach are the capacity of his system to generate fine-grained distinctions of meaning, and the attention paid to both linguistic constraints and computational concerns. Although only English is discussed in this paper, Stede&apos;s system uses the same mechanism to generate alternations in German as well. 3.3 Mittal, Moore, Carenini, and Roth Generation technology is now increasingly finding a place in applied systems. One such application is described by Mittal, Moore, Carenini, and Roth, who have developed a system to generate captions to accompany the graphical presentations produced by SAGE (Roth et al. 1994). It is well known that the interpretation of even simple, conventional, graphics can be difficult without accompanying textual pointers (e.g., keys, labels of axes, and the like). SAGE is innovative in its ability to produce novel graphics for highly abstract and complex data. The comprehension of these presentations is often heavily reliant on captions: extended textual descriptions of the relation of the presentation to the data it depicts. Mittal et al. show how a SAGE graphic, together with information about the perceptual complexity of its elements and the structure of its underlying dat</context>
</contexts>
<marker>Roth, Kolojejchick, Mattis, Goldstein, 1994</marker>
<rawString>Roth, Steven F., John Kolojejchick, Joe Mattis, and Jade Goldstein. 1994. Interactive graphic design using automatic presentation knowledge. In Proceedings of CHI94: Human Factors in Computing Systems, pages 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donia Scott</author>
<author>Richard Power</author>
<author>editors</author>
</authors>
<title>Characteristics of administrative forms</title>
<date>1994</date>
<booktitle>in English, German and Italian. Deliverable EV-1, GIST project LRE 062-09. http:/ / ecate.itc.it:1024 /projects / gist/ gist-bibliography.html.</booktitle>
<marker>Scott, Power, editors, 1994</marker>
<rawString>Scott, Donia and Richard Power, editors. 1994. Characteristics of administrative forms in English, German and Italian. Deliverable EV-1, GIST project LRE 062-09. http:/ / ecate.itc.it:1024 /projects / gist/ gist-bibliography.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Gertjan van Noord</author>
<author>Fernando Pereira</author>
<author>Robert Moore</author>
</authors>
<date>1990</date>
<booktitle>Semantic head-driven generation. Computational Linguistics,</booktitle>
<pages>16--1</pages>
<marker>Shieber, van Noord, Pereira, Moore, 1990</marker>
<rawString>Shieber, Stuart, Gertjan van Noord, Fernando Pereira, and Robert Moore. 1990. Semantic head-driven generation. Computational Linguistics, 16(1):30-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosemary Stevenson</author>
<author>Rosalind Crawley</author>
<author>David Kleinman</author>
</authors>
<title>Thematic roles, focus and the representation of events.</title>
<date>1994</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>9--519</pages>
<marker>Stevenson, Crawley, Kleinman, 1994</marker>
<rawString>Stevenson, Rosemary, Rosalind Crawley, and David Kleinman. 1994. Thematic roles, focus and the representation of events. Language and Cognitive Processes, 9:519-548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosemary Stevenson</author>
<author>Agnieszka Urbanowicz</author>
</authors>
<title>Structural focusing, thematic role focusing and the comprehension of pronouns.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventeenth Annual Conference of the Cognive Science Society,</booktitle>
<pages>328--332</pages>
<contexts>
<context position="17719" citStr="Stevenson and Urbanowicz 1995" startWordPosition="2812" endWordPosition="2815">uestion (see, e.g., Scott and Power [1994] and Paris et al. [1995]). But this approach is not always possible (e.g., for novel domains), and even in cases where there is an available corpus, judgments of quality can often only be made by appealing to convention. Psycholinguistic studies suggest themselves as a useful source of guidance but this too can be problematic: what speakers or writers typically produce often conflicts with their preferences as perceivers, as shown, for example, with regard to referring expressions by Stevenson and her colleagues (Stevenson, Crawley, and Kleinman 1994; Stevenson and Urbanowicz 1995). Oberlander discusses this apparent paradox with reference to the generation of referring expressions—in particular, the suggestion by Dale and Reiter (1995) that generation algorithms for definite noun phrases should be based on observations about human language production rather than on a strict observation of the Gricean maxims (Grice 1975). Oberlander calls this the Spike Lee maxim: Do the right thing—where &amp;quot;right&amp;quot; is that which is human and simple. He shows that, when generating referring expressions, we can&apos;t always tell whether the right thing is to mimic the preferences of language pr</context>
</contexts>
<marker>Stevenson, Urbanowicz, 1995</marker>
<rawString>Stevenson, Rosemary and Agnieszka Urbanowicz. 1995. Structural focusing, thematic role focusing and the comprehension of pronouns. In Proceedings of the Seventeenth Annual Conference of the Cognive Science Society, pages 328-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
</authors>
<title>Strategy and tactics: A model for language production.</title>
<date>1977</date>
<booktitle>Papers from the 13th Regional Meeting of the Chicago Linguistics Society,</booktitle>
<pages>651--668</pages>
<editor>In W. A. Beach, S. E. Fox, and S. Philosoph, editors,</editor>
<location>Chicago, IL.</location>
<contexts>
<context position="19329" citStr="Thompson (1977)" startWordPosition="3074" endWordPosition="3075"> would all agree that even that is better than the Cole Porter maxim.&apos; 4. Future Directions for Research in Natural Language Generation We said earlier that NLG research has come a long way since its beginnings, but there is still a long way to go. What does the future hold? Crystal-ball gazing is always a risky business, but on the basis of our experience and some of the issues that arise both in the work presented here and in other submissions to the special issue, we would suggest the following aspects of NLG will be seen as important areas in the next five years. Microplanning. Ever since Thompson (1977), there has been a tendency to see NLG as involving two problems, which Thompson characterized as being concerned with decisions of strategy and tactics: in short, questions about what to say and questions about how to say it. In the field, this translated into work in the two areas of text planning and linguistic realization, with researchers often declaring themselves as working on one or the other. In more recent years, there has been the realization that something is required in the middle; this was most notably expressed in Meteer&apos;s work on what she called &amp;quot;the generation gap&amp;quot; (Meteer 199</context>
</contexts>
<marker>Thompson, 1977</marker>
<rawString>Thompson, Henry S. 1977. Strategy and tactics: A model for language production. In W. A. Beach, S. E. Fox, and S. Philosoph, editors, Papers from the 13th Regional Meeting of the Chicago Linguistics Society, pages 651-668, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Chris Mellish</author>
</authors>
<title>An empirical study of the generation of anaphora in Chinese.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="22808" citStr="Yeh and Mellish 1997" startWordPosition="3643" endWordPosition="3646">ning a reference architecture for NLG; if successful, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &amp;quot;task efficacy&amp;quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to generation. Dale and Mellish (1998) suggest that the NLG community could make progress by devising specific evaluation </context>
</contexts>
<marker>Yeh, Mellish, 1997</marker>
<rawString>Yeh, Ching-Long and Chris Mellish. 1997. An empirical study of the generation of anaphora in Chinese. Computational Linguistics, 23(1):169-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Michael Young</author>
</authors>
<title>Generating Descriptions of Complex Activities.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="22923" citStr="Young 1997" startWordPosition="3660" endWordPosition="3661">ough the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &amp;quot;task efficacy&amp;quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to generation. Dale and Mellish (1998) suggest that the NLG community could make progress by devising specific evaluation methods for NLG subtasks such as content determination, text structuring, and realization; this &amp;quot;glass box&amp;quot; approac</context>
</contexts>
<marker>Young, 1997</marker>
<rawString>Young, R. Michael. 1997. Generating Descriptions of Complex Activities. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
<author>Gerard Sabah</author>
<author>editors</author>
</authors>
<date>1988</date>
<booktitle>Advances in Natural Language Generation: An Interdisciplinary Perspective. Ablex Publishing Corp.,</booktitle>
<location>Norwood, NJ.</location>
<marker>Zock, Sabah, editors, 1988</marker>
<rawString>Zock, Michael and Gerard Sabah, editors. 1988. Advances in Natural Language Generation: An Interdisciplinary Perspective. Ablex Publishing Corp., Norwood, NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>