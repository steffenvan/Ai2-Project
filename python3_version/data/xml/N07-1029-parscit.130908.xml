<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000487">
<title confidence="0.923281">
Combining Outputs from Multiple Machine Translation Systems
</title>
<note confidence="0.604864333333333">
Antti-Veikko I. Rosti and Necip Fazil Ayan and Bing Xiang and
Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
</note>
<email confidence="0.84339">
arosti,bxiang,smatsouk,schwartz @bbn.com
</email>
<affiliation confidence="0.962882">
Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742
</affiliation>
<email confidence="0.989509">
nfa,bonnie @umiacs.umd.edu
</email>
<sectionHeader confidence="0.995436" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999765363636364">
Currently there are several approaches to
machine translation (MT) based on differ-
ent paradigms; e.g., phrasal, hierarchical
and syntax-based. These three approaches
yield similar translation accuracy despite
using fairly different levels of linguistic
knowledge. The availability of such a
variety of systems has led to a growing
interest toward finding better translations
by combining outputs from multiple sys-
tems. This paper describes three differ-
ent approaches to MT system combina-
tion. These combination methods oper-
ate on sentence, phrase and word level
exploiting information from -best lists,
system scores and target-to-source phrase
alignments. The word-level combination
provides the most robust gains but the
best results on the development test sets
(NIST MT05 and the newsgroup portion
of GALE 2006 dry-run) were achieved by
combining all three methods.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973219512195">
In recent years, machine translation systems based
on new paradigms have emerged. These systems
employ more than just the surface-level information
used by the state-of-the-art phrase-based translation
systems. For example, hierarchical (Chiang, 2005)
and syntax-based (Galley et al., 2006) systems have
recently improved in both accuracy and scalability.
Combined with the latest advances in phrase-based
translation systems, it has become more attractive
to take advantage of the various outputs in forming
consensus translations (Frederking and Nirenburg,
1994; Bangalore et al., 2001; Jayaraman and Lavie,
2005; Matusov et al., 2006).
System combination has been successfully ap-
plied in state-of-the-art speech recognition evalua-
tion systems for several years (Fiscus, 1997). Even
though the underlying modeling techniques are sim-
ilar, many systems produce very different outputs
with approximately the same accuracy. One of the
most successful approaches is consensus network
decoding (Mangu et al., 2000) which assumes that
the confidence of a word in a certain position is
based on the sum of confidences from each system
output having the word in that position. This re-
quires aligning the system outputs to form a con-
sensus network and – during decoding – simply
finding the highest scoring path through this net-
work. The alignment of speech recognition outputs
is fairly straightforward due to the strict constraint in
word order. However, machine translation outputs
do not have this constraint as the word order may be
different between the source and target languages.
MT systems employ various re-ordering (distortion)
models to take this into account.
Three MT system combination methods are pre-
sented in this paper. They operate on the sentence,
phrase and word level. The sentence-level combi-
nation is based on selecting the best hypothesis out
of the merged N-best lists. This method does not
generate new hypotheses – unlike the phrase and
word-level methods. The phrase-level combination
</bodyText>
<page confidence="0.97266">
228
</page>
<note confidence="0.7985975">
Proceedings of NAACL HLT 2007, pages 228–235,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999832171428572">
is based on extracting sentence-specific phrase trans-
lation tables from system outputs with alignments
to source and running a phrasal decoder with this
new translation table. This approach is similar to
the multi-engine MT framework proposed in (Fred-
erking and Nirenburg, 1994) which is not capable of
re-ordering. The word-level combination is based
on consensus network decoding. Translation edit
rate (TER) (Snover et al., 2006) is used to align
the hypotheses and minimum Bayes risk decoding
under TER (Sim et al., 2007) is used to select the
alignment hypothesis. All combination methods use
weights which may be tuned using Powell’s method
(Brent, 1973) on -best lists. Both sentence and
phrase-level combination methods can generate -
best lists which may also be used as new system out-
puts in the word-level combination.
Experiments on combining six machine transla-
tion system outputs were performed. Three sys-
tems were phrasal, two hierarchical and one syntax-
based. The systems were evaluated on NIST MT05
and the newsgroup portion of the GALE 2006 dry-
run sets. The outputs were evaluated on both TER
and BLEU. As the target evaluation metric in the
GALE program was human-mediated TER (HTER)
(Snover et al., 2006), it was found important to im-
prove both of these automatic metrics.
This paper is organized as follows. Section 2
describes the evaluation metrics and a generic dis-
criminative optimization technique used in tuning of
the various system combination weights. Sentence,
phrase and word-level system combination methods
are presented in Sections 3, 4 and 5. Experimental
results on Arabic and Chinese to English newswire
and newsgroup test data are presented in Section 6.
</bodyText>
<sectionHeader confidence="0.76213" genericHeader="introduction">
2 Evaluation Metrics and Discriminative
Tuning
</sectionHeader>
<bodyText confidence="0.998866469387755">
The official metric of the 2006 DARPA GALE
evaluation was human-mediated translation edit rate
(HTER). HTER is computed as the minimum trans-
lation edit rate (TER) between a system output and
a targeted reference which preserves the meaning
and fluency of the sentence (Snover et al., 2006).
The targeted reference is generated by human post-
editors who make edits to a reference translation so
as to minimize the TER between the reference and
the MT output without changing the meaning of the
reference. Computing the HTER is very time con-
suming due to the human post-editing. It is desir-
able to have an automatic evaluation metric that cor-
relates well with the HTER to allow fast evaluation
of the MT systems during development. Correla-
tions of different evaluation metrics have been stud-
ied (Snover et al., 2006) but according to various
internal HTER experiments it is not clear whether
TER or BLEU correlates better. Therefore it is prob-
ably safest to try and not degrade either.
The TER of a translation is computed as
where is the total number of words in the ref-
erence translation . In the case of multiple ref-
erences, the edits are counted against all references,
is the average number of words in the reference
translations and the final TER is computed using the
minimum number of edits. The NIST BLEU-4 is a
variant of BLEU (Papineni et al., 2002) and is com-
puted as
where is the precision of -grams in
the hypothesis given the reference and
is a brevity penalty. The -gram
counts from multiple references are accumulated in
estimating the precisions.
All system combination methods presented in this
paper may be tuned to directly optimize either one
of these automatic evaluation metrics. The tuning
uses -best lists of hypotheses with various fea-
ture scores. The feature scores may be combined
with tunable weights forming an arbitrary scoring
function. As the derivatives of this function are not
usually available, Brent’s modification of Powell’s
method (Brent, 1973) may be used to find weights
that optimize the appropriate evaluation metric in
the re-scored -best list. The optimization starts
at a random initial point in the -dimensional pa-
rameter space, first searching through an initial set
of basis vectors. As searching repeatedly through
the set of basis vectors is inefficient, the direction of
</bodyText>
<page confidence="0.997346">
229
</page>
<bodyText confidence="0.9999093">
the vectors is gradually moved toward a larger posi-
tive change in the evaluation metric. To improve the
chances of finding a global optimum, the algorithm
is repeated with varying initial values. The modified
Powell’s method has been previously used in opti-
mizing the weights of a standard feature-based MT
decoder in (Och, 2003) where a more efficient algo-
rithm for log-linear models was proposed. However,
this is specific to log-linear models and cannot be
easily extended for more complicated functions.
</bodyText>
<sectionHeader confidence="0.995824" genericHeader="method">
3 Sentence-Level Combination
</sectionHeader>
<bodyText confidence="0.999827428571429">
The first combination method is based on re-ranking
a merged -best list. A confidence score from each
system is assigned to each unique hypothesis in the
merged list. The confidence scores for each hypoth-
esis are used to produce a single score which, com-
bined with a 5-gram language model score, deter-
mines a new ranking of the hypotheses.
</bodyText>
<subsectionHeader confidence="0.999294">
3.1 Hypothesis Confidence Estimation
</subsectionHeader>
<bodyText confidence="0.951108909090909">
Generalized linear models (GLMs) have been ap-
plied for confidence estimation in speech recogni-
tion (Siu and Gish, 1999). The logit model, which
models the log odds of an event as a linear function
of the features, can be used in confidence estima-
tion. The confidence for a system generating a
hypothesis may be modeled as
(3)
where each system has weights , and is
the th feature for system and hypothesis . The
features used in this work were:
</bodyText>
<listItem confidence="0.999800333333333">
1. Rank in the system’s -best list;
2. Sentence posterior with system-specific total
score scaling factors;
3. System’s total score;
4. Number of words in the hypothesis;
5. System-specific bias.
</listItem>
<bodyText confidence="0.998852166666666">
If the system did not generate the hypothesis , the
confidence is set to zero. To prevent overflow in ex-
ponentiating the summation in Equation 3, the fea-
tures have to be scaled. In the experiments, feature
scaling factors were estimated from the tuning data
to limit the feature values between . The same
scaling factors have to be applied to the features ob-
tained from the test data.
The total confidence score of hypothesis is ob-
tained from the system confidences as
where is the number of systems generating the
hypothesis (i.e., the number of non-zero for )
and is the number of systems. The weights
through are constrained to sum to one; i.e., there
are three free parameters. These weights can balance
the total confidence between the number of systems
generating the hypothesis (votes), and the sum, max-
imum and average of the system confidences.
</bodyText>
<subsectionHeader confidence="0.999735">
3.2 Sentence Posterior Estimation
</subsectionHeader>
<bodyText confidence="0.999986944444444">
The second feature in the GLM is the sentence pos-
terior estimated from the -best list. A sentence
posterior may simply be estimated from an -best
list by scaling the system scores for all hypotheses to
sum to one. When combining several systems based
on different translation paradigms and feature sets,
the system scores may not be comparable. The to-
tal scores may be scaled to obtain more consistent
sentence posteriors. The scaled posterior estimated
from an -best list may be written as
where is the scaling factor for system and is
the log-score system assigns to hypothesis . The
scaling factors may be tuned to optimize the evalua-
tion metric in the same fashion as the logit model
weights in Section 3.1. Equation 4 may be used
to assign total posteriors for each unique hypothesis
and the weights may be tuned using Powell’s method
on -best lists as described in Section 2.
</bodyText>
<subsectionHeader confidence="0.998808">
3.3 Hypothesis Re-ranking
</subsectionHeader>
<bodyText confidence="0.999928666666667">
The hypothesis confidence may be log-linearly com-
bined with a 5-gram language model (LM) score to
yield the final score as follows
</bodyText>
<page confidence="0.96218">
230
</page>
<bodyText confidence="0.999595529411765">
where is the number of words in hypothesis .
The number of words is commonly used in LM re-
scoring to balance the LM scores between hypothe-
ses of different lengths. The number of free pa-
rameters in the sentence-level combination method
is given by where is the num-
ber of systems and is the number of features; i.e.,
system score scaling factors ( ), three free inter-
polation weights (Equation 4) for the scaling factor
estimation, GLM weights ( ), three free in-
terpolation weights (Equation 4) for the hypothesis
confidence estimation and two free LM re-scoring
weights (Equation 6). All parameters may be tuned
using Powell’s method on -best lists as described
in Section 2.
The tuning of the sentence-level combination
method may be summarized as follows:
</bodyText>
<listItem confidence="0.997185454545454">
2. Estimate total score scaling factors as described
in Section 3.2;
3. Collect GLM feature scores for each unique hy-
pothesis;
4. Estimate GLM feature scaling factors as de-
scribed in Section 3.1;
5. Scale the GLM features;
6. Estimate GLM weights, combination weights
and LM re-scoring weights as described above;
7. Re-rank the merged -best list using the new
weights.
</listItem>
<bodyText confidence="0.999703666666667">
Testing the sentence-level combination has the same
steps as the tuning apart from all estimation steps;
i.e., steps 1, 3, 5 and 7.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="method">
4 Phrase-Level Combination
</sectionHeader>
<bodyText confidence="0.9999815">
The phrase-level combination is based on extracting
a new phrase translation table from each system’s
target-to-source phrase alignments and re-decoding
the source sentence using this new translation table
and a language model. In this work, the target-to-
source phrase alignments were available from the
individual systems. If the alignments are not avail-
able, they can be automatically generated; e.g., us-
ing GIZA++ (Och and Ney, 2003). The phrase trans-
lation table is generated for each source sentence us-
ing confidence scores derived from sentence poste-
riors with system-specific total score scaling factors
and similarity scores based on the agreement among
the phrases from all systems.
</bodyText>
<subsectionHeader confidence="0.991037">
4.1 Phrase Confidence Estimation
</subsectionHeader>
<bodyText confidence="0.987803166666667">
Each phrase has an initial confidence based on the
sentence posterior estimated from an -best list
in the same fashion as in Section 3.2. The confi-
dence of the phrase table entry is increased if several
systems agree on the target words. The agreement is
measured by four levels of similarity:
</bodyText>
<listItem confidence="0.998375428571429">
1. Same source interval, same target words, and
same original distortion;
2. Same source interval, same target words, with
different original distortion;
3. Overlapping source intervals with the same tar-
get words;
4. Overlapping target words.
</listItem>
<bodyText confidence="0.893001235294118">
represents the similarity of a given phrase
to all the hypotheses in the system at the similar-
ity level . Basically, if there is a similar phrase in a
given hypothesis in the system to the phrase ,
the similarity score is increased by . Note
that each phrase in one hypothesis is similar to an-
other hypothesis at only one similarity level, so one
hypothesis can contribute to at only one simi-
larity level. The final confidence of the phrase table
entry is defined as
(7)
where are system weights and are similarity
score weights. The parameters through interpo-
late between the sum, average and maximum of the
similarity scores. These interpolation weights and
1. Merge individual -best lists to form a large
-best list with unique hypotheses;
</bodyText>
<page confidence="0.990607">
231
</page>
<bodyText confidence="0.999560571428572">
the system weights are constrained to sum to one.
The number of tunable combination weights, in ad-
dition to normal decoder weights, is
where is the number of systems and is the
number of similarity levels; i.e., free system
weights, similarity score weights and two free in-
terpolation weights.
</bodyText>
<subsectionHeader confidence="0.935313">
4.2 Phrase-Based Decoding
</subsectionHeader>
<bodyText confidence="0.999414928571428">
The phrasal decoder used in the phrase-level com-
bination is based on standard beam search (Koehn,
2004). The decoder features are: a trigram lan-
guage model score, number of target phrases, num-
ber of target words, phrase distortion, phrase dis-
tortion computed over the original translations and
phrase translation confidences estimated in Section
4.1. The total score for a hypothesis is computed as
a log-linear combination of these features. The fea-
ture weights and combination weights (system and
similarity) may be tuned using Powell’s method on
-best lists as described in Section 2.
The phrase-level combination tuning can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.982212818181818">
1. Estimate sentence posteriors given the total
score scaling factors;
2. Collect all unique phrase table entries from
each hypothesis accumulating the similarity
scores ;
3. Combine the similarity scores to form phrase
confidences according to Equation 7;
4. Decode the source sentences using the current
weights to generate an -best list;
5. Estimate new decoder and combination
weights as described above.
</listItem>
<bodyText confidence="0.983669">
Testing the phrase-level combination is performed
by following steps 1 through 4.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="method">
5 Word-Level Combination
</sectionHeader>
<bodyText confidence="0.9999762">
The third combination method is based on confusion
network decoding. In confusion network decoding,
the words in all hypotheses are aligned against each
other to form a graph with word alternatives (in-
cluding nulls) for each alignment position. Each
aligned word is assigned a score relative to the votes
or word confidence scores (Fiscus, 1997; Mangu et
al., 2000) derived from the hypotheses. The decod-
ing is carried out by picking the words with the high-
est scores along the graph. In speech recognition,
this results in minimum expected word error rate
(WER) hypothesis (Mangu et al., 2000) or equiva-
lently minimum Bayes risk (MBR) under WER with
uniform target sentence posterior distribution (Sim
et al., 2007).
In machine translation, aligning hypotheses is
more complicated compared to speech recognition
since the target words do not necessarily appear in
the same order. So far, confusion networks have
been applied in MT system combination using three
different alignment procedures: WER (Bangalore
et al., 2001), GIZA++ alignments (Matusov et al.,
2006) and TER (Sim et al., 2007). WER align-
ments do not allow shifts, GIZA++ alignments re-
quire careful training and are not always reliable.
TER alignments do not guarantee that similar but
lexically different words are aligned correctly but
TER does not require training new models and al-
lows shifts (Snover et al., 2006). This work extends
the approach proposed in (Sim et al., 2007).
</bodyText>
<subsectionHeader confidence="0.957709">
5.1 Confusion Network Generation
</subsectionHeader>
<bodyText confidence="0.9999465">
Due to the varying word order in the MT hypotheses,
the decision of confusion network skeleton is essen-
tial. The skeleton determines the general word order
of the combined hypothesis. One option would be to
use the output from the system with the best perfor-
mance on some development set. However, it was
found that this approach did not always yield bet-
ter combination output compared to the best single
system on all evaluation metrics. Instead of using a
single system output as the skeleton, the hypothesis
that best agrees with the other hypotheses on aver-
age may be used. In this paper, the minimum av-
erage TER score of one hypothesis against all other
hypotheses was used as follows
</bodyText>
<equation confidence="0.459893">
(8)
</equation>
<bodyText confidence="0.92503625">
This may be viewed as the MBR hypothesis under
TER given uniform target sentence posterior distri-
bution (Sim et al., 2007). It is also possible to com-
pute the MBR hypothesis under BLEU.
</bodyText>
<page confidence="0.987288">
232
</page>
<bodyText confidence="0.99989325">
Finding the MBR hypothesis requires computing
the TER against all hypotheses to be aligned. It
was found that aligning more than one hypothesis
( ) from each system to the skeleton im-
proves the combination outputs. However, only the
rank-1 hypotheses were considered as skeletons due
to the complexity of the TER alignment. The con-
fidence score assigned to each word was chosen to
be where the was based on the
rank of the aligned hypothesis in the system’s -
best. This was found to yield better scores than sim-
ple votes.
</bodyText>
<subsectionHeader confidence="0.998351">
5.2 Tunable System Weights
</subsectionHeader>
<bodyText confidence="0.999904666666667">
The word-level combination method described so
far does not require any tuning. To allow a variety
of outputs with different degrees of confidence to be
combined, system weights may be used. A confu-
sion network may be represented as a standard word
lattice with all paths traveling via all nodes. The
links in this lattice represent the alternative words
(including nulls) at the corresponding position in the
string. Confusion network decoding may be viewed
as finding the highest scoring path through this lat-
tice with summing all word scores along the path.
The standard lattice decoding algorithms may also
be used to generate -best lists from the confu-
sion network. The simplest way to introduce sys-
tem weights is to accumulate system-specific scores
along the paths and combine these scores linearly
with the weights. The system weights may be tuned
using Powell’s method on -best lists as described
in Section 2.
The word-level combination tuning can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.997318555555555">
1. Extract 10-best lists from the MT outputs;
2. Align each 10-best against each rank-1 hypoth-
esis using TER;
3. Choose the skeleton (Equation 8);
4. Generate a confusion network lattice with the
current system weights;
5. Generate -best list hypothesis and score files
from the lattice;
6. Estimate system weights as described above;
</listItem>
<table confidence="0.999822076923077">
Arabic Newswire Newsgroups
TER BLEU TER BLEU
system A 42.98 49.58 59.73 20.36
system B 43.79 47.06 61.55 18.08
system C 43.92 47.87 60.81 18.08
system D 40.75 52.09 59.25 20.28
system E 42.19 50.86 59.85 19.73
system F 44.30 50.15 61.74 20.61
phrcomb 40.45 53.70 59.90 21.49
sentcomb 41.56 52.18 60.21 19.77
no weights 6 39.33 53.66 58.15 20.61
TER 6 39.41 54.37 58.21 20.85
TER 8 39.43 54.40 57.96 21.44
</table>
<tableCaption confidence="0.827600333333333">
Table 1: Mixed-case TER and BLEU scores on
Arabic NIST MT05 (newswire) and the newsgroups
portion of the GALE 2006 dry-run data.
</tableCaption>
<bodyText confidence="0.868179333333333">
7. Re-rank the -best list using the new weights.
Testing the word-level combination has the same
steps as the tuning apart from steps 6 and 7.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999952083333333">
Six systems trained on all data available for GALE
2006 evaluation were used in the experiments to
demonstrate the performance of all three system
combination methods on Arabic and Chinese to En-
glish MT tasks. Three systems were phrase-based
(A, C and E), two hierarchical (B and D) and one
syntax-based (F). The phrase-based systems used
different sets of features and re-ordering approaches.
The hierarchical systems used different rule sets. All
systems were tuned on NIST MT02 evaluation sets
with four references. Systems A and B were tuned
to minimize TER, the other systems were tuned to
maximize BLEU.
As discussed in Section 2, the system combina-
tion tuning metric was chosen so that gains were ob-
served in both TER and BLEU on development test
sets. NIST MT05 comprising only newswire data
(1056 Arabic and 1082 Chinese sentences) with four
reference translations and the newsgroup portion of
the GALE 2006 dry-run (203 Arabic and 126 Chi-
nese sentences) with one reference translation were
used as the test sets. It was found that minimiz-
ing TER on Arabic also resulted in higher BLEU
scores compared to the best single system. However,
</bodyText>
<page confidence="0.996232">
233
</page>
<table confidence="0.999817923076923">
Chinese Newswire Newsgroups
TER BLEU TER BLEU
system A 56.57 29.63 68.61 13.20
system B 56.30 29.62 69.87 12.33
system C 59.48 31.32 69.37 13.91
system D 58.32 33.77 67.61 16.86
system E 58.46 32.40 69.08 15.08
system F 56.79 35.30 68.08 16.31
phrcomb 56.50 35.33 68.48 15.88
sentcomb 56.71 36.24 69.50 16.11
no weights 6 53.80 36.17 66.87 15.90
BLEU 6 54.34 36.44 66.50 16.44
BLEU 8 54.86 36.90 66.45 17.32
</table>
<tableCaption confidence="0.992206">
Table 2: Mixed-case TER and BLEU scores on Chi-
</tableCaption>
<bodyText confidence="0.984353450980392">
nese NIST MT05 (newswire) and the newsgroups
portion of the GALE 2006 dry-run data.
minimizing TER on Chinese resulted in significantly
lower BLEU. So, TER was used in tuning the com-
bination weights on Arabic and BLEU on Chinese.
The sentence and phrase-level combination
weights were tuned on NIST MT03 evaluation sets.
On the tuning sets, both methods yield about 0.5%-
1.0% gain in TER and BLEU. The mixed-case TER
and BLEU scores on both test sets are shown in Ta-
ble 1 for Arabic and Table 2 for Chinese (phrcomb
represents phrase and sentcomb sentence-level
combination). The phrase-level combination seems
to outperform the sentence-level combination in
terms of both metrics on Arabic although gains over
the best single system are modest, if any. On Chi-
nese, the sentence-level combination yields higher
BLEU scores than the phrase-level combination.
The combination BLEU scores on the newsgroup
data are not higher than the best system, though.
The word-level combination was evaluated in
three settings. First, simple confusion network de-
coding with six systems without system weights
was performed (no weights 6 in the tables).
Second, system weights were trained for combin-
ing six systems (TER/BLEU 6 in the tables). Fi-
nally, all six system outputs as well as the sen-
tence and phrase-level combination outputs were
combined with system weights (TER/BLEU 8 in
the tables). The 6-way combination weights were
tuned on merged NIST MT03 and MT04 evaluation
sets and the 8-way combination weights were tuned
only on NIST MT04 since the sentence and phrase-
level combination methods were already tuned on
NIST MT03. The word-level combination yields
about 2.0%-3.0% gain in TER and 2.0%-4.0% gain
in BLEU on the tuning sets. The test set results show
that the simple confusion network decoding with-
out system weights yields very good scores, mostly
better than either sentence or phrase-level combina-
tion. The system weights seem to yield even higher
BLEU scores but not always lower TER scores on
both languages. Despite slightly hurting the TER
score on Arabic, the TER 8 combination result was
considered the best due to the highest BLEU and sig-
nificantly lower TER compared to any single sys-
tem. Similarly, the BLEU 8 was considered the
best combination result on Chinese. Internal HTER
experiments showed that BLEU 8 yielded lower
scores after post-editing even though no weights
6 had lower automatic TER score.
</bodyText>
<sectionHeader confidence="0.997697" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99991708">
Three methods for machine translation system com-
bination were presented in this paper. The sentence-
level combination was based on re-ranking a merged
-best list using generalized linear models with fea-
tures derived from each system’s output. The com-
bination yields slight gains on the tuning set. How-
ever, the gains were very small, if any, on the test
sets. The re-ranked -best lists were used success-
fully in the word-level combination method as new
system outputs. Various other features may be ex-
plored in this framework although the tuning may
be limited by the chosen optimization method in the
higher dimensional parameter space.
The phrase-level combination was based on de-
riving a new phrase translation table from the align-
ments to source provided in all system outputs. The
phrase translation scores were based on the level of
agreement between the system outputs and sentence
posterior estimates. A standard phrasal decoder with
the new phrase table was used to produce the fi-
nal combination output. The handling of the align-
ments from non-phrasal decoders may not be opti-
mal, though. The phrase-level combination yields
fairly good gains on the tuning sets. However, the
performance does not seem to generalize to the test
</bodyText>
<page confidence="0.991709">
234
</page>
<bodyText confidence="0.999934793103448">
sets used in this work. As usual, the phrasal decoder
can generate -best lists which were used success-
fully in the word-level combination method as new
system outputs.
The word-level combination method based on
consensus network decoding seems to be very ro-
bust and yield good gains over the best single sys-
tem even without any tunable weights. The decision
of the skeleton is crucial. Minimum Bayes Risk de-
coding under translation edit rate was used to select
the skeleton. Compared to the best possible skeleton
decision – according to an oracle experiment – fur-
ther gains might be obtained by using better decision
approach. Also, the alignment may be improved by
taking the target-to-source alignments into account
and allowing synonyms to align. The confusion net-
work decoding at the word level does not necessarily
retain coherent phrases as no language model con-
straints are taken into account. LM re-scoring might
alleviate this problem.
This paper has provided evidence that outputs
from six very different MT systems, tuned for two
different evaluation metrics, may be combined to
yield better outputs in terms of different evaluation
metrics. The focus of the future work will be to ad-
dress the individual issues in the combination meth-
ods mentioned above. It would also be interesting to
investigate how much different systems contribute to
the overall gain achieved via system combination.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999718">
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program
(approved for public release, distribution unlimited).
The authors would like to thank ISI and University
of Edinburgh for sharing their MT system outputs.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922754385965">
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351–354.
Richard P. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL, pages
263–270.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347–354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95–
100.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. COL-
ING/ACL, pages 961–968.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. AMTA.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373–
400.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33–40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160–
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311–318.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Manhung Siu and Herbert Gish. 1999. Evaluation
of word confidence for speech recognition systems.
Computer Speech and Language, 13(4):299–319.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA.
</reference>
<page confidence="0.998533">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530096">
<title confidence="0.999979">Combining Outputs from Multiple Machine Translation Systems</title>
<author confidence="0.9546115">I Rosti Fazil Ayan Xiang Matsoukas Schwartz J Dorr</author>
<address confidence="0.97231">BBN Technologies, 10 Moulton Street, Cambridge, MA 02138</address>
<email confidence="0.983529">arosti,bxiang,smatsouk,schwartz@bbn.com</email>
<address confidence="0.729689">Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742</address>
<email confidence="0.984775">nfa,bonnie@umiacs.umd.edu</email>
<abstract confidence="0.98515952173913">Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>351--354</pages>
<contexts>
<context position="1873" citStr="Bangalore et al., 2001" startWordPosition="258" endWordPosition="261"> all three methods. 1 Introduction In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This requires ali</context>
<context position="16781" citStr="Bangalore et al., 2001" startWordPosition="2685" endWordPosition="2688">ecoding is carried out by picking the words with the highest scores along the graph. In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim et al., 2007). In machine translation, aligning hypotheses is more complicated compared to speech recognition since the target words do not necessarily appear in the same order. So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007). WER alignments do not allow shifts, GIZA++ alignments require careful training and are not always reliable. TER alignments do not guarantee that similar but lexically different words are aligned correctly but TER does not require training new models and allows shifts (Snover et al., 2006). This work extends the approach proposed in (Sim et al., 2007). 5.1 Confusion Network Generation Due to the varying word order in the MT hypotheses, the decision of confusion network skeleton is essential. The skeleton determines the gener</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Proc. ASRU, pages 351–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard P Brent</author>
</authors>
<title>Algorithms for Minimization Without Derivatives.</title>
<date>1973</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="4079" citStr="Brent, 1973" startWordPosition="603" endWordPosition="604">translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate - best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgroup portion of the GALE 2006 dryrun sets. The outputs were evaluated on both TER and BLEU. As the target evaluation metric in the GALE program was human-mediated TER (HTER) (Snover et al., 2006), it was found important to improve </context>
<context position="7143" citStr="Brent, 1973" startWordPosition="1109" endWordPosition="1110">ed as where is the precision of -grams in the hypothesis given the reference and is a brevity penalty. The -gram counts from multiple references are accumulated in estimating the precisions. All system combination methods presented in this paper may be tuned to directly optimize either one of these automatic evaluation metrics. The tuning uses -best lists of hypotheses with various feature scores. The feature scores may be combined with tunable weights forming an arbitrary scoring function. As the derivatives of this function are not usually available, Brent’s modification of Powell’s method (Brent, 1973) may be used to find weights that optimize the appropriate evaluation metric in the re-scored -best list. The optimization starts at a random initial point in the -dimensional parameter space, first searching through an initial set of basis vectors. As searching repeatedly through the set of basis vectors is inefficient, the direction of 229 the vectors is gradually moved toward a larger positive change in the evaluation metric. To improve the chances of finding a global optimum, the algorithm is repeated with varying initial values. The modified Powell’s method has been previously used in opt</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>Richard P. Brent. 1973. Algorithms for Minimization Without Derivatives. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1537" citStr="Chiang, 2005" startWordPosition="212" endWordPosition="213">ate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods. 1 Introduction In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many system</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="2064" citStr="Fiscus, 1997" startWordPosition="287" endWordPosition="288">-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This requires aligning the system outputs to form a consensus network and – during decoding – simply finding the highest scoring path through this network. The alignment of speech recognition outputs is fairl</context>
<context position="16102" citStr="Fiscus, 1997" startWordPosition="2580" endWordPosition="2581">ation 7; 4. Decode the source sentences using the current weights to generate an -best list; 5. Estimate new decoder and combination weights as described above. Testing the phrase-level combination is performed by following steps 1 through 4. 5 Word-Level Combination The third combination method is based on confusion network decoding. In confusion network decoding, the words in all hypotheses are aligned against each other to form a graph with word alternatives (including nulls) for each alignment position. Each aligned word is assigned a score relative to the votes or word confidence scores (Fiscus, 1997; Mangu et al., 2000) derived from the hypotheses. The decoding is carried out by picking the words with the highest scores along the graph. In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim et al., 2007). In machine translation, aligning hypotheses is more complicated compared to speech recognition since the target words do not necessarily appear in the same order. So far, confusion networks have been applied in MT system combi</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In Proc. ASRU, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proc. ANLP,</booktitle>
<pages>95--100</pages>
<contexts>
<context position="1849" citStr="Frederking and Nirenburg, 1994" startWordPosition="254" endWordPosition="257">-run) were achieved by combining all three methods. 1 Introduction In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that pos</context>
<context position="3697" citStr="Frederking and Nirenburg, 1994" startWordPosition="539" endWordPosition="543">el. The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination 228 Proceedings of NAACL HLT 2007, pages 228–235, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate - best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proc. ANLP, pages 95– 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inferences and training of context-rich syntax translation models.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1576" citStr="Galley et al., 2006" startWordPosition="216" endWordPosition="219"> level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods. 1 Introduction In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with a</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inferences and training of context-rich syntax translation models. In Proc. COLING/ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyamsundar Jayaraman</author>
<author>Alon Lavie</author>
</authors>
<title>Multiengine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proc. EAMT.</booktitle>
<contexts>
<context position="1900" citStr="Jayaraman and Lavie, 2005" startWordPosition="262" endWordPosition="265">troduction In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This requires aligning the system outputs to</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>Shyamsundar Jayaraman and Alon Lavie. 2005. Multiengine machine translation guided by explicit word matching. In Proc. EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. AMTA.</booktitle>
<contexts>
<context position="14691" citStr="Koehn, 2004" startWordPosition="2362" endWordPosition="2363">polate between the sum, average and maximum of the similarity scores. These interpolation weights and 1. Merge individual -best lists to form a large -best list with unique hypotheses; 231 the system weights are constrained to sum to one. The number of tunable combination weights, in addition to normal decoder weights, is where is the number of systems and is the number of similarity levels; i.e., free system weights, similarity score weights and two free interpolation weights. 4.2 Phrase-Based Decoding The phrasal decoder used in the phrase-level combination is based on standard beam search (Koehn, 2004). The decoder features are: a trigram language model score, number of target phrases, number of target words, phrase distortion, phrase distortion computed over the original translations and phrase translation confidences estimated in Section 4.1. The total score for a hypothesis is computed as a log-linear combination of these features. The feature weights and combination weights (system and similarity) may be tuned using Powell’s method on -best lists as described in Section 2. The phrase-level combination tuning can be summarized as follows: 1. Estimate sentence posteriors given the total s</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proc. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
<author>Andreas Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>400</pages>
<contexts>
<context position="2296" citStr="Mangu et al., 2000" startWordPosition="319" endWordPosition="322">n phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This requires aligning the system outputs to form a consensus network and – during decoding – simply finding the highest scoring path through this network. The alignment of speech recognition outputs is fairly straightforward due to the strict constraint in word order. However, machine translation outputs do not have this constraint as the word order may be different between the source and target languages. MT systems employ various re-</context>
<context position="16123" citStr="Mangu et al., 2000" startWordPosition="2582" endWordPosition="2585">code the source sentences using the current weights to generate an -best list; 5. Estimate new decoder and combination weights as described above. Testing the phrase-level combination is performed by following steps 1 through 4. 5 Word-Level Combination The third combination method is based on confusion network decoding. In confusion network decoding, the words in all hypotheses are aligned against each other to form a graph with word alternatives (including nulls) for each alignment position. Each aligned word is assigned a score relative to the votes or word confidence scores (Fiscus, 1997; Mangu et al., 2000) derived from the hypotheses. The decoding is carried out by picking the words with the highest scores along the graph. In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim et al., 2007). In machine translation, aligning hypotheses is more complicated compared to speech recognition since the target words do not necessarily appear in the same order. So far, confusion networks have been applied in MT system combination using three di</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373– 400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1923" citStr="Matusov et al., 2006" startWordPosition="266" endWordPosition="269"> machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This requires aligning the system outputs to form a consensus netwo</context>
<context position="16823" citStr="Matusov et al., 2006" startWordPosition="2691" endWordPosition="2694">with the highest scores along the graph. In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim et al., 2007). In machine translation, aligning hypotheses is more complicated compared to speech recognition since the target words do not necessarily appear in the same order. So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007). WER alignments do not allow shifts, GIZA++ alignments require careful training and are not always reliable. TER alignments do not guarantee that similar but lexically different words are aligned correctly but TER does not require training new models and allows shifts (Snover et al., 2006). This work extends the approach proposed in (Sim et al., 2007). 5.1 Confusion Network Generation Due to the varying word order in the MT hypotheses, the decision of confusion network skeleton is essential. The skeleton determines the general word order of the combined hypothesis. </context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proc. EACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12695" citStr="Och and Ney, 2003" startWordPosition="2031" endWordPosition="2034">using the new weights. Testing the sentence-level combination has the same steps as the tuning apart from all estimation steps; i.e., steps 1, 3, 5 and 7. 4 Phrase-Level Combination The phrase-level combination is based on extracting a new phrase translation table from each system’s target-to-source phrase alignments and re-decoding the source sentence using this new translation table and a language model. In this work, the target-tosource phrase alignments were available from the individual systems. If the alignments are not available, they can be automatically generated; e.g., using GIZA++ (Och and Ney, 2003). The phrase translation table is generated for each source sentence using confidence scores derived from sentence posteriors with system-specific total score scaling factors and similarity scores based on the agreement among the phrases from all systems. 4.1 Phrase Confidence Estimation Each phrase has an initial confidence based on the sentence posterior estimated from an -best list in the same fashion as in Section 3.2. The confidence of the phrase table entry is increased if several systems agree on the target words. The agreement is measured by four levels of similarity: 1. Same source in</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="7816" citStr="Och, 2003" startWordPosition="1218" endWordPosition="1219">uation metric in the re-scored -best list. The optimization starts at a random initial point in the -dimensional parameter space, first searching through an initial set of basis vectors. As searching repeatedly through the set of basis vectors is inefficient, the direction of 229 the vectors is gradually moved toward a larger positive change in the evaluation metric. To improve the chances of finding a global optimum, the algorithm is repeated with varying initial values. The modified Powell’s method has been previously used in optimizing the weights of a standard feature-based MT decoder in (Och, 2003) where a more efficient algorithm for log-linear models was proposed. However, this is specific to log-linear models and cannot be easily extended for more complicated functions. 3 Sentence-Level Combination The first combination method is based on re-ranking a merged -best list. A confidence score from each system is assigned to each unique hypothesis in the merged list. The confidence scores for each hypothesis are used to produce a single score which, combined with a 5-gram language model score, determines a new ranking of the hypotheses. 3.1 Hypothesis Confidence Estimation Generalized lin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6517" citStr="Papineni et al., 2002" startWordPosition="1009" endWordPosition="1012">ations of different evaluation metrics have been studied (Snover et al., 2006) but according to various internal HTER experiments it is not clear whether TER or BLEU correlates better. Therefore it is probably safest to try and not degrade either. The TER of a translation is computed as where is the total number of words in the reference translation . In the case of multiple references, the edits are counted against all references, is the average number of words in the reference translations and the final TER is computed using the minimum number of edits. The NIST BLEU-4 is a variant of BLEU (Papineni et al., 2002) and is computed as where is the precision of -grams in the hypothesis given the reference and is a brevity penalty. The -gram counts from multiple references are accumulated in estimating the precisions. All system combination methods presented in this paper may be tuned to directly optimize either one of these automatic evaluation metrics. The tuning uses -best lists of hypotheses with various feature scores. The feature scores may be combined with tunable weights forming an arbitrary scoring function. As the derivatives of this function are not usually available, Brent’s modification of Pow</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark J F Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="3944" citStr="Sim et al., 2007" startWordPosition="580" endWordPosition="583"> pages 228–235, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate - best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgroup portion of the GALE 2006 dryrun sets. The outputs were evaluated on both TER and BLEU. As t</context>
<context position="16474" citStr="Sim et al., 2007" startWordPosition="2640" endWordPosition="2643"> the words in all hypotheses are aligned against each other to form a graph with word alternatives (including nulls) for each alignment position. Each aligned word is assigned a score relative to the votes or word confidence scores (Fiscus, 1997; Mangu et al., 2000) derived from the hypotheses. The decoding is carried out by picking the words with the highest scores along the graph. In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim et al., 2007). In machine translation, aligning hypotheses is more complicated compared to speech recognition since the target words do not necessarily appear in the same order. So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007). WER alignments do not allow shifts, GIZA++ alignments require careful training and are not always reliable. TER alignments do not guarantee that similar but lexically different words are aligned correctly but TER does not </context>
<context position="18055" citStr="Sim et al., 2007" startWordPosition="2902" endWordPosition="2905">d be to use the output from the system with the best performance on some development set. However, it was found that this approach did not always yield better combination output compared to the best single system on all evaluation metrics. Instead of using a single system output as the skeleton, the hypothesis that best agrees with the other hypotheses on average may be used. In this paper, the minimum average TER score of one hypothesis against all other hypotheses was used as follows (8) This may be viewed as the MBR hypothesis under TER given uniform target sentence posterior distribution (Sim et al., 2007). It is also possible to compute the MBR hypothesis under BLEU. 232 Finding the MBR hypothesis requires computing the TER against all hypotheses to be aligned. It was found that aligning more than one hypothesis ( ) from each system to the skeleton improves the combination outputs. However, only the rank-1 hypotheses were considered as skeletons due to the complexity of the TER alignment. The confidence score assigned to each word was chosen to be where the was based on the rank of the aligned hypothesis in the system’s - best. This was found to yield better scores than simple votes. 5.2 Tunab</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J. Byrne, Mark J.F. Gales, Hichem Sahbi, and Phil C. Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manhung Siu</author>
<author>Herbert Gish</author>
</authors>
<title>Evaluation of word confidence for speech recognition systems.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="8520" citStr="Siu and Gish, 1999" startWordPosition="1328" endWordPosition="1331">s specific to log-linear models and cannot be easily extended for more complicated functions. 3 Sentence-Level Combination The first combination method is based on re-ranking a merged -best list. A confidence score from each system is assigned to each unique hypothesis in the merged list. The confidence scores for each hypothesis are used to produce a single score which, combined with a 5-gram language model score, determines a new ranking of the hypotheses. 3.1 Hypothesis Confidence Estimation Generalized linear models (GLMs) have been applied for confidence estimation in speech recognition (Siu and Gish, 1999). The logit model, which models the log odds of an event as a linear function of the features, can be used in confidence estimation. The confidence for a system generating a hypothesis may be modeled as (3) where each system has weights , and is the th feature for system and hypothesis . The features used in this work were: 1. Rank in the system’s -best list; 2. Sentence posterior with system-specific total score scaling factors; 3. System’s total score; 4. Number of words in the hypothesis; 5. System-specific bias. If the system did not generate the hypothesis , the confidence is set to zero.</context>
</contexts>
<marker>Siu, Gish, 1999</marker>
<rawString>Manhung Siu and Herbert Gish. 1999. Evaluation of word confidence for speech recognition systems. Computer Speech and Language, 13(4):299–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciula</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. AMTA.</booktitle>
<contexts>
<context position="3851" citStr="Snover et al., 2006" startWordPosition="563" endWordPosition="566">e phrase and word-level methods. The phrase-level combination 228 Proceedings of NAACL HLT 2007, pages 228–235, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate - best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgrou</context>
<context position="5449" citStr="Snover et al., 2006" startWordPosition="821" endWordPosition="824">ion technique used in tuning of the various system combination weights. Sentence, phrase and word-level system combination methods are presented in Sections 3, 4 and 5. Experimental results on Arabic and Chinese to English newswire and newsgroup test data are presented in Section 6. 2 Evaluation Metrics and Discriminative Tuning The official metric of the 2006 DARPA GALE evaluation was human-mediated translation edit rate (HTER). HTER is computed as the minimum translation edit rate (TER) between a system output and a targeted reference which preserves the meaning and fluency of the sentence (Snover et al., 2006). The targeted reference is generated by human posteditors who make edits to a reference translation so as to minimize the TER between the reference and the MT output without changing the meaning of the reference. Computing the HTER is very time consuming due to the human post-editing. It is desirable to have an automatic evaluation metric that correlates well with the HTER to allow fast evaluation of the MT systems during development. Correlations of different evaluation metrics have been studied (Snover et al., 2006) but according to various internal HTER experiments it is not clear whether </context>
<context position="17141" citStr="Snover et al., 2006" startWordPosition="2745" endWordPosition="2748">s more complicated compared to speech recognition since the target words do not necessarily appear in the same order. So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007). WER alignments do not allow shifts, GIZA++ alignments require careful training and are not always reliable. TER alignments do not guarantee that similar but lexically different words are aligned correctly but TER does not require training new models and allows shifts (Snover et al., 2006). This work extends the approach proposed in (Sim et al., 2007). 5.1 Confusion Network Generation Due to the varying word order in the MT hypotheses, the decision of confusion network skeleton is essential. The skeleton determines the general word order of the combined hypothesis. One option would be to use the output from the system with the best performance on some development set. However, it was found that this approach did not always yield better combination output compared to the best single system on all evaluation metrics. Instead of using a single system output as the skeleton, the hy</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciula, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciula, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. AMTA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>