<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013843">
<title confidence="0.99486">
Automated Scoring Using A Hybrid Feature Identification Technique
</title>
<author confidence="0.981369">
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lut
</author>
<affiliation confidence="0.861923666666667">
Martin Chodorowt, Lisa Braden-Hardertt, and Mary Dee Harristtt
tEducational Testing Service, Princeton NJ, tHunter College, New York City, NY,
ttButler-Hill Group, Reston, VA, and tttLanguage Technology, Inc, Austin, TX
</affiliation>
<sectionHeader confidence="0.981969" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878210526316">
This study exploits statistical redundancy
inherent in natural language to automatically
predict scores for essays. We use a hybrid
feature identification method, including
syntactic structure analysis, rhetorical structure
analysis, and topical analysis, to score essay
responses from test-takers of the Graduate
Management Admissions Test (GMAT) and
the Test of Written English (TWE). For each
essay question, a stepwise linear regression
analysis is run on a training set (sample of
human scored essay responses) to extract a
weighted set of predictive features for each test
question. Score prediction for cross-validation
sets is calculated from the set of predictive
features. Exact or adjacent agreement between
the Electronic Essay Rater (e-rater) score
predictions and human rater scores ranged from
87% to 94% across the 15 test questions.
</bodyText>
<sectionHeader confidence="0.99836" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999976659090909">
This paper describes the development and
evaluation of a prototype system designed for
the purpose of automatically scoring essay
responses. The paper reports on evaluation
results from scoring 13 sets of essay data from
the Analytical Writing Assessments of the
Graduate Management Admissions Test
(GMAT) (see the GMAT Web site at
http://www.gmat.org/ for sample questions)
and 2 sets of essay data from the Test of Written
English (TWE) (see http://www.toefl.org/
tstprpmt.html for sample TWE questions).
Electronic Essay Rater (e-rater) was designed to
automatically analyze essay features based on
writing characteristics specified at each of six
score points in the scoring guide used by human
raters for manual scoring (also available at
http://www.gmat.org/). The scoring guide
indicates that an essay that stays on the topic of
the question has a strong, coherent and well-
organized argument structure, and displays a
variety of word use and syntactic structure will
receive a score at the higher end of the six-point
scale (5 or 6). Lower scores are assigned to
essays as these characteristics diminish.
One of our main goals was to design a system that
could score an essay based on features specified
in the scoring guide for manual scoring. E-rater
features include rhetorical structure, syntactic
structure, and topical analysis. For each essay
question, a stepwise linear regression analysis is
run on a set of training data (human-scored
essay responses) to extract a weighted set of
predictive features for each test question.
Final score prediction for cross-validation uses
the weighted predictive feature set identified
during training. Score prediction accuracy is
determined by measuring agreement between
human rater scores and e-rater score
predictions. In accordance with human
interrater &amp;quot;agreement&amp;quot; standards, human and e-
rater scores also &amp;quot;agree&amp;quot; if there is an exact
match or if the scores differ by no more than
one point (adjacent agreement).
</bodyText>
<page confidence="0.999092">
206
</page>
<sectionHeader confidence="0.962612" genericHeader="method">
2. Hybrid Feature Methodology
</sectionHeader>
<bodyText confidence="0.999869222222222">
E-rater uses a hybrid feature methodology that
incorporates several variables either derived
statistically, or extracted through NLP
techniques. The final linear regression model
used for predicting scores includes syntactic,
rhetorical and topical features. The next three
sections present a conceptual rationale and a
description of feature identification in essay
responses.
</bodyText>
<subsectionHeader confidence="0.988932">
2.1 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999991210526316">
The scoring guides indicate that one feature
used to evaluate an essay is syntactic variety.
All sentences in the essays were parsed using
the Microsoft Natural Language Processing
tool (MSNLP) (see MSNLP (1997)) so that
syntactic structure information could be
accessed. The identification of syntactic
structures in essay responses yields information
about the syntactic variety in an essay with
regard to the identification of clause or verb
types.
A program was implemented to identify the
number of complement clauses, subordinate
clauses, infinitive clauses, relative clauses and
occurrences of the subjunctive modal auxiliary
verbs, would, could, should, might and may, for
each sentence in an essay. Ratios of syntactic
structure types per essay and per sentence were
also used as measures of syntactic variety.
</bodyText>
<subsectionHeader confidence="0.99988">
2.2 Rhetorical Structure Analysis
</subsectionHeader>
<bodyText confidence="0.999236396551725">
GMAT essay questions are of two types:
Analysis of an Issue (issue) and Analysis of an
Argunzent (argument). The GMAT issue essay
asks the writer to respond to a general question
and to provide &amp;quot;reasons and/or examples&amp;quot; to
support his or her position on an issue
introduced by the test question. The GMAT
argument essay focuses the writer on the
argument in a given piece of text, using the
term argument in the sense of a rational
presentation of points with the purpose of
persuading the reader. The scoring guides
indicate that an essay will receive a score based
on the examinee&apos;s demonstration of a well-
developed essay. In this study, we try to identify
organization of an essay through automated
analysis and identification of the rhetorical (or
argument) structure of the essay.
Argument structure in the rhetorical sense may
or may not correspond to paragraph divisions.
One can make a point in a phrase, a sentence,
two or three sentences, a paragraph, and so on.
For automated argument identification, e-rater
identifies &apos;rhetorical&apos; relations, such as
Parallelism and Contrast that can appear at
almost any level of discourse. This is part of the
reason that human readers must also rely on cue
words to identify new arguments in an essay.
Literature in the field of discourse analysis
supports our approach. It points out that
rhetorical cue words and structures can be
identified and used for computer-based
discourse analysis (Cohen (1984), (Mann and
Thompson (1988), Hovy, et al (1992),
Hirschberg and Litman (1993), Vander Linden
and Martin (1995), and Knott (1996)). E-rater
follows this approach by using rhetorical cue
words and structure features, in addition to other
topical and syntactic information. We adapted
the conceptual framework of conjunctive
relations from Quirk, et al (1985) in which cue
terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In
conclusion,&amp;quot; are classified as conjuncts used for
summarizing. Cue words such as &amp;quot;perhaps,&amp;quot;
and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot;
words used by the writer to express a belief in
developing an argument in the essay. Words
like &amp;quot;this&amp;quot; and &amp;quot;these&amp;quot; may often be used to flag
that the writer has not changed topics (Sidner
(1986)). We also observed that in certain
discourse contexts structures such as infinitive
clauses mark the beginning of a new argument.
E-rater&apos;s automated argument partitioning and
annotation program (APA) outputs an annotated
version of each essay in which the argument
units of the essays are labeled with regard to
their status as &amp;quot;marking the beginning of an
argument,&amp;quot; Or &amp;quot;marking argument
</bodyText>
<page confidence="0.988454">
207
</page>
<bodyText confidence="0.9997936">
development.&amp;quot; APA also outputs a version of
the essay that has been partitioned &amp;quot;by
argument&amp;quot;, instead of &amp;quot;by paragraph,&amp;quot; as it was
originally partitioned by the test-taker. APA
uses rules for argument annotation and
partitioning based on syntactic and paragraph-
based distribution of cue words, phrases and
structures to identify rhetorical structure.
Relevant cue words and terms are stored in a
cue word lexicon.
</bodyText>
<subsectionHeader confidence="0.99903">
2.3 Topical Analysis
</subsectionHeader>
<bodyText confidence="0.999990275">
Good essays are relevant to the assigned topic.
They also tend to use a more specialized and
precise vocabulary in discussing the topic than
poorer essays do. We should therefore expect
a good essay to resemble other good essays in
its choice of words and, conversely, a poor
essay to resemble other poor ones. E-rater
evaluates the lexical and topical content of an
essay by comparing the words it contains to the
words found in manually graded training
examples for each of the six score categories.
Two programs were implemented that compute
measures of content similarity, one based on
word frequency (EssayContent) and the other
on word weight (ArgContent), as in
information retrieval applications (Salton
(1988)).
In EssayContent. the vocabulary of each score
category is converted to a single vector whose
elements represent the total frequency of each
word in the training essays for that category. In
effect, this merges the essays for each score. (A
stop list of some function words is removed
prior to vector construction.) The system
computes cosine correlations between the
vector for a given test essay and the six vectors
representing the trained categories; the
category that is most similar to the test essay is
assigned as the evaluation of its content. An
advantage of using the cosine correlation is that
it is not sensitive to essay length, which may
vary considerably.
The other content similarity measure, is
computed separately by ArgContent for each
argument in the test essay and is based on the
kind of term weighting used in information
retrieval. For this purpose, the word frequency
vectors for the six score categories, described
above, are converted to vectors of word weights.
The weight for word i in score category s is:
</bodyText>
<equation confidence="0.998372">
wi.s =
(freq. / max_freqs) * log(n_essaystotal in_essaysi)
</equation>
<bodyText confidence="0.992200514285714">
where freq., is the frequency of word i in
category s, max_freq, is the frequency of the
most frequent word in s (after a stop list of
words has been removed), n_essaysto„,, is the
total number of training essays across all six
categories, and n_essaysi is the number of
training essays containing word i.
The first part of the weight formula represents
the prominence of word i in the score category,
and the second part is the log of the word&apos;s
inverse document frequency. For each argument
in the test essay, a vector of word weights is
also constructed. Each argument is evaluated by
computing cosine correlations between its
weighted vector and those of the six score
categories, and the most similar category is
assigned to the argument. As a result of this
analysis, c-rater has a set of scores (one per
argument) for each test essay.
In a preliminary study, we looked at how well
the minimum, maximum, mode, median, and
mean of the set of argument scores agreed with
the judgments of human raters for the essay as a
whole. The greatest agreement was obtained
from an adjusted mean of the argument scores
that compensated for an effect of the number of
arguments in the essay. For example, essays
which contained only one or two arguments
tended to receive slightly lower scores from the
human raters than the mean of the argument
scores, and essays which contained many
arguments tended to receive slightly higher
scores than the mean of the argument scores. To
compensate for this, an adjusted mean is used as
e-raters ArgContent,
</bodyText>
<page confidence="0.964658">
208
</page>
<equation confidence="0.8138045">
ArgContent =
(Iarg_scores + n_args) / (n_args + 1)
</equation>
<sectionHeader confidence="0.501883" genericHeader="method">
3. Training and Testing
</sectionHeader>
<bodyText confidence="0.989670115384615">
In all, e-raters syntactic, rhetorical, and topical
analyses yielded a total of 57 features for each
essay. The training sets for each test question
consisted of 5 essays for score 0, 15 essays for
score 1, and 50 essays each for scores 2
through 6. To predict the score assigned by
human raters, a stepwise linear regression
analysis was used to compute the optimal
weights for these predictors based on manually
scored training essays. For example, Figure 1,
below, shows the predictive feature set
generated for the ARG1 test question (see
results in Table 1). The predictive feature set
for ARG I illustrates how criteria specified for
manual scoring described earlier, such as
argument topic and development (using the
ArgContent score and argument development
terms), syntactic structure usage, and word
usage (using the EssayContent score), are
represented by e-rater. After training, c-rater
analyzed new test essays, and the regression
weights were used to combine the measures
into a predicted score for each one. This
prediction was then compared to the scores
assigned by two human raters to check for
exact or adjacent agreement.
</bodyText>
<listItem confidence="0.931174461538462">
I. ArgContent Score
2. EssavContent Score
3. Total Argument Development
Words/Phrases
4. Total Pronouns Beginning Arguments
5. Total Complement Clauses Beginning
Arguments
6. Total Summary Words Beginning
Arguments
7. Total Detail Words Beginning Arguments
8. Total Rhetorical Words Developing
Arguments
9. Subjunctive Modal Verbs
</listItem>
<figureCaption confidence="0.981518">
Figure 1: Predictive Feature Set for
</figureCaption>
<sectionHeader confidence="0.445409" genericHeader="method">
ARG1 Test Question
</sectionHeader>
<subsectionHeader confidence="0.970916">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.998591388888889">
Table 1 shows the overall results for 8 GMAT
argument questions, 5 GMAT issue questions
and 2 TWE questions. There was an average of
638 response essays per test question. E-rater
and human rater mean agreement across the 15
data sets was 89%. In many cases, agreement
was as high as that found between the two
human raters.
The items that were tested represented a wide
variety of topics (see http://www.gmatorg/ for
GMAT sample questions and
http://www.toeitordistprpmt.html for sample TWE
questions). The data also represented a wide
variety of English writing competency. In fact,
the majority of test-takers from the 2 TWE data
sets were nonnative English speakers. Despite
these differences in topic and writing skill e-
rater performed consistently well across items.
</bodyText>
<tableCaption confidence="0.995339">
Table 1: Mean Percentage and Standard
</tableCaption>
<table confidence="0.926353333333333">
Deviation for E-rater (E) and Human Rater
(H) Agreement &amp; Human Interrater
Agreement For 15 Cross-Validation Tests
H1—H2 H1—E H2—E
Mean 90.4 89.1 89.0
S.D 2.1 2.3 7.7
</table>
<bodyText confidence="0.999869615384615">
To determine the features that were the most
reliable predictors of essay score, we examined
the regression models built during training. A
feature type was considered to be a reliable
predictor if it proved to be significant in at least
12 of the 15 regression analyses. Using this
criterion, the most reliable predictors were the
ArgContent and EssayContent scores, the
number of cue words or phrases indicating the
development of an argument, the number of
syntactic verb and clause types, and the number
of cue words or phrases indicating the beginning
of an argument.
</bodyText>
<page confidence="0.998184">
209
</page>
<sectionHeader confidence="0.988339" genericHeader="conclusions">
4. Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999965285714286">
This study shows how natural language
processing methods and statistical techniques
can be used for the evaluation of text. The
study indicates that rhetorical, syntactic, and
topical information can be automatically
extracted and used for machine-based score
prediction of essay responses. These three
types of information model features specified
in the manual scoring guides. This study also
shows that e-rater adapts well to many
different topical domains and populations of
test-takers.
The information used for automated score
prediction by e-rater can also be used as
building blocks for automated generation of
diagnostic and instructional summaries.
Clauses and sentences annotated by APA as
&amp;quot;the beginning of a new argument&amp;quot; might be
used to identify main points of an essay (Marcu
(1997)). In turn, identifying the main points in
the text of an essay could be used to generate
feedback reflecting essay topic and
organization. Other features could be used to
automatically generate statements that
explicate the basis on which e-rater generates
scores. Such statements could supplement
manually created qualitative feedback about an
essay.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="references">
6. References
</sectionHeader>
<reference confidence="0.998604923076924">
Cohen, Robin (1984). &amp;quot;A computational theory
of the function of clue words in argument
understanding.&amp;quot; In Proceedings of 1984
International Computational Linguistics
Conference. California, 251-255..
Hirschberg, Julia and Diane Litman (1993).
&amp;quot;Empirical Studies on the Disambiguation of
Cue Phrases.&amp;quot; Computational Linguistics
(19)3, 501-530.
Hovy, Eduard, Julia Lavid, Elisabeth Maier,
&amp;quot;Employing Knowledge Resources in a New
Text Planner Architecture,&amp;quot; In Aspects of
Automated NL Generation, Dale, Hovy, Rosner
and Stoch (Eds), Springer-Verlag Lecture Notes
in AT no. 587, 57-72.
GMAT (1997). http://www.gmat.org/
Knott, Alistair. (1996). &amp;quot;A Data-Driven
Methodology for Motivating a Set of Coherence
Relations.&amp;quot; Ph.D. Dissertation, available at
www.cogsci.edu.ac.uk/—alik/publications.html,
under the Heading, Unpublished Stuff.
Mann, William C. and Sandra A. Thompson
(1988). &amp;quot;Rhetorical Structure Theory: Toward a
functional theory of text organization.&amp;quot; Text
8(3), 243-281.
Marcu, Daniel. (1997). &amp;quot;From Discourse
Structures to Text Summaries.&amp;quot;, In Proceedings
of the Intelligent Scalable Text Summarization
Workshop, Association for Computational
Linguistics, Universidad Nacional de
Educacion a Distancia, Madrid, Spain.
MSNLP (1997) http://research.microsoft.com/n1p/
Quirk, Randolph, Sidney Greenbaum, Geoffrey
Leech, and Jan Svartik (1985). A
Comprehensive Grammar of the English
Language. Longman, New York.
Sidner, Candace. (1986). Focusing in the
Comprehension of Definite Anaphora. In
Readings in Natural Language Processing,
Barbara Grosz, Karen Sparck Jones, and Bonnie
Lynn Webber (Eds.), Morgan Kaufmann
Publishers, Los Altos, California, 363-394.
Salton, Gerard. (1988). Automatic text
processing • the transformation analysis and
retrieval of information by computer. Addison-
Wesley, Reading, Mass.
TOEFL (1997). http://www.toefl.org/tstprpmt.html
Vander Linden, Keith and James H. Martin
(1995). &amp;quot;Expressing Rhetorical Relations in
Instructional Text: A Case Study in Purpose
Relation.&amp;quot; Computational Linguistics 21(1), 29-
57.
</reference>
<page confidence="0.998963">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289453">
<title confidence="0.996961">Automated Scoring Using A Hybrid Feature Identification Technique</title>
<author confidence="0.815166333333333">Jill Burstein</author>
<author confidence="0.815166333333333">Karen Kukich</author>
<author confidence="0.815166333333333">Susanne Wolff</author>
<author confidence="0.815166333333333">Chi Lut Martin Chodorowt</author>
<author confidence="0.815166333333333">Lisa Braden-Hardertt</author>
<author confidence="0.815166333333333">Mary Dee Harristtt tEducational Testing Service</author>
<author confidence="0.815166333333333">Princeton NJ</author>
<author confidence="0.815166333333333">tHunter College</author>
<author confidence="0.815166333333333">New York City</author>
<author confidence="0.815166333333333">NY</author>
<email confidence="0.547248">ttButler-HillGroup,Reston,VA,andtttLanguageTechnology,Inc,Austin,TX</email>
<abstract confidence="0.9859691">This study exploits statistical redundancy inherent in natural language to automatically predict scores for essays. We use a hybrid feature identification method, including syntactic structure analysis, rhetorical structure analysis, and topical analysis, to score essay responses from test-takers of the Graduate Management Admissions Test (GMAT) and the Test of Written English (TWE). For each essay question, a stepwise linear regression analysis is run on a training set (sample of human scored essay responses) to extract a weighted set of predictive features for each test question. Score prediction for cross-validation sets is calculated from the set of predictive features. Exact or adjacent agreement between Electronic Essay Rater predictions and human rater scores ranged from 87% to 94% across the 15 test questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>A computational theory of the function of clue words in argument understanding.&amp;quot;</title>
<date>1984</date>
<booktitle>In Proceedings of 1984 International Computational Linguistics Conference.</booktitle>
<pages>251--255</pages>
<location>California,</location>
<contexts>
<context position="5926" citStr="Cohen (1984)" startWordPosition="897" endWordPosition="898">ot correspond to paragraph divisions. One can make a point in a phrase, a sentence, two or three sentences, a paragraph, and so on. For automated argument identification, e-rater identifies &apos;rhetorical&apos; relations, such as Parallelism and Contrast that can appear at almost any level of discourse. This is part of the reason that human readers must also rely on cue words to identify new arguments in an essay. Literature in the field of discourse analysis supports our approach. It points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (Cohen (1984), (Mann and Thompson (1988), Hovy, et al (1992), Hirschberg and Litman (1993), Vander Linden and Martin (1995), and Knott (1996)). E-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information. We adapted the conceptual framework of conjunctive relations from Quirk, et al (1985) in which cue terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In conclusion,&amp;quot; are classified as conjuncts used for summarizing. Cue words such as &amp;quot;perhaps,&amp;quot; and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot; words used by the writer to express a belief in developin</context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Cohen, Robin (1984). &amp;quot;A computational theory of the function of clue words in argument understanding.&amp;quot; In Proceedings of 1984 International Computational Linguistics Conference. California, 251-255..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical Studies on the Disambiguation of Cue Phrases.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>501--530</pages>
<contexts>
<context position="6003" citStr="Hirschberg and Litman (1993)" startWordPosition="907" endWordPosition="910"> a phrase, a sentence, two or three sentences, a paragraph, and so on. For automated argument identification, e-rater identifies &apos;rhetorical&apos; relations, such as Parallelism and Contrast that can appear at almost any level of discourse. This is part of the reason that human readers must also rely on cue words to identify new arguments in an essay. Literature in the field of discourse analysis supports our approach. It points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (Cohen (1984), (Mann and Thompson (1988), Hovy, et al (1992), Hirschberg and Litman (1993), Vander Linden and Martin (1995), and Knott (1996)). E-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information. We adapted the conceptual framework of conjunctive relations from Quirk, et al (1985) in which cue terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In conclusion,&amp;quot; are classified as conjuncts used for summarizing. Cue words such as &amp;quot;perhaps,&amp;quot; and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot; words used by the writer to express a belief in developing an argument in the essay. Words like &amp;quot;this&amp;quot; and &amp;quot;these&amp;quot; may often be used t</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Hirschberg, Julia and Diane Litman (1993). &amp;quot;Empirical Studies on the Disambiguation of Cue Phrases.&amp;quot; Computational Linguistics (19)3, 501-530.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eduard Hovy</author>
<author>Julia Lavid</author>
<author>Elisabeth Maier</author>
</authors>
<title>Employing Knowledge Resources in a New Text Planner Architecture,&amp;quot;</title>
<booktitle>In Aspects of Automated NL Generation, Dale, Hovy, Rosner and Stoch (Eds), Springer-Verlag Lecture Notes in AT no.</booktitle>
<volume>587</volume>
<pages>57--72</pages>
<marker>Hovy, Lavid, Maier, </marker>
<rawString>Hovy, Eduard, Julia Lavid, Elisabeth Maier, &amp;quot;Employing Knowledge Resources in a New Text Planner Architecture,&amp;quot; In Aspects of Automated NL Generation, Dale, Hovy, Rosner and Stoch (Eds), Springer-Verlag Lecture Notes in AT no. 587, 57-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GMAT</author>
</authors>
<date>1997</date>
<note>http://www.gmat.org/</note>
<marker>GMAT, 1997</marker>
<rawString>GMAT (1997). http://www.gmat.org/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.&amp;quot; Ph.D. Dissertation, available at www.cogsci.edu.ac.uk/—alik/publications.html, under the Heading, Unpublished Stuff.</title>
<date>1996</date>
<contexts>
<context position="6054" citStr="Knott (1996)" startWordPosition="917" endWordPosition="918">on. For automated argument identification, e-rater identifies &apos;rhetorical&apos; relations, such as Parallelism and Contrast that can appear at almost any level of discourse. This is part of the reason that human readers must also rely on cue words to identify new arguments in an essay. Literature in the field of discourse analysis supports our approach. It points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (Cohen (1984), (Mann and Thompson (1988), Hovy, et al (1992), Hirschberg and Litman (1993), Vander Linden and Martin (1995), and Knott (1996)). E-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information. We adapted the conceptual framework of conjunctive relations from Quirk, et al (1985) in which cue terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In conclusion,&amp;quot; are classified as conjuncts used for summarizing. Cue words such as &amp;quot;perhaps,&amp;quot; and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot; words used by the writer to express a belief in developing an argument in the essay. Words like &amp;quot;this&amp;quot; and &amp;quot;these&amp;quot; may often be used to flag that the writer has not changed topics (Sidn</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Knott, Alistair. (1996). &amp;quot;A Data-Driven Methodology for Motivating a Set of Coherence Relations.&amp;quot; Ph.D. Dissertation, available at www.cogsci.edu.ac.uk/—alik/publications.html, under the Heading, Unpublished Stuff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.&amp;quot;</title>
<date>1988</date>
<journal>Text</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<contexts>
<context position="5953" citStr="Mann and Thompson (1988)" startWordPosition="899" endWordPosition="902">o paragraph divisions. One can make a point in a phrase, a sentence, two or three sentences, a paragraph, and so on. For automated argument identification, e-rater identifies &apos;rhetorical&apos; relations, such as Parallelism and Contrast that can appear at almost any level of discourse. This is part of the reason that human readers must also rely on cue words to identify new arguments in an essay. Literature in the field of discourse analysis supports our approach. It points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (Cohen (1984), (Mann and Thompson (1988), Hovy, et al (1992), Hirschberg and Litman (1993), Vander Linden and Martin (1995), and Knott (1996)). E-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information. We adapted the conceptual framework of conjunctive relations from Quirk, et al (1985) in which cue terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In conclusion,&amp;quot; are classified as conjuncts used for summarizing. Cue words such as &amp;quot;perhaps,&amp;quot; and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot; words used by the writer to express a belief in developing an argument in the essay.</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, William C. and Sandra A. Thompson (1988). &amp;quot;Rhetorical Structure Theory: Toward a functional theory of text organization.&amp;quot; Text 8(3), 243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From Discourse Structures to Text Summaries.&amp;quot;,</title>
<date>1997</date>
<booktitle>In Proceedings of the Intelligent Scalable Text Summarization Workshop, Association for Computational Linguistics, Universidad Nacional de Educacion a Distancia,</booktitle>
<location>Madrid, Spain. MSNLP</location>
<note>http://research.microsoft.com/n1p/</note>
<marker>Marcu, 1997</marker>
<rawString>Marcu, Daniel. (1997). &amp;quot;From Discourse Structures to Text Summaries.&amp;quot;, In Proceedings of the Intelligent Scalable Text Summarization Workshop, Association for Computational Linguistics, Universidad Nacional de Educacion a Distancia, Madrid, Spain. MSNLP (1997) http://research.microsoft.com/n1p/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="6282" citStr="Quirk, et al (1985)" startWordPosition="948" endWordPosition="951">lso rely on cue words to identify new arguments in an essay. Literature in the field of discourse analysis supports our approach. It points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (Cohen (1984), (Mann and Thompson (1988), Hovy, et al (1992), Hirschberg and Litman (1993), Vander Linden and Martin (1995), and Knott (1996)). E-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information. We adapted the conceptual framework of conjunctive relations from Quirk, et al (1985) in which cue terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In conclusion,&amp;quot; are classified as conjuncts used for summarizing. Cue words such as &amp;quot;perhaps,&amp;quot; and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot; words used by the writer to express a belief in developing an argument in the essay. Words like &amp;quot;this&amp;quot; and &amp;quot;these&amp;quot; may often be used to flag that the writer has not changed topics (Sidner (1986)). We also observed that in certain discourse contexts structures such as infinitive clauses mark the beginning of a new argument. E-rater&apos;s automated argument partitioning and annotation program (APA) outputs an annota</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartik, 1985</marker>
<rawString>Quirk, Randolph, Sidney Greenbaum, Geoffrey Leech, and Jan Svartik (1985). A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace Sidner</author>
</authors>
<title>Focusing in the Comprehension of Definite Anaphora.</title>
<date>1986</date>
<booktitle>In Readings in Natural Language Processing,</booktitle>
<pages>363--394</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<institution>Grosz, Karen Sparck Jones, and Bonnie Lynn Webber (Eds.),</institution>
<location>Barbara</location>
<contexts>
<context position="6663" citStr="Sidner (1986)" startWordPosition="1016" endWordPosition="1017">996)). E-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information. We adapted the conceptual framework of conjunctive relations from Quirk, et al (1985) in which cue terms, such as &amp;quot;In summary&amp;quot; and &amp;quot;In conclusion,&amp;quot; are classified as conjuncts used for summarizing. Cue words such as &amp;quot;perhaps,&amp;quot; and &amp;quot;possibly&amp;quot; are considered to be &amp;quot;belief&amp;quot; words used by the writer to express a belief in developing an argument in the essay. Words like &amp;quot;this&amp;quot; and &amp;quot;these&amp;quot; may often be used to flag that the writer has not changed topics (Sidner (1986)). We also observed that in certain discourse contexts structures such as infinitive clauses mark the beginning of a new argument. E-rater&apos;s automated argument partitioning and annotation program (APA) outputs an annotated version of each essay in which the argument units of the essays are labeled with regard to their status as &amp;quot;marking the beginning of an argument,&amp;quot; Or &amp;quot;marking argument 207 development.&amp;quot; APA also outputs a version of the essay that has been partitioned &amp;quot;by argument&amp;quot;, instead of &amp;quot;by paragraph,&amp;quot; as it was originally partitioned by the test-taker. APA uses rules for argument ann</context>
</contexts>
<marker>Sidner, 1986</marker>
<rawString>Sidner, Candace. (1986). Focusing in the Comprehension of Definite Anaphora. In Readings in Natural Language Processing, Barbara Grosz, Karen Sparck Jones, and Bonnie Lynn Webber (Eds.), Morgan Kaufmann Publishers, Los Altos, California, 363-394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic text processing • the transformation analysis and retrieval of information by computer.</title>
<date>1988</date>
<publisher>AddisonWesley,</publisher>
<location>Reading, Mass.</location>
<contexts>
<context position="8212" citStr="Salton (1988)" startWordPosition="1260" endWordPosition="1261">bulary in discussing the topic than poorer essays do. We should therefore expect a good essay to resemble other good essays in its choice of words and, conversely, a poor essay to resemble other poor ones. E-rater evaluates the lexical and topical content of an essay by comparing the words it contains to the words found in manually graded training examples for each of the six score categories. Two programs were implemented that compute measures of content similarity, one based on word frequency (EssayContent) and the other on word weight (ArgContent), as in information retrieval applications (Salton (1988)). In EssayContent. the vocabulary of each score category is converted to a single vector whose elements represent the total frequency of each word in the training essays for that category. In effect, this merges the essays for each score. (A stop list of some function words is removed prior to vector construction.) The system computes cosine correlations between the vector for a given test essay and the six vectors representing the trained categories; the category that is most similar to the test essay is assigned as the evaluation of its content. An advantage of using the cosine correlation </context>
</contexts>
<marker>Salton, 1988</marker>
<rawString>Salton, Gerard. (1988). Automatic text processing • the transformation analysis and retrieval of information by computer. AddisonWesley, Reading, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TOEFL</author>
</authors>
<title>Expressing Rhetorical Relations in Instructional Text: A Case Study in Purpose Relation.&amp;quot;</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<issue>1</issue>
<pages>29--57</pages>
<note>http://www.toefl.org/tstprpmt.html</note>
<marker>TOEFL, 1997</marker>
<rawString>TOEFL (1997). http://www.toefl.org/tstprpmt.html Vander Linden, Keith and James H. Martin (1995). &amp;quot;Expressing Rhetorical Relations in Instructional Text: A Case Study in Purpose Relation.&amp;quot; Computational Linguistics 21(1), 29-57.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>