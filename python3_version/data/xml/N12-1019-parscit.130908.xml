<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000953">
<title confidence="0.982978">
Re-examining Machine Translation Metrics for Paraphrase Identification
</title>
<author confidence="0.968523">
Nitin Madnani Joel Tetreault
</author>
<affiliation confidence="0.856075">
Educational Testing Service
</affiliation>
<address confidence="0.880697">
Princeton, NJ, USA
</address>
<email confidence="0.987411">
{nmadnani,jtetreault}@ets.org
</email>
<author confidence="0.994387">
Martin Chodorow
</author>
<affiliation confidence="0.986723">
Hunter College of CUNY
</affiliation>
<address confidence="0.909578">
New York, NY, USA
</address>
<email confidence="0.999623">
martin.chodorow@hunter.cuny.edu
</email>
<sectionHeader confidence="0.995654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999396631578948">
We propose to re-examine the hypothesis that
automated metrics developed for MT evalu-
ation can prove useful for paraphrase iden-
tification in light of the significant work on
the development of new MT metrics over the
last 4 years. We show that a meta-classifier
trained using nothing but recent MT metrics
outperforms all previous paraphrase identifi-
cation approaches on the Microsoft Research
Paraphrase corpus. In addition, we apply our
system to a second corpus developed for the
task of plagiarism detection and obtain ex-
tremely positive results. Finally, we conduct
extensive error analysis and uncover the top
systematic sources of error for a paraphrase
identification approach relying solely on MT
metrics. We release both the new dataset and
the error analysis annotations for use by the
community.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973957446808">
One of the most important reasons for the recent
advances made in Statistical Machine Translation
(SMT) has been the development of automated met-
rics for evaluation of translation quality. The goal
of any such metric is to assess whether the trans-
lation hypothesis produced by a system is seman-
tically equivalent to the source sentence that was
translated. However, cross-lingual semantic equiv-
alence is even harder to assess than monolingual,
therefore, most MT metrics instead try to measure
whether the hypothesis is semantically equivalent to
a human-authored reference translation of the same
source sentence. Using such automated metrics as
proxies for human judgments can provide a quick as-
sessment of system performance and allow for short
feature and system development cycles, which are
important for evaluating research ideas.
In the last 5 years, several shared tasks and com-
petitions have led to the development of increasingly
sophisticated metrics that go beyond the computa-
tion of n-gram overlaps (BLEU, NIST) or edit dis-
tances (TER, WER, PER etc.). Note that the task
of an MT metric is essentially one of identifying
whether the translation produced by a system is a
paraphrase of the reference translation. Although
the notion of using MT metrics for the task of para-
phrase identification is not novel (Finch et al., 2005;
Wan et al., 2006), it merits a re-examination in the
light of the development of these novel MT metrics
for which we can ask “How much better, if at all,
do these newer metrics perform for the task of para-
phrase identification?”
This paper describes such a re-examination. We
employ 8 different MT metrics for identifying
paraphrases across two different datasets - the
well-known Microsoft Research paraphrase corpus
(MSRP) (Dolan et al., 2004) and the plagiarism
detection corpus (PAN) from the 2010 Uncovering
Plagiarism, Authorship and Social Software Misuse
shared task (Potthast et al., 2010). We include both
MSRP and PAN in our study because they represent
two very different sources of paraphrased text. The
creation of MSRP relied on the massive redundancy
of news articles on the web and extracted senten-
tial paraphrases from different stories written about
the same topic. In the case of PAN, humans con-
sciously paraphrased existing text to generate new,
</bodyText>
<page confidence="0.648400333333333">
182
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.997986933333333">
plagiarized text.
In the next section, we discuss previous work on
paraphrase identification. In §3, we describe our ap-
proach to paraphrase identification using MT met-
rics as features. Our approach yields impressive re-
sults – the current state of the art for MSRP and ex-
tremely positive for PAN. In the same section, we
examine whether each metric’s purported strength is
demonstrated in our datasets. Next, in §4 we con-
duct an analysis of our system’s misclassifications
for both datasets and outline a taxonomy of errors
that our system makes. We also look at annotation
errors in the datasets themselves. We discuss the
findings of the error analysis in §5 and conclude in
§6.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="related work">
2 Related Work &amp; Our Contributions
</sectionHeader>
<bodyText confidence="0.9998888">
Our goal in this paper is to examine the utility of a
paraphrase identification approach that relies solely
on MT evaluation metrics and no other evidence of
semantic equivalence. Given this setup, the most rel-
evant previous work is by Finch et al. (2005) which
uses BLEU, NIST, WER and PER as features for
a supervised classification approach using SVMs.
In addition, they also incorporate part-of-speech in-
formation as well as the Jiang-Conrath WordNet-
based lexical relatedness measure (Jiang and Con-
rath, 1997) into their edit distance calculations. In
the first part of our paper, we present classification
experiments with newer MT metrics not available in
2005, a worthwhile exercise in itself. However, we
go much further in our study:
</bodyText>
<listItem confidence="0.998688461538461">
• We apply our approach to two different para-
phrase datasets (MSRP and PAN) that were cre-
ated via different processes.
• We attempt to find evidence of each metric’s
purported strength in both datasets.
• We conduct an extensive error analysis to find
types of errors that a system based solely on
MT metrics is likely to make. In addition, we
also discover interesting paraphrase pairs in the
datasets.
• We release our sentence-level PAN dataset (see
§3.3.2) which contains more realistic exam-
ples of paraphrase and can prove useful to the
</listItem>
<bodyText confidence="0.997454564102564">
community for future evaluations of paraphrase
identification.
BLEU-based features were also employed by
Wan et al. (2006) who use them in combination with
several other features based on dependency relations
and tree edit-distance inside an SVM.
There are several other supervised approaches to
paraphrase identification that do not use any features
based on MT metrics. Mihalcea et al. (2006) com-
bine pointwise mutual information, latent semantic
analysis and WordNet-based measures of word se-
mantic similarity into an arbitrary text-to-text sim-
ilarity metric. Qiu et al. (2006) build a frame-
work that detects dissimilarities between sentences
and makes its paraphrase judgment based on the
significance of such dissimilarities. Kozareva and
Montoyo (2006) use features based on LCS, skip
n-grams and WordNet with a meta-classifier com-
posed of SVM, k-nearest neighbor and maximum
entropy classifiers. Islam and Inkpen (2007) mea-
sure semantic similarity using a corpus-based mea-
sure and a modified version of the Longest Common
Subsequence (LCS) algorithm. Rus et al. (2008)
take a graph-based approach originally developed
for recognizing textual entailment and adapt it for
paraphrase identification. Fernando and Stevenson
(2008) construct a matrix of word similarities be-
tween all pairs of words in both sentences instead
of relying only on the maximal similarities. Das and
Smith (2009) use an explicit model of alignment be-
tween the corresponding parts of two paraphrastic
sentences and combine it with a logistic regression
classifier built from n-gram overlap features. Most
recently, Socher et al. (2011) employ a joint model
that incorporates the similarities between both sin-
gle word features as well as multi-word phrases ex-
tracted from the parse trees of the two sentences.
We compare our results to those from all the ap-
proaches described in this section later in §3.4.
</bodyText>
<sectionHeader confidence="0.825757" genericHeader="method">
3 Classifying with MT Metrics
</sectionHeader>
<bodyText confidence="0.9997304">
In this section, we first describe our overall approach
to paraphrase identification that utilizes only MT
metrics. We then discuss the actual MT metrics we
used. Finally, we describe the datasets on which we
evaluated our approach and present our results.
</bodyText>
<page confidence="0.988255">
183
</page>
<bodyText confidence="0.998905625">
MSRP They had published an advertisement on the Internet on June 10,
offering the cargo for sale, he added.
On June 10, the ship’s owners had published an advertisement on the
Internet, offering the explosives for sale.
Security lights have also been installed and police have swept
the grounds for booby traps.
Security lights have also been installed on a barn near the front gate.
PAN Dense fogs wrapped the mountains that shut in the little hamlet,
but overhead the stars were shining in the near heaven.
The hamlet is surrounded by mountains which is wrapped with dense
fogs, though above it, near heaven, the stars were shining.
In still other places, the strong winds carry soil over long
distances to be mixed with other soils.
In other places, where strong winds blow with frequent regularity,
sharp soil grains are picked up by the air and hurled against the
rocks, which, under this action, are carved into fantastic forms.
</bodyText>
<tableCaption confidence="0.998455">
Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora.
</tableCaption>
<subsectionHeader confidence="0.990818">
3.1 Classifier
</subsectionHeader>
<bodyText confidence="0.999966181818182">
Our best system utilized a classifier combination ap-
proach. We used a simple meta-classifier that uses
the average of the unweighted probability estimates
from the constituent classifiers to make its final de-
cision. We used three constituent classifiers: Logis-
tic regression, the SMO implementation of a support
vector machine (Platt, 1999; Keerthi et al., 2001)
and a lazy, instance-based classifier that extends the
nearest neighbor algorithm (Aha et al., 1991). We
used the WEKA machine learning toolkit to perform
our experiments (Hall et al., 2009). 1
</bodyText>
<subsectionHeader confidence="0.997437">
3.2 MT metrics used
</subsectionHeader>
<bodyText confidence="0.995083133333333">
1. BLEU (Papineni et al., 2002) is the most com-
monly used metric for MT evaluation. It is
computed as the amount of n-gram overlap—
for different values of n—between the system
output and the reference translation, tempered
by a penalty for translations that might be too
short. BLEU relies on exact matching and has
no concept of synonymy or paraphrasing. We
use BLEU1 through BLEU4 as 4 different fea-
1These constituent classifiers were chosen since they were
the top 3 performers in 5-fold cross-validation experiments
conducted on both MSRP and PAN training sets. The meta-
classifier was chosen similarly once the constituent classifiers
had been chosen.
tures for our classifier (hereafter BLEU(1-4)).
</bodyText>
<listItem confidence="0.98719892">
2. NIST (Doddington, 2002) is a variant of BLEU
that uses the arithmetic mean of n-gram over-
laps, rather than the geometric mean. It also
weights each n-gram according to its informa-
tiveness as indicated by its frequency. We use
NIST1 through NIST5 as 5 different features
for our classifier (hereafter NIST(1-5)).
3. TER (Snover et al., 2006) is defined as the
number of edits needed to “fix” the translation
output so that it matches the reference. TER
differs from WER in that it includes a heuris-
tic algorithm to deal with shifts in addition to
insertions, deletions and substitutions.
4. TERp (TER-Plus) (Snover et al., 2009) builds
upon the core TER algorithm by providing ad-
ditional edit operations based on stemming,
synonymy and paraphrase.
5. METEOR (Denkowski and Lavie, 2010) uses
a combination of both precision and recall un-
like BLEU which focuses on precision. Fur-
thermore, it incorporates stemming, synonymy
(via WordNet) and paraphrase (via a lookup ta-
ble).
6. SEPIA (Habash and El Kholy, 2008) is a
syntactically-aware metric designed to focus on
</listItem>
<page confidence="0.995645">
184
</page>
<bodyText confidence="0.942661708333333">
structural n-grams with long surface spans that
cannot be captured efficiently with surface n-
gram metrics. Like BLEU, it is a precision-
based metric and requires a length penalty to
minimize the effects of length.
7. BADGER (Parker, 2008) is a language inde-
pendent metric based on compression and in-
formation theory. It computes a compression
distance between the two sentences that utilizes
the Burrows Wheeler Transformation (BWT).
The BWT enables taking into account common
sentence contexts with no limit on the size of
these contexts.
8. MAXSIM (Chan and Ng, 2008) treats the
problem as one of bipartite graph matching and
maps each word in one sentence to at most one
word in the other sentence. It allows the use of
arbitrary similarity functions between words.2
Our choice of metrics was based on their popular-
ity in the MT community, their performance in open
competitions such as the NIST MetricsMATR chal-
lenge (NIST, 2008) and the WMT shared evaluation
task (Callison-Burch et al., 2010), their availability,
and their relative complementarity.
</bodyText>
<subsectionHeader confidence="0.989313">
3.3 Datasets
</subsectionHeader>
<bodyText confidence="0.999946">
In this section, we describe the two datasets that we
used to evaluate our approach.
</bodyText>
<subsectionHeader confidence="0.926721">
3.3.1 Microsoft Research Paraphrase Corpus
</subsectionHeader>
<bodyText confidence="0.999793352941176">
The MSRP corpus was created by mining news
articles on the web for topically similar articles and
then extracting potential sentential paraphrases us-
ing a set of heuristics. Extracted pairs were then
shown to two human judges with disagreements
handled by a third adjudicator. The kappa was re-
ported as 0.62, which indicates moderate to high
agreement. We used the pre-stipulated train-test
splits (4,076 sentence pairs in training and 1,725 in
test) to train and test our classifier.
2We also experimented with TESLA—a variant of
MAXSIM that performs better for MT evaluation—in our pre-
liminary experiments However, both MAXSIM and TESLA
performed almost identically in our cross-validation experi-
ments. Therefore, we only retained MAXSIM in our final ex-
periment since it was significantly faster to run than the version
of TESLA we had.
</bodyText>
<subsectionHeader confidence="0.862653">
3.3.2 Plagiarism Detection Corpus (PAN)
</subsectionHeader>
<bodyText confidence="0.999857972222222">
We wanted to evaluate our approach on a set of
paraphrases where the semantic similarity was not
simply an accidental by-product of topical similarity
but rather consciously generated. We used the test
collection from the PAN 2010 plagiarism detection
competition. This dataset consists of 41,233 text
documents from Project Gutenberg in which 94,202
cases of plagiarism have been inserted. The pla-
giarism was created either by using an algorithm or
by explicitly asking Turkers to paraphrase passages
from the original text. We focus only on the human-
created plagiarism instances.
Note also that although the original PAN dataset
has been used in plagiarism detection shared tasks,
those tasks are generally formulated differently in
that the goal is to find all potentially plagiarized pas-
sages in a given set of documents along with the cor-
responding source passages from other documents.
In this paper, we wanted to focus on the task of iden-
tifying whether two given sentences can be consid-
ered paraphrases.
To generate a sentence-level PAN dataset, we
wrote a heuristic alignment algorithm to find cor-
responding pairs of sentences within a passage pair
linked by the plagiarism relationship. The align-
ment algorithm utilized only bag-of-words overlap
and length ratios and no MT metrics. For our nega-
tive evidence, we sampled sentences from the same
document and extracted sentence pairs that have at
least 4 content words in common. We then sampled
randomly from both the positive and negative evi-
dence files to create a training set of 10,000 sentence
pairs and a test set of 3,000 sentence pairs.
Table 1 shows examples of paraphrastic and non-
paraphrastic sentence pairs from both the MSRP and
PAN datasets.
</bodyText>
<sectionHeader confidence="0.837502" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999483375">
Before presenting the results of experiments that
used multiple metrics as features, we wanted to de-
termine how well each metric performs on its own
when used for paraphrase identification. Table 2
shows the classification results on both the MSRP
and PAN datasets using each metric as the only fea-
ture. Although previously explored metrics such as
BLEU and NIST perform reasonably well, they are
</bodyText>
<page confidence="0.986608">
185
</page>
<table confidence="0.9997871">
Metric MSRP PAN
Acc. F1 Acc. F1
MAXSIM 67.2 79.4 84.7 83.4
BADGER 67.6 79.9 88.5 87.9
SEPIA 68.1 79.8 87.7 86.8
TER 69.9 80.9 85.7 83.8
BLEU(1-4) 72.3 80.9 87.9 87.1
NIST(1-5) 72.8 81.2 88.2 87.3
METEOR 73.1 81.0 89.5 88.9
TERp 74.3 81.8 91.2 90.9
</table>
<tableCaption confidence="0.997430666666667">
Table 2: Classification results for MSRP and PAN with
individual metrics as features. Entries are sorted by accu-
racies on MSRP.
</tableCaption>
<bodyText confidence="0.998765333333333">
clearly outperformed by some of the more robust
metrics such as TERp and METEOR.
Table 3 shows the results of our experiments em-
ploying multiple metrics as features, for both MSRP
and PAN. The final row in the table shows the results
of our best system. The remaining rows of this table
show the top performing metrics for both datasets;
we treat BLEU, NIST and TER as our baseline met-
rics since they are not new and are not the primary
focus of our investigation. In terms of novel met-
rics, we find that the top 3 metrics for both datasets
were TERp, METEOR and BADGER respectively
as shown. Combining all 8 metrics led to the best
performance for MSRP but showed no performance
increase for PAN.
</bodyText>
<table confidence="0.999156428571429">
MSRP PAN
Features Acc. F1 Acc. F1
Base Metrics 74.1 81.5 88.6 87.8
+ TERp 75.6 82.5 91.5 91.2
+ METEOR 76.6 83.2 92.0 91.8
+ BADGER 77.0 83.7 92.3 92.1
+ Others 77.4 84.1 92.3 92.1
</table>
<tableCaption confidence="0.795129">
Table 3: The top 3 performing MT metrics for both
MSRP and PAN datasets as identified by ablation stud-
</tableCaption>
<bodyText confidence="0.886457">
ies. BLEU(1-4), NIST(1-5) and TER were used as the 10
base features in the classifiers.
Our results for the PAN dataset are much better than
those for MSRP since:
</bodyText>
<listItem confidence="0.782159">
(a) It is likely that our negative evidence is too easy
for most MT metrics.
(b) Many plagiarized pairs are linked simply via
</listItem>
<bodyText confidence="0.995131578947368">
lexical synonymy which can be easily captured
by metrics like METEOR and TERp, e.g., the
sentence “Young’s main contention is that in lit-
erature genius must make rules for itself, and
that imitation is suicidal” is simply plagiarized
as “Young’s major argument is that in litera-
ture intellect must make rules for itself, and
that replication is dangerous.” However, the
PAN corpus does contains some very challeng-
ing and interesting examples of paraphrases—
even more so than MSRP—which we describe
in §4.
Finally, Table 4 shows that the results from our
best system are the best ever reported on the MSRP
test set when compared to all previously published
work. Furthermore, the single best performing met-
ric (TERp)—also shown in the table—outperforms,
by itself, many previous approaches utilizing multi-
ple, complex features.
</bodyText>
<table confidence="0.999911857142857">
Model Acc. F1
All Paraphrase Baseline 66.5 79.9
(Mihalcea et al., 2006) 70.3 81.3
(Rus et al., 2008) 70.6 80.5
(Qiu et al., 2006) 72.0 81.6
(Islam and Inkpen, 2007) 72.6 81.3
(Fernando and Stevenson, 2008) 74.1 82.4
TERp 74.3 81.8
(Finch et al., 2005) 75.0 82.7
(Wan et al., 2006) 75.6 83.0
(Das and Smith, 2009) 76.1 82.7
(Kozareva and Montoyo, 2006) 76.6 79.6
(Socher et al., 2011) 76.8 83.6
Best MT Metrics 77.4 84.1
</table>
<tableCaption confidence="0.7146346">
Table 4: Comparing the accuracy and F-score for the sin-
gle best performing MT metric TERp (in gray) as well as
the best metric combination system (in gray and bold)
with previously reported results on the MSRP test set
(N = 1, 752). Entries are sorted by accuracy.
</tableCaption>
<subsectionHeader confidence="0.723375">
3.5 Metric Contributions
</subsectionHeader>
<bodyText confidence="0.9998626">
In addition to quantitative results, we also wanted to
highlight specific examples from our datasets that
can demonstrate the strength of the new metrics
over simple n-gram overlap and edit-distance based
metrics. Below we present examples for the 4 best
</bodyText>
<page confidence="0.997566">
186
</page>
<bodyText confidence="0.847115">
metrics across both datasets:
</bodyText>
<listItem confidence="0.986651307692308">
• TERp uses stemming and phrasal paraphrase
recognition to accurately classify the sentence
pair “For the weekend, the top 12 movies
grossed $157.1 million, up 52 percent from
the same weekend a year earlier.” and “The
overall box office soared, with the top 12
movies grossing $157.1 million, up 52 percent
from a year ago.” from MSRP as paraphrases.
• METEOR uses synonymy and stemming
to accurately classify the sentence pair “Her
letters at this time exhibited the two extremes of
feeling in a marked degree.” and “Her letters
at this time showed two extremes of feelings.”
from PAN as plagiarized.
• BADGER uses unsupervised contextual
similarity detection to accurately classify the
sentence pair “Otherwise they were false or
mistaken reactions” and “Otherwise, were false
or wrong responses” from PAN as plagiarized.
• SEPIA uses structural n-grams via dependency
trees to accurately classify the sentence pair
“At his sentencing, Avants had tubes in his
nose and a portable oxygen tank beside him.”
and “Avants, wearing a light brown jumpsuit,
had tubes in his nose and a portable oxygen
tank beside him.” from MSRP as paraphrases.
</listItem>
<sectionHeader confidence="0.997842" genericHeader="method">
4 Error Analysis
</sectionHeader>
<bodyText confidence="0.9999728">
In this section, we conduct an analysis of the
misclassifications that our system makes on both
datasets. Our analyses consisted of finding the sen-
tences pairs from the test set for each dataset which
none of our systems (not just the best one) ever clas-
sified correctly and inspecting a random sample of
100 of these. This inspection yields not only the top
sources of error for an approach that relies solely on
MT metrics but also uncovers sources of annotation
errors in both datasets themselves.
</bodyText>
<subsectionHeader confidence="0.913294">
4.1 MSRP
</subsectionHeader>
<bodyText confidence="0.980996911764706">
In their paper describing the creation of the MSRP
corpus, Dolan et al. (2004) clearly state that “the de-
gree of mismatch allowed before the pair was judged
non-equivalent was left to the discretion of the indi-
vidual rater” and that “many of the 33% of sentence
pairs judged to be not equivalent still overlap signif-
icantly in information content and even wording”.
We found evidence that the raters were not always
consistent in applying the annotation guidelines. For
example, in some cases the lack of attribution for a
quotation led the raters to label a pair as paraphrastic
whereas in other cases it did not. For example, the
pair “These are real crimes that hurt a lot ofpeople.”
and “‘These are real crimes that disrupt the lives of
real people,’ Smith said.” was not marked as para-
phrastic. Furthermore, even though the guidelines
instruct the raters to “treat anaphors and their full
forms as equivalent, regardless of how great the dis-
parity in length or lexical content between the two
sentences”, we found pairs of sentences marked as
non-paraphrastic which only differed in anaphora.
However, the primary goal of this analysis is to find
sources of errors in an MT-metric driven approach
and below we present the top 5 such sources:
1. Misleading Lexical Overlap. Non-
paraphrastic pairs where there is large
lexical overlap of secondary material between
the two sentences but the primary semantic
content is different. For example, “Gyorgy
Heizler, head of the local disaster unit, said the
coach had been carrying 38 passengers.”
and “The head of the local disaster
unit, Gyorgy Heizler, said the coach
driver had failed to heed red stop lights.”.
</bodyText>
<listItem confidence="0.997442181818182">
2. Lack of World Knowledge. Paraphrastic
pairs that require world knowledge. For ex-
ample, “Security experts are warning that a
new mass-mailing worm is spreading widely
across the Internet, sometimes posing as e-
mail from the Microsoft founder.” and “A
new worm has been spreading rapidly across
the Internet, sometimes pretending to be
an e-mail from Microsoft Chairman Bill Gates,
antivirus vendors said Monday.”.
3. Tricky Phrasal Paraphrases. Paraphras-
</listItem>
<page confidence="0.996945">
187
</page>
<bodyText confidence="0.998027666666667">
tic pairs that contain domain-dependent se-
mantic alternations. For example, “The
leading actress nod went to energetic new-
comer Marissa Jaret Winokur as Edna’s
daughter Tracy.” and “Marissa Jaret Winokur,
as Tracy, won for best actress in a musical.”.
</bodyText>
<listItem confidence="0.889717238095238">
4. Date, Time and Currency Differences. Para-
phrastic pairs that contain different temporal
or currency references. These references were
normalized to generic tokens (e.g., $NUMBER)
before being shown to MSRP raters but are re-
tained in the released dataset. For example,
“Expenses are expected to be approximately
$2.3 billion, at the high end of the previous ex-
pectation of $2.2-to-$2.3 billion.” and “Spend-
ing on research and development is expected to
be $4.4 billion for the year, compared with the
previous expectation of $4.3 billion.”.
5. Anaphoric References. Paraphrastic pairs
wherein one member of the pair contains
anaphora and the other doesn’t (these are con-
sidered paraphrases according to MSRP guide-
lines). For example, “They certainly reveal a
very close relationship between Boeing and se-
nior Washington officials.” and “The e-mails
reveal the close relationship between Boeing
and the Air Force.”.
</listItem>
<bodyText confidence="0.999505333333333">
Note that most misclassified sentence pairs can be
categorized into more than one of the above cate-
gories.
</bodyText>
<subsectionHeader confidence="0.964971">
4.2 PAN
</subsectionHeader>
<bodyText confidence="0.983824357142857">
For the PAN corpus, the only real source of error in
the dataset itself was the sentence alignment algo-
rithm. There were many sentence pairs that were
erroneously linked as paraphrases. Leaving aside
such pairs, the 3 largest sources of error for our MT-
metric based approach were:
1. Complex Sentential Paraphrases. By far,
most of the misclassified pairs were paraphras-
tic pairs that could be categorized as real world
plagiarism, i.e., where the plagiarizer copies
the idea from the source but makes several
complex transformations, e.g., sentence split-
ting, structural paraphrasing etc. so as to ren-
der an MT-metric based approach powerless.
For example, consider the pair “The school
bears the honored name of one who, in the long
years of the anti-slavery agitation, was known
as an uncompromising friend of human free-
dom.” and “The school is named after a man
who defended the right of all men and women
to be free, all through the years when people
campaigned against slavery.” Another inter-
esting example is the pair “The most unpromis-
ing weakly-looking creatures sometimes live to
ninety while strong robust men are carried off
in their prime.” and “Sometimes the strong per-
sonalities live shorter than those who are unex-
pected.”.
</bodyText>
<listItem confidence="0.998675666666666">
2. Misleading Lexical Overlap. Similar to
MSRP. For example, “Here was the second pe-
riod of Hebraic influence, an influence wholly
moral and religious.” and “This was the sec-
ond period of Hellenic influence, an influence
wholly intellectual and artistic.”.
3. Typographical and Spelling Errors. Para-
phrastic pairs where the Turkers creating the
plagiarism also introduced other typos and
spelling errors. For example, “The boat then
had on board over 1,000 souls in all” and
“1000 people where on board at that tim”.
</listItem>
<sectionHeader confidence="0.993287" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999987166666667">
The misses due to “Date, Time, and Currency Dif-
ferences” are really just the result of an artifact in
the testing. It is possible that an MT metrics based
approach could accurately predict these cases if the
references to dates etc. were replaced with generic
tokens as was done for the human raters. In a
similar vein, some of the misses that are due to a
lack of world knowledge might become hits if a
named entity recognizer could discover that “Mi-
crosoft founder” is the same as “Microsoft Chair-
man”. Similarly, some of the cases of anaphoric ref-
erence might be recognized with an anaphora res-
olution system. And the problem of misspelling in
PAN could be remedied with automatic spelling cor-
rection. Therefore, it is possible to improve the MT
metrics based approach further by utilizing certain
NLP systems as pre-processing modules for the text.
The only error category in MSRP and PAN
</bodyText>
<page confidence="0.995631">
188
</page>
<bodyText confidence="0.999968238095238">
that caused false positives was “Misleading Lexical
Overlap”. Here, the take-away message is that not
every part of a sentence is equally important for rec-
ognizing semantic equivalence or non-equivalence.
In a sentence that describes what someone commu-
nicated, the content of what was said is crucial. For
example, despite lexical matches everywhere else,
the mismatch of “the coach had been carrying 38
passengers” and “the driver had failed to heed the
red stop lights” disqualifies the respective sentences
from being paraphrases. Along the same line, dif-
ferences in proper names and their variants should
receive more weight than other words. A sentence
about “Hebraic influence” on a period in history is
not the same as a sentence which matches in ev-
ery other way but is instead about “Hellenic influ-
ence”. These sentences represent a bigger chal-
lenge for an approach based solely on MT metrics.
Given enough pairs of “near-miss” non-paraphrases,
our system might be able to figure this out, but this
would require a large amount of annotated data.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999779">
In this paper, we re-examined the idea that automatic
metrics used for evaluating translation quality can
perform well explicitly for the task of paraphrase
recognition. The goal of our paper was to deter-
mine whether approaches developed for the related
but different task of MT evaluation can be as com-
petitive as approaches developed specifically for the
task of paraphrase identification. While we do treat
the metrics as black boxes to an extent, we explic-
itly chose metrics that were high performing but also
complementary in nature.
Specifically, our re-examination focused on the
more sophisticated MT metrics of the last few years
that claim to go beyond simple n-gram overlap and
edit distance. We found that a meta-classifier trained
using only MT metrics outperforms all previous ap-
proaches for the MSRP corpus. Unlike previous
studies, we also applied our approach to a new pla-
giarism dataset and obtained extremely positive re-
sults. We examined both datasets not only to find
pairs that demonstrated the strength of each met-
ric but also to conduct an error analysis to discover
the top sources of errors that an MT metric based
approach is susceptible to. Finally, we discovered
that using the TERp metric by itself provides fairly
good performance and can outperform many other
supervised classification approaches utilizing multi-
ple, complex features.
We also have two specific suggestions that we be-
lieve can benefit the community. First, we believe
that binary indicators of semantic equivalence are
not ideal and a continuous value between 0 and 1
indicating the degree to which two pairs are para-
phrastic is more suitable for most approaches. How-
ever, rather than asking annotators to rate pairs on
a scale, a better idea might be to show the sentence
pairs to a large number of Turkers (&gt; 20) on Ama-
zon Mechanical Turk and ask them to classify it as
either a paraphrase or a non-paraphrase. A simple
estimate of the degree of semantic equivalence of
the pair is simply the proportion of the Turkers who
classified the pair as paraphrastic. An example of
such an approach, as applied to the task of grammat-
ical error detection, can be found in (Madnani et al.,
2011).3 Second, we believe that the PAN corpus—
with Turker simulated plagiarism—contains much
more realistic examples of paraphrase and should
be incorporated into future evaluations of paraphrase
identification. In order to encourage this, we are re-
leasing our PAN dataset containing 13,000 sentence
pairs.
We are also releasing our error analysis data (100
pairs for MSRP and 100 pairs for PAN) since they
might prove useful to other researchers as well. Note
that the annotations for this analysis were produced
by the authors themselves and, although, they at-
tempted to accurately identify all error categories for
most sentence pairs, it is possible that the errors in
some sentence pairs were not comprehensively iden-
tified.4
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999517333333333">
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions.
</bodyText>
<footnote confidence="0.9925268">
3A good approximation is to use an ordinal scale for the
human judgments as in the Semantic Textual Similarity task
of SemEval 2012. See http://www.cs.york.ac.uk/
semeval-2012/task6/ for more details.
4The data is available at http://bit.ly/mt-para.
</footnote>
<page confidence="0.997678">
189
</page>
<sectionHeader confidence="0.989929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999541115384615">
D. W. Aha, D. Kibler, and M. K. Albert. 1991. Instance-
based learning algorithms. Mach. Learn., 6:37–66.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, and
O. Zaidan, editors. 2010. Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR.
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maxi-
mum similarity metric for machine translation evalua-
tion. In Proceedings of ACL-HLT, pages 55–62.
D. Das and N.A. Smith. 2009. Paraphrase Identifica-
tion as Probabilistic Quasi-synchronous Recognition.
In Proceedings of ACL-IJCNLP, pages 468–476.
M. Denkowski and M. Lavie. 2010. Extending the
METEOR Machine Translation Metric to the Phrase
Level. In Proceedings of NAACL.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In Proceedings of HLT, pages 138–145.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources. In Proceed-
ings of COLING, pages 350–356, Geneva, Switzer-
land.
S. Fernando and M. Stevenson. 2008. A Semantic Simi-
larity Approach to Paraphrase Detection. In Proceed-
ings of the Computational Linguistics UK (CLUK)
11th Annual Research Colloquium.
A. Finch, Y.S. Hwang, and E. Sumita. 2005. Using Ma-
chine Translation Evaluation Techniques to Determine
Sentence-level Semantic Equivalence. In Proceedings
of the Third International Workshop on Paraphrasing,
pages 17–24.
N. Habash and A. El Kholy. 2008. SEPIA: Surface
Span Extension to Syntactic Dependency Precision-
based MT Evaluation. In Proceedings of the Workshop
on Metrics for Machine Translation at AMTA.
M. Hall, E. Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
A. Islam and D. Inkpen. 2007. Semantic Similarity of
Short Texts. In Proceedings of RANLP, pages 291–
297.
J. J. Jiang and D. W. Conrath. 1997. Semantic Similar-
ity Based on Corpus Statistics and Lexical Taxonomy.
CoRR, cmp-lg/9709008.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt’s SMO
Algorithm for SVM Classifier Design. Neural Com-
put., 13(3):637–649.
Z. Kozareva and A. Montoyo. 2006. Paraphrase Identi-
fication on the Basis of Supervised Machine Learning
Techniques. In Proceedings of FinTAL, pages 524–
233.
N. Madnani, J. Tetreault, M. Chodorow, and A. Ro-
zovskaya. 2011. They Can Help: Using Crowdsourc-
ing to Improve the Evaluation of Grammatical Error
Detection Systems. In Proceedings of ACL (Short Pa-
pers).
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures Of
Text Semantic Similarity. In Proceedings of AAAI,
pages 775–780.
NIST. 2008. NIST MetricsMATR Challenge. Informa-
tion Access Division. http://www.itl.nist.
gov/iad/mig/tests/metricsmatr/.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL.
S. Parker. 2008. BADGER: A New Machine Translation
Metric. In Proceedings of the Workshop on Metrics
for Machine Translation at AMTA.
John C. Platt. 1999. Advances in kernel methods. chap-
ter Fast Training of Support Vector Machines using Se-
quential Minimal Optimization, pages 185–208. MIT
Press.
M. Potthast, B. Stein, A. Barr´on-Cede˜no, and P. Rosso.
2010. An Evaluation Framework for Plagiarism De-
tection. In Proceedings of COLING, pages 997–1005.
L. Qiu, M. Y. Kan, and T. S. Chua. 2006. Paraphrase
Recognition via Dissimilarity Significance Classifica-
tion. In Proceedings of the EMNLP, pages 18–26.
V. Rus, P.M. McCarthy, M.C. Lintean, D.S. McNamara,
and A.C. Graesser. 2008. Paraphrase Identification
with Lexico-Syntactic Graph Subsumption. In Pro-
ceedings of FLAIRS, pages 201–206.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23(2–3):117–127.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D.
Manning. 2011. Dynamic Pooling and Unfolding
Recursive Autoencoders for Paraphrase Detection. In
Advances in Neural Information Processing Systems
24 (NIPS).
S. Wan, R. Dras, M. Dale, and C. Paris. 2006. Using
Dependency-based Features to Take the ”para-farce”
Out of Paraphrase. In Proceedings of the Australasian
Language Technology Workshop (ALTW), pages 131–
138.
</reference>
<page confidence="0.997855">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.164329">
<title confidence="0.999969">Re-examining Machine Translation Metrics for Paraphrase Identification</title>
<author confidence="0.970973">Nitin Madnani Joel</author>
<degree confidence="0.52317575">Educational Testing Princeton, NJ, Martin Hunter College of</degree>
<author confidence="0.794883">New York</author>
<author confidence="0.794883">NY</author>
<email confidence="0.998274">martin.chodorow@hunter.cuny.edu</email>
<abstract confidence="0.98561275">We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years. We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M K Albert</author>
</authors>
<title>Instancebased learning algorithms.</title>
<date>1991</date>
<booktitle>Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</booktitle>
<editor>Mach. Learn., 6:37–66. C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, and O. Zaidan, editors.</editor>
<contexts>
<context position="9321" citStr="Aha et al., 1991" startWordPosition="1471" endWordPosition="1474"> action, are carved into fantastic forms. Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora. 3.1 Classifier Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1These constituent classifiers were chosen since they were the top 3 performers in 5</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, and M. K. Albert. 1991. Instancebased learning algorithms. Mach. Learn., 6:37–66. C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, and O. Zaidan, editors. 2010. Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>MAXSIM: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="11777" citStr="Chan and Ng, 2008" startWordPosition="1873" endWordPosition="1876">ntactically-aware metric designed to focus on 184 structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It computes a compression distance between the two sentences that utilizes the Burrows Wheeler Transformation (BWT). The BWT enables taking into account common sentence contexts with no limit on the size of these contexts. 8. MAXSIM (Chan and Ng, 2008) treats the problem as one of bipartite graph matching and maps each word in one sentence to at most one word in the other sentence. It allows the use of arbitrary similarity functions between words.2 Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST MetricsMATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity. 3.3 Datasets In this section, we describe the two datasets that we used to evaluate our approach. 3.3.1 Microsoft</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL-HLT, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase Identification as Probabilistic Quasi-synchronous Recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="7027" citStr="Das and Smith (2009)" startWordPosition="1097" endWordPosition="1100">based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities between both single word features as well as multi-word phrases extracted from the parse trees of the two sentences. We compare our results to those from all the approaches described in this section later in §3.4. 3 Classifying with MT Metrics In this section, we first describe our overall approach to paraphrase identif</context>
<context position="18199" citStr="Das and Smith, 2009" startWordPosition="2950" endWordPosition="2953"> 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to highlight specific examples from our datasets that can demonstrate the strength of the new metrics over simple n-gram overlap and edit-distance bas</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>D. Das and N.A. Smith. 2009. Paraphrase Identification as Probabilistic Quasi-synchronous Recognition. In Proceedings of ACL-IJCNLP, pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
<author>M Lavie</author>
</authors>
<title>Extending the METEOR Machine Translation Metric to the Phrase Level.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="10927" citStr="Denkowski and Lavie, 2010" startWordPosition="1735" endWordPosition="1738">ording to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and paraphrase (via a lookup table). 6. SEPIA (Habash and El Kholy, 2008) is a syntactically-aware metric designed to focus on 184 structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>M. Denkowski and M. Lavie. 2010. Extending the METEOR Machine Translation Metric to the Phrase Level. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="10166" citStr="Doddington, 2002" startWordPosition="1610" endWordPosition="1611">f n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent classifiers had been chosen. tures for our classifier (hereafter BLEU(1-4)). 2. NIST (Doddington, 2002) is a variant of BLEU that uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. It also weights each n-gram according to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics. In Proceedings of HLT, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>350--356</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2885" citStr="Dolan et al., 2004" startWordPosition="443" endWordPosition="446">by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” This paper describes such a re-examination. We employ 8 different MT metrics for identifying paraphrases across two different datasets - the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al., 2010). We include both MSRP and PAN in our study because they represent two very different sources of paraphrased text. The creation of MSRP relied on the massive redundancy of news articles on the web and extracted sentential paraphrases from different stories written about the same topic. In the case of PAN, humans consciously paraphrased existing text to generate new, 182 2012 Conference of the North American Chapter of the Association for Compu</context>
<context position="20632" citStr="Dolan et al. (2004)" startWordPosition="3355" endWordPosition="3358"> paraphrases. 4 Error Analysis In this section, we conduct an analysis of the misclassifications that our system makes on both datasets. Our analyses consisted of finding the sentences pairs from the test set for each dataset which none of our systems (not just the best one) ever classified correctly and inspecting a random sample of 100 of these. This inspection yields not only the top sources of error for an approach that relies solely on MT metrics but also uncovers sources of annotation errors in both datasets themselves. 4.1 MSRP In their paper describing the creation of the MSRP corpus, Dolan et al. (2004) clearly state that “the degree of mismatch allowed before the pair was judged non-equivalent was left to the discretion of the individual rater” and that “many of the 33% of sentence pairs judged to be not equivalent still overlap significantly in information content and even wording”. We found evidence that the raters were not always consistent in applying the annotation guidelines. For example, in some cases the lack of attribution for a quotation led the raters to label a pair as paraphrastic whereas in other cases it did not. For example, the pair “These are real crimes that hurt a lot of</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of COLING, pages 350–356, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fernando</author>
<author>M Stevenson</author>
</authors>
<title>A Semantic Similarity Approach to Paraphrase Detection.</title>
<date>2008</date>
<booktitle>In Proceedings of the Computational Linguistics UK (CLUK) 11th Annual Research Colloquium.</booktitle>
<contexts>
<context position="6868" citStr="Fernando and Stevenson (2008)" startWordPosition="1070" endWordPosition="1073">detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities between both single word features as well as multi-word phrases extracted from the parse trees of the two sentences. We compare our results to those from all the </context>
<context position="18092" citStr="Fernando and Stevenson, 2008" startWordPosition="2929" endWordPosition="2932"> challenging and interesting examples of paraphrases— even more so than MSRP—which we describe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to highlight specific examples from our dat</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>S. Fernando and M. Stevenson. 2008. A Semantic Similarity Approach to Paraphrase Detection. In Proceedings of the Computational Linguistics UK (CLUK) 11th Annual Research Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finch</author>
<author>Y S Hwang</author>
<author>E Sumita</author>
</authors>
<title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2438" citStr="Finch et al., 2005" startWordPosition="370" endWordPosition="373">mance and allow for short feature and system development cycles, which are important for evaluating research ideas. In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” This paper describes such a re-examination. We employ 8 different MT metrics for identifying paraphrases across two different datasets - the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al., 2010)</context>
<context position="4605" citStr="Finch et al. (2005)" startWordPosition="720" endWordPosition="723">urported strength is demonstrated in our datasets. Next, in §4 we conduct an analysis of our system’s misclassifications for both datasets and outline a taxonomy of errors that our system makes. We also look at annotation errors in the datasets themselves. We discuss the findings of the error analysis in §5 and conclude in §6. 2 Related Work &amp; Our Contributions Our goal in this paper is to examine the utility of a paraphrase identification approach that relies solely on MT evaluation metrics and no other evidence of semantic equivalence. Given this setup, the most relevant previous work is by Finch et al. (2005) which uses BLEU, NIST, WER and PER as features for a supervised classification approach using SVMs. In addition, they also incorporate part-of-speech information as well as the Jiang-Conrath WordNetbased lexical relatedness measure (Jiang and Conrath, 1997) into their edit distance calculations. In the first part of our paper, we present classification experiments with newer MT metrics not available in 2005, a worthwhile exercise in itself. However, we go much further in our study: • We apply our approach to two different paraphrase datasets (MSRP and PAN) that were created via different proc</context>
<context position="18138" citStr="Finch et al., 2005" startWordPosition="2938" endWordPosition="2941">en more so than MSRP—which we describe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to highlight specific examples from our datasets that can demonstrate the strength of the</context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>A. Finch, Y.S. Hwang, and E. Sumita. 2005. Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence. In Proceedings of the Third International Workshop on Paraphrasing, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>A El Kholy</author>
</authors>
<title>SEPIA: Surface Span Extension to Syntactic Dependency Precisionbased MT Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Metrics for Machine Translation at AMTA.</booktitle>
<marker>Habash, El Kholy, 2008</marker>
<rawString>N. Habash and A. El Kholy. 2008. SEPIA: Surface Span Extension to Syntactic Dependency Precisionbased MT Evaluation. In Proceedings of the Workshop on Metrics for Machine Translation at AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<contexts>
<context position="9411" citStr="Hall et al., 2009" startWordPosition="1486" endWordPosition="1489">rases (in italics) from the MSRP and PAN corpora. 3.1 Classifier Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metac</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Islam</author>
<author>D Inkpen</author>
</authors>
<title>Semantic Similarity of Short Texts.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>291--297</pages>
<contexts>
<context position="6561" citStr="Islam and Inkpen (2007)" startWordPosition="1026" endWordPosition="1029">ase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regre</context>
<context position="18051" citStr="Islam and Inkpen, 2007" startWordPosition="2923" endWordPosition="2926"> PAN corpus does contains some very challenging and interesting examples of paraphrases— even more so than MSRP—which we describe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to</context>
</contexts>
<marker>Islam, Inkpen, 2007</marker>
<rawString>A. Islam and D. Inkpen. 2007. Semantic Similarity of Short Texts. In Proceedings of RANLP, pages 291– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<date>1997</date>
<booktitle>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. CoRR, cmp-lg/9709008.</booktitle>
<contexts>
<context position="4863" citStr="Jiang and Conrath, 1997" startWordPosition="758" endWordPosition="762">es. We discuss the findings of the error analysis in §5 and conclude in §6. 2 Related Work &amp; Our Contributions Our goal in this paper is to examine the utility of a paraphrase identification approach that relies solely on MT evaluation metrics and no other evidence of semantic equivalence. Given this setup, the most relevant previous work is by Finch et al. (2005) which uses BLEU, NIST, WER and PER as features for a supervised classification approach using SVMs. In addition, they also incorporate part-of-speech information as well as the Jiang-Conrath WordNetbased lexical relatedness measure (Jiang and Conrath, 1997) into their edit distance calculations. In the first part of our paper, we present classification experiments with newer MT metrics not available in 2005, a worthwhile exercise in itself. However, we go much further in our study: • We apply our approach to two different paraphrase datasets (MSRP and PAN) that were created via different processes. • We attempt to find evidence of each metric’s purported strength in both datasets. • We conduct an extensive error analysis to find types of errors that a system based solely on MT metrics is likely to make. In addition, we also discover interesting </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. J. Jiang and D. W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. CoRR, cmp-lg/9709008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Keerthi</author>
<author>S K Shevade</author>
<author>C Bhattacharyya</author>
<author>K R K Murthy</author>
</authors>
<date>2001</date>
<booktitle>Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Comput.,</booktitle>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="9220" citStr="Keerthi et al., 2001" startWordPosition="1456" endWordPosition="1459">nt regularity, sharp soil grains are picked up by the air and hurled against the rocks, which, under this action, are carved into fantastic forms. Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora. 3.1 Classifier Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 a</context>
</contexts>
<marker>Keerthi, Shevade, Bhattacharyya, Murthy, 2001</marker>
<rawString>S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Comput., 13(3):637–649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>A Montoyo</author>
</authors>
<title>Paraphrase Identification on the Basis of Supervised Machine Learning Techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of FinTAL,</booktitle>
<pages>524--233</pages>
<contexts>
<context position="6393" citStr="Kozareva and Montoyo (2006)" startWordPosition="1000" endWordPosition="1003">them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal </context>
<context position="18238" citStr="Kozareva and Montoyo, 2006" startWordPosition="2956" endWordPosition="2959">ur best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to highlight specific examples from our datasets that can demonstrate the strength of the new metrics over simple n-gram overlap and edit-distance based metrics. Below we present examples f</context>
</contexts>
<marker>Kozareva, Montoyo, 2006</marker>
<rawString>Z. Kozareva and A. Montoyo. 2006. Paraphrase Identification on the Basis of Supervised Machine Learning Techniques. In Proceedings of FinTAL, pages 524– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>J Tetreault</author>
<author>M Chodorow</author>
<author>A Rozovskaya</author>
</authors>
<title>They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL (Short Papers).</booktitle>
<contexts>
<context position="29880" citStr="Madnani et al., 2011" startWordPosition="4875" endWordPosition="4878">1 indicating the degree to which two pairs are paraphrastic is more suitable for most approaches. However, rather than asking annotators to rate pairs on a scale, a better idea might be to show the sentence pairs to a large number of Turkers (&gt; 20) on Amazon Mechanical Turk and ask them to classify it as either a paraphrase or a non-paraphrase. A simple estimate of the degree of semantic equivalence of the pair is simply the proportion of the Turkers who classified the pair as paraphrastic. An example of such an approach, as applied to the task of grammatical error detection, can be found in (Madnani et al., 2011).3 Second, we believe that the PAN corpus— with Turker simulated plagiarism—contains much more realistic examples of paraphrase and should be incorporated into future evaluations of paraphrase identification. In order to encourage this, we are releasing our PAN dataset containing 13,000 sentence pairs. We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they attempted to accurately identify all error </context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, Rozovskaya, 2011</marker>
<rawString>N. Madnani, J. Tetreault, M. Chodorow, and A. Rozovskaya. 2011. They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems. In Proceedings of ACL (Short Papers).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures Of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="6029" citStr="Mihalcea et al. (2006)" startWordPosition="948" endWordPosition="951">kely to make. In addition, we also discover interesting paraphrase pairs in the datasets. • We release our sentence-level PAN dataset (see §3.3.2) which contains more realistic examples of paraphrase and can prove useful to the community for future evaluations of paraphrase identification. BLEU-based features were also employed by Wan et al. (2006) who use them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modi</context>
<context position="17958" citStr="Mihalcea et al., 2006" startWordPosition="2905" endWordPosition="2908">ature intellect must make rules for itself, and that replication is dangerous.” However, the PAN corpus does contains some very challenging and interesting examples of paraphrases— even more so than MSRP—which we describe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and Knowledge-based Measures Of Text Semantic Similarity. In Proceedings of AAAI, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>NIST MetricsMATR Challenge. Information Access Division.</title>
<date>2008</date>
<note>http://www.itl.nist. gov/iad/mig/tests/metricsmatr/.</note>
<contexts>
<context position="12141" citStr="NIST, 2008" startWordPosition="1938" endWordPosition="1939">mputes a compression distance between the two sentences that utilizes the Burrows Wheeler Transformation (BWT). The BWT enables taking into account common sentence contexts with no limit on the size of these contexts. 8. MAXSIM (Chan and Ng, 2008) treats the problem as one of bipartite graph matching and maps each word in one sentence to at most one word in the other sentence. It allows the use of arbitrary similarity functions between words.2 Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST MetricsMATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity. 3.3 Datasets In this section, we describe the two datasets that we used to evaluate our approach. 3.3.1 Microsoft Research Paraphrase Corpus The MSRP corpus was created by mining news articles on the web for topically similar articles and then extracting potential sentential paraphrases using a set of heuristics. Extracted pairs were then shown to two human judges with disagreements handled by a third adjudicator. The kappa was reported as 0.62, which indicates moderate to</context>
</contexts>
<marker>NIST, 2008</marker>
<rawString>NIST. 2008. NIST MetricsMATR Challenge. Information Access Division. http://www.itl.nist. gov/iad/mig/tests/metricsmatr/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9466" citStr="Papineni et al., 2002" startWordPosition="1497" endWordPosition="1500">.1 Classifier Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent cla</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Parker</author>
</authors>
<title>BADGER: A New Machine Translation Metric.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Metrics for Machine Translation at AMTA.</booktitle>
<contexts>
<context position="11446" citStr="Parker, 2008" startWordPosition="1822" endWordPosition="1823">t operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and paraphrase (via a lookup table). 6. SEPIA (Habash and El Kholy, 2008) is a syntactically-aware metric designed to focus on 184 structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It computes a compression distance between the two sentences that utilizes the Burrows Wheeler Transformation (BWT). The BWT enables taking into account common sentence contexts with no limit on the size of these contexts. 8. MAXSIM (Chan and Ng, 2008) treats the problem as one of bipartite graph matching and maps each word in one sentence to at most one word in the other sentence. It allows the use of arbitrary similarity functions between words.2 Our choice of metrics was based on their popularity in the MT commun</context>
</contexts>
<marker>Parker, 2008</marker>
<rawString>S. Parker. 2008. BADGER: A New Machine Translation Metric. In Proceedings of the Workshop on Metrics for Machine Translation at AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Advances in kernel methods. chapter Fast Training of Support Vector Machines using Sequential Minimal Optimization,</title>
<date>1999</date>
<pages>185--208</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9197" citStr="Platt, 1999" startWordPosition="1454" endWordPosition="1455">w with frequent regularity, sharp soil grains are picked up by the air and hurled against the rocks, which, under this action, are carved into fantastic forms. Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora. 3.1 Classifier Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We us</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Advances in kernel methods. chapter Fast Training of Support Vector Machines using Sequential Minimal Optimization, pages 185–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Potthast</author>
<author>B Stein</author>
<author>A Barr´on-Cede˜no</author>
<author>P Rosso</author>
</authors>
<title>An Evaluation Framework for Plagiarism Detection.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>997--1005</pages>
<marker>Potthast, Stein, Barr´on-Cede˜no, Rosso, 2010</marker>
<rawString>M. Potthast, B. Stein, A. Barr´on-Cede˜no, and P. Rosso. 2010. An Evaluation Framework for Plagiarism Detection. In Proceedings of COLING, pages 997–1005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Paraphrase Recognition via Dissimilarity Significance Classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>18--26</pages>
<contexts>
<context position="6215" citStr="Qiu et al. (2006)" startWordPosition="975" endWordPosition="978">raphrase and can prove useful to the community for future evaluations of paraphrase identification. BLEU-based features were also employed by Wan et al. (2006) who use them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for par</context>
<context position="18016" citStr="Qiu et al., 2006" startWordPosition="2917" endWordPosition="2920">n is dangerous.” However, the PAN corpus does contains some very challenging and interesting examples of paraphrases— even more so than MSRP—which we describe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quan</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>L. Qiu, M. Y. Kan, and T. S. Chua. 2006. Paraphrase Recognition via Dissimilarity Significance Classification. In Proceedings of the EMNLP, pages 18–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>P M McCarthy</author>
<author>M C Lintean</author>
<author>D S McNamara</author>
<author>A C Graesser</author>
</authors>
<title>Paraphrase Identification with Lexico-Syntactic Graph Subsumption.</title>
<date>2008</date>
<booktitle>In Proceedings of FLAIRS,</booktitle>
<pages>201--206</pages>
<contexts>
<context position="6710" citStr="Rus et al. (2008)" startWordPosition="1050" endWordPosition="1053">and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities betw</context>
<context position="17987" citStr="Rus et al., 2008" startWordPosition="2911" endWordPosition="2914">r itself, and that replication is dangerous.” However, the PAN corpus does contains some very challenging and interesting examples of paraphrases— even more so than MSRP—which we describe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Cont</context>
</contexts>
<marker>Rus, McCarthy, Lintean, McNamara, Graesser, 2008</marker>
<rawString>V. Rus, P.M. McCarthy, M.C. Lintean, D.S. McNamara, and A.C. Graesser. 2008. Paraphrase Identification with Lexico-Syntactic Graph Subsumption. In Proceedings of FLAIRS, pages 201–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="10483" citStr="Snover et al., 2006" startWordPosition="1662" endWordPosition="1665">iers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent classifiers had been chosen. tures for our classifier (hereafter BLEU(1-4)). 2. NIST (Doddington, 2002) is a variant of BLEU that uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. It also weights each n-gram according to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and p</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate. Machine Translation,</title>
<date>2009</date>
<pages>23--2</pages>
<contexts>
<context position="10770" citStr="Snover et al., 2009" startWordPosition="1711" endWordPosition="1714">oddington, 2002) is a variant of BLEU that uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. It also weights each n-gram according to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and paraphrase (via a lookup table). 6. SEPIA (Habash and El Kholy, 2008) is a syntactically-aware metric designed to focus on 184 structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009. TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate. Machine Translation, 23(2–3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24 (NIPS).</booktitle>
<contexts>
<context position="7249" citStr="Socher et al. (2011)" startWordPosition="1131" endWordPosition="1134">fied version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities between both single word features as well as multi-word phrases extracted from the parse trees of the two sentences. We compare our results to those from all the approaches described in this section later in §3.4. 3 Classifying with MT Metrics In this section, we first describe our overall approach to paraphrase identification that utilizes only MT metrics. We then discuss the actual MT metrics we used. Finally, we describe the datasets on which we evaluated our approach and present our results. 183 MSRP They had published an advertiseme</context>
<context position="18270" citStr="Socher et al., 2011" startWordPosition="2962" endWordPosition="2965">ed on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to highlight specific examples from our datasets that can demonstrate the strength of the new metrics over simple n-gram overlap and edit-distance based metrics. Below we present examples for the 4 best 186 metrics across</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems 24 (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>R Dras</author>
<author>M Dale</author>
<author>C Paris</author>
</authors>
<title>Using Dependency-based Features to Take the ”para-farce” Out of Paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop (ALTW),</booktitle>
<pages>131--138</pages>
<contexts>
<context position="2457" citStr="Wan et al., 2006" startWordPosition="374" endWordPosition="377">short feature and system development cycles, which are important for evaluating research ideas. In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” This paper describes such a re-examination. We employ 8 different MT metrics for identifying paraphrases across two different datasets - the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al., 2010). We include both M</context>
<context position="5757" citStr="Wan et al. (2006)" startWordPosition="906" endWordPosition="909">rase datasets (MSRP and PAN) that were created via different processes. • We attempt to find evidence of each metric’s purported strength in both datasets. • We conduct an extensive error analysis to find types of errors that a system based solely on MT metrics is likely to make. In addition, we also discover interesting paraphrase pairs in the datasets. • We release our sentence-level PAN dataset (see §3.3.2) which contains more realistic examples of paraphrase and can prove useful to the community for future evaluations of paraphrase identification. BLEU-based features were also employed by Wan et al. (2006) who use them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimil</context>
<context position="18167" citStr="Wan et al., 2006" startWordPosition="2944" endWordPosition="2947">escribe in §4. Finally, Table 4 shows that the results from our best system are the best ever reported on the MSRP test set when compared to all previously published work. Furthermore, the single best performing metric (TERp)—also shown in the table—outperforms, by itself, many previous approaches utilizing multiple, complex features. Model Acc. F1 All Paraphrase Baseline 66.5 79.9 (Mihalcea et al., 2006) 70.3 81.3 (Rus et al., 2008) 70.6 80.5 (Qiu et al., 2006) 72.0 81.6 (Islam and Inkpen, 2007) 72.6 81.3 (Fernando and Stevenson, 2008) 74.1 82.4 TERp 74.3 81.8 (Finch et al., 2005) 75.0 82.7 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 76.1 82.7 (Kozareva and Montoyo, 2006) 76.6 79.6 (Socher et al., 2011) 76.8 83.6 Best MT Metrics 77.4 84.1 Table 4: Comparing the accuracy and F-score for the single best performing MT metric TERp (in gray) as well as the best metric combination system (in gray and bold) with previously reported results on the MSRP test set (N = 1, 752). Entries are sorted by accuracy. 3.5 Metric Contributions In addition to quantitative results, we also wanted to highlight specific examples from our datasets that can demonstrate the strength of the new metrics over simple n-gr</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>S. Wan, R. Dras, M. Dale, and C. Paris. 2006. Using Dependency-based Features to Take the ”para-farce” Out of Paraphrase. In Proceedings of the Australasian Language Technology Workshop (ALTW), pages 131– 138.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>