<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001068">
<title confidence="0.98295">
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking
</title>
<author confidence="0.968057">
Eugene Charniak and Mark Johnson
</author>
<affiliation confidence="0.9477185">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.904481">
Providence, RI 02912
</address>
<email confidence="0.999113">
{mj|ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931352941176">
Discriminative reranking is one method
for constructing high-performance statis-
tical parsers (Collins, 2000). A discrim-
inative reranker requires a source of can-
didate parses for each sentence. This pa-
per describes a simple yet novel method
for constructing sets of 50-best parses
based on a coarse-to-fine generative parser
(Charniak, 2000). This method gener-
ates 50-best lists that are of substantially
higher quality than previously obtainable.
We used these parses as the input to a
MaxEnt reranker (Johnson et al., 1999;
Riezler et al., 2002) that selects the best
parse from the set of parses for each sen-
tence, obtaining an f-score of 91.0% on
sentences of length 100 or less.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990884630434783">
We describe a reranking parser which uses a reg-
ularized MaxEnt reranker to select the best parse
from the 50-best parses returned by a generative
parsing model. The 50-best parser is a probabilistic
parser that on its own produces high quality parses;
the maximum probability parse trees (according to
the parser’s model) have an f-score of 0.897 on
section 23 of the Penn Treebank (Charniak, 2000),
which is still state-of-the-art. However, the 50 best
(i.e., the 50 highest probability) parses of a sentence
often contain considerably better parses (in terms of
f-score); this paper describes a 50-best parsing al-
gorithm with an oracle f-score of 96.8 on the same
data.
The reranker attempts to select the best parse for
a sentence from the 50-best list of possible parses
for the sentence. Because the reranker only has
to consider a relatively small number of parses per
sentences, it is not necessary to use dynamic pro-
gramming, which permits the features to be essen-
tially arbitrary functions of the parse trees. While
our reranker does not achieve anything like the ora-
cle f-score, the parses it selects do have an f-score
of 91.0, which is considerably better than the maxi-
mum probability parses of the n-best parser.
In more detail, for each string s the n-best parsing
algorithm described in section 2 returns the n high-
est probability parses Y(s) = {y1(s), ... , yn(s)}
together with the probability p(y) of each parse y ac-
cording to the parser’s probability model. The num-
ber n of parses was set to 50 for the experiments
described here, but some simple sentences actually
received fewer than 50 parses (so n is actually a
function of s). Each yield or terminal string in the
training, development and test data sets is mapped
to such an n-best list of parse/probability pairs; the
cross-validation scheme described in Collins (2000)
was used to avoid training the n-best parser on the
sentence it was being used to parse.
A feature extractor, described in section 3, is a
vector of m functions f = (fl, ... , fm), where each
fj maps a parse y to a real number fj(y), which
is the value of the jth feature on y. So a feature
extractor maps each y to a vector of feature values
f(y) = (f1(y), ..., fm(y)).
Our reranking parser associates a parse with a
</bodyText>
<page confidence="0.983657">
173
</page>
<note confidence="0.992388">
Proceedings of the 43rd Annual Meeting of the ACL, pages 173–180,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.9094675">
score vθ(y), which is a linear function of the feature
values f(y). That is, each feature fj is associated
with a weight θj, and the feature values and weights
define the score vθ(y) of each parse y as follows:
</bodyText>
<equation confidence="0.923664333333333">
m
vθ(y) = θ - f(y) = E θjfj(y).
j=1
</equation>
<bodyText confidence="0.998136666666667">
Given a string s, the reranking parser’s output ˆy(s)
on string s is the highest scoring parse in the n-best
parses Y(s) for s, i.e.,
</bodyText>
<equation confidence="0.993696">
ˆy(s) = arg max
y∈Y(s)
</equation>
<bodyText confidence="0.999950545454546">
The feature weight vector θ is estimated from the
labelled training corpus as described in section 4.
Because we use labelled training data we know the
correct parse y?(s) for each sentence s in the training
data. The correct parse y?(s) is not always a mem-
ber of the n-best parser’s output Y(s), but we can
identify the parses Y+(s) in Y(s) with the highest
f-scores. Informally, the estimation procedure finds
a weight vector θ that maximizes the score vθ(y) of
the parses y E Y+(s) relative to the scores of the
other parses in Y(s), for each s in the training data.
</bodyText>
<sectionHeader confidence="0.899787" genericHeader="method">
2 Recovering the n-best parses using
coarse-to-fine parsing
</sectionHeader>
<bodyText confidence="0.993891536231884">
The major difficulty in n-best parsing, compared to
1-best parsing, is dynamic programming. For exam-
ple, n-best parsing is straight-forward in best-first
search or beam search approaches that do not use
dynamic programming: to generate more than one
parse, one simply allows the search mechanism to
create successive versions to one’s heart’s content.
A good example of this is the Roark parser
(Roark, 2001) which works left-to right through the
sentence, and abjures dynamic programming in fa-
vor of a beam search, keeping some large number of
possibilities to extend by adding the next word, and
then re-pruning. At the end one has a beam-width’s
number of best parses (Roark, 2001).
The Collins parser (Collins, 1997) does use dy-
namic programming in its search. That is, whenever
a constituent with the same history is generated a
second time, it is discarded if its probability is lower
than the original version. If the opposite is true, then
the original is discarded. This is fine if one only
wants the first-best, but obviously it does not directly
enumerate the n-best parses.
However, Collins (Collins, 2000; Collins
and Koo, in submission) has created an n-
best version of his parser by turning off dy-
namic programming (see the user’s guide to
Bikel’s re-implementation of Collins’ parser,
http://www.cis.upenn.edu/ dbikel/software.html#stat-
parser). As with Roark’s parser, it is necessary to
add a beam-width constraint to make the search
tractable. With a beam width of 1000 the parser
returns something like a 50-best list (Collins,
personal communication), but the actual number of
parses returned for each sentences varies. However,
turning off dynamic programming results in a loss in
efficiency. Indeed, Collins’s n-best list of parses for
section 24 of the Penn tree-bank has some sentences
with only a single parse, because the n-best parser
could not find any parses.
Now there are two known ways to produce n-best
parses while retaining the use of dynamic program-
ming: the obvious way and the clever way.
The clever way is based upon an algorithm devel-
oped by Schwartz and Chow (1990). Recall the key
insight in the Viterbi algorithm: in the optimal parse
the parsing decisions at each of the choice points that
determine a parse must be optimal, since otherwise
one could find a better parse. This insight extends
to n-best parsing as follows. Consider the second-
best parse: if it is to differ from the best parse, then
at least one of its parsing decisions must be subop-
timal. In fact, all but one of the parsing decisions
in second-best parse must be optimal, and the one
suboptimal decision must be the second-best choice
at that choice point. Further, the nth-best parse can
only involve at most n suboptimal parsing decisions,
and all but one of these must be involved in one of
the second through the n−1th-best parses. Thus the
basic idea behind this approach to n-best parsing is
to first find the best parse, then find the second-best
parse, then the third-best, and so on. The algorithm
was originally described for hidden Markov models.
Since this first draft of this paper we have be-
come aware of two PCFG implementations of this
algorithm (Jimenez and Marzal, 2000; Huang and
Chang, 2005). The first was tried on relatively small
grammars, while the second was implemented on
top of the Bikel re-implementation of the Collins
vθ(y).
</bodyText>
<page confidence="0.98406">
174
</page>
<bodyText confidence="0.999978257575758">
parser (Bikel, 2004) and achieved oracle results for
50-best parses similar to those we report below.
Here, however, we describe how to find n-best
parses in a more straight-forward fashion. Rather
than storing a single best parse of each edge, one
stores n of them. That is, when using dynamic pro-
gramming, rather than throwing away a candidate if
it scores less than the best, one keeps it if it is one
of the top n analyses for this edge discovered so far.
This is really very straight-forward. The problem
is space. Dynamic programming parsing algorithms
for PCFGs require O(m2) dynamic programming
states, where m is the length of the sentence, so an
n-best parsing algorithm requires O(nm2). How-
ever things get much worse when the grammar is bi-
lexicalized. As shown by Eisner (Eisner and Satta,
1999) the dynamic programming algorithms for bi-
lexicalized PCFGs require O(m3) states, so a n-best
parser would require O(nm3) states. Things be-
come worse still in a parser like the one described in
Charniak (2000) because it conditions on (and hence
splits the dynamic programming states according to)
features of the grandparent node in addition to the
parent, thus multiplying the number of possible dy-
namic programming states even more. Thus nobody
has implemented this version.
There is, however, one particular feature of the
Charniak parser that mitigates the space problem: it
is a “coarse-to-fine” parser. By “coarse-to-fine” we
mean that it first produces a crude version of the
parse using coarse-grained dynamic programming
states, and then builds fine-grained analyses by split-
ting the most promising of coarse-grained states.
A prime example of this idea is from Goodman
(1997), who describes amethod for producing a sim-
ple but crude approximate grammar of a standard
context-free grammar. He parses a sentence using
the approximate grammar, and the results are used
to constrain the search for a parse with the full CFG.
He finds that total parsing time is greatly reduced.
A somewhat different take on this paradigm is
seen in the parser we use in this paper. Here the
parser first creates a parse forest based upon a much
less complex version of the complete grammar. In
particular, it only looks at standard CFG features,
the parent and neighbor labels. Because this gram-
mar encodes relatively little state information, its dy-
namic programming states are relatively coarse and
hence there are comparatively few of them, so it can
be efficiently parsed using a standard dynamic pro-
gramming bottom-up CFG parser. However, pre-
cisely because this first stage uses a grammar that
ignores many important contextual features, the best
parse it finds will not, in general, be the best parse
according to the finer-grained second-stage gram-
mar, so clearly we do not want to perform best-first
parsing with this grammar. Instead, the output of
the first stage is a polynomial-sized packed parse
forest which records the left and right string posi-
tions for each local tree in the parses generated by
this grammar. The edges in the packed parse for-
est are then pruned, to focus attention on the coarse-
grained states that are likely to correspond to high-
probability fine-grained states. The edges are then
pruned according to their marginal probability con-
ditioned on the string s being parsed as follows:
</bodyText>
<equation confidence="0.953896666666667">
α(ni j,k)Q(ni j,k)
p(ni j,k  |s) = (1)
p(s)
</equation>
<bodyText confidence="0.999823192307692">
Here nij,k is a constituent of type i spanning the
words from j to k, α(nij,k) is the outside probability
of this constituent, and Q(nij,k) is its inside proba-
bility. From parse forest both α and Q can be com-
puted in time proportional to the size of the compact
forest. The parser then removes all constituents nij,k
whose probability falls below some preset threshold.
In the version of this parser available on the web, this
threshold is on the order of 10−4.
The unpruned edges are then exhaustively eval-
uated according to the fine-grained probabilistic
model; in effect, each coarse-grained dynamic pro-
gramming state is split into one or more fine-grained
dynamic programming states. As noted above, the
fine-grained model conditions on information that is
not available in the coarse-grained model. This in-
cludes the lexical head of one’s parents, the part of
speech of this head, the parent’s and grandparent’s
category labels, etc. The fine-grained states inves-
tigated by the parser are constrained to be refine-
ments of the coarse-grained states, which drastically
reduces the number of fine-grained states that need
to be investigated.
It is certainly possible to do dynamic program-
ming parsing directly with the fine-grained gram-
mar, but precisely because the fine-grained grammar
</bodyText>
<page confidence="0.993291">
175
</page>
<bodyText confidence="0.999843854166666">
conditions on a wide variety of non-local contex-
tual information there would be a very large number
of different dynamic programming states, so direct
dynamic programming parsing with the fine-grained
grammar would be very expensive in terms of time
and memory.
As the second stage parse evaluates all the re-
maining constituents in all of the contexts in which
they appear (e.g., what are the possible grand-parent
labels) it keeps track of the most probable expansion
of the constituent in that context, and at the end is
able to start at the root and piece together the overall
best parse.
Now comes the easy part. To create a 50-best
parser we simply change the fine-grained version of
1-best algorithm in accordance with the “obvious”
scheme outlined earlier in this section. The first,
coarse-grained, pass is not changed, but the second,
fine-grained, pass keeps the n-best possibilities at
each dynamic programming state, rather than keep-
ing just first best. When combining two constituents
to form a larger constituent, we keep the best 50 of
the 2500 possibilities they offer. Naturally, if we
keep each 50-best list sorted, we do nothing like
2500 operations.
The experimental question is whether, in practice,
the coarse-to-fine architecture keeps the number of
dynamic programming states sufficiently low that
space considerations do not defeat us.
The answer seems to be yes. We ran the algorithm
on section 24 of the Penn WSJ tree-bank using the
default pruning settings mentioned above. Table 1
shows how the number of fine-grained dynamic pro-
gramming states increases as a function of sentence
length for the sentences in section 24 of the Tree-
bank. There are no sentences of length greater than
69 in this section. Columns two to four show the
number of sentences in each bucket, their average
length, and the average number of fine-grained dy-
namic programming structures per sentence. The fi-
nal column gives the value of the function 100*L1.5
where L is the average length of sentences in the
bucket. Except for bucket 6, which is abnormally
low, it seems that this add-hoc function tracks the
number of structures quite well. Thus the number of
dynamic programming states does not grow as L2,
much less as L3.
To put the number of these structures per sen-
</bodyText>
<table confidence="0.999086777777778">
Len Num Av sen Av strs 100 * L1.5
sents length per sent
0–9 225 6.04 1167 1484
10–19 725 15.0 4246 5808
20–29 795 24.2 9357 11974
30–39 465 33.8 15893 19654
40–49 162 43.2 21015 28440
50–59 35 52.8 30670 38366
60–69 9 62.8 23405 49740
</table>
<tableCaption confidence="0.963638">
Table 1: Number of structures created as a function
</tableCaption>
<table confidence="0.82815">
of sentence length
n 1 2 10 25 50
f-score 0.897 0.914 0.948 0.960 0.968
</table>
<tableCaption confidence="0.8909735">
Table 2: Oracle f-score as a function of number n
of n-best parses
</tableCaption>
<bodyText confidence="0.99951768">
tence in perspective, consider the size of such struc-
tures. Each one must contain a probability, the non-
terminal label of the structure, and a vector of point-
ers to it’s children (an average parent has slightly
more than two children). If one were concerned
about every byte this could be made quite small. In
our implementation probably the biggest factor is
the STL overhead on vectors. If we figure we are
using, say, 25 bytes per structure, the total space re-
quired is only 1.25Mb even for 50,000 dynamic pro-
gramming states, so it is clearly not worth worrying
about the memory required.
The resulting n-bests are quite good, as shown in
Table 2. (The results are for all sentences of sec-
tion 23 of the WSJ tree-bank of length ≤ 100.) From
the 1-best result we see that the base accuracy of the
parser is 89.7%.1 2-best and 10-best show dramatic
oracle-rate improvements. After that things start to
slow down, and we achieve an oracle rate of 0.968
at 50-best. To put this in perspective, Roark (Roark,
2001) reports oracle results of 0.941 (with the same
experimental setup) using his parser to return a vari-
able number of parses. For the case cited his parser
returns, on average, 70 parses per sentence.
Finally, we note that 50-best parsing is only a fac-
</bodyText>
<footnote confidence="0.963836">
1Charniak in (Charniak, 2000) cites an accuracy of 89.5%.
Fixing a few very small bugs discovered by users of the parser
accounts for the difference.
</footnote>
<page confidence="0.995967">
176
</page>
<bodyText confidence="0.970781">
tor of two or three slower than 1-best.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="method">
3 Features for reranking parses
</sectionHeader>
<bodyText confidence="0.999948089887641">
This section describes how each parse y is mapped
to a feature vector f(y) = (f1(y), ... , fm(y)). Each
feature fj is a function that maps a parse to a real
number. The first feature f1(y) = log p(y) is the
logarithm of the parse probability p according to
the n-best parser model. The other features are
integer valued; informally, each feature is associ-
ated with a particular configuration, and the feature’s
value fj(y) is the number of times that the config-
uration that fj indicates. For example, the feature
feat pizza(y) counts the number of times that a phrase
in y headed by eat has a complement phrase headed
by pizza.
Features belong to feature schema, which are ab-
stract schema from which specific features are in-
stantiated. For example, the feature feat pizza is an
instance of the “Heads” schema. Feature schema are
often parameterized in various ways. For example,
the “Heads” schema is parameterized by the type of
heads that the feature schema identifies. Following
Grimshaw (1997), we associate each phrase with a
lexical head and a function head. For example, the
lexical head of an NP is a noun while the functional
head of an NP is a determiner, and the lexical head
of a VP is a main verb while the functional head of
VP is an auxiliary verb.
We experimented with various kinds of feature
selection, and found that a simple count threshold
performs as well as any of the methods we tried.
Specifically, we ignored all features that did not vary
on the parses of at least t sentences, where t is the
count threshold. In the experiments described below
t = 5, though we also experimented with t = 2.
The rest of this section outlines the feature
schemata used in the experiments below. These fea-
ture schemata used here were developed using the
n-best parses provided to us by Michael Collins
approximately a year before the n-best parser de-
scribed here was developed. We used the division
into preliminary training and preliminary develop-
ment data sets described in Collins (2000) while
experimenting with feature schemata; i.e., the first
36,000 sentences of sections 2–20 were used as pre-
liminary training data, and the remaining sentences
of sections 20 and 21 were used as preliminary de-
velopment data. It is worth noting that develop-
ing feature schemata is much more of an art than
a science, as adding or deleting a single schema
usually does not have a significant effect on perfor-
mance, yet the overall impact of many well-chosen
schemata can be dramatic.
Using the 50-best parser output described here,
there are 1,148,697 features that meet the count
threshold of at least 5 on the main training data
(i.e., Penn treebank sections 2–21). We list each
feature schema’s name, followed by the number of
features in that schema with a count of at least 5, to-
gether with a brief description of the instances of the
schema and the schema’s parameters.
CoPar (10) The instances of this schema indicate
conjunct parallelism at various different depths.
For example, conjuncts which have the same
label are parallel at depth 0, conjuncts with the
same label and whose children have the same
label are parallel at depth 1, etc.
CoLenPar (22) The instances of this schema indi-
cate the binned difference in length (in terms
of number of preterminals dominated) in adja-
cent conjuncts in the same coordinated struc-
tures, conjoined with a boolean flag that indi-
cates whether the pair is final in the coordinated
phrase.
RightBranch (2) This schema enables the reranker
to prefer right-branching trees. One instance of
this schema returns the number of nonterminal
nodes that lie on the path from the root node
to the right-most non-punctuation preterminal
node, and the other instance of this schema
counts the number of the other nonterminal
nodes in the parse tree.
Heavy (1049) This schema classifies nodes by their
category, their binned length (i.e., the number
of preterminals they dominate), whether they
are at the end of the sentence and whether they
are followed by punctuation.
Neighbours (38,245) This schema classifies nodes
by their category, their binned length, and the
part of speech categories of the E1 preterminals
to the node’s left and the $2 preterminals to the
</bodyText>
<page confidence="0.99517">
177
</page>
<bodyText confidence="0.998705384615384">
node’s right. E1 and E2 are parameters of this
schema; here E1 = 1 or E1 = 2 and E2 = 1.
Rule (271,655) The instances of this schema are
local trees, annotated with varying amounts
of contextual information controlled by the
schema’s parameters. This schema was in-
spired by a similar schema in Collins and Koo
(in submission). The parameters to this schema
control whether nodes are annotated with their
preterminal heads, their terminal heads and
their ancestors’ categories. An additional pa-
rameter controls whether the feature is special-
ized to embedded or non-embedded clauses,
which roughly corresponds to Emonds’ “non-
root” and “root” contexts (Emonds, 1976).
NGram (54,567) The instances of this schema are
E-tuples of adjacent children nodes of the same
parent. This schema was inspired by a simi-
lar schema in Collins and Koo (in submission).
This schema has the same parameters as the
Rule schema, plus the length E of the tuples of
children (E = 2 here).
Heads (208,599) The instances of this schema are
tuples of head-to-head dependencies, as men-
tioned above. The category of the node that
is the least common ancestor of the head and
the dependent is included in the instance (this
provides a crude distinction between different
classes of arguments). The parameters of this
schema are whether the heads involved are lex-
ical or functional heads, the number of heads
in an instance, and whether the lexical item or
just the head’s part of speech are included in the
instance.
LezFunHeads (2,299) The instances of this feature
are the pairs of parts of speech of the lexical
head and the functional head of nodes in parse
trees.
WProj (158,771) The instances of this schema are
preterminals together with the categories of E of
their closest maximal projection ancestors. The
parameters of this schema control the number E
of maximal projections, and whether the preter-
minals and the ancestors are lexicalized.
Word (49,097) The instances of this schema are
lexical items together with the categories of E
of their immediate ancestor nodes, where E is
a schema parameter (E = 2 or E = 3 here).
This feature was inspired by a similar feature
in Klein and Manning (2003).
HeadTree (72,171) The instances of this schema
are tree fragments consisting of the local trees
consisting of the projections of a preterminal
node and the siblings of such projections. This
schema is parameterized by the head type (lex-
ical or functional) used to determine the pro-
jections of a preterminal, and whether the head
preterminal is lexicalized.
NGramTree (291,909) The instances of this
schema are subtrees rooted in the least com-
mon ancestor of E contiguous preterminal
nodes. This schema is parameterized by the
number E of contiguous preterminals (E = 2 or
E = 3 here) and whether these preterminals are
lexicalized.
</bodyText>
<sectionHeader confidence="0.936052" genericHeader="method">
4 Estimating feature weights
</sectionHeader>
<bodyText confidence="0.9998382">
This section explains how we estimate the feature
weights 0 = (01, ... , 0m) for the feature functions
f = (f1, ... , fm). We use a MaxEnt estimator to
find the feature weights ˆ0, where L is the loss func-
tion and R is a regularization penalty term:
</bodyText>
<equation confidence="0.625144">
0ˆ = arg min LD(0) + R(0).
θ
</equation>
<bodyText confidence="0.999801">
The training data D = (s1, ... , sn,) is a se-
quence of sentences and their correct parses
y?(s1), ... , y?(sn). We used the 20-fold cross-
validation technique described in Collins (2000)
to compute the n-best parses Y(s) for each sen-
tence s in D. In general the correct parse y?(s)
is not a member of Y(s), so instead we train the
reranker to identify one of the best parses Y+(s) =
arg maxy∈Y(s) Fy,,(s)(y) in the n-best parser’s out-
put, where Fy,,(y) is the Parseval f-score of y eval-
uated with respect to y?.
Because there may not be a unique best parse for
each sentence (i.e., |Y+(s) |&gt; 1 for some sentences
s) we used the variant of MaxEnt described in Rie-
zler et al. (2002) for partially labelled training data.
</bodyText>
<page confidence="0.996165">
178
</page>
<bodyText confidence="0.9895725">
Recall the standard MaxEnt conditional probability
model for a parse y E Y:
</bodyText>
<equation confidence="0.943933166666667">
exp vθ(y)
Pθ(y|Y) = where
Py,EY exp vθ (yl)
m
vθ(y) = θ · f(y) = X θjfj(y).
j=1
</equation>
<bodyText confidence="0.966727">
The loss function LD proposed in Riezler et al.
(2002) is just the negative log conditional likelihood
of the best parses Y+(s) relative to the n-best parser
output Y(s):
</bodyText>
<equation confidence="0.987269">
n�
LD(θ) = − X log Pθ(Y+(si)|Y(si)), where
i=1
X
Pθ(Y+|Y) =
yEY+
</equation>
<bodyText confidence="0.998798666666667">
The partial derivatives of this loss function, which
are required by the numerical estimation procedure,
are:
</bodyText>
<equation confidence="0.98875625">
Eθ[fj|Y(si)] − Eθ[fj|Y+(si)]
X
Eθ[f|Y] =
yEY
</equation>
<bodyText confidence="0.999928833333333">
In the experiments reported here, we used a Gaus-
sian or quadratic regularizer R(w) = cPmj=1 w2j,
where c is an adjustable parameter that controls
the amount of regularization, chosen to optimize
the reranker’s f-score on the development set (sec-
tion 24 of the treebank).
We used the Limited Memory Variable Metric op-
timization algorithm from the PETSc/TAO optimiza-
tion toolkit (Benson et al., 2004) to find the optimal
feature weights θˆ because this method seems sub-
stantially faster than comparable methods (Malouf,
2002). The PETSc/TAO toolkit provides a variety of
other optimization algorithms and flags for control-
ling convergence, but preliminary experiments on
the Collins’ trees with different algorithms and early
stopping did not show any performance improve-
ments, so we used the default PETSc/TAO setting for
our experiments here.
</bodyText>
<sectionHeader confidence="0.999366" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.9204698">
We evaluated the performance of our reranking
parser using the standard PARSEVAL metrics. We
n-best trees f-score
New 0.9102
Collins 0.9037
</bodyText>
<tableCaption confidence="0.655424">
Table 3: Results on new n-best trees and Collins n-
</tableCaption>
<bodyText confidence="0.999138705882353">
best trees, with weights estimated from sections 2–
21 and the regularizer constant c adjusted for op-
timal f-score on section 24 and evaluated on sen-
tences of length less than 100 in section 23.
trained the n-best parser on sections 2–21 of the
Penn Treebank, and used section 24 as development
data to tune the mixing parameters of the smooth-
ing model. Similarly, we trained the feature weights
θ with the MaxEnt reranker on sections 2–21, and
adjusted the regularizer constant c to maximize the
f-score on section 24 of the treebank. We did this
both on the trees supplied to us by Michael Collins,
and on the output of the n-best parser described in
this paper. The results are presented in Table 3. The
n-best parser’s most probable parses are already of
state-of-the-art quality, but the reranker further im-
proves the f-score.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999945285714286">
This paper has described a dynamic programming
n-best parsing algorithm that utilizes a heuristic
coarse-to-fine refinement of parses. Because the
coarse-to-fine approach prunes the set of possible
parse edges beforehand, a simple approach which
enumerates the n-best analyses of each parse edge
is not only practical but quite efficient.
We use the 50-best parses produced by this algo-
rithm as input to a MaxEnt discriminative reranker.
The reranker selects the best parse from this set of
parses using a wide variety of features. The sys-
tem we described here has an f-score of 0.91 when
trained and tested using the standard PARSEVAL
framework.
This result is only slightly higher than the highest
reported result for this test-set, Bod’s (.907) (Bod,
2003). More to the point, however, is that the sys-
tem we describe is reasonably efficient so it can
be used for the kind of routine parsing currently
being handled by the Charniak or Collins parsers.
A 91.0 f-score represents a 13% reduction in f-
</bodyText>
<figure confidence="0.895369857142857">
Pθ(y|Y)
n�
∂LD
θj
X
i=1
f(y)Pθ(y|Y)
</figure>
<page confidence="0.99326">
179
</page>
<bodyText confidence="0.999904454545455">
measure error over the best of these parsers.2 Both
the 50-best parser, and the reranking parser can be
found at ftp://ftp.cs.brown.edu/pub/nlparser/, named
parser and reranker respectively.
Acknowledgements We would like to thanks
Michael Collins for the use of his data and many
helpful comments, and Liang Huang for providing
an early draft of his paper and very useful comments
on our paper. Finally thanks to the National Science
Foundation for its support (NSF IIS-0112432, NSF
9721276, and NSF DMS-0074276).
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999431">
Steve Benson, Lois Curfman McInnes, Jorge J. Mor, and
Jason Sarich. 2004. Tao users manual. Technical Re-
port ANL/MCS-TM-242-Revision 1.6, Argonne Na-
tional Laboratory.
Daniel M. Bikel. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4).
Rens Bod. 2003. An efficient implementation of an new
dop model. In Proceedings of the European Chapter
of the Association for Computational Linguists.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132–139.
Michael Collins and Terry Koo. in submission. Discrim-
inative reranking for natural language parsing. Tech-
nical report, Computer Science and Artificial Intelli-
gence Laboratory, Massachusetts Institute of Technol-
ogy.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Morgan Kauf-
mann.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175–182, Stanford, California.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th Annual
</reference>
<footnote confidence="0.4091175">
2This probably underestimates the actual improvement.
There are no currently accepted figures for inter-annotater
agreement on Penn WSJ, but it is no doubt well short of 100%.
If we take 97% as a reasonable estimate of the the upper bound
on tree-bank accuracy, we are instead talking about an 18% er-
ror reduction.
</footnote>
<reference confidence="0.998819916666667">
Meeting of the Association for Computational Linguis-
tics, pages 457–464.
Joseph Emonds. 1976. A Transformational Approach to
English Syntax: Root, Structure-Preserving and Local
Transformations. Academic Press, New York, NY.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 1997).
Jane Grimshaw. 1997. Projection, heads, and optimality.
Linguistic Inquiry, 28(3):373–422.
Liang Huang and David Chang. 2005. Better k-best pars-
ing. Technical Report MS-CIS-05-08, Department of
Computer Science, University of Pennsylvania.
Victor M. Jimenez and Andres Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proceedings of the Joint
IAPR International Workshops on Advances in Pattern
Recognition. Springer LNCS 1876.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic “unification-based” grammars. In The Proceedings
of the 37th Annual Conference of the Association for
Computational Linguistics, pages 535–541, San Fran-
cisco. Morgan Kaufmann.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the Sixth Conference on Natural Language
Learning (CoNLL-2002), pages 49–55.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 271–278. Morgan Kaufmann.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
R. Schwartz and Y.L. Chow. 1990. The n-best algo-
rithm: An efficient and exact procedure for finding
the n most likely sentence hypotheses. In Proceed-
ings of the IEEE International Conference on Acous-
tic, Speech, and Signal, Processing, pages 81–84.
</reference>
<page confidence="0.997751">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961916">
<title confidence="0.999514">parsing and MaxEnt discriminative reranking</title>
<author confidence="0.999956">Eugene Charniak</author>
<author confidence="0.999956">Mark Johnson</author>
<affiliation confidence="0.99931">Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University</affiliation>
<address confidence="0.999335">Providence, RI 02912</address>
<abstract confidence="0.997816833333333">Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steve Benson</author>
<author>Lois Curfman McInnes</author>
<author>Jorge J Mor</author>
<author>Jason Sarich</author>
</authors>
<title>Tao users manual.</title>
<date>2004</date>
<tech>Technical Report ANL/MCS-TM-242-Revision 1.6,</tech>
<institution>Argonne National Laboratory.</institution>
<contexts>
<context position="25565" citStr="Benson et al., 2004" startWordPosition="4342" endWordPosition="4345">: n� LD(θ) = − X log Pθ(Y+(si)|Y(si)), where i=1 X Pθ(Y+|Y) = yEY+ The partial derivatives of this loss function, which are required by the numerical estimation procedure, are: Eθ[fj|Y(si)] − Eθ[fj|Y+(si)] X Eθ[f|Y] = yEY In the experiments reported here, we used a Gaussian or quadratic regularizer R(w) = cPmj=1 w2j, where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the reranker’s f-score on the development set (section 24 of the treebank). We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002). The PETSc/TAO toolkit provides a variety of other optimization algorithms and flags for controlling convergence, but preliminary experiments on the Collins’ trees with different algorithms and early stopping did not show any performance improvements, so we used the default PETSc/TAO setting for our experiments here. 5 Experimental results We evaluated the performance of our reranking parser using the standard PARSEVAL metrics. We n-best trees f-score New 0.9102 Collins</context>
</contexts>
<marker>Benson, McInnes, Mor, Sarich, 2004</marker>
<rawString>Steve Benson, Lois Curfman McInnes, Jorge J. Mor, and Jason Sarich. 2004. Tao users manual. Technical Report ANL/MCS-TM-242-Revision 1.6, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of collins parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7759" citStr="Bikel, 2004" startWordPosition="1306" endWordPosition="1307">ust be involved in one of the second through the n−1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins vθ(y). 174 parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2) dynamic programming states, where m i</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of collins parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of an new dop model.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguists.</booktitle>
<contexts>
<context position="27827" citStr="Bod, 2003" startWordPosition="4711" endWordPosition="4712">arse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient. We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker. The reranker selects the best parse from this set of parses using a wide variety of features. The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework. This result is only slightly higher than the highest reported result for this test-set, Bod’s (.907) (Bod, 2003). More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers. A 91.0 f-score represents a 13% reduction in fPθ(y|Y) n� ∂LD θj X i=1 f(y)Pθ(y|Y) 179 measure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftp://ftp.cs.brown.edu/pub/nlparser/, named parser and reranker respectively. Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for </context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of an new dop model. In Proceedings of the European Chapter of the Association for Computational Linguists.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In The Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1326" citStr="Charniak, 2000" startWordPosition="202" endWordPosition="203">to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. 1 Introduction We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model. The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser’s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art. However, the 50 best (i.e., the 50 highest probability) parses of a sentence often contain considerably better parses (in terms of f-score); this paper describes a 50-best parsing algorithm with an oracle f-score of 96.8 on the same data. The reranker attempts to select the best parse for a sentence from the 50-best list of possible parses for the sentence. Because the reranker only has to consider a relatively small number of parses per sentences, it is not necessary to use dynamic programming, which permits the features to be essentially arbitrary functions</context>
<context position="8754" citStr="Charniak (2000)" startWordPosition="1475" endWordPosition="1476"> one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2). However things get much worse when the grammar is bilexicalized. As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3) states, so a n-best parser would require O(nm3) states. Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser. By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising </context>
<context position="16379" citStr="Charniak, 2000" startWordPosition="2760" endWordPosition="2761">sentences of section 23 of the WSJ tree-bank of length ≤ 100.) From the 1-best result we see that the base accuracy of the parser is 89.7%.1 2-best and 10-best show dramatic oracle-rate improvements. After that things start to slow down, and we achieve an oracle rate of 0.968 at 50-best. To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses. For the case cited his parser returns, on average, 70 parses per sentence. Finally, we note that 50-best parsing is only a fac1Charniak in (Charniak, 2000) cites an accuracy of 89.5%. Fixing a few very small bugs discovered by users of the parser accounts for the difference. 176 tor of two or three slower than 1-best. 3 Features for reranking parses This section describes how each parse y is mapped to a feature vector f(y) = (f1(y), ... , fm(y)). Each feature fj is a function that maps a parse to a real number. The first feature f1(y) = log p(y) is the logarithm of the parse probability p according to the n-best parser model. The other features are integer valued; informally, each feature is associated with a particular configuration, and the fe</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In The Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>in submission. Discriminative reranking for natural language parsing.</title>
<tech>Technical report,</tech>
<institution>Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology.</institution>
<marker>Collins, Koo, </marker>
<rawString>Michael Collins and Terry Koo. in submission. Discriminative reranking for natural language parsing. Technical report, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In The Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="5087" citStr="Collins, 1997" startWordPosition="859" endWordPosition="860">-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one’s heart’s content. A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only wants the first-best, but obviously it does not directly enumerate the n-best parses. However, Collins (Collins, 2000; Collins and Koo, in submission) has created an nbest version of his parser by turning off dynamic programming (see the user’s guide to Bikel’s re-implementation of Collins’ parser, http://www.cis.upenn</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In The Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Machine Learning: Proceedings of the Seventeenth International Conference (ICML</booktitle>
<pages>175--182</pages>
<location>Stanford, California.</location>
<contexts>
<context position="2773" citStr="Collins (2000)" startWordPosition="448" endWordPosition="449">re detail, for each string s the n-best parsing algorithm described in section 2 returns the n highest probability parses Y(s) = {y1(s), ... , yn(s)} together with the probability p(y) of each parse y according to the parser’s probability model. The number n of parses was set to 50 for the experiments described here, but some simple sentences actually received fewer than 50 parses (so n is actually a function of s). Each yield or terminal string in the training, development and test data sets is mapped to such an n-best list of parse/probability pairs; the cross-validation scheme described in Collins (2000) was used to avoid training the n-best parser on the sentence it was being used to parse. A feature extractor, described in section 3, is a vector of m functions f = (fl, ... , fm), where each fj maps a parse y to a real number fj(y), which is the value of the jth feature on y. So a feature extractor maps each y to a vector of feature values f(y) = (f1(y), ..., fm(y)). Our reranking parser associates a parse with a 173 Proceedings of the 43rd Annual Meeting of the ACL, pages 173–180, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics score vθ(y), which is a linear function </context>
<context position="5484" citStr="Collins, 2000" startWordPosition="926" endWordPosition="927">eam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only wants the first-best, but obviously it does not directly enumerate the n-best parses. However, Collins (Collins, 2000; Collins and Koo, in submission) has created an nbest version of his parser by turning off dynamic programming (see the user’s guide to Bikel’s re-implementation of Collins’ parser, http://www.cis.upenn.edu/ dbikel/software.html#statparser). As with Roark’s parser, it is necessary to add a beam-width constraint to make the search tractable. With a beam width of 1000 the parser returns something like a 50-best list (Collins, personal communication), but the actual number of parses returned for each sentences varies. However, turning off dynamic programming results in a loss in efficiency. Inde</context>
<context position="18575" citStr="Collins (2000)" startWordPosition="3145" endWordPosition="3146">of the methods we tried. Specifically, we ignored all features that did not vary on the parses of at least t sentences, where t is the count threshold. In the experiments described below t = 5, though we also experimented with t = 2. The rest of this section outlines the feature schemata used in the experiments below. These feature schemata used here were developed using the n-best parses provided to us by Michael Collins approximately a year before the n-best parser described here was developed. We used the division into preliminary training and preliminary development data sets described in Collins (2000) while experimenting with feature schemata; i.e., the first 36,000 sentences of sections 2–20 were used as preliminary training data, and the remaining sentences of sections 20 and 21 were used as preliminary development data. It is worth noting that developing feature schemata is much more of an art than a science, as adding or deleting a single schema usually does not have a significant effect on performance, yet the overall impact of many well-chosen schemata can be dramatic. Using the 50-best parser output described here, there are 1,148,697 features that meet the count threshold of at lea</context>
<context position="24083" citStr="Collins (2000)" startWordPosition="4080" endWordPosition="4081">terized by the number E of contiguous preterminals (E = 2 or E = 3 here) and whether these preterminals are lexicalized. 4 Estimating feature weights This section explains how we estimate the feature weights 0 = (01, ... , 0m) for the feature functions f = (f1, ... , fm). We use a MaxEnt estimator to find the feature weights ˆ0, where L is the loss function and R is a regularization penalty term: 0ˆ = arg min LD(0) + R(0). θ The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?(s1), ... , y?(sn). We used the 20-fold crossvalidation technique described in Collins (2000) to compute the n-best parses Y(s) for each sentence s in D. In general the correct parse y?(s) is not a member of Y(s), so instead we train the reranker to identify one of the best parses Y+(s) = arg maxy∈Y(s) Fy,,(s)(y) in the n-best parser’s output, where Fy,,(y) is the Parseval f-score of y evaluated with respect to y?. Because there may not be a unique best parse for each sentence (i.e., |Y+(s) |&gt; 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data. 178 Recall the standard MaxEnt conditional probability model for a </context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Machine Learning: Proceedings of the Seventeenth International Conference (ICML 2000), pages 175–182, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>457--464</pages>
<contexts>
<context position="8545" citStr="Eisner and Satta, 1999" startWordPosition="1439" endWordPosition="1442">ashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2). However things get much worse when the grammar is bilexicalized. As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3) states, so a n-best parser would require O(nm3) states. Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 457–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Emonds</author>
</authors>
<title>A Transformational Approach to English Syntax: Root, Structure-Preserving and Local Transformations.</title>
<date>1976</date>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="21442" citStr="Emonds, 1976" startWordPosition="3620" endWordPosition="3621">ema; here E1 = 1 or E1 = 2 and E2 = 1. Rule (271,655) The instances of this schema are local trees, annotated with varying amounts of contextual information controlled by the schema’s parameters. This schema was inspired by a similar schema in Collins and Koo (in submission). The parameters to this schema control whether nodes are annotated with their preterminal heads, their terminal heads and their ancestors’ categories. An additional parameter controls whether the feature is specialized to embedded or non-embedded clauses, which roughly corresponds to Emonds’ “nonroot” and “root” contexts (Emonds, 1976). NGram (54,567) The instances of this schema are E-tuples of adjacent children nodes of the same parent. This schema was inspired by a similar schema in Collins and Koo (in submission). This schema has the same parameters as the Rule schema, plus the length E of the tuples of children (E = 2 here). Heads (208,599) The instances of this schema are tuples of head-to-head dependencies, as mentioned above. The category of the node that is the least common ancestor of the head and the dependent is included in the instance (this provides a crude distinction between different classes of arguments). </context>
</contexts>
<marker>Emonds, 1976</marker>
<rawString>Joseph Emonds. 1976. A Transformational Approach to English Syntax: Root, Structure-Preserving and Local Transformations. Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="9431" citStr="Goodman (1997)" startWordPosition="1579" endWordPosition="1580">ming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser. By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states. A prime example of this idea is from Goodman (1997), who describes amethod for producing a simple but crude approximate grammar of a standard context-free grammar. He parses a sentence using the approximate grammar, and the results are used to constrain the search for a parse with the full CFG. He finds that total parsing time is greatly reduced. A somewhat different take on this paradigm is seen in the parser we use in this paper. Here the parser first creates a parse forest based upon a much less complex version of the complete grammar. In particular, it only looks at standard CFG features, the parent and neighbor labels. Because this gramma</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Projection, heads, and optimality.</title>
<date>1997</date>
<journal>Linguistic Inquiry,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="17573" citStr="Grimshaw (1997)" startWordPosition="2966" endWordPosition="2967">iguration, and the feature’s value fj(y) is the number of times that the configuration that fj indicates. For example, the feature feat pizza(y) counts the number of times that a phrase in y headed by eat has a complement phrase headed by pizza. Features belong to feature schema, which are abstract schema from which specific features are instantiated. For example, the feature feat pizza is an instance of the “Heads” schema. Feature schema are often parameterized in various ways. For example, the “Heads” schema is parameterized by the type of heads that the feature schema identifies. Following Grimshaw (1997), we associate each phrase with a lexical head and a function head. For example, the lexical head of an NP is a noun while the functional head of an NP is a determiner, and the lexical head of a VP is a main verb while the functional head of VP is an auxiliary verb. We experimented with various kinds of feature selection, and found that a simple count threshold performs as well as any of the methods we tried. Specifically, we ignored all features that did not vary on the parses of at least t sentences, where t is the count threshold. In the experiments described below t = 5, though we also exp</context>
</contexts>
<marker>Grimshaw, 1997</marker>
<rawString>Jane Grimshaw. 1997. Projection, heads, and optimality. Linguistic Inquiry, 28(3):373–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-08,</tech>
<institution>Department of Computer Science, University of Pennsylvania.</institution>
<contexts>
<context position="7590" citStr="Huang and Chang, 2005" startWordPosition="1277" endWordPosition="1280">timal decision must be the second-best choice at that choice point. Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n−1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins vθ(y). 174 parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered</context>
</contexts>
<marker>Huang, Chang, 2005</marker>
<rawString>Liang Huang and David Chang. 2005. Better k-best parsing. Technical Report MS-CIS-05-08, Department of Computer Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor M Jimenez</author>
<author>Andres Marzal</author>
</authors>
<title>Computation of the n best parse trees for weighted and stochastic context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition. Springer LNCS</booktitle>
<pages>1876</pages>
<contexts>
<context position="7566" citStr="Jimenez and Marzal, 2000" startWordPosition="1273" endWordPosition="1276">optimal, and the one suboptimal decision must be the second-best choice at that choice point. Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n−1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins vθ(y). 174 parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses </context>
</contexts>
<marker>Jimenez, Marzal, 2000</marker>
<rawString>Victor M. Jimenez and Andres Marzal. 2000. Computation of the n best parse trees for weighted and stochastic context-free grammars. In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition. Springer LNCS 1876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>535--541</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="753" citStr="Johnson et al., 1999" startWordPosition="102" endWordPosition="105">formation Processing (BLLIP) Brown University Providence, RI 02912 {mj|ec}@cs.brown.edu Abstract Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. 1 Introduction We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model. The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser’s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-t</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics, pages 535–541, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22956" citStr="Klein and Manning (2003)" startWordPosition="3879" endWordPosition="3882">parts of speech of the lexical head and the functional head of nodes in parse trees. WProj (158,771) The instances of this schema are preterminals together with the categories of E of their closest maximal projection ancestors. The parameters of this schema control the number E of maximal projections, and whether the preterminals and the ancestors are lexicalized. Word (49,097) The instances of this schema are lexical items together with the categories of E of their immediate ancestor nodes, where E is a schema parameter (E = 2 or E = 3 here). This feature was inspired by a similar feature in Klein and Manning (2003). HeadTree (72,171) The instances of this schema are tree fragments consisting of the local trees consisting of the projections of a preterminal node and the siblings of such projections. This schema is parameterized by the head type (lexical or functional) used to determine the projections of a preterminal, and whether the head preterminal is lexicalized. NGramTree (291,909) The instances of this schema are subtrees rooted in the least common ancestor of E contiguous preterminal nodes. This schema is parameterized by the number E of contiguous preterminals (E = 2 or E = 3 here) and whether th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>49--55</pages>
<contexts>
<context position="25690" citStr="Malouf, 2002" startWordPosition="4363" endWordPosition="4364"> by the numerical estimation procedure, are: Eθ[fj|Y(si)] − Eθ[fj|Y+(si)] X Eθ[f|Y] = yEY In the experiments reported here, we used a Gaussian or quadratic regularizer R(w) = cPmj=1 w2j, where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the reranker’s f-score on the development set (section 24 of the treebank). We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002). The PETSc/TAO toolkit provides a variety of other optimization algorithms and flags for controlling convergence, but preliminary experiments on the Collins’ trees with different algorithms and early stopping did not show any performance improvements, so we used the default PETSc/TAO setting for our experiments here. 5 Experimental results We evaluated the performance of our reranking parser using the standard PARSEVAL metrics. We n-best trees f-score New 0.9102 Collins 0.9037 Table 3: Results on new n-best trees and Collins nbest trees, with weights estimated from sections 2– 21 and the regu</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002), pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="776" citStr="Riezler et al., 2002" startWordPosition="106" endWordPosition="109">BLLIP) Brown University Providence, RI 02912 {mj|ec}@cs.brown.edu Abstract Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. 1 Introduction We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model. The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser’s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art. However, the 50</context>
<context position="24577" citStr="Riezler et al. (2002)" startWordPosition="4173" endWordPosition="4177">tences and their correct parses y?(s1), ... , y?(sn). We used the 20-fold crossvalidation technique described in Collins (2000) to compute the n-best parses Y(s) for each sentence s in D. In general the correct parse y?(s) is not a member of Y(s), so instead we train the reranker to identify one of the best parses Y+(s) = arg maxy∈Y(s) Fy,,(s)(y) in the n-best parser’s output, where Fy,,(y) is the Parseval f-score of y evaluated with respect to y?. Because there may not be a unique best parse for each sentence (i.e., |Y+(s) |&gt; 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data. 178 Recall the standard MaxEnt conditional probability model for a parse y E Y: exp vθ(y) Pθ(y|Y) = where Py,EY exp vθ (yl) m vθ(y) = θ · f(y) = X θjfj(y). j=1 The loss function LD proposed in Riezler et al. (2002) is just the negative log conditional likelihood of the best parses Y+(s) relative to the n-best parser output Y(s): n� LD(θ) = − X log Pθ(Y+(si)|Y(si)), where i=1 X Pθ(Y+|Y) = yEY+ The partial derivatives of this loss function, which are required by the numerical estimation procedure, are: Eθ[fj|Y(si)] − Eθ[fj|Y+(si)] X Eθ[f|Y] = yEY In the exp</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. III Maxwell, and Mark Johnson. 2002. Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 271–278. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="4775" citStr="Roark, 2001" startWordPosition="807" endWordPosition="808">r θ that maximizes the score vθ(y) of the parses y E Y+(s) relative to the scores of the other parses in Y(s), for each s in the training data. 2 Recovering the n-best parses using coarse-to-fine parsing The major difficulty in n-best parsing, compared to 1-best parsing, is dynamic programming. For example, n-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one’s heart’s content. A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only wants th</context>
<context position="16100" citStr="Roark, 2001" startWordPosition="2712" endWordPosition="2713">e we are using, say, 25 bytes per structure, the total space required is only 1.25Mb even for 50,000 dynamic programming states, so it is clearly not worth worrying about the memory required. The resulting n-bests are quite good, as shown in Table 2. (The results are for all sentences of section 23 of the WSJ tree-bank of length ≤ 100.) From the 1-best result we see that the base accuracy of the parser is 89.7%.1 2-best and 10-best show dramatic oracle-rate improvements. After that things start to slow down, and we achieve an oracle rate of 0.968 at 50-best. To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses. For the case cited his parser returns, on average, 70 parses per sentence. Finally, we note that 50-best parsing is only a fac1Charniak in (Charniak, 2000) cites an accuracy of 89.5%. Fixing a few very small bugs discovered by users of the parser accounts for the difference. 176 tor of two or three slower than 1-best. 3 Features for reranking parses This section describes how each parse y is mapped to a feature vector f(y) = (f1(y), ... , fm(y)). Each feature fj is a func</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>Y L Chow</author>
</authors>
<title>The n-best algorithm: An efficient and exact procedure for finding the n most likely sentence hypotheses.</title>
<date>1990</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal, Processing,</booktitle>
<pages>81--84</pages>
<contexts>
<context position="6472" citStr="Schwartz and Chow (1990)" startWordPosition="1083" endWordPosition="1086"> 1000 the parser returns something like a 50-best list (Collins, personal communication), but the actual number of parses returned for each sentences varies. However, turning off dynamic programming results in a loss in efficiency. Indeed, Collins’s n-best list of parses for section 24 of the Penn tree-bank has some sentences with only a single parse, because the n-best parser could not find any parses. Now there are two known ways to produce n-best parses while retaining the use of dynamic programming: the obvious way and the clever way. The clever way is based upon an algorithm developed by Schwartz and Chow (1990). Recall the key insight in the Viterbi algorithm: in the optimal parse the parsing decisions at each of the choice points that determine a parse must be optimal, since otherwise one could find a better parse. This insight extends to n-best parsing as follows. Consider the secondbest parse: if it is to differ from the best parse, then at least one of its parsing decisions must be suboptimal. In fact, all but one of the parsing decisions in second-best parse must be optimal, and the one suboptimal decision must be the second-best choice at that choice point. Further, the nth-best parse can only</context>
</contexts>
<marker>Schwartz, Chow, 1990</marker>
<rawString>R. Schwartz and Y.L. Chow. 1990. The n-best algorithm: An efficient and exact procedure for finding the n most likely sentence hypotheses. In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal, Processing, pages 81–84.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>