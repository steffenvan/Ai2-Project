<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9518855">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 343-351.
</note>
<title confidence="0.990197">
Word Translation Disambiguation Using Bilingual Bootstrapping
</title>
<author confidence="0.998516">
Cong Li
</author>
<affiliation confidence="0.996329">
Microsoft Research Asia
</affiliation>
<address confidence="0.9772895">
5F Sigma Center, No.49 Zhichun Road, Haidian
Beijing, China, 100080
</address>
<email confidence="0.994377">
i-congl@microsoft.com
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998950173913044">
This paper proposes a new method for
word translation disambiguation using
a machine learning technique called
‘Bilingual Bootstrapping’. Bilingual
Bootstrapping makes use of in
learning a small number of classified
data and a large number of unclassified
data in the source and the target
languages in translation. It constructs
classifiers in the two languages in
parallel and repeatedly boosts the
performances of the classifiers by
further classifying data in each of the
two languages and by exchanging
between the two languages
information regarding the classified
data. Experimental results indicate that
word translation disambiguation based
on Bilingual Bootstrapping
consistently and significantly
outperforms the existing methods
based on ‘Monolingual
Bootstrapping’.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997170818181818">
We address here the problem of word translation
disambiguation. For instance, we are concerned
with an ambiguous word in English (e.g., ‘plant’),
which has multiple translations in Chinese (e.g.,
‘ (gongchang)’ and ‘ (zhiwu)’). Our
goal is to determine the correct Chinese
translation of the ambiguous English word, given
an English sentence which contains the word.
Word translation disambiguation is actually a
special case of word sense disambiguation (in the
example above, ‘gongchang’ corresponds to the
</bodyText>
<author confidence="0.913133">
Hang Li
</author>
<affiliation confidence="0.964687">
Microsoft Research Asia
</affiliation>
<address confidence="0.965422">
5F Sigma Center, No.49 Zhichun Road, Haidian
Beijing, China, 100080
</address>
<email confidence="0.969932">
hangli@microsoft.com
</email>
<bodyText confidence="0.9986484">
sense of ‘factory’ and ‘zhiwu’ corresponds to the
sense of ‘vegetation’).1
Yarowsky (1995) proposes a method for word
sense (translation) disambiguation that is based
on a bootstrapping technique, which we refer to
here as ‘Monolingual Bootstrapping (MB)’.
In this paper, we propose a new method for word
translation disambiguation using a bootstrapping
technique we have developed. We refer to the
technique as ‘Bilingual Bootstrapping (BB)’.
In order to evaluate the performance of BB, we
conducted some experiments on word translation
disambiguation using the BB technique and the
MB technique. All of the results indicate that BB
consistently and significantly outperforms MB.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99873975">
The problem of word translation disambiguation
(in general, word sense disambiguation) can be
viewed as that of classification and can be
addressed by employing a supervised learning
method. In such a learning method, for instance,
an English sentence containing an ambiguous
English word corresponds to an example, and the
Chinese translation of the word under the context
corresponds to a classification decision (a label).
Many methods for word sense disambiguation
using a supervised learning technique have been
proposed. They include those using Naïve Bayes
(Gale et al. 1992a), Decision List (Yarowsky
1994), Nearest Neighbor (Ng and Lee 1996),
Transformation Based Learning (Mangu and
Brill 1997), Neural Network (Towell and
</bodyText>
<footnote confidence="0.98506725">
1 In this paper, we take English-Chinese translation as
example; it is a relatively easy process, however, to
extend the discussions to translations between other
language pairs.
</footnote>
<bodyText confidence="0.9949304">
Voorhess 1998), Winnow (Golding and Roth
1999), Boosting (Escudero et al. 2000), and
Naïve Bayesian Ensemble (Pedersen 2000).
Among these methods, the one using Naïve
Bayesian Ensemble (i.e., an ensemble of Naïve
Bayesian Classifiers) is reported to perform the
best for word sense disambiguation with respect
to a benchmark data set (Pedersen 2000).
The assumption behind the proposed methods is
that it is nearly always possible to determine the
translation of a word by referring to its context,
and thus all of the methods actually manage to
build a classifier (i.e., a classification program)
using features representing context information
(e.g., co-occurring words).
Since preparing supervised learning data is
expensive (in many cases, manually labeling data
is required), it is desirable to develop a
bootstrapping method that starts learning with a
small number of classified data but is still able to
achieve high performance under the help of a
large number of unclassified data which is not
expensive anyway.
Yarowsky (1995) proposes a method for word
sense disambiguation, which is based on
Monolingual Bootstrapping. When applied to our
current task, his method starts learning with a
small number of English sentences which contain
an ambiguous English word and which are
respectively assigned with the correct Chinese
translations of the word. It then uses the
classified sentences as training data to learn a
classifier (e.g., a decision list) and uses the
constructed classifier to classify some
unclassified sentences containing the ambiguous
word as additional training data. It also adopts the
heuristics of ‘one sense per discourse’ (Gale et al.
1992b) to further classify unclassified sentences.
By repeating the above processes, it can create an
accurate classifier for word translation
disambiguation.
For other related work, see, for example, (Brown
et al. 1991; Dagan and Itai 1994; Pedersen and
Bruce 1997; Schutze 1998; Kikui 1999;
Mihalcea and Moldovan 1999).
</bodyText>
<sectionHeader confidence="0.97424" genericHeader="method">
3 Bilingual Bootstrapping
</sectionHeader>
<subsectionHeader confidence="0.977573">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999967333333333">
Instead of using Monolingual Bootstrapping, we
propose a new method for word translation
disambiguation using Bilingual Bootstrapping.
In translation from English to Chinese, for
instance, BB makes use of not only unclassified
data in English, but also unclassified data in
Chinese. It also uses a small number of classified
data in English and, optionally, a small number
of classified data in Chinese. The data in English
and in Chinese are supposed to be not in parallel
but from the same domain.
BB constructs classifiers for English to Chinese
translation disambiguation by repeating the
following two steps: (1) constructing classifiers
for each of the languages on the basis of the
classified data in both languages, (2) using the
constructed classifiers in each of the languages to
classify some unclassified data and adding them
to the classified training data set of the language.
The reason that we can use classified data in both
languages at step (1) is that words in one
language generally have translations in the other
and we can find their translation relationship by
using a dictionary.
</bodyText>
<subsectionHeader confidence="0.998708">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.948634142857143">
Let E denote a set of words in English, C a set of
words in Chinese, and T a set of links in a
translation dictionary as shown in Figure 1. (Any
two linked words can be translation of each other.)
Mathematically, T is defined as a relation
between E and C , i.e., T ❑ E 11C.
Let Ostand for a random variable on E, Oa
random variable on C. Also let e stand for a
random variable on E, c a random variable on C,
and t a random variable on T. While O and
L7ffl*resent words to be translated, e and c
represent context words.
For an English word q
To ❑ {t  |t ❑ (C, ��), t ❑ T} represents the links
</bodyText>
<figureCaption confidence="0.998618">
Figure 1: Example of translation dictionary
</figureCaption>
<equation confidence="0.993669885057472">
M
M
M
M
M
M
Input : E, C, T, LE , UE , LC , UC , Parameter : b, θ
Repeat in parallel the following processes for English (left) and Chinese (right), until unable to continue :
1. for each ( ε∈ E) { for each ( γ∈ C) {
for each ( t ∈ Tε) {
use Lε and Lγ (γ∈ Cε) to create classifier:
Pε(t  |e), t ∈ Tε &amp; Pε(t  |e), t ∈ Tε − {t} ; }}
for each ( t ∈ Tγ) {
use Lγ and Lε (ε∈ Eγ)to create classifier:
Pγ(t  |c), t ∈ Tγ &amp; Pγ(t  |c), t ∈ Tγ − {t} ; }}
for each ( γ∈ C) {
NU ← {};NL ← {};
2. for each ( ε∈ E) {
NU ← {};NL ← {};
for each ( t ∈Tε) {St ← {};Qt ← { } ;}
for each ( e ∈ Uε){
for each ( t∈Tγ) {St ← {};Qt ← { } ;}
for each ( c ∈ Uγ){
*
t
(c)
let
P t e
calculate ε(  |)
λ * ( ) max
e = ;
t T ε
∈ P t
(  |)
e
ε
= arg max
Pγ(t |c) ;
t T
∈ γP t
(  |)
c
γ
P t
(  |)
e
let ε
t * ( ) arg max
e = ;
t T
∈ ε P t
(  |)
e
ε
calculate
P t
γ(  |)
c
= max ;
λ * ( )
c
c)
t ∈ T γ
Pγ(t |
if (λ*(e) &gt;θ &amp; t*(e) =t)
put e into St ;}
for each ( t ∈ Tε){
sort e ∈ St in descending order of ( )
λ * e and
put the top b elements into Qt ;}
for each ( e ∈ UQt
t
put e into NU and put (e, ∗(e))
t into NL;}
if ( λ*(c) &gt;θ &amp; t*(c) =t)
put c into St ;}
for each ( t ∈ Tγ){
sort c ∈ St in descending order of ( )
λ * c and
put the top b elements into Qt ;}
for each (c ∈ U Qt ){
t
put c into NU and put (c, ∗(c))
t into NL;}
){
Lε ← LεU NL; Uε← Uε−NU;} Lγ ← LγUNL;Uγ← Uγ−NU;}
Output: classifiers in English and Chinese
</equation>
<figureCaption confidence="0.996811">
Figure 2: Bilingual Bootstrapping
</figureCaption>
<bodyText confidence="0.993320777777778">
from it, and Cε = {γ′ |(ε, γ′) ∈ T} represents the
Chinese words which are linked to it. For a
Chinese word γ, let Tγ = {t  |t = (ε,′γ), t ∈ T} and
Eγ ={ε′ |(ε,′γ) ∈ T} . We can define Ce and Ec
similarly.
Let e denote a sequence of words (e.g., a sentence
or a text) in English
e = e e L e m e i ∈ E i = L m
{ 1 , 2 , , }, ( 1,2, , ) .
Let c denote a sequence of words in Chinese
c = c c L c n c i ∈ C i = L n
{ 1 , 2 , , } , ( 1,2, , ) .
We view e and c as examples representing
context information for translation
disambiguation.
For an English word ε, we define a binary
classifier for resolving each of its translation
ambiguities in Tε in a general form as:
</bodyText>
<equation confidence="0.9973078">
P t t T
ε e ∈ ε
(  |), &amp; (  |), {
P t t T t
ε e ∈ ε −
</equation>
<bodyText confidence="0.99995">
where e denotes an example in English. Similarly,
for a Chinese word γ, we define a classifier as:
</bodyText>
<equation confidence="0.998925666666667">
P t t T
γ c ∈ γ
(  |), &amp; (  |), {
P t
γ c ∈ γ −
t T t
</equation>
<bodyText confidence="0.943383333333333">
where c denotes an example in Chinese.
Let Lε denote a set of classified examples in
English, each representing one context of ε
</bodyText>
<equation confidence="0.964672">
L ={( 1 , 1) , ( 2, 2) , , (
e t e t L e
ε ε ε
</equation>
<bodyText confidence="0.995041333333333">
ti ∈Tε(i =1,2,L ,k
and Uε a set of unclassified examples in English,
each representing one context of ε
</bodyText>
<equation confidence="0.993822">
U = e e L el
{( 1) , ( 2) , , ( ) } .
ε ε ε ε
</equation>
<bodyText confidence="0.9928625">
Similarly, we denote the sets of classified and
unclassified examples with respect to γ in
Chinese as Lγ and Uγ respectively.
Furthermore, we have
</bodyText>
<equation confidence="0.9429165">
LE = U Lε, LC = U Lγ, UE = U Uε, UC = U Uγ.
ε∈E γ∈C ε∈E γ∈C
</equation>
<bodyText confidence="0.9999326">
We perform Bilingual Bootstrapping as
described in Figure 2. Hereafter, we will only
explain the process for English (left-hand side);
the process for Chinese (right-hand side) can be
conducted similarly.
</bodyText>
<subsectionHeader confidence="0.997096">
3.3 Naïve Bayesian Classifier
</subsectionHeader>
<bodyText confidence="0.8813735">
} ,
} ,
</bodyText>
<equation confidence="0.811803583333333">
,
)
k
ε} ,
tk
),
.
e) =max Pε(t)Pε(e  |t)
fc t P e c t
( , ) (  |, )
∑
c C
∈
←
( , )
c t
f
Lγ
= {(c1, t1 )γ, (c2, t2
c C
∈ e
if
(1)
),
estimate (  |)
P ( E ) e t with MLE using Lε as data;
ε
estimate (  |)
P ( C ) e t with EM Algorithm using L γ
ε
for each γ ∈ Cε as data;
calculate Pε(e  |t) as a linear combination of
P( E) e t
(  |) and (  |)
P( C) e t ;
ε ε
</equation>
<table confidence="0.622747222222222">
estimate Pε(t) with MLE using Lε;
calculate Pε(e  |t) and Pε(t ) similarly.
Figure 3: Creating Naïve Bayesian Classifier
While we can in principle employ any kind of
classifier in BB, we use here a Naïve Bayesian
Classifier. At step 1 in BB, we construct the
classifier as described in Figure 3. At step 2, for
each example e, we calculate with the Naïve
Bayesian Classifier:
</table>
<equation confidence="0.774333125">
t T
∈ t T ε
P t
(  |) ∈
ε e P t P t
( ) (  |)
e
ε ε ε
</equation>
<bodyText confidence="0.586738666666667">
The second equation is based on Bayes’ rule.
In the calculation, we assume that the context
words in e (i.e., e 1 , e2,L , em) are independently
</bodyText>
<figure confidence="0.7821915">
generated from Pε(e  |t) and thus we have
m
</figure>
<figureCaption confidence="0.991481">
Figure 4: EM Algorithm
</figureCaption>
<bodyText confidence="0.534826">
the form P(c  |t) = ∑ P c e t P e t
</bodyText>
<equation confidence="0.95492775">
(  |, ) (  |) and for a
e E
∈
specific ε we assume that the data in
t T i
γ ( 1, , ),
L h γ C
i ∈ = ∀ ∈ ε
</equation>
<bodyText confidence="0.995983285714286">
are independently generated on the basis of the
model. We can, therefore, employ the
Expectation and Maximization Algorithm (EM
Algorithm) (Dempster et al. 1977) to estimate the
parameters of the model including P(e  |t) . We
also use the relation T in the estimation.
Initially, we set
</bodyText>
<equation confidence="0.983962260869565">
λ *(e)
For Pε(e |t), we calculate it at step 1 by linearly
combining (  |)
P ( E ) e t estimated from English
ε
and (  |)
P ( C ) e t estimated from Chinese:
ε
(  |) (1
= − − ) (  |)
( )
P e t P e t
E
α β
ε ε
P e t P e
( )
C
+ (  |) (
( )
U
α + β
ε
</equation>
<bodyText confidence="0.992452333333333">
where 0≤ α≤1 , 0≤ β≤1 , α+ β≤1 , and
P(U) e is a uniform distribution over E , which
( )
is used for avoiding zero probability. In this way,
we estimate Pε(e  |t) using information from not
only English but also Chinese.
</bodyText>
<figure confidence="0.606140777777778">
For (  |)
P( E) e t , we estimate it with MLE
ε
(Maximum Likelihood Estimation) using Lε as
data. For (  |)
P( C) e t , we estimate it as is
ε
described in Section 3.4.
3.4 EM Algorithm
</figure>
<figureCaption confidence="0.155365">
For the sake of readability, we rewrite (  |)
</figureCaption>
<equation confidence="0.8349733">
P( C) e t
ε
as P(e  |t) . We define a finite mixture model of
1
P e t =
(  |) , .
e E
∈
 ||
E
</equation>
<bodyText confidence="0.9995055">
We next estimate the parameters by iteratively
updating them ass described in Figure 4 until
they converge. Here f (c, t) stands for the
frequency of c related to t. The context
information in Chinese is then ‘translated’ into
that in English through the links in T.
</bodyText>
<sectionHeader confidence="0.509933" genericHeader="method">
4 Comparison between BB and MB
</sectionHeader>
<bodyText confidence="0.989572727272727">
We note that Monolingual Bootstrapping is a
special case of Bilingual Bootstrapping (consider
the situation in which α equals 0 in formula (1)).
Moreover, it seems safe to say that BB can
always perform better than MB.
The many-to-many relationship between the
words in the two languages stands out as key to
the higher performance of BB.
Suppose that the classifier with respect to ‘plant’
has two decisions (denoted as A and B in Figure
5). Further suppose that the classifiers with
</bodyText>
<figure confidence="0.908919867924528">
|
Pε(t
= max
,
L , (c
) } ,
γ
) ,
γ
th
h
Pε e t
(  |) = ∏ P ε e i t
( |
=1
We can calculate Pε(e  |t) similarly.
 1

P c e t
(  |, ) =  |Ce

,
c C
∉ e
0 , if
).
i
|
,
E-step: P(e c t) ← P
M-step: f
P c e t
(  |, ) ←
f
c C
∈
∑
c C
∈
(c  |e, t)P(e  |t)
∑
e E
∈
P
(c  |e, t)P(e  |t)
∑
( , ) (  |, )
c t P e c t
)
P e t
( |
( , ) (  |, )
c t P e c t
</figure>
<figureCaption confidence="0.999821">
Figure 5: Example of BB
</figureCaption>
<bodyText confidence="0.99965048">
respect to ‘gongchang’ and ‘zhiwu’ in Chinese
have two decisions respectively, (C and D) (E
and F). A and D are equivalent to each other (i.e.,
they represent the same sense), and so are B and
E.
Assume that examples are classified after several
iterations in BB as depicted in Figure 5. Here,
circles denote the examples that are correctly
classified and crosses denote the examples that
are incorrectly classified.
Since A and D are equivalent to each other, we
can ‘translate’ the examples with D and use them
to boost the performance of classification to A.
This is because the misclassified examples
(crosses) with D are those mistakenly classified
from C and they will not have much negative
effect on classification to A, even though the
translation from Chinese into English can
introduce some noises. Similar explanations can
be stated to other classification decisions.
In contrast, MB only uses the examples in A and
B to construct a classifier, and when the number
of misclassified examples increases (this is
inevitable in bootstrapping), its performance will
stop improving.
</bodyText>
<sectionHeader confidence="0.985832" genericHeader="method">
5 Word Translation Disambiguation
</sectionHeader>
<subsectionHeader confidence="0.998815">
5.1 Using Bilingual Bootstrapping
</subsectionHeader>
<bodyText confidence="0.9877755">
While it is possible to straightforwardly apply the
algorithm of BB described in Section 3 to word
translation disambiguation, we use here a variant
of it for a better adaptation to the task and for a
fairer comparison with existing technologies.
The variant of BB has four modifications.
</bodyText>
<listItem confidence="0.8494650625">
(1) It actually employs an ensemble of the Naïve
Bayesian Classifiers (NBC), because an
ensemble of NBCs generally performs better
than a single NBC (Pedersen 2000). In an
ensemble, it creates different NBCs using as data
the words within different window sizes
surrounding the word to be disambiguated (e.g.,
‘plant’ or ‘zhiwu’) and further constructs a new
classifier by linearly combining the NBCs.
(2) It employs the heuristics of ‘one sense per
discourse’ (cf., Yarowsky 1995) after using an
ensemble of NBCs.
(3) It uses only classified data in English at the
beginning.
(4) It individually resolves ambiguities on
selected English words such as ‘plant’, ‘interest’.
</listItem>
<bodyText confidence="0.965396285714286">
As a result, in the case of ‘plant’; for example, the
classifiers with respect to ‘gongchang’ and
‘zhiwu’ only make classification decisions to D
and E but not C and F (in Figure 5). It calculates
λ * c as ( ) (  |)
( ) λ * c =P c t and sets θ = 0 at the
right-hand side of step 2.
</bodyText>
<subsectionHeader confidence="0.999813">
5.2 Using Monolingual Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999211833333333">
We consider here two implementations of MB
for word translation disambiguation.
In the first implementation, in addition to the
basic algorithm of MB, we also use (1) an
ensemble of Naïve Bayesian Classifiers, (2) the
heuristics of ‘one sense per discourse’, and (3) a
small number of classified data in English at the
beginning. We will denote this implementation
as MB-B hereafter.
The second implementation is different from the
first one only in (1). That is, it employs as a
classifier a decision list instead of an ensemble of
NBCs. This implementation is exactly the one
proposed in (Yarowsky 1995), and we will
denote it as MB-D hereafter.
MB-B and MB-D can be viewed as the
state-of-the-art methods for word translation
disambiguation using bootstrapping.
</bodyText>
<sectionHeader confidence="0.949595" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<figure confidence="0.99888937037037">
M
M
o
o
× o
×
o
×
o o
×
o o
o o
×
×o
o
o
o
o o
o
o
× ×
M
M
o × ×o
o o
× ×
o o
</figure>
<tableCaption confidence="0.99897">
Table 1: Data descriptions in Experiment 1
</tableCaption>
<bodyText confidence="0.591884111111111">
Words Chinese translations Corresponding English senses Seed words
interest readiness to give attention show
money paid for the use of money rate
, a share in company or business hold
advantage, advancement or favor conflict
line , a thin flexible object cut
, written or spoken text write
telephone connection telephone
, formation of people or things wait
</bodyText>
<table confidence="0.8864075">
, an artificial division between
, product product
</table>
<tableCaption confidence="0.944273">
Table 2: Data sizes in Experiment 1
</tableCaption>
<table confidence="0.998907">
Words Unclassified sentences Test
sentences
English Chinese
interest 1927 8811 2291
line 3666 5398 4148
</table>
<tableCaption confidence="0.998944">
Table 3: Accuracies in Experiment 1
</tableCaption>
<table confidence="0.9955525">
Words Major MB-D MB-B BB
(%) (%) (%) (%)
interest 54.6 54.7 69.3 75.5
line 53.5 55.6 54.1 62.7
</table>
<figureCaption confidence="0.999261">
Figure 6: Learning curves with ‘interest’ Figure 7: Learning curves with ‘line’
Figure 8: Accuracies of BB with different α
</figureCaption>
<bodyText confidence="0.9998205">
We conducted two experiments on
English-Chinese translation disambiguation.
</bodyText>
<subsectionHeader confidence="0.998882">
6.1 Experiment 1: WSD Benchmark Data
</subsectionHeader>
<bodyText confidence="0.9991926">
We first applied BB, MB-B, and MB-D to
translation of the English words ‘line’ and
‘interest’ using a benchmark data2. The data
mainly consists of articles in the Wall Street
Journal and it is designed for conducting Word
</bodyText>
<tableCaption confidence="0.999581">
Table 4: Accuracies of supervised methods
</tableCaption>
<table confidence="0.99959">
interest (%) line (%)
Ensembles of NBC 89 88
Naïve Bayes 74 72
Decision Tree 78 -
Neural Network - 76
Nearest Neighbor 87 -
</table>
<bodyText confidence="0.979846">
Sense Disambiguation (WSD) on the two words
(e.g., Pedersen 2000).
We adopted from the HIT dictionary 3 the
Chinese translations of the two English words, as
listed in Table 1. One sense of the words
corresponds to one group of translations.
We then used the benchmark data as our test data.
(For the word ‘interest’, we only used its four
major senses, because the remaining two minor
senses occur in only 3.3% of the data)
α
</bodyText>
<page confidence="0.617110666666667">
3 The dictionary is created by Harbin Institute of
Technology.
2 http://www.d.umn.edu/~tpederse/data.html.
</page>
<tableCaption confidence="0.997552">
Table 5: Data descriptions and data sizes in Experiment 2
</tableCaption>
<table confidence="0.728262">
Words Chinese translations Unclassified sentences Seed words Test
sentences
English Chinese
</table>
<bodyText confidence="0.987648507936508">
bass , / , 142 8811 fish / music 200
drug , / 3053 5398 treatment / smuggler 197
duty , / , 1428 4338 discharge / export 197
palm , / 366 465 tree / hand 197
plant , / 7542 24977 industry / life 197
space , / , 3897 14178 volume / outer 197
tank / , 417 1400 combat / fuel 199
Total - 16845 59567 - 1384
As classified data in English, we defined a ‘seed
word’ for each group of translations based on our
intuition (cf., Table 1). Each of the seed words
was then used as a classified ‘sentence’. This way
of creating classified data is similar to that in
(Yarowsky, 1995). As unclassified data in
English, we collected sentences in news articles
from a web site (www.news.com), and as
unclassified data in Chinese, we collected
sentences in news articles from another web site
(news.cn.tom.com). We observed that the
distribution of translations in the unclassified
data was balanced.
Table 2 shows the sizes of the data. Note that
there are in general more unclassified sentences
in Chinese than in English because an English
word usually has several Chinese words as
translations (cf., Figure 5).
As a translation dictionary, we used the HIT
dictionary, which contains about 76000 Chinese
words, 60000 English words, and 118000 links.
We then used the data to conduct translation
disambiguation with BB, MB-B, and MB-D, as
described in Section 5.
For both BB and MB-B, we used an ensemble of
five Naïve Bayesian Classifiers with the window
sizes being ±1, ±3, ±5, ±7, ±9 words. For both
BB and MB-B, we set the parameters of β, b, and
θ to 0.2, 15, and 1.5 respectively. The
parameters were tuned based on our preliminary
experimental results on MB-B, they were not
tuned, however, for BB. For the BB specific
parameter α, we set it to 0.4, which meant that we
treated the information from English and that
from Chinese equally.
Table 3 shows the translation disambiguation
accuracies of the three methods as well as that of
a baseline method in which we always choose the
major translation. Figures 6 and 7 show the
learning curves of MB-D, MB-B, and BB. Figure
8 shows the accuracies of BB with different
α values.
From the results, we see that BB consistently and
significantly outperforms both MB-D and MB-B.
The results from the sign test are statistically
significant (p-value &lt; 0.001).
Table 4 shows the results achieved by some
existing supervised learning methods with
respect to the benchmark data (cf., Pedersen
2000). Although BB is a method nearly
equivalent to one based on unsupervised learning,
it still performs favorably well when compared
with the supervised methods (note that since the
experimental settings are different, the results
cannot be directly compared).
</bodyText>
<subsectionHeader confidence="0.999101">
6.2 Experiment 2: Yarowsky’s Words
</subsectionHeader>
<bodyText confidence="0.99929725">
We also conducted translation on seven of the
twelve English words studied in (Yarowsky,
1995). Table 5 shows the list of the words.
For each of the words, we extracted about 200
sentences containing the word from the Encarta4
English corpus and labeled those sentences with
Chinese translations ourselves. We used the
labeled sentences as test data and the remaining
sentences as unclassified data in English. We
also used the sentences in the Great
Encyclopedia 5 Chinese corpus as unclassified
data in Chinese. We defined, for each translation,
</bodyText>
<footnote confidence="0.99285">
4 http://encarta.msn.com/default.asp
5 http://www.whlib.ac.cn/sjk/bkqs.htm
</footnote>
<tableCaption confidence="0.996539">
Table 6: Accuracies in Experiment 2
</tableCaption>
<table confidence="0.993721333333333">
Words Major MB-D MB-B BB
(%) (%) (%) (%)
bass 61.0 57.0 87.0 89.0
drug 77.7 78.7 79.7 86.8
duty 86.3 86.8 72.0 75.1
palm 82.2 80.7 83.3 92.4
plant 71.6 89.3 95.4 95.9
space 64.5 71.6 84.3 87.8
tank 60.3 62.8 76.9 84.4
Total 71.9 75.2 82.6 87.4
MB-B BB
payment saving
cut payment
earn benchmark
short whose
short-term base
yield prefer
u.s. fixed
margin debt
benchmark annual
regard dividend
</table>
<bodyText confidence="0.998912722222222">
a seed word in English as a classified example
(cf., Table 5).
We did not, however, conduct translation
disambiguation on the words ‘crane’, ‘sake’,
‘poach’, ‘axes’, and ‘motion’, because the first
four words do not frequently occur in the Encarta
corpus, and the accuracy of choosing the major
translation for the last word has already exceeded
98%.
We next applied BB, MB-B, and MB-D to word
translation disambiguation. The experiment
settings were the same as those in Experiment 1.
From Table 6, we see again that BB significantly
outperforms MB-D and MB-B. (We will describe
the results in detail in the full version of this
paper.) Note that the results of MB-D here cannot
be directly compared with those in (Yarowsky,
1995), mainly because the data used are different.
</bodyText>
<subsectionHeader confidence="0.980536">
6.3 Discussions
</subsectionHeader>
<bodyText confidence="0.99981325">
We investigated the reason of BB’s
outperforming MB and found that the
explanation on the reason in Section 4 appears to
be true according to the following observations.
</bodyText>
<figureCaption confidence="0.9841515">
Figure 9: Number of relevant words
Figure 10: When more unlabeled data available
</figureCaption>
<listItem confidence="0.505677">
(1) In a Naïve Bayesian Classifier, words having
</listItem>
<bodyText confidence="0.990669272727273">
large values of probability ratio Pie i t) have
strong influence on the classification of t when
they occur, particularly, when they frequently
occur. We collected the words having large
values of probability ratio for each t in both BB
and MB-B and found that BB obviously has more
‘relevant words’ than MB-B. Here ‘relevant
words’ for t refer to the words which are strongly
indicative to t on the basis of human judgments.
Table 7 shows the top ten words in terms of
probability ratio for the ‘ ’ translation
(‘money paid for the use of money’) with respect
to BB and MB-B, in which relevant words are
underlined. Figure 9 shows the numbers of
relevant words for the four translations of
‘interest’ with respect to BB and MB-B.
(2) From Figure 8, we see that the performance of
BB remains high or gets higher when α becomes
larger than 0.4 (recall that β was fixed to 0.2).
This result strongly indicates that the information
from Chinese has positive effects on
disambiguation.
</bodyText>
<listItem confidence="0.686176666666667">
(3) One may argue that the higher performance of
BB might be attributed to the larger unclassified
data size it uses, and thus if we increase the
</listItem>
<tableCaption confidence="0.983084">
Table 7: Top words for ‘ ’ of ‘interest’
</tableCaption>
<bodyText confidence="0.999844642857143">
unclassified data size for MB, it is likely that MB
can perform as well as BB.
We conducted an additional experiment and
found that this is not the case. Figure 10 shows
the accuracies achieved by MB-B when data
sizes increase. Actually, the accuracies of MB-B
cannot further improve when unlabeled data
sizes increase. Figure 10 plots again the results of
BB as well as those of a method referred to as
MB-C. In MB-C, we linearly combine two MB-B
classifiers constructed with two different
unlabeled data sets and we found that although
the accuracies get some improvements in MB-C,
they are still much lower than those of BB.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999956333333333">
This paper has presented a new word translation
disambiguation method using a bootstrapping
technique called Bilingual Bootstrapping.
Experimental results indicate that BB
significantly outperforms the existing
Monolingual Bootstrapping technique in word
translation disambiguation. This is because BB
can effectively make use of information from two
sources rather than from one source as in MB.
</bodyText>
<sectionHeader confidence="0.996006" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999578666666667">
We thank Ming Zhou, Ashley Chang and Yao
Meng for their valuable comments on an early
draft of this paper.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892422535211">
P. Brown, S. D. Pietra, V. D. Pietra, and R. Mercer,
1991. Word Sense Disambiguation Using
Statistical Methods. In Proceedings of the 29th
Annual Meeting of the Association for
Computational Linguistics, pp. 264-270.
I. Dagan and A. Itai, 1994. Word Sense
Disambiguation Using a Second Language
Monolingual Corpus. Computational Linguistics,
vol. 20, pp. 563-596.
A. P. Dempster, N. M. Laird, and D. B. Rubin, 1977.
Maximum Likelihood from Incomplete Data via
the EM Algorithm. Journal of the Royal Statistical
Society B, vol. 39, pp. 1-38.
G. Escudero, L. Marquez, and G. Rigau, 2000.
Boosting Applied to Word Sense Disambiguation.
In Proceedings of the 12th European Conference
on Machine Learning.
W. Gale, K. Church, and D. Yarowsky, 1992a. A
Method for Disambiguating Word Senses in a
Large Corpus. Computers and Humanities, vol. 26,
pp. 415-439.
W. Gale, K. Church, and D. Yarowsky, 1992b. One
sense per discourse. In Proceedings of DARPA
speech and Natural Language Workshop.
A. R. Golding and D. Roth, 1999. A Winnow-Based
Approach to Context-Sensitive Spelling
Correction. Machine Learning, vol. 34, pp.
107-130.
G. Kikui, 1999. Resolving Translation Ambiguity
Using Non-parallel Bilingual Corpora. In
Proceedings of ACL ’99 Workshop on
Unsupervised Learning in Natural Language
Processing.
L. Mangu and E. Brill, 1997. Automatic rule
acquisition for spelling correction. In Proceedings
of the 14th International Conference on Machine
Learning.
R. Mihalcea and D. Moldovan, 1999. A method for
Word Sense Disambiguation of unrestricted text.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics.
H. T. Ng and H. B. Lee, 1996. Integrating Multiple
Knowledge Sources to Disambiguate Word Sense:
An Exemplar-based Approach. In Proceedings of
the 34th Annual Meeting of the Association for
Computational Linguistics, pp. 40-47.
T. Pedersen and R. Bruce, 1997. Distinguishing Word
Senses in Untagged Text. In Proceedings of the
2nd Conference on Empirical Methods in Natural
Language Processing, pp. 197-207.
T. Pedersen, 2000. A Simple Approach to Building
Ensembles of Naïve Bayesian Classifiers for Word
Sense Disambiguation. In Proceedings of the 1st
Meeting of the North American Chapter of the
Association for Computational Linguistics.
H. Schutze, 1998. Automatic Word Sense
Discrimination. In Computational Linguistics, vol.
24, no. 1, pp. 97-124.
G. Towell and E. Voothees, 1998. Disambiguating
Highly Ambiguous Words. Computational
Linguistics, vol. 24, no. 1, pp. 125-146.
D. Yarowsky, 1994. Decision Lists for Lexical
Ambiguity Resolution: Application to Accent
Restoration in Spanish and French. In Proceedings
of the 32nd Annual Meeting of the Association for
Computational Linguistics, pp. 88-95.
D. Yarowsky, 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pp.
189-196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000111">
<note confidence="0.996767">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 343-351.</note>
<title confidence="0.987449">Word Translation Disambiguation Using Bilingual Bootstrapping</title>
<author confidence="0.987823">Cong Li</author>
<affiliation confidence="0.999915">Microsoft Research Asia</affiliation>
<address confidence="0.997456">5F Sigma Center, No.49 Zhichun Road, Haidian Beijing, China, 100080</address>
<email confidence="0.999279">i-congl@microsoft.com</email>
<abstract confidence="0.999490914285714">This paper proposes a new method for word translation disambiguation using a machine learning technique called ‘Bilingual Bootstrapping’. Bilingual Bootstrapping makes use of in learning a small number of classified data and a large number of unclassified the source and the target translation. It constructs classifiers in the two languages in parallel and repeatedly boosts the performances of the classifiers by further classifying data in each of the two languages and by exchanging between the two languages information regarding the classified data. Experimental results indicate that word translation disambiguation based on Bilingual Bootstrapping significantly outperforms the existing methods based on ‘Monolingual Bootstrapping’. We address here the problem of word translation disambiguation. For instance, we are concerned with an ambiguous word in English (e.g., ‘plant’), which has multiple translations in Chinese (e.g., ‘ (gongchang)’ and ‘ (zhiwu)’). Our goal is to determine the correct Chinese translation of the ambiguous English word, given an English sentence which contains the word. Word translation disambiguation is actually a special case of word sense disambiguation (in the example above, ‘gongchang’ corresponds to the</abstract>
<author confidence="0.961393">Hang Li</author>
<affiliation confidence="0.999919">Microsoft Research Asia</affiliation>
<address confidence="0.9974245">5F Sigma Center, No.49 Zhichun Road, Haidian Beijing, China, 100080</address>
<email confidence="0.96778">hangli@microsoft.com</email>
<abstract confidence="0.996329138888889">sense of ‘factory’ and ‘zhiwu’ corresponds to the of Yarowsky (1995) proposes a method for word sense (translation) disambiguation that is based on a bootstrapping technique, which we refer to here as ‘Monolingual Bootstrapping (MB)’. In this paper, we propose a new method for word translation disambiguation using a bootstrapping technique we have developed. We refer to the technique as ‘Bilingual Bootstrapping (BB)’. In order to evaluate the performance of BB, we conducted some experiments on word translation disambiguation using the BB technique and the MB technique. All of the results indicate that BB consistently and significantly outperforms MB. Work The problem of word translation disambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be by employing a method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs.</abstract>
<note confidence="0.860456666666667">Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000).</note>
<title confidence="0.8515345">Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve</title>
<abstract confidence="0.994468222222222">Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). The assumption behind the proposed methods is that it is nearly always possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information (e.g., co-occurring words). Since preparing supervised learning data is expensive (in many cases, manually labeling data is required), it is desirable to develop a that starts learning with a small number of classified data but is still able to achieve high performance under the help of a large number of unclassified data which is not expensive anyway. Yarowsky (1995) proposes a method for word sense disambiguation, which is based on Monolingual Bootstrapping. When applied to our current task, his method starts learning with a small number of English sentences which contain an ambiguous English word and which are respectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation.</abstract>
<note confidence="0.82228625">For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999).</note>
<title confidence="0.424477">Bootstrapping</title>
<abstract confidence="0.984173308383235">3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified in English and, small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs classifiers for English to Chinese translation disambiguation by repeating the following two steps: (1) constructing classifiers for each of the languages on the basis of the data both (2) using the constructed classifiers in each of the languages to classify some unclassified data and adding them to the classified training data set of the language. The reason that we can use classified data in both languages at step (1) is that words in one language generally have translations in the other and we can find their translation relationship by using a dictionary. 3.2 Algorithm a set of words in English, set of in Chinese, and set of links in a translation dictionary as shown in Figure 1. (Any two linked words can be translation of each other.) defined as a i.e., for a random variable on variable on let for a variable on random variable on random variable on While words to be translated, represent context words. an English word ❑ the links Figure 1: Example of translation dictionary M M M M M M : , , , , Parameter : parallel following processes for English (left) and Chinese (right), until unable to continue : for each { each { each { and to create classifier: &amp; − ; }} each { and create classifier: &amp; − ; each { for each { each ← ← } ;} each each ← ← } ;} each * t let t  |) ) max T t (  |) e ε max t T t (  |) c γ P t (  |) e ε ) arg max t T t (  |) e calculate P t  |) c ) c ;} each in descending order of ( ) the top into ;} each t put ;} each in descending order of ( ) the top into ;} each ){ t put ){ ← ← Output: classifiers in English and Chinese Figure 2: Bilingual Bootstrapping it, and = the Chinese words which are linked to it. For a word let = We can define and similarly. a sequence of words (e.g., a sentence or a text) in English e i 2, , }, ( 1,2, , ) . a sequence of words in Chinese c i 2, , } , ( 1,2, , ) view examples representing context information for translation disambiguation. an English word we define a classifier for resolving each of its translation in in a general form as: P t t T (  |), &amp; (  |), { P t t T t an example in English. Similarly, a Chinese word we define a classifier as: P t t T (  |), &amp; (  |), { P t t T t an example in Chinese. denote a set of classified examples in each representing one context of 1, , ( , , ( ε ε ε a set of unclassified examples in English, representing one context of e , ( , , ( ) } . ε ε ε ε Similarly, we denote the sets of classified and examples with respect to as and respectively. Furthermore, we have = = = = We perform Bilingual Bootstrapping as described in Figure 2. Hereafter, we will only explain the process for English (left-hand side); the process for Chinese (right-hand side) can be conducted similarly. 3.3 Naïve Bayesian Classifier } , } , , ) k , ), . t P e c t ( , ) (  |, ) ∑ c C ∈ ← ( , ) c t f c C if (1) ),  |) t MLE using as data; ε  |) t EM Algorithm using ε each as data; a linear combination of t  |)  |) t MLE using Figure 3: Creating Naïve Bayesian Classifier While we can in principle employ any kind of classifier in BB, we use here a Naïve Bayesian Classifier. At step 1 in BB, we construct the classifier as described in Figure 3. At step 2, for example we calculate with the Naïve Bayesian Classifier: t T T P t  |) t P t ( ) (  |) e ε ε ε The second equation is based on Bayes’ rule. In the calculation, we assume that the context in 1, are independently from thus we have m Figure 4: EM Algorithm form c e t P e t  |, ) (  |) for a e E ∈ assume that the data in t T i 1, , ), = ∀ ∈ are independently generated on the basis of the model. We can, therefore, employ the Expectation and Maximization Algorithm (EM Algorithm) (Dempster et al. 1977) to estimate the of the model including We use the relation the estimation. Initially, we set we calculate it at step 1 by linearly combining (  |) t from English ε  |) t from Chinese: (  |) (1 − − (  |) ( ) P e t P e t E α β ε ε P e t P e ( ) C  |) ( ( ) U and a uniform distribution over which ( ) is used for avoiding zero probability. In this way, estimate using information from not only English but also Chinese.  |) t we estimate it with MLE ε Likelihood Estimation) using as For  |) t we estimate it as is ε described in Section 3.4. 3.4 EM Algorithm the sake of readability, we rewrite  |) t ε We define a finite mixture model of 1 e t (  |) , . e E ∈  || E We next estimate the parameters by iteratively updating them ass described in Figure 4 until converge. Here for the of to The context information in Chinese is then ‘translated’ into in English through the links in between BB and MB We note that Monolingual Bootstrapping is a special case of Bilingual Bootstrapping (consider situation in which 0 in formula (1)). Moreover, it seems safe to say that BB can always perform better than MB. The many-to-many relationship between the words in the two languages stands out as key to the higher performance of BB. Suppose that the classifier with respect to ‘plant’ has two decisions (denoted as A and B in Figure 5). Further suppose that the classifiers with | , ) } , γ ) , γ h e  |) εe it ( | can calculate  P c e t  |, )  , c C 0 , if ). i | , c P P c e t  |, ) f c C ∈ ∑ c C ∈ ∑ e E ∈ P ∑ ( , ) (  |, ) c t P e c t ) P e t ( | ( , ) (  |, ) c t P e c t Figure 5: Example of BB respect to ‘gongchang’ and ‘zhiwu’ in Chinese have two decisions respectively, (C and D) (E and F). A and D are equivalent to each other (i.e., they represent the same sense), and so are B and E. Assume that examples are classified after several iterations in BB as depicted in Figure 5. Here, circles denote the examples that are correctly classified and crosses denote the examples that are incorrectly classified. Since A and D are equivalent to each other, we can ‘translate’ the examples with D and use them to boost the performance of classification to A. This is because the misclassified examples (crosses) with D are those mistakenly classified from C and they will not have much negative effect on classification to A, even though the translation from Chinese into English can introduce some noises. Similar explanations can be stated to other classification decisions. In contrast, MB only uses the examples in A and B to construct a classifier, and when the number of misclassified examples increases (this is inevitable in bootstrapping), its performance will stop improving. Translation Disambiguation 5.1 Using Bilingual Bootstrapping While it is possible to straightforwardly apply the algorithm of BB described in Section 3 to word translation disambiguation, we use here a variant of it for a better adaptation to the task and for a fairer comparison with existing technologies. The variant of BB has four modifications. (1) It actually employs an ensemble of the Naïve Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). In an ensemble, it creates different NBCs using as data the words within different window sizes surrounding the word to be disambiguated (e.g., ‘plant’ or ‘zhiwu’) and further constructs a new classifier by linearly combining the NBCs. (2) It employs the heuristics of ‘one sense per discourse’ (cf., Yarowsky 1995) after using an ensemble of NBCs. (3) It uses only classified data in English at the beginning. (4) It individually resolves ambiguities on selected English words such as ‘plant’, ‘interest’. As a result, in the case of ‘plant’; for example, the classifiers with respect to ‘gongchang’ and ‘zhiwu’ only make classification decisions to D and E but not C and F (in Figure 5). It calculates ) (  |) ) sets the right-hand side of step 2. 5.2 Using Monolingual Bootstrapping We consider here two implementations of MB for word translation disambiguation. In the first implementation, in addition to the basic algorithm of MB, we also use (1) an ensemble of Naïve Bayesian Classifiers, (2) the heuristics of ‘one sense per discourse’, and (3) a small number of classified data in English at the beginning. We will denote this implementation as MB-B hereafter. The second implementation is different from the first one only in (1). That is, it employs as a classifier a decision list instead of an ensemble of NBCs. This implementation is exactly the one proposed in (Yarowsky 1995), and we will denote it as MB-D hereafter. MB-B and MB-D can be viewed as the for word translation disambiguation using bootstrapping. Results M M o o × o × o o × o o o o × o o o o o o o × × M M × o o Table 1: Data descriptions in Experiment 1 Words Chinese translations Corresponding English senses Seed words interest readiness to give attention show money paid for the use of money rate , a share in company or business hold advantage, advancement or favor conflict line , a thin flexible object cut , written or spoken text write telephone connection telephone , formation of people or things wait , an artificial division between , product product Table 2: Data sizes in Experiment 1 Words Unclassified sentences Test sentences English Chinese interest 1927 8811 2291 line 3666 5398 4148 Table 3: Accuracies in Experiment 1 Words Major MB-D MB-B BB (%) (%) (%) (%) interest 54.6 54.7 69.3 75.5 line 53.5 55.6 54.1 62.7 Figure 6: Learning curves with ‘interest’ Figure 7: Learning curves with ‘line’ 8: BB with different We conducted two experiments on English-Chinese translation disambiguation. 6.1 Experiment 1: WSD Benchmark Data We first applied BB, MB-B, and MB-D to translation of the English words ‘line’ and using a benchmark The data mainly consists of articles in the Wall Street Journal and it is designed for conducting Word Table 4: Accuracies of supervised methods interest (%) line (%) Ensembles of NBC 89 88 Naïve Bayes 74 72 Decision Tree 78 - Neural Network - 76 Nearest Neighbor 87 - Sense Disambiguation (WSD) on the two words (e.g., Pedersen 2000). adopted from the HIT dictionary 3the Chinese translations of the two English words, as listed in Table 1. One sense of the words corresponds to one group of translations. then used the benchmark data as our (For the word ‘interest’, we only used its four major senses, because the remaining two minor senses occur in only 3.3% of the data) α dictionary is created by Harbin Institute of Technology. Table 5: Data descriptions and data sizes in Experiment 2 Words Chinese translations Unclassified sentences Seed words Test sentences English Chinese bass , / , 142 8811 fish / music 200 drug , / 3053 5398 treatment / smuggler 197 duty , / , 1428 4338 discharge / export 197 palm , / 366 465 tree / hand 197 plant , / 7542 24977 industry / life 197 space , / , 3897 14178 volume / outer 197 tank / , 417 1400 combat / fuel 199 Total - 16845 59567 - 1384 As classified data in English, we defined a ‘seed word’ for each group of translations based on our intuition (cf., Table 1). Each of the seed words was then used as a classified ‘sentence’. This way of creating classified data is similar to that in (Yarowsky, 1995). As unclassified data in English, we collected sentences in news articles from a web site (www.news.com), and as unclassified data in Chinese, we collected sentences in news articles from another web site (news.cn.tom.com). We observed that the distribution of translations in the unclassified data was balanced. Table 2 shows the sizes of the data. Note that there are in general more unclassified sentences in Chinese than in English because an English word usually has several Chinese words as translations (cf., Figure 5). As a translation dictionary, we used the HIT dictionary, which contains about 76000 Chinese words, 60000 English words, and 118000 links. We then used the data to conduct translation disambiguation with BB, MB-B, and MB-D, as described in Section 5. For both BB and MB-B, we used an ensemble of five Naïve Bayesian Classifiers with the window being words. For both and MB-B, we set the parameters of and 0.2, 15, and 1.5 respectively. The parameters were tuned based on our preliminary experimental results on MB-B, they were not tuned, however, for BB. For the BB specific we set it to 0.4, which meant that we treated the information from English and that from Chinese equally. Table 3 shows the translation disambiguation accuracies of the three methods as well as that of a baseline method in which we always choose the Figures 6 and 7 show the learning curves of MB-D, MB-B, and BB. Figure 8 shows the accuracies of BB with different the results, we see that BB both MB-D and MB-B. results from the test statistically significant (p-value &lt; 0.001). Table 4 shows the results achieved by some methods with respect to the benchmark data (cf., Pedersen 2000). Although BB is a method nearly equivalent to one based on unsupervised learning, it still performs favorably well when compared with the supervised methods (note that since the experimental settings are different, the results be 6.2 Experiment 2: Yarowsky’s Words We also conducted translation on seven of the twelve English words studied in (Yarowsky, 1995). Table 5 shows the list of the words. For each of the words, we extracted about 200 containing the word from the English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great 5Chinese corpus as unclassified data in Chinese. We defined, for each translation, Table 6: Accuracies in Experiment 2 Words Major MB-D MB-B BB (%) (%) (%) (%) bass 61.0 57.0 87.0 89.0 drug 77.7 78.7 79.7 86.8 duty 86.3 86.8 72.0 75.1 palm 82.2 80.7 83.3 92.4 plant 71.6 89.3 95.4 95.9 space 64.5 71.6 84.3 87.8 tank 60.3 62.8 76.9 84.4 Total 71.9 75.2 82.6 87.4 MB-B BB payment saving cut payment earn benchmark short whose short-term base yield prefer u.s. fixed margin debt benchmark annual regard dividend a seed word in English as a classified example (cf., Table 5). We did not, however, conduct translation disambiguation on the words ‘crane’, ‘sake’, ‘poach’, ‘axes’, and ‘motion’, because the first four words do not frequently occur in the Encarta corpus, and the accuracy of choosing the major translation for the last word has already exceeded 98%. We next applied BB, MB-B, and MB-D to word translation disambiguation. The experiment settings were the same as those in Experiment 1. Table 6, we see again that BB outperforms MB-D and MB-B. (We will describe the results in detail in the full version of this paper.) Note that the results of MB-D here cannot with those in (Yarowsky, 1995), mainly because the data used are different. 6.3 Discussions We investigated the reason of BB’s outperforming MB and found that the explanation on the reason in Section 4 appears to be true according to the following observations. Figure 9: Number of relevant words Figure 10: When more unlabeled data available (1) In a Naïve Bayesian Classifier, words having values of probability ratio influence on the classification of they occur, particularly, when they frequently occur. We collected the words having large of probability ratio for each both BB and MB-B and found that BB obviously has more ‘relevant words’ than MB-B. Here ‘relevant for to the words which are strongly to the basis of human judgments. Table 7 shows the top ten words in terms of probability ratio for the ‘ ’ translation (‘money paid for the use of money’) with respect to BB and MB-B, in which relevant words are underlined. Figure 9 shows the numbers of relevant words for the four translations of ‘interest’ with respect to BB and MB-B. (2) From Figure 8, we see that the performance of remains high or gets higher when than 0.4 (recall that fixed to 0.2). This result strongly indicates that the information from Chinese has positive effects on disambiguation. (3) One may argue that the higher performance of BB might be attributed to the larger unclassified data size it uses, and thus if we increase the Table 7: Top words for ‘ ’ of ‘interest’ unclassified data size for MB, it is likely that MB can perform as well as BB. We conducted an additional experiment and found that this is not the case. Figure 10 shows the accuracies achieved by MB-B when data sizes increase. Actually, the accuracies of MB-B cannot further improve when unlabeled data sizes increase. Figure 10 plots again the results of BB as well as those of a method referred to as MB-C. In MB-C, we linearly combine two MB-B classifiers constructed with two different unlabeled data sets and we found that although the accuracies get some improvements in MB-C, they are still much lower than those of BB. This paper has presented a new word translation disambiguation method using a bootstrapping technique called Bilingual Bootstrapping. Experimental results indicate that BB significantly outperforms the existing Monolingual Bootstrapping technique in word translation disambiguation. This is because BB can effectively make use of information from two sources rather than from one source as in MB. Acknowledgements We thank Ming Zhou, Ashley Chang and Yao Meng for their valuable comments on an early draft of this paper.</abstract>
<title confidence="0.864796">References</title>
<author confidence="0.901664">P Brown</author>
<author confidence="0.901664">S D Pietra</author>
<author confidence="0.901664">V D Pietra</author>
<author confidence="0.901664">R Mercer</author>
<address confidence="0.67747">1991. Word Sense Disambiguation Using</address>
<note confidence="0.92394325">Methods. In of the 29th Annual Meeting of the Association for pp. 264-270. I. Dagan and A. Itai, 1994. Word Sense Disambiguation Using a Second Language Corpus. vol. 20, pp. 563-596. A. P. Dempster, N. M. Laird, and D. B. Rubin, 1977.</note>
<title confidence="0.818779">Maximum Likelihood from Incomplete Data via</title>
<author confidence="0.872788">EM Journal of the Royal Statistical</author>
<note confidence="0.909281105263158">vol. 39, pp. 1-38. G. Escudero, L. Marquez, and G. Rigau, 2000. Boosting Applied to Word Sense Disambiguation. of the 12th European Conference Machine W. Gale, K. Church, and D. Yarowsky, 1992a. A Method for Disambiguating Word Senses in a Corpus. and vol. 26, pp. 415-439. W. Gale, K. Church, and D. Yarowsky, 1992b. One per discourse. In of DARPA and Natural Language A. R. Golding and D. Roth, 1999. A Winnow-Based Approach to Context-Sensitive Spelling vol. 34, pp. 107-130. G. Kikui, 1999. Resolving Translation Ambiguity Using Non-parallel Bilingual Corpora. In Proceedings of ACL ’99 Workshop on</note>
<title confidence="0.85681">Unsupervised Learning in Natural Language</title>
<author confidence="0.751594">L Mangu</author>
<author confidence="0.751594">E Brill</author>
<abstract confidence="0.7771964">for spelling correction. In of the 14th International Conference on Machine R. Mihalcea and D. Moldovan, 1999. A method for Word Sense Disambiguation of unrestricted text. of the 37th Annual Meeting of the</abstract>
<title confidence="0.505564333333333">for Computational H. T. Ng and H. B. Lee, 1996. Integrating Multiple Knowledge Sources to Disambiguate Word Sense:</title>
<note confidence="0.936209642857143">Exemplar-based Approach. In of the 34th Annual Meeting of the Association for pp. 40-47. T. Pedersen and R. Bruce, 1997. Distinguishing Word in Untagged Text. In of the 2nd Conference on Empirical Methods in Natural pp. 197-207. T. Pedersen, 2000. A Simple Approach to Building Ensembles of Naïve Bayesian Classifiers for Word Disambiguation. In of the 1st Meeting of the North American Chapter of the for Computational H. Schutze, 1998. Automatic Word Sense In vol. 24, no. 1, pp. 97-124. G. Towell and E. Voothees, 1998. Disambiguating Ambiguous Words. vol. 24, no. 1, pp. 125-146. D. Yarowsky, 1994. Decision Lists for Lexical Ambiguity Resolution: Application to Accent in Spanish and French. In of the 32nd Annual Meeting of the Association for pp. 88-95. D. Yarowsky, 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd Annual Meeting of the for Computational pp. 189-196.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="5246" citStr="Brown et al. 1991" startWordPosition="768" endWordPosition="771">rd and which are respectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>P. Brown, S. D. Pietra, V. D. Pietra, and R. Mercer, 1991. Word Sense Disambiguation Using Statistical Methods. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pp. 264-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Word Sense Disambiguation Using a Second Language Monolingual Corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<pages>563--596</pages>
<contexts>
<context position="5267" citStr="Dagan and Itai 1994" startWordPosition="772" endWordPosition="775">spectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but f</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>I. Dagan and A. Itai, 1994. Word Sense Disambiguation Using a Second Language Monolingual Corpus. Computational Linguistics, vol. 20, pp. 563-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="11496" citStr="Dempster et al. 1977" startWordPosition="2214" endWordPosition="2217">ample e, we calculate with the Naïve Bayesian Classifier: t T ∈ t T ε P t ( |) ∈ ε e P t P t ( ) ( |) e ε ε ε The second equation is based on Bayes’ rule. In the calculation, we assume that the context words in e (i.e., e 1 , e2,L , em) are independently generated from Pε(e |t) and thus we have m Figure 4: EM Algorithm the form P(c |t) = ∑ P c e t P e t ( |, ) ( |) and for a e E ∈ specific ε we assume that the data in t T i γ ( 1, , ), L h γ C i ∈ = ∀ ∈ ε are independently generated on the basis of the model. We can, therefore, employ the Expectation and Maximization Algorithm (EM Algorithm) (Dempster et al. 1977) to estimate the parameters of the model including P(e |t) . We also use the relation T in the estimation. Initially, we set λ *(e) For Pε(e |t), we calculate it at step 1 by linearly combining ( |) P ( E ) e t estimated from English ε and ( |) P ( C ) e t estimated from Chinese: ε ( |) (1 = − − ) ( |) ( ) P e t P e t E α β ε ε P e t P e ( ) C + ( |) ( ( ) U α + β ε where 0≤ α≤1 , 0≤ β≤1 , α+ β≤1 , and P(U) e is a uniform distribution over E , which ( ) is used for avoiding zero probability. In this way, we estimate Pε(e |t) using information from not only English but also Chinese. For ( |) P(</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin, 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society B, vol. 39, pp. 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L Marquez</author>
<author>G Rigau</author>
</authors>
<title>Boosting Applied to Word Sense Disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="3437" citStr="Escudero et al. 2000" startWordPosition="491" endWordPosition="494">ext corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). The assumption behind the proposed methods is that it is nearly always possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information (e.g., co-occurring words). Since</context>
</contexts>
<marker>Escudero, Marquez, Rigau, 2000</marker>
<rawString>G. Escudero, L. Marquez, and G. Rigau, 2000. Boosting Applied to Word Sense Disambiguation. In Proceedings of the 12th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A Method for Disambiguating Word Senses in a Large Corpus.</title>
<date>1992</date>
<journal>Computers and Humanities,</journal>
<volume>26</volume>
<pages>415--439</pages>
<contexts>
<context position="3027" citStr="Gale et al. 1992" startWordPosition="431" endWordPosition="434">tly outperforms MB. 2 Related Work The problem of word translation disambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best fo</context>
<context position="5031" citStr="Gale et al. 1992" startWordPosition="737" endWordPosition="740">or word sense disambiguation, which is based on Monolingual Bootstrapping. When applied to our current task, his method starts learning with a small number of English sentences which contain an ambiguous English word and which are respectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English,</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky, 1992a. A Method for Disambiguating Word Senses in a Large Corpus. Computers and Humanities, vol. 26, pp. 415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of DARPA speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="3027" citStr="Gale et al. 1992" startWordPosition="431" endWordPosition="434">tly outperforms MB. 2 Related Work The problem of word translation disambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best fo</context>
<context position="5031" citStr="Gale et al. 1992" startWordPosition="737" endWordPosition="740">or word sense disambiguation, which is based on Monolingual Bootstrapping. When applied to our current task, his method starts learning with a small number of English sentences which contain an ambiguous English word and which are respectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English,</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky, 1992b. One sense per discourse. In Proceedings of DARPA speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow-Based Approach to Context-Sensitive Spelling Correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<pages>107--130</pages>
<contexts>
<context position="3404" citStr="Golding and Roth 1999" startWordPosition="486" endWordPosition="489">slation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). The assumption behind the proposed methods is that it is nearly always possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information </context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth, 1999. A Winnow-Based Approach to Context-Sensitive Spelling Correction. Machine Learning, vol. 34, pp. 107-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kikui</author>
</authors>
<title>Resolving Translation Ambiguity Using Non-parallel Bilingual Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL ’99 Workshop on Unsupervised Learning in Natural Language Processing.</booktitle>
<contexts>
<context position="5318" citStr="Kikui 1999" startWordPosition="782" endWordPosition="783"> the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs classifiers for </context>
</contexts>
<marker>Kikui, 1999</marker>
<rawString>G. Kikui, 1999. Resolving Translation Ambiguity Using Non-parallel Bilingual Corpora. In Proceedings of ACL ’99 Workshop on Unsupervised Learning in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3150" citStr="Mangu and Brill 1997" startWordPosition="448" endWordPosition="451">on) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). The assumption behind the proposed method</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>L. Mangu and E. Brill, 1997. Automatic rule acquisition for spelling correction. In Proceedings of the 14th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>A method for Word Sense Disambiguation of unrestricted text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5347" citStr="Mihalcea and Moldovan 1999" startWordPosition="784" endWordPosition="787">t then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs classifiers for English to Chinese translatio</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>R. Mihalcea and D. Moldovan, 1999. A method for Word Sense Disambiguation of unrestricted text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-based Approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="3096" citStr="Ng and Lee 1996" startWordPosition="441" endWordPosition="444">sambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Ped</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H. T. Ng and H. B. Lee, 1996. Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-based Approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pp. 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>Distinguishing Word Senses in Untagged Text.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>197--207</pages>
<contexts>
<context position="5292" citStr="Pedersen and Bruce 1997" startWordPosition="776" endWordPosition="779">ith the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB c</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>T. Pedersen and R. Bruce, 1997. Distinguishing Word Senses in Untagged Text. In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing, pp. 197-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>A Simple Approach to Building Ensembles of Naïve Bayesian Classifiers for Word Sense Disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3482" citStr="Pedersen 2000" startWordPosition="499" endWordPosition="500">l). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). The assumption behind the proposed methods is that it is nearly always possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information (e.g., co-occurring words). Since preparing supervised learning data is expens</context>
<context position="15162" citStr="Pedersen 2000" startWordPosition="2976" endWordPosition="2977">amples increases (this is inevitable in bootstrapping), its performance will stop improving. 5 Word Translation Disambiguation 5.1 Using Bilingual Bootstrapping While it is possible to straightforwardly apply the algorithm of BB described in Section 3 to word translation disambiguation, we use here a variant of it for a better adaptation to the task and for a fairer comparison with existing technologies. The variant of BB has four modifications. (1) It actually employs an ensemble of the Naïve Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). In an ensemble, it creates different NBCs using as data the words within different window sizes surrounding the word to be disambiguated (e.g., ‘plant’ or ‘zhiwu’) and further constructs a new classifier by linearly combining the NBCs. (2) It employs the heuristics of ‘one sense per discourse’ (cf., Yarowsky 1995) after using an ensemble of NBCs. (3) It uses only classified data in English at the beginning. (4) It individually resolves ambiguities on selected English words such as ‘plant’, ‘interest’. As a result, in the case of ‘plant’; for example, the classifiers with respect to ‘gongchan</context>
<context position="18267" citStr="Pedersen 2000" startWordPosition="3522" endWordPosition="3523">ne’ Figure 8: Accuracies of BB with different α We conducted two experiments on English-Chinese translation disambiguation. 6.1 Experiment 1: WSD Benchmark Data We first applied BB, MB-B, and MB-D to translation of the English words ‘line’ and ‘interest’ using a benchmark data2. The data mainly consists of articles in the Wall Street Journal and it is designed for conducting Word Table 4: Accuracies of supervised methods interest (%) line (%) Ensembles of NBC 89 88 Naïve Bayes 74 72 Decision Tree 78 - Neural Network - 76 Nearest Neighbor 87 - Sense Disambiguation (WSD) on the two words (e.g., Pedersen 2000). We adopted from the HIT dictionary 3 the Chinese translations of the two English words, as listed in Table 1. One sense of the words corresponds to one group of translations. We then used the benchmark data as our test data. (For the word ‘interest’, we only used its four major senses, because the remaining two minor senses occur in only 3.3% of the data) α 3 The dictionary is created by Harbin Institute of Technology. 2 http://www.d.umn.edu/~tpederse/data.html. Table 5: Data descriptions and data sizes in Experiment 2 Words Chinese translations Unclassified sentences Seed words Test sentenc</context>
<context position="21315" citStr="Pedersen 2000" startWordPosition="4050" endWordPosition="4051">inese equally. Table 3 shows the translation disambiguation accuracies of the three methods as well as that of a baseline method in which we always choose the major translation. Figures 6 and 7 show the learning curves of MB-D, MB-B, and BB. Figure 8 shows the accuracies of BB with different α values. From the results, we see that BB consistently and significantly outperforms both MB-D and MB-B. The results from the sign test are statistically significant (p-value &lt; 0.001). Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). Although BB is a method nearly equivalent to one based on unsupervised learning, it still performs favorably well when compared with the supervised methods (note that since the experimental settings are different, the results cannot be directly compared). 6.2 Experiment 2: Yarowsky’s Words We also conducted translation on seven of the twelve English words studied in (Yarowsky, 1995). Table 5 shows the list of the words. For each of the words, we extracted about 200 sentences containing the word from the Encarta4 English corpus and labeled those sentences with Chinese translations ourselves. </context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>T. Pedersen, 2000. A Simple Approach to Building Ensembles of Naïve Bayesian Classifiers for Word Sense Disambiguation. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>In Computational Linguistics,</journal>
<volume>24</volume>
<pages>97--124</pages>
<contexts>
<context position="5306" citStr="Schutze 1998" startWordPosition="780" endWordPosition="781">ranslations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs clas</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>H. Schutze, 1998. Automatic Word Sense Discrimination. In Computational Linguistics, vol. 24, no. 1, pp. 97-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Towell</author>
<author>E Voothees</author>
</authors>
<title>Disambiguating Highly Ambiguous Words.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>125--146</pages>
<marker>Towell, Voothees, 1998</marker>
<rawString>G. Towell and E. Voothees, 1998. Disambiguating Highly Ambiguous Words. Computational Linguistics, vol. 24, no. 1, pp. 125-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="3060" citStr="Yarowsky 1994" startWordPosition="437" endWordPosition="438">The problem of word translation disambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with </context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky, 1994. Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pp. 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1840" citStr="Yarowsky (1995)" startWordPosition="254" endWordPosition="255">biguous word in English (e.g., ‘plant’), which has multiple translations in Chinese (e.g., ‘ (gongchang)’ and ‘ (zhiwu)’). Our goal is to determine the correct Chinese translation of the ambiguous English word, given an English sentence which contains the word. Word translation disambiguation is actually a special case of word sense disambiguation (in the example above, ‘gongchang’ corresponds to the Hang Li Microsoft Research Asia 5F Sigma Center, No.49 Zhichun Road, Haidian Beijing, China, 100080 hangli@microsoft.com sense of ‘factory’ and ‘zhiwu’ corresponds to the sense of ‘vegetation’).1 Yarowsky (1995) proposes a method for word sense (translation) disambiguation that is based on a bootstrapping technique, which we refer to here as ‘Monolingual Bootstrapping (MB)’. In this paper, we propose a new method for word translation disambiguation using a bootstrapping technique we have developed. We refer to the technique as ‘Bilingual Bootstrapping (BB)’. In order to evaluate the performance of BB, we conducted some experiments on word translation disambiguation using the BB technique and the MB technique. All of the results indicate that BB consistently and significantly outperforms MB. 2 Related</context>
<context position="4395" citStr="Yarowsky (1995)" startWordPosition="641" endWordPosition="642">possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information (e.g., co-occurring words). Since preparing supervised learning data is expensive (in many cases, manually labeling data is required), it is desirable to develop a bootstrapping method that starts learning with a small number of classified data but is still able to achieve high performance under the help of a large number of unclassified data which is not expensive anyway. Yarowsky (1995) proposes a method for word sense disambiguation, which is based on Monolingual Bootstrapping. When applied to our current task, his method starts learning with a small number of English sentences which contain an ambiguous English word and which are respectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one se</context>
<context position="15479" citStr="Yarowsky 1995" startWordPosition="3026" endWordPosition="3027">or a better adaptation to the task and for a fairer comparison with existing technologies. The variant of BB has four modifications. (1) It actually employs an ensemble of the Naïve Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). In an ensemble, it creates different NBCs using as data the words within different window sizes surrounding the word to be disambiguated (e.g., ‘plant’ or ‘zhiwu’) and further constructs a new classifier by linearly combining the NBCs. (2) It employs the heuristics of ‘one sense per discourse’ (cf., Yarowsky 1995) after using an ensemble of NBCs. (3) It uses only classified data in English at the beginning. (4) It individually resolves ambiguities on selected English words such as ‘plant’, ‘interest’. As a result, in the case of ‘plant’; for example, the classifiers with respect to ‘gongchang’ and ‘zhiwu’ only make classification decisions to D and E but not C and F (in Figure 5). It calculates λ * c as ( ) ( |) ( ) λ * c =P c t and sets θ = 0 at the right-hand side of step 2. 5.2 Using Monolingual Bootstrapping We consider here two implementations of MB for word translation disambiguation. In the firs</context>
<context position="19456" citStr="Yarowsky, 1995" startWordPosition="3740" endWordPosition="3741">es Seed words Test sentences English Chinese bass , / , 142 8811 fish / music 200 drug , / 3053 5398 treatment / smuggler 197 duty , / , 1428 4338 discharge / export 197 palm , / 366 465 tree / hand 197 plant , / 7542 24977 industry / life 197 space , / , 3897 14178 volume / outer 197 tank / , 417 1400 combat / fuel 199 Total - 16845 59567 - 1384 As classified data in English, we defined a ‘seed word’ for each group of translations based on our intuition (cf., Table 1). Each of the seed words was then used as a classified ‘sentence’. This way of creating classified data is similar to that in (Yarowsky, 1995). As unclassified data in English, we collected sentences in news articles from a web site (www.news.com), and as unclassified data in Chinese, we collected sentences in news articles from another web site (news.cn.tom.com). We observed that the distribution of translations in the unclassified data was balanced. Table 2 shows the sizes of the data. Note that there are in general more unclassified sentences in Chinese than in English because an English word usually has several Chinese words as translations (cf., Figure 5). As a translation dictionary, we used the HIT dictionary, which contains </context>
<context position="21702" citStr="Yarowsky, 1995" startWordPosition="4108" endWordPosition="4109">-D and MB-B. The results from the sign test are statistically significant (p-value &lt; 0.001). Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). Although BB is a method nearly equivalent to one based on unsupervised learning, it still performs favorably well when compared with the supervised methods (note that since the experimental settings are different, the results cannot be directly compared). 6.2 Experiment 2: Yarowsky’s Words We also conducted translation on seven of the twelve English words studied in (Yarowsky, 1995). Table 5 shows the list of the words. For each of the words, we extracted about 200 sentences containing the word from the Encarta4 English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia 5 Chinese corpus as unclassified data in Chinese. We defined, for each translation, 4 http://encarta.msn.com/default.asp 5 http://www.whlib.ac.cn/sjk/bkqs.htm Table 6: Accuracies in Experiment 2 Words Major MB-D MB-B BB (%) (%) (%</context>
<context position="23389" citStr="Yarowsky, 1995" startWordPosition="4384" endWordPosition="4385">on on the words ‘crane’, ‘sake’, ‘poach’, ‘axes’, and ‘motion’, because the first four words do not frequently occur in the Encarta corpus, and the accuracy of choosing the major translation for the last word has already exceeded 98%. We next applied BB, MB-B, and MB-D to word translation disambiguation. The experiment settings were the same as those in Experiment 1. From Table 6, we see again that BB significantly outperforms MB-D and MB-B. (We will describe the results in detail in the full version of this paper.) Note that the results of MB-D here cannot be directly compared with those in (Yarowsky, 1995), mainly because the data used are different. 6.3 Discussions We investigated the reason of BB’s outperforming MB and found that the explanation on the reason in Section 4 appears to be true according to the following observations. Figure 9: Number of relevant words Figure 10: When more unlabeled data available (1) In a Naïve Bayesian Classifier, words having large values of probability ratio Pie i t) have strong influence on the classification of t when they occur, particularly, when they frequently occur. We collected the words having large values of probability ratio for each t in both BB a</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky, 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pp. 189-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>