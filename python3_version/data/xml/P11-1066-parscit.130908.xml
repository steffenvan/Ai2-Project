<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9990865">
Phrase-Based Translation Model for Question Retrieval in Community
Question Answer Archives
</title>
<author confidence="0.999251">
Guangyou Zhou, Li Cai, Jun Zhao; and Kang Liu
</author>
<affiliation confidence="0.9965935">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
</affiliation>
<address confidence="0.972699">
95 Zhongguancun East Road, Beijing 100190, China
</address>
<email confidence="0.999526">
{gyzhou,lcai,jzhao,kliu}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.990118" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.61467795">
Community-based question answer (Q&amp;A)
has become an important issue due to the pop-
ularity of Q&amp;A archives on the web. This pa-
per is concerned with the problem of ques-
tion retrieval. Question retrieval in Q&amp;A
archives aims to find historical questions that
are semantically equivalent or relevant to the
queried questions. In this paper, we propose
a novel phrase-based translation model for
question retrieval. Compared to the traditional
word-based translation models, the phrase-
based translation model is more effective be-
cause it captures contextual information in
modeling the translation of phrases as a whole,
rather than translating single words in isola-
tion. Experiments conducted on real Q&amp;A
data demonstrate that our proposed phrase-
based translation model significantly outper-
forms the state-of-the-art word-based transla-
tion model.
</bodyText>
<sectionHeader confidence="0.998333" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997507571428571">
Over the past few years, large scale question and
answer (Q&amp;A) archives have become an important
information resource on the Web. These include
the traditional Frequently Asked Questions (FAQ)
archives and the emerging community-based Q&amp;A
services, such as Yahoo! Answers1, Live QnA2, and
Baidu Zhidao3.
</bodyText>
<footnote confidence="0.97528575">
Correspondence author: jzhao@nlpr.ia.ac.cn
1http://answers.yahoo.com/
2http://qna.live.com/
3http://zhidao.baidu.com/
</footnote>
<bodyText confidence="0.997525">
Community-based Q&amp;A services can directly re-
turn answers to the queried questions instead of a
list of relevant documents, thus provide an effective
alternative to the traditional adhoc information re-
trieval. To make full use of the large scale archives
of question-answer pairs, it is critical to have func-
tionality helping users to retrieve historical answers
(Duan et al., 2008). Therefore, it is a meaningful
task to retrieve the questions that are semantically
equivalent or relevant to the queried questions. For
example in Table 1, given question Q1, Q2 can be re-
turned and their answers will then be used to answer
Q1 because the answer of Q2 is expected to partially
satisfy the queried question Q1. This is what we
called question retrieval in this paper.
The major challenge for Q&amp;A retrieval, as for
</bodyText>
<note confidence="0.45918">
Query:
Q1: How to get rid of stuffy nose?
Expected:
</note>
<footnote confidence="0.537222">
Q2: What is the best way to prevent a cold?
</footnote>
<note confidence="0.449918">
Not Expected:
Q3: How do I air out my stuffy room?
Q4: How do you make a nose bleed stop quicker?
</note>
<tableCaption confidence="0.999396">
Table 1: An example on question retrieval
</tableCaption>
<bodyText confidence="0.999956">
most information retrieval models, such as vector
space model (VSM) (Salton et al., 1975), Okapi
model (Robertson et al., 1994), language model
(LM) (Ponte and Croft, 1998), is the lexical gap (or
lexical chasm) between the queried questions and
the historical questions in the archives (Jeon et al.,
2005; Xue et al., 2008). For example in Table 1, Q1
and Q2 are two semantically similar questions, but
they have very few words in common. This prob-
</bodyText>
<page confidence="0.986688">
653
</page>
<note confidence="0.9795945">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997465">
lem is more serious for Q&amp;A retrieval, since the
question-answer pairs are usually short and there is
little chance of finding the same content expressed
using different wording (Xue et al., 2008). To solve
the lexical gap problem, most researchers regarded
the question retrieval task as a statistical machine
translation problem by using IBM model 1 (Brown
et al., 1993) to learn the word-to-word translation
probabilities (Berger and Lafferty, 1999; Jeon et al.,
2005; Xue et al., 2008; Lee et al., 2008; Bernhard
and Gurevych, 2009). Experiments consistently re-
ported that the word-based translation models could
yield better performance than the traditional meth-
ods (e.g., VSM. Okapi and LM). However, all these
existing approaches are considered to be context in-
dependent in that they do not take into account any
contextual information in modeling word translation
probabilities. For example in Table 1, although nei-
ther of the individual word pair (e.g., “stuffy”/“cold”
and “nose”/“cold”) might have a high translation
probability, the sequence of words “stuffy nose” can
be easily translated from a single word “cold” in Q2
with a relative high translation probability.
In this paper, we argue that it is beneficial to cap-
ture contextual information for question retrieval.
To this end, inspired by the phrase-based statistical
machine translation (SMT) systems (Koehn et al.,
2003; Och and Ney, 2004), we propose a phrase-
based translation model (P-Trans) for question re-
trieval, and we assume that question retrieval should
be performed at the phrase level. This model learns
the probability of translating one sequence of words
(e.g., phrase) into another sequence of words, e.g.,
translating a phrase in a historical question into an-
other phrase in a queried question. Compared to the
traditional word-based translation models that ac-
count for translating single words in isolation, the
phrase-based translation model is potentially more
effective because it captures some contextual infor-
mation in modeling the translation of phrases as a
whole. More precise translation can be determined
for phrases than for words. It is thus reasonable to
expect that using such phrase translation probabili-
ties as ranking features is likely to improve the ques-
tion retrieval performance, as we will show in our
experiments.
Unlike the general natural language translation,
the parallel sentences between questions and an-
swers in community-based Q&amp;A have very different
lengths, leaving many words in answers unaligned
to any word in queried questions. Following (Berger
and Lafferty, 1999), we restrict our attention to those
phrase translations consistent with a good word-
level alignment.
Specifically, we make the following contribu-
tions:
</bodyText>
<listItem confidence="0.917041538461539">
• we formulate the question retrieval task as a
phrase-based translation problem by modeling
the contextual information (in Section 3.1).
• we linearly combine the phrase-based transla-
tion model for the question part and answer part
(in Section 3.2).
• we propose a linear ranking model framework
for question retrieval in which different models
are incorporated as features because the phrase-
based translation model cannot be interpolated
with a unigram language model (in Section
3.3).
• finally, we conduct the experiments on
</listItem>
<bodyText confidence="0.8761307">
community-based Q&amp;A data for question re-
trieval. The results show that our proposed ap-
proach significantly outperforms the baseline
methods (in Section 4).
The remainder of this paper is organized as fol-
lows. Section 2 introduces the existing state-of-the-
art methods. Section 3 describes our phrase-based
translation model for question retrieval. Section 4
presents the experimental results. In Section 5, we
conclude with ideas for future research.
</bodyText>
<sectionHeader confidence="0.983707" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.882306">
2.1 Language Model
</subsectionHeader>
<bodyText confidence="0.99991075">
The unigram language model has been widely used
for question retrieval on community-based Q&amp;A
data (Jeon et al., 2005; Xue et al., 2008; Cao et al.,
2010). To avoid zero probability, we use Jelinek-
Mercer smoothing (Zhai and Lafferty, 2001) due to
its good performance and cheap computational cost.
So the ranking function for the query likelihood lan-
guage model with Jelinek-Mercer smoothing can be
</bodyText>
<page confidence="0.991365">
654
</page>
<bodyText confidence="0.554933">
D: ... for good cold home remedies ... document
</bodyText>
<equation confidence="0.911490416666667">
E: [for, good, cold, home remedies] segmentation
F: [for1, best2, stuffy nose3, home remedy4] translation
M: (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation
q: best home remedy for stuffy nose queried question
(1 − λ)P..i(w|D) + λP..l(w|C)
written as:
H
Score(q, D) =
wEq
(1)
P..t(w|D) = #(w, D) , P,n,t(wl C) = #(w, C) (2)
|D |Icl
</equation>
<bodyText confidence="0.99834275">
where q is the queried question, D is a document, C
is background collection, A is smoothing parameter.
#(t, D) is the frequency of term t in D, |D |and |C|
denote the length of D and C respectively.
</bodyText>
<subsectionHeader confidence="0.998133">
2.2 Word-Based Translation Model
</subsectionHeader>
<bodyText confidence="0.997918444444444">
Previous work (Berger et al., 2000; Jeon et al., 2005;
Xue et al., 2008) consistently reported that the word-
based translation models (Trans) yielded better per-
formance than the traditional methods (VSM, Okapi
and LM) for question retrieval. These models ex-
ploit the word translation probabilities in a language
modeling framework. Following Jeon et al. (2005)
and Xue et al. (2008), the ranking function can be
written as:
</bodyText>
<equation confidence="0.9978995">
Score(q, D) = H (1−λ)Pt,(w|D)+λP..t(w|C) (3)
wEq
P(w|t)P..t(t|D), P..t(t|D) = #(t, D)
|D|
</equation>
<bodyText confidence="0.999979">
where P(w|t) denotes the translation probability
from word t to word w.
</bodyText>
<subsectionHeader confidence="0.996613">
2.3 Word-Based Translation Language Model
</subsectionHeader>
<bodyText confidence="0.999886875">
Xue et al. (2008) proposed to linearly mix two dif-
ferent estimations by combining language model
and word-based translation model into a unified
framework, called TransLM. The experiments show
that this model gains better performance than both
the language model and the word-based translation
model. Following Xue et al. (2008), this model can
be written as:
</bodyText>
<equation confidence="0.99386675">
Score(q, D) = H (1 − λ)P...,(w|D) + λP..l(w|C)
wEq
EP...,(w|D) = α P(w|t)P..1(t|D)+(1−α)P..1(w|D)
tED
</equation>
<page confidence="0.873602">
(6)
</page>
<figureCaption confidence="0.9966355">
Figure 1: Example describing the generative procedure
of the phrase-based translation model.
</figureCaption>
<sectionHeader confidence="0.995712666666667" genericHeader="method">
3 Our Approach: Phrase-Based
Translation Model for Question
Retrieval
</sectionHeader>
<subsectionHeader confidence="0.999686">
3.1 Phrase-Based Translation Model
</subsectionHeader>
<bodyText confidence="0.999829606060606">
Phrase-based machine translation models (Koehn
et al., 2003; D. Chiang, 2005; Och and Ney,
2004) have shown superior performance compared
to word-based translation models. In this paper,
the goal of phrase-based translation model is to
translate a document4 D into a queried question
q. Rather than translating single words in isola-
tion, the phrase-based model translates one sequence
of words into another sequence of words, thus in-
corporating contextual information. For example,
we might learn that the phrase “stuffy nose” can be
translated from “cold” with relative high probabil-
ity, even though neither of the individual word pairs
(e.g., “stuffy”/“cold” and “nose”/“cold”) might have
a high word translation probability. Inspired by the
work of (Sun et al., 2010; Gao et al., 2010), we
assume the following generative process: first the
document D is broken into K non-empty word se-
quences t1, ... , tK, then each t is translated into a
new non-empty word sequence w1, ... , wK, and fi-
nally these phrases are permutated and concatenated
to form the queried questions q, where t and w de-
note the phrases or consecutive sequence of words.
To formulate this generative process, let E
denote the segmentation of D into K phrases
t1, ... , tK, and let F denote the K translation
phrases w1, ... , wK −we refer to these (ti, wi)
pairs as bi-phrases. Finally, let M denote a permuta-
tion of K elements representing the final reordering
step. Figure 1 describes an example of the genera-
tive procedure.
Next let us place a probability distribution over
rewrite pairs. Let B(D, q) denote the set of E,
</bodyText>
<footnote confidence="0.8407435">
4In this paper, a document has the same meaning as a histor-
ical question-answer pair in the Q&amp;A archives.
</footnote>
<equation confidence="0.9893735">
Pt,(w|D) = E
tED
</equation>
<page confidence="0.974048">
655
</page>
<bodyText confidence="0.997682">
F, M triples that translate D into q. Here we as-
sume a uniform probability over segmentations, so
the phrase-based translation model can be formu-
lated as:
</bodyText>
<equation confidence="0.995484333333333">
P(q|D) ∝ E P(F|D, E) · P(M|D, E, F) (7)
(E,F,M)E
B(D,q)
</equation>
<bodyText confidence="0.999612222222222">
consistent with A, which we denote as B(D, q, A).
Here, consistency requires that if two words are
aligned in A, then they must appear in the same bi-
phrase (tz, wz). Once the word alignment is fixed,
the final permutation is uniquely determined, so we
can safely discard that factor. Thus equation (8) can
be written as:
As is common practice in SMT, we use the maxi- P(q|D) ≈ max P(F|D, E) (10)
mum approximation to the sum: (E,F,M)EB(D,q, A)
</bodyText>
<equation confidence="0.985674333333333">
P(q|D) ≈ max
(E,F,M)�
B(D,q)
</equation>
<bodyText confidence="0.999992107142857">
Although we have defined a generative model for
translating D into q, our goal is to calculate the rank-
ing score function over existing q and D, rather than
generating new queried questions. Equation (8) can-
not be used directly for document ranking because
q and D are often of very different lengths, leav-
ing many words in D unaligned to any word in q.
This is the key difference between the community-
based question retrieval and the general natural lan-
guage translation. As pointed out by Berger and Laf-
ferty (1999) and Gao et al. (2010), document-query
translation requires a distillation of the document,
while translation of natural language tolerates little
being thrown away.
Thus we attempt to extract the key document
words that form the distillation of the document, and
assume that a queried question is translated only
from the key document words. In this paper, the
key document words are identified via word align-
ment. We introduce the “hidden alignments” A =
al ... aj ... aJ, which describe the mapping from a
word position j in queried question to a document
word position i = aj. The different alignment mod-
els we present provide different decompositions of
P(q, A|D). We assume that the position of the key
document words are determined by the Viterbi align-
ment, which can be obtained using IBM model 1 as
follows:
</bodyText>
<equation confidence="0.9877006">
A� = arg maxP(q, A|D)
A
}P(wj|taj)
= [arg maxP(wj |taj)] (9)
aj jJ=1
</equation>
<bodyText confidence="0.998707285714286">
Given A, when scoring a given Q&amp;A pair, we re-
strict our attention to those E, F, M triples that are
For the sole remaining factor P(F|D, E), we
make the assumption that a segmented queried ques-
tion F = w1, ... , wK is generated from left to
right by translating each phrase ti,... , tK indepen-
dently:
</bodyText>
<equation confidence="0.999511">
K
P(F|D, E) = H P(wk|tk) (11)
k=1
</equation>
<bodyText confidence="0.9997497">
where P(wk|tk) is a phrase translation probability,
the estimation will be described in Section 3.3.
To find the maximum probability assignment ef-
ficiently, we use a dynamic programming approach,
somewhat similar to the monotone decoding algo-
rithm described in (Och, 2002). We define αj to
be the probability of the most likely sequence of
phrases covering the first j words in a queried ques-
tion, then the probability can be calculated using the
following recursion:
</bodyText>
<equation confidence="0.9301154">
(1) Initialization:
α0 = 1 (12)
� 1
αj′P(w|tw) (13)
P(q|D) = αJ (14)
</equation>
<subsectionHeader confidence="0.9954765">
3.2 Phrase-Based Translation Model for
Question Part and Answer Part
</subsectionHeader>
<bodyText confidence="0.999834111111111">
In Q&amp;A, a document D is decomposed into (q, a),
where q� denotes the question part of the historical
question in the archives and a� denotes the answer
part. Although it has been shown that doing Q&amp;A
retrieval based solely on the answer part does not
perform well (Jeon et al., 2005; Xue et al., 2008),
the answer part should provide additional evidence
about relevance and, therefore, it should be com-
bined with the estimation based on the question part.
</bodyText>
<figure confidence="0.869329285714286">
P(F|D, E) · P(M|D, E, F) (8)
= arg max {P(J|I) HJ
A j=1
(2) Induction:
Eαj =
j′&lt;j,w=wj′+1...wj
(3) Total:
</figure>
<page confidence="0.993349">
656
</page>
<bodyText confidence="0.999599">
In this combined model, P(q|q) and P(q|a) are cal-
culated with equations (12) to (14). So P(q|D) will
be written as:
</bodyText>
<equation confidence="0.9339765">
P(q|D) = µ1P(q|¯q) + µ2P(q|¯a) (15)
where µ1 + µ2 = 1.
</equation>
<bodyText confidence="0.999932833333333">
In equation (15), the relative importance of ques-
tion part and answer part is adjusted through µ1 and
µ2. When µ1 = 1, the retrieval model is based
on phrase-based translation model for the question
part. When µ2 = 1, the retrieval model is based on
phrase-based translation model for the answer part.
</bodyText>
<subsectionHeader confidence="0.8941275">
3.3 Parameter Estimation
3.3.1 Parallel Corpus Collection
</subsectionHeader>
<bodyText confidence="0.99138056">
In Q&amp;A archives, question-answer pairs can be con-
sidered as a type of parallel corpus, which is used for
estimating the translation probabilities. Unlike the
bilingual machine translation, the questions and an-
swers in a Q&amp;A archive are written in the same lan-
guage, the translation probability can be calculated
through setting either as the source and the other as
the target. In this paper, P(a|q) is used to denote
the translation probability with the question as the
source and the answer as the target. P(g|a) is used
to denote the opposite configuration.
For a given word or phrase, the related words
or phrases differ when it appears in the ques-
tion or in the answer. Following Xue et
al. (2008), a pooling strategy is adopted. First,
we pool the question-answer pairs used to learn
P(a|q) and the answer-question pairs used to
learn P(g|a), and then use IBM model 1 (Brown
et al., 1993) to learn the combined translation
probabilities. Suppose we use the collection
{(q, a)1, ... , (q, a)m} to learn P(a|q) and use the
collection {(a, q)1, . . . , (a, q)m} to learn P(g|a),
then {(q, a)1, ... , (q, a)m, (��, q)1, ... , (��, q)-} is
used here to learn the combination translation prob-
ability Ppool(wi|tj).
</bodyText>
<subsectionHeader confidence="0.828534">
3.3.2 Parallel Corpus Preprocessing
</subsectionHeader>
<bodyText confidence="0.999923">
Unlike the bilingual parallel corpus used in SMT,
our parallel corpus is collected from Q&amp;A archives,
which is more noisy. Directly using the IBM model
1 can be problematic, it is possible for translation
model to contain “unnecessary” translations (Lee et
al., 2008). In this paper, we adopt a variant of Tex-
tRank algorithm (Mihalcea and Tarau, 2004) to iden-
tify and eliminate unimportant words from parallel
corpus, assuming that a word in a question or an-
swer is unimportant if it holds a relatively low sig-
nificance in the parallel corpus.
Following (Lee et al., 2008), the ranking algo-
rithm proceeds as follows. First, all the words in
a given document are added as vertices in a graph
G. Then edges are added between words if the
words co-occur in a fixed-sized window. The num-
ber of co-occurrences becomes the weight of an
edge. When the graph is constructed, the score of
each vertex is initialized as 1, and the PageRank-
based ranking algorithm is run on the graph itera-
tively until convergence. The TextRank score of a
word w in document D at kth iteration is defined as
follows:
</bodyText>
<equation confidence="0.98878525">
ei,j �Rk−1
w,D
Vl:(j,l)EG ej,l
(16)
</equation>
<bodyText confidence="0.9991164">
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
We use average TextRank score as threshold:
words are removed if their scores are lower than the
average score of all words in a document.
</bodyText>
<subsubsectionHeader confidence="0.679928">
3.3.3 Translation Probability Estimation
</subsubsectionHeader>
<bodyText confidence="0.999834533333333">
After preprocessing the parallel corpus, we will cal-
culate P(w|t), following the method commonly
used in SMT (Koehn et al., 2003; Och, 2002) to ex-
tract bi-phrases and estimate their translation proba-
bilities.
First, we learn the word-to-word translation prob-
ability using IBM model 1 (Brown et al., 1993).
Then, we perform Viterbi word alignment according
to equation (9). Finally, the bi-phrases that are con-
sistent with the word alignment are extracted using
the heuristics proposed in (Och, 2002). We set the
maximum phrase length to five in our experiments.
After gathering all such bi-phrases from the train-
ing data, we can estimate conditional relative fre-
quency estimates without smoothing:
</bodyText>
<equation confidence="0.996730666666667">
N(t, w)
P(w|t) = (17)
N(t)
</equation>
<bodyText confidence="0.998291">
where N(t, w) is the number of times that t is
aligned to w in training data. These estimates are
</bodyText>
<equation confidence="0.949993666666667">
�
Rkw,D = (1 − d) + d ·
Vj:(i,j)EG
</equation>
<page confidence="0.9947">
657
</page>
<table confidence="0.998749">
source stuffy nose internet explorer
1 stuffy nose internet explorer
2 cold ie
3 stuffy internet browser
4 sore throat explorer
5 sneeze browser
</table>
<tableCaption confidence="0.996453">
Table 2: Phrase translation probability examples. Each
column shows the top 5 target phrases learned from the
word-aligned question-answer pairs.
</tableCaption>
<bodyText confidence="0.988088090909091">
useful for contextual lexical selection with sufficient
training data, but can be subject to data sparsity is-
sues (Sun et al., 2010; Gao et al., 2010). An alter-
nate translation probability estimate not subject to
data sparsity is the so-called lexical weight estimate
(Koehn et al., 2003). Let P(w|t) be the word-to-
word translation probability, and let A be the word
alignment between w and t. Here, the word align-
ment contains (i, j) pairs, where i E 1... |w |and
j E 0 ... |t|, with 0 indicating a null word. Then we
use the following estimate:
</bodyText>
<equation confidence="0.999366">
Pt(wlt, A) = � |W |14) ,= All E P(wiltj)
i�� V(i,j)EA
(18)
</equation>
<bodyText confidence="0.999937380952381">
We assume that for each position in w, there is ei-
ther a single alignment to 0, or multiple alignments
to non-zero positions in t. In fact, equation (18)
computes a product of per-word translation scores;
the per-word scores are the averages of all the trans-
lations for the alignment links of that word. The
word translation probabilities are calculated using
IBM 1, which has been widely used for question re-
trieval (Jeon et al., 2005; Xue et al., 2008; Lee et al.,
2008; Bernhard and Gurevych, 2009). These word-
based scores of bi-phrases, though not as effective
in contextual selection, are more robust to noise and
sparsity.
A sample of the resulting phrase translation ex-
amples is shown in Table 2, where the top 5 target
phrases are translated from the source phrases ac-
cording to the phrase-based translation model. For
example, the term “explorer” used alone, most likely
refers to a person who engages in scientific explo-
ration, while the phrase “internet explorer” has a
very different meaning.
</bodyText>
<subsectionHeader confidence="0.991951">
3.4 Ranking Candidate Historical Questions
</subsectionHeader>
<bodyText confidence="0.99997525">
Unlike the word-based translation models, the
phrase-based translation model cannot be interpo-
lated with a unigram language model. Following
(Sun et al., 2010; Gao et al., 2010), we resort to
a linear ranking framework for question retrieval in
which different models are incorporated as features.
We consider learning a relevance function of the
following general, linear form:
</bodyText>
<equation confidence="0.948436">
Score(q, D) = BT · P(q, D) (19)
</equation>
<bodyText confidence="0.999756">
where the feature vector Φ(q, D) is an arbitrary
function that maps (q, D) to a real value, i.e.,
Φ(q, D) E R. 0 is the corresponding weight vec-
tor, we optimize this parameter for our evaluation
metrics directly using the Powell Search algorithm
(Paul et al., 1992) via cross-validation.
The features used in this paper are as follows:
</bodyText>
<listItem confidence="0.965292166666667">
• Phrase translation features (PT):
ΦPT(q, D, A) = logP(q|D), where P(q|D)
is computed using equations (12) to (15), and
the phrase translation probability P(w|t) is
estimated using equation (17).
• Inverted Phrase translation features (IPT):
ΦIPT(D, q, A) = logP(D|q), where P(D|q)
is computed using equations (12) to (15) ex-
cept that we set µ2 = 0 in equation (15), and
the phrase translation probability P(w|t) is es-
timated using equation (17).
• Lexical weight feature (LW):
ΦLW (q, D, A) = logP(q|D), here P(q|D)
is computed by equations (12) to (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
• Inverted Lexical weight feature (ILW):
ΦILW (D, q, A) = logP(D|q), here P(D|q)
</listItem>
<bodyText confidence="0.8387125">
is computed by equations (12) to (15) except
that we set µ2 = 0 in equation (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
</bodyText>
<listItem confidence="0.608547">
• Phrase alignment features (PA):
ΦPA(q, D, B) = E2 |ak − bk−1 − 1|,
</listItem>
<bodyText confidence="0.990482">
where B is a set of K bi-phrases, ak is the start
position of the phrase in D that was translated
</bodyText>
<page confidence="0.997318">
658
</page>
<bodyText confidence="0.999879785714286">
into the kth phrase in queried question, and
bk_1 is the end position of the phrase in D
that was translated into the (k − 1)th phrase in
queried question. The feature, inspired by the
distortion model in SMT (Koehn et al., 2003),
models the degree to which the queried phrases
are reordered. For all possible B, we only
compute the feature value according to the
Viterbi alignment, B� = arg maxB P(q, B|D).
We find B� using the Viterbi algorithm, which is
almost identical to the dynamic programming
recursion of equations (12) to (14), except that
the sum operator in equation (13) is replaced
with the max operator.
</bodyText>
<listItem confidence="0.998203538461538">
• Unaligned word penalty features (UWP):
4)UWP(q, D), which is defined as the ratio be-
tween the number of unaligned words and the
total number of words in queried questions.
• Language model features (LM):
4)LM(q, D, A) = logPLM(q|D), where
PLM(q|D) is the unigram language model
with Jelinek-Mercer smoothing defined by
equations (1) and (2).
• Word translation features (WT):
4)WT(q, D) = logP(q|D), where P(q|D) is
the word-based translation model defined by
equations (3) and (4).
</listItem>
<sectionHeader confidence="0.998128" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999945">
4.1 Data Set and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99990425">
We collect the questions from Yahoo! Answers and
use the getByCategory function provided in Yahoo!
Answers API5 to obtain Q&amp;A threads from the Ya-
hoo! site. More specifically, we utilize the resolved
questions under the top-level category at Yahoo!
Answers, namely “Computers &amp; Internet”. The re-
sulting question repository that we use for question
retrieval contains 518,492 questions. To learn the
translation probabilities, we use about one million
question-answer pairs from another data set.6
In order to create the test set, we randomly se-
lect 300 questions for this category, denoted as
</bodyText>
<footnote confidence="0.99923375">
5http://developer.yahoo.com/answers
6The Yahoo! Webscope dataset Yahoo answers com-
prehensive questions and answers version 1.0.2, available at
http://reseach.yahoo.com/Academic Relations.
</footnote>
<bodyText confidence="0.99482975">
“CI TST”. To obtain the ground-truth of ques-
tion retrieval, we employ the Vector Space Model
(VSM) (Salton et al., 1975) to retrieve the top 20 re-
sults and obtain manual judgements. The top 20 re-
sults don’t include the queried question itself. Given
a returned result by VSM, an annotator is asked to
label it with “relevant” or “irrelevant”. If a returned
result is considered semantically equivalent to the
queried question, the annotator will label it as “rel-
evant”; otherwise, the annotator will label it as “ir-
relevant”. Two annotators are involved in the anno-
tation process. If a conflict happens, a third person
will make judgement for the final result. In the pro-
cess of manually judging questions, the annotators
are presented only the questions. Table 3 provides
the statistics on the final test set.
</bodyText>
<table confidence="0.962268">
#queries #returned #relevant
CI TST 300 6,000 798
</table>
<tableCaption confidence="0.999849">
Table 3: Statistics on the Test Data
</tableCaption>
<bodyText confidence="0.999956714285714">
We evaluate the performance of our approach us-
ing Mean Average Precision (MAP). We perform
a significant test, i.e., a t-test with a default signif-
icant level of 0.05. Following the literature, we set
the parameters A = 0.2 (Cao et al., 2010) in equa-
tions (1), (3) and (5), and α = 0.8 (Xue et al., 2008)
in equation (6).
</bodyText>
<subsectionHeader confidence="0.998884">
4.2 Question Retrieval Results
</subsectionHeader>
<bodyText confidence="0.999986277777778">
We randomly divide the test questions into five
subsets and conduct 5-fold cross-validation experi-
ments. In each trial, we tune the parameters µ1 and
µ2 with four of the five subsets and then apply it to
one remaining subset. The experiments reported be-
low are those averaged over the five trials.
Table 4 presents the main retrieval performance.
Row 1 to row 3 are baseline systems, all these meth-
ods use word-based translation models and obtain
the state-of-the-art performance in previous work
(Jeon et al., 2005; Xue et al., 2008). Row 3 is simi-
lar to row 2, the only difference is that TransLM only
considers the question part, while Xue et al. (2008)
incorporates the question part and answer part. Row
4 and row 5 are our proposed phrase-based trans-
lation model with maximum phrase length of five.
Row 4 is phrase-based translation model purely
based on question part, this model is equivalent to
</bodyText>
<page confidence="0.997036">
659
</page>
<table confidence="0.846035333333333">
# Methods Trans Prob MAP
1 Jeon et al. (2005) Ppoot 0.289
2 TransLM Ppoot 0.324
3 Xue et al. (2008) Ppoot 0.352
4 P-Trans (µl = 1, l = 5) Ppoot 0.366
5 P-Trans (l = 5) Ppoot 0.391
</table>
<tableCaption confidence="0.9860875">
Table 4: Comparison with different methods for question
retrieval.
</tableCaption>
<bodyText confidence="0.999692444444444">
setting pi = 1 in equation (15). Row 5 is the phrase-
based combination model which linearly combines
the question part and answer part. As expected,
different parts can play different roles: a phrase to
be translated in queried questions may be translated
from the question part or answer part. All these
methods use pooling strategy to estimate the transla-
tion probabilities. There are some clear trends in the
result of Table 4:
</bodyText>
<listItem confidence="0.866528785714286">
(1) Word-based translation language model
(TransLM) significantly outperforms word-based
translation model of Jeon et al. (2005) (row 1 vs. row
2). Similar observations have been made by Xue et
al. (2008).
(2) Incorporating the answer part into the models,
either word-based or phrase-based, can significantly
improve the performance of question retrieval (row
2 vs. row 3; row 4 vs. row 5).
(3) Our proposed phrase-based translation model
(P-Trans) significantly outperforms the state-of-the-
art word-based translation models (row 2 vs. row 4
and row 3 vs. row 5, all these comparisons are sta-
tistically significant at p &lt; 0.05).
</listItem>
<subsectionHeader confidence="0.99955">
4.3 Impact of Phrase Length
</subsectionHeader>
<bodyText confidence="0.999971333333333">
Our proposed phrase-based translation model, due to
its capability of capturing contextual information, is
more effective than the state-of-the-art word-based
translation models. It is important to investigate the
impact of the phrase length on the final retrieval per-
formance. Table 5 shows the results, it is seen that
using the longer phrases up to the maximum length
of five can consistently improve the retrieval per-
formance. However, using much longer phrases in
the phrase-based translation model does not seem to
produce significantly better performance (row 8 and
row 9 vs. row 10 are not statistically significant).
</bodyText>
<figure confidence="0.976402333333333">
# Systems MAP
6 P-Trans (l = 1) 0.352
7 P-Trans (l = 2) 0.373
8 P-Trans (l = 3) 0.386
9 P-Trans (l = 4) 0.390
10 P-Trans (l = 5) 0.391
</figure>
<tableCaption confidence="0.9817">
Table 5: The impact of the phrase length on retrieval per-
formance.
</tableCaption>
<table confidence="0.998871333333333">
Model # Methods Average MAP
P-Trans (l = 5) 11 Initial 69 0.380
12 TextRank 24 0.391
</table>
<tableCaption confidence="0.999158">
Table 6: Effectiveness of parallel corpus preprocessing.
</tableCaption>
<subsectionHeader confidence="0.985919">
4.4 Effectiveness of Parallel Corpus
Preprocessing
</subsectionHeader>
<bodyText confidence="0.999981">
Question-answer pairs collected from Yahoo! an-
swers are very noisy, it is possible for translation
models to contain “unnecessary” translations. In this
paper, we attempt to identify and decrease the pro-
portion of unnecessary translations in a translation
model by using TextRank algorithm. This kind of
“unnecessary” translation between words will even-
tually affect the bi-phrase translation.
Table 6 shows the effectiveness of parallel corpus
preprocessing. Row 11 reports the average number
of translations per word and the question retrieval
performance when only stopwords 7 are removed.
When using the TextRank algorithm for parallel cor-
pus preprocessing, the average number of transla-
tions per word is reduced from 69 to 24, but the
performance of question retrieval is significantly im-
proved (row 11 vs. row 12). Similar results have
been made by Lee et al. (2008).
</bodyText>
<subsectionHeader confidence="0.997857">
4.5 Impact of Pooling Strategy
</subsectionHeader>
<bodyText confidence="0.999784444444444">
The correspondence of words or phrases in the
question-answer pair is not as strong as in the bilin-
gual sentence pair, thus noise will be inevitably in-
troduced for both P(ajq) and P(qja).
To see how much the pooling strategy benefit the
question retrieval, we introduce two baseline meth-
ods for comparison. The first method (denoted as
P(ajq)) is used to denote the translation probabil-
ity with the question as the source and the answer as
</bodyText>
<footnote confidence="0.957819">
7http://truereader.com/manuals/onix/stopwords1.html
</footnote>
<page confidence="0.978043">
660
</page>
<table confidence="0.997735">
Model # Trans Prob MAP
13 P(dlq) 0.387
P-Trans (l = 5) 14 P(qja) 0.381
15 Ppoot 0.391
</table>
<tableCaption confidence="0.9823555">
Table 7: The impact of pooling strategy for question re-
trieval.
</tableCaption>
<bodyText confidence="0.999317714285714">
the target. The second (denoted as P(ajq)) is used
to denote the translation probability with the answer
as the source and the question as the target. Table 7
provides the comparison. From this Table, we see
that the pooling strategy significantly outperforms
the two baseline methods for question retrieval (row
13 and row 14 vs. row 15).
</bodyText>
<sectionHeader confidence="0.998358" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999968210526316">
In this paper, we propose a novel phrase-based trans-
lation model for question retrieval. Compared to
the traditional word-based translation models, the
proposed approach is more effective in that it can
capture contextual information instead of translating
single words in isolation. Experiments conducted
on real Q&amp;A data demonstrate that the phrase-
based translation model significantly outperforms
the state-of-the-art word-based translation models.
There are some ways in which this research could
be continued. First, question structure should be
considered, so it is necessary to combine the pro-
posed approach with other question retrieval meth-
ods (e.g., (Duan et al., 2008; Wang et al., 2009;
Bunescu and Huang, 2010)) to further improve the
performance. Second, we will try to investigate the
use of the proposed approach for other kinds of data
set, such as categorized questions from forum sites
and FAQ sites.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999852166666667">
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106). We thank the anonymous reviewers
for their insightful comments. We also thank Maoxi
Li and Jiajun Zhang for suggestion to use the align-
ment toolkits.
</bodyText>
<sectionHeader confidence="0.994325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934705882353">
A. Berger and R. Caruana and D. Cohn and D. Freitag and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222-229.
D. Bernhard and I. Gurevych. 2009. Combining lexical
semantic resources with question &amp; answer archives
for translation-based answer finding. In Proceedings
of ACL, pages 728-736.
P. F. Brown and V. J. D. Pietra and S. A. D. Pietra and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
R. Bunescu and Y. Huang. 2010. Learning the relative
usefulness of questions in community QA. In Pro-
ceedings of EMNLP, pages 97-107.
X. Cao and G. Cong and B. Cui and C. S. Jensen. 2010.
A generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings ofACL.
H. Duan and Y. Cao and C. Y. Lin and Y. Yu. 2008.
Searching questions by identifying questions topics
and question focus. In Proceedings of ACL, pages
156-164.
J. Gao and X. He and J. Nie. 2010. Clickthrough-based
translation models for web search: from word models
to phrase models. In Proceedings of CIKM.
J. Jeon and W. Bruce Croft and J. H. Lee. 2005. Find-
ing similar questions in large question and answer
archives. In Proceedings of CIKM, pages 84-90.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into text. In Proceedings of EMNLP, pages 404-
411.
P. Koehn and F. Och and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL,
pages 48-54.
J. -T. Lee and S. -B. Kim and Y. -I. Song and H. -C. Rim.
2008. Bridging lexical gaps between queries and ques-
tions on large online Q&amp;A collections with compact
translation models. In Proceedings of EMNLP, pages
410-418.
F. Och. 2002. Statistical mahcine translation: from sin-
gle word models to alignment templates. Ph.D thesis,
RWTH Aachen.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417-449.
</reference>
<page confidence="0.972047">
661
</page>
<reference confidence="0.99768568">
J. M. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
SIGIR.
W. H. Press and S. A. Teukolsky and W. T. Vetterling
and B. P. Flannery. 1992. Numerical Recipes In C.
Cambridge Univ. Press.
S. Robertson and S. Walker and S. Jones and M.
Hancock-Beaulieu and M. Gatford. 1994. Okapi at
trec-3. In Proceedings of TREC, pages 109-126.
G. Salton and A. Wong and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications
of the ACM, 18(11):613-620.
X. Sun and J. Gao and D. Micol and C. Quirk. 2010.
Learning phrase-based spelling error models from
clickthrough data. In Proceedings of ACL.
K. Wang and Z. Ming and T-S. Chua. 2009. A syntactic
tree matching approach to finding similar questions in
community-based qa services. In Proceedings of SI-
GIR, pages 187-194.
X. Xue and J. Jeon and W. B. Croft. 2008. Retrieval
models for question and answer archives. In Proceed-
ings of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
</reference>
<page confidence="0.997235">
662
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822127">
<title confidence="0.9943305">Phrase-Based Translation Model for Question Retrieval in Question Answer Archives</title>
<author confidence="0.998769">Li Cai Zhou</author>
<author confidence="0.998769">Jun Kang</author>
<affiliation confidence="0.9879895">National Laboratory of Pattern Institute of Automation, Chinese Academy of</affiliation>
<address confidence="0.864755">95 Zhongguancun East Road, Beijing 100190,</address>
<abstract confidence="0.998955238095238">Community-based question answer (Q&amp;A) has become an important issue due to the popularity of Q&amp;A archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&amp;A archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&amp;A data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>R Caruana</author>
<author>D Cohn</author>
<author>D Freitag</author>
<author>V Mittal</author>
</authors>
<title>Bridging the lexical chasm: statistical approach to answer-finding.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="8089" citStr="Berger et al., 2000" startWordPosition="1253" endWordPosition="1256"> good cold home remedies ... document E: [for, good, cold, home remedies] segmentation F: [for1, best2, stuffy nose3, home remedy4] translation M: (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation q: best home remedy for stuffy nose queried question (1 − λ)P..i(w|D) + λP..l(w|C) written as: H Score(q, D) = wEq (1) P..t(w|D) = #(w, D) , P,n,t(wl C) = #(w, C) (2) |D |Icl where q is the queried question, D is a document, C is background collection, A is smoothing parameter. #(t, D) is the frequency of term t in D, |D |and |C| denote the length of D and C respectively. 2.2 Word-Based Translation Model Previous work (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008) consistently reported that the wordbased translation models (Trans) yielded better performance than the traditional methods (VSM, Okapi and LM) for question retrieval. These models exploit the word translation probabilities in a language modeling framework. Following Jeon et al. (2005) and Xue et al. (2008), the ranking function can be written as: Score(q, D) = H (1−λ)Pt,(w|D)+λP..t(w|C) (3) wEq P(w|t)P..t(t|D), P..t(t|D) = #(t, D) |D| where P(w|t) denotes the translation probability from word t to word w. 2.3 Word-Based Translation Language Model Xue et </context>
</contexts>
<marker>Berger, Caruana, Cohn, Freitag, Mittal, 2000</marker>
<rawString>A. Berger and R. Caruana and D. Cohn and D. Freitag and V. Mittal. 2000. Bridging the lexical chasm: statistical approach to answer-finding. In Proceedings of SIGIR, pages 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information retrieval as statistical translation.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>222--229</pages>
<contexts>
<context position="3752" citStr="Berger and Lafferty, 1999" startWordPosition="570" endWordPosition="573">the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequen</context>
<context position="5891" citStr="Berger and Lafferty, 1999" startWordPosition="900" endWordPosition="903">ause it captures some contextual information in modeling the translation of phrases as a whole. More precise translation can be determined for phrases than for words. It is thus reasonable to expect that using such phrase translation probabilities as ranking features is likely to improve the question retrieval performance, as we will show in our experiments. Unlike the general natural language translation, the parallel sentences between questions and answers in community-based Q&amp;A have very different lengths, leaving many words in answers unaligned to any word in queried questions. Following (Berger and Lafferty, 1999), we restrict our attention to those phrase translations consistent with a good wordlevel alignment. Specifically, we make the following contributions: • we formulate the question retrieval task as a phrase-based translation problem by modeling the contextual information (in Section 3.1). • we linearly combine the phrase-based translation model for the question part and answer part (in Section 3.2). • we propose a linear ranking model framework for question retrieval in which different models are incorporated as features because the phrasebased translation model cannot be interpolated with a u</context>
<context position="12272" citStr="Berger and Lafferty (1999)" startWordPosition="1952" endWordPosition="1956">xi- P(q|D) ≈ max P(F|D, E) (10) mum approximation to the sum: (E,F,M)EB(D,q, A) P(q|D) ≈ max (E,F,M)� B(D,q) Although we have defined a generative model for translating D into q, our goal is to calculate the ranking score function over existing q and D, rather than generating new queried questions. Equation (8) cannot be used directly for document ranking because q and D are often of very different lengths, leaving many words in D unaligned to any word in q. This is the key difference between the communitybased question retrieval and the general natural language translation. As pointed out by Berger and Lafferty (1999) and Gao et al. (2010), document-query translation requires a distillation of the document, while translation of natural language tolerates little being thrown away. Thus we attempt to extract the key document words that form the distillation of the document, and assume that a queried question is translated only from the key document words. In this paper, the key document words are identified via word alignment. We introduce the “hidden alignments” A = al ... aj ... aJ, which describe the mapping from a word position j in queried question to a document word position i = aj. The different align</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>A. Berger and J. Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of SIGIR, pages 222-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bernhard</author>
<author>I Gurevych</author>
</authors>
<title>Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>728--736</pages>
<contexts>
<context position="3837" citStr="Bernhard and Gurevych, 2009" startWordPosition="586" endWordPosition="589">3–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 wi</context>
<context position="20104" citStr="Bernhard and Gurevych, 2009" startWordPosition="3319" endWordPosition="3322"> with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equation (18) computes a product of per-word translation scores; the per-word scores are the averages of all the translations for the alignment links of that word. The word translation probabilities are calculated using IBM 1, which has been widely used for question retrieval (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). These wordbased scores of bi-phrases, though not as effective in contextual selection, are more robust to noise and sparsity. A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model. For example, the term “explorer” used alone, most likely refers to a person who engages in scientific exploration, while the phrase “internet explorer” has a very different meaning. 3.4 Ranking Candidate Historical Questions Unlike the word-based translation models, the phrase-</context>
</contexts>
<marker>Bernhard, Gurevych, 2009</marker>
<rawString>D. Bernhard and I. Gurevych. 2009. Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding. In Proceedings of ACL, pages 728-736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="3673" citStr="Brown et al., 1993" startWordPosition="560" endWordPosition="563">ons, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/</context>
<context position="16090" citStr="Brown et al., 1993" startWordPosition="2620" endWordPosition="2623">lation probability can be calculated through setting either as the source and the other as the target. In this paper, P(a|q) is used to denote the translation probability with the question as the source and the answer as the target. P(g|a) is used to denote the opposite configuration. For a given word or phrase, the related words or phrases differ when it appears in the question or in the answer. Following Xue et al. (2008), a pooling strategy is adopted. First, we pool the question-answer pairs used to learn P(a|q) and the answer-question pairs used to learn P(g|a), and then use IBM model 1 (Brown et al., 1993) to learn the combined translation probabilities. Suppose we use the collection {(q, a)1, ... , (q, a)m} to learn P(a|q) and use the collection {(a, q)1, . . . , (a, q)m} to learn P(g|a), then {(q, a)1, ... , (q, a)m, (��, q)1, ... , (��, q)-} is used here to learn the combination translation probability Ppool(wi|tj). 3.3.2 Parallel Corpus Preprocessing Unlike the bilingual parallel corpus used in SMT, our parallel corpus is collected from Q&amp;A archives, which is more noisy. Directly using the IBM model 1 can be problematic, it is possible for translation model to contain “unnecessary” translat</context>
<context position="18148" citStr="Brown et al., 1993" startWordPosition="2978" endWordPosition="2981">lows: ei,j �Rk−1 w,D Vl:(j,l)EG ej,l (16) where d is a damping factor usually set to 0.85, and ei,j is an edge weight between i and j. We use average TextRank score as threshold: words are removed if their scores are lower than the average score of all words in a document. 3.3.3 Translation Probability Estimation After preprocessing the parallel corpus, we will calculate P(w|t), following the method commonly used in SMT (Koehn et al., 2003; Och, 2002) to extract bi-phrases and estimate their translation probabilities. First, we learn the word-to-word translation probability using IBM model 1 (Brown et al., 1993). Then, we perform Viterbi word alignment according to equation (9). Finally, the bi-phrases that are consistent with the word alignment are extracted using the heuristics proposed in (Och, 2002). We set the maximum phrase length to five in our experiments. After gathering all such bi-phrases from the training data, we can estimate conditional relative frequency estimates without smoothing: N(t, w) P(w|t) = (17) N(t) where N(t, w) is the number of times that t is aligned to w in training data. These estimates are � Rkw,D = (1 − d) + d · Vj:(i,j)EG 657 source stuffy nose internet explorer 1 stu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown and V. J. D. Pietra and S. A. D. Pietra and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>Y Huang</author>
</authors>
<title>Learning the relative usefulness of questions in community QA.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>97--107</pages>
<marker>Bunescu, Huang, 2010</marker>
<rawString>R. Bunescu and Y. Huang. 2010. Learning the relative usefulness of questions in community QA. In Proceedings of EMNLP, pages 97-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Cao</author>
<author>G Cong</author>
<author>B Cui</author>
<author>C S Jensen</author>
</authors>
<title>A generalized framework of exploring category information for question retrieval in community question answer archives.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="7211" citStr="Cao et al., 2010" startWordPosition="1103" endWordPosition="1106">ta for question retrieval. The results show that our proposed approach significantly outperforms the baseline methods (in Section 4). The remainder of this paper is organized as follows. Section 2 introduces the existing state-of-theart methods. Section 3 describes our phrase-based translation model for question retrieval. Section 4 presents the experimental results. In Section 5, we conclude with ideas for future research. 2 Preliminaries 2.1 Language Model The unigram language model has been widely used for question retrieval on community-based Q&amp;A data (Jeon et al., 2005; Xue et al., 2008; Cao et al., 2010). To avoid zero probability, we use JelinekMercer smoothing (Zhai and Lafferty, 2001) due to its good performance and cheap computational cost. So the ranking function for the query likelihood language model with Jelinek-Mercer smoothing can be 654 D: ... for good cold home remedies ... document E: [for, good, cold, home remedies] segmentation F: [for1, best2, stuffy nose3, home remedy4] translation M: (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation q: best home remedy for stuffy nose queried question (1 − λ)P..i(w|D) + λP..l(w|C) written as: H Score(q, D) = wEq (1) P..t(w|D) = #(w, D) , P,n,t(wl C) = #(w, C) (</context>
<context position="25557" citStr="Cao et al., 2010" startWordPosition="4219" endWordPosition="4222">”. Two annotators are involved in the annotation process. If a conflict happens, a third person will make judgement for the final result. In the process of manually judging questions, the annotators are presented only the questions. Table 3 provides the statistics on the final test set. #queries #returned #relevant CI TST 300 6,000 798 Table 3: Statistics on the Test Data We evaluate the performance of our approach using Mean Average Precision (MAP). We perform a significant test, i.e., a t-test with a default significant level of 0.05. Following the literature, we set the parameters A = 0.2 (Cao et al., 2010) in equations (1), (3) and (5), and α = 0.8 (Xue et al., 2008) in equation (6). 4.2 Question Retrieval Results We randomly divide the test questions into five subsets and conduct 5-fold cross-validation experiments. In each trial, we tune the parameters µ1 and µ2 with four of the five subsets and then apply it to one remaining subset. The experiments reported below are those averaged over the five trials. Table 4 presents the main retrieval performance. Row 1 to row 3 are baseline systems, all these methods use word-based translation models and obtain the state-of-the-art performance in previo</context>
</contexts>
<marker>Cao, Cong, Cui, Jensen, 2010</marker>
<rawString>X. Cao and G. Cong and B. Cui and C. S. Jensen. 2010. A generalized framework of exploring category information for question retrieval in community question answer archives. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="9422" citStr="Chiang, 2005" startWordPosition="1455" endWordPosition="1456">l into a unified framework, called TransLM. The experiments show that this model gains better performance than both the language model and the word-based translation model. Following Xue et al. (2008), this model can be written as: Score(q, D) = H (1 − λ)P...,(w|D) + λP..l(w|C) wEq EP...,(w|D) = α P(w|t)P..1(t|D)+(1−α)P..1(w|D) tED (6) Figure 1: Example describing the generative procedure of the phrase-based translation model. 3 Our Approach: Phrase-Based Translation Model for Question Retrieval 3.1 Phrase-Based Translation Model Phrase-based machine translation models (Koehn et al., 2003; D. Chiang, 2005; Och and Ney, 2004) have shown superior performance compared to word-based translation models. In this paper, the goal of phrase-based translation model is to translate a document4 D into a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, we might learn that the phrase “stuffy nose” can be translated from “cold” with relative high probability, even though neither of the individual word pairs (e.g., “stuffy”/“cold” and “nose”/“col</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Duan</author>
<author>Y Cao</author>
<author>C Y Lin</author>
<author>Y Yu</author>
</authors>
<title>Searching questions by identifying questions topics and question focus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>156--164</pages>
<contexts>
<context position="1992" citStr="Duan et al., 2008" startWordPosition="276" endWordPosition="279">s (FAQ) archives and the emerging community-based Q&amp;A services, such as Yahoo! Answers1, Live QnA2, and Baidu Zhidao3. Correspondence author: jzhao@nlpr.ia.ac.cn 1http://answers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ Community-based Q&amp;A services can directly return answers to the queried questions instead of a list of relevant documents, thus provide an effective alternative to the traditional adhoc information retrieval. To make full use of the large scale archives of question-answer pairs, it is critical to have functionality helping users to retrieve historical answers (Duan et al., 2008). Therefore, it is a meaningful task to retrieve the questions that are semantically equivalent or relevant to the queried questions. For example in Table 1, given question Q1, Q2 can be returned and their answers will then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1. This is what we called question retrieval in this paper. The major challenge for Q&amp;A retrieval, as for Query: Q1: How to get rid of stuffy nose? Expected: Q2: What is the best way to prevent a cold? Not Expected: Q3: How do I air out my stuffy room? Q4: How do you make a </context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>H. Duan and Y. Cao and C. Y. Lin and Y. Yu. 2008. Searching questions by identifying questions topics and question focus. In Proceedings of ACL, pages 156-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>X He</author>
<author>J Nie</author>
</authors>
<title>Clickthrough-based translation models for web search: from word models to phrase models.</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="10134" citStr="Gao et al., 2010" startWordPosition="1564" endWordPosition="1567"> In this paper, the goal of phrase-based translation model is to translate a document4 D into a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, we might learn that the phrase “stuffy nose” can be translated from “cold” with relative high probability, even though neither of the individual word pairs (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high word translation probability. Inspired by the work of (Sun et al., 2010; Gao et al., 2010), we assume the following generative process: first the document D is broken into K non-empty word sequences t1, ... , tK, then each t is translated into a new non-empty word sequence w1, ... , wK, and finally these phrases are permutated and concatenated to form the queried questions q, where t and w denote the phrases or consecutive sequence of words. To formulate this generative process, let E denote the segmentation of D into K phrases t1, ... , tK, and let F denote the K translation phrases w1, ... , wK −we refer to these (ti, wi) pairs as bi-phrases. Finally, let M denote a permutation o</context>
<context position="12294" citStr="Gao et al. (2010)" startWordPosition="1958" endWordPosition="1961"> mum approximation to the sum: (E,F,M)EB(D,q, A) P(q|D) ≈ max (E,F,M)� B(D,q) Although we have defined a generative model for translating D into q, our goal is to calculate the ranking score function over existing q and D, rather than generating new queried questions. Equation (8) cannot be used directly for document ranking because q and D are often of very different lengths, leaving many words in D unaligned to any word in q. This is the key difference between the communitybased question retrieval and the general natural language translation. As pointed out by Berger and Lafferty (1999) and Gao et al. (2010), document-query translation requires a distillation of the document, while translation of natural language tolerates little being thrown away. Thus we attempt to extract the key document words that form the distillation of the document, and assume that a queried question is translated only from the key document words. In this paper, the key document words are identified via word alignment. We introduce the “hidden alignments” A = al ... aj ... aJ, which describe the mapping from a word position j in queried question to a document word position i = aj. The different alignment models we present</context>
<context position="19147" citStr="Gao et al., 2010" startWordPosition="3146" endWordPosition="3149"> smoothing: N(t, w) P(w|t) = (17) N(t) where N(t, w) is the number of times that t is aligned to w in training data. These estimates are � Rkw,D = (1 − d) + d · Vj:(i,j)EG 657 source stuffy nose internet explorer 1 stuffy nose internet explorer 2 cold ie 3 stuffy internet browser 4 sore throat explorer 5 sneeze browser Table 2: Phrase translation probability examples. Each column shows the top 5 target phrases learned from the word-aligned question-answer pairs. useful for contextual lexical selection with sufficient training data, but can be subject to data sparsity issues (Sun et al., 2010; Gao et al., 2010). An alternate translation probability estimate not subject to data sparsity is the so-called lexical weight estimate (Koehn et al., 2003). Let P(w|t) be the word-toword translation probability, and let A be the word alignment between w and t. Here, the word alignment contains (i, j) pairs, where i E 1... |w |and j E 0 ... |t|, with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equa</context>
<context position="20828" citStr="Gao et al., 2010" startWordPosition="3433" endWordPosition="3436"> noise and sparsity. A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model. For example, the term “explorer” used alone, most likely refers to a person who engages in scientific exploration, while the phrase “internet explorer” has a very different meaning. 3.4 Ranking Candidate Historical Questions Unlike the word-based translation models, the phrase-based translation model cannot be interpolated with a unigram language model. Following (Sun et al., 2010; Gao et al., 2010), we resort to a linear ranking framework for question retrieval in which different models are incorporated as features. We consider learning a relevance function of the following general, linear form: Score(q, D) = BT · P(q, D) (19) where the feature vector Φ(q, D) is an arbitrary function that maps (q, D) to a real value, i.e., Φ(q, D) E R. 0 is the corresponding weight vector, we optimize this parameter for our evaluation metrics directly using the Powell Search algorithm (Paul et al., 1992) via cross-validation. The features used in this paper are as follows: • Phrase translation features </context>
</contexts>
<marker>Gao, He, Nie, 2010</marker>
<rawString>J. Gao and X. He and J. Nie. 2010. Clickthrough-based translation models for web search: from word models to phrase models. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W Bruce Croft</author>
<author>J H Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="2964" citStr="Jeon et al., 2005" startWordPosition="448" endWordPosition="451">tion retrieval in this paper. The major challenge for Q&amp;A retrieval, as for Query: Q1: How to get rid of stuffy nose? Expected: Q2: What is the best way to prevent a cold? Not Expected: Q3: How do I air out my stuffy room? Q4: How do you make a nose bleed stop quicker? Table 1: An example on question retrieval most information retrieval models, such as vector space model (VSM) (Salton et al., 1975), Okapi model (Robertson et al., 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al., 2005; Xue et al., 2008). For example in Table 1, Q1 and Q2 are two semantically similar questions, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the q</context>
<context position="7174" citStr="Jeon et al., 2005" startWordPosition="1095" endWordPosition="1098">experiments on community-based Q&amp;A data for question retrieval. The results show that our proposed approach significantly outperforms the baseline methods (in Section 4). The remainder of this paper is organized as follows. Section 2 introduces the existing state-of-theart methods. Section 3 describes our phrase-based translation model for question retrieval. Section 4 presents the experimental results. In Section 5, we conclude with ideas for future research. 2 Preliminaries 2.1 Language Model The unigram language model has been widely used for question retrieval on community-based Q&amp;A data (Jeon et al., 2005; Xue et al., 2008; Cao et al., 2010). To avoid zero probability, we use JelinekMercer smoothing (Zhai and Lafferty, 2001) due to its good performance and cheap computational cost. So the ranking function for the query likelihood language model with Jelinek-Mercer smoothing can be 654 D: ... for good cold home remedies ... document E: [for, good, cold, home remedies] segmentation F: [for1, best2, stuffy nose3, home remedy4] translation M: (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation q: best home remedy for stuffy nose queried question (1 − λ)P..i(w|D) + λP..l(w|C) written as: H Score(q, D) = wEq (1) P..t(w|D</context>
<context position="8414" citStr="Jeon et al. (2005)" startWordPosition="1303" endWordPosition="1306"> C) = #(w, C) (2) |D |Icl where q is the queried question, D is a document, C is background collection, A is smoothing parameter. #(t, D) is the frequency of term t in D, |D |and |C| denote the length of D and C respectively. 2.2 Word-Based Translation Model Previous work (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008) consistently reported that the wordbased translation models (Trans) yielded better performance than the traditional methods (VSM, Okapi and LM) for question retrieval. These models exploit the word translation probabilities in a language modeling framework. Following Jeon et al. (2005) and Xue et al. (2008), the ranking function can be written as: Score(q, D) = H (1−λ)Pt,(w|D)+λP..t(w|C) (3) wEq P(w|t)P..t(t|D), P..t(t|D) = #(t, D) |D| where P(w|t) denotes the translation probability from word t to word w. 2.3 Word-Based Translation Language Model Xue et al. (2008) proposed to linearly mix two different estimations by combining language model and word-based translation model into a unified framework, called TransLM. The experiments show that this model gains better performance than both the language model and the word-based translation model. Following Xue et al. (2008), th</context>
<context position="14381" citStr="Jeon et al., 2005" startWordPosition="2323" endWordPosition="2326">). We define αj to be the probability of the most likely sequence of phrases covering the first j words in a queried question, then the probability can be calculated using the following recursion: (1) Initialization: α0 = 1 (12) � 1 αj′P(w|tw) (13) P(q|D) = αJ (14) 3.2 Phrase-Based Translation Model for Question Part and Answer Part In Q&amp;A, a document D is decomposed into (q, a), where q� denotes the question part of the historical question in the archives and a� denotes the answer part. Although it has been shown that doing Q&amp;A retrieval based solely on the answer part does not perform well (Jeon et al., 2005; Xue et al., 2008), the answer part should provide additional evidence about relevance and, therefore, it should be combined with the estimation based on the question part. P(F|D, E) · P(M|D, E, F) (8) = arg max {P(J|I) HJ A j=1 (2) Induction: Eαj = j′&lt;j,w=wj′+1...wj (3) Total: 656 In this combined model, P(q|q) and P(q|a) are calculated with equations (12) to (14). So P(q|D) will be written as: P(q|D) = µ1P(q|¯q) + µ2P(q|¯a) (15) where µ1 + µ2 = 1. In equation (15), the relative importance of question part and answer part is adjusted through µ1 and µ2. When µ1 = 1, the retrieval model is bas</context>
<context position="20038" citStr="Jeon et al., 2005" startWordPosition="3307" endWordPosition="3310">ins (i, j) pairs, where i E 1... |w |and j E 0 ... |t|, with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equation (18) computes a product of per-word translation scores; the per-word scores are the averages of all the translations for the alignment links of that word. The word translation probabilities are calculated using IBM 1, which has been widely used for question retrieval (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). These wordbased scores of bi-phrases, though not as effective in contextual selection, are more robust to noise and sparsity. A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model. For example, the term “explorer” used alone, most likely refers to a person who engages in scientific exploration, while the phrase “internet explorer” has a very different meaning. 3.4 Ranking Candidate Historic</context>
<context position="26183" citStr="Jeon et al., 2005" startWordPosition="4327" endWordPosition="4330">ions (1), (3) and (5), and α = 0.8 (Xue et al., 2008) in equation (6). 4.2 Question Retrieval Results We randomly divide the test questions into five subsets and conduct 5-fold cross-validation experiments. In each trial, we tune the parameters µ1 and µ2 with four of the five subsets and then apply it to one remaining subset. The experiments reported below are those averaged over the five trials. Table 4 presents the main retrieval performance. Row 1 to row 3 are baseline systems, all these methods use word-based translation models and obtain the state-of-the-art performance in previous work (Jeon et al., 2005; Xue et al., 2008). Row 3 is similar to row 2, the only difference is that TransLM only considers the question part, while Xue et al. (2008) incorporates the question part and answer part. Row 4 and row 5 are our proposed phrase-based translation model with maximum phrase length of five. Row 4 is phrase-based translation model purely based on question part, this model is equivalent to 659 # Methods Trans Prob MAP 1 Jeon et al. (2005) Ppoot 0.289 2 TransLM Ppoot 0.324 3 Xue et al. (2008) Ppoot 0.352 4 P-Trans (µl = 1, l = 5) Ppoot 0.366 5 P-Trans (l = 5) Ppoot 0.391 Table 4: Comparison with di</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon and W. Bruce Croft and J. H. Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of CIKM, pages 84-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing order into text.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="16797" citStr="Mihalcea and Tarau, 2004" startWordPosition="2741" endWordPosition="2744">q, a)1, ... , (q, a)m} to learn P(a|q) and use the collection {(a, q)1, . . . , (a, q)m} to learn P(g|a), then {(q, a)1, ... , (q, a)m, (��, q)1, ... , (��, q)-} is used here to learn the combination translation probability Ppool(wi|tj). 3.3.2 Parallel Corpus Preprocessing Unlike the bilingual parallel corpus used in SMT, our parallel corpus is collected from Q&amp;A archives, which is more noisy. Directly using the IBM model 1 can be problematic, it is possible for translation model to contain “unnecessary” translations (Lee et al., 2008). In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004) to identify and eliminate unimportant words from parallel corpus, assuming that a word in a question or answer is unimportant if it holds a relatively low significance in the parallel corpus. Following (Lee et al., 2008), the ranking algorithm proceeds as follows. First, all the words in a given document are added as vertices in a graph G. Then edges are added between words if the words co-occur in a fixed-sized window. The number of co-occurrences becomes the weight of an edge. When the graph is constructed, the score of each vertex is initialized as 1, and the PageRankbased ranking algorith</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into text. In Proceedings of EMNLP, pages 404-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="4692" citStr="Koehn et al., 2003" startWordPosition="716" endWordPosition="719">dent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 with a relative high translation probability. In this paper, we argue that it is beneficial to capture contextual information for question retrieval. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrasebased translation model (P-Trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level. This model learns the probability of translating one sequence of words (e.g., phrase) into another sequence of words, e.g., translating a phrase in a historical question into another phrase in a queried question. Compared to the traditional word-based translation models that account for translating single words in isolation, the phrase-based translation model is potentially more effective because it captures some conte</context>
<context position="9405" citStr="Koehn et al., 2003" startWordPosition="1450" endWordPosition="1453">-based translation model into a unified framework, called TransLM. The experiments show that this model gains better performance than both the language model and the word-based translation model. Following Xue et al. (2008), this model can be written as: Score(q, D) = H (1 − λ)P...,(w|D) + λP..l(w|C) wEq EP...,(w|D) = α P(w|t)P..1(t|D)+(1−α)P..1(w|D) tED (6) Figure 1: Example describing the generative procedure of the phrase-based translation model. 3 Our Approach: Phrase-Based Translation Model for Question Retrieval 3.1 Phrase-Based Translation Model Phrase-based machine translation models (Koehn et al., 2003; D. Chiang, 2005; Och and Ney, 2004) have shown superior performance compared to word-based translation models. In this paper, the goal of phrase-based translation model is to translate a document4 D into a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, we might learn that the phrase “stuffy nose” can be translated from “cold” with relative high probability, even though neither of the individual word pairs (e.g., “stuffy”/“cold</context>
<context position="17972" citStr="Koehn et al., 2003" startWordPosition="2950" endWordPosition="2953">s 1, and the PageRankbased ranking algorithm is run on the graph iteratively until convergence. The TextRank score of a word w in document D at kth iteration is defined as follows: ei,j �Rk−1 w,D Vl:(j,l)EG ej,l (16) where d is a damping factor usually set to 0.85, and ei,j is an edge weight between i and j. We use average TextRank score as threshold: words are removed if their scores are lower than the average score of all words in a document. 3.3.3 Translation Probability Estimation After preprocessing the parallel corpus, we will calculate P(w|t), following the method commonly used in SMT (Koehn et al., 2003; Och, 2002) to extract bi-phrases and estimate their translation probabilities. First, we learn the word-to-word translation probability using IBM model 1 (Brown et al., 1993). Then, we perform Viterbi word alignment according to equation (9). Finally, the bi-phrases that are consistent with the word alignment are extracted using the heuristics proposed in (Och, 2002). We set the maximum phrase length to five in our experiments. After gathering all such bi-phrases from the training data, we can estimate conditional relative frequency estimates without smoothing: N(t, w) P(w|t) = (17) N(t) whe</context>
<context position="19285" citStr="Koehn et al., 2003" startWordPosition="3167" endWordPosition="3170"> � Rkw,D = (1 − d) + d · Vj:(i,j)EG 657 source stuffy nose internet explorer 1 stuffy nose internet explorer 2 cold ie 3 stuffy internet browser 4 sore throat explorer 5 sneeze browser Table 2: Phrase translation probability examples. Each column shows the top 5 target phrases learned from the word-aligned question-answer pairs. useful for contextual lexical selection with sufficient training data, but can be subject to data sparsity issues (Sun et al., 2010; Gao et al., 2010). An alternate translation probability estimate not subject to data sparsity is the so-called lexical weight estimate (Koehn et al., 2003). Let P(w|t) be the word-toword translation probability, and let A be the word alignment between w and t. Here, the word alignment contains (i, j) pairs, where i E 1... |w |and j E 0 ... |t|, with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equation (18) computes a product of per-word translation scores; the per-word scores are the averages of all the translations for the alignmen</context>
<context position="22712" citStr="Koehn et al., 2003" startWordPosition="3762" endWordPosition="3765">ΦILW (D, q, A) = logP(D|q), here P(D|q) is computed by equations (12) to (15) except that we set µ2 = 0 in equation (15), and the phrase translation probability is computed as lexical weight according to equation (18). • Phrase alignment features (PA): ΦPA(q, D, B) = E2 |ak − bk−1 − 1|, where B is a set of K bi-phrases, ak is the start position of the phrase in D that was translated 658 into the kth phrase in queried question, and bk_1 is the end position of the phrase in D that was translated into the (k − 1)th phrase in queried question. The feature, inspired by the distortion model in SMT (Koehn et al., 2003), models the degree to which the queried phrases are reordered. For all possible B, we only compute the feature value according to the Viterbi alignment, B� = arg maxB P(q, B|D). We find B� using the Viterbi algorithm, which is almost identical to the dynamic programming recursion of equations (12) to (14), except that the sum operator in equation (13) is replaced with the max operator. • Unaligned word penalty features (UWP): 4)UWP(q, D), which is defined as the ratio between the number of unaligned words and the total number of words in queried questions. • Language model features (LM): 4)LM</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn and F. Och and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J -T Lee</author>
<author>S -B Kim</author>
<author>Y -I Song</author>
<author>H -C Rim</author>
</authors>
<title>Bridging lexical gaps between queries and questions on large online Q&amp;A collections with compact translation models.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>410--418</pages>
<contexts>
<context position="3807" citStr="Lee et al., 2008" startWordPosition="582" endWordPosition="585">guistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from</context>
<context position="16713" citStr="Lee et al., 2008" startWordPosition="2726" endWordPosition="2729">arn the combined translation probabilities. Suppose we use the collection {(q, a)1, ... , (q, a)m} to learn P(a|q) and use the collection {(a, q)1, . . . , (a, q)m} to learn P(g|a), then {(q, a)1, ... , (q, a)m, (��, q)1, ... , (��, q)-} is used here to learn the combination translation probability Ppool(wi|tj). 3.3.2 Parallel Corpus Preprocessing Unlike the bilingual parallel corpus used in SMT, our parallel corpus is collected from Q&amp;A archives, which is more noisy. Directly using the IBM model 1 can be problematic, it is possible for translation model to contain “unnecessary” translations (Lee et al., 2008). In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004) to identify and eliminate unimportant words from parallel corpus, assuming that a word in a question or answer is unimportant if it holds a relatively low significance in the parallel corpus. Following (Lee et al., 2008), the ranking algorithm proceeds as follows. First, all the words in a given document are added as vertices in a graph G. Then edges are added between words if the words co-occur in a fixed-sized window. The number of co-occurrences becomes the weight of an edge. When the graph is constructed, </context>
<context position="20074" citStr="Lee et al., 2008" startWordPosition="3315" endWordPosition="3318">and j E 0 ... |t|, with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equation (18) computes a product of per-word translation scores; the per-word scores are the averages of all the translations for the alignment links of that word. The word translation probabilities are calculated using IBM 1, which has been widely used for question retrieval (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). These wordbased scores of bi-phrases, though not as effective in contextual selection, are more robust to noise and sparsity. A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model. For example, the term “explorer” used alone, most likely refers to a person who engages in scientific exploration, while the phrase “internet explorer” has a very different meaning. 3.4 Ranking Candidate Historical Questions Unlike the word-based t</context>
<context position="29804" citStr="Lee et al. (2008)" startWordPosition="4929" endWordPosition="4932"> model by using TextRank algorithm. This kind of “unnecessary” translation between words will eventually affect the bi-phrase translation. Table 6 shows the effectiveness of parallel corpus preprocessing. Row 11 reports the average number of translations per word and the question retrieval performance when only stopwords 7 are removed. When using the TextRank algorithm for parallel corpus preprocessing, the average number of translations per word is reduced from 69 to 24, but the performance of question retrieval is significantly improved (row 11 vs. row 12). Similar results have been made by Lee et al. (2008). 4.5 Impact of Pooling Strategy The correspondence of words or phrases in the question-answer pair is not as strong as in the bilingual sentence pair, thus noise will be inevitably introduced for both P(ajq) and P(qja). To see how much the pooling strategy benefit the question retrieval, we introduce two baseline methods for comparison. The first method (denoted as P(ajq)) is used to denote the translation probability with the question as the source and the answer as 7http://truereader.com/manuals/onix/stopwords1.html 660 Model # Trans Prob MAP 13 P(dlq) 0.387 P-Trans (l = 5) 14 P(qja) 0.381 </context>
</contexts>
<marker>Lee, Kim, Song, Rim, 2008</marker>
<rawString>J. -T. Lee and S. -B. Kim and Y. -I. Song and H. -C. Rim. 2008. Bridging lexical gaps between queries and questions on large online Q&amp;A collections with compact translation models. In Proceedings of EMNLP, pages 410-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical mahcine translation: from single word models to alignment templates.</title>
<date>2002</date>
<tech>Ph.D thesis,</tech>
<institution>RWTH Aachen.</institution>
<contexts>
<context position="13765" citStr="Och, 2002" startWordPosition="2215" endWordPosition="2216">=1 Given A, when scoring a given Q&amp;A pair, we restrict our attention to those E, F, M triples that are For the sole remaining factor P(F|D, E), we make the assumption that a segmented queried question F = w1, ... , wK is generated from left to right by translating each phrase ti,... , tK independently: K P(F|D, E) = H P(wk|tk) (11) k=1 where P(wk|tk) is a phrase translation probability, the estimation will be described in Section 3.3. To find the maximum probability assignment efficiently, we use a dynamic programming approach, somewhat similar to the monotone decoding algorithm described in (Och, 2002). We define αj to be the probability of the most likely sequence of phrases covering the first j words in a queried question, then the probability can be calculated using the following recursion: (1) Initialization: α0 = 1 (12) � 1 αj′P(w|tw) (13) P(q|D) = αJ (14) 3.2 Phrase-Based Translation Model for Question Part and Answer Part In Q&amp;A, a document D is decomposed into (q, a), where q� denotes the question part of the historical question in the archives and a� denotes the answer part. Although it has been shown that doing Q&amp;A retrieval based solely on the answer part does not perform well (J</context>
<context position="17984" citStr="Och, 2002" startWordPosition="2954" endWordPosition="2955">kbased ranking algorithm is run on the graph iteratively until convergence. The TextRank score of a word w in document D at kth iteration is defined as follows: ei,j �Rk−1 w,D Vl:(j,l)EG ej,l (16) where d is a damping factor usually set to 0.85, and ei,j is an edge weight between i and j. We use average TextRank score as threshold: words are removed if their scores are lower than the average score of all words in a document. 3.3.3 Translation Probability Estimation After preprocessing the parallel corpus, we will calculate P(w|t), following the method commonly used in SMT (Koehn et al., 2003; Och, 2002) to extract bi-phrases and estimate their translation probabilities. First, we learn the word-to-word translation probability using IBM model 1 (Brown et al., 1993). Then, we perform Viterbi word alignment according to equation (9). Finally, the bi-phrases that are consistent with the word alignment are extracted using the heuristics proposed in (Och, 2002). We set the maximum phrase length to five in our experiments. After gathering all such bi-phrases from the training data, we can estimate conditional relative frequency estimates without smoothing: N(t, w) P(w|t) = (17) N(t) where N(t, w) i</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>F. Och. 2002. Statistical mahcine translation: from single word models to alignment templates. Ph.D thesis, RWTH Aachen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="4712" citStr="Och and Ney, 2004" startWordPosition="720" endWordPosition="723"> not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 with a relative high translation probability. In this paper, we argue that it is beneficial to capture contextual information for question retrieval. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrasebased translation model (P-Trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level. This model learns the probability of translating one sequence of words (e.g., phrase) into another sequence of words, e.g., translating a phrase in a historical question into another phrase in a queried question. Compared to the traditional word-based translation models that account for translating single words in isolation, the phrase-based translation model is potentially more effective because it captures some contextual information in</context>
<context position="9442" citStr="Och and Ney, 2004" startWordPosition="1457" endWordPosition="1460">ed framework, called TransLM. The experiments show that this model gains better performance than both the language model and the word-based translation model. Following Xue et al. (2008), this model can be written as: Score(q, D) = H (1 − λ)P...,(w|D) + λP..l(w|C) wEq EP...,(w|D) = α P(w|t)P..1(t|D)+(1−α)P..1(w|D) tED (6) Figure 1: Example describing the generative procedure of the phrase-based translation model. 3 Our Approach: Phrase-Based Translation Model for Question Retrieval 3.1 Phrase-Based Translation Model Phrase-based machine translation models (Koehn et al., 2003; D. Chiang, 2005; Och and Ney, 2004) have shown superior performance compared to word-based translation models. In this paper, the goal of phrase-based translation model is to translate a document4 D into a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, we might learn that the phrase “stuffy nose” can be translated from “cold” with relative high probability, even though neither of the individual word pairs (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a hig</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2831" citStr="Ponte and Croft, 1998" startWordPosition="426" endWordPosition="429">l then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1. This is what we called question retrieval in this paper. The major challenge for Q&amp;A retrieval, as for Query: Q1: How to get rid of stuffy nose? Expected: Q2: What is the best way to prevent a cold? Not Expected: Q3: How do I air out my stuffy room? Q4: How do you make a nose bleed stop quicker? Table 1: An example on question retrieval most information retrieval models, such as vector space model (VSM) (Salton et al., 1975), Okapi model (Robertson et al., 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al., 2005; Xue et al., 2008). For example in Table 1, Q1 and Q2 are two semantically similar questions, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J. M. Ponte and W. B. Croft. 1998. A language modeling approach to information retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical Recipes In C.</title>
<date>1992</date>
<publisher>Cambridge Univ. Press.</publisher>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>W. H. Press and S. A. Teukolsky and W. T. Vetterling and B. P. Flannery. 1992. Numerical Recipes In C. Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M Hancock-Beaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at trec-3.</title>
<date>1994</date>
<booktitle>In Proceedings of TREC,</booktitle>
<pages>109--126</pages>
<contexts>
<context position="2786" citStr="Robertson et al., 1994" startWordPosition="419" endWordPosition="422">n Q1, Q2 can be returned and their answers will then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1. This is what we called question retrieval in this paper. The major challenge for Q&amp;A retrieval, as for Query: Q1: How to get rid of stuffy nose? Expected: Q2: What is the best way to prevent a cold? Not Expected: Q3: How do I air out my stuffy room? Q4: How do you make a nose bleed stop quicker? Table 1: An example on question retrieval most information retrieval models, such as vector space model (VSM) (Salton et al., 1975), Okapi model (Robertson et al., 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al., 2005; Xue et al., 2008). For example in Table 1, Q1 and Q2 are two semantically similar questions, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually sh</context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1994</marker>
<rawString>S. Robertson and S. Walker and S. Jones and M. Hancock-Beaulieu and M. Gatford. 1994. Okapi at trec-3. In Proceedings of TREC, pages 109-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<pages>18--11</pages>
<contexts>
<context position="2748" citStr="Salton et al., 1975" startWordPosition="413" endWordPosition="416">r example in Table 1, given question Q1, Q2 can be returned and their answers will then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1. This is what we called question retrieval in this paper. The major challenge for Q&amp;A retrieval, as for Query: Q1: How to get rid of stuffy nose? Expected: Q2: What is the best way to prevent a cold? Not Expected: Q3: How do I air out my stuffy room? Q4: How do you make a nose bleed stop quicker? Table 1: An example on question retrieval most information retrieval models, such as vector space model (VSM) (Salton et al., 1975), Okapi model (Robertson et al., 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al., 2005; Xue et al., 2008). For example in Table 1, Q1 and Q2 are two semantically similar questions, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since th</context>
<context position="24537" citStr="Salton et al., 1975" startWordPosition="4045" endWordPosition="4048">ernet”. The resulting question repository that we use for question retrieval contains 518,492 questions. To learn the translation probabilities, we use about one million question-answer pairs from another data set.6 In order to create the test set, we randomly select 300 questions for this category, denoted as 5http://developer.yahoo.com/answers 6The Yahoo! Webscope dataset Yahoo answers comprehensive questions and answers version 1.0.2, available at http://reseach.yahoo.com/Academic Relations. “CI TST”. To obtain the ground-truth of question retrieval, we employ the Vector Space Model (VSM) (Salton et al., 1975) to retrieve the top 20 results and obtain manual judgements. The top 20 results don’t include the queried question itself. Given a returned result by VSM, an annotator is asked to label it with “relevant” or “irrelevant”. If a returned result is considered semantically equivalent to the queried question, the annotator will label it as “relevant”; otherwise, the annotator will label it as “irrelevant”. Two annotators are involved in the annotation process. If a conflict happens, a third person will make judgement for the final result. In the process of manually judging questions, the annotator</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton and A. Wong and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613-620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>J Gao</author>
<author>D Micol</author>
<author>C Quirk</author>
</authors>
<title>Learning phrase-based spelling error models from clickthrough data.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10115" citStr="Sun et al., 2010" startWordPosition="1560" endWordPosition="1563">ranslation models. In this paper, the goal of phrase-based translation model is to translate a document4 D into a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, we might learn that the phrase “stuffy nose” can be translated from “cold” with relative high probability, even though neither of the individual word pairs (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high word translation probability. Inspired by the work of (Sun et al., 2010; Gao et al., 2010), we assume the following generative process: first the document D is broken into K non-empty word sequences t1, ... , tK, then each t is translated into a new non-empty word sequence w1, ... , wK, and finally these phrases are permutated and concatenated to form the queried questions q, where t and w denote the phrases or consecutive sequence of words. To formulate this generative process, let E denote the segmentation of D into K phrases t1, ... , tK, and let F denote the K translation phrases w1, ... , wK −we refer to these (ti, wi) pairs as bi-phrases. Finally, let M den</context>
<context position="19128" citStr="Sun et al., 2010" startWordPosition="3142" endWordPosition="3145"> estimates without smoothing: N(t, w) P(w|t) = (17) N(t) where N(t, w) is the number of times that t is aligned to w in training data. These estimates are � Rkw,D = (1 − d) + d · Vj:(i,j)EG 657 source stuffy nose internet explorer 1 stuffy nose internet explorer 2 cold ie 3 stuffy internet browser 4 sore throat explorer 5 sneeze browser Table 2: Phrase translation probability examples. Each column shows the top 5 target phrases learned from the word-aligned question-answer pairs. useful for contextual lexical selection with sufficient training data, but can be subject to data sparsity issues (Sun et al., 2010; Gao et al., 2010). An alternate translation probability estimate not subject to data sparsity is the so-called lexical weight estimate (Koehn et al., 2003). Let P(w|t) be the word-toword translation probability, and let A be the word alignment between w and t. Here, the word alignment contains (i, j) pairs, where i E 1... |w |and j E 0 ... |t|, with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions </context>
<context position="20809" citStr="Sun et al., 2010" startWordPosition="3429" endWordPosition="3432">are more robust to noise and sparsity. A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model. For example, the term “explorer” used alone, most likely refers to a person who engages in scientific exploration, while the phrase “internet explorer” has a very different meaning. 3.4 Ranking Candidate Historical Questions Unlike the word-based translation models, the phrase-based translation model cannot be interpolated with a unigram language model. Following (Sun et al., 2010; Gao et al., 2010), we resort to a linear ranking framework for question retrieval in which different models are incorporated as features. We consider learning a relevance function of the following general, linear form: Score(q, D) = BT · P(q, D) (19) where the feature vector Φ(q, D) is an arbitrary function that maps (q, D) to a real value, i.e., Φ(q, D) E R. 0 is the corresponding weight vector, we optimize this parameter for our evaluation metrics directly using the Powell Search algorithm (Paul et al., 1992) via cross-validation. The features used in this paper are as follows: • Phrase tr</context>
</contexts>
<marker>Sun, Gao, Micol, Quirk, 2010</marker>
<rawString>X. Sun and J. Gao and D. Micol and C. Quirk. 2010. Learning phrase-based spelling error models from clickthrough data. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wang</author>
<author>Z Ming</author>
<author>T-S Chua</author>
</authors>
<title>A syntactic tree matching approach to finding similar questions in community-based qa services.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>187--194</pages>
<marker>Wang, Ming, Chua, 2009</marker>
<rawString>K. Wang and Z. Ming and T-S. Chua. 2009. A syntactic tree matching approach to finding similar questions in community-based qa services. In Proceedings of SIGIR, pages 187-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xue</author>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="2983" citStr="Xue et al., 2008" startWordPosition="452" endWordPosition="455">his paper. The major challenge for Q&amp;A retrieval, as for Query: Q1: How to get rid of stuffy nose? Expected: Q2: What is the best way to prevent a cold? Not Expected: Q3: How do I air out my stuffy room? Q4: How do you make a nose bleed stop quicker? Table 1: An example on question retrieval most information retrieval models, such as vector space model (VSM) (Salton et al., 1975), Okapi model (Robertson et al., 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al., 2005; Xue et al., 2008). For example in Table 1, Q1 and Q2 are two semantically similar questions, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics lem is more serious for Q&amp;A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval t</context>
<context position="7192" citStr="Xue et al., 2008" startWordPosition="1099" endWordPosition="1102">unity-based Q&amp;A data for question retrieval. The results show that our proposed approach significantly outperforms the baseline methods (in Section 4). The remainder of this paper is organized as follows. Section 2 introduces the existing state-of-theart methods. Section 3 describes our phrase-based translation model for question retrieval. Section 4 presents the experimental results. In Section 5, we conclude with ideas for future research. 2 Preliminaries 2.1 Language Model The unigram language model has been widely used for question retrieval on community-based Q&amp;A data (Jeon et al., 2005; Xue et al., 2008; Cao et al., 2010). To avoid zero probability, we use JelinekMercer smoothing (Zhai and Lafferty, 2001) due to its good performance and cheap computational cost. So the ranking function for the query likelihood language model with Jelinek-Mercer smoothing can be 654 D: ... for good cold home remedies ... document E: [for, good, cold, home remedies] segmentation F: [for1, best2, stuffy nose3, home remedy4] translation M: (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation q: best home remedy for stuffy nose queried question (1 − λ)P..i(w|D) + λP..l(w|C) written as: H Score(q, D) = wEq (1) P..t(w|D) = #(w, D) , P,n,</context>
<context position="8436" citStr="Xue et al. (2008)" startWordPosition="1308" endWordPosition="1311">cl where q is the queried question, D is a document, C is background collection, A is smoothing parameter. #(t, D) is the frequency of term t in D, |D |and |C| denote the length of D and C respectively. 2.2 Word-Based Translation Model Previous work (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008) consistently reported that the wordbased translation models (Trans) yielded better performance than the traditional methods (VSM, Okapi and LM) for question retrieval. These models exploit the word translation probabilities in a language modeling framework. Following Jeon et al. (2005) and Xue et al. (2008), the ranking function can be written as: Score(q, D) = H (1−λ)Pt,(w|D)+λP..t(w|C) (3) wEq P(w|t)P..t(t|D), P..t(t|D) = #(t, D) |D| where P(w|t) denotes the translation probability from word t to word w. 2.3 Word-Based Translation Language Model Xue et al. (2008) proposed to linearly mix two different estimations by combining language model and word-based translation model into a unified framework, called TransLM. The experiments show that this model gains better performance than both the language model and the word-based translation model. Following Xue et al. (2008), this model can be writte</context>
<context position="14400" citStr="Xue et al., 2008" startWordPosition="2327" endWordPosition="2330">be the probability of the most likely sequence of phrases covering the first j words in a queried question, then the probability can be calculated using the following recursion: (1) Initialization: α0 = 1 (12) � 1 αj′P(w|tw) (13) P(q|D) = αJ (14) 3.2 Phrase-Based Translation Model for Question Part and Answer Part In Q&amp;A, a document D is decomposed into (q, a), where q� denotes the question part of the historical question in the archives and a� denotes the answer part. Although it has been shown that doing Q&amp;A retrieval based solely on the answer part does not perform well (Jeon et al., 2005; Xue et al., 2008), the answer part should provide additional evidence about relevance and, therefore, it should be combined with the estimation based on the question part. P(F|D, E) · P(M|D, E, F) (8) = arg max {P(J|I) HJ A j=1 (2) Induction: Eαj = j′&lt;j,w=wj′+1...wj (3) Total: 656 In this combined model, P(q|q) and P(q|a) are calculated with equations (12) to (14). So P(q|D) will be written as: P(q|D) = µ1P(q|¯q) + µ2P(q|¯a) (15) where µ1 + µ2 = 1. In equation (15), the relative importance of question part and answer part is adjusted through µ1 and µ2. When µ1 = 1, the retrieval model is based on phrase-based </context>
<context position="15898" citStr="Xue et al. (2008)" startWordPosition="2587" endWordPosition="2590">, which is used for estimating the translation probabilities. Unlike the bilingual machine translation, the questions and answers in a Q&amp;A archive are written in the same language, the translation probability can be calculated through setting either as the source and the other as the target. In this paper, P(a|q) is used to denote the translation probability with the question as the source and the answer as the target. P(g|a) is used to denote the opposite configuration. For a given word or phrase, the related words or phrases differ when it appears in the question or in the answer. Following Xue et al. (2008), a pooling strategy is adopted. First, we pool the question-answer pairs used to learn P(a|q) and the answer-question pairs used to learn P(g|a), and then use IBM model 1 (Brown et al., 1993) to learn the combined translation probabilities. Suppose we use the collection {(q, a)1, ... , (q, a)m} to learn P(a|q) and use the collection {(a, q)1, . . . , (a, q)m} to learn P(g|a), then {(q, a)1, ... , (q, a)m, (��, q)1, ... , (��, q)-} is used here to learn the combination translation probability Ppool(wi|tj). 3.3.2 Parallel Corpus Preprocessing Unlike the bilingual parallel corpus used in SMT, ou</context>
<context position="20056" citStr="Xue et al., 2008" startWordPosition="3311" endWordPosition="3314">here i E 1... |w |and j E 0 ... |t|, with 0 indicating a null word. Then we use the following estimate: Pt(wlt, A) = � |W |14) ,= All E P(wiltj) i�� V(i,j)EA (18) We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equation (18) computes a product of per-word translation scores; the per-word scores are the averages of all the translations for the alignment links of that word. The word translation probabilities are calculated using IBM 1, which has been widely used for question retrieval (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). These wordbased scores of bi-phrases, though not as effective in contextual selection, are more robust to noise and sparsity. A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model. For example, the term “explorer” used alone, most likely refers to a person who engages in scientific exploration, while the phrase “internet explorer” has a very different meaning. 3.4 Ranking Candidate Historical Questions Unlik</context>
<context position="25619" citStr="Xue et al., 2008" startWordPosition="4234" endWordPosition="4237"> conflict happens, a third person will make judgement for the final result. In the process of manually judging questions, the annotators are presented only the questions. Table 3 provides the statistics on the final test set. #queries #returned #relevant CI TST 300 6,000 798 Table 3: Statistics on the Test Data We evaluate the performance of our approach using Mean Average Precision (MAP). We perform a significant test, i.e., a t-test with a default significant level of 0.05. Following the literature, we set the parameters A = 0.2 (Cao et al., 2010) in equations (1), (3) and (5), and α = 0.8 (Xue et al., 2008) in equation (6). 4.2 Question Retrieval Results We randomly divide the test questions into five subsets and conduct 5-fold cross-validation experiments. In each trial, we tune the parameters µ1 and µ2 with four of the five subsets and then apply it to one remaining subset. The experiments reported below are those averaged over the five trials. Table 4 presents the main retrieval performance. Row 1 to row 3 are baseline systems, all these methods use word-based translation models and obtain the state-of-the-art performance in previous work (Jeon et al., 2005; Xue et al., 2008). Row 3 is simila</context>
<context position="27457" citStr="Xue et al. (2008)" startWordPosition="4550" endWordPosition="4553"> equation (15). Row 5 is the phrasebased combination model which linearly combines the question part and answer part. As expected, different parts can play different roles: a phrase to be translated in queried questions may be translated from the question part or answer part. All these methods use pooling strategy to estimate the translation probabilities. There are some clear trends in the result of Table 4: (1) Word-based translation language model (TransLM) significantly outperforms word-based translation model of Jeon et al. (2005) (row 1 vs. row 2). Similar observations have been made by Xue et al. (2008). (2) Incorporating the answer part into the models, either word-based or phrase-based, can significantly improve the performance of question retrieval (row 2 vs. row 3; row 4 vs. row 5). (3) Our proposed phrase-based translation model (P-Trans) significantly outperforms the state-of-theart word-based translation models (row 2 vs. row 4 and row 3 vs. row 5, all these comparisons are statistically significant at p &lt; 0.05). 4.3 Impact of Phrase Length Our proposed phrase-based translation model, due to its capability of capturing contextual information, is more effective than the state-of-the-ar</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>X. Xue and J. Jeon and W. B. Croft. 2008. Retrieval models for question and answer archives. In Proceedings of SIGIR, pages 475-482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A study of smooth methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="7296" citStr="Zhai and Lafferty, 2001" startWordPosition="1116" endWordPosition="1119">cantly outperforms the baseline methods (in Section 4). The remainder of this paper is organized as follows. Section 2 introduces the existing state-of-theart methods. Section 3 describes our phrase-based translation model for question retrieval. Section 4 presents the experimental results. In Section 5, we conclude with ideas for future research. 2 Preliminaries 2.1 Language Model The unigram language model has been widely used for question retrieval on community-based Q&amp;A data (Jeon et al., 2005; Xue et al., 2008; Cao et al., 2010). To avoid zero probability, we use JelinekMercer smoothing (Zhai and Lafferty, 2001) due to its good performance and cheap computational cost. So the ranking function for the query likelihood language model with Jelinek-Mercer smoothing can be 654 D: ... for good cold home remedies ... document E: [for, good, cold, home remedies] segmentation F: [for1, best2, stuffy nose3, home remedy4] translation M: (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation q: best home remedy for stuffy nose queried question (1 − λ)P..i(w|D) + λP..l(w|C) written as: H Score(q, D) = wEq (1) P..t(w|D) = #(w, D) , P,n,t(wl C) = #(w, C) (2) |D |Icl where q is the queried question, D is a document, C is background collecti</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty. 2001. A study of smooth methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334-342.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>