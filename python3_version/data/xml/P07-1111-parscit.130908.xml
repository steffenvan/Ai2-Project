<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021442">
<title confidence="0.998887">
A Re-examination of Machine Learning Approaches
for Sentence-Level MT Evaluation
</title>
<author confidence="0.980934">
Joshua S. Albrecht and Rebecca Hwa
</author>
<affiliation confidence="0.9983405">
Department of Computer Science
University of Pittsburgh
</affiliation>
<email confidence="0.99922">
{jsa8,hwa}@cs.pitt.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852411764706">
Recent studies suggest that machine learn-
ing can be applied to develop good auto-
matic evaluation metrics for machine trans-
lated sentences. This paper further ana-
lyzes aspects of learning that impact per-
formance. We argue that previously pro-
posed approaches of training a Human-
Likeness classifier is not as well correlated
with human judgments of translation qual-
ity, but that regression-based learning pro-
duces more reliable metrics. We demon-
strate the feasibility of regression-based
metrics through empirical analysis of learn-
ing curves and generalization studies and
show that they can achieve higher correla-
tions with human judgments than standard
automatic metrics.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999991270833334">
As machine translation (MT) research advances, the
importance of its evaluation also grows. Efficient
evaluation methodologies are needed both for facili-
tating the system development cycle and for provid-
ing an unbiased comparison between systems. To
this end, a number of automatic evaluation metrics
have been proposed to approximate human judg-
ments of MT output quality. Although studies have
shown them to correlate with human judgments at
the document level, they are not sensitive enough
to provide reliable evaluations at the sentence level
(Blatz et al., 2003). This suggests that current met-
rics do not fully reflect the set of criteria that people
use in judging sentential translation quality.
A recent direction in the development of met-
rics for sentence-level evaluation is to apply ma-
chine learning to create an improved composite met-
ric out of less indicative ones (Corston-Oliver et al.,
2001; Kulesza and Shieber, 2004). Under the as-
sumption that good machine translation will pro-
duce “human-like” sentences, classifiers are trained
to predict whether a sentence is authored by a human
or by a machine based on features of that sentence,
which may be the sentence’s scores from individ-
ual automatic evaluation metrics. The confidence of
the classifier’s prediction can then be interpreted as a
judgment on the translation quality of the sentence.
Thus, the composite metric is encoded in the confi-
dence scores of the classification labels.
While the learning approach to metric design of-
fers the promise of ease of combining multiple met-
rics and the potential for improved performance,
several salient questions should be addressed more
fully. First, is learning a “Human Likeness” classi-
fier the most suitable approach for framing the MT-
evaluation question? An alternative is regression, in
which the composite metric is explicitly learned as
a function that approximates humans’ quantitative
judgments, based on a set of human evaluated train-
ing sentences. Although regression has been con-
sidered on a small scale for a single system as con-
fidence estimation (Quirk, 2004), this approach has
not been studied as extensively due to scalability and
generalization concerns. Second, how does the di-
versity of the model features impact the learned met-
ric? Third, how well do learning-based metrics gen-
eralize beyond their training examples? In particu-
lar, how well can a metric that was developed based
</bodyText>
<page confidence="0.961262">
880
</page>
<note confidence="0.925387">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880–887,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999008171052632">
on one group of MT systems evaluate the translation MT developers need a way to evaluate them. One
qualities of new systems? possibility is to examine whether the automatic met-
In this paper, we argue for the viability of a ric ranks the human reference translations highly
regression-based framework for sentence-level MT- with respect to machine translations (Lin and Och,
evaluation. Through empirical studies, we first 2004b; Amig´o et al., 2006). The reliability of a
show that having an accurate Human-Likeness clas- metric can also be more directly assessed by de-
sifier does not necessarily imply having a good MT- termining how well it correlates with human judg-
evaluation metric. Second, we analyze the resource ments of the same data. For instance, as a part of the
requirement for regression models for different sizes recent NIST sponsored MT Evaluation, each trans-
of feature sets through learning curves. Finally, we lated sentence by participating systems is evaluated
show that SVM-regression metrics generalize better by two (non-reference) human judges on a five point
than SVM-classification metrics in their evaluation scale for its adequacy (does the translation retain the
of systems that are different from those in the train- meaning of the original source text?) and fluency
ing set (by languages and by years), and their corre- (does the translation sound natural in the target lan-
lations with human assessment are higher than stan- guage?). These human assessment data are an in-
dard automatic evaluation metrics. valuable resource for measuring the reliability of au-
2 MT Evaluation tomatic evaluation metrics. In this paper, we show
Recent automatic evaluation metrics typically frame that they are also informative in developing better
the evaluation problem as a comparison task: how metrics.
similar is the machine-produced output to a set of 3 MT Evaluation with Machine Learning
human-produced reference translations for the same A good automatic evaluation metric can be seen as
source text? However, as the notion of similar- a computational model that captures a human’s de-
ity is itself underspecified, several different fami- cision process in making judgments about the ade-
lies of metrics have been developed. First, simi- quacy and fluency of translation outputs. Inferring a
larity can be expressed in terms of string edit dis- cognitive model of human judgments is a challeng-
tances. In addition to the well-known word error ing problem because the ultimate judgment encom-
rate (WER), more sophisticated modifications have passes a multitude of fine-grained decisions, and the
been proposed (Tillmann et al., 1997; Snover et decision process may differ slightly from person to
al., 2006; Leusch et al., 2006). Second, similar- person. The metrics cited in the previous section
ity can be expressed in terms of common word se- aim to capture certain aspects of human judgments.
quences. Since the introduction of BLEU (Papineni One way to combine these metrics in a uniform and
et al., 2002) the basic n-gram precision idea has principled manner is through a learning framework.
been augmented in a number of ways. Metrics in the The individual metrics participate as input features,
Rouge family allow for skip n-grams (Lin and Och, from which the learning algorithm infers a compos-
2004a); Kauchak and Barzilay (2006) take para- ite metric that is optimized on training examples.
phrasing into account; metrics such as METEOR Reframing sentence-level translation evaluation
(Banerjee and Lavie, 2005) and GTM (Melamed et as a classification task was first proposed by
al., 2003) calculate both recall and precision; ME- Corston-Oliver et al. (2001). Interestingly, instead
TEOR is also similar to SIA (Liu and Gildea, 2006) of recasting the classification problem as a “Hu-
in that word class information is used. Finally, re- man Acceptability” test (distinguishing good trans-
searchers have begun to look for similarities at a lations outputs from bad one), they chose to develop
deeper structural level. For example, Liu and Gildea a Human-Likeness classifier (distinguishing out-
(2005) developed the Sub-Tree Metric (STM) over puts seem human-produced from machine-produced
constituent parse trees and the Head-Word Chain ones) to avoid the necessity of obtaining manu-
Metric (HWCM) over dependency parse trees. ally labeled training examples. Later, Kulesza and
With this wide array of metrics to choose from, Shieber (2004) noted that if a classifier provides a
881
confidence score for its output, that value can be
interpreted as a quantitative estimate of the input
instance’s translation quality. In particular, they
trained an SVM classifier that makes its decisions
based on a set of input features computed from the
sentence to be evaluated; the distance between input
feature vector and the separating hyperplane then
serves as the evaluation score. The underlying as-
sumption for both is that improving the accuracy of
the classifier on the Human-Likeness test will also
improve the implicit MT evaluation metric.
A more direct alternative to the classification ap-
proach is to learn via regression and explicitly op-
timize for a function (i.e. MT evaluation metric)
that approximates human judgments in training ex-
amples. Kulesza and Shieber (2004) raised two
main objections against regression for MT evalua-
tions. One is that regression requires a large set of
labeled training examples. Another is that regression
may not generalize well over time, and re-training
may become necessary, which would require col-
lecting additional human assessment data. While
these are legitimate concerns, we show through em-
pirical studies (in Section 4.2) that the additional re-
source requirement is not impractically high, and
that a regression-based metric has higher correla-
tions with human judgments and generalizes better
than a metric derived from a Human-Likeness clas-
sifier.
</bodyText>
<sectionHeader confidence="0.7511235" genericHeader="method">
3.1 Relationship between Classification and
Regression
</sectionHeader>
<bodyText confidence="0.999895263157895">
Classification and regression are both processes of
function approximation; they use training examples
as sample instances to learn the mapping from in-
puts to the desired outputs. The major difference be-
tween classification and regression is that the func-
tion learned by a classifier is a set of decision bound-
aries by which to classify its inputs; thus its outputs
are discrete. In contrast, a regression model learns
a continuous function that directly maps an input
to a continuous value. An MT evaluation metric is
inherently a continuous function. Casting the task
as a 2-way classification may be too coarse-grained.
The Human-Likeness formulation of the problem in-
troduces another layer of approximation by assum-
ing equivalence between “Like Human-Produced”
and “Well-formed” sentences. In Section 4.1, we
show empirically that high accuracy in the Human-
Likeness test does not necessarily entail good MT
evaluation judgments.
</bodyText>
<subsectionHeader confidence="0.999489">
3.2 Feature Representation
</subsectionHeader>
<bodyText confidence="0.9999745">
To ascertain the resource requirements for different
model sizes, we considered two feature models. The
smaller one uses the same nine features as Kulesza
and Shieber, which were derived from BLEU and
WER. The full model consists of 53 features: some
are adapted from recently developed metrics; others
are new features of our own. They fall into the fol-
lowing major categories1:
String-based metrics over references These in-
clude the nine Kulesza and Shieber features as well
as precision, recall, and fragmentation, as calcu-
lated in METEOR; ROUGE-inspired features that
are non-consecutive bigrams with a gap size of m,
where 1 &lt; m &lt; 5 (skip-m-bigram), and ROUGE-L
(longest common subsequence).
Syntax-based metrics over references We un-
rolled HWCM into their individual chains of length
c (where 2 &lt; c &lt; 4); we modified STM so that it is
computed over unlexicalized constituent parse trees
as well as over dependency parse trees.
String-based metrics over corpus Features in
this category are similar to those in String-based
metric over reference except that a large English cor-
pus is used as “reference” instead.
Syntax-based metrics over corpus A large de-
pendency treebank is used as the “reference” instead
of parsed human translations. In addition to adap-
tations of the Syntax-based metrics over references,
we have also created features to verify the argument
structures for certain syntactic categories.
</bodyText>
<sectionHeader confidence="0.981673" genericHeader="method">
4 Empirical Studies
</sectionHeader>
<bodyText confidence="0.88572875">
In these studies, the learning models used for both
classification and regression are support vector ma-
chines (SVM) with Gaussian kernels. All models
are trained with SVM-Light (Joachims, 1999). Our
primary experimental dataset is from NIST’s 2003
1As feature engineering is not the primary focus of this pa-
per, the features are briefly described here, but implementa-
tional details will be made available in a technical report.
</bodyText>
<page confidence="0.977457">
882
</page>
<figure confidence="0.991006">
Correlation Coefficient with Human Judgement
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
45 50 55 60 65 70 75 80 85
Human-Likeness Classifier Accuracy (%)
</figure>
<bodyText confidence="0.9978112">
Chinese MT Evaluations, in which the fluency and
adequacy of 919 sentences produced by six MT sys-
tems are scored by two human judges on a 5-point
scale2. Because the judges evaluate sentences ac-
cording to their individual standards, the resulting
scores may exhibit a biased distribution. We normal-
ize human judges’ scores following the process de-
scribed by Blatz et al. (2003). The overall human as-
sessment score for a translation output is the average
of the sum of two judges’ normalized fluency and
adequacy scores. The full dataset (6 x 919 = 5514
instances) is split into sets of training, heldout and
test data. Heldout data is used for parameter tuning
(i.e., the slack variable and the width of the Gaus-
sian). When training classifiers, assessment scores
are not used, and the training set is augmented with
all available human reference translation sentences
(4 x 919 = 3676 instances) to serve as positive ex-
amples.
To judge the quality of a metric, we compute
Spearman rank-correlation coefficient, which is a
real number ranging from -1 (indicating perfect neg-
ative correlations) to +1 (indicating perfect posi-
tive correlations), between the metric’s scores and
the averaged human assessments on test sentences.
We use Spearman instead of Pearson because it
is a distribution-free test. To evaluate the rela-
tive reliability of different metrics, we use boot-
strapping re-sampling and paired t-test to determine
whether the difference between the metrics’ correla-
tion scores has statistical significance (at 99.8% con-
fidence level)(Koehn, 2004). Each reported correla-
tion rate is the average of 1000 trials; each trial con-
sists of n sampled points, where n is the size of the
test set. Unless explicitly noted, the qualitative dif-
ferences between metrics we report are statistically
significant. As a baseline comparison, we report the
correlation rates of three standard automatic metrics:
BLEU, METEOR, which incorporates recall and
stemming, and HWCM, which uses syntax. BLEU
is smoothed to be more appropriate for sentence-
level evaluation (Lin and Och, 2004b), and the bi-
gram versions of BLEU and HWCM are reported
because they have higher correlations than when
longer n-grams are included. This phenomenon has
</bodyText>
<footnote confidence="0.9974705">
2This corpus is available from the Linguistic Data Consor-
tium as Multiple Translation Chinese Part 4.
</footnote>
<figureCaption confidence="0.997451">
Figure 1: This scatter plot compares classifiers’ ac-
curacy with their corresponding metrics’ correla-
tions with human assessments
</figureCaption>
<bodyText confidence="0.9036">
been previously observed by Liu and Gildea (2005).
</bodyText>
<subsectionHeader confidence="0.70541">
4.1 Relationship between Classification
Accuracy and Quality of Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999968962962963">
A concern in using a metric derived from a Human-
Likeness classifier is whether it would be predic-
tive for MT evaluation. Kulesza and Shieber (2004)
tried to demonstrate a positive correlation between
the Human-Likeness classification task and the MT
evaluation task empirically. They plotted the clas-
sification accuracy and evaluation reliability for a
number of classifiers, which were generated as a
part of a greedy search for kernel parameters and
found some linear correlation between the two. This
proof of concept is a little misleading, however, be-
cause the population of the sampled classifiers was
biased toward those from the same neighborhood as
the local optimal classifier (so accuracy and corre-
lation may only exhibit linear relationship locally).
Here, we perform a similar study except that we
sampled the kernel parameter more uniformly (on
a log scale). As Figure 1 confirms, having an ac-
curate Human-Likeness classifier does not necessar-
ily entail having a good MT evaluation metric. Al-
though the two tasks do seem to be positively re-
lated, and in the limit there may be a system that is
good at both tasks, one may improve classification
without improving MT evaluation. For this set of
heldout data, at the near 80% accuracy range, a de-
rived metric might have an MT evaluation correla-
tion coefficient anywhere between 0.25 (on par with
</bodyText>
<page confidence="0.949046">
883
</page>
<bodyText confidence="0.987120788732394">
unsmoothed BLEU, which is known to be unsuitable correlations of the metrics derived from the corre-
for sentence-level evaluation) and 0.35 (competitive sponding classifiers. The pair of graphs show, es-
with standard metrics). pecially in the case of the larger feature set, that a
4.2 Learning Curves large improvement in classification accuracy does
To investigate the feasibility of training regression not bring proportional improvement in its corre-
models from assessment data that are currently sponding metrics’s correlation; with an accuracy of
available, we consider both a small and a large near 90%, its correlation coefficient is 0.362, well
regression model. The smaller model consists of below METEOR.
nine features (same as the set used by Kulesza and This experiment further confirms that judging
Shieber); the other uses the full set of 53 features Human-Likeness and judging Human-Acceptability
as described in Section 3.2. The reliability of the are not tightly coupled. Earlier, we have shown in
trained metrics are compared with those developed Figure 1 that different SVM parameterizations may
from Human-Likeness classifiers. We follow a sim- result in classifiers with the same accuracy rate but
ilar training and testing methodology as previous different correlations rates. As a way to incorpo-
studies: we held out 1/6 of the assessment dataset for rate some assessment information into classification
SVM parameter tuning; five-fold cross validation is training, we modify the parameter tuning process so
performed with the remaining sentences. Although that SVM parameters are chosen to optimize for as-
the metrics are evaluated on unseen test sentences, sessment correlations in the heldout data. By incur-
the sentences are produced by the same MT systems ring this small amount of human assessed data, this
that produced the training sentences. In later exper- parameter search improves the classifier’s correla-
iments, we investigate generalizing to more distant tions: the metric using the smaller feature set in-
MT systems. creased from 0.423 to 0.431, and that of the larger
Figure 2(a) shows the learning curves for the two set increased from 0.361 to 0.422.
regression models. As the graph indicates, even
with a limited amount of human assessment data,
regression models can be trained to be comparable
to standard metrics (represented by METEOR in the
graph). The small feature model is close to conver-
gence after 1000 training examples3. The model
with a more complex feature set does require more
training data, but its correlation began to overtake
METEOR after 2000 training examples. This study
suggests that the start-up cost of building even a
moderately complex regression model is not impos-
sibly high.
Although we cannot directly compare the learning
curves of the Human-Likeness classifiers to those of
the regression models (since the classifier’s training
examples are automatically labeled), training exam-
ples for classifiers are not entirely free: human ref-
erence translations still must be developed for the
source sentences. Figure 2(c) shows the learning
curves for training Human-Likeness classifiers (in
terms of improving a classifier’s accuracy) using the
same two feature sets, and Figure 2(b) shows the
4.3 Generalization
We conducted two generalization studies. The first
investigates how well the trained metrics evaluate
systems from other years and systems developed
for a different source language. The second study
delves more deeply into how variations in the train-
ing examples affect a learned metric’s ability to gen-
eralize to distant systems. The learning models for
both experiments use the full feature set.
Cross-Year Generalization To test how well the
learning-based metrics generalize to systems from
different years, we trained both a regression-based
metric (R03) and a classifier-based metric (C03)
with the entire NIST 2003 Chinese dataset (using
20% of the data as heldout4). All metrics are then
applied to three new datasets: NIST 2002 Chinese
MT Evaluation (3 systems, 2634 sentences total),
NIST 2003 Arabic MT Evaluation (2 systems, 1326
sentences total), and NIST 2004 Chinese MT Evalu-
ation (10 systems, 4470 sentences total). The results
4Here, too, we allowed the classifier’s parameters to be
tuned for correlation with human assessment on the heldout data
rather than accuracy.
3The total number of labeled examples required is closer to
2000, since the heldout set uses 919 labeled examples.
884
</bodyText>
<figure confidence="0.99473">
(a) (b) (c)
</figure>
<figureCaption confidence="0.990685">
Figure 2: Learning curves: (a) correlations with human assessment using regression models; (b) correlations
with human assessment using classifiers; (c) classifier accuracy on determining Human-Likeness.
</figureCaption>
<table confidence="0.999158">
Dataset R03 C03 BLEU MET. HWCM
2002 Ara 0.466 0.384 0.423 0.431 0.424
2002 Chn 0.309 0.250 0.269 0.290 0.260
2004 Chn 0.602 0.566 0.588 0.563 0.546
</table>
<tableCaption confidence="0.763563">
Table 1: Correlations for cross-year generalization.
Learning-based metrics are developed from NIST
2003 Chinese data. All metrics are tested on datasets
from 2003 Arabic, 2002 Chinese and 2004 Chinese.
</tableCaption>
<bodyText confidence="0.995579516666667">
are summarized in Table 1. We see that R03 con-
sistently has a better correlation rate than the other
metrics.
At first, it may seem as if the difference between
R03 and BLEU is not as pronounced for the 2004
dataset, calling to question whether a learned met-
ric might become quickly out-dated, we argue that
this is not the case. The 2004 dataset has many
more participating systems, and they span a wider
range of qualities. Thus, it is easier to achieve a
high rank correlation on this dataset than previous
years because most metrics can qualitatively discern
that sentences from one MT system are better than
those from another. In the next experiment, we ex-
amine the performance of R03 with respect to each
MT system in the 2004 dataset and show that its cor-
relation rate is higher for better MT systems.
Relationship between Training Examples and
Generalization Table 2 shows the result of a gen-
eralization study similar to before, except that cor-
relations are performed on each system. The rows
order the test systems by their translation quali-
ties from the best performing system (2004-Chn1,
whose average human assessment score is 0.655 out
of 1.0) to the worst (2004-Chn10, whose score is
0.255). In addition to the regression metric from
the previous experiment (R03-all), we consider two
more regression metrics trained from subsets of the
2003 dataset: R03-Bottom5 is trained from the sub-
set that excludes the best 2003 MT system, and R03-
Top5 is trained from the subset that excludes the
worst 2003 MT system.
We first observe that on a per test-system basis,
the regression-based metrics generally have better
correlation rates than BLEU, and that the gap is as
wide as what we have observed in the earlier cross-
years studies. The one exception is when evaluating
2004-Chn8. None of the metrics seems to correlate
very well with human judges on this system. Be-
cause the regression-based metric uses these individ-
ual metrics as features, its correlation also suffers.
During regression training, the metric is opti-
mized to minimize the difference between its pre-
diction and the human assessments of the training
data. If the input feature vector of a test instance
is in a very distant space from training examples,
the chance for error is higher. As seen from the
results, the learned metrics typically perform better
when the training examples include sentences from
higher-quality systems. Consider, for example, the
differences between R03-all and R03-Top5 versus
the differences between R03-all and R03-Bottom5.
Both R03-Top5 and R03-Bottom5 differ from R03-
all by one subset of training examples. Since R03-
all’s correlation rates are generally closer to R03-
Top5 than to R03-Bottom5, we see that having seen
extra training examples from a bad system is not as
harmful as having not seen training examples from a
good system. This is expected, since there are many
ways to create bad translations, so seeing a partic-
</bodyText>
<page confidence="0.996161">
885
</page>
<table confidence="0.999756916666667">
R03-all R03-Bottom5 R03-Top5 BLEU METEOR HWCM
2004-Chn1 0.495 0.460 0.518 0.456 0.457 0.444
2004-Chn2 0.398 0.330 0.440 0.352 0.347 0.344
2004-Chn3 0.425 0.389 0.459 0.369 0.402 0.369
2004-Chn4 0.432 0.392 0.434 0.400 0.400 0.362
2004-Chn5 0.452 0.441 0.443 0.370 0.426 0.326
2004-Chn6 0.405 0.392 0.406 0.390 0.357 0.380
2004-Chn7 0.443 0.432 0.448 0.390 0.408 0.392
2004-Chn8 0.237 0.256 0.256 0.265 0.259 0.179
2004-Chn9 0.581 0.569 0.591 0.527 0.537 0.535
2004-Chn10 0.314 0.313 0.354 0.321 0.303 0.358
2004-all 0.602 0.567 0.617 0.588 0.563 0.546
</table>
<tableCaption confidence="0.995419">
Table 2: Metric correlations within each system. The columns specify which metric is used. The rows
</tableCaption>
<bodyText confidence="0.989673111111111">
specify which MT system is under evaluation; they are ordered by human-judged system quality, from best
to worst. For each evaluated MT system (row), the highest coefficient in bold font, and those that are
statistically comparable to the highest are shown in italics.
ular type of bad translations from one system may
not be very informative. In contrast, the neighbor-
hood of good translations is much smaller, and is
where all the systems are aiming for; thus, assess-
ments of sentences from a good system can be much
more informative.
</bodyText>
<subsectionHeader confidence="0.983411">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.994468456521739">
Experimental results confirm that learning from
training examples that have been doubly approx-
imated (class labels instead of ordinals, human-
likeness instead of human-acceptability) does nega-
tively impact the performance of the derived metrics.
In particular, we showed that they do not generalize
as well to new data as metrics trained from direct
regression.
We see two lingering potential objections toward
developing metrics with regression-learning. One
is the concern that a system under evaluation might
try to explicitly “game the metric5.” This is a con-
cern shared by all automatic evaluation metrics, and
potential problems in stand-alone metrics have been
analyzed (Callison-Burch et al., 2006). In a learning
framework, potential pitfalls for individual metrics
are ameliorated through a combination of evidences.
That said, it is still prudent to defend against the po-
tential of a system gaming a subset of the features.
For example, our fluency-predictor features are not
strong indicators of translation qualities by them-
selves. We want to avoid training a metric that as-
5Or, in a less adversarial setting, a system may be perform-
ing minimum error-rate training (Och, 2003)
signs a higher than deserving score to a sentence that
just happens to have many n-gram matches against
the target-language reference corpus. This can be
achieved by supplementing the current set of hu-
man assessed training examples with automatically
assessed training examples, similar to the labeling
process used in the Human-Likeness classification
framework. For instance, as negative training ex-
amples, we can incorporate fluent sentences that are
not adequate translations and assign them low over-
all assessment scores.
A second, related concern is that because the met-
ric is trained on examples from current systems us-
ing currently relevant features, even though it gener-
alizes well in the near term, it may not continue to
be a good predictor in the distant future. While pe-
riodic retraining may be necessary, we see value in
the flexibility of the learning framework, which al-
lows for new features to be added. Moreover, adap-
tive learning methods may be applicable if a small
sample of outputs of some representative translation
systems is manually assessed periodically.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999210571428572">
Human judgment of sentence-level translation qual-
ity depends on many criteria. Machine learning af-
fords a unified framework to compose these crite-
ria into a single metric. In this paper, we have
demonstrated the viability of a regression approach
to learning the composite metric. Our experimental
results show that by training from some human as-
</bodyText>
<page confidence="0.994555">
886
</page>
<bodyText confidence="0.999681333333333">
sessments, regression methods result in metrics that
have better correlations with human judgments even
as the distribution of the tested population changes.
</bodyText>
<sectionHeader confidence="0.997738" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9806655">
This work has been supported by NSF Grants IIS-0612791 and
IIS-0710695. We would like to thank Regina Barzilay, Ric
Crabbe, Dan Gildea, Alex Kulesza, Alon Lavie, and Matthew
Stone as well as the anonymous reviewers for helpful comments
and suggestions. We are also grateful to NIST for making their
assessment data available to us.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803208791209">
Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Lluis
M`arquez. 2006. MT evaluation: Human-like vs. human ac-
ceptable. In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, Sydney, Australia, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An auto-
matic metric for MT evaluation with improved correlation
with human judgments. In ACL 2005 Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence estimation for machine trans-
lation. Technical Report Natural Language Engineering
Workshop Final Report, Johns Hopkins University.
Christopher Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in machine
translation research. In The Proceedings of the Thirteenth
Conference of the European Chapter of the Association for
Computational Linguistics.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of the 39th
Annual Meeting of the Association for Computational Lin-
guistics, July.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Bernhard Sch¨oelkopf, Christopher Burges, and
Alexander Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
David Kauchak and Regina Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main Confer-
ence, New York City, USA, June.
Philipp Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP-04).
Alex Kulesza and Stuart M. Shieber. 2004. A learning ap-
proach to improving sentence-level MT evaluation. In Pro-
ceedings of the 10th International Conference on Theoretical
and Methodological Issues in Machine Translation (TMI),
Baltimore, MD, October.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT evaluation using block movements. In
The Proceedings of the Thirteenth Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics.
Chin-Yew Lin and Franz Josef Och. 2004a. Automatic evalu-
ation of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceedings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics, July.
Chin-Yew Lin and Franz Josef Och. 2004b. Orange: a
method for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING 2004),
August.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, June.
Ding Liu and Daniel Gildea. 2006. Stochastic iterative align-
ment for machine translation evaluation. In Proceedings
of the Joint Conference of the International Conference on
Computational Linguistics and the Association for Com-
putational Linguistics (COLING-ACL’2006) Poster Session,
July.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Preci-
sion and recall of machine translation. In In Proceedings of
the HLT-NAACL 2003: Short Papers, pages 61–63, Edmon-
ton, Alberta.
Franz Josef Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings ofthe 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, PA.
Christopher Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of LREC
2004.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of the
8th Conference of the Association for Machine Translation
in the Americas (AMTA-2006).
Christoph Tillmann, Stephan Vogel, Hermann Ney, Hassan
Sawaf, and Alex Zubiaga. 1997. Accelerated DP-based
search for statistical translation. In Proceedings of the 5th
European Conference on Speech Communication and Tech-
nology (EuroSpeech ’97).
</reference>
<page confidence="0.998003">
887
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983568">
<title confidence="0.9990025">A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation</title>
<author confidence="0.999977">Joshua S Albrecht</author>
<author confidence="0.999977">Rebecca Hwa</author>
<affiliation confidence="0.999879">Department of Computer Science University of Pittsburgh</affiliation>
<abstract confidence="0.999206333333333">Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proapproaches of training a Humanclassifier not as well correlated with human judgments of translation qualbut that learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jes´us Gim´enez</author>
<author>Julio Gonzalo</author>
<author>Lluis M`arquez</author>
</authors>
<title>MT evaluation: Human-like vs. human acceptable.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<location>Sydney, Australia,</location>
<marker>Amig´o, Gim´enez, Gonzalo, M`arquez, 2006</marker>
<rawString>Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Lluis M`arquez. 2006. MT evaluation: Human-like vs. human acceptable. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<contexts>
<context position="7034" citStr="Banerjee and Lavie, 2005" startWordPosition="1089" endWordPosition="1092">. quences. Since the introduction of BLEU (Papineni One way to combine these metrics in a uniform and et al., 2002) the basic n-gram precision idea has principled manner is through a learning framework. been augmented in a number of ways. Metrics in the The individual metrics participate as input features, Rouge family allow for skip n-grams (Lin and Och, from which the learning algorithm infers a compos2004a); Kauchak and Barzilay (2006) take para- ite metric that is optimized on training examples. phrasing into account; metrics such as METEOR Reframing sentence-level translation evaluation (Banerjee and Lavie, 2005) and GTM (Melamed et as a classification task was first proposed by al., 2003) calculate both recall and precision; ME- Corston-Oliver et al. (2001). Interestingly, instead TEOR is also similar to SIA (Liu and Gildea, 2006) of recasting the classification problem as a “Huin that word class information is used. Finally, re- man Acceptability” test (distinguishing good transsearchers have begun to look for similarities at a lations outputs from bad one), they chose to develop deeper structural level. For example, Liu and Gildea a Human-Likeness classifier (distinguishing out(2005) developed the </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical Report Natural Language Engineering Workshop Final Report,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="1457" citStr="Blatz et al., 2003" startWordPosition="212" endWordPosition="215">nts than standard automatic metrics. 1 Introduction As machine translation (MT) research advances, the importance of its evaluation also grows. Efficient evaluation methodologies are needed both for facilitating the system development cycle and for providing an unbiased comparison between systems. To this end, a number of automatic evaluation metrics have been proposed to approximate human judgments of MT output quality. Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (Blatz et al., 2003). This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality. A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004). Under the assumption that good machine translation will produce “human-like” sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be </context>
<context position="12818" citStr="Blatz et al. (2003)" startWordPosition="1995" endWordPosition="1998">scribed here, but implementational details will be made available in a technical report. 882 Correlation Coefficient with Human Judgement 0.35 0.25 0.15 0.05 0.4 0.3 0.2 0.1 0 45 50 55 60 65 70 75 80 85 Human-Likeness Classifier Accuracy (%) Chinese MT Evaluations, in which the fluency and adequacy of 919 sentences produced by six MT systems are scored by two human judges on a 5-point scale2. Because the judges evaluate sentences according to their individual standards, the resulting scores may exhibit a biased distribution. We normalize human judges’ scores following the process described by Blatz et al. (2003). The overall human assessment score for a translation output is the average of the sum of two judges’ normalized fluency and adequacy scores. The full dataset (6 x 919 = 5514 instances) is split into sets of training, heldout and test data. Heldout data is used for parameter tuning (i.e., the slack variable and the width of the Gaussian). When training classifiers, assessment scores are not used, and the training set is augmented with all available human reference translation sentences (4 x 919 = 3676 instances) to serve as positive examples. To judge the quality of a metric, we compute Spear</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. Technical Report Natural Language Engineering Workshop Final Report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In The Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26295" citStr="Callison-Burch et al., 2006" startWordPosition="4129" endWordPosition="4132">een doubly approximated (class labels instead of ordinals, humanlikeness instead of human-acceptability) does negatively impact the performance of the derived metrics. In particular, we showed that they do not generalize as well to new data as metrics trained from direct regression. We see two lingering potential objections toward developing metrics with regression-learning. One is the concern that a system under evaluation might try to explicitly “game the metric5.” This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al., 2006). In a learning framework, potential pitfalls for individual metrics are ameliorated through a combination of evidences. That said, it is still prudent to defend against the potential of a system gaming a subset of the features. For example, our fluency-predictor features are not strong indicators of translation qualities by themselves. We want to avoid training a metric that as5Or, in a less adversarial setting, a system may be performing minimum error-rate training (Och, 2003) signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-l</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Christopher Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In The Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Chris Brockett</author>
</authors>
<title>A machine learning approach to the automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1797" citStr="Corston-Oliver et al., 2001" startWordPosition="268" endWordPosition="271">c evaluation metrics have been proposed to approximate human judgments of MT output quality. Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (Blatz et al., 2003). This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality. A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004). Under the assumption that good machine translation will produce “human-like” sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be the sentence’s scores from individual automatic evaluation metrics. The confidence of the classifier’s prediction can then be interpreted as a judgment on the translation quality of the sentence. Thus, the composite metric is encoded in the confidence scores of the classification labels. While the learning approach to metric design offers</context>
<context position="7182" citStr="Corston-Oliver et al. (2001)" startWordPosition="1113" endWordPosition="1116">ea has principled manner is through a learning framework. been augmented in a number of ways. Metrics in the The individual metrics participate as input features, Rouge family allow for skip n-grams (Lin and Och, from which the learning algorithm infers a compos2004a); Kauchak and Barzilay (2006) take para- ite metric that is optimized on training examples. phrasing into account; metrics such as METEOR Reframing sentence-level translation evaluation (Banerjee and Lavie, 2005) and GTM (Melamed et as a classification task was first proposed by al., 2003) calculate both recall and precision; ME- Corston-Oliver et al. (2001). Interestingly, instead TEOR is also similar to SIA (Liu and Gildea, 2006) of recasting the classification problem as a “Huin that word class information is used. Finally, re- man Acceptability” test (distinguishing good transsearchers have begun to look for similarities at a lations outputs from bad one), they chose to develop deeper structural level. For example, Liu and Gildea a Human-Likeness classifier (distinguishing out(2005) developed the Sub-Tree Metric (STM) over puts seem human-produced from machine-produced constituent parse trees and the Head-Word Chain ones) to avoid the necessi</context>
</contexts>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning.</booktitle>
<editor>In Bernhard Sch¨oelkopf, Christopher Burges, and Alexander Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12053" citStr="Joachims, 1999" startWordPosition="1868" endWordPosition="1869"> in String-based metric over reference except that a large English corpus is used as “reference” instead. Syntax-based metrics over corpus A large dependency treebank is used as the “reference” instead of parsed human translations. In addition to adaptations of the Syntax-based metrics over references, we have also created features to verify the argument structures for certain syntactic categories. 4 Empirical Studies In these studies, the learning models used for both classification and regression are support vector machines (SVM) with Gaussian kernels. All models are trained with SVM-Light (Joachims, 1999). Our primary experimental dataset is from NIST’s 2003 1As feature engineering is not the primary focus of this paper, the features are briefly described here, but implementational details will be made available in a technical report. 882 Correlation Coefficient with Human Judgement 0.35 0.25 0.15 0.05 0.4 0.3 0.2 0.1 0 45 50 55 60 65 70 75 80 85 Human-Likeness Classifier Accuracy (%) Chinese MT Evaluations, in which the fluency and adequacy of 919 sentences produced by six MT systems are scored by two human judges on a 5-point scale2. Because the judges evaluate sentences according to their i</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Bernhard Sch¨oelkopf, Christopher Burges, and Alexander Smola, editors, Advances in Kernel Methods -Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<location>New York City, USA,</location>
<contexts>
<context position="6851" citStr="Kauchak and Barzilay (2006)" startWordPosition="1064" endWordPosition="1067">usch et al., 2006). Second, similar- person. The metrics cited in the previous section ity can be expressed in terms of common word se- aim to capture certain aspects of human judgments. quences. Since the introduction of BLEU (Papineni One way to combine these metrics in a uniform and et al., 2002) the basic n-gram precision idea has principled manner is through a learning framework. been augmented in a number of ways. Metrics in the The individual metrics participate as input features, Rouge family allow for skip n-grams (Lin and Och, from which the learning algorithm infers a compos2004a); Kauchak and Barzilay (2006) take para- ite metric that is optimized on training examples. phrasing into account; metrics such as METEOR Reframing sentence-level translation evaluation (Banerjee and Lavie, 2005) and GTM (Melamed et as a classification task was first proposed by al., 2003) calculate both recall and precision; ME- Corston-Oliver et al. (2001). Interestingly, instead TEOR is also similar to SIA (Liu and Gildea, 2006) of recasting the classification problem as a “Huin that word class information is used. Finally, re- man Acceptability” test (distinguishing good transsearchers have begun to look for similarit</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-04).</booktitle>
<contexts>
<context position="13996" citStr="Koehn, 2004" startWordPosition="2184" endWordPosition="2185">lity of a metric, we compute Spearman rank-correlation coefficient, which is a real number ranging from -1 (indicating perfect negative correlations) to +1 (indicating perfect positive correlations), between the metric’s scores and the averaged human assessments on test sentences. We use Spearman instead of Pearson because it is a distribution-free test. To evaluate the relative reliability of different metrics, we use bootstrapping re-sampling and paired t-test to determine whether the difference between the metrics’ correlation scores has statistical significance (at 99.8% confidence level)(Koehn, 2004). Each reported correlation rate is the average of 1000 trials; each trial consists of n sampled points, where n is the size of the test set. Unless explicitly noted, the qualitative differences between metrics we report are statistically significant. As a baseline comparison, we report the correlation rates of three standard automatic metrics: BLEU, METEOR, which incorporates recall and stemming, and HWCM, which uses syntax. BLEU is smoothed to be more appropriate for sentencelevel evaluation (Lin and Och, 2004b), and the bigram versions of BLEU and HWCM are reported because they have higher </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Baltimore, MD,</location>
<contexts>
<context position="1825" citStr="Kulesza and Shieber, 2004" startWordPosition="272" endWordPosition="275">n proposed to approximate human judgments of MT output quality. Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (Blatz et al., 2003). This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality. A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004). Under the assumption that good machine translation will produce “human-like” sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be the sentence’s scores from individual automatic evaluation metrics. The confidence of the classifier’s prediction can then be interpreted as a judgment on the translation quality of the sentence. Thus, the composite metric is encoded in the confidence scores of the classification labels. While the learning approach to metric design offers the promise of ease of comb</context>
<context position="8790" citStr="Kulesza and Shieber (2004)" startWordPosition="1362" endWordPosition="1365">an SVM classifier that makes its decisions based on a set of input features computed from the sentence to be evaluated; the distance between input feature vector and the separating hyperplane then serves as the evaluation score. The underlying assumption for both is that improving the accuracy of the classifier on the Human-Likeness test will also improve the implicit MT evaluation metric. A more direct alternative to the classification approach is to learn via regression and explicitly optimize for a function (i.e. MT evaluation metric) that approximates human judgments in training examples. Kulesza and Shieber (2004) raised two main objections against regression for MT evaluations. One is that regression requires a large set of labeled training examples. Another is that regression may not generalize well over time, and re-training may become necessary, which would require collecting additional human assessment data. While these are legitimate concerns, we show through empirical studies (in Section 4.2) that the additional resource requirement is not impractically high, and that a regression-based metric has higher correlations with human judgments and generalizes better than a metric derived from a Human-</context>
<context position="15179" citStr="Kulesza and Shieber (2004)" startWordPosition="2368" endWordPosition="2371">HWCM are reported because they have higher correlations than when longer n-grams are included. This phenomenon has 2This corpus is available from the Linguistic Data Consortium as Multiple Translation Chinese Part 4. Figure 1: This scatter plot compares classifiers’ accuracy with their corresponding metrics’ correlations with human assessments been previously observed by Liu and Gildea (2005). 4.1 Relationship between Classification Accuracy and Quality of Evaluation Metric A concern in using a metric derived from a HumanLikeness classifier is whether it would be predictive for MT evaluation. Kulesza and Shieber (2004) tried to demonstrate a positive correlation between the Human-Likeness classification task and the MT evaluation task empirically. They plotted the classification accuracy and evaluation reliability for a number of classifiers, which were generated as a part of a greedy search for kernel parameters and found some linear correlation between the two. This proof of concept is a little misleading, however, because the population of the sampled classifiers was biased toward those from the same neighborhood as the local optimal classifier (so accuracy and correlation may only exhibit linear relatio</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT evaluation using block movements.</title>
<date>2006</date>
<booktitle>In The Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6242" citStr="Leusch et al., 2006" startWordPosition="964" endWordPosition="967">rspecified, several different fami- cision process in making judgments about the adelies of metrics have been developed. First, simi- quacy and fluency of translation outputs. Inferring a larity can be expressed in terms of string edit dis- cognitive model of human judgments is a challengtances. In addition to the well-known word error ing problem because the ultimate judgment encomrate (WER), more sophisticated modifications have passes a multitude of fine-grained decisions, and the been proposed (Tillmann et al., 1997; Snover et decision process may differ slightly from person to al., 2006; Leusch et al., 2006). Second, similar- person. The metrics cited in the previous section ity can be expressed in terms of common word se- aim to capture certain aspects of human judgments. quences. Since the introduction of BLEU (Papineni One way to combine these metrics in a uniform and et al., 2002) the basic n-gram precision idea has principled manner is through a learning framework. been augmented in a number of ways. Metrics in the The individual metrics participate as input features, Rouge family allow for skip n-grams (Lin and Och, from which the learning algorithm infers a compos2004a); Kauchak and Barzil</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT evaluation using block movements. In The Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="14513" citStr="Lin and Och, 2004" startWordPosition="2265" endWordPosition="2268">he metrics’ correlation scores has statistical significance (at 99.8% confidence level)(Koehn, 2004). Each reported correlation rate is the average of 1000 trials; each trial consists of n sampled points, where n is the size of the test set. Unless explicitly noted, the qualitative differences between metrics we report are statistically significant. As a baseline comparison, we report the correlation rates of three standard automatic metrics: BLEU, METEOR, which incorporates recall and stemming, and HWCM, which uses syntax. BLEU is smoothed to be more appropriate for sentencelevel evaluation (Lin and Och, 2004b), and the bigram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. This phenomenon has 2This corpus is available from the Linguistic Data Consortium as Multiple Translation Chinese Part 4. Figure 1: This scatter plot compares classifiers’ accuracy with their corresponding metrics’ correlations with human assessments been previously observed by Liu and Gildea (2005). 4.1 Relationship between Classification Accuracy and Quality of Evaluation Metric A concern in using a metric derived from a HumanLikeness classifier is whether it</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004a. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="14513" citStr="Lin and Och, 2004" startWordPosition="2265" endWordPosition="2268">he metrics’ correlation scores has statistical significance (at 99.8% confidence level)(Koehn, 2004). Each reported correlation rate is the average of 1000 trials; each trial consists of n sampled points, where n is the size of the test set. Unless explicitly noted, the qualitative differences between metrics we report are statistically significant. As a baseline comparison, we report the correlation rates of three standard automatic metrics: BLEU, METEOR, which incorporates recall and stemming, and HWCM, which uses syntax. BLEU is smoothed to be more appropriate for sentencelevel evaluation (Lin and Och, 2004b), and the bigram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. This phenomenon has 2This corpus is available from the Linguistic Data Consortium as Multiple Translation Chinese Part 4. Figure 1: This scatter plot compares classifiers’ accuracy with their corresponding metrics’ correlations with human assessments been previously observed by Liu and Gildea (2005). 4.1 Relationship between Classification Accuracy and Quality of Evaluation Metric A concern in using a metric derived from a HumanLikeness classifier is whether it</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004b. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<contexts>
<context position="14948" citStr="Liu and Gildea (2005)" startWordPosition="2332" endWordPosition="2335">omatic metrics: BLEU, METEOR, which incorporates recall and stemming, and HWCM, which uses syntax. BLEU is smoothed to be more appropriate for sentencelevel evaluation (Lin and Och, 2004b), and the bigram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. This phenomenon has 2This corpus is available from the Linguistic Data Consortium as Multiple Translation Chinese Part 4. Figure 1: This scatter plot compares classifiers’ accuracy with their corresponding metrics’ correlations with human assessments been previously observed by Liu and Gildea (2005). 4.1 Relationship between Classification Accuracy and Quality of Evaluation Metric A concern in using a metric derived from a HumanLikeness classifier is whether it would be predictive for MT evaluation. Kulesza and Shieber (2004) tried to demonstrate a positive correlation between the Human-Likeness classification task and the MT evaluation task empirically. They plotted the classification accuracy and evaluation reliability for a number of classifiers, which were generated as a part of a greedy search for kernel parameters and found some linear correlation between the two. This proof of con</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic iterative alignment for machine translation evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Conference on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL’2006) Poster Session,</booktitle>
<contexts>
<context position="7257" citStr="Liu and Gildea, 2006" startWordPosition="1125" endWordPosition="1128">er of ways. Metrics in the The individual metrics participate as input features, Rouge family allow for skip n-grams (Lin and Och, from which the learning algorithm infers a compos2004a); Kauchak and Barzilay (2006) take para- ite metric that is optimized on training examples. phrasing into account; metrics such as METEOR Reframing sentence-level translation evaluation (Banerjee and Lavie, 2005) and GTM (Melamed et as a classification task was first proposed by al., 2003) calculate both recall and precision; ME- Corston-Oliver et al. (2001). Interestingly, instead TEOR is also similar to SIA (Liu and Gildea, 2006) of recasting the classification problem as a “Huin that word class information is used. Finally, re- man Acceptability” test (distinguishing good transsearchers have begun to look for similarities at a lations outputs from bad one), they chose to develop deeper structural level. For example, Liu and Gildea a Human-Likeness classifier (distinguishing out(2005) developed the Sub-Tree Metric (STM) over puts seem human-produced from machine-produced constituent parse trees and the Head-Word Chain ones) to avoid the necessity of obtaining manuMetric (HWCM) over dependency parse trees. ally labeled</context>
</contexts>
<marker>Liu, Gildea, 2006</marker>
<rawString>Ding Liu and Daniel Gildea. 2006. Stochastic iterative alignment for machine translation evaluation. In Proceedings of the Joint Conference of the International Conference on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL’2006) Poster Session, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph Turian</author>
</authors>
<title>Precision and recall of machine translation. In</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003: Short Papers,</booktitle>
<pages>61--63</pages>
<location>Edmonton, Alberta.</location>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Precision and recall of machine translation. In In Proceedings of the HLT-NAACL 2003: Short Papers, pages 61–63, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26778" citStr="Och, 2003" startWordPosition="4209" endWordPosition="4210">ll automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al., 2006). In a learning framework, potential pitfalls for individual metrics are ameliorated through a combination of evidences. That said, it is still prudent to defend against the potential of a system gaming a subset of the features. For example, our fluency-predictor features are not strong indicators of translation qualities by themselves. We want to avoid training a metric that as5Or, in a less adversarial setting, a system may be performing minimum error-rate training (Och, 2003) signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-language reference corpus. This can be achieved by supplementing the current set of human assessed training examples with automatically assessed training examples, similar to the labeling process used in the Human-Likeness classification framework. For instance, as negative training examples, we can incorporate fluent sentences that are not adequate translations and assign them low overall assessment scores. A second, related concern is that because the metric is trained on examp</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofthe 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofthe 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="2980" citStr="Quirk, 2004" startWordPosition="457" endWordPosition="458">roach to metric design offers the promise of ease of combining multiple metrics and the potential for improved performance, several salient questions should be addressed more fully. First, is learning a “Human Likeness” classifier the most suitable approach for framing the MTevaluation question? An alternative is regression, in which the composite metric is explicitly learned as a function that approximates humans’ quantitative judgments, based on a set of human evaluated training sentences. Although regression has been considered on a small scale for a single system as confidence estimation (Quirk, 2004), this approach has not been studied as extensively due to scalability and generalization concerns. Second, how does the diversity of the model features impact the learned metric? Third, how well do learning-based metrics generalize beyond their training examples? In particular, how well can a metric that was developed based 880 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880–887, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics on one group of MT systems evaluate the translation MT developers need a way to ev</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Christopher Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 8th Conference of the Association for Machine Translation in the Americas (AMTA-2006).</booktitle>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 8th Conference of the Association for Machine Translation in the Americas (AMTA-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Hassan Sawaf</author>
<author>Alex Zubiaga</author>
</authors>
<title>Accelerated DP-based search for statistical translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology (EuroSpeech ’97).</booktitle>
<contexts>
<context position="6147" citStr="Tillmann et al., 1997" startWordPosition="948" endWordPosition="951">er, as the notion of similar- a computational model that captures a human’s deity is itself underspecified, several different fami- cision process in making judgments about the adelies of metrics have been developed. First, simi- quacy and fluency of translation outputs. Inferring a larity can be expressed in terms of string edit dis- cognitive model of human judgments is a challengtances. In addition to the well-known word error ing problem because the ultimate judgment encomrate (WER), more sophisticated modifications have passes a multitude of fine-grained decisions, and the been proposed (Tillmann et al., 1997; Snover et decision process may differ slightly from person to al., 2006; Leusch et al., 2006). Second, similar- person. The metrics cited in the previous section ity can be expressed in terms of common word se- aim to capture certain aspects of human judgments. quences. Since the introduction of BLEU (Papineni One way to combine these metrics in a uniform and et al., 2002) the basic n-gram precision idea has principled manner is through a learning framework. been augmented in a number of ways. Metrics in the The individual metrics participate as input features, Rouge family allow for skip n-</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Sawaf, Zubiaga, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, Hassan Sawaf, and Alex Zubiaga. 1997. Accelerated DP-based search for statistical translation. In Proceedings of the 5th European Conference on Speech Communication and Technology (EuroSpeech ’97).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>