<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016012">
<title confidence="0.997582">
Towards Automatic Image Region Annotation - Image Region Textual
Coreference Resolution
</title>
<author confidence="0.995056">
Emilia Apostolova
</author>
<affiliation confidence="0.923993333333333">
College of Computing and Digital Media
DePaul University
Chicago, IL 60604, USA
</affiliation>
<email confidence="0.994624">
emilia.aposto@gmail.com
</email>
<author confidence="0.926109">
Dina Demner-Fushman
</author>
<affiliation confidence="0.862966333333333">
Communications Engineering Branch
National Library of Medicine
Bethesda, MD 20894, USA
</affiliation>
<email confidence="0.994832">
ddemner@mail.nih.gov
</email>
<sectionHeader confidence="0.995571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998714214285714">
Detailed image annotation necessary for reli-
able image retrieval involves not only annotat-
ing the image as a single artifact, but also an-
notating specific objects or regions within the
image. Such detailed annotation is a costly en-
deavor and the available annotated image data
are quite limited. This paper explores the fea-
sibility of using image captions from scientific
journals for the purpose of automatically an-
notating image regions. Salient image clues,
such as an object location within the image or
an object color, together with the associated
explicit object mention, are extracted and clas-
sified using rule-based and SVM learners.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969235294118">
The profusion of digitally available images has nat-
urally led to an interest in the field of automatic im-
age annotation and retrieval. A number of studies
attempt to associate image regions with the corre-
sponding concepts. In (Duygulu et al., 2002), for
example, the problem of annotation is treated as a
translation from a set of image segments (or blobs)
to a set of words. Modeling the association between
blobs and words for the purpose of automated an-
notation has also been proposed by (Barnard et al.,
2003; Jeon et al., 2003).
A recurring hindrance that appears in studies aim-
ing at automatic image region annotation is the lack
of an appropriate dataset. All of the above studies
use the Corel image dataset that consists of 60,000
images annotated with 3 to 5 keywords. The need
for an image dataset with annotated image regions
</bodyText>
<page confidence="0.991073">
41
</page>
<bodyText confidence="0.999986470588235">
has been recognized by many researchers. For ex-
ample, Russell et al (2008) have developed a tool
and a general purpose image database designed to
delineate and annotate objects within image scenes.
The need for an image dataset with annotated ob-
ject boundaries appears to be especially pertinent in
the biomedical field. Organizing and using for re-
search the available medical imaging data proved to
be a challenge and a goal of the ongoing research.
Rubin et al (2008), for example, propose an ontol-
ogy and annotation tool for semantic annotation of
image regions in radiology.
However, creating a dataset of image regions
manually annotated and delineated by domain ex-
perts, is a costly enterprise. Any attempts to auto-
mate or semi-automate the process would be of a
substantial value.
This work proposes an approach towards auto-
matic annotation of regions of interest in images
used in scientific publications. Publications abun-
dant in image data are an untapped source of an-
notated image data. Due to publication standards,
meaningful image captions are almost always pro-
vided within scientific articles. In addition, image
Regions of Interest (ROIs) are commonly referred to
within the image caption. Such ROIs are also com-
monly delineated with some kind of an overlay that
helps locating the ROI. This is especially true for
hard to interpret scientific images such as radiology
images. ROIs are also described in terms of location
within the image, or by the presence of a particular
color. Identifying ROI mentions within image cap-
tions and visual clues pinpointing the ROI within the
image would be the first step in building an object
</bodyText>
<subsubsectionHeader confidence="0.804094">
Proceedings of NAACL HLT 2009: Short Papers, pages 41–44,
</subsubsectionHeader>
<bodyText confidence="0.430187">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.833768">
1. Object Location - explicit ROI location, e.g. front row, back-
</listItem>
<bodyText confidence="0.985086666666667">
ground, top, bottom, left, right.
Shells of planktonic animals called formainifera record cli-
matic conditions as they are formed. This one, Globigeri-
noides ruber, lives year-round at the surface of the Sargasso Sea.
The form of the live animal is shown at right, and its shell, which
is actually about the size of a fine grain of sand, at left.
2. Object Color - presence of a distinct color that identifies a
ROI.
Anterior SSD image shows an elongated splenorenal varix (blue
area). The varix travels from the splenic hilar region inferiorly
along the left flank, down into the pelvis, and eventually back up to
the left renal vein via the left gonadal vein. The kidney is encoded
yellow, the portal system is encoded magenta, and the spleen is
encoded tan.
3. Overlay Marker - an overlay marker used to pinpoint the loca-
tion of the ROI, e.g. arrows, asterisks, bounding boxes, or circles.
Transverse sonograms obtained with a 7.5-MHz linear trans-
ducer in the subareolar region. The straight arrows
show a dilated tubular structure. The curved arrow indicates
an intraluminal solid mass.
4. Overlay Label - an overlay label used to pinpoint the location
of the ROI, e.g. numbers, letters, words, abbreviations.
Location of the calf veins. Transverse US image just
above ankle demonstrates the paired posterior tibial veins (V)
and posterior tibial artery (A) imaged from a posteromedial ap-
proach. Note there is inadequate venous flow velocity to visualize
with color Doppler without flow augmentation.
</bodyText>
<tableCaption confidence="0.991299666666667">
Table 1: Image Markers divided into four categories, followed by
a sample image caption1 in which Image Markers are marked in bold,
Image Marker Referents are underlined.
</tableCaption>
<bodyText confidence="0.890465">
delineated and annotated image dataset.
</bodyText>
<sectionHeader confidence="0.88727" genericHeader="introduction">
2 Problem Definition
</sectionHeader>
<bodyText confidence="0.9997935">
The goal of this research is to locate visually salient
image region characteristics in the text surrounding
scientific images that could be used to facilitate the
delineation of the image object boundaries. This
task could be broken down into two related subtasks
- 1) locating and classifying textual clues for visu-
ally salient ROI features (Image Markers), and 2) lo-
cating the corresponding ROI text mentions (Image
Marker Referents). Table 1 gives a classification of
Image Markers including examples of Image Mark-
ers and Image Marker Referents. Figure 1 shows the
frequency of Image Marker occurrences.
</bodyText>
<footnote confidence="0.974415666666667">
1The captions were extracted from Radiology and Ra-
diographics © Radiological Society of North America and
Oceanus © Woods Hole Oceanographic Institution.
</footnote>
<sectionHeader confidence="0.998846" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99994775862069">
Cohen et al (2003) attempt to identify what they
refer to as “image pointers” within captions in
biomedical publications. The image pointers of in-
terest are, for example, image panel labels, or letters
and abbreviations used as an overlay within the im-
age, similar to the Overlay Labels described in Table
1. They developed a set of hand-crafted rules, and a
learning method involving Boosted Wrapper Induc-
tion on a dataset consisting of biomedical articles
related to fluorescence microscope images.
Deschacht and Moens (2007) analyze text sur-
rounding images in news articles trying to identify
persons and objects in the text that appear in the
corresponding image. They start by extracting per-
sons’ names and visual objects using Named Entity
Recognition (NER) tools. Next, they measure the
“salience” of the extracted named entities within the
text with the assumption that more salient named en-
tities in the text will also be present in the accompa-
nying image.
Davis et al (2003) develop a NER tool to iden-
tify references to a single art object (for example a
specific building within an image) in text related to
art images for the purpose of automatic cataloging
of images. They take a semi-supervised approach to
locating the named entities of interest by first provid-
ing an authoritative list of art objects of interest and
then seeking to match variants of the seed named en-
tities in related text.
</bodyText>
<sectionHeader confidence="0.999749" genericHeader="method">
4 Experimental Methods and Results
</sectionHeader>
<subsectionHeader confidence="0.892593">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.990989714285714">
The chosen date-
set contains more
than 60,000 images
together with their as-
sociated captions from
three online life and
earth sciences jour-
nals1. 400 randomly
selected image cap-
tions were manually
annotated by a single
annotator with their
Image Markers and Image Marker Referents and
used for testing and for cross-validation respectively
</bodyText>
<figureCaption confidence="0.970208333333333">
Figure 1: Distribution of Image
Marker types across 400 annotated
image captions.
</figureCaption>
<page confidence="0.991566">
42
</page>
<bodyText confidence="0.991909">
in the two methods described below.
</bodyText>
<subsectionHeader confidence="0.977803">
4.2 Rule Based Approach
</subsectionHeader>
<bodyText confidence="0.999926">
First, we developed a two-stage rule-based, boot-
strapping algorithm for locating the image markers
and their coreferents from unannotated data. The al-
gorithm is based on the observation that textual im-
age markers commonly appear in parentheses and
are usually closely related semantic concepts. Thus
the seed for the algorithm consists of:
</bodyText>
<listItem confidence="0.932771066666667">
1. The predominant syntactic pattern - parenthe-
ses, as in ‘hooking of the soft palate (arrow)’. This
pattern could easily be captured by a regular expres-
sion and doesn’t require sentence parsing.
2. A dozen seed phrases (e.g ‘left’, ‘circle’, ‘as-
terisk’, ‘blue’) identified by initially annotating a
small subset of the data (20 captions). Wordnet was
used to look up and prepare a list of their corre-
sponding inherited hypernyms. This hypernym list
contains concepts such as ‘a spatially limited lo-
cation’, ‘a two-dimensional shape’, ‘a written or
printed symbol’, ‘a visual attribute of things that
results from the light they emit or transmit or re-
flect’. Best results were achieved when inherited hy-
pernyms up to the third parent were used.
</listItem>
<bodyText confidence="0.999894043478261">
In the first stage of the algorithm, all image cap-
tions were searched for parenthesized expressions
that share the seed hypernyms. This step of the al-
gorithm will result in high precision, but a low re-
call since image markers do not necessarily appear
in parentheses. To increase recall, in stage 2 a full
text search was performed for the stemmed versions
of the expressions identified in stage 1.
A baseline measure was also computed for the
identification of the Image Marker Referents using a
simple heuristic - the coreferent of the Image Marker
is usually the closest Noun Phrase (NP). In the case
of parenthesized image markers, it is the closest NP
to the left of the image marker; in the case of non-
parenthesized image markers, the referent is usually
the complement of the verb; and in the case of pas-
sive voice, the NP preceding the verb phrase. The
Stanford parser was used to parse the sentences.
Table 2 summarizes the results validated against
the annotated dataset (excluding the 20 captions
used to identify the seed phrases). It appears that the
relatively low accuracy for Image Marker Referent
identification was mostly due to parsing errors since
</bodyText>
<table confidence="0.999740333333333">
Precision Recall F1-score
Image Marker 87.70 68.10 76.66
Image Marker Referent Accuracy 59.10
</table>
<tableCaption confidence="0.911488">
Table 2: Rule-based approach results for Image Marker and Im-
age Marker Referent identification. Image Marker Referent results are
reported as accuracy because the algorithm involves locating an Image
Marker Referent for each Image Marker. Referent identification accu-
racy was computed for all annotated Image Markers.
</tableCaption>
<table confidence="0.999826333333333">
Kind k-5 ... k0 ... k+5
Orth o-5 ... o0 ... o+5
Stem s-5 ... s0 ... s+5
Hypernym h-5 ... h0 ... h+5
Dep Path d-5 ... d0 ... d+5
Category [c0]
</table>
<tableCaption confidence="0.969807333333333">
Table 3: Features from a surrounding token window are used to
classify the current token into category [c0]. Best results were achieved
with a five-token window.
</tableCaption>
<bodyText confidence="0.999555333333333">
the syntactic structure of the image caption texts is
quite distinct from the Penn Treebank dataset used
for training the Stanford parser.
</bodyText>
<subsectionHeader confidence="0.999824">
4.3 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999627956521739">
Next we explored the possibility of improving the
rule-based method results by applying a machine
learning technique on the set of annotated data. Sup-
port Vector Machines (SVM) (Vapnik, 2000) was
the approach taken because it is a state-of-the-art
classification approach proven to perform well on
many NLP tasks.
In our approach, each sentence was tokenized,
and tokens were classified as Beginning, Inside, or
Outside an Image Marker type or Image Marker Ref-
erent. Image Marker Referents are not related to Im-
age Markers and creating a classifier trained on this
task is planned as future work. SVM classifiers were
trained for each of these categories, and combined
via ‘one-vs-all’ classification (the category of the
classifier with the largest output was selected). Fea-
tures of the surrounding context are used as shown
in Table 3 and Table 4.
Table 5 summarizes the results of a 10-fold cross-
validation. SVM performed well overall for iden-
tifying Image Markers, Location being the hardest
because of higher variability of expressing ROI posi-
tion. Image Marker Referents are harder to classify,
</bodyText>
<page confidence="0.999125">
43
</page>
<table confidence="0.998900666666667">
Token Kind The general type of the sentence to-
ken (Word, Number, Symbol, Punctuation,
White space).
Orthography Orthographic categorization of the token
(Upper initial, All capitals, Lower case,
Mixed case).
Stem The stem of the token, extracted with the
Porter stemmer.
Wordnet Super- Wordnet hypernyms (nouns, verbs); the hy-
class pernym of the derivationally related form
(adjectives); the superclass of the pertanym
(adverbs).
POS Category POS categories extracted using Brill’s tag-
ger.
Dependency The smallest sentence parse subtree includ-
Path* ing both the current token and the anno-
tated image marker(s), encoded as an undi-
rected path across POS categories.
</table>
<tableCaption confidence="0.989773333333333">
Table 4: Orthographic, semantic, and grammatical classification
features computed for each token (*Dependency Path is used only for
classifying Image Marker Referents).
</tableCaption>
<bodyText confidence="0.9999284">
as deeper syntactic knowledge is necessary. Idiosyn-
cratic syntactic structures in image captions pose
a problem for the general-purpose trained Stanford
parser and performance is hindered by the accuracy
of computing Dependency Path feature.
</bodyText>
<sectionHeader confidence="0.990524" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999987947368421">
We explored the feasibility of determining the con-
tent of ROIs in images from scientific publications
using image captions. We developed a two-stage
rule-based approach that utilizes WordNet to find
ROI pointers (Image Markers) and their referents.
We also explored a supervised machine learning ap-
proach. Both approaches are promising. The rule-
based approach seeded with a small manually an-
notated set resulted in 78.7% precision and 68.1%
recall for Image Markers recognition. The SVM ap-
proach (which requires a greater annotation effort)
outperformed the rule based approach (p=93.6%,
r=87.7%). Future plans include training SVMs on
the results of the rule-based annotation. Further
work is also needed in improving Image Marker
Referent identification and co-reference resolution.
We also plan to involve two annotators in order
to collect a more robust dataset based on inter-
annotator agreement.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9458495">
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
</reference>
<table confidence="0.999886857142857">
Precision Recall F1-score
Location 60.93 45.15 51.86
Color 100.00 51.32 67.82
Overlay Marker 97.43 95.39 96.39
Overlay Label 85.74 87.69 86.70
Overall 93.64 87.69 90.56
Image Marker Referent Accuracy 61.15
</table>
<tableCaption confidence="0.828755">
Table 5: SVM classification results for the four types of Image
Markers, and for Image Marker Referents. LibSVM software was used
(3-degree polynomial kernel, cost parameter = 1, τ = 0.6 empirically
determined).
</tableCaption>
<reference confidence="0.999214459459459">
pictures. The Journal of Machine Learning Research,
3:1107–1135.
W.W. Cohen, R. Wang, and R.F. Murphy. 2003. Un-
derstanding captions in biomedical publications. In
Proceedings of the 9th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 499–504. ACM New York, NY, USA.
P.T. Davis, D.K. Elson, and J.L. Klavans. 2003. Methods
for precise named entity matching in digital collec-
tions. In Proceedings of the 3rd ACM/IEEE-CS joint
conference on Digital libraries, pages 125–127. IEEE
Computer Society Washington, DC, USA.
K. Deschacht and M. Moens. 2007. Text analysis for au-
tomatic image annotation. In Proceedings of the 45th
Annual ACL Meeting, pages 1000–1007. ACL.
P. Duygulu, K. Barnard, JFG de Freitas, and D.A.
Forsyth. 2002. Object Recognition as Machine Trans-
lation: Learning a Lexicon for a Fixed Image Vocab-
ulary. LECTURE NOTES IN COMPUTER SCIENCE,
pages 97–112.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Au-
tomatic image annotation and retrieval using cross-
media relevance models. In Proceedings of the 26th
annual international ACM SIGIR conference on Re-
search and development in informaion retrieval, pages
119–126. ACM New York, NY, USA.
D. Rubin, P. Mongkolwat, V. Kleper, K. Supekar, and
D. Channin. 2008. Medical imaging on the Semantic
Web: Annotation and image markup. In AAAI Spring
Symposium Series, Semantic Scientific Knowledge In-
tegration.
B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Free-
man. 2008. LabelMe: A Database and Web-Based
Tool for Image Annotation. International Journal of
Computer Vision, 77(1):157–173.
V.N. Vapnik. 2000. The Nature of Statistical Learning
Theory. Springer.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552958">
<title confidence="0.999453">Towards Automatic Image Region Annotation - Image Region Coreference Resolution</title>
<author confidence="0.973177">Emilia</author>
<affiliation confidence="0.9265575">College of Computing and Digital DePaul</affiliation>
<address confidence="0.814424">Chicago, IL 60604,</address>
<email confidence="0.999101">emilia.aposto@gmail.com</email>
<author confidence="0.920022">Dina</author>
<affiliation confidence="0.9514415">Communications Engineering National Library of</affiliation>
<address confidence="0.995313">Bethesda, MD 20894,</address>
<email confidence="0.997957">ddemner@mail.nih.gov</email>
<abstract confidence="0.9967178">Detailed image annotation necessary for reliable image retrieval involves not only annotating the image as a single artifact, but also annotating specific objects or regions within the image. Such detailed annotation is a costly endeavor and the available annotated image data are quite limited. This paper explores the feasibility of using image captions from scientific journals for the purpose of automatically annotating image regions. Salient image clues, such as an object location within the image or an object color, together with the associated explicit object mention, are extracted and classified using rule-based and SVM learners.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>D Forsyth</author>
<author>N de Freitas</author>
<author>D M Blei</author>
<author>M I Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1107</pages>
<marker>Barnard, Duygulu, Forsyth, de Freitas, Blei, Jordan, 2003</marker>
<rawString>K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D.M. Blei, and M.I. Jordan. 2003. Matching words and pictures. The Journal of Machine Learning Research, 3:1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>R Wang</author>
<author>R F Murphy</author>
</authors>
<title>Understanding captions in biomedical publications.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>499--504</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6222" citStr="Cohen et al (2003)" startWordPosition="993" endWordPosition="996">f the image object boundaries. This task could be broken down into two related subtasks - 1) locating and classifying textual clues for visually salient ROI features (Image Markers), and 2) locating the corresponding ROI text mentions (Image Marker Referents). Table 1 gives a classification of Image Markers including examples of Image Markers and Image Marker Referents. Figure 1 shows the frequency of Image Marker occurrences. 1The captions were extracted from Radiology and Radiographics © Radiological Society of North America and Oceanus © Woods Hole Oceanographic Institution. 3 Related Work Cohen et al (2003) attempt to identify what they refer to as “image pointers” within captions in biomedical publications. The image pointers of interest are, for example, image panel labels, or letters and abbreviations used as an overlay within the image, similar to the Overlay Labels described in Table 1. They developed a set of hand-crafted rules, and a learning method involving Boosted Wrapper Induction on a dataset consisting of biomedical articles related to fluorescence microscope images. Deschacht and Moens (2007) analyze text surrounding images in news articles trying to identify persons and objects in</context>
</contexts>
<marker>Cohen, Wang, Murphy, 2003</marker>
<rawString>W.W. Cohen, R. Wang, and R.F. Murphy. 2003. Understanding captions in biomedical publications. In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 499–504. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P T Davis</author>
<author>D K Elson</author>
<author>J L Klavans</author>
</authors>
<title>Methods for precise named entity matching in digital collections.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd ACM/IEEE-CS joint conference on Digital libraries,</booktitle>
<pages>125--127</pages>
<publisher>IEEE Computer Society</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7188" citStr="Davis et al (2003)" startWordPosition="1150" endWordPosition="1153">g method involving Boosted Wrapper Induction on a dataset consisting of biomedical articles related to fluorescence microscope images. Deschacht and Moens (2007) analyze text surrounding images in news articles trying to identify persons and objects in the text that appear in the corresponding image. They start by extracting persons’ names and visual objects using Named Entity Recognition (NER) tools. Next, they measure the “salience” of the extracted named entities within the text with the assumption that more salient named entities in the text will also be present in the accompanying image. Davis et al (2003) develop a NER tool to identify references to a single art object (for example a specific building within an image) in text related to art images for the purpose of automatic cataloging of images. They take a semi-supervised approach to locating the named entities of interest by first providing an authoritative list of art objects of interest and then seeking to match variants of the seed named entities in related text. 4 Experimental Methods and Results 4.1 Dataset The chosen dateset contains more than 60,000 images together with their associated captions from three online life and earth scie</context>
</contexts>
<marker>Davis, Elson, Klavans, 2003</marker>
<rawString>P.T. Davis, D.K. Elson, and J.L. Klavans. 2003. Methods for precise named entity matching in digital collections. In Proceedings of the 3rd ACM/IEEE-CS joint conference on Digital libraries, pages 125–127. IEEE Computer Society Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Deschacht</author>
<author>M Moens</author>
</authors>
<title>Text analysis for automatic image annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual ACL Meeting,</booktitle>
<pages>1000--1007</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6731" citStr="Deschacht and Moens (2007)" startWordPosition="1073" endWordPosition="1076">ogical Society of North America and Oceanus © Woods Hole Oceanographic Institution. 3 Related Work Cohen et al (2003) attempt to identify what they refer to as “image pointers” within captions in biomedical publications. The image pointers of interest are, for example, image panel labels, or letters and abbreviations used as an overlay within the image, similar to the Overlay Labels described in Table 1. They developed a set of hand-crafted rules, and a learning method involving Boosted Wrapper Induction on a dataset consisting of biomedical articles related to fluorescence microscope images. Deschacht and Moens (2007) analyze text surrounding images in news articles trying to identify persons and objects in the text that appear in the corresponding image. They start by extracting persons’ names and visual objects using Named Entity Recognition (NER) tools. Next, they measure the “salience” of the extracted named entities within the text with the assumption that more salient named entities in the text will also be present in the accompanying image. Davis et al (2003) develop a NER tool to identify references to a single art object (for example a specific building within an image) in text related to art imag</context>
</contexts>
<marker>Deschacht, Moens, 2007</marker>
<rawString>K. Deschacht and M. Moens. 2007. Text analysis for automatic image annotation. In Proceedings of the 45th Annual ACL Meeting, pages 1000–1007. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Duygulu</author>
<author>K Barnard</author>
<author>JFG de Freitas</author>
<author>D A Forsyth</author>
</authors>
<title>Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary.</title>
<date>2002</date>
<journal>LECTURE NOTES IN COMPUTER SCIENCE,</journal>
<pages>97--112</pages>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>P. Duygulu, K. Barnard, JFG de Freitas, and D.A. Forsyth. 2002. Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary. LECTURE NOTES IN COMPUTER SCIENCE, pages 97–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>V Lavrenko</author>
<author>R Manmatha</author>
</authors>
<title>Automatic image annotation and retrieval using crossmedia relevance models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>119--126</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1537" citStr="Jeon et al., 2003" startWordPosition="234" endWordPosition="237">ntion, are extracted and classified using rule-based and SVM learners. 1 Introduction The profusion of digitally available images has naturally led to an interest in the field of automatic image annotation and retrieval. A number of studies attempt to associate image regions with the corresponding concepts. In (Duygulu et al., 2002), for example, the problem of annotation is treated as a translation from a set of image segments (or blobs) to a set of words. Modeling the association between blobs and words for the purpose of automated annotation has also been proposed by (Barnard et al., 2003; Jeon et al., 2003). A recurring hindrance that appears in studies aiming at automatic image region annotation is the lack of an appropriate dataset. All of the above studies use the Corel image dataset that consists of 60,000 images annotated with 3 to 5 keywords. The need for an image dataset with annotated image regions 41 has been recognized by many researchers. For example, Russell et al (2008) have developed a tool and a general purpose image database designed to delineate and annotate objects within image scenes. The need for an image dataset with annotated object boundaries appears to be especially perti</context>
</contexts>
<marker>Jeon, Lavrenko, Manmatha, 2003</marker>
<rawString>J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Automatic image annotation and retrieval using crossmedia relevance models. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 119–126. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rubin</author>
<author>P Mongkolwat</author>
<author>V Kleper</author>
<author>K Supekar</author>
<author>D Channin</author>
</authors>
<title>Medical imaging on the Semantic Web: Annotation and image markup.</title>
<date>2008</date>
<booktitle>In AAAI Spring Symposium Series, Semantic Scientific Knowledge Integration.</booktitle>
<contexts>
<context position="2315" citStr="Rubin et al (2008)" startWordPosition="366" endWordPosition="369">the Corel image dataset that consists of 60,000 images annotated with 3 to 5 keywords. The need for an image dataset with annotated image regions 41 has been recognized by many researchers. For example, Russell et al (2008) have developed a tool and a general purpose image database designed to delineate and annotate objects within image scenes. The need for an image dataset with annotated object boundaries appears to be especially pertinent in the biomedical field. Organizing and using for research the available medical imaging data proved to be a challenge and a goal of the ongoing research. Rubin et al (2008), for example, propose an ontology and annotation tool for semantic annotation of image regions in radiology. However, creating a dataset of image regions manually annotated and delineated by domain experts, is a costly enterprise. Any attempts to automate or semi-automate the process would be of a substantial value. This work proposes an approach towards automatic annotation of regions of interest in images used in scientific publications. Publications abundant in image data are an untapped source of annotated image data. Due to publication standards, meaningful image captions are almost alwa</context>
</contexts>
<marker>Rubin, Mongkolwat, Kleper, Supekar, Channin, 2008</marker>
<rawString>D. Rubin, P. Mongkolwat, V. Kleper, K. Supekar, and D. Channin. 2008. Medical imaging on the Semantic Web: Annotation and image markup. In AAAI Spring Symposium Series, Semantic Scientific Knowledge Integration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Russell</author>
<author>A Torralba</author>
<author>K P Murphy</author>
<author>W T Freeman</author>
</authors>
<title>LabelMe: A Database and Web-Based Tool for Image Annotation.</title>
<date>2008</date>
<journal>International Journal of Computer Vision,</journal>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="1920" citStr="Russell et al (2008)" startWordPosition="300" endWordPosition="303">ated as a translation from a set of image segments (or blobs) to a set of words. Modeling the association between blobs and words for the purpose of automated annotation has also been proposed by (Barnard et al., 2003; Jeon et al., 2003). A recurring hindrance that appears in studies aiming at automatic image region annotation is the lack of an appropriate dataset. All of the above studies use the Corel image dataset that consists of 60,000 images annotated with 3 to 5 keywords. The need for an image dataset with annotated image regions 41 has been recognized by many researchers. For example, Russell et al (2008) have developed a tool and a general purpose image database designed to delineate and annotate objects within image scenes. The need for an image dataset with annotated object boundaries appears to be especially pertinent in the biomedical field. Organizing and using for research the available medical imaging data proved to be a challenge and a goal of the ongoing research. Rubin et al (2008), for example, propose an ontology and annotation tool for semantic annotation of image regions in radiology. However, creating a dataset of image regions manually annotated and delineated by domain expert</context>
</contexts>
<marker>Russell, Torralba, Murphy, Freeman, 2008</marker>
<rawString>B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. 2008. LabelMe: A Database and Web-Based Tool for Image Annotation. International Journal of Computer Vision, 77(1):157–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>2000</date>
<publisher>Springer.</publisher>
<contexts>
<context position="11470" citStr="Vapnik, 2000" startWordPosition="1857" endWordPosition="1858">tem s-5 ... s0 ... s+5 Hypernym h-5 ... h0 ... h+5 Dep Path d-5 ... d0 ... d+5 Category [c0] Table 3: Features from a surrounding token window are used to classify the current token into category [c0]. Best results were achieved with a five-token window. the syntactic structure of the image caption texts is quite distinct from the Penn Treebank dataset used for training the Stanford parser. 4.3 Support Vector Machines Next we explored the possibility of improving the rule-based method results by applying a machine learning technique on the set of annotated data. Support Vector Machines (SVM) (Vapnik, 2000) was the approach taken because it is a state-of-the-art classification approach proven to perform well on many NLP tasks. In our approach, each sentence was tokenized, and tokens were classified as Beginning, Inside, or Outside an Image Marker type or Image Marker Referent. Image Marker Referents are not related to Image Markers and creating a classifier trained on this task is planned as future work. SVM classifiers were trained for each of these categories, and combined via ‘one-vs-all’ classification (the category of the classifier with the largest output was selected). Features of the sur</context>
</contexts>
<marker>Vapnik, 2000</marker>
<rawString>V.N. Vapnik. 2000. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>