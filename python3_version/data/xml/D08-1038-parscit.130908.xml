<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.983722">
Studying the History of Ideas Using Topic Models
</title>
<author confidence="0.815586">
David Hall
</author>
<affiliation confidence="0.761598">
Symbolic Systems
Stanford University
</affiliation>
<address confidence="0.854816">
Stanford, CA 94305, USA
</address>
<email confidence="0.998565">
dlwh@stanford.edu
</email>
<author confidence="0.773685">
Daniel Jurafsky
</author>
<affiliation confidence="0.6967435">
Linguistics
Stanford University
</affiliation>
<address confidence="0.756794">
Stanford, CA 94305, USA
</address>
<email confidence="0.998596">
jurafsky@stanford.edu
</email>
<author confidence="0.993133">
Christopher D. Manning
</author>
<affiliation confidence="0.985061">
Computer Science
Stanford University
</affiliation>
<address confidence="0.747437">
Stanford, CA 94305, USA
</address>
<email confidence="0.99926">
manning@stanford.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999555363636364">
How can the development of ideas in a sci-
entific field be studied over time? We ap-
ply unsupervised topic modeling to the ACL
Anthology to analyze historical trends in the
field of Computational Linguistics from 1978
to 2006. We induce topic clusters using Latent
Dirichlet Allocation, and examine the strength
of each topic over time. Our methods find
trends in the field including the rise of prob-
abilistic methods starting in 1988, a steady in-
crease in applications, and a sharp decline of
research in semantics and understanding be-
tween 1978 and 2001, possibly rising again
after 2001. We also introduce a model of the
diversity of ideas, topic entropy, using it to
show that COLING is a more diverse confer-
ence than ACL, but that both conferences as
well as EMNLP are becoming broader over
time. Finally, we apply Jensen-Shannon di-
vergence of topic distributions to show that all
three conferences are converging in the topics
they cover.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982113636363">
How can we identify and study the exploration of
ideas in a scientific field over time, noting periods of
gradual development, major ruptures, and the wax-
ing and waning of both topic areas and connections
with applied topics and nearby fields? One im-
portant method is to make use of citation graphs
(Garfield, 1955). This enables the use of graph-
based algorithms like PageRank for determining re-
searcher or paper centrality, and examining whether
their influence grows or diminishes over time.
However, because we are particularly interested
in the change of ideas in a field over time, we have
chosen a different method, following Kuhn (1962).
In Kuhn’s model of scientific change, science pro-
ceeds by shifting from one paradigm to another.
Because researchers’ ideas and vocabulary are con-
strained by their paradigm, successive incommensu-
rate paradigms will naturally have different vocabu-
lary and framing.
Kuhn’s model is intended to apply only to very
large shifts in scientific thought rather than at the
micro level of trends in research foci. Nonetheless,
we propose to apply Kuhn’s insight that vocabulary
and vocabulary shift is a crucial indicator of ideas
and shifts in ideas. Our operationalization of this in-
sight is based on the unsupervised topic model La-
tent Dirichlet Allocation (LDA; Blei et al. (2003)).
For many fields, doing this kind of historical study
would be very difficult. Computational linguistics
has an advantage, however: the ACL Anthology, a
public repository of all papers in the Computational
Linguistics journal and the conferences and work-
shops associated with the ACL, COLING, EMNLP,
and so on. The ACL Anthology (Bird, 2008), and
comprises over 14,000 documents from conferences
and the journal, beginning as early as 1965 through
2008, indexed by conference and year. This re-
source has already been the basis of citation anal-
ysis work, for example, in the ACL Anthology Net-
work of Joseph and Radev (2007). We apply LDA
to the text of the papers in the ACL Anthology to
induce topics, and use the trends in these topics over
time and over conference venues to address ques-
tions about the development of the field.
</bodyText>
<page confidence="0.989214">
363
</page>
<note confidence="0.9690895">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363–371,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<table confidence="0.998393">
Venue # Papers Years Frequency
Journal 1291 1974–Present Quarterly
ACL 2037 1979-Present Yearly
EACL 596 1983–Present ∼2 Years
NAACL 293 2000–Present ∼Yearly
Applied NLP 346 1983–2000 ∼3 Years
COLING 2092 1965-Present 2 Years
HLT 957 1986–Present ∼2 Years
Workshops 2756 1990-Present Yearly
TINLAP 128 1975–1987 Rarely
MUC 160 1991–1998 ∼2 Years
IJCNLP 143 2005 ——
Other 120 —— ——
</table>
<tableCaption confidence="0.999966">
Table 1: Data in the ACL Anthology
</tableCaption>
<bodyText confidence="0.99990465">
Despite the relative youth of our field, computa-
tional linguistics has witnessed a number of research
trends and shifts in focus. While some trends are
obvious (such as the rise in machine learning meth-
ods), others may be more subtle. Has the field got-
ten more theoretical over the years or has there been
an increase in applications? What topics have de-
clined over the years, and which ones have remained
roughly constant? How have fields like Dialogue or
Machine Translation changed over the years? Are
there differences among the conferences, for exam-
ple between COLING and ACL, in their interests
and breadth of focus? As our field matures, it is im-
portant to go beyond anecdotal description to give
grounded answers to these questions. Such answers
could also help give formal metrics to model the dif-
ferences between the many conferences and venues
in our field, which could influence how we think
about reviewing, about choosing conference topics,
and about long range planning in our field.
</bodyText>
<sectionHeader confidence="0.991456" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.94429">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.99749625">
The analyses in this paper are based on a text-
only version of the Anthology that comprises some
12,500 papers. The distribution of the Anthology
data is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.997142">
2.2 Topic Modeling
</subsectionHeader>
<bodyText confidence="0.999930135135135">
Our experiments employ Latent Dirichlet Allocation
(LDA; Blei et al. (2003)), a generative latent variable
model that treats documents as bags of words gener-
ated by one or more topics. Each document is char-
acterized by a multinomial distribution over topics,
and each topic is in turn characterized by a multino-
mial distribution over words. We perform parame-
ter estimation using collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004).
Possible extensions to this model would be to in-
tegrate topic modelling with citations (e.g., Dietz et
al. (2007), Mann et al. (2006), and Jo et al. (2007)).
Another option is the use of more fine-grained or hi-
erarchical model (e.g., Blei et al. (2004), and Li and
McCallum (2006)).
All our studies measure change in various as-
pects of the ACL Anthology over time. LDA, how-
ever, does not explicitly model temporal relation-
ships. One way to model temporal relationships is
to employ an extension to LDA. The Dynamic Topic
Model (Blei and Lafferty, 2006), for example, rep-
resents each year’s documents as generated from a
normal distribution centroid over topics, with the
following year’s centroid generated from the pre-
ceding year’s. The Topics over Time Model (Wang
and McCallum, 2006) assumes that each document
chooses its own time stamp based on a topic-specific
beta distribution.
Both of these models, however, impose con-
straints on the time periods. The Dynamic Topic
Model penalizes large changes from year to year
while the beta distributions in Topics over Time are
relatively inflexible. We chose instead to perform
post hoc calculations based on the observed proba-
bility of each topic given the current year. We define
p(z|y) as the empirical probability that an arbitrary
paper d written in year y was about topic z:
</bodyText>
<equation confidence="0.980140833333333">
��(z|y) = E p(z|d)p(d|y)
d:td=y
1 C p(z|d) (1)
d:td=y
1 C E I(z&apos; � = z)
d:td=y z&apos;Ed
</equation>
<bodyText confidence="0.999401">
where I is the indicator function, td is the date docu-
ment d was written, p(d|y) is set to a constant 1/C.
</bodyText>
<sectionHeader confidence="0.916449" genericHeader="method">
3 Summary of Topics
</sectionHeader>
<bodyText confidence="0.996484">
We first ran LDA with 100 topics, and took 36 that
we found to be relevant. We then hand-selected seed
</bodyText>
<page confidence="0.996494">
364
</page>
<figure confidence="0.871104">
1
</figure>
<figureCaption confidence="0.98476">
Figure 1: Topics in the ACL Anthology that show a
strong recent increase in strength.
</figureCaption>
<bodyText confidence="0.999870166666667">
words for 10 more topics to improve coverage of the
field. These 46 topics were then used as priors to a
new 100-topic run. The top ten most frequent words
for 43 of the topics along with hand-assigned labels
are listed in Table 2. Topics deriving from manual
seeds are marked with an asterisk.
</bodyText>
<sectionHeader confidence="0.9272985" genericHeader="method">
4 Historical Trends in Computational
Linguistics
</sectionHeader>
<bodyText confidence="0.999992">
Given the space of possible topics defined in the pre-
vious section, we now examine the history of these
in the entire ACL Anthology from 1978 until 2006.
To visualize some trends, we show the probability
mass associated with various topics over time, plot-
ted as (a smoothed version of) p(zly).
</bodyText>
<subsectionHeader confidence="0.998971">
4.1 Topics Becoming More Prominent
</subsectionHeader>
<bodyText confidence="0.998913">
Figure 1 shows topics that have become more promi-
nent more recently.
Of these new topics, the rise in probabilistic mod-
els and classification/tagging is unsurprising. In or-
der to distinguish these two topics, we show 20 of
the strongly weighted words:
</bodyText>
<construct confidence="0.949242285714286">
Probabilistic Models: model word probability set data
number algorithm language corpus method figure proba-
bilities table test statistical distribution function al values
performance
Classification/Tagging: features data corpus set feature
table word tag al test accuracy pos classification perfor-
mance tags tagging text task information class
</construct>
<bodyText confidence="0.9845495">
Some of the papers with the highest weights for
the probabilistic models class include:
</bodyText>
<reference confidence="0.698350129032258">
N04-1039 Goodman, Joshua. Exponential Priors For Maximum
Entropy Models (HLT-NAACL, 2004)
W97-0309 Saul, Lawrence, Pereira, Fernando C. N. Aggregate And
Mixed-Order Markov Models For Statistical Language
Processing (EMNLP, 1997)
P96-1041 Chen, Stanley F., Goodman, Joshua. An Empirical
Study Of Smoothing Techniques For Language Model-
ing (ACL, 1996)
H89-2013 Church, Kenneth Ward, Gale, William A. Enhanced
Good-Turing And CatCal: Two New Methods For Esti-
mating Probabilities Of English Bigrams (Workshop On
Speech And Natural Language, 1989)
P02-1023 Gao, Jianfeng, Zhang, Min Improving Language Model
Size Reduction Using Better Pruning Criteria (ACL,
2002)
P94-1038 Dagan, Ido, Pereira, Fernando C. N. Similarity-Based
Estimation Of Word Cooccurrence Probabilities (ACL,
1994)
Some of the papers with the highest weights for
the classification/tagging class include:
W00-0713 Van Den Bosch, Antal Using Induced Rules As Com-
plex Features In Memory-Based Language Learning
(CoNLL, 2000)
W01-0709 Estabrooks, Andrew, Japkowicz, Nathalie A Mixture-Of-
Experts Framework For Text Classification (Workshop
On Computational Natural Language Learning CoNLL,
2001)
A00-2035 Mikheev, Andrei. Tagging Sentence Boundaries (ANLP-
NAACL, 2000)
H92-1022 Brill, Eric. A Simple Rule-Based Part Of Speech Tagger
(Workshop On Speech And Natural Language, 1992)
</reference>
<bodyText confidence="0.999481111111111">
As Figure 1 shows, probabilistic models seem to
have arrived significantly before classifiers. The
probabilistic model topic increases around 1988,
which seems to have been an important year for
probabilistic models, including high-impact papers
like A88-1019 and C88-1016 below. The ten papers
from 1988 with the highest weights for the proba-
bilistic model and classifier topics were the follow-
ing:
</bodyText>
<reference confidence="0.79137245">
C88-1071 Kuhn, Roland. Speech Recognition and the Frequency
of Recently Used Words (COLING)
J88-1003 DeRose, Steven. Grammatical Category Disambiguation
by Statistical Optimization. (CL Journal)
C88-2133 Su, Keh-Yi, and Chang, Jing-Shin. Semantic and Syn-
tactic Aspects of Score Function. (COLING)
A88-1019 Church, Kenneth Ward. A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text. (ANLP)
C88-2134 Sukhotin, B.V. Optimization Algorithms of Deciphering
as the Elements of a Linguistic Theory. (COLING)
P88-1013 Haigh, Robin, Sampson, Geoffrey, and Atwell, Eric.
Project APRIL: a progress report. (ACL)
A88-1005 Boggess, Lois. Two Simple Prediction Algorithms to Fa-
cilitate Text Production. (ANLP)
C88-1016 Peter F. Brown, et al. A Statistical Approach to Machine
Translation. (COLING)
A88-1028 Oshika, Beatrice, et al.. Computational Techniques for
Improved Name Search. (ANLP)
C88-1020 Campbell, W.N. Speech-rate Variation and the Prediction
of Duration. (COLING)
</reference>
<figure confidence="0.925681">
What do these early papers tell us about how
0.2
0.15
0.1
0.05
0
</figure>
<page confidence="0.986351">
365
</page>
<table confidence="0.996559232558139">
Anaphora Resolution resolution anaphora pronoun discourse antecedent pronouns coreference reference definite algorithm
Automata string state set finite context rule algorithm strings language symbol
Biomedical medical protein gene biomedical wkh abstracts medline patient clinical biological
Call Routing call caller routing calls destination vietnamese routed router destinations gorin
Categorial Grammar proof formula graph logic calculus axioms axiom theorem proofs lambek
Centering* centering cb discourse cf utterance center utterances theory coherence entities local
Classical MT japanese method case sentence analysis english dictionary figure japan word
Classification/Tagging features data corpus set feature table word tag al test
Comp. Phonology vowel phonological syllable phoneme stress phonetic phonology pronunciation vowels phonemes
Comp. Semantics* semantic logical semantics john sentence interpretation scope logic form set
Dialogue Systems user dialogue system speech information task spoken human utterance language
Discourse Relations discourse text structure relations rhetorical relation units coherence texts rst
Discourse Segment. segment segmentation segments chain chains boundaries boundary seg cohesion lexical
Events/Temporal event temporal time events tense state aspect reference relations relation
French Function de le des les en une est du par pour
Generation generation text system language information knowledge natural figure domain input
Genre Detection genre stylistic style genres fiction humor register biber authorship registers
Info. Extraction system text information muc extraction template names patterns pattern domain
Information Retrieval document documents query retrieval question information answer term text web
Lexical Semantics semantic relations domain noun corpus relation nouns lexical ontology patterns
MUC Terrorism slot incident tgt target id hum phys type fills perp
Metaphor metaphor literal metonymy metaphors metaphorical essay metonymic essays qualia analogy
Morphology word morphological lexicon form dictionary analysis morphology lexical stem arabic
Named Entities* entity named entities ne names ner recognition ace nes mentions mention
Paraphrase/RTE paraphrases paraphrase entailment paraphrasing textual para rte pascal entailed dagan
Parsing parsing grammar parser parse rule sentence input left grammars np
Plan-Based Dialogue plan discourse speaker action model goal act utterance user information
Probabilistic Models model word probability set data number algorithm language corpus method
Prosody prosodic speech pitch boundary prosody phrase boundaries accent repairs intonation
Semantic Roles* semantic verb frame argument verbs role roles predicate arguments
Yale School Semantics knowledge system semantic language concept representation information network concepts base
Sentiment subjective opinion sentiment negative polarity positive wiebe reviews sentence opinions
Speech Recognition speech recognition word system language data speaker error test spoken
Spell Correction errors error correction spelling ocr correct corrections checker basque corrected detection
Statistical MT english word alignment language source target sentence machine bilingual mt
Statistical Parsing dependency parsing treebank parser tree parse head model al np
Summarization sentence text evaluation document topic summary summarization human summaries score
Syntactic Structure verb noun syntactic sentence phrase np subject structure case clause
TAG Grammars* tree node trees nodes derivation tag root figure adjoining grammar
Unification feature structure grammar lexical constraints unification constraint type structures rule
WSD* word senses wordnet disambiguation lexical semantic context similarity dictionary
Word Segmentation chinese word character segmentation corpus dictionary korean language table system
WordNet* synset wordnet synsets hypernym ili wordnets hypernyms eurowordnet hyponym ewn wn
</table>
<tableCaption confidence="0.999298">
Table 2: Top 10 words for 43 of the topics. Starred topics are hand-seeded.
</tableCaption>
<page confidence="0.972025">
366
</page>
<figure confidence="0.998478166666667">
0.2
0.15
0.1
0.05
0
1
</figure>
<figureCaption confidence="0.9855405">
Figure 2: Topics in the ACL Anthology that show a
strong decline from 1978 to 2006.
</figureCaption>
<bodyText confidence="0.999231791666667">
probabilistic models and classifiers entered the
field? First, not surprisingly, we note that the vast
majority (9 of 10) of the papers appeared in con-
ference proceedings rather than the journal, con-
firming that in general new ideas appear in confer-
ences. Second, of the 9 conference papers, most
of them appeared in the COLING conference (5) or
the ANLP workshop (3) compared to only 1 in the
ACL conference. This suggests that COLING may
have been more receptive than ACL to new ideas
at the time, a point we return to in Section 6. Fi-
nally, we examined the background of the authors of
these papers. Six of the 10 papers either focus on
speech (C88-1010, A88-1028, C88-1071) or were
written by authors who had previously published on
speech recognition topics, including the influential
IBM (Brown et al.) and AT&amp;T (Church) labs (C88-
1016, A88-1005, A88-1019). Speech recognition
is historically an electrical engineering field which
made quite early use of probabilistic and statistical
methodologies. This suggests that researchers work-
ing on spoken language processing were an impor-
tant conduit for the borrowing of statistical method-
ologies into computational linguistics.
</bodyText>
<subsectionHeader confidence="0.970027">
4.2 Topics That Have Declined
</subsectionHeader>
<bodyText confidence="0.8933898">
Figure 2 shows several topics that were more promi-
nent at the beginning of the ACL but which have
shown the most precipitous decline. Papers strongly
associated with the plan-based dialogue topic in-
clude:
</bodyText>
<table confidence="0.96486672">
J99-1001 Carberry, Sandra, Lambert, Lynn. A Process Model For
Recognizing Communicative Acts And Modeling Nego-
tiation Subdialogues (CL, 1999)
J95-4001 McRoy, Susan W., Hirst, Graeme. The Repair Of Speech
Act Misunderstandings By Abductive Inference (CL,
1995)
P93-1039 Chu, Jennifer. Responding To User Queries In A Collab-
orative Environment (ACL, 1993)
P86-1032 Pollack, Martha E. A Model Of Plan Inference That
Distinguishes Between The Beliefs Of Actors And Ob-
servers (ACL, 1986)
T78-1017 Perrault, Raymond C., Allen, James F. Speech Acts As
A Basis For Understanding Dialogue Coherence (Theo-
retical Issues In Natural Language Processing, 1978)
P84-1063 Litman, Diane J., Allen, James F. A Plan Recognition
Model For Clarification Subdialogues (COLING-ACL,
1984)
Papers strongly associated with the computational
semantics topic include:
J90-4002 Haas, Andrew R. Sentential Semantics For Propositional
Attitudes (CL, 1990)
P83-1009 Hobbs, Jerry R. An Improper Treatment Of Quantifica-
tion In Ordinary English (ACL, 1983)
J87-1005 Hobbs, Jerry R., Shieber, Stuart M. An Algorithm For
Generating Quantifier Scopings (CL, 1987)
C90-1003 Johnson, Mark, Kay, Martin. Semantic Abstraction And
Anaphora (COLING, 1990)
P89-1004 Alshawi, Hiyan, Van Eijck, Jan. Logical Forms In The
Core Language Engine (ACL, 1989)
Papers strongly associated with the conceptual se-
mantics/story understanding topic include:
C80-1022 Ogawa, Hitoshi, Nishi, Junichiro, Tanaka, Kokichi. The
Knowledge Representation For A Story Understanding
And Simulation System (COLING, 1980)
A83-1012 Pazzani, Michael J., Engelman, Carl. Knowledge Based
Question Answering (ANLP, 1983)
P82-1029 McCoy, Kathleen F. Augmenting A Database Knowl-
edge Representation For Natural Language Generation
(ACL, 1982)
H86-1010 Ksiezyk, Tomasz, Grishman, Ralph An Equipment
Model And Its Role In The Interpretation Of Nominal
Compounds (Workshop On Strategic Computing - Natu-
ral Language, 1986)
P80-1030 Wilensky, Robert, Arens, Yigal. PHRAN - A
Knowledge-Based Natural Language Understander
(ACL, 1980)
A83-1013 Boguraev, Branimir K., Sparck Jones, Karen. How To
Drive A Database Front End Using General Semantic In-
formation (ANLP, 1983)
P79-1003 Small, Steven L. Word Expert Parsing (ACL, 1979)
</table>
<bodyText confidence="0.9938216">
The declines in both computational semantics and
conceptual semantics/story understanding suggests
that it is possible that the entire field of natural lan-
guage understanding and computational semantics
broadly construed has fallen out of favor. To see
if this was in fact the case we created a metatopic
called semantics in which we combined various se-
mantics topics (not including pragmatic topics like
anaphora resolution or discourse coherence) includ-
ing: lexical semantics, conceptual semantics/story
</bodyText>
<page confidence="0.977783">
367
</page>
<figure confidence="0.997871625">
0.25 0.2
0.2 0.15
0.15 0.1
0.1 0.05
0.05
0
1 0
198
</figure>
<figureCaption confidence="0.9998905">
Figure 3: Semantics over time
Figure 4: Translation over time
</figureCaption>
<bodyText confidence="0.9932785">
understanding, computational semantics, WordNet,
word sense disambiguation, semantic role labeling,
RTE and paraphrase, MUC information extraction,
and events/temporal. We then plotted p(z E S|y),
the sum of the proportions per year for these top-
ics, as shown in Figure 3. The steep decrease in se-
mantics is readily apparent. The last few years has
shown a levelling off of the decline, and possibly a
revival of this topic; this possibility will need to be
confirmed as we add data from 2007 and 2008.
We next chose two fields, Dialogue and Machine
Translation, in which it seemed to us that the topics
discovered by LDA suggested a shift in paradigms in
these fields. Figure 4 shows the shift in translation,
while Figure 5 shows the change in dialogue.
The shift toward statistical machine translation is
well known, at least anecdotally. The shift in di-
alogue seems to be a move toward more applied,
speech-oriented, or commercial dialogue systems
and away from more theoretical models.
Finally, Figure 6 shows the history of several top-
ics that peaked at intermediate points throughout the
history of the field. We can see the peak of unifica-
tion around 1990, of syntactic structure around 1985
of automata in 1985 and again in 1997, and of word
sense disambiguation around 1998.
</bodyText>
<sectionHeader confidence="0.855533" genericHeader="method">
5 Is Computational Linguistics Becoming
More Applied?
</sectionHeader>
<bodyText confidence="0.999412">
We don’t know whether our field is becoming more
applied, or whether perhaps there is a trend to-
wards new but unapplied theories. We therefore
</bodyText>
<figure confidence="0.572411">
1980 1985 1990 1995 2000 2005
</figure>
<figureCaption confidence="0.998871">
Figure 5: Dialogue over time
</figureCaption>
<figure confidence="0.831631">
1980 1985 1990 1995 2000 2005
</figure>
<figureCaption confidence="0.998936">
Figure 6: Peaked topics
</figureCaption>
<figure confidence="0.988881714285714">
0.15
0.05
0.2
0.1
0
Dialogue Systems
Plan-Based Dialogue and Discourse
0.15
0.05
0.2
0.1
0
TAG
Generation
Automata
Unification
Syntactic Structure
Events
WSD
368
0.25 0.2
0.2 0.15
0.15 0.1
0.1 0.05
0.05
0
1 0
1980
</figure>
<figureCaption confidence="0.998635">
Figure 7: Applications over time
</figureCaption>
<figure confidence="0.73934">
1980 1985 1990 1995 2000 2005
</figure>
<figureCaption confidence="0.869445857142857">
Figure 8: Six applied topics over time
looked at trends over time for the following appli-
cations: Machine Translation, Spelling Correction,
Dialogue Systems, Information Retrieval, Call Rout-
ing, Speech Recognition, and Biomedical applica-
tions.
Figure 7 shows a clear trend toward an increase
</figureCaption>
<bodyText confidence="0.995345">
in applications over time. The figure also shows an
interesting bump near 1990. Why was there such
a sharp temporary increase in applications at that
time? Figure 8 shows details for each application,
making it clear that the bump is caused by a tempo-
rary spike in the Speech Recognition topic.
In order to understand why we see this temporary
spike, Figure 9 shows the unsmoothed values of the
Speech Recognition topic prominence over time.
</bodyText>
<figureCaption confidence="0.720321">
Figure 9 clearly shows a huge spike for the years
1989–1994. These years correspond exactly to the
DARPA Speech and Natural Language Workshop,
Figure 9: Speech recognition over time
</figureCaption>
<bodyText confidence="0.999809">
held at different locations from 1989–1994. That
workshop contained a significant amount of speech
until its last year (1994), and then it was revived
in 2001 as the Human Language Technology work-
shop with a much smaller emphasis on speech pro-
cessing. It is clear from Figure 9 that there is still
some speech research appearing in the Anthology
after 1995, certainly more than the period before
1989, but it’s equally clear that speech recognition
is not an application that the ACL community has
been successful at attracting.
</bodyText>
<sectionHeader confidence="0.985653" genericHeader="method">
6 Differences and Similarities Among
COLING, ACL, and EMNLP
</sectionHeader>
<bodyText confidence="0.999976235294118">
The computational linguistics community has two
distinct conferences, COLING and ACL, with dif-
ferent histories, organizing bodies, and philoso-
phies. Traditionally, COLING was larger, with par-
allel sessions and presumably a wide variety of top-
ics, while ACL had single sessions and a more nar-
row scope. In recent years, however, ACL has
moved to parallel sessions, and the conferences are
of similar size. Has the distinction in breadth of top-
ics also been blurred? What are the differences and
similarities in topics and trends between these two
conferences?
More recently, the EMNLP conference grew out
of the Workshop on Very Large Corpora, sponsored
by the Special Interest Group on Linguistic Data
and corpus-based approaches to NLP (SIGDAT).
EMNLP started as a much smaller and narrower
</bodyText>
<figure confidence="0.998614545454545">
0.15
0.05
0.2
0.1
0
Statistical MT
Dialogue Systems
Spelling Correction
Call Routing
Speech Recognition
Biomedical
</figure>
<page confidence="0.996326">
369
</page>
<bodyText confidence="0.99989295">
conference but more recently, while still smaller
than both COLING and ACL, it has grown large
enough to be considered with them. How does the
breadth of its topics compare with the others?
Our hypothesis, based on our intuitions as con-
ference attendees, is that ACL is still more narrow
in scope than COLING, but has broadened consid-
erably. Similarly, our hypothesis is that EMNLP has
begun to broaden considerably as well, although not
to the extent of the other two.
In addition, we’re interested in whether the topics
of these conferences are converging or not. Are the
probabilistic and machine learning trends that are
dominant in ACL becoming dominant in COLING
as well? Is EMNLP adopting some of the topics that
are popular at COLING?
To investigate both of these questions, we need a
model of the topic distribution for each conference.
We define the empirical distribution of a topic z at a
conference c, denoted by p(z|c) by:
</bodyText>
<equation confidence="0.9143636">
��(z|c) = E p(z|d)p(d|c)
d:cd=c
= 1 C d:cd=c p(z|d) (2)
= 1 C E I(z&apos; � = z)
d:cd=c zi&apos;Ed
</equation>
<bodyText confidence="0.999181">
We also condition on the year for each conference,
giving us p(z|y, c).
We propose to measure the breadth of a confer-
ence by using what we call topic entropy: the condi-
tional entropy of this conference topic distribution.
Entropy measures the average amount of informa-
tion expressed by each assignment to a random vari-
able. If a conference has higher topic entropy, then it
more evenly divides its probability mass across the
generated topics. If it has lower, it has a far more
narrow focus on just a couple of topics. We there-
fore measured topic entropy:
</bodyText>
<equation confidence="0.891436666666667">
K
H(z|c, y) = − p(zz|c, y) log p(zz|c, y) (3)
Z=i
</equation>
<bodyText confidence="0.466552666666667">
Figure 10 shows the conditional topic entropy
of each conference over time. We removed from
the ACL and COLING lines the years when ACL
</bodyText>
<figure confidence="0.989456">
5.6
5.4
5.2
5
4.8
4.6
4.4
4.2
4
3.8
3.6
198
</figure>
<figureCaption confidence="0.991939">
Figure 10: Entropy of the three major conferences per
year
</figureCaption>
<bodyText confidence="0.99995628">
and COLING are colocated (1984, 1998, 2006),
and marked those colocated years as points separate
from either plot. As expected, COLING has been
historically the broadest of the three conferences,
though perhaps slightly less so in recent years. ACL
started with a fairly narrow focus, but became nearly
as broad as COLING during the 1990’s. However, in
the past 8 years it has become more narrow again,
with a steeper decline in breadth than COLING.
EMNLP, true to its status as a “Special Interest” con-
ference, began as a very narrowly focused confer-
ence, but now it seems to be catching up to at least
ACL in terms of the breadth of its focus.
Since the three major conferences seem to be con-
verging in terms of breadth, we investigated whether
or not the topic distributions of the conferences were
also converging. To do this, we plotted the Jensen-
Shannon (JS) divergence between each pair of con-
ferences. The Jensen-Shannon divergence is a sym-
metric measure of the similarity of two pairs of dis-
tributions. The measure is 0 only for identical dis-
tributions and approaches infinity as the two differ
more and more. Formally, it is defined as the aver-
age of the KL divergence of each distribution to the
average of the two distributions:
</bodyText>
<equation confidence="0.94973925">
D�S(P ||Q) = �DKL(P ||R) + �DKL(Q||R)
R = �(P + Q)
1 1
1 (4)
</equation>
<bodyText confidence="0.787362">
Figure 11 shows the JS divergence between each
pair of conferences over time. Note that EMNLP
</bodyText>
<page confidence="0.993018">
370
</page>
<sectionHeader confidence="0.898504" genericHeader="method">
References
</sectionHeader>
<figureCaption confidence="0.986779">
Figure 11: JS Divergence between the three major con-
ferences
</figureCaption>
<bodyText confidence="0.999626714285714">
and COLING have historically met very infre-
quently in the same year, so those similarity scores
are plotted as points and not smoothed. The trend
across all three conferences is clear: each confer-
ence is not only increasing in breadth, but also in
similarity. In particular, EMNLP and ACL’s differ-
ences, once significant, are nearly erased.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999955">
Our method discovers a number of trends in the
field, such as the general increase in applications,
the steady decline in semantics, and its possible re-
versal. We also showed a convergence over time in
topic coverage of ACL, COLING, and EMNLP as
well an expansion of topic diversity. This growth
and convergence of the three conferences, perhaps
influenced by the need to increase recall (Church,
2005) seems to be leading toward a tripartite real-
ization of a single new “latent” conference.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990232">
Many thanks to Bryan Gibson and Dragomir Radev
for providing us with the data behind the ACL An-
thology Network. Also to Sharon Goldwater and the
other members of the Stanford NLP Group as well
as project Mimir for helpful advice. Finally, many
thanks to the Office of the President, Stanford Uni-
versity, for partial funding.
</bodyText>
<reference confidence="0.997649533333333">
Steven Bird. 2008. Association of Computational Lin-
guists Anthology. http://www.aclweb.org/anthology-
index/.
David Blei and John D. Lafferty. 2006. Dynamic topic
models. ICML.
David Blei, Andrew Ng, , and Michael Jordan. 2003. La-
tent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022.
D. Blei, T. Gri, M. Jordan, and J. Tenenbaum. 2004. Hi-
erarchical topic models and the nested Chinese restau-
rant process.
Kenneth Church. 2005. Reviewing the reviewers. Com-
put. Linguist., 31(4):575–578.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In
ICML, pages 233–240. ACM.
Eugene Garfield. 1955. Citation indexes to science: A
new dimension in documentation through association
of ideas. Science, 122:108–111.
Tom L. Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. PNAS, 101 Suppl 1:5228–5235, April.
Yookyung Jo, Carl Lagoze, and C. Lee Giles. 2007.
Detecting research topics via the correlation between
graphs and texts. In KDD, pages 370–379, New York,
NY, USA. ACM.
Mark T. Joseph and Dragomir R. Radev. 2007. Citation
analysis, centrality, and the ACL anthology. Techni-
cal Report CSE-TR-535-07, University of Michigan.
Department of Electrical Engineering and Computer
Science.
Thomas S. Kuhn. 1962. The Structure of Scientific Rev-
olutions. University Of Chicago Press.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In ICML, pages 577–584, New York, NY, USA.
ACM.
Gideon S. Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leveraging
topic analysis. In JCDL ’06: Proceedings of the 6th
ACM/IEEE-CS joint conference on Digital libraries,
pages 65–74, New York, NY, USA. ACM.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In KDD, pages 424–433, New York, NY, USA.
ACM.
</reference>
<figure confidence="0.996877">
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
198
EM
</figure>
<page confidence="0.965068">
371
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.351061">
<title confidence="0.999802">Studying the History of Ideas Using Topic Models</title>
<author confidence="0.950301">David</author>
<affiliation confidence="0.741418">Symbolic Stanford University</affiliation>
<address confidence="0.999898">Stanford, CA 94305, USA</address>
<email confidence="0.999851">dlwh@stanford.edu</email>
<author confidence="0.785838">Daniel</author>
<affiliation confidence="0.947694">Stanford</affiliation>
<address confidence="0.994113">Stanford, CA 94305,</address>
<email confidence="0.999788">jurafsky@stanford.edu</email>
<author confidence="0.999835">Christopher D Manning</author>
<affiliation confidence="0.999821">Computer Science Stanford University</affiliation>
<address confidence="0.999618">Stanford, CA 94305, USA</address>
<email confidence="0.999759">manning@stanford.edu</email>
<abstract confidence="0.998648043478261">How can the development of ideas in a scientific field be studied over time? We apply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the field of Computational Linguistics from 1978 to 2006. We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time. Our methods find trends in the field including the rise of probabilistic methods starting in 1988, a steady increase in applications, and a sharp decline of research in semantics and understanding between 1978 and 2001, possibly rising again after 2001. We also introduce a model of the of ideas, using it to show that COLING is a more diverse conference than ACL, but that both conferences as well as EMNLP are becoming broader over time. Finally, we apply Jensen-Shannon divergence of topic distributions to show that all three conferences are converging in the topics they cover.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N04-1039 Goodman</author>
<author>Joshua</author>
</authors>
<title>Exponential Priors For Maximum Entropy Models (HLT-NAACL,</title>
<date>2004</date>
<marker>Goodman, Joshua, 2004</marker>
<rawString>N04-1039 Goodman, Joshua. Exponential Priors For Maximum Entropy Models (HLT-NAACL, 2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>W97-0309 Saul</author>
<author>Pereira Lawrence</author>
<author>C N Fernando</author>
</authors>
<title>Aggregate And Mixed-Order Markov Models For Statistical Language Processing (EMNLP,</title>
<date>1997</date>
<marker>Saul, Lawrence, Fernando, 1997</marker>
<rawString>W97-0309 Saul, Lawrence, Pereira, Fernando C. N. Aggregate And Mixed-Order Markov Models For Statistical Language Processing (EMNLP, 1997)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P96-1041 Chen</author>
<author>F Stanley</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study Of Smoothing Techniques For Language Modeling (ACL,</title>
<date>1996</date>
<marker>Chen, Stanley, Goodman, 1996</marker>
<rawString>P96-1041 Chen, Stanley F., Goodman, Joshua. An Empirical Study Of Smoothing Techniques For Language Modeling (ACL, 1996)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H89-2013 Church</author>
<author>Kenneth Ward</author>
<author>William A Gale</author>
</authors>
<title>Enhanced Good-Turing And CatCal: Two New Methods For Estimating Probabilities Of English Bigrams (Workshop On Speech And Natural Language,</title>
<date>1989</date>
<marker>Church, Ward, Gale, 1989</marker>
<rawString>H89-2013 Church, Kenneth Ward, Gale, William A. Enhanced Good-Turing And CatCal: Two New Methods For Estimating Probabilities Of English Bigrams (Workshop On Speech And Natural Language, 1989)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P02-1023 Gao</author>
<author>Zhang Jianfeng</author>
</authors>
<title>Min Improving Language Model Size Reduction Using Better Pruning Criteria (ACL,</title>
<date>2002</date>
<marker>Gao, Jianfeng, 2002</marker>
<rawString>P02-1023 Gao, Jianfeng, Zhang, Min Improving Language Model Size Reduction Using Better Pruning Criteria (ACL, 2002)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P94-1038 Dagan</author>
<author>Pereira Ido</author>
<author>C N Fernando</author>
</authors>
<title>Similarity-Based Estimation Of Word Cooccurrence Probabilities (ACL,</title>
<date>1994</date>
<marker>Dagan, Ido, Fernando, 1994</marker>
<rawString>P94-1038 Dagan, Ido, Pereira, Fernando C. N. Similarity-Based Estimation Of Word Cooccurrence Probabilities (ACL, 1994)</rawString>
</citation>
<citation valid="true">
<title>Some of the papers with the highest weights for the classification/tagging class include: W00-0713 Van Den Bosch, Antal Using Induced Rules As Complex Features In Memory-Based Language Learning (CoNLL,</title>
<date>2000</date>
<marker>2000</marker>
<rawString>Some of the papers with the highest weights for the classification/tagging class include: W00-0713 Van Den Bosch, Antal Using Induced Rules As Complex Features In Memory-Based Language Learning (CoNLL, 2000)</rawString>
</citation>
<citation valid="true">
<authors>
<author>W01-0709 Estabrooks</author>
<author>Japkowicz Andrew</author>
</authors>
<title>Nathalie A Mixture-OfExperts Framework For Text Classification (Workshop On Computational Natural Language Learning CoNLL,</title>
<date>2001</date>
<marker>Estabrooks, Andrew, 2001</marker>
<rawString>W01-0709 Estabrooks, Andrew, Japkowicz, Nathalie A Mixture-OfExperts Framework For Text Classification (Workshop On Computational Natural Language Learning CoNLL, 2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A00-2035 Mikheev</author>
<author>Andrei</author>
</authors>
<title>Tagging Sentence Boundaries (ANLPNAACL,</title>
<date>2000</date>
<marker>Mikheev, Andrei, 2000</marker>
<rawString>A00-2035 Mikheev, Andrei. Tagging Sentence Boundaries (ANLPNAACL, 2000)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H92-1022 Brill</author>
</authors>
<title>Eric. A Simple Rule-Based Part Of Speech Tagger (Workshop On Speech And Natural Language,</title>
<date>1992</date>
<marker>Brill, 1992</marker>
<rawString>H92-1022 Brill, Eric. A Simple Rule-Based Part Of Speech Tagger (Workshop On Speech And Natural Language, 1992)</rawString>
</citation>
<citation valid="false">
<authors>
<author>C88-1071 Kuhn</author>
<author>Roland</author>
</authors>
<title>Speech Recognition and the Frequency of Recently Used Words (COLING)</title>
<marker>Kuhn, Roland, </marker>
<rawString>C88-1071 Kuhn, Roland. Speech Recognition and the Frequency of Recently Used Words (COLING)</rawString>
</citation>
<citation valid="false">
<authors>
<author>J88-1003 DeRose</author>
<author>Steven</author>
</authors>
<title>Grammatical Category Disambiguation by Statistical Optimization.</title>
<journal>(CL Journal)</journal>
<marker>DeRose, Steven, </marker>
<rawString>J88-1003 DeRose, Steven. Grammatical Category Disambiguation by Statistical Optimization. (CL Journal)</rawString>
</citation>
<citation valid="false">
<authors>
<author>C88-2133 Su</author>
<author>Keh-Yi</author>
<author>Jing-Shin Chang</author>
</authors>
<title>Semantic and Syntactic Aspects of Score Function.</title>
<journal>(COLING)</journal>
<pages>88--1019</pages>
<marker>Su, Keh-Yi, Chang, </marker>
<rawString>C88-2133 Su, Keh-Yi, and Chang, Jing-Shin. Semantic and Syntactic Aspects of Score Function. (COLING) A88-1019 Church, Kenneth Ward. A Stochastic Parts Program and</rawString>
</citation>
<citation valid="false">
<authors>
<author>Noun Phrase</author>
</authors>
<title>Parser for Unrestricted Text. (ANLP) C88-2134 Sukhotin, B.V. Optimization Algorithms of Deciphering as the Elements of a Linguistic Theory.</title>
<publisher>(COLING)</publisher>
<marker>Phrase, </marker>
<rawString>Noun Phrase Parser for Unrestricted Text. (ANLP) C88-2134 Sukhotin, B.V. Optimization Algorithms of Deciphering as the Elements of a Linguistic Theory. (COLING)</rawString>
</citation>
<citation valid="false">
<authors>
<author>P88-1013 Haigh</author>
<author>Sampson Robin</author>
<author>Geoffrey</author>
<author>Eric Atwell</author>
</authors>
<title>Project APRIL: a progress report.</title>
<publisher>(ACL)</publisher>
<marker>Haigh, Robin, Geoffrey, Atwell, </marker>
<rawString>P88-1013 Haigh, Robin, Sampson, Geoffrey, and Atwell, Eric. Project APRIL: a progress report. (ACL)</rawString>
</citation>
<citation valid="false">
<authors>
<author>A88-1005 Boggess</author>
<author>Lois</author>
</authors>
<title>Two Simple Prediction Algorithms to Facilitate Text Production.</title>
<publisher>(ANLP)</publisher>
<marker>Boggess, Lois, </marker>
<rawString>A88-1005 Boggess, Lois. Two Simple Prediction Algorithms to Facilitate Text Production. (ANLP)</rawString>
</citation>
<citation valid="false">
<authors>
<author>C88-1016 Peter F Brown</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<publisher>(COLING)</publisher>
<marker>Brown, </marker>
<rawString>C88-1016 Peter F. Brown, et al. A Statistical Approach to Machine Translation. (COLING)</rawString>
</citation>
<citation valid="false">
<authors>
<author>A88-1028 Oshika</author>
<author>Beatrice</author>
</authors>
<title>Computational Techniques for Improved Name Search.</title>
<publisher>(ANLP)</publisher>
<marker>Oshika, Beatrice, </marker>
<rawString>A88-1028 Oshika, Beatrice, et al.. Computational Techniques for Improved Name Search. (ANLP)</rawString>
</citation>
<citation valid="false">
<authors>
<author>C88-1020 Campbell</author>
<author>W N</author>
</authors>
<title>Speech-rate Variation and the Prediction of Duration.</title>
<publisher>(COLING)</publisher>
<marker>Campbell, N, </marker>
<rawString>C88-1020 Campbell, W.N. Speech-rate Variation and the Prediction of Duration. (COLING)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<date>2008</date>
<journal>Association of Computational Linguists Anthology. http://www.aclweb.org/anthologyindex/.</journal>
<contexts>
<context position="2968" citStr="Bird, 2008" startWordPosition="469" endWordPosition="470">netheless, we propose to apply Kuhn’s insight that vocabulary and vocabulary shift is a crucial indicator of ideas and shifts in ideas. Our operationalization of this insight is based on the unsupervised topic model Latent Dirichlet Allocation (LDA; Blei et al. (2003)). For many fields, doing this kind of historical study would be very difficult. Computational linguistics has an advantage, however: the ACL Anthology, a public repository of all papers in the Computational Linguistics journal and the conferences and workshops associated with the ACL, COLING, EMNLP, and so on. The ACL Anthology (Bird, 2008), and comprises over 14,000 documents from conferences and the journal, beginning as early as 1965 through 2008, indexed by conference and year. This resource has already been the basis of citation analysis work, for example, in the ACL Anthology Network of Joseph and Radev (2007). We apply LDA to the text of the papers in the ACL Anthology to induce topics, and use the trends in these topics over time and over conference venues to address questions about the development of the field. 363 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363–371, Hon</context>
</contexts>
<marker>Bird, 2008</marker>
<rawString>Steven Bird. 2008. Association of Computational Linguists Anthology. http://www.aclweb.org/anthologyindex/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<publisher>ICML.</publisher>
<contexts>
<context position="6253" citStr="Blei and Lafferty, 2006" startWordPosition="1012" endWordPosition="1015">timation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the preceding year’s. The Topics over Time Model (Wang and McCallum, 2006) assumes that each document chooses its own time stamp based on a topic-specific beta distribution. Both of these models, however, impose constraints on the time periods. The Dynamic Topic Model penalizes large changes from year to year while the beta distributions in Topics over Time are relatively inflexible. We chose instead to perform post hoc calculations bas</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John D. Lafferty. 2006. Dynamic topic models. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, 2003</marker>
<rawString>David Blei, Andrew Ng, , and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Gri</author>
<author>M Jordan</author>
<author>J Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested Chinese restaurant process.</title>
<date>2004</date>
<contexts>
<context position="5953" citStr="Blei et al. (2004)" startWordPosition="961" endWordPosition="964">2003)), a generative latent variable model that treats documents as bags of words generated by one or more topics. Each document is characterized by a multinomial distribution over topics, and each topic is in turn characterized by a multinomial distribution over words. We perform parameter estimation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the preceding year’s. The Topics over Time Model (Wang and McCallum, 2006) assumes that each document chooses its own time stamp based on a </context>
</contexts>
<marker>Blei, Gri, Jordan, Tenenbaum, 2004</marker>
<rawString>D. Blei, T. Gri, M. Jordan, and J. Tenenbaum. 2004. Hierarchical topic models and the nested Chinese restaurant process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>Reviewing the reviewers.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Church, 2005</marker>
<rawString>Kenneth Church. 2005. Reviewing the reviewers. Comput. Linguist., 31(4):575–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Dietz</author>
<author>Steffen Bickel</author>
<author>Tobias Scheffer</author>
</authors>
<title>Unsupervised prediction of citation influences.</title>
<date>2007</date>
<booktitle>In ICML,</booktitle>
<pages>233--240</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5814" citStr="Dietz et al. (2007)" startWordPosition="935" endWordPosition="938">ibution of the Anthology data is shown in Table 1. 2.2 Topic Modeling Our experiments employ Latent Dirichlet Allocation (LDA; Blei et al. (2003)), a generative latent variable model that treats documents as bags of words generated by one or more topics. Each document is characterized by a multinomial distribution over topics, and each topic is in turn characterized by a multinomial distribution over words. We perform parameter estimation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from t</context>
</contexts>
<marker>Dietz, Bickel, Scheffer, 2007</marker>
<rawString>Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007. Unsupervised prediction of citation influences. In ICML, pages 233–240. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Garfield</author>
</authors>
<title>Citation indexes to science: A new dimension in documentation through association of ideas.</title>
<date>1955</date>
<journal>Science,</journal>
<pages>122--108</pages>
<contexts>
<context position="1620" citStr="Garfield, 1955" startWordPosition="256" endWordPosition="257">sing it to show that COLING is a more diverse conference than ACL, but that both conferences as well as EMNLP are becoming broader over time. Finally, we apply Jensen-Shannon divergence of topic distributions to show that all three conferences are converging in the topics they cover. 1 Introduction How can we identify and study the exploration of ideas in a scientific field over time, noting periods of gradual development, major ruptures, and the waxing and waning of both topic areas and connections with applied topics and nearby fields? One important method is to make use of citation graphs (Garfield, 1955). This enables the use of graphbased algorithms like PageRank for determining researcher or paper centrality, and examining whether their influence grows or diminishes over time. However, because we are particularly interested in the change of ideas in a field over time, we have chosen a different method, following Kuhn (1962). In Kuhn’s model of scientific change, science proceeds by shifting from one paradigm to another. Because researchers’ ideas and vocabulary are constrained by their paradigm, successive incommensurate paradigms will naturally have different vocabulary and framing. Kuhn’s</context>
</contexts>
<marker>Garfield, 1955</marker>
<rawString>Eugene Garfield. 1955. Citation indexes to science: A new dimension in documentation through association of ideas. Science, 122:108–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<tech>PNAS, 101 Suppl 1:5228–5235,</tech>
<contexts>
<context position="5699" citStr="Griffiths and Steyvers, 2004" startWordPosition="915" endWordPosition="919">Data The analyses in this paper are based on a textonly version of the Anthology that comprises some 12,500 papers. The distribution of the Anthology data is shown in Table 1. 2.2 Topic Modeling Our experiments employ Latent Dirichlet Allocation (LDA; Blei et al. (2003)), a generative latent variable model that treats documents as bags of words generated by one or more topics. Each document is characterized by a multinomial distribution over topics, and each topic is in turn characterized by a multinomial distribution over words. We perform parameter estimation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s document</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Tom L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101 Suppl 1:5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yookyung Jo</author>
<author>Carl Lagoze</author>
<author>C Lee Giles</author>
</authors>
<title>Detecting research topics via the correlation between graphs and texts.</title>
<date>2007</date>
<booktitle>In KDD,</booktitle>
<pages>370--379</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5856" citStr="Jo et al. (2007)" startWordPosition="944" endWordPosition="947">le 1. 2.2 Topic Modeling Our experiments employ Latent Dirichlet Allocation (LDA; Blei et al. (2003)), a generative latent variable model that treats documents as bags of words generated by one or more topics. Each document is characterized by a multinomial distribution over topics, and each topic is in turn characterized by a multinomial distribution over words. We perform parameter estimation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the preceding year’s. The Topics over Time </context>
</contexts>
<marker>Jo, Lagoze, Giles, 2007</marker>
<rawString>Yookyung Jo, Carl Lagoze, and C. Lee Giles. 2007. Detecting research topics via the correlation between graphs and texts. In KDD, pages 370–379, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark T Joseph</author>
<author>Dragomir R Radev</author>
</authors>
<title>Citation analysis, centrality, and the ACL anthology.</title>
<date>2007</date>
<tech>Technical Report CSE-TR-535-07,</tech>
<institution>University of Michigan. Department of Electrical Engineering and Computer Science.</institution>
<contexts>
<context position="3249" citStr="Joseph and Radev (2007)" startWordPosition="515" endWordPosition="518">. For many fields, doing this kind of historical study would be very difficult. Computational linguistics has an advantage, however: the ACL Anthology, a public repository of all papers in the Computational Linguistics journal and the conferences and workshops associated with the ACL, COLING, EMNLP, and so on. The ACL Anthology (Bird, 2008), and comprises over 14,000 documents from conferences and the journal, beginning as early as 1965 through 2008, indexed by conference and year. This resource has already been the basis of citation analysis work, for example, in the ACL Anthology Network of Joseph and Radev (2007). We apply LDA to the text of the papers in the ACL Anthology to induce topics, and use the trends in these topics over time and over conference venues to address questions about the development of the field. 363 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363–371, Honolulu, October 2008.c�2008 Association for Computational Linguistics Venue # Papers Years Frequency Journal 1291 1974–Present Quarterly ACL 2037 1979-Present Yearly EACL 596 1983–Present ∼2 Years NAACL 293 2000–Present ∼Yearly Applied NLP 346 1983–2000 ∼3 Years COLING 2092 1965-Pr</context>
</contexts>
<marker>Joseph, Radev, 2007</marker>
<rawString>Mark T. Joseph and Dragomir R. Radev. 2007. Citation analysis, centrality, and the ACL anthology. Technical Report CSE-TR-535-07, University of Michigan. Department of Electrical Engineering and Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Kuhn</author>
</authors>
<title>The Structure of Scientific Revolutions.</title>
<date>1962</date>
<publisher>University Of Chicago Press.</publisher>
<contexts>
<context position="1948" citStr="Kuhn (1962)" startWordPosition="308" endWordPosition="309">e exploration of ideas in a scientific field over time, noting periods of gradual development, major ruptures, and the waxing and waning of both topic areas and connections with applied topics and nearby fields? One important method is to make use of citation graphs (Garfield, 1955). This enables the use of graphbased algorithms like PageRank for determining researcher or paper centrality, and examining whether their influence grows or diminishes over time. However, because we are particularly interested in the change of ideas in a field over time, we have chosen a different method, following Kuhn (1962). In Kuhn’s model of scientific change, science proceeds by shifting from one paradigm to another. Because researchers’ ideas and vocabulary are constrained by their paradigm, successive incommensurate paradigms will naturally have different vocabulary and framing. Kuhn’s model is intended to apply only to very large shifts in scientific thought rather than at the micro level of trends in research foci. Nonetheless, we propose to apply Kuhn’s insight that vocabulary and vocabulary shift is a crucial indicator of ideas and shifts in ideas. Our operationalization of this insight is based on the </context>
</contexts>
<marker>Kuhn, 1962</marker>
<rawString>Thomas S. Kuhn. 1962. The Structure of Scientific Revolutions. University Of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: DAG-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In ICML,</booktitle>
<pages>577--584</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5981" citStr="Li and McCallum (2006)" startWordPosition="966" endWordPosition="969">ent variable model that treats documents as bags of words generated by one or more topics. Each document is characterized by a multinomial distribution over topics, and each topic is in turn characterized by a multinomial distribution over words. We perform parameter estimation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the preceding year’s. The Topics over Time Model (Wang and McCallum, 2006) assumes that each document chooses its own time stamp based on a topic-specific beta distribu</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correlations. In ICML, pages 577–584, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Bibliometric impact measures leveraging topic analysis.</title>
<date>2006</date>
<booktitle>In JCDL ’06: Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries,</booktitle>
<pages>65--74</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5834" citStr="Mann et al. (2006)" startWordPosition="939" endWordPosition="942">ogy data is shown in Table 1. 2.2 Topic Modeling Our experiments employ Latent Dirichlet Allocation (LDA; Blei et al. (2003)), a generative latent variable model that treats documents as bags of words generated by one or more topics. Each document is characterized by a multinomial distribution over topics, and each topic is in turn characterized by a multinomial distribution over words. We perform parameter estimation using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). Possible extensions to this model would be to integrate topic modelling with citations (e.g., Dietz et al. (2007), Mann et al. (2006), and Jo et al. (2007)). Another option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the preceding year’s.</context>
</contexts>
<marker>Mann, Mimno, McCallum, 2006</marker>
<rawString>Gideon S. Mann, David Mimno, and Andrew McCallum. 2006. Bibliometric impact measures leveraging topic analysis. In JCDL ’06: Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, pages 65–74, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
</authors>
<title>Topics over time: a non-Markov continuous-time model of topical trends.</title>
<date>2006</date>
<booktitle>In KDD,</booktitle>
<pages>424--433</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6487" citStr="Wang and McCallum, 2006" startWordPosition="1048" endWordPosition="1051">her option is the use of more fine-grained or hierarchical model (e.g., Blei et al. (2004), and Li and McCallum (2006)). All our studies measure change in various aspects of the ACL Anthology over time. LDA, however, does not explicitly model temporal relationships. One way to model temporal relationships is to employ an extension to LDA. The Dynamic Topic Model (Blei and Lafferty, 2006), for example, represents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the preceding year’s. The Topics over Time Model (Wang and McCallum, 2006) assumes that each document chooses its own time stamp based on a topic-specific beta distribution. Both of these models, however, impose constraints on the time periods. The Dynamic Topic Model penalizes large changes from year to year while the beta distributions in Topics over Time are relatively inflexible. We chose instead to perform post hoc calculations based on the observed probability of each topic given the current year. We define p(z|y) as the empirical probability that an arbitrary paper d written in year y was about topic z: ��(z|y) = E p(z|d)p(d|y) d:td=y 1 C p(z|d) (1) d:td=y 1 </context>
</contexts>
<marker>Wang, McCallum, 2006</marker>
<rawString>Xuerui Wang and Andrew McCallum. 2006. Topics over time: a non-Markov continuous-time model of topical trends. In KDD, pages 424–433, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>