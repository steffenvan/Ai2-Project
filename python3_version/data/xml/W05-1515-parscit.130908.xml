<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.963541">
Constituent Parsing by Classification
</title>
<author confidence="0.847683">
Joseph Turian and I. Dan Melamed
</author>
<email confidence="0.83328">
{lastname}@cs.nyu.edu
</email>
<affiliation confidence="0.849328">
Computer Science Department
</affiliation>
<address confidence="0.645849">
New York University
New York, New York 10003
</address>
<sectionHeader confidence="0.968979" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998117318181818">
Ordinary classification techniques can
drive a conceptually simple constituent
parser that achieves near state-of-the-art
accuracy on standard test sets. Here we
present such a parser, which avoids some
of the limitations of other discriminative
parsers. In particular, it does not place
any restrictions upon which types of fea-
tures are allowed. We also present sev-
eral innovations for faster training of dis-
criminative parsers: we show how train-
ing can be parallelized, how examples
can be generated prior to training with-
out a working parser, and how indepen-
dently trained sub-classifiers that have
never done any parsing can be effectively
combined into a working parser. Finally,
we propose a new figure-of-merit for best-
first parsing with confidence-rated infer-
ences. Our implementation is freely avail-
able at: http://cs.nyu.edu/˜turian/
software/parser/
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999088666666667">
Discriminative machine learning methods have im-
proved accuracy on many NLP tasks, such as POS-
tagging (Toutanova et al., 2003), machine translation
(Och &amp; Ney, 2002), and relation extraction (Zhao &amp;
Grishman, 2005). There are strong reasons to believe
the same would be true of parsing. However, only
limited advances have been made thus far, perhaps
due to various limitations of extant discriminative
parsers. In this paper, we present some innovations
aimed at reducing or eliminating some of these lim-
itations, specifically for the task of constituent pars-
ing:
</bodyText>
<listItem confidence="0.993087444444444">
• We show how constituent parsing can be per-
formed using standard classification techniques.
• Classifiers for different non-terminal labels can be
induced independently and hence training can be
parallelized.
• The parser can use arbitrary information to evalu-
ate candidate constituency inferences.
• Arbitrary confidence scores can be aggregated in
a principled manner, which allows beam search.
</listItem>
<bodyText confidence="0.99758095">
In Section 2 we describe our approach to parsing. In
Section 3 we present experimental results.
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put sentence. Spans cross if they overlap but nei-
ther contains the other. An item (or constituent) is
a (span, label) pair. A state is a set of parse items,
none of which may cross. A parse inference is a pair
(S, i), given by the current state S and an item i to be
added to it. A parse path (or history) is a sequence
of parse inferences over some input sentence (Klein
&amp; Manning, 2001). An item ordering (ordering, for
short) constrains the order in which items may be in-
ferred. In particular, if we prescribe a complete item
ordering, the parser is deterministic (Marcus, 1980)
and each state corresponds to a unique parse path.
For some input sentence and gold-standard parse, a
state is correct if the parser can infer zero or more
additional items to obtain the gold-standard parse. A
parse path is correct if it leads to a correct state. An
</bodyText>
<page confidence="0.976626">
141
</page>
<note confidence="0.7422035">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141–151,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.77189">
inference is correct if adding its item to its state is
correct.
</bodyText>
<sectionHeader confidence="0.578636" genericHeader="method">
2 Parsing by Classification
</sectionHeader>
<bodyText confidence="0.998360666666667">
Recall that with typical probabilistic parsers, our
goal is to output the parse Pˆ with the highest like-
lihood for the given input sentence x:
</bodyText>
<equation confidence="0.998273666666667">
Pˆ = arg max Pr(P) (1)
PP(x)
�
= arg max Pr(I) (2)
PP(x) IP
or, equivalently,
�
= arg max log(Pr(I)) (3)
PP(x) IP
</equation>
<bodyText confidence="0.992725285714286">
where each I is a constituency inference in the parse
path P.
In this work, we explore a generalization in which
each inference I is assigned a real-valued confidence
score Q(I) and individual confidences are aggre-
gated using some function A, which need not be a
sum or product:
</bodyText>
<equation confidence="0.953517">
Q(I) (4)
</equation>
<bodyText confidence="0.9988962">
In Section 2.1 we describe how we induce scoring
function Q(I). In Section 2.2 we discuss the aggre-
gation function A. In Section 2.3 we describe the
method used to restrict the size of the search space
over P(x).
</bodyText>
<subsectionHeader confidence="0.970815">
2.1 Learning the Scoring Function Q(I)
</subsectionHeader>
<bodyText confidence="0.99517705">
During training, our goal is to induce the scoring
function Q, which assigns a real-valued confidence
score Q(I) to each candidate inference I (Equa-
tion 4). We treat this as a classification task: If infer-
ence I is correct, we would like Q(I) to be a positive
value, and if inference I is incorrect, we would like
Q(I) to be a negative value.
Training discriminative parsers can be computa-
tionally very expensive. Instead of having a single
classifier score every inference, we parallelize train-
ing by inducing 26 sub-classifiers, one for each con-
stituent label A in the Penn Treebank (Taylor, Mar-
cus, &amp; Santorini, 2003): Q(I) = Q(I), where
Q is the A-classifier and I is an inference that in-
fers a constituent with label A. For example, the VP-
classifier Qvp would score the VP-inference in Fig-
ure 1, preferably assigning it a positive confidence.
Figure 1 A candidate VP-inference, with head-
children annotated using the rules given in (Collins,
1999).
</bodyText>
<equation confidence="0.992774666666667">
VP (was)
NP (timing) VBD / was ADJP (perfect)
DT / The NN / timing JJ / perfect
</equation>
<bodyText confidence="0.987513285714286">
Each A-classifier is independently trained on training
set E, where each example e E E is a tuple (I, y),
I is a candidate A-inference, and y E {t11. y = +1 if
I is a correct inference and −1 otherwise. This ap-
proach differs from that of Yamada and Matsumoto
(2003) and Sagae and Lavie (2005), who parallelize
according to the POS tag of one of the child items.
</bodyText>
<subsectionHeader confidence="0.952231">
2.1.1 Generating Training Examples
</subsectionHeader>
<bodyText confidence="0.999893066666667">
Our method of generating training examples does
not require a working parser, and can be run prior to
any training. It is similar to the method used in the
literature by deterministic parsers (Yamada &amp; Mat-
sumoto, 2003; Sagae &amp; Lavie, 2005) with one ex-
ception: Depending upon the order constituents are
inferred, there may be multiple bottom-up paths that
lead to the same final parse, so to generate training
examples we choose a single random path that leads
to the gold-standard parse tree.1 The training ex-
amples correspond to all candidate inferences con-
sidered in every state along this path, nearly all of
which are incorrect inferences (with y = −1). For
instance, only 4.4% of candidate NP-inferences are
correct.
</bodyText>
<subsectionHeader confidence="0.89996">
2.1.2 Training Algorithm
</subsectionHeader>
<bodyText confidence="0.982081">
During training, for each label A we induce scor-
ing function Q to minimize the loss over training
examples E:
</bodyText>
<equation confidence="0.916929333333333">
�
Q = arg min L(y · Q(I)) (5)
Qa (I�,y)E�
</equation>
<footnote confidence="0.53526575">
&apos; The particular training tree paths used in our experiments are
included in the aforementioned implementation so that our
results can be replicated under the same experimental condi-
tions.
</footnote>
<equation confidence="0.9370985">
P = arg max
PP(x)
A
IP
</equation>
<page confidence="0.991508">
142
</page>
<bodyText confidence="0.998490428571429">
where y · Q(I) is the margin of example (I, y).
Hence, the learning task is to maximize the margins
of the training examples, i.e. induce scoring function
Q such that it classifies correct inferences with pos-
itive confidence and incorrect inferences with nega-
tive confidence. In our work, we minimized the lo-
gistic loss:
</bodyText>
<equation confidence="0.999752">
L(z) = log(1 + exp(−z)) (6)
</equation>
<bodyText confidence="0.997292875">
i.e. the negative log-likelihood of the training sam-
ple.
Our classifiers are ensembles of decisions trees,
which we boost (Schapire &amp; Singer, 1999) to min-
imize the above loss using the update equations
given in Collins, Schapire, and Singer (2002). More
specifically, classifier QT is an ensemble comprising
decision trees q1, ... , qT, where:
</bodyText>
<equation confidence="0.999150333333333">
T
QT  (I) = Z qt(I) (7)
t=1
</equation>
<bodyText confidence="0.9842506">
At iteration t, decision tree qt  is grown, its leaves
are confidence-rated, and it is added to the ensemble.
The classifier for each constituent label is trained in-
dependently, so we henceforth omit λ subscripts.
An example (I, y) is assigned weight wt(I, y):2
</bodyText>
<equation confidence="0.998780666666667">
1
wt(I,y) = (8)
1 + exp(y · Qt−1(I))
</equation>
<bodyText confidence="0.7886475">
The total weight ofy-value examples that fall in leaf
f is Wtf,y:
</bodyText>
<equation confidence="0.990804333333333">
Wt = Z wt(I,y) (9)
(I,y&apos;)E
y�=y, If
</equation>
<bodyText confidence="0.982542">
and this leaf has loss Ztf:
</bodyText>
<equation confidence="0.848869285714286">
J
Zt f = 2 · Wt f,+ · Wt (10)
f,−
Growing the decision tree: The loss of the entire
decision tree qt is
Zt (11)
f
</equation>
<bodyText confidence="0.999091769230769">
2 If we were to replace this equation with wt(I, y) =
exp(y·Qt−&apos;(I))−&apos;, but leave the remainder of the algorithm un-
changed, this algorithm would be confidence-rated AdaBoost
(Schapire &amp; Singer, 1999), minimizing the exponential loss
L(z) = exp(−z). In preliminary experiments, however, we
found that the logistic loss provided superior generalization
accuracy.
We will use Zt as a shorthand for Z(qt). When grow-
ing the decision tree, we greedily choose node splits
to minimize this Z (Kearns &amp; Mansour, 1999). In
particular, the loss reduction of splitting leaf f us-
ing feature φ into two children, f A φ and f A -,φ, is
ΔZt f (φ):
</bodyText>
<equation confidence="0.998589">
ΔZtf(φ) = Ztf − (Ztf + Ztf¬) (12)
</equation>
<bodyText confidence="0.646094">
To split node f, we choose the φˆ that reduces loss
the most:
</bodyText>
<equation confidence="0.829638714285714">
φˆ = argmax ΔZtf(φ) (13)

Confidence-rating the leaves: Each leaf f is
confidence-rated as κtf:
Wt 1 f,+ + �
κt f = 2 · log (14)
Wt f,− + �
</equation>
<bodyText confidence="0.9780648">
Equation 14 is smoothed by the c term (Schapire
&amp; Singer, 1999) to prevent numerical instability in
the case that either Wtf,+or Wt f,− is 0. In our ex-
periments, we used c = 10−8. Although our exam-
ple weights are unnormalized, so far we’ve found
no benefit from scaling c as Collins and Koo (2005)
suggest. All inferences that fall in a particular leaf
node are assigned the same confidence: if inference
I falls in leaf node f in the tth decision tree, then
qt(I) = κtf .
</bodyText>
<subsectionHeader confidence="0.989876">
2.1.3 Calibrating the Sub-Classifiers
</subsectionHeader>
<bodyText confidence="0.999981421052632">
An important concern is when to stop growing the
decision tree. We propose the minimum reduction
in loss (MRL) stopping criterion: During training,
there is a value Θt at iteration t which serves as a
threshold on the minimum reduction in loss for leaf
splits. If there is no splitting feature for leaf f that
reduces loss by at least Θt then f is not split. For-
mally, leaf f will not be bisected during iteration t if
max ΔZtf(φ) &lt; Θt. The MRL stopping criterion
is essentially `0 regularization: Θt corresponds to the
`0 penalty parameter and each feature with non-zero
confidence incurs a penalty of Θt, so to outweigh the
penalty each split must reduce loss by at least Θt.
Θt decreases monotonically during training at
the slowest rate possible that still allows train-
ing to proceed. We start by initializing Θ1 to oo,
and at the beginning of iteration t we decrease Θt
only if the root node 0 of the decision tree can-
not be split. Otherwise, Θt is set to Θt−1. Formally,
</bodyText>
<equation confidence="0.922021333333333">
Z
Z(qt) =
leaf fqt
</equation>
<page confidence="0.981417">
143
</page>
<bodyText confidence="0.988358">
Ot = min(Ot−1, maxED AZt�(0)). In this manner, the
decision trees are induced in order of decreasing 0t.
During training, the constituent classifiers Q
never do any parsing per se, and they train at dif-
ferent rates: If A # A&apos;, then 0t isn’t necessarily
equal to 0t,. We calibrate the different classifiers by
O and insisting that
the sub-classifiers comprised by a particular parser
have all reached some fixed O in training. Given ˆ�,
the constituent classifier for label A is Qt, where
0t &gt;_ O &gt; �t+1
 . To obtain the final parser, we
cross-validate ˆO, picking the value whose set of con-
stituent classifiers maximizes accuracy on a devel-
opment set.
</bodyText>
<subsectionHeader confidence="0.4763155">
2.1.4 Types of Features used by the Scoring
Function
</subsectionHeader>
<bodyText confidence="0.98866464516129">
Our parser operates bottom-up. Let the frontier of
a state be the top-most items (i.e. the items with no
parents). The children of a candidate inference are
those frontier items below the item to be inferred, the
left context items are those frontier items to the left
of the children, and the right context items are those
frontier items to the right of the children. For exam-
ple, in the candidate VP-inference shown in Figure 1,
the frontier comprises the NP, VBD, and ADJP items,
the VBD and ADJP items are the children of the VP-
inference (the VBD is its head child), the NP is the left
context item, and there are no right context items.
The design of some parsers in the literature re-
stricts the kinds of features that can be usefully and
efficiently evaluated. Our scoring function and pars-
ing algorithm have no such limitations. Q can, in
principle, use arbitrary information from the history
to evaluate constituent inferences. Although some of
our feature types are based on prior work (Collins,
1999; Klein &amp; Manning, 2003; Bikel, 2004), we
note that our scoring function uses more history in-
formation than typical parsers.
All features check whether an item has some
property; specifically, whether the item’s la-
bel/headtag/headword is a certain value. These fea-
tures perform binary tests on the state directly, un-
like Henderson (2003) which works with an inter-
mediate representation of the history. In our baseline
setup, feature set (D contained five different feature
types, described in Table 1.
Table 2 Feature item groups.
</bodyText>
<listItem confidence="0.99995275">
• all children
• all non-head children
• all non-leftmost children
• all non-rightmost children
• all children left of the head
• all children right of the head
• head-child and all children left of the head
• head-child and all children right of the head
</listItem>
<subsectionHeader confidence="0.999795">
2.2 Aggregating Confidences
</subsectionHeader>
<bodyText confidence="0.9992074">
To get the cumulative score of a parse path P, we ap-
ply aggregator A over the confidences Q(I) in Equa-
tion 4. Initially, we defined A in the customary fash-
ion as summing the loss of each inference’s confi-
dence:
</bodyText>
<equation confidence="0.9994025">
Pˆ = arg max E�������− ������ (15)
PEP(x) IEP L (Q(I)) �
</equation>
<bodyText confidence="0.999837684210526">
with the logistic loss L as defined in Equation 6. (We
negate the final sum because we want to minimize
the loss.) This definition of A is motivated by view-
ing L as a negative log-likelihood given by a logistic
function (Collins et al., 2002), and then using Equa-
tion 3. It is also inspired by the multiclass loss-based
decoding method of Schapire and Singer (1999).
With this additive aggregator, loss monotonically in-
creases as inferences are added, as in a PCFG-based
parser in which all productions decrease the cumu-
lative probability of the parse tree.
In preliminary experiments, this aggregator gave
disappointing results: precision increased slightly,
but recall dropped sharply. Exploratory data analy-
sis revealed that, because each inference incurs some
positive loss, the aggregator very cautiously builds
the smallest trees possible, thus harming recall. We
had more success by defining A to maximize the
minimum confidence. Essentially,
</bodyText>
<equation confidence="0.6003725">
min Q(I) (16)
IEP
</equation>
<bodyText confidence="0.999547">
Ties are broken according to the second lowest con-
fidence, then the third lowest, and so on.
</bodyText>
<subsectionHeader confidence="0.995728">
2.3 Search
</subsectionHeader>
<bodyText confidence="0.999797333333333">
Given input sentence x, we choose the parse path P
in P(x) with the maximum aggregated score (Equa-
tion 4). Since it is computationally intractable to
</bodyText>
<equation confidence="0.839378666666667">
picking some meta-parameter
Pˆ = arg max
PEP(x)
</equation>
<page confidence="0.996549">
144
</page>
<tableCaption confidence="0.964368">
Table 1 Types of features.
</tableCaption>
<listItem confidence="0.989902875">
• Child item features test if a particular child item has some property. E.g. does the item one right of the
head have headword “perfect”? (True in Figure 1)
• Context item features test if a particular context item has some property. E.g. does the first item of left
context have headtag NN? (True)
• Grandchild item features test if a particular grandchild item has some property. E.g. does the leftmost
child of the rightmost child item have label ]]? (True)
• Exists features test if a particular group of items contains an item with some property. E.g. does some
non-head child item have label AD]P? (True) Exists features select one of the groups of items specified in
</listItem>
<tableCaption confidence="0.5349755">
Table 2. Alternately, they can select the terminals dominated by that group. E.g. is there some terminal
item dominated by non-rightmost children items that has headword “quux”? (False)
</tableCaption>
<bodyText confidence="0.99023175">
consider every possible sequence of inferences, we
use beam search to restrict the size of P(x). As
an additional guard against excessive computation,
search stopped if more than a fixed maximum num-
ber of states were popped from the agenda. As usual,
search also ended if the highest-priority state in the
agenda could not have a better aggregated score than
the best final parse found thus far.
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999889263157895">
Following Taskar, Klein, Collins, Koller, and Man-
ning (2004), we trained and tested on &lt; 15 word sen-
tences in the English Penn Treebank (Taylor et al.,
2003), 10% of the entire treebank by word count.3
We used sections 02–21 (9753 sentences) for train-
ing, section 24 (321 sentences) for development,
and section 23 (603 sentences) for testing, prepro-
cessed as per Table 3. We evaluated our parser us-
ing the standard PARSEVAL measures (Black et
al., 1991): labelled precision, recall, and F-measure
(LPRC, LRCL, and LFMS, respectively), which are
computed based on the number of constituents in the
parser’s output that match those in the gold-standard
parse. We tested whether the observed differences in
PARSEVAL measures are significant at p = 0.05 us-
ing a stratified shuffling test (Cohen, 1995, Section
5.3.2) with one million trials.4
As mentioned in Section 1, the parser cannot in-
fer any item that crosses an item already in the state.
</bodyText>
<tableCaption confidence="0.590491">
3 There was insufficient time before deadline to train on all
sentences.
4 The shuffling test we used was originally implemented
by Dan Bikel (http://www.cis.upenn.edu/˜dbikel/
software.html) and subsequently modified to compute p-
values for LFMS differences.
</tableCaption>
<bodyText confidence="0.999895384615385">
We placed three additional candidacy restrictions
on inferences: (a) Items must be inferred under the
bottom-up item ordering; (b) To ensure the parser
does not enter an infinite loop, no two items in a state
can have both the same span and the same label;
(c) An item can have no more than K = 5 children.
(Only 0.24% of non-terminals in the preprocessed
development set have more than five children.) The
number of candidate inferences at each state, as well
as the number of training examples generated by the
algorithm in Section 2.1.1, is proportional to K. In
our experiment, there were roughly |ET |`z� 1.7 mil-
lion training examples for each classifier.
</bodyText>
<subsectionHeader confidence="0.970182">
3.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999276307692308">
In the baseline setting, context item features (Sec-
tion 2.1.4) could refer to the two nearest items of
context in each direction. The parser used a beam
width of 1000, and was terminated in the rare event
that more than 10,000 states were popped from the
agenda. Figure 2 shows the accuracy of the base-
line on the development set as training progresses.
Cross-validating the choice of O against the LFMS
(Section 2.1.3) suggested an optimum of O = 1.42.
At this ˆ�, there were a total of 9297 decision tree
splits in the parser (summed over all constituent
classifiers), LFMS = 87.16, LRCL = 86.32, and
LPRC = 88.02.
</bodyText>
<subsectionHeader confidence="0.999666">
3.2 Beam Width
</subsectionHeader>
<bodyText confidence="0.9998034">
To determine the effect of the beam width on the
accuracy, we evaluated the baseline on the devel-
opment set using a beam width of 1, i.e. parsing
entirely greedily (Wong &amp; Wu, 1999; Kalt, 2004;
Sagae &amp; Lavie, 2005). Table 4 compares the base-
</bodyText>
<page confidence="0.997766">
145
</page>
<tableCaption confidence="0.651547">
Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure.
</tableCaption>
<listItem confidence="0.999198444444444">
1. * Strip functional tags and trace indices, and remove traces.
2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).)
3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004)
4. * Raise punctuation. (Bikel, 2004)
5. Remove outermost punctuation.&apos;
6. * Remove unary projections to self (i.e. duplicate items with the same span and label).
7. POS tag the text using Ratnaparkhi (1996).
8. Lowercase headwords.
9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK.
</listItem>
<bodyText confidence="0.522865666666667">
a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information.
It’s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser
after adding punctuation features, one of which encoded the sentence-final punctuation.
</bodyText>
<figureCaption confidence="0.943417333333333">
Figure 2 PARSEVAL scores of the baseline on the &lt;_ 15 words development set of the Penn Treebank. The
top x-axis shows accuracy as the minimum reduction in loss 6 decreases. The bottom shows the correspond-
ing number of decision tree splits in the parser, summed over all classifiers.
</figureCaption>
<figure confidence="0.994107807692307">
Minimum reduction in loss
Total # of splits
120 40 25 10 5.0 2.7 1.0 0.34
250 1000 2500 5000 10000 20000
PARSEVAL score
90%
88%
86%
84%
82%
80%
78%
76%
74%
Labelled precision
Labelled F-measure
Labelled recall
90%
88%
86%
84%
82%
80%
78%
76%
74%
</figure>
<bodyText confidence="0.86513925">
line results on the development set with a beam
width of 1 and a beam width of 1000.5 The wider
beam seems to improve the PARSEVAL scores of
the parser, although we were unable to detect a sta-
tistically significant improvement in LFMS on our
relatively small development set.
s Using a beam width of 100,000 yielded output identical to
using a beam width of 1000.
</bodyText>
<subsectionHeader confidence="0.999157">
3.3 Context Size
</subsectionHeader>
<bodyText confidence="0.999887571428572">
Table 5 compares the baseline to parsers that could
not examine as many context items. A significant
portion of the baseline’s accuracy is due to contex-
tual clues, as evidenced by the poor accuracy of the
no context run. However, we did not detect a signif-
icant difference between using one context item or
two.
</bodyText>
<page confidence="0.997875">
146
</page>
<bodyText confidence="0.98528275">
Table 4 PARSEVAL results on the &lt;_ 15 words
development set of the baseline, varying the beam
width. Also, the MRL that achieved this LFMS and
the total number of decision tree splits at this MRL.
</bodyText>
<table confidence="0.95713175">
Dev Dev Dev MRL #splits
LFMS LRCL LPRC Oˆ total
Beam=1 86.36 86.20 86.53 2.03 7068
Baseline 87.16 86.32 88.02 1.42 9297
</table>
<bodyText confidence="0.935818666666667">
Table 5 PARSEVAL results on the &lt;_ 15 words de-
velopment set, given the amount of context avail-
able. is statistically significant. The score differences
between “context 0” and “context 1” are significant,
whereas the differences between “context 1” and the
baseline are not.
</bodyText>
<table confidence="0.9463574">
Dev Dev Dev MRL #splits
LFMS LRCL LPRC Oˆ total
Context 0 75.15 75.28 75.03 3.38 3815
Context 1 86.93 85.78 88.12 2.45 5588
Baseline 87.16 86.32 88.02 1.42 9297
</table>
<bodyText confidence="0.936857625">
Figure 3 An example of a decision (a) stump and
(b) tree for scoring NP-inferences. Each leaf’s value
is the confidence assigned to all inferences that fall
in this leaf. 01 asks “does the first child have a de-
terminer headtag?”. 02 asks “does the last child have
a noun label?”. NP classification is better served by
the informative conjunction 01 n02 found by the de-
cision trees.
</bodyText>
<figure confidence="0.9793068">
01
true false
02
true false
+1.0 -0.2
</figure>
<figureCaption confidence="0.717767833333333">
Table 7 PARSEVAL results of deterministic parsers
on the &lt;_ 15 words development set through 8700
splits. A shaded cell means that the difference be-
tween this value and that of the baseline is statisti-
cally significant. All differences between l2r and r2l
are significant.
</figureCaption>
<figure confidence="0.9399795">
(a)
01
true false
(b)
0
+0.5 0
Table 6 PARSEVAL results of decision stumps on
the &lt;_ 15 words development set, through 8200
splits. The differences between the stumps run and
the baseline are statistically significant.
Dev Dev Dev
LFMS LRCL LPRC
l2r 83.61 82.71 84.54
r2l 85.76 85.37 86.15
MRL #splits
Oˆ total
3.37 2157
3.39 1881
</figure>
<table confidence="0.55944725">
Dev Dev Dev MRL #splits
LFMS LRCL LPRC Oˆ total
Stumps 85.72 84.65 86.82 2.39 5217
Baseline 87.07 86.05 88.12 1.92 7283
</table>
<subsectionHeader confidence="0.972298">
3.4 Decision Stumps
</subsectionHeader>
<bodyText confidence="0.999555375">
Our features are of relatively fine granularity. To test
if a less powerful machine could provide accuracy
comparable to the baseline, we trained a parser in
which we boosted decisions stumps, i.e. decision
trees of depth 1. Stumps are equivalent to learning
a linear discriminant over the atomic features. Since
the stumps run trained quite slowly, it only reached
8200 splits total. To ensure a fair comparison, in Ta-
ble 6 we chose the best baseline parser with at most
8200 splits. The LFMS of the stumps run on the de-
velopment set was 85.72%, significantly less accu-
rate than the baseline.
For example, Figure 3 shows a case where NP
classification better served by the informative con-
junction 01 n 02 found by the decision trees. Given
Baseline 87.07 86.05 88.12 1.92 7283
the sentence “The man left”, at the initial state there
are six candidate NP-inferences, one for each span,
and “(NP The man)” is the only candidate inference
that is correct. 01 is true for the correct inference and
two of the incorrect inferences (“(NP The)” and “(NP
The man left)”). 01 n 02, on the other hand, is true
only for the correct inference, and so it is better at
discriminating NPs over this sample.
</bodyText>
<subsectionHeader confidence="0.976139">
3.5 Deterministic Parsing
</subsectionHeader>
<bodyText confidence="0.999540888888889">
Our baseline parser simulates a non-deterministic
machine, as at any state there may be several correct
decisions. We trained deterministic variations of the
parser, for which we imposed strict left-to-right (l2r)
and right-to-left (r2l) item orderings. For these vari-
ations we generated training examples using the cor-
responding unique path to each gold-standard train-
ing tree. The r2l run reached only 8700 splits to-
tal, so in Table 7 we chose the best baseline and l2r
</bodyText>
<page confidence="0.997598">
147
</page>
<tableCaption confidence="0.49600225">
Table 8 PARSEVAL results of the full vocabulary
parser on the &lt;_ 15 words development set. The dif-
ferences between the full vocabulary run and the
baseline are not statistically significant.
</tableCaption>
<table confidence="0.93677875">
Dev Dev Dev MRL #splits
LFMS LRCL LPRC O total
Baseline 87.16 86.32 88.02 1.42 9297
Full vocab 87.50 86.85 88.15 1.27 10711
</table>
<bodyText confidence="0.998312258064516">
parser with at most 8700 splits.
r2l parsing is significantly more accurate than l2r.
The reason is that the deterministic runs (l2r and r2l)
must avoid prematurely inferring items that come
later in the item ordering. This puts the l2r parser
in a tough spot. If it makes far-right decisions, it’s
more likely to prevent correct subsequent decisions
that are earlier in the l2r ordering, i.e. to the left.
But if it makes far-left decisions, then it goes against
the right-branching tendency of English sentences.
In contrast, the r2l parser is more likely to be correct
when it infers far-right constituents.
We also observed that the accuracy of the de-
terministic parsers dropped sharply as training pro-
gressed (See Figure 4). This behavior was unex-
pected, as the accuracy curve levelled off in every
other experiment. In fact, the accuracy of the deter-
ministic parsers fell even when parsing the training
data. To explain this behavior, we examined the mar-
gin distributions of the r2l NP-classifier (Figure 5).
As training progressed, the NP-classifier was able to
reduce loss by driving up the margins of the incor-
rect training examples, at the expense of incorrectly
classifying a slightly increased number of correct
training examples. However, this is detrimental to
parsing accuracy. The more correct inferences with
negative confidence, the less likely it is at some state
that the highest confidence inference is correct. This
effect is particularly pronounced in the deterministic
setting, where there is only one correct inference per
state.
</bodyText>
<subsectionHeader confidence="0.995659">
3.6 Full Vocabulary
</subsectionHeader>
<bodyText confidence="0.999898642857143">
As in traditional parsers, the baseline was smoothed
by replacing any word that occurs fewer than five
times in the training data with the special token UNK
(Table 3.9). Table 8 compares the baseline to a full
vocabulary run, in which the vocabulary contained
all words observed in the training data. As evidenced
by the results therein, controlling for lexical sparsity
did not significantly improve accuracy in our setting.
In fact, the full vocabulary run is slightly more ac-
curate than the baseline on the development set, al-
though this difference was not statistically signifi-
cant. This was a late-breaking result, and we used
the full vocabulary condition as our final parser for
parsing the test set.
</bodyText>
<subsectionHeader confidence="0.999805">
3.7 Test Set Results
</subsectionHeader>
<bodyText confidence="0.999936">
Table 9 shows the results of our best parser on the
&lt; 15 words test set, as well as the accuracy reported
for a recent discriminative parser (Taskar et al.,
2004) and scores we obtained by training and test-
ing the parsers of Charniak (2000) and Bikel (2004)
on the same data. Bikel (2004) is a “clean room”
reimplementation of the Collins parser (Collins,
1999) with comparable accuracy. Both Charniak
(2000) and Bikel (2004) were trained using the gold-
standard tags, as this produced higher accuracy on
the development set than using Ratnaparkhi (1996)’s
tags.
</bodyText>
<subsectionHeader confidence="0.992887">
3.8 Exploratory Data Analysis
</subsectionHeader>
<bodyText confidence="0.984281565217391">
To gain a better understanding of the weaknesses of
our parser, we examined a sample of 50 develop-
ment sentences that the full vocabulary parser did
not get entirely correct. Besides noise and cases of
genuine ambiguity, the following list outlines all er-
ror types that occurred in more than five sentences,
in roughly decreasing order of frequency. (Note that
there is some overlap between these groups.)
• ADVPs and ADJPs A disproportionate amount of
the parser’s error was due to ADJPs and ADVPs.
Out of the 12.5% total error of the parser on the
development set, an absolute 1.0% was due to
ADVPs, and 0.9% due to ADJPs. The parser had
LFMS = 78.9%, LPRC = 82.5%, LRCL = 75.6%
on ADVPs, and LFMS = 68.0%, LPRC =
71.2%, LRCL = 65.0% on ADJPs.
These constructions can sometimes involve tricky
attachment decisions. For example, in the frag-
ment “to get fat in times of crisis”, the parser’s
output was “(VP to (VP get (ADJP fat (PP in (NP
(NP times) (PP of (NP crisis)))))))” instead of the
correct construction “(VP to (VP get (ADJP fat) (PP
in (NP (NP times) (PP of (NP crisis))))))”.
</bodyText>
<page confidence="0.996254">
148
</page>
<bodyText confidence="0.852680333333333">
Figure 4 LFMS of the baseline and the deterministic runs on the &lt;_ 15 words development set of the Penn
Treebank. The x-axis shows the LFMS as training progresses and the number of decision tree splits in-
creases.
</bodyText>
<subsectionHeader confidence="0.313732">
Total # of splits
</subsectionHeader>
<figureCaption confidence="0.8731075">
Figure 5 The margin distributions of the r2l NP-classifier, early in training and late in training, (a) over the
incorrect training examples and (b) over the correct training examples.
</figureCaption>
<figure confidence="0.99786128">
250 1000 2500 5000 8700
Parseval FMS
88
86
84
82
80
78
76
74
Baseline
Right-to-left
Left-to-right
88
86
84
82
80
78
76
74
Margin
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Percentile
(a)
160
140
120
100
-20
40
80
60
20
0
Late in training
Early in training
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Percentile
(b)
-10
-20
-30
-40
20
10
0
Late in training
Early in training
Margin
</figure>
<bodyText confidence="0.999890166666667">
The amount of noise present in ADJP and ADVP
annotations in the PTB is unusually high. Annota-
tion of ADJP and ADVP unary projections is partic-
ularly inconsistent. For example, the development
set contains the sentence “The dollar was trading
sharply lower in Tokyo .”, with “sharply lower”
bracketed as “(ADVP (ADVP sharply) lower)”.
“sharply lower” appears 16 times in the complete
training section, every time bracketed as “(ADVP
sharply lower)”, and “sharply higher” 10 times,
always as “(ADVP sharply higher)”. Because of the
high number of negative examples, the classifiers’
</bodyText>
<page confidence="0.998574">
149
</page>
<tableCaption confidence="0.65123525">
Table 9 PARSEVAL results of on the &lt;_ 15 words test set of various parsers in the literature. The differ-
ences between the full vocabulary run and Bikel or Charniak are significant. Taskar et al. (2004)’s output
was unavailable for significance testing, but presumably its differences from the full vocab parser are also
significant.
</tableCaption>
<table confidence="0.971655166666667">
Test Test Test Dev Dev Dev
LFMS LRCL LPRC LFMS LRCL LPRC
Full vocab 87.13 86.47 87.80 87.50 86.85 88.15
Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22
Taskar et al. (2004) 89.12 89.10 89.14 89.98 90.22 89.74
Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32
</table>
<bodyText confidence="0.97484972">
bias is to cope with the noise by favoring negative
confidences predictions for ambiguous ADJP and
ADVP decisions, hence their abysmal labelled re-
call. One potential solution is the weight-sharing
strategy described in Section 3.5.
• Tagging Errors Many of the parser’s errors
were due to poor tagging. Preprocessing sentence
“Would service be voluntary or compulsory ?”
gives “would/MD service/VB be/VB voluntary/JJ
or/CC UNK/JJ” and, as a result, the parser brack-
ets “service ... compulsory” as a VP instead of
correctly bracketing “service” as an NP. We also
found that the tagger we used has difficulties with
completely capitalized words, and tends to tag
them NNP. By giving the parser access to the same
features used by taggers, especially rich lexical
features (Toutanova et al., 2003), the parser might
learn to compensate for tagging errors.
• Attachment decisions The parser does not de-
tect affinities between certain word pairs, so it has
difficulties with bilexical dependency decisions.
In principle, bilexical dependencies can be rep-
resented as conjunctions of feature given in Sec-
tion 2.1.4. Given more training data, the parser
might learn these affinities.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999972648648649">
In this work, we presented a near state-of-the-
art approach to constituency parsing which over-
comes some of the limitations of other discrimina-
tive parsers. Like Yamada and Matsumoto (2003)
and Sagae and Lavie (2005), our parser is driven by
classifiers. Even though these classifiers themselves
never do any parsing during training, they can be
combined into an effective parser. We also presented
abeam search method under the objective function
of maximizing the minimum confidence.
To ensure efficiency, some discriminative parsers
place stringent requirements on which types of fea-
tures are permitted. Our approach requires no such
restrictions and our scoring function can, in prin-
ciple, use arbitrary information from the history to
evaluate constituent inferences. Even though our
features may be of too fine granularity to dis-
criminate through linear combination, discrimina-
tively trained decisions trees determine useful fea-
ture combinations automatically, so adding new fea-
tures requires minimal human effort.
Training discriminative parsers is notoriously
slow, especially if it requires generating examples by
repeatedly parsing the treebank (Collins &amp; Roark,
2004; Taskar et al., 2004). Although training time
is still a concern in our setup, the situation is ame-
liorated by generating training examples in advance
and inducing one-vs-all classifiers in parallel, a tech-
nique similar in spirit to the POS-tag parallelization
in Yamada and Matsumoto (2003) and Sagae and
Lavie (2005).
This parser serves as a proof-of-concept, in that
we have not fully exploited the possibilities of en-
gineering intricate features or trying more complex
search methods. Its flexibility offers many oppor-
tunities for improvement, which we leave to future
work.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.649274">
The authors would like to thank Dan Bikel, Mike
Collins, Ralph Grishman, Adam Meyers, Mehryar
Mohri, Satoshi Sekine, and Wei Wang, as well as the
anonymous reviewers, for their helpful comments
</bodyText>
<page confidence="0.993994">
150
</page>
<bodyText confidence="0.998751666666667">
and constructive criticism. This research was spon-
sored by an NSF CAREER award, and by an equip-
ment gift from Sun Microsystems.
</bodyText>
<sectionHeader confidence="0.996265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999622644736842">
Bikel, D. M. (2004). Intricacies of Collins’ pars-
ing model. Computational Linguistics, 30(4),
479–511.
Black, E., Abney, S., Flickenger, D., Gdaniec, C.,
Grishman, R., Harrison, P., et al. (1991).
A procedure for quantitatively comparing the
syntactic coverage of English grammars. In
Speech and Natural Language (pp. 306–311).
Charniak, E. (2000). A maximum-entropy-inspired
parser. In NAACL (pp. 132–139).
Cohen, P. R. (1995). Empirical methods for artificial
intelligence. MIT Press.
Collins, M. (1999). Head-driven statistical models
for natural language parsing. Unpublished
doctoral dissertation, UPenn.
Collins, M. (2003). Head-driven statistical models
for natural language parsing. Computational
Linguistics, 29(4), 589–637.
Collins, M., &amp; Koo, T. (2005). Discriminative
reranking for natural language parsing. Com-
putational Linguistics, 31(1), 25–69.
Collins, M., &amp; Roark, B. (2004). Incremental pars-
ing with the perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., &amp; Singer, Y. (2002).
Logistic regression, AdaBoost and Bregman
distances. Machine Learning, 48(1-3), 253–
285.
Henderson, J. (2003). Inducing history representa-
tions for broad coverage statistical parsing. In
HLT/NAACL.
Kalt, T. (2004). Induction of greedy controllers
for deterministic treebank parsers. In EMNLP
(pp. 17–24).
Kearns, M. J., &amp; Mansour, Y. (1999). On the boost-
ing ability of top-down decision tree learning
algorithms. Journal of Computer and Systems
Sciences, 58(1), 109–128.
Klein, D., &amp; Manning, C. D. (2001). Parsing and
hypergraphs. In IWPT (pp. 123–134).
Klein, D., &amp; Manning, C. D. (2003). Accurate un-
lexicalized parsing. In ACL (pp. 423–430).
Magerman, D. M. (1995). Statistical decision-tree
models for parsing. In ACL (pp. 276–283).
Marcus, M. P. (1980). Theory ofsyntactic recogni-
tion for natural languages. MIT Press.
Och, F. J., &amp; Ney, H. (2002). Discriminative training
and maximum entropy models for statistical
machine translation. In ACL.
Ratnaparkhi, A. (1996). A maximum entropy part-
of-speech tagger. In EMNLP (pp. 133–142).
Sagae, K., &amp; Lavie, A. (2005). A classifier-based
parser with linear run-time complexity. In
IWPT.
Schapire, R. E., &amp; Singer, Y. (1999). Improved
boosting using confidence-rated predictions.
Machine Learning, 37(3), 297–336.
Taskar, B., Klein, D., Collins, M., Koller, D., &amp;
Manning, C. (2004). Max-margin parsing.
In EMNLP (pp. 1–8).
Taylor, A., Marcus, M., &amp; Santorini, B. (2003). The
Penn Treebank: an overview. In A. Abeill´e
(Ed.), Treebanks: Building and using parsed
corpora (pp. 5–22).
Toutanova, K., Klein, D., Manning, C. D., &amp; Singer,
Y. (2003). Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In
HLT/NAACL (pp. 252–259).
Wong, A., &amp; Wu, D. (1999). Learning a
lightweight robust deterministic parser. In
EUROSPEECH.
Yamada, H., &amp; Matsumoto, Y. (2003). Statistical
dependency analysis with support vector ma-
chines. In IWPT.
Zhao, S., &amp; Grishman, R. (2005). Extracting rela-
tions with integrated information using kernel
methods. In ACL.
</reference>
<page confidence="0.998352">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.615273">
<title confidence="0.999761">Constituent Parsing by Classification</title>
<author confidence="0.999923">Joseph Turian</author>
<author confidence="0.999923">I Dan</author>
<affiliation confidence="0.999212">Computer Science</affiliation>
<address confidence="0.9803505">New York New York, New York 10003</address>
<abstract confidence="0.992839681818182">Ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets. Here we present such a parser, which avoids some of the limitations of other discriminative parsers. In particular, it does not place any restrictions upon which types of features are allowed. We also present several innovations for faster training of discriminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have done any parsing can be combined into a working parser. Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences. Our implementation is freely availat:</abstract>
<intro confidence="0.737499">software/parser/</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<title>Intricacies of Collins’ parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>479--511</pages>
<contexts>
<context position="12117" citStr="Bikel, 2004" startWordPosition="2096" endWordPosition="2097"> frontier comprises the NP, VBD, and ADJP items, the VBD and ADJP items are the children of the VPinference (the VBD is its head child), the NP is the left context item, and there are no right context items. The design of some parsers in the literature restricts the kinds of features that can be usefully and efficiently evaluated. Our scoring function and parsing algorithm have no such limitations. Q can, in principle, use arbitrary information from the history to evaluate constituent inferences. Although some of our feature types are based on prior work (Collins, 1999; Klein &amp; Manning, 2003; Bikel, 2004), we note that our scoring function uses more history information than typical parsers. All features check whether an item has some property; specifically, whether the item’s label/headtag/headword is a certain value. These features perform binary tests on the state directly, unlike Henderson (2003) which works with an intermediate representation of the history. In our baseline setup, feature set (D contained five different feature types, described in Table 1. Table 2 Feature item groups. • all children • all non-head children • all non-leftmost children • all non-rightmost children • all chil</context>
<context position="18816" citStr="Bikel, 2004" startWordPosition="3226" endWordPosition="3227">6.32, and LPRC = 88.02. 3.2 Beam Width To determine the effect of the beam width on the accuracy, we evaluated the baseline on the development set using a beam width of 1, i.e. parsing entirely greedily (Wong &amp; Wu, 1999; Kalt, 2004; Sagae &amp; Lavie, 2005). Table 4 compares the base145 Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure. 1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowercase headwords. 9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK. a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information. It’s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punct</context>
<context position="27201" citStr="Bikel (2004)" startWordPosition="4652" endWordPosition="4653"> not significantly improve accuracy in our setting. In fact, the full vocabulary run is slightly more accurate than the baseline on the development set, although this difference was not statistically significant. This was a late-breaking result, and we used the full vocabulary condition as our final parser for parsing the test set. 3.7 Test Set Results Table 9 shows the results of our best parser on the &lt; 15 words test set, as well as the accuracy reported for a recent discriminative parser (Taskar et al., 2004) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data. Bikel (2004) is a “clean room” reimplementation of the Collins parser (Collins, 1999) with comparable accuracy. Both Charniak (2000) and Bikel (2004) were trained using the goldstandard tags, as this produced higher accuracy on the development set than using Ratnaparkhi (1996)’s tags. 3.8 Exploratory Data Analysis To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the full vocabulary parser did not get entirely correct. Besides noise and cases of genuine ambiguity, the following list outlines all error types </context>
<context position="30452" citStr="Bikel (2004)" startWordPosition="5221" endWordPosition="5222">as “(ADVP sharply lower)”, and “sharply higher” 10 times, always as “(ADVP sharply higher)”. Because of the high number of negative examples, the classifiers’ 149 Table 9 PARSEVAL results of on the &lt;_ 15 words test set of various parsers in the literature. The differences between the full vocabulary run and Bikel or Charniak are significant. Taskar et al. (2004)’s output was unavailable for significance testing, but presumably its differences from the full vocab parser are also significant. Test Test Test Dev Dev Dev LFMS LRCL LPRC LFMS LRCL LPRC Full vocab 87.13 86.47 87.80 87.50 86.85 88.15 Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22 Taskar et al. (2004) 89.12 89.10 89.14 89.98 90.22 89.74 Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32 bias is to cope with the noise by favoring negative confidences predictions for ambiguous ADJP and ADVP decisions, hence their abysmal labelled recall. One potential solution is the weight-sharing strategy described in Section 3.5. • Tagging Errors Many of the parser’s errors were due to poor tagging. Preprocessing sentence “Would service be voluntary or compulsory ?” gives “would/MD service/VB be/VB voluntary/JJ or/CC UNK/JJ” and, as a result, the p</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Bikel, D. M. (2004). Intricacies of Collins’ parsing model. Computational Linguistics, 30(4), 479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Speech and Natural Language</booktitle>
<pages>306--311</pages>
<contexts>
<context position="16189" citStr="Black et al., 1991" startWordPosition="2780" endWordPosition="2783">sual, search also ended if the highest-priority state in the agenda could not have a better aggregated score than the best final parse found thus far. 3 Experiments Following Taskar, Klein, Collins, Koller, and Manning (2004), we trained and tested on &lt; 15 word sentences in the English Penn Treebank (Taylor et al., 2003), 10% of the entire treebank by word count.3 We used sections 02–21 (9753 sentences) for training, section 24 (321 sentences) for development, and section 23 (603 sentences) for testing, preprocessed as per Table 3. We evaluated our parser using the standard PARSEVAL measures (Black et al., 1991): labelled precision, recall, and F-measure (LPRC, LRCL, and LFMS, respectively), which are computed based on the number of constituents in the parser’s output that match those in the gold-standard parse. We tested whether the observed differences in PARSEVAL measures are significant at p = 0.05 using a stratified shuffling test (Cohen, 1995, Section 5.3.2) with one million trials.4 As mentioned in Section 1, the parser cannot infer any item that crosses an item already in the state. 3 There was insufficient time before deadline to train on all sentences. 4 The shuffling test we used was origi</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, 1991</marker>
<rawString>Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman, R., Harrison, P., et al. (1991). A procedure for quantitatively comparing the syntactic coverage of English grammars. In Speech and Natural Language (pp. 306–311).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL</booktitle>
<pages>132--139</pages>
<contexts>
<context position="27184" citStr="Charniak (2000)" startWordPosition="4649" endWordPosition="4650">lexical sparsity did not significantly improve accuracy in our setting. In fact, the full vocabulary run is slightly more accurate than the baseline on the development set, although this difference was not statistically significant. This was a late-breaking result, and we used the full vocabulary condition as our final parser for parsing the test set. 3.7 Test Set Results Table 9 shows the results of our best parser on the &lt; 15 words test set, as well as the accuracy reported for a recent discriminative parser (Taskar et al., 2004) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data. Bikel (2004) is a “clean room” reimplementation of the Collins parser (Collins, 1999) with comparable accuracy. Both Charniak (2000) and Bikel (2004) were trained using the goldstandard tags, as this produced higher accuracy on the development set than using Ratnaparkhi (1996)’s tags. 3.8 Exploratory Data Analysis To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the full vocabulary parser did not get entirely correct. Besides noise and cases of genuine ambiguity, the following list outlines</context>
<context position="30561" citStr="Charniak (2000)" startWordPosition="5239" endWordPosition="5240">e high number of negative examples, the classifiers’ 149 Table 9 PARSEVAL results of on the &lt;_ 15 words test set of various parsers in the literature. The differences between the full vocabulary run and Bikel or Charniak are significant. Taskar et al. (2004)’s output was unavailable for significance testing, but presumably its differences from the full vocab parser are also significant. Test Test Test Dev Dev Dev LFMS LRCL LPRC LFMS LRCL LPRC Full vocab 87.13 86.47 87.80 87.50 86.85 88.15 Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22 Taskar et al. (2004) 89.12 89.10 89.14 89.98 90.22 89.74 Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32 bias is to cope with the noise by favoring negative confidences predictions for ambiguous ADJP and ADVP decisions, hence their abysmal labelled recall. One potential solution is the weight-sharing strategy described in Section 3.5. • Tagging Errors Many of the parser’s errors were due to poor tagging. Preprocessing sentence “Would service be voluntary or compulsory ?” gives “would/MD service/VB be/VB voluntary/JJ or/CC UNK/JJ” and, as a result, the parser brackets “service ... compulsory” as a VP instead of correctly bracketing “service” as an NP. We also f</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. (2000). A maximum-entropy-inspired parser. In NAACL (pp. 132–139).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>Empirical methods for artificial intelligence.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16532" citStr="Cohen, 1995" startWordPosition="2835" endWordPosition="2836">by word count.3 We used sections 02–21 (9753 sentences) for training, section 24 (321 sentences) for development, and section 23 (603 sentences) for testing, preprocessed as per Table 3. We evaluated our parser using the standard PARSEVAL measures (Black et al., 1991): labelled precision, recall, and F-measure (LPRC, LRCL, and LFMS, respectively), which are computed based on the number of constituents in the parser’s output that match those in the gold-standard parse. We tested whether the observed differences in PARSEVAL measures are significant at p = 0.05 using a stratified shuffling test (Cohen, 1995, Section 5.3.2) with one million trials.4 As mentioned in Section 1, the parser cannot infer any item that crosses an item already in the state. 3 There was insufficient time before deadline to train on all sentences. 4 The shuffling test we used was originally implemented by Dan Bikel (http://www.cis.upenn.edu/˜dbikel/ software.html) and subsequently modified to compute pvalues for LFMS differences. We placed three additional candidacy restrictions on inferences: (a) Items must be inferred under the bottom-up item ordering; (b) To ensure the parser does not enter an infinite loop, no two ite</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Cohen, P. R. (1995). Empirical methods for artificial intelligence. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<note>Unpublished doctoral dissertation, UPenn.</note>
<contexts>
<context position="5082" citStr="Collins, 1999" startWordPosition="836" endWordPosition="837"> Training discriminative parsers can be computationally very expensive. Instead of having a single classifier score every inference, we parallelize training by inducing 26 sub-classifiers, one for each constituent label A in the Penn Treebank (Taylor, Marcus, &amp; Santorini, 2003): Q(I) = Q(I), where Q is the A-classifier and I is an inference that infers a constituent with label A. For example, the VPclassifier Qvp would score the VP-inference in Figure 1, preferably assigning it a positive confidence. Figure 1 A candidate VP-inference, with headchildren annotated using the rules given in (Collins, 1999). VP (was) NP (timing) VBD / was ADJP (perfect) DT / The NN / timing JJ / perfect Each A-classifier is independently trained on training set E, where each example e E E is a tuple (I, y), I is a candidate A-inference, and y E {t11. y = +1 if I is a correct inference and −1 otherwise. This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. 2.1.1 Generating Training Examples Our method of generating training examples does not require a working parser, and can be run prior to any training.</context>
<context position="12080" citStr="Collins, 1999" startWordPosition="2090" endWordPosition="2091">te VP-inference shown in Figure 1, the frontier comprises the NP, VBD, and ADJP items, the VBD and ADJP items are the children of the VPinference (the VBD is its head child), the NP is the left context item, and there are no right context items. The design of some parsers in the literature restricts the kinds of features that can be usefully and efficiently evaluated. Our scoring function and parsing algorithm have no such limitations. Q can, in principle, use arbitrary information from the history to evaluate constituent inferences. Although some of our feature types are based on prior work (Collins, 1999; Klein &amp; Manning, 2003; Bikel, 2004), we note that our scoring function uses more history information than typical parsers. All features check whether an item has some property; specifically, whether the item’s label/headtag/headword is a certain value. These features perform binary tests on the state directly, unlike Henderson (2003) which works with an intermediate representation of the history. In our baseline setup, feature set (D contained five different feature types, described in Table 1. Table 2 Feature item groups. • all children • all non-head children • all non-leftmost children • </context>
<context position="27305" citStr="Collins, 1999" startWordPosition="4669" endWordPosition="4670">accurate than the baseline on the development set, although this difference was not statistically significant. This was a late-breaking result, and we used the full vocabulary condition as our final parser for parsing the test set. 3.7 Test Set Results Table 9 shows the results of our best parser on the &lt; 15 words test set, as well as the accuracy reported for a recent discriminative parser (Taskar et al., 2004) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data. Bikel (2004) is a “clean room” reimplementation of the Collins parser (Collins, 1999) with comparable accuracy. Both Charniak (2000) and Bikel (2004) were trained using the goldstandard tags, as this produced higher accuracy on the development set than using Ratnaparkhi (1996)’s tags. 3.8 Exploratory Data Analysis To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the full vocabulary parser did not get entirely correct. Besides noise and cases of genuine ambiguity, the following list outlines all error types that occurred in more than five sentences, in roughly decreasing order of frequency. (Note that there is</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999). Head-driven statistical models for natural language parsing. Unpublished doctoral dissertation, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<pages>589--637</pages>
<contexts>
<context position="19207" citStr="Collins (2003)" startWordPosition="3290" endWordPosition="3291">1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowercase headwords. 9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK. a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information. It’s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation. Figure 2 PARSEVAL scores of the baseline on the &lt;_ 15 words development set of the Penn Treebank. The top x-axis shows accuracy as the minimum reduction in loss 6 decreases. The bottom shows the corresponding number of decision tree splits in the parser, summed over all classifiers. Minimum reduction in loss Total # of </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Collins, M. (2003). Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4), 589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>25--69</pages>
<contexts>
<context position="9141" citStr="Collins and Koo (2005)" startWordPosition="1568" endWordPosition="1571">itting leaf f using feature φ into two children, f A φ and f A -,φ, is ΔZt f (φ): ΔZtf(φ) = Ztf − (Ztf + Ztf¬) (12) To split node f, we choose the φˆ that reduces loss the most: φˆ = argmax ΔZtf(φ) (13)  Confidence-rating the leaves: Each leaf f is confidence-rated as κtf: Wt 1 f,+ + � κt f = 2 · log (14) Wt f,− + � Equation 14 is smoothed by the c term (Schapire &amp; Singer, 1999) to prevent numerical instability in the case that either Wtf,+or Wt f,− is 0. In our experiments, we used c = 10−8. Although our example weights are unnormalized, so far we’ve found no benefit from scaling c as Collins and Koo (2005) suggest. All inferences that fall in a particular leaf node are assigned the same confidence: if inference I falls in leaf node f in the tth decision tree, then qt(I) = κtf . 2.1.3 Calibrating the Sub-Classifiers An important concern is when to stop growing the decision tree. We propose the minimum reduction in loss (MRL) stopping criterion: During training, there is a value Θt at iteration t which serves as a threshold on the minimum reduction in loss for leaf splits. If there is no splitting feature for leaf f that reduces loss by at least Θt then f is not split. Formally, leaf f will not b</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Collins, M., &amp; Koo, T. (2005). Discriminative reranking for natural language parsing. Computational Linguistics, 31(1), 25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19324" citStr="Collins and Roark (2004)" startWordPosition="3304" endWordPosition="3307"> was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowercase headwords. 9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK. a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information. It’s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation. Figure 2 PARSEVAL scores of the baseline on the &lt;_ 15 words development set of the Penn Treebank. The top x-axis shows accuracy as the minimum reduction in loss 6 decreases. The bottom shows the corresponding number of decision tree splits in the parser, summed over all classifiers. Minimum reduction in loss Total # of splits 120 40 25 10 5.0 2.7 1.0 0.34 250 1000 2500 5000 10000 20000 PARSEVAL score 90% 88% 86% 84% 82% 80% 78% 76% 74</context>
<context position="32964" citStr="Collins &amp; Roark, 2004" startWordPosition="5602" endWordPosition="5605">irements on which types of features are permitted. Our approach requires no such restrictions and our scoring function can, in principle, use arbitrary information from the history to evaluate constituent inferences. Even though our features may be of too fine granularity to discriminate through linear combination, discriminatively trained decisions trees determine useful feature combinations automatically, so adding new features requires minimal human effort. Training discriminative parsers is notoriously slow, especially if it requires generating examples by repeatedly parsing the treebank (Collins &amp; Roark, 2004; Taskar et al., 2004). Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). This parser serves as a proof-of-concept, in that we have not fully exploited the possibilities of engineering intricate features or trying more complex search methods. Its flexibility offers many opportunities for improvement, which we leave to future work. Acknowledgments The </context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Collins, M., &amp; Roark, B. (2004). Incremental parsing with the perceptron algorithm. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Logistic regression, AdaBoost and Bregman distances.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>48--1</pages>
<contexts>
<context position="13405" citStr="Collins et al., 2002" startWordPosition="2320" endWordPosition="2323"> and all children left of the head • head-child and all children right of the head 2.2 Aggregating Confidences To get the cumulative score of a parse path P, we apply aggregator A over the confidences Q(I) in Equation 4. Initially, we defined A in the customary fashion as summing the loss of each inference’s confidence: Pˆ = arg max E�������− ������ (15) PEP(x) IEP L (Q(I)) � with the logistic loss L as defined in Equation 6. (We negate the final sum because we want to minimize the loss.) This definition of A is motivated by viewing L as a negative log-likelihood given by a logistic function (Collins et al., 2002), and then using Equation 3. It is also inspired by the multiclass loss-based decoding method of Schapire and Singer (1999). With this additive aggregator, loss monotonically increases as inferences are added, as in a PCFG-based parser in which all productions decrease the cumulative probability of the parse tree. In preliminary experiments, this aggregator gave disappointing results: precision increased slightly, but recall dropped sharply. Exploratory data analysis revealed that, because each inference incurs some positive loss, the aggregator very cautiously builds the smallest trees possib</context>
</contexts>
<marker>Collins, Schapire, Singer, 2002</marker>
<rawString>Collins, M., Schapire, R. E., &amp; Singer, Y. (2002). Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1-3), 253– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="12417" citStr="Henderson (2003)" startWordPosition="2143" endWordPosition="2144"> usefully and efficiently evaluated. Our scoring function and parsing algorithm have no such limitations. Q can, in principle, use arbitrary information from the history to evaluate constituent inferences. Although some of our feature types are based on prior work (Collins, 1999; Klein &amp; Manning, 2003; Bikel, 2004), we note that our scoring function uses more history information than typical parsers. All features check whether an item has some property; specifically, whether the item’s label/headtag/headword is a certain value. These features perform binary tests on the state directly, unlike Henderson (2003) which works with an intermediate representation of the history. In our baseline setup, feature set (D contained five different feature types, described in Table 1. Table 2 Feature item groups. • all children • all non-head children • all non-leftmost children • all non-rightmost children • all children left of the head • all children right of the head • head-child and all children left of the head • head-child and all children right of the head 2.2 Aggregating Confidences To get the cumulative score of a parse path P, we apply aggregator A over the confidences Q(I) in Equation 4. Initially, w</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>Henderson, J. (2003). Inducing history representations for broad coverage statistical parsing. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kalt</author>
</authors>
<title>Induction of greedy controllers for deterministic treebank parsers.</title>
<date>2004</date>
<booktitle>In EMNLP</booktitle>
<pages>17--24</pages>
<contexts>
<context position="18435" citStr="Kalt, 2004" startWordPosition="3162" endWordPosition="3163"> than 10,000 states were popped from the agenda. Figure 2 shows the accuracy of the baseline on the development set as training progresses. Cross-validating the choice of O against the LFMS (Section 2.1.3) suggested an optimum of O = 1.42. At this ˆ�, there were a total of 9297 decision tree splits in the parser (summed over all constituent classifiers), LFMS = 87.16, LRCL = 86.32, and LPRC = 88.02. 3.2 Beam Width To determine the effect of the beam width on the accuracy, we evaluated the baseline on the development set using a beam width of 1, i.e. parsing entirely greedily (Wong &amp; Wu, 1999; Kalt, 2004; Sagae &amp; Lavie, 2005). Table 4 compares the base145 Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure. 1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowerc</context>
</contexts>
<marker>Kalt, 2004</marker>
<rawString>Kalt, T. (2004). Induction of greedy controllers for deterministic treebank parsers. In EMNLP (pp. 17–24).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Kearns</author>
<author>Y Mansour</author>
</authors>
<title>On the boosting ability of top-down decision tree learning algorithms.</title>
<date>1999</date>
<journal>Journal of Computer and Systems Sciences,</journal>
<volume>58</volume>
<issue>1</issue>
<pages>109--128</pages>
<contexts>
<context position="8477" citStr="Kearns &amp; Mansour, 1999" startWordPosition="1432" endWordPosition="1435"> J Zt f = 2 · Wt f,+ · Wt (10) f,− Growing the decision tree: The loss of the entire decision tree qt is Zt (11) f 2 If we were to replace this equation with wt(I, y) = exp(y·Qt−&apos;(I))−&apos;, but leave the remainder of the algorithm unchanged, this algorithm would be confidence-rated AdaBoost (Schapire &amp; Singer, 1999), minimizing the exponential loss L(z) = exp(−z). In preliminary experiments, however, we found that the logistic loss provided superior generalization accuracy. We will use Zt as a shorthand for Z(qt). When growing the decision tree, we greedily choose node splits to minimize this Z (Kearns &amp; Mansour, 1999). In particular, the loss reduction of splitting leaf f using feature φ into two children, f A φ and f A -,φ, is ΔZt f (φ): ΔZtf(φ) = Ztf − (Ztf + Ztf¬) (12) To split node f, we choose the φˆ that reduces loss the most: φˆ = argmax ΔZtf(φ) (13)  Confidence-rating the leaves: Each leaf f is confidence-rated as κtf: Wt 1 f,+ + � κt f = 2 · log (14) Wt f,− + � Equation 14 is smoothed by the c term (Schapire &amp; Singer, 1999) to prevent numerical instability in the case that either Wtf,+or Wt f,− is 0. In our experiments, we used c = 10−8. Although our example weights are unnormalized, so far</context>
</contexts>
<marker>Kearns, Mansour, 1999</marker>
<rawString>Kearns, M. J., &amp; Mansour, Y. (1999). On the boosting ability of top-down decision tree learning algorithms. Journal of Computer and Systems Sciences, 58(1), 109–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In IWPT</booktitle>
<pages>123--134</pages>
<contexts>
<context position="2597" citStr="Klein &amp; Manning, 2001" startWordPosition="409" endWordPosition="412">led manner, which allows beam search. In Section 2 we describe our approach to parsing. In Section 3 we present experimental results. The following terms will help to explain our work. A span is a range over contiguous words in the input sentence. Spans cross if they overlap but neither contains the other. An item (or constituent) is a (span, label) pair. A state is a set of parse items, none of which may cross. A parse inference is a pair (S, i), given by the current state S and an item i to be added to it. A parse path (or history) is a sequence of parse inferences over some input sentence (Klein &amp; Manning, 2001). An item ordering (ordering, for short) constrains the order in which items may be inferred. In particular, if we prescribe a complete item ordering, the parser is deterministic (Marcus, 1980) and each state corresponds to a unique parse path. For some input sentence and gold-standard parse, a state is correct if the parser can infer zero or more additional items to obtain the gold-standard parse. A parse path is correct if it leads to a correct state. An 141 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141–151, Vancouver, October 2005. c�2005 Associat</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Klein, D., &amp; Manning, C. D. (2001). Parsing and hypergraphs. In IWPT (pp. 123–134).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<pages>423--430</pages>
<contexts>
<context position="12103" citStr="Klein &amp; Manning, 2003" startWordPosition="2092" endWordPosition="2095"> shown in Figure 1, the frontier comprises the NP, VBD, and ADJP items, the VBD and ADJP items are the children of the VPinference (the VBD is its head child), the NP is the left context item, and there are no right context items. The design of some parsers in the literature restricts the kinds of features that can be usefully and efficiently evaluated. Our scoring function and parsing algorithm have no such limitations. Q can, in principle, use arbitrary information from the history to evaluate constituent inferences. Although some of our feature types are based on prior work (Collins, 1999; Klein &amp; Manning, 2003; Bikel, 2004), we note that our scoring function uses more history information than typical parsers. All features check whether an item has some property; specifically, whether the item’s label/headtag/headword is a certain value. These features perform binary tests on the state directly, unlike Henderson (2003) which works with an intermediate representation of the history. In our baseline setup, feature set (D contained five different feature types, described in Table 1. Table 2 Feature item groups. • all children • all non-head children • all non-leftmost children • all non-rightmost child</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D., &amp; Manning, C. D. (2003). Accurate unlexicalized parsing. In ACL (pp. 423–430).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In ACL</booktitle>
<pages>276--283</pages>
<contexts>
<context position="18735" citStr="Magerman (1995)" startWordPosition="3213" endWordPosition="3214">lits in the parser (summed over all constituent classifiers), LFMS = 87.16, LRCL = 86.32, and LPRC = 88.02. 3.2 Beam Width To determine the effect of the beam width on the accuracy, we evaluated the baseline on the development set using a beam width of 1, i.e. parsing entirely greedily (Wong &amp; Wu, 1999; Kalt, 2004; Sagae &amp; Lavie, 2005). Table 4 compares the base145 Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure. 1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowercase headwords. 9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK. a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information. It’s also worth noting that Collins and Roark (2004) saw a LFMS</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, D. M. (1995). Statistical decision-tree models for parsing. In ACL (pp. 276–283).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>Theory ofsyntactic recognition for natural languages.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2790" citStr="Marcus, 1980" startWordPosition="442" endWordPosition="443"> over contiguous words in the input sentence. Spans cross if they overlap but neither contains the other. An item (or constituent) is a (span, label) pair. A state is a set of parse items, none of which may cross. A parse inference is a pair (S, i), given by the current state S and an item i to be added to it. A parse path (or history) is a sequence of parse inferences over some input sentence (Klein &amp; Manning, 2001). An item ordering (ordering, for short) constrains the order in which items may be inferred. In particular, if we prescribe a complete item ordering, the parser is deterministic (Marcus, 1980) and each state corresponds to a unique parse path. For some input sentence and gold-standard parse, a state is correct if the parser can infer zero or more additional items to obtain the gold-standard parse. A parse path is correct if it leads to a correct state. An 141 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141–151, Vancouver, October 2005. c�2005 Association for Computational Linguistics inference is correct if adding its item to its state is correct. 2 Parsing by Classification Recall that with typical probabilistic parsers, our goal is to out</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. P. (1980). Theory ofsyntactic recognition for natural languages. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1215" citStr="Och &amp; Ney, 2002" startWordPosition="175" endWordPosition="178">sers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser. Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences. Our implementation is freely available at: http://cs.nyu.edu/˜turian/ software/parser/ 1 Introduction Discriminative machine learning methods have improved accuracy on many NLP tasks, such as POStagging (Toutanova et al., 2003), machine translation (Och &amp; Ney, 2002), and relation extraction (Zhao &amp; Grishman, 2005). There are strong reasons to believe the same would be true of parsing. However, only limited advances have been made thus far, perhaps due to various limitations of extant discriminative parsers. In this paper, we present some innovations aimed at reducing or eliminating some of these limitations, specifically for the task of constituent parsing: • We show how constituent parsing can be performed using standard classification techniques. • Classifiers for different non-terminal labels can be induced independently and hence training can be para</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, F. J., &amp; Ney, H. (2002). Discriminative training and maximum entropy models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In EMNLP</booktitle>
<pages>133--142</pages>
<contexts>
<context position="19024" citStr="Ratnaparkhi (1996)" startWordPosition="3260" endWordPosition="3261"> (Wong &amp; Wu, 1999; Kalt, 2004; Sagae &amp; Lavie, 2005). Table 4 compares the base145 Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure. 1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowercase headwords. 9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK. a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information. It’s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation. Figure 2 PARSEVAL scores of the baseline on the &lt;_ 15 words development set of the Penn Treebank. The top x-axis shows accuracy as the min</context>
<context position="27497" citStr="Ratnaparkhi (1996)" startWordPosition="4698" endWordPosition="4699"> our final parser for parsing the test set. 3.7 Test Set Results Table 9 shows the results of our best parser on the &lt; 15 words test set, as well as the accuracy reported for a recent discriminative parser (Taskar et al., 2004) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data. Bikel (2004) is a “clean room” reimplementation of the Collins parser (Collins, 1999) with comparable accuracy. Both Charniak (2000) and Bikel (2004) were trained using the goldstandard tags, as this produced higher accuracy on the development set than using Ratnaparkhi (1996)’s tags. 3.8 Exploratory Data Analysis To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the full vocabulary parser did not get entirely correct. Besides noise and cases of genuine ambiguity, the following list outlines all error types that occurred in more than five sentences, in roughly decreasing order of frequency. (Note that there is some overlap between these groups.) • ADVPs and ADJPs A disproportionate amount of the parser’s error was due to ADJPs and ADVPs. Out of the 12.5% total error of the parser on the development</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. (1996). A maximum entropy partof-speech tagger. In EMNLP (pp. 133–142).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="5462" citStr="Sagae and Lavie (2005)" startWordPosition="909" endWordPosition="912">t with label A. For example, the VPclassifier Qvp would score the VP-inference in Figure 1, preferably assigning it a positive confidence. Figure 1 A candidate VP-inference, with headchildren annotated using the rules given in (Collins, 1999). VP (was) NP (timing) VBD / was ADJP (perfect) DT / The NN / timing JJ / perfect Each A-classifier is independently trained on training set E, where each example e E E is a tuple (I, y), I is a candidate A-inference, and y E {t11. y = +1 if I is a correct inference and −1 otherwise. This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. 2.1.1 Generating Training Examples Our method of generating training examples does not require a working parser, and can be run prior to any training. It is similar to the method used in the literature by deterministic parsers (Yamada &amp; Matsumoto, 2003; Sagae &amp; Lavie, 2005) with one exception: Depending upon the order constituents are inferred, there may be multiple bottom-up paths that lead to the same final parse, so to generate training examples we choose a single random path that leads to the gold-standard parse tree.1 T</context>
<context position="32003" citStr="Sagae and Lavie (2005)" startWordPosition="5464" endWordPosition="5467"> al., 2003), the parser might learn to compensate for tagging errors. • Attachment decisions The parser does not detect affinities between certain word pairs, so it has difficulties with bilexical dependency decisions. In principle, bilexical dependencies can be represented as conjunctions of feature given in Section 2.1.4. Given more training data, the parser might learn these affinities. 4 Conclusions In this work, we presented a near state-of-theart approach to constituency parsing which overcomes some of the limitations of other discriminative parsers. Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. Even though these classifiers themselves never do any parsing during training, they can be combined into an effective parser. We also presented abeam search method under the objective function of maximizing the minimum confidence. To ensure efficiency, some discriminative parsers place stringent requirements on which types of features are permitted. Our approach requires no such restrictions and our scoring function can, in principle, use arbitrary information from the history to evaluate constituent inferences. Even though our features may be of too fine</context>
<context position="33283" citStr="Sagae and Lavie (2005)" startWordPosition="5653" endWordPosition="5656">iscriminatively trained decisions trees determine useful feature combinations automatically, so adding new features requires minimal human effort. Training discriminative parsers is notoriously slow, especially if it requires generating examples by repeatedly parsing the treebank (Collins &amp; Roark, 2004; Taskar et al., 2004). Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). This parser serves as a proof-of-concept, in that we have not fully exploited the possibilities of engineering intricate features or trying more complex search methods. Its flexibility offers many opportunities for improvement, which we leave to future work. Acknowledgments The authors would like to thank Dan Bikel, Mike Collins, Ralph Grishman, Adam Meyers, Mehryar Mohri, Satoshi Sekine, and Wei Wang, as well as the anonymous reviewers, for their helpful comments 150 and constructive criticism. This research was sponsored by an NSF CAREER award, and by an equipment gift from Sun Microsystem</context>
<context position="5806" citStr="Sagae &amp; Lavie, 2005" startWordPosition="968" endWordPosition="971">ndependently trained on training set E, where each example e E E is a tuple (I, y), I is a candidate A-inference, and y E {t11. y = +1 if I is a correct inference and −1 otherwise. This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. 2.1.1 Generating Training Examples Our method of generating training examples does not require a working parser, and can be run prior to any training. It is similar to the method used in the literature by deterministic parsers (Yamada &amp; Matsumoto, 2003; Sagae &amp; Lavie, 2005) with one exception: Depending upon the order constituents are inferred, there may be multiple bottom-up paths that lead to the same final parse, so to generate training examples we choose a single random path that leads to the gold-standard parse tree.1 The training examples correspond to all candidate inferences considered in every state along this path, nearly all of which are incorrect inferences (with y = −1). For instance, only 4.4% of candidate NP-inferences are correct. 2.1.2 Training Algorithm During training, for each label A we induce scoring function Q to minimize the loss over tr</context>
<context position="18457" citStr="Sagae &amp; Lavie, 2005" startWordPosition="3164" endWordPosition="3167"> states were popped from the agenda. Figure 2 shows the accuracy of the baseline on the development set as training progresses. Cross-validating the choice of O against the LFMS (Section 2.1.3) suggested an optimum of O = 1.42. At this ˆ�, there were a total of 9297 decision tree splits in the parser (summed over all constituent classifiers), LFMS = 87.16, LRCL = 86.32, and LPRC = 88.02. 3.2 Beam Width To determine the effect of the beam width on the accuracy, we evaluated the baseline on the development set using a beam width of 1, i.e. parsing entirely greedily (Wong &amp; Wu, 1999; Kalt, 2004; Sagae &amp; Lavie, 2005). Table 4 compares the base145 Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure. 1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996). 8. Lowercase headwords. 9. Repl</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Sagae, K., &amp; Lavie, A. (2005). A classifier-based parser with linear run-time complexity. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<pages>297--336</pages>
<contexts>
<context position="13528" citStr="Schapire and Singer (1999)" startWordPosition="2341" endWordPosition="2344">the cumulative score of a parse path P, we apply aggregator A over the confidences Q(I) in Equation 4. Initially, we defined A in the customary fashion as summing the loss of each inference’s confidence: Pˆ = arg max E�������− ������ (15) PEP(x) IEP L (Q(I)) � with the logistic loss L as defined in Equation 6. (We negate the final sum because we want to minimize the loss.) This definition of A is motivated by viewing L as a negative log-likelihood given by a logistic function (Collins et al., 2002), and then using Equation 3. It is also inspired by the multiclass loss-based decoding method of Schapire and Singer (1999). With this additive aggregator, loss monotonically increases as inferences are added, as in a PCFG-based parser in which all productions decrease the cumulative probability of the parse tree. In preliminary experiments, this aggregator gave disappointing results: precision increased slightly, but recall dropped sharply. Exploratory data analysis revealed that, because each inference incurs some positive loss, the aggregator very cautiously builds the smallest trees possible, thus harming recall. We had more success by defining A to maximize the minimum confidence. Essentially, min Q(I) (16) I</context>
<context position="7191" citStr="Schapire &amp; Singer, 1999" startWordPosition="1202" endWordPosition="1205">d implementation so that our results can be replicated under the same experimental conditions. P = arg max PP(x) A IP 142 where y · Q(I) is the margin of example (I, y). Hence, the learning task is to maximize the margins of the training examples, i.e. induce scoring function Q such that it classifies correct inferences with positive confidence and incorrect inferences with negative confidence. In our work, we minimized the logistic loss: L(z) = log(1 + exp(−z)) (6) i.e. the negative log-likelihood of the training sample. Our classifiers are ensembles of decisions trees, which we boost (Schapire &amp; Singer, 1999) to minimize the above loss using the update equations given in Collins, Schapire, and Singer (2002). More specifically, classifier QT is an ensemble comprising decision trees q1, ... , qT, where: T QT  (I) = Z qt(I) (7) t=1 At iteration t, decision tree qt  is grown, its leaves are confidence-rated, and it is added to the ensemble. The classifier for each constituent label is trained independently, so we henceforth omit λ subscripts. An example (I, y) is assigned weight wt(I, y):2 1 wt(I,y) = (8) 1 + exp(y · Qt−1(I)) The total weight ofy-value examples that fall in leaf f is Wtf,y: Wt</context>
<context position="8907" citStr="Schapire &amp; Singer, 1999" startWordPosition="1524" endWordPosition="1527">oss provided superior generalization accuracy. We will use Zt as a shorthand for Z(qt). When growing the decision tree, we greedily choose node splits to minimize this Z (Kearns &amp; Mansour, 1999). In particular, the loss reduction of splitting leaf f using feature φ into two children, f A φ and f A -,φ, is ΔZt f (φ): ΔZtf(φ) = Ztf − (Ztf + Ztf¬) (12) To split node f, we choose the φˆ that reduces loss the most: φˆ = argmax ΔZtf(φ) (13)  Confidence-rating the leaves: Each leaf f is confidence-rated as κtf: Wt 1 f,+ + � κt f = 2 · log (14) Wt f,− + � Equation 14 is smoothed by the c term (Schapire &amp; Singer, 1999) to prevent numerical instability in the case that either Wtf,+or Wt f,− is 0. In our experiments, we used c = 10−8. Although our example weights are unnormalized, so far we’ve found no benefit from scaling c as Collins and Koo (2005) suggest. All inferences that fall in a particular leaf node are assigned the same confidence: if inference I falls in leaf node f in the tth decision tree, then qt(I) = κtf . 2.1.3 Calibrating the Sub-Classifiers An important concern is when to stop growing the decision tree. We propose the minimum reduction in loss (MRL) stopping criterion: During training, ther</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>Schapire, R. E., &amp; Singer, Y. (1999). Improved boosting using confidence-rated predictions. Machine Learning, 37(3), 297–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="27106" citStr="Taskar et al., 2004" startWordPosition="4633" endWordPosition="4636">bserved in the training data. As evidenced by the results therein, controlling for lexical sparsity did not significantly improve accuracy in our setting. In fact, the full vocabulary run is slightly more accurate than the baseline on the development set, although this difference was not statistically significant. This was a late-breaking result, and we used the full vocabulary condition as our final parser for parsing the test set. 3.7 Test Set Results Table 9 shows the results of our best parser on the &lt; 15 words test set, as well as the accuracy reported for a recent discriminative parser (Taskar et al., 2004) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data. Bikel (2004) is a “clean room” reimplementation of the Collins parser (Collins, 1999) with comparable accuracy. Both Charniak (2000) and Bikel (2004) were trained using the goldstandard tags, as this produced higher accuracy on the development set than using Ratnaparkhi (1996)’s tags. 3.8 Exploratory Data Analysis To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the full vocabulary parser did not get entirely corr</context>
<context position="30204" citStr="Taskar et al. (2004)" startWordPosition="5179" endWordPosition="5182"> example, the development set contains the sentence “The dollar was trading sharply lower in Tokyo .”, with “sharply lower” bracketed as “(ADVP (ADVP sharply) lower)”. “sharply lower” appears 16 times in the complete training section, every time bracketed as “(ADVP sharply lower)”, and “sharply higher” 10 times, always as “(ADVP sharply higher)”. Because of the high number of negative examples, the classifiers’ 149 Table 9 PARSEVAL results of on the &lt;_ 15 words test set of various parsers in the literature. The differences between the full vocabulary run and Bikel or Charniak are significant. Taskar et al. (2004)’s output was unavailable for significance testing, but presumably its differences from the full vocab parser are also significant. Test Test Test Dev Dev Dev LFMS LRCL LPRC LFMS LRCL LPRC Full vocab 87.13 86.47 87.80 87.50 86.85 88.15 Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22 Taskar et al. (2004) 89.12 89.10 89.14 89.98 90.22 89.74 Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32 bias is to cope with the noise by favoring negative confidences predictions for ambiguous ADJP and ADVP decisions, hence their abysmal labelled recall. One potential solution is the weight-sharing strategy </context>
<context position="32986" citStr="Taskar et al., 2004" startWordPosition="5606" endWordPosition="5609"> of features are permitted. Our approach requires no such restrictions and our scoring function can, in principle, use arbitrary information from the history to evaluate constituent inferences. Even though our features may be of too fine granularity to discriminate through linear combination, discriminatively trained decisions trees determine useful feature combinations automatically, so adding new features requires minimal human effort. Training discriminative parsers is notoriously slow, especially if it requires generating examples by repeatedly parsing the treebank (Collins &amp; Roark, 2004; Taskar et al., 2004). Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). This parser serves as a proof-of-concept, in that we have not fully exploited the possibilities of engineering intricate features or trying more complex search methods. Its flexibility offers many opportunities for improvement, which we leave to future work. Acknowledgments The authors would like to </context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, B., Klein, D., Collins, M., Koller, D., &amp; Manning, C. (2004). Max-margin parsing. In EMNLP (pp. 1–8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Taylor</author>
<author>M Marcus</author>
<author>B Santorini</author>
</authors>
<title>The Penn Treebank: an overview. In A. Abeill´e (Ed.), Treebanks: Building and using parsed corpora</title>
<date>2003</date>
<pages>5--22</pages>
<contexts>
<context position="4745" citStr="Taylor, Marcus, &amp; Santorini, 2003" startWordPosition="775" endWordPosition="780">Function Q(I) During training, our goal is to induce the scoring function Q, which assigns a real-valued confidence score Q(I) to each candidate inference I (Equation 4). We treat this as a classification task: If inference I is correct, we would like Q(I) to be a positive value, and if inference I is incorrect, we would like Q(I) to be a negative value. Training discriminative parsers can be computationally very expensive. Instead of having a single classifier score every inference, we parallelize training by inducing 26 sub-classifiers, one for each constituent label A in the Penn Treebank (Taylor, Marcus, &amp; Santorini, 2003): Q(I) = Q(I), where Q is the A-classifier and I is an inference that infers a constituent with label A. For example, the VPclassifier Qvp would score the VP-inference in Figure 1, preferably assigning it a positive confidence. Figure 1 A candidate VP-inference, with headchildren annotated using the rules given in (Collins, 1999). VP (was) NP (timing) VBD / was ADJP (perfect) DT / The NN / timing JJ / perfect Each A-classifier is independently trained on training set E, where each example e E E is a tuple (I, y), I is a candidate A-inference, and y E {t11. y = +1 if I is a correct </context>
<context position="15892" citStr="Taylor et al., 2003" startWordPosition="2730" endWordPosition="2733">ost children items that has headword “quux”? (False) consider every possible sequence of inferences, we use beam search to restrict the size of P(x). As an additional guard against excessive computation, search stopped if more than a fixed maximum number of states were popped from the agenda. As usual, search also ended if the highest-priority state in the agenda could not have a better aggregated score than the best final parse found thus far. 3 Experiments Following Taskar, Klein, Collins, Koller, and Manning (2004), we trained and tested on &lt; 15 word sentences in the English Penn Treebank (Taylor et al., 2003), 10% of the entire treebank by word count.3 We used sections 02–21 (9753 sentences) for training, section 24 (321 sentences) for development, and section 23 (603 sentences) for testing, preprocessed as per Table 3. We evaluated our parser using the standard PARSEVAL measures (Black et al., 1991): labelled precision, recall, and F-measure (LPRC, LRCL, and LFMS, respectively), which are computed based on the number of constituents in the parser’s output that match those in the gold-standard parse. We tested whether the observed differences in PARSEVAL measures are significant at p = 0.05 using </context>
</contexts>
<marker>Taylor, Marcus, Santorini, 2003</marker>
<rawString>Taylor, A., Marcus, M., &amp; Santorini, B. (2003). The Penn Treebank: an overview. In A. Abeill´e (Ed.), Treebanks: Building and using parsed corpora (pp. 5–22).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>HLT/NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="1176" citStr="Toutanova et al., 2003" startWordPosition="169" endWordPosition="172">ions for faster training of discriminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser. Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences. Our implementation is freely available at: http://cs.nyu.edu/˜turian/ software/parser/ 1 Introduction Discriminative machine learning methods have improved accuracy on many NLP tasks, such as POStagging (Toutanova et al., 2003), machine translation (Och &amp; Ney, 2002), and relation extraction (Zhao &amp; Grishman, 2005). There are strong reasons to believe the same would be true of parsing. However, only limited advances have been made thus far, perhaps due to various limitations of extant discriminative parsers. In this paper, we present some innovations aimed at reducing or eliminating some of these limitations, specifically for the task of constituent parsing: • We show how constituent parsing can be performed using standard classification techniques. • Classifiers for different non-terminal labels can be induced indep</context>
<context position="31392" citStr="Toutanova et al., 2003" startWordPosition="5369" endWordPosition="5372">ion is the weight-sharing strategy described in Section 3.5. • Tagging Errors Many of the parser’s errors were due to poor tagging. Preprocessing sentence “Would service be voluntary or compulsory ?” gives “would/MD service/VB be/VB voluntary/JJ or/CC UNK/JJ” and, as a result, the parser brackets “service ... compulsory” as a VP instead of correctly bracketing “service” as an NP. We also found that the tagger we used has difficulties with completely capitalized words, and tends to tag them NNP. By giving the parser access to the same features used by taggers, especially rich lexical features (Toutanova et al., 2003), the parser might learn to compensate for tagging errors. • Attachment decisions The parser does not detect affinities between certain word pairs, so it has difficulties with bilexical dependency decisions. In principle, bilexical dependencies can be represented as conjunctions of feature given in Section 2.1.4. Given more training data, the parser might learn these affinities. 4 Conclusions In this work, we presented a near state-of-theart approach to constituency parsing which overcomes some of the limitations of other discriminative parsers. Like Yamada and Matsumoto (2003) and Sagae and L</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Toutanova, K., Klein, D., Manning, C. D., &amp; Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In HLT/NAACL (pp. 252–259).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wong</author>
<author>D Wu</author>
</authors>
<title>Learning a lightweight robust deterministic parser.</title>
<date>1999</date>
<booktitle>In EUROSPEECH.</booktitle>
<contexts>
<context position="18423" citStr="Wong &amp; Wu, 1999" startWordPosition="3158" endWordPosition="3161">e event that more than 10,000 states were popped from the agenda. Figure 2 shows the accuracy of the baseline on the development set as training progresses. Cross-validating the choice of O against the LFMS (Section 2.1.3) suggested an optimum of O = 1.42. At this ˆ�, there were a total of 9297 decision tree splits in the parser (summed over all constituent classifiers), LFMS = 87.16, LRCL = 86.32, and LPRC = 88.02. 3.2 Beam Width To determine the effect of the beam width on the accuracy, we evaluated the baseline on the development set using a beam width of 1, i.e. parsing entirely greedily (Wong &amp; Wu, 1999; Kalt, 2004; Sagae &amp; Lavie, 2005). Table 4 compares the base145 Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure. 1. * Strip functional tags and trace indices, and remove traces. 2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).) 3. Remove quotation marks (i.e. terminal items tagged ‘‘ or ’’). (Bikel, 2004) 4. * Raise punctuation. (Bikel, 2004) 5. Remove outermost punctuation.&apos; 6. * Remove unary projections to self (i.e. duplicate items with the same span and label). 7. POS tag the text using Ratnaparkhi (1996</context>
</contexts>
<marker>Wong, Wu, 1999</marker>
<rawString>Wong, A., &amp; Wu, D. (1999). Learning a lightweight robust deterministic parser. In EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="5435" citStr="Yamada and Matsumoto (2003)" startWordPosition="904" endWordPosition="907">ference that infers a constituent with label A. For example, the VPclassifier Qvp would score the VP-inference in Figure 1, preferably assigning it a positive confidence. Figure 1 A candidate VP-inference, with headchildren annotated using the rules given in (Collins, 1999). VP (was) NP (timing) VBD / was ADJP (perfect) DT / The NN / timing JJ / perfect Each A-classifier is independently trained on training set E, where each example e E E is a tuple (I, y), I is a candidate A-inference, and y E {t11. y = +1 if I is a correct inference and −1 otherwise. This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. 2.1.1 Generating Training Examples Our method of generating training examples does not require a working parser, and can be run prior to any training. It is similar to the method used in the literature by deterministic parsers (Yamada &amp; Matsumoto, 2003; Sagae &amp; Lavie, 2005) with one exception: Depending upon the order constituents are inferred, there may be multiple bottom-up paths that lead to the same final parse, so to generate training examples we choose a single random path that leads to the g</context>
<context position="31976" citStr="Yamada and Matsumoto (2003)" startWordPosition="5459" endWordPosition="5462">h lexical features (Toutanova et al., 2003), the parser might learn to compensate for tagging errors. • Attachment decisions The parser does not detect affinities between certain word pairs, so it has difficulties with bilexical dependency decisions. In principle, bilexical dependencies can be represented as conjunctions of feature given in Section 2.1.4. Given more training data, the parser might learn these affinities. 4 Conclusions In this work, we presented a near state-of-theart approach to constituency parsing which overcomes some of the limitations of other discriminative parsers. Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. Even though these classifiers themselves never do any parsing during training, they can be combined into an effective parser. We also presented abeam search method under the objective function of maximizing the minimum confidence. To ensure efficiency, some discriminative parsers place stringent requirements on which types of features are permitted. Our approach requires no such restrictions and our scoring function can, in principle, use arbitrary information from the history to evaluate constituent inferences. Even though our </context>
<context position="33256" citStr="Yamada and Matsumoto (2003)" startWordPosition="5648" endWordPosition="5651">te through linear combination, discriminatively trained decisions trees determine useful feature combinations automatically, so adding new features requires minimal human effort. Training discriminative parsers is notoriously slow, especially if it requires generating examples by repeatedly parsing the treebank (Collins &amp; Roark, 2004; Taskar et al., 2004). Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). This parser serves as a proof-of-concept, in that we have not fully exploited the possibilities of engineering intricate features or trying more complex search methods. Its flexibility offers many opportunities for improvement, which we leave to future work. Acknowledgments The authors would like to thank Dan Bikel, Mike Collins, Ralph Grishman, Adam Meyers, Mehryar Mohri, Satoshi Sekine, and Wei Wang, as well as the anonymous reviewers, for their helpful comments 150 and constructive criticism. This research was sponsored by an NSF CAREER award, and by an equipmen</context>
<context position="5784" citStr="Yamada &amp; Matsumoto, 2003" startWordPosition="963" endWordPosition="967">ect Each A-classifier is independently trained on training set E, where each example e E E is a tuple (I, y), I is a candidate A-inference, and y E {t11. y = +1 if I is a correct inference and −1 otherwise. This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. 2.1.1 Generating Training Examples Our method of generating training examples does not require a working parser, and can be run prior to any training. It is similar to the method used in the literature by deterministic parsers (Yamada &amp; Matsumoto, 2003; Sagae &amp; Lavie, 2005) with one exception: Depending upon the order constituents are inferred, there may be multiple bottom-up paths that lead to the same final parse, so to generate training examples we choose a single random path that leads to the gold-standard parse tree.1 The training examples correspond to all candidate inferences considered in every state along this path, nearly all of which are incorrect inferences (with y = −1). For instance, only 4.4% of candidate NP-inferences are correct. 2.1.2 Training Algorithm During training, for each label A we induce scoring function Q to min</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, H., &amp; Matsumoto, Y. (2003). Statistical dependency analysis with support vector machines. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhao</author>
<author>R Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1264" citStr="Zhao &amp; Grishman, 2005" startWordPosition="182" endWordPosition="185">zed, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser. Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences. Our implementation is freely available at: http://cs.nyu.edu/˜turian/ software/parser/ 1 Introduction Discriminative machine learning methods have improved accuracy on many NLP tasks, such as POStagging (Toutanova et al., 2003), machine translation (Och &amp; Ney, 2002), and relation extraction (Zhao &amp; Grishman, 2005). There are strong reasons to believe the same would be true of parsing. However, only limited advances have been made thus far, perhaps due to various limitations of extant discriminative parsers. In this paper, we present some innovations aimed at reducing or eliminating some of these limitations, specifically for the task of constituent parsing: • We show how constituent parsing can be performed using standard classification techniques. • Classifiers for different non-terminal labels can be induced independently and hence training can be parallelized. • The parser can use arbitrary informat</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Zhao, S., &amp; Grishman, R. (2005). Extracting relations with integrated information using kernel methods. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>