<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988584">
Intricacies of Collins’ Parsing Model
</title>
<author confidence="0.999103">
Daniel M. Bikel∗
</author>
<affiliation confidence="0.991512">
University of Pennsylvania
</affiliation>
<bodyText confidence="0.999095916666667">
This article documents a large set of heretofore unpublished details Collins used in his parser, such
that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate
Collins’ benchmark results. Indeed, these as-yet-unpublished details account for an 11% relative
increase in error from an implementation including all details to a clean-room implementation of
Collins’ model. We also show a cleaner and equally well-performing method for the handling of
punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain
well-known details, revealing that bilexical dependencies are barely used by the model and that
head choice is not nearly as important to overall parsing performance as once thought. Finally,
we perform experiments that show that the true discriminative power of lexicalization appears to
lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword
and its part of speech.
</bodyText>
<sectionHeader confidence="0.997005" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999862555555556">
Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the
field of natural language processing. Not only did they achieve new performance
benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz
1993), and not only did they serve as the basis of Collins’ own future work (Collins
2000; Collins and Duffy 2002), but they also served as the basis of important work
on parser selection (Henderson and Brill 1999), an investigation of corpus variation
and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa
2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the
automatic labeling of semantic roles and predicate-argument extraction (Gildea and
Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts.
Recently, in order to continue our work combining word sense with parsing (Bikel
2000) and the study of language-dependent and -independent parsing features (Bikel
and Chiang 2000), we built a multilingual parsing engine that is capable of instanti-
ating a wide variety of generative statistical parsing models (Bikel 2002).1 As an ap-
propriate baseline model, we chose to instantiate the parameters of Collins’ Model 2.
This task proved more difficult than it initially appeared. Starting with Collins’ (1999)
thesis, we reproduced all the parameters described but did not achieve nearly the
same high performance on the well-established development test set of Section 00 of
</bodyText>
<affiliation confidence="0.633004">
∗ Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104. E-mail:
</affiliation>
<email confidence="0.683934">
dbikel@linc.cis.upenn.edu
</email>
<footnote confidence="0.623416">
1 This engine is publicly available at http://www.cis.upenn.edu/∼dbikel/software.html
</footnote>
<note confidence="0.85439425">
Submission received: 18 January 2003; Revised submission received: 20 March 2004; Accepted for
publication: 10 June 2004
© 2004 Association for Computational Linguistics
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.998187642857143">
the Penn Treebank. Together with Collins’ thesis, this article contains all the informa-
tion necessary to replicate Collins’ parsing results.2 Specifically, this article describes
all the as-yet-unpublished details and features of Collins’ model and some analysis
of the effect of these features with respect to parsing performance, as well as some
comparative analysis of the effects of published features.3 In particular, implementing
Collins’ model using only the published details causes an 11% increase in relative error
over Collins’ own published results. That is, taken together, all the unpublished details
have a significant effect on overall parsing performance. In addition to the effects of
the unpublished details, we also have new evidence to show that the discriminative
power of Collins’ model does not lie where once thought: Bilexical dependencies play
an extremely small role in Collins’ models (Gildea 2001), and head choice is not nearly
as critical as once thought. This article also discusses the rationale for various param-
eter choices. In general, we will limit our discussion to Collins’ Model 2, but we make
occasional reference to Model 3, as well.
</bodyText>
<sectionHeader confidence="0.995899" genericHeader="keywords">
2. Motivation
</sectionHeader>
<bodyText confidence="0.94082537037037">
There are three primary motivations for this work. First, Collins’ parsing model repre-
sents a widely used and cited parsing model. As such, if it is not desirable to use it as
a black box (it has only recently been made publicly available), then it should be pos-
sible to replicate the model in full, providing a necessary consistency among research
efforts employing it. Careful examination of its intricacies will also allow researchers
to deviate from the original model when they think it is warranted and accurately
document those deviations, as well as understand the implications of doing so.
The second motivation is related to the first: science dictates that experiments be
replicable, for this is the way we may test and validate them. The work described here
comes in the wake of several previous efforts to replicate this particular model, but
this is the first such effort to provide a faithful and equally well-performing emulation
of the original.
The third motivation is that a deep understanding of an existing model—its in-
tricacies, the interplay of its many features—provides the necessary platform for ad-
vancement to newer, “better” models. This is especially true in an area like statis-
tical parsing that has seen rapid maturation followed by a soft “plateau” in per-
formance. Rather than simply throwing features into a new model and measuring
their effect in a crude way using standard evaluation metrics, this work aims to
provide a more thorough understanding of the nature of a model’s features. This
understanding not only is useful in its own right but should help point the way
toward newer features to model or better modeling techniques, for we are in the
best position for advancement when we understand existing strengths and limita-
tions.
2 In the course of replicating Collins’ results, it was brought to our attention that several other
researchers had also tried to do this and had also gotten performance that fell short of Collins’
published results. For example, Gildea (2001) reimplemented Collins’ Model 1 but obtained results
with roughly 16.7% more relative error than Collins’ reported results using that model.
</bodyText>
<footnote confidence="0.956205">
3 Discovering these details and features involved a great deal of reverse engineering, and ultimately,
much discussion with Collins himself and perusal of his code. Many thanks to Mike Collins for his
generosity. As a word of caution, this article is exhaustive in its presentation of all such details and
features, and we cannot guarantee that every reader will find every detail interesting.
</footnote>
<page confidence="0.993135">
480
</page>
<note confidence="0.920237">
Bikel Intricacies of Collins’ Parsing Model
</note>
<sectionHeader confidence="0.806341" genericHeader="introduction">
3. Model Overview
</sectionHeader>
<bodyText confidence="0.999551714285714">
The Collins parsing model decomposes the generation of a parse tree into many small
steps, using reasonable independence assumptions to make the parameter estimation
problem tractable. Even though decoding proceeds bottom-up, the model is defined
in a top-down manner. Every nonterminal label in every tree is lexicalized: the label
is augmented to include a unique headword (and that headword’s part of speech) that
the node dominates. The lexicalized PCFG that sits behind Model 2 has rules of the
form
</bodyText>
<equation confidence="0.995558">
P —&apos; LnLn−1 ··· L1HR1 ··· Rn−1Rn (1)
</equation>
<bodyText confidence="0.999957105263158">
where P, L;, R;, and H are all lexicalized nonterminals, and P inherits its lexical head
from its distinguished head-child, H. In this generative model, first P is generated, then
its head-child H, then each of the left- and right-modifying nonterminals are generated
from the head outward. The modifying nonterminals L; and R; are generated condi-
tioning on P and H, as well as a distance metric (based on what material intervenes
between the currently generated modifying nonterminal and H) and an incremental
subcategorization frame feature (a multiset containing the arguments of H that have
yet to be generated on the side of H in which the currently generated nonterminal
falls). Note that if the modifying nonterminals were generated completely indepen-
dently, the model would be very impoverished, but in actuality, because it includes
the distance and subcategorization frame features, the model captures a crucial bit of
linguistic reality, namely, that words often have well-defined sets of complements and
adjuncts, occurring with some well-defined distribution in the right-hand sides of a
(context-free) rewriting system.
The process proceeds recursively, treating each newly generated modifier as a
parent and then generating its head and modifier children; the process terminates
when (lexicalized) preterminals are generated. As a way to guarantee the consistency
of the model, the model also generates two hidden +STOP+ nonterminals as the leftmost
and rightmost children of every parent (see Figure 7).
</bodyText>
<sectionHeader confidence="0.868249" genericHeader="method">
4. Preprocessing Training Trees
</sectionHeader>
<bodyText confidence="0.999373">
To the casual reader of Collins’ thesis, it may not be immediately apparent that there
are quite a few preprocessing steps for each annotated training tree and that these
steps are crucial to the performance of the parser. We identified 11 preprocessing steps
necessary to prepare training trees when using Collins’ parsing model:
</bodyText>
<listItem confidence="0.999024666666667">
1. pruning of unnecessary nodes
2. adding base NP nodes (NPBs)
3. “repairing” base NPs
4. adding gap information (applicable to Model 3 only)
5. relabeling of sentences with no subjects (subjectless sentences)
6. removing null elements
7. raising punctuation
8. identification of argument nonterminals
9. stripping unused nonterminal augmentations
</listItem>
<page confidence="0.996333">
481
</page>
<figure confidence="0.567355666666667">
Computational Linguistics Volume 30, Number 4
10. “repairing” subjectless sentences
11. head-finding
</figure>
<bodyText confidence="0.999693571428572">
The order of presentation in the foregoing list is not arbitrary, as some of the steps
depend on results produced in previous steps. Also, we have separated the steps into
their functional units; an implementation could combine steps that are independent
of one another (for clarity, our implementation does not, however). Finally, we note
that the final step, head-finding, is actually required by some of the previous steps
in certain cases; in our implementation, we selectively employ a head-finding module
during the first 10 steps where necessary.
</bodyText>
<subsectionHeader confidence="0.963712">
4.1 Coordinated Phrases
</subsectionHeader>
<bodyText confidence="0.9997335">
A few of the preprocessing steps rely on the notion of a coordinated phrase. In this
article, the conditions under which a phrase is considered coordinated are slightly
more detailed than is described in Collins’ thesis. A node represents a coordinated
phrase if
</bodyText>
<listItem confidence="0.9858706">
• it has a nonhead child that is a coordinating conjunction and
• that conjunction is either
• posthead but nonfinal, or
• immediately prehead but noninitial (where “immediately”
means “with nothing intervening except punctuation”).4
</listItem>
<bodyText confidence="0.94615875">
In the Penn Treebank, a coordinating conjunction is any preterminal node with the
label CC. This definition essentially picks out all phrases in which the head-child is
truly conjoined to some other phrase, as opposed to a phrase in which, say, there is
an initial CC, such as an S that begins with the conjunction but.
</bodyText>
<subsectionHeader confidence="0.999011">
4.2 Pruning of Unnecessary Nodes
</subsectionHeader>
<bodyText confidence="0.999934444444444">
As a preprocessing step, pruning of unnecessary nodes simply removes preterminals
that should have little or no bearing on parser performance. In the case of the English
Treebank, the pruned subtrees are all preterminal subtrees whose root label is one of
{‘‘, ’’, .}. There are two reasons to remove these types of subtrees when parsing
the English Treebank: First, in the treebanking guidelines (Bies 1995), quotation marks
were given the lowest possible priority and thus cannot be expected to appear within
constituent boundaries in any kind of consistent way, and second, neither of these
types of preterminals—nor any punctuation marks, for that matter—counts towards
the parsing score.
</bodyText>
<subsectionHeader confidence="0.999299">
4.3 Adding Base NP Nodes
</subsectionHeader>
<bodyText confidence="0.998908333333333">
An NP is basal when it does not itself dominate an NP; such NP nodes are relabeled
NPB. More accurately, an NP is basal when it dominates no other NPs except possessive
NPs, where a possessive NP is an NP that dominates POS, the preterminal possessive
</bodyText>
<footnote confidence="0.968594">
4 Our positional descriptions here, such as “posthead but nonfinal,” refer to positions within the list of
immediately dominated children of the coordinated phrase node, as opposed to positions within the
entire sentence.
</footnote>
<page confidence="0.98981">
482
</page>
<figure confidence="0.979663617647059">
Bikel Intricacies of Collins’ Parsing Model
NP
✟✟ ❍ ❍
NP
✟✟ ❍ ❍
NP
✟✟ ❍ ❍
NP
NNP
John
CC
and
NP
NNP
Jane
NPB
NNP
John
CC
and
NPB
NNP
Jane
NP
NPB
NNP
John
CC
and
NP
NPB
NNP
Jane
(a) Coordinated phrase (b) Base NPs relabeled (c) Extra NP nodes inserted
</figure>
<figureCaption confidence="0.673914">
Figure 1
</figureCaption>
<figure confidence="0.787561074074074">
An NP that constitutes a coordinated phrase.
NP
❍
✟✟✟✟✟ ❍❍ ❍ ❍
, NPB
✏✏✏ �� �
, Tom Foolery
NP
❍
✟✟✟✟✟ ❍❍ ❍ ❍
NPB
✏✏✏ �
� �
NPB
✏✏✏ �� �
Tom Foolery
NPB
✏✏✏ �
� �
the comedian
the comedian
,
,
NP
(a) Before extra NP addition
(the NPB the comedian is the head (b) After extra NP insertion.
child).
</figure>
<figureCaption confidence="0.979647">
Figure 2
</figureCaption>
<bodyText confidence="0.99552975">
A nonhead NPB child of NP requires insertion of extra NP.
marker for the Penn Treebank. These possessive NPs are almost always themselves
base NPs and are therefore (almost always) relabeled NPB.
For consistency’s sake, when an NP has been relabeled as NPB, a normal NP node
is often inserted as a parent nonterminal. This insertion ensures that NPB nodes are
always dominated by NP nodes. The conditions for inserting this “extra” NP level are
slightly more detailed than is described in Collins’ thesis, however. The extra NP level
is added if one of the following conditions holds:
</bodyText>
<listItem confidence="0.985542857142857">
• The parent of the NPB is not an NP.
• The parent of the NPB is an NP but constitutes a coordinated phrase (see
Figure 1).
• The parent of the NPB is an NP but
• the parent’s head-child is not the NPB, and
• the parent has not already been relabeled as an NPB (see
Figure 2).5
</listItem>
<bodyText confidence="0.912099333333333">
In postprocessing, when an NPB is an only child of an NP node, the extra NP level
is removed by merging the two nodes into a single NP node, and all remaining NPB
nodes are relabeled NP.
</bodyText>
<footnote confidence="0.601408">
5 Only applicable if relabeling of NPs is performed using a preorder tree traversal.
</footnote>
<page confidence="0.997538">
483
</page>
<figure confidence="0.972384454545455">
Computational Linguistics Volume 30, Number 4
VB
NP
NPB
❍
✟✟✟✟ ❍ ❍ ❍
NN S
✏✏✏ �
��
will to continue
need
VP
✟✟✟ ❍ ❍ ❍
✟VP
DT
the
NPB
✟❍
DT
the
S
✏✏✏ �� �
to continue
need
VB
NP
✟✟✟ ❍
❍❍
NN
will
(a) Before repair. (b) After repair.
Figure 3
An NPB is “repaired.”
</figure>
<subsectionHeader confidence="0.998785">
4.4 Repairing Base NPs
</subsectionHeader>
<bodyText confidence="0.999982777777778">
The insertion of extra NP levels above certain NPB nodes achieves a degree of con-
sistency for NPs, effectively causing the portion of the model that generates children
of NP nodes to have less perplexity. Collins appears to have made a similar effort to
improve the consistency of the NPB model. NPB nodes that have sentential nodes as
their final (rightmost) child are “repaired”: The sentential child is raised so that it
becomes a new right-sibling of the NPB node (see Figure 3).6 While such a transforma-
tion is reasonable, it is interesting to note that Collins’ parser performs no equivalent
detransformation when parsing is complete, meaning that when the parser produces
the “repaired” structure during testing, there is a spurious NP bracket.7
</bodyText>
<subsectionHeader confidence="0.999841">
4.5 Adding Gap Information
</subsectionHeader>
<bodyText confidence="0.9999959">
The gap feature is discussed extensively in chapter 7 of Collins’ thesis and is applicable
only to his Model 3. The preprocessing step in which gap information is added locates
every null element preterminal, finds its co-indexed WHNP antecedent higher up in the
tree, replaces the null element preterminal with a special trace tag, and threads the
gap feature in every nonterminal in the chain between the common ancestor of the
antecedent and the trace. The threaded-gap feature is represented by appending -g to
every node label in the chain. The only detail we would like to highlight here is that an
implementation of this preprocessing step should check for cases in which threading
is impossible, such as when two filler-gap dependencies cross. An implementation
should be able to handle nested filler-gap dependencies, however.
</bodyText>
<subsectionHeader confidence="0.997356">
4.6 Relabeling Subjectless Sentences
</subsectionHeader>
<bodyText confidence="0.99994875">
The node labels of sentences with no subjects are transformed from S to SG. This step
enables the parsing model to be sensitive to the different contexts in which such sub-
jectless sentences occur as compared to normal S nodes, since the subjectless sentences
are functionally acting as noun phrases. Collins’ example of
</bodyText>
<equation confidence="0.419929">
[S [S Flying planes] is dangerous ]
</equation>
<footnote confidence="0.9932115">
6 Collins defines a sentential node, for the purposes of repairing NPBs, to be any node that begins with
the letter S. For the Penn Treebank, this defines the set {S, SBAR, SBARQ, SINV, SQ}.
7 Since, as mentioned above, the only time an NPB is merged with its parent is when it is the only child
of an NP.
</footnote>
<page confidence="0.998062">
484
</page>
<figure confidence="0.992573878787879">
Bikel Intricacies of Collins’ Parsing Model
NP
✟✟✟✟ ❍❍ ❍ ❍
−→
,
,
NP
✟ ❍
NP
✟ ❍
NNP ,
John
,
,
,
NP
✘✘✘✘✘✘
✟✟✟ ❍ � � � � � �
NP ❍ ❍
,
NP
NNP
John
,CC
and
NP
NNP
Jane
CC
and
NP
NNP
Jane
</figure>
<figureCaption confidence="0.621928666666667">
Figure 4
Raising punctuation: Perverse case in which multiple punctuation elements appear along a
frontier of a subtree.
</figureCaption>
<bodyText confidence="0.9985184">
illustrates the utility of this transformation. However, the conditions under which an
S may be relabeled are not spelled out; one might assume that every S whose subject
(identified in the Penn Treebank with the -SBJ function tag) dominates a null element
should be relabeled SG. In actuality, the conditions are much stricter. An S is relabeled
SG when the following conditions hold:
</bodyText>
<listItem confidence="0.999977666666667">
• One of its children dominates a null element child marked with -SBJ.
• Its head-child is a VP.
• No arguments appear prior to the head-child (see Sections 4.9 and 4.11)
</listItem>
<bodyText confidence="0.9992225">
The latter two conditions appear to be an effort to capture only those subjectless
sentences that are based around gerunds, as in the flying planes example.8
</bodyText>
<subsectionHeader confidence="0.997866">
4.7 Removing Null Elements
</subsectionHeader>
<bodyText confidence="0.9994952">
Removing null elements simply involves pruning the tree to eliminate any subtree
that dominates only null elements. The special trace tag that is inserted in the step
that adds gap information (Section 4.5) is excluded, as it is specifically chosen to be
something other than the null-element preterminal marker (which is -NONE- in the
Penn Treebank).
</bodyText>
<subsectionHeader confidence="0.999192">
4.8 Raising Punctuation
</subsectionHeader>
<bodyText confidence="0.999741818181818">
The step in which punctuation is raised is discussed in detail in chapter 7 of Collins’
thesis. The main idea is to raise punctuation—which is any preterminal subtree in
which the part of speech is either a comma or a colon—to the highest possible point
in the tree, so that it always sits between two other nonterminals. Punctuation that
occurs at the very beginning or end of a sentence is “raised away,” that is, pruned. In
addition, any implementation of this step should handle the case in which multiple
punctuation elements appear as the initial or final children of some node, as well
as the more pathological case in which multiple punctuation elements appear along
the left or right frontier of a subtree (see Figure 4). Finally, it is not clear what to do
with nodes that dominate only punctuation preterminals. Our implementation simply
issues a warning in such cases and leaves the punctuation symbols untouched.
</bodyText>
<footnote confidence="0.568187">
8 We assume the G in the label SG was chosen to stand for the word gerund.
</footnote>
<page confidence="0.995489">
485
</page>
<figure confidence="0.982648666666667">
Computational Linguistics Volume 30, Number 4
S
❍
✟✟✟✟✟✟ ❍❍ ❍ ❍ ❍
VP-HEAD
❍
✟✟✟✟✟ ❍ ❍ ❍ ❍
VBD-HEAD VP-A
was ✟✟✟✟ ❍❍ ❍ ❍
NP-A
NNP
Elizabeth
VBN-HEAD S-A
elected NP-HEAD-A
NPB
✟✟❍❍
DT NN
a director
</figure>
<figureCaption confidence="0.920995">
Figure 5
</figureCaption>
<bodyText confidence="0.771437">
Head-children are not exempt from being relabeled as arguments.
</bodyText>
<subsectionHeader confidence="0.99601">
4.9 Identification of Argument Nonterminals
</subsectionHeader>
<bodyText confidence="0.753286">
Collins employs a small set of heuristics to mark certain nonterminals as arguments, by
appending -A to the nonterminal label. This section reveals three unpublished details
about Collins’ argument finding:
</bodyText>
<listItem confidence="0.934173571428571">
• The published argument-finding rule for PPs is to choose the first
nonterminal after the head-child. In a large majority of cases, this marks
the NP argument of the preposition. The actual rule used is slightly
more complicated: The first nonterminal to the right of the head-child
that is neither PRN nor a part-of-speech tag is marked as an argument.
The nonterminal PRN in the Penn Treebank marks parenthetical
expressions, which can occur fairly often inside a PP, as in the phrase on
(or above) the desk.
• Children that are part of a coordinated phrase (see Section 4.1) are
exempt from being relabeled as argument nonterminals.
• Head-children are distinct from their siblings by virtue of the
head-generation parameter class in the parsing model. In spite of this,
Collins’ trainer actually does not exempt head-children from being
relabeled as arguments (see Figure 5).9
</listItem>
<subsectionHeader confidence="0.947541">
4.10 Stripping Unused Nonterminal Augmentations
</subsectionHeader>
<bodyText confidence="0.9989085">
This step simply involves stripping away all nonterminal augmentations, except those
that have been added from other preprocessing steps (such as the -A augmentation
for argument labels). This includes the stripping away of all function tags and indices
marked by the Treebank annotators.
</bodyText>
<footnote confidence="0.978199">
9 It is not clear why this is done, and so in our parsing engine, we make such behavior optional via a
run-time setting.
</footnote>
<page confidence="0.99685">
486
</page>
<figure confidence="0.902423166666666">
Bikel Intricacies of Collins’ Parsing Model
−→
NP
❍
✟✟✟✟✟ ❍❍ ❍ ❍
NP CC NP-HEAD
NNP and NNP
John Jane
NPB
✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍
NP
❍
✟✟✟✟✟ ❍❍ ❍ ❍
NP-HEAD CC NP
NNP and NNP
John Jane
NNP CC NNP-HEAD
John and Jane
</figure>
<figureCaption confidence="0.986247">
Figure 6
</figureCaption>
<bodyText confidence="0.971195">
Head moves from right to left conjunct in a coordinated phrase, except when the parent
nonterminal is NPB.
</bodyText>
<subsectionHeader confidence="0.912267">
4.11 Repairing Subjectless Sentences
</subsectionHeader>
<bodyText confidence="0.999954666666667">
With arguments identified as described in Section 4.9, if a subjectless sentence is found
to have an argument prior to its head, this step detransforms the SG so that it reverts
to being an S.
</bodyText>
<subsectionHeader confidence="0.981995">
4.12 Head-Finding
</subsectionHeader>
<bodyText confidence="0.958898304347826">
Head-finding is discussed at length in Collins’ thesis, and the head-finding rules used
are included in his Appendix A. There are a few unpublished details worth mention-
ing, however.
There is no head-finding rule for NX nonterminals, so the default rule of picking
the leftmost child is used.10 NX nodes roughly represent the N’ level of syntax and in
practice often denote base NPs. As such, the default rule often picks out a less-than-
ideal head-child, such as an adjective that is the leftmost child in a base NP.
Collins’ thesis discusses a case in which the initial head is modified when it is
found to denote the right conjunct in a coordinated phrase. That is, if the head rules
pick out a head that is preceded by a CC that is non-initial, the head should be modified
to be the nonterminal immediately to the left of the CC (see Figure 6). An important
detail is that such “head movement” does not occur inside base NPs. That is, a phrase
headed by NPB may indeed look as though it constitutes a coordinated phrase—it has
a CC that is noninitial but to the left of the currently chosen head—but the currently
chosen head should remain chosen.11 As we shall see, there is exceptional behavior
for base NPs in almost every part of the Collins parser.
10 In our first attempt at replicating Collins’ results, we simply employed the same head-finding rule for
NX nodes as for NP nodes. This choice yields different—but not necessarily inferior—results.
11 In Section 4.1, we defined coordinated phrases in terms of heads, but here we are discussing how the
head-finder itself needs to determine whether a phrase is coordinated. It does this by considering the
potential new choice of head: If the head-finding rules pick out a head that is preceded by a noninitial
CC (Jane), will moving the head to be a child to the left of the CC (John) yield a coordinated phrase? If
so, then the head should be moved—except when the parent is NPB.
</bodyText>
<page confidence="0.974223">
487
</page>
<figure confidence="0.813720666666667">
Computational Linguistics Volume 30, Number 4
VP
✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘
✟✟✟✟✟✟✟✟✟ ❍ ❍
�� � � � � � � � � � � � � � � � �
❍ ❍
❍ ❍ ❍ ❍ ❍
ADVP
RB
undoubtedly
NPB
✏✏ ��
the will
S
✏✏✏ �� �
to continue
+STOP+ VB-HEAD
need
NP
✟✟✟ ❍ ❍ ❍
+STOP+
</figure>
<figureCaption confidence="0.999213">
Figure 7
</figureCaption>
<bodyText confidence="0.96329">
vi feature is true when generating right-hand +STOP+ nonterminal, because the NP the will to
continue contains a verb.
</bodyText>
<sectionHeader confidence="0.979137" genericHeader="method">
5. Training
</sectionHeader>
<bodyText confidence="0.99998425">
The trainer’s job is to decompose annotated training trees into a series of head- and
modifier-generation steps, recording the counts of each of these steps. Referring to
(1), each H, Li, and Ri are generated conditioning on previously generated items,
and each of these events consisting of a generated item and some maximal history
context is counted. Even with all this decomposition, sparse data are still a problem,
and so each probability estimate for some generated item given a maximal context
is smoothed with coarser distributions using less context, whose counts are derived
from these “top-level” head- and modifier-generation counts.
</bodyText>
<subsectionHeader confidence="0.976833">
5.1 Verb Intervening
</subsectionHeader>
<bodyText confidence="0.999152272727273">
As mentioned in Section 3, instead of generating each modifier independently, the
model conditions the generation of modifiers on certain aspects of the history. One
such function of the history is the distance metric. One of the two components of
this distance metric is what we will call the “verb intervening” feature, which is a
predicate vi that is true if a verb has been generated somewhere in the surface string
of the previously generated modifiers on the current side of the head. For example,
in Figure 7, when generating the right-hand +STOP+ nonterminal child of the VP, the
vi predicate is true, because one of the previously generated modifiers on the right
side of the head dominates a verb, continue.12 More formally, this feature is most easily
defined in terms of a recursively defined cv (“contains verb”) predicate, which is true
if and only if a node dominates a verb:
</bodyText>
<figure confidence="0.9314415">
V cv(M) if M is not a preterminal
M child of P
true if P is a verb preterminal
false otherwise
cv(P) = I
(2)
</figure>
<footnote confidence="0.931858333333333">
12 Note that any word in the surface strings dominated by the previously generated modifiers will trigger
the vi predicate. This is possible because in a history-based model (cf. Black et al. 1992), anything
previously generated—that is, anything in the history—can appear in the conditioning context.
</footnote>
<page confidence="0.990464">
488
</page>
<bodyText confidence="0.810348">
Bikel Intricacies of Collins’ Parsing Model
Referring to (2), we define the verb-intervening predicate recursively on the first-order
Markov process generating modifying nonterminals:
</bodyText>
<equation confidence="0.999242666666667">
�cv(Li−1) ∨ vi(Li−2)
false if i ≤ 1
vi(Li) = if i &gt; 1 (3)
</equation>
<bodyText confidence="0.9999355">
and similarly for right modifiers.
What is considered to be a verb? While this is not spelled out, as it happens, a
verb is any word whose part-of-speech tag is one of {VB, VBD, VBG, VBN, VBP, VBZ}. That
is, the cv predicate returns true only for these preterminals and false for all other
preterminals. Crucially, this set omits MD, which is the marker for modal verbs. Another
crucial point about the vi predicate is that it does not include verbs that appear within
base NPs. Put another way, in order to emulate Collins’ model, we need to amend the
definition of cv by stipulating that cv(NPB) = false.
</bodyText>
<subsectionHeader confidence="0.99906">
5.2 Skip Certain Trees
</subsectionHeader>
<bodyText confidence="0.999969272727273">
One oddity of Collins’ trainer that we mention here for the sake of completeness is
that it skips certain training trees. For “odd historical reasons,”13 the trainer skips
all trees with more than 500 tokens, where a token is considered in this context to
be a word, a nonterminal label, or a parenthesis. This oddity entails that even some
relatively short sentences get skipped because they have lots of tree structure. In the
standard Wall Street Journal training corpus, Sections 02–21 of the Penn Treebank,
there are 120 such sentences that are skipped. Unless there is something inherently
wrong with these trees, one would predict that adding them to the training set would
improve a parser’s performance. As it happens, there is actually a minuscule (and
probably statistically insignificant) drop in performance (see Table 5) when these trees
are included.
</bodyText>
<subsectionHeader confidence="0.997848">
5.3 Unknown Words
</subsectionHeader>
<bodyText confidence="0.962498111111111">
5.3.1 The Threshold Problem. Collins mentions in chapter 7 of his thesis that “[a]ll
words occurring less than 5 times in training data, and words in test data which have
never been seen in training, are replaced with the ‘UNKNOWN’ token (page 186).” The
frequency below which words are considered unknown is often called the unknown-
word threshold. Unfortunately, this term can also refer to the frequency above which
words are considered known. As it happens, the unknown-word threshold Collins
uses in his parser for English is six, not five.14 To be absolutely unambiguous, words
that occur fewer than six times, which is to say, words that occur five times or fewer, in
the data are considered “unknown.”
</bodyText>
<subsubsectionHeader confidence="0.882442">
5.3.2 Not Handled in a Uniform Way. The obvious way to incorporate unknown
</subsubsectionHeader>
<bodyText confidence="0.8650084">
words into the parsing model, then, is simply to map all low-frequency words in
the training data to some special +UNKNOWN+ token before counting top-level events
for parameter estimation (where “low-frequency” means “below the unknown-word
threshold”). Collins’ trainer actually does not do this. Instead, it does not directly
modify any of the words in the original training trees and proceeds to break up these
unmodified trees into the top-level events. After these events have been collected
13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.
14 As with many of the discovered discrepancies between the thesis and the implementation, we
determined the different unknown-word threshold through reverse engineering, in this case, through
an analysis of the events output by Collins’ trainer.
</bodyText>
<page confidence="0.986627">
489
</page>
<note confidence="0.216106">
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.999945444444444">
and counted, the trainer selectively maps low-frequency words when deriving counts
for the various context (back-off) levels of the parameters that make use of bilexical
statistics. If this mapping were performed uniformly, then it would be identical to
mapping low-frequency words prior to top-level event counting; this is not the case,
however. We describe the details of this unknown-word mapping in Section 6.9.2.
While there is a negligible yet detrimental effect on overall parsing performance
when one uses an unknown-word threshold of five instead of six, when this change is
combined with the “obvious” method for handling unknown words, there is actually
a minuscule improvement in overall parsing performance (see Table 5).
</bodyText>
<sectionHeader confidence="0.67706" genericHeader="method">
6. Parameter Classes and Their Estimation
</sectionHeader>
<bodyText confidence="0.999863666666667">
All parameters that generate trees in Collins’ model are estimates of conditional prob-
abilities. Even though the following overview of parameter classes presents only the
maximal contexts of the conditional probability estimates, it is important to bear in
mind that the model always makes use of smoothed probability estimates that are
the linear interpolation of several raw maximum-likelihood estimates, using various
amounts of context (we explore smoothing in detail in Section 6.8).
</bodyText>
<subsectionHeader confidence="0.998814">
6.1 Mapped Versions of the Set of Nonterminals
</subsectionHeader>
<bodyText confidence="0.999895428571429">
In Sections 4.5 and 4.9, we saw how the raw Treebank nonterminal set is expanded to
include nonterminals augmented with -A and -g. Although it is not made explicit in
Collins’ thesis, Collins’ model uses two mapping functions to remove these augmenta-
tions when including nonterminals in the history contexts of conditional probabilities.
Presumably this was done to help alleviate sparse-data problems. We denote the “ar-
gument removal” mapping function as alpha and the “gap removal” mapping function
as gamma. For example:
</bodyText>
<listItem confidence="0.999889666666667">
• α(NP-A-g) = NP-g
• γ(NP-A-g) = NP-A
• α(γ(NP-A-g)) = NP
</listItem>
<bodyText confidence="0.908071">
Since gap augmentations are present only in Model 3, the gamma function effectively
is the identity function in the context of Models 1 and 2.
</bodyText>
<subsectionHeader confidence="0.999772">
6.2 The Head Parameter Class
</subsectionHeader>
<bodyText confidence="0.9999936">
The head nonterminal is generated conditioning on its parent nonterminal label, as
well as the headword and head tag which they share, since parents inherit their lexi-
cal head information from their head-children. More specifically, an unlexicalized head
nonterminal label is generated conditioning on the fully lexicalized parent nontermi-
nal. We denote the parameter class as follows:
</bodyText>
<equation confidence="0.729487">
PH(H  |γ(P), wh, th) (4)
</equation>
<subsectionHeader confidence="0.997781">
6.3 The Subcategorization Parameter Class
</subsectionHeader>
<bodyText confidence="0.999669">
When the model generates a head-child nonterminal for some lexicalized parent non-
terminal, it also generates a kind of subcategorization frame (subcat) on either side of
the head-child, with the following maximal context:
</bodyText>
<equation confidence="0.972024">
PsubcatL(subcatL |α(γ(H)),α(γ(P)),wh,th) (5)
</equation>
<page confidence="0.894626">
490
</page>
<note confidence="0.32213">
Bikel Intricacies of Collins’ Parsing Model
</note>
<equation confidence="0.7756244">
S(sat–VBD)
✟✟✟✟ ❍ ❍ ❍ ❍
NP-A(John–NNP) VP(sat–VBD)
NNP(John–NNP) VBD(sat–VBD)
John sat
</equation>
<figureCaption confidence="0.838654">
Figure 8
</figureCaption>
<bodyText confidence="0.925489">
A fully lexicalized tree. The VP node is the head-child of S.
</bodyText>
<equation confidence="0.981471">
PsubcatR(subcatR |α(&apos;y(H)),α(&apos;y(P)),wh,th) (6)
</equation>
<bodyText confidence="0.996582954545454">
Probabilistically, it is as though these subcats are generated with the head-child, via
application of the chain rule, but they are conditionally independent.15 These subcats
may be thought of as lists of requirements on a particular side of a head. For example,
in Figure 8, after the root node of the tree has been generated (see Section 6.10), the
head child VP is generated, conditioning on both the parent label S and the headword
of that parent, sat–VBD. Before any modifiers of the head-child are generated, both a
left- and right-subcat frame are generated. In this case, the left subcat is {NP-A} and
the right subcat is {}, meaning that there are no required elements to be generated on
the right side of the head. Subcats do not specify the order of the required arguments.
They are dynamically updated multisets: When a requirement has been generated, it
is removed from the multiset, and subsequent modifiers are generated conditioning
on the updated multiset.16
The implementation of subcats in Collins’ parser is even more specific: Subcats
are multisets containing various numbers of precisely six types of items: NP-A, S-A,
SBAR-A, VP-A, g, and miscellaneous. The g indicates that a gap must be generated and
is applicable only to Model 3. Miscellaneous items include all nonterminals that were
marked as arguments in the training data that were not any of the other named types.
There are rules for determining whether NPs, Ss, SBARs, and VPs are arguments, and
the miscellaneous arguments occur as the result of the argument-finding rule for PPs,
which states that the first non-PRN, non-part-of-speech tag that occurs after the head
of a PP should be marked as an argument, and therefore nodes that are not one of the
four named types can be marked.
</bodyText>
<subsectionHeader confidence="0.993077">
6.4 The Modifying Nonterminal Parameter Class
</subsectionHeader>
<bodyText confidence="0.999982857142857">
As mentioned above, after a head-child and its left and right subcats are generated,
modifiers are generated from the head outward, as indicated by the modifier nonter-
minal indices in Figure 1. A fully lexicalized nonterminal has three components: the
nonterminal label, the headword, and the headword’s part of speech. Fully lexicalized
modifying nonterminals are generated in two steps to allow for the parameters to be
independently smoothed, which, in turn, is done to avoid sparse-data problems. These
two steps estimate the joint event of all three components using the chain rule. In the
</bodyText>
<footnote confidence="0.996004">
15 Using separate steps to generate subcats on either side of the head allows not only for conditional
independence between the left and right subcats, but also for these parameters to be separately
smoothed from the head-generation parameter.
16 Our parsing engine allows an arbitrary mechanism for storage and discharge of requirements: They
can be multisets, ordered lists, integers (simply to constrain the number of requirements), or any other
mechanism. The mechanism used is determined at runtime.
</footnote>
<page confidence="0.991837">
491
</page>
<figure confidence="0.990993666666667">
Computational Linguistics Volume 30, Number 4
NP
✘✘✘✘✘✘✘✘✘✘✘✘✘
✟✟✟✟✟✟ ❍ ❍
�� � � � � � � � � � � �
❍
❍ ❍ ❍
NP
CC
and
,
,
NPB
✟✟ ❍❍
NNS
NN
JJ
tall
trees
grass
JJ
bushy
NP
NPB
✟✟ ❍ ❍
JJ
short
NP
NPB
✟✟ ❍
❍
NNS
bushes
</figure>
<figureCaption confidence="0.94716">
Figure 9
</figureCaption>
<bodyText confidence="0.963025">
A tree containing both punctuation and conjunction.
first step, a partially lexicalized version of the nonterminal is generated, consisting
of the unlexicalized label plus the part of speech of its headword. These partially lex-
icalized modifying nonterminals are generated conditioning on the parent label, the
head label, the headword, the head tag, the current state of the dynamic subcat, and
a distance metric. Symbolically, the parameter classes are
</bodyText>
<equation confidence="0.9999215">
PL(L(t)i I α(P),γ(H),wh,th,subcatL,∆L) (7)
PR(R(t)i I α(P),γ(H),wh,th,subcatR,∆R) (8)
</equation>
<bodyText confidence="0.999955857142857">
where ∆ denotes the distance metric.17 As discussed above, one of the two components
of this distance metric is the vi predicate. The other is a predicate that simply reports
whether the current modifier is the first modifier being generated, that is, whether i =
1. The second step is to generate the headword itself, where, because of the chain rule,
the conditioning context consists of everything in the histories of expressions (7) and (8)
plus the partially lexicalized modifier. As there are some interesting idiosyncrasies with
these headword-generation parameters, we describe them in more detail in Section 6.9.
</bodyText>
<subsectionHeader confidence="0.998295">
6.5 The Punctuation and Coordinating Conjunction Parameter Classes
</subsectionHeader>
<bodyText confidence="0.989048357142857">
6.5.1 Inconsistent Model. As discussed in Section 4.8, punctuation is raised to the
highest position in the tree. This means that in some sense, punctuation acts very
much like a coordinating conjunction, in that it “conjoins” the two siblings between
which it sits. Observing that it might be helpful for conjunctions to be generated
conditioning on both of their conjuncts, Collins introduced two new parameter classes
in his thesis parser, Ppunc and PCC.18
As per the definition of a coordinated phrase in Section 4.1, conjunction via a
CC node or a punctuation node always occurs posthead (i.e., as a right-sibling of the
head). Put another way, if a conjunction or punctuation mark occurs prehead, it is
17 Throughout this article we use the notation L(w, t)i to refer to the three items that constitute a fully
lexicalized left-modifying nonterminal, which are the unlexicalized label Li, its headword wLi, and its
part of speech tLi, and similarly for right modifiers. We use L(t)i to refer to the two items Li and tLi of a
partially lexicalized nonterminal. Finally, when we do not wish to distinguish between a left and right
modifier, we use M(w, t)i, M(t)i, and Mi.
</bodyText>
<footnote confidence="0.580484666666667">
18 Collins’ thesis does not say what the back-off structure of these new parameter classes is, that is, how
they should be smoothed. We have included this information in the complete smoothing table in the
Appendix.
</footnote>
<page confidence="0.988029">
492
</page>
<note confidence="0.678574">
Bikel Intricacies of Collins’ Parsing Model
</note>
<bodyText confidence="0.999926608695652">
not generated via this mechanism.19 Furthermore, even if there is arbitrary material
between the right conjunct and the head, the parameters effectively assume that the
left conjunct is always the head-child. For example, in Figure 9, the rightmost NP
(bushy bushes) is considered to be conjoined to the leftmost NP (short grass), which is
the head-child, even though there is an intervening NP (tall trees).
The new parameters are incorporated into the model by requiring that all mod-
ifying nonterminals be generated with two boolean flags: coord, indicating that the
nonterminal is conjoined to the head via a CC, and punc, indicating that the nonter-
minal is conjoined to the head via a punctuation mark. When either or both of these
flags is true, the intervening punctuation or conjunction is generated via appropriate
instances of the Ppun,/PCC parameter classes.
For example, the model generates the five children in Figure 9 in the following
order: first, the head-child is generated, which is the leftmost NP (short grass), con-
ditioning on the parent label and the headword and tag. Then, since modifiers are
always generated from the head outward, the right-sibling of the head, which is the
tall trees NP, is generated with both the punc and CC flags false. Then, the rightmost
NP (bushy bushes) is generated with both the punc and CC booleans true, since it is
considered to be conjoined to the head-child and requires the generation of an in-
tervening punctuation mark and conjunction. Finally, the intervening punctuation is
generated conditioning on the parent, the head, and the right conjunct, including the
headwords of the two conjoined phrases, and the intervening CC is similarly generated.
A simplified version of the probability of generating all these children is summarized
as follows:
</bodyText>
<equation confidence="0.9969768">
ˆpH(NP|NP,grass,NN)·
ˆpR(NP(trees,NNS),punc=0,coord=0|NP,NP,grass,NN)·
ˆpR(NP(bushes,NNS),punc=1,coord=1|NP,NP,grass,NN)·
ˆppun,(,(,) |NP,NP,NP, bushes, NNS,grass,NN)·
ˆpCC(CC(and)|NP,NP,NP,bushes,NNS,grass,NN) (9)
</equation>
<bodyText confidence="0.988559533333333">
The idea is that using the chain rule, the generation of two conjuncts and that which
conjoins them is estimated as one large joint event.20
This scheme of using flags to trigger the Ppun, and PCC parameters is problematic,
at least from a theoretical standpoint, as it causes the model to be inconsistent. Fig-
ure 10 shows three different trees that would all receive the same probability from
Collins’ model. The problem is that coordinating conjunctions and punctuation are
not generated as first-class words, but only as triggered from these punc and coord
flags, meaning that the number of such intervening conjunctive items (and the order
in which they are to be generated) is not specified. So for a given sentence/tree pair
containing a conjunction and/or a punctuation mark, there is an infinite number of
similar sentence/tree pairs with arbitrary amounts of “conjunctive” material between
the same two nodes. Because all of these trees have the same, nonzero probability, the
sum ETP(T), where T is a possible tree generated by the model, diverges, meaning
the model is inconsistent (Booth and Thompson 1973). Another consequence of not
generating posthead conjunctions and punctuation as first-class words is that they
</bodyText>
<footnote confidence="0.982671333333333">
19 In fact, if punctuation occurs before the head, it is not generated at all—a deficiency in the parsing
model that appears to be a holdover from the deficient punctuation handling in the model of Collins
(1997).
20 In (9), for clarity we have left out subcat generation and the use of Collins’ distance metric in the
conditioning contexts. We have also glossed over the fact that lexicalized modifying nonterminals are
actually generated in two steps, using two differently smoothed parameters.
</footnote>
<page confidence="0.997514">
493
</page>
<figure confidence="0.971360611111111">
Computational Linguistics Volume 30, Number 4
NP
✘✘✘✘✘✘✘
✟✟✟✟ ❍❍ � � � � � � �
NP ❍ ❍
(a)
NPB
NN
fire
,
,
CC
and
, ADVP...
, RB
ultimately
(b)
NP
</figure>
<equation confidence="0.749318818181818">
✏✏✏✏✏✏✏ � �� �
❅ � � �
� ❅ �
NP , CC ADVP...
NPB , and RB
NN ultimately
(c)
fire
NP
✏✏✏✏✏ � �
�� ❅ ❅ � � �
</equation>
<figure confidence="0.971507111111111">
NP
NPB
NN
CC
and
, ADVP...
, RB
ultimately
fire
</figure>
<figureCaption confidence="0.959076">
Figure 10
</figureCaption>
<bodyText confidence="0.972118625">
The Collins model assigns equal probability to these three trees.
do not count when calculating the head-adjacency component of Collins’ distance
metric.
When emulating Collins’ model, instead of reproducing the Ppun, and PCC param-
eter classes directly in our parsing engine, we chose to use a different mechanism that
does not yield an inconsistent model but still estimates the large joint event that was
the motivation behind these parameters in the first place.
6.5.2 History Mechanism. In our emulation of Collins’ model, we use the history,
rather than the dedicated parameter classes PCC and Ppun,, to estimate the joint event
of generating a conjunction (or punctuation mark) and its two conjuncts. The first
big change that results is that we treat punctuation preterminals and CCs as first-class
objects, meaning that they are generated in the same way as any other modifying
nonterminal.
The second change is a little more involved. First, we redefine the distance met-
ric to consist solely of the vi predicate. Then, we add to the conditioning context
a mapped version of the previously generated modifier according to the following
</bodyText>
<page confidence="0.99521">
494
</page>
<figure confidence="0.244176666666667">
Bikel Intricacies of Collins’ Parsing Model
mapping function:
δ(Mi) = I +START+ if i = 0 (10)
CC if Mi = CC
+PUNC+ if Mi = , or Mi = :
+OTHER+ otherwise
</figure>
<bodyText confidence="0.9368505">
where Mi is some modifier Li or Ri.21 So, the maximal context for our modifying
nonterminal parameter class is now defined as follows:
</bodyText>
<equation confidence="0.907841">
PM (M(t)i  |α(P),y(H),wh,th,subcatside,vi(Mi),δ(Mi−1),side) (11)
</equation>
<bodyText confidence="0.99998025">
where side is a boolean-valued event that indicates whether the modifier is on the left or
right side of the head. By treating CC and punctuation nodes as first-class nonterminals
and by adding the mapped version of the previously generated modifier, we have, in
one fell swoop, incorporated the “no intervening” component of Collins’ distance
metric (the i = 0 case of the delta function) and achieved an estimate of the joint
event of a conjunction and its conjuncts, albeit with different dependencies, that is, a
different application of the chain rule. To put this parameterization change in sharp
relief, consider the abstract tree structure
</bodyText>
<equation confidence="0.97905325">
P
✘✘✘✘✘
✟✟ ❍ ❍ �� � � �
... ... H CC R1
</equation>
<bodyText confidence="0.71950975">
To a first approximation, under the old parameterization, the conjunction of some node
R1 with a head H and a parent P looked like this:
ˆpH(H  |P) · ˆpR(R1, coord=1  |P,H) · ˆpCC(CC  |P,H,R1)
whereas under the new parameterization, it looks like this:
</bodyText>
<equation confidence="0.983212">
ˆpH(H  |P) · ˆpR(CC  |P,H,+START+) · ˆpR(R1  |P, H, CC)
</equation>
<bodyText confidence="0.9994516">
Either way, the probability of the joint conditional event {H, CC, R1  |P} is being esti-
mated, but with the new method, there is no need to add two new specialized pa-
rameter classes, and the new method does not introduce inconsistency into the model.
Using less simplification, the probability of generating the five children of Figure 9 is
now
</bodyText>
<equation confidence="0.9992084">
ˆpH(NP  |NP,grass,NN)·
ˆpM(NP(trees,NNS)  |NP, NP, grass, NN, {}, false, +START+,right)·
ˆpM(,(,, ,) |NP, NP, grass, NN, {},false, +OTHER+, right)·
ˆpM(CC(and,CC)  |NP, NP, grass, NN, {},false, +PUNC+, right)·
ˆpM(NP(bushes,NNS) |NP,NP,grass,NN,{},false,CC,right) (12)
</equation>
<bodyText confidence="0.995075428571429">
21 Originally, we had an additional mechanism that attempted to generate punctuation and conjunctions
with conditional independence. One of our reviewers astutely pointed out that the mechanism led to a
deficient model (the very thing we have been trying to avoid), and so we have subsequently removed
it from our model. The removal leads to a 0.05% absolute reduction in F-measure (which in this case is
also a 0.05% relative increase in error) on sentences of length ≤ 40 words in Section 00 of the Penn
Treebank. As this difference is not at all statistically significant (according to a randomized stratified
shuffling test [Cohen 1995]), all evaluations reported in this article are with the original model.
</bodyText>
<page confidence="0.99557">
495
</page>
<note confidence="0.393217">
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.9795055">
As shown in Section 8.1, this new parameterization yields virtually identical perfor-
mance to that of the Collins model.22
</bodyText>
<subsectionHeader confidence="0.973245">
6.6 The Base NP Model: A Model unto Itself
</subsectionHeader>
<bodyText confidence="0.999972444444445">
As we have already seen, there are several ways in which base NPs are exceptional
in Collins’ parsing model. This is partly because the flat structure of base NPs in the
Penn Treebank suggested the use of a completely different model by which to generate
them. Essentially, the model for generating children of NPB nodes is a “bigrams of
nonterminals” model. That is, it looks a great deal like a bigram language model,
except that the items being generated are not words, but lexicalized nonterminals.
Heads of NPB nodes are generated using the normal head-generation parameter, but
modifiers are always generated conditioning not on the head, but on the previously
generated modifier. That is, we modify expressions (7) and (8) to be
</bodyText>
<equation confidence="0.999918">
PL,NPB(L(t)i P, L(w, t)i−1) (13)
PR,NPB(R(t)i P,R(w,t)i−1) (14)
</equation>
<bodyText confidence="0.9941158">
Though it is not entirely spelled out in his thesis, Collins considers the previously
generated modifier to be the head-child, for all intents and purposes. Thus, the subcat
and distance metrics are always irrelevant, since it is as though the current modifier is
right next to the head.23 Another consequence of this is that NPBs are never considered
to be coordinated phrases (as mentioned in Section 4.12), and thus CCs dominated
by NPB are never generated using a PCC parameter; instead, they are generated us-
ing a normal modifying-nonterminal parameter. Punctuation dominated by NPB, on
the other hand, is still, as always, generated via Ppunc parameters, but crucially, the
modifier is always conjoined (via the punctuation mark) to the “pseudohead” that
is the previously generated modifier. Consequently, when some right modifier Ri is
generated, the previously generated modifier on the right side of the head, Ri−1, is
never a punctuation preterminal, but always the previous “real” (i.e., nonpunctuation)
preterminal.24
Base NPs are also exceptional with respect to determining chart item equality, the
comma-pruning rule, and general beam pruning (see Section 7.2 for details).
</bodyText>
<subsectionHeader confidence="0.993657">
6.7 Parameter Classes for Priors on Lexicalized Nonterminals
</subsectionHeader>
<bodyText confidence="0.948975384615385">
Two parameter classes that make their appearance only in Appendix E of Collins’
thesis are those that compute priors on lexicalized nonterminals. These priors are
used as a crude proxy for the outside probability of a chart item (see Baker [1979] and
Lari and Young [1990] for full descriptions of the Inside–Outside algorithm). Previous
work (Goodman 1997) has shown that the inside probability alone is an insufficient
scoring metric when comparing chart items covering the same span during decoding
and that some estimate of the outside probability of a chart item should be factored
into the score. A prior on the root (lexicalized) nonterminal label of the derivation
forest represented by a particular chart item is used for this purpose in Collins’ parser.
22 As described in Bikel (2002), our parsing engine allows easy experimentation with a wide variety of
different generative models, including the ability to construct history contexts from arbitrary numbers
of previously generated modifiers. The mapping function delta and the transition function tau
presented in this section are just two examples of this capability.
</bodyText>
<footnote confidence="0.9428518">
23 This is the main reason that the cv (“contains verb”) predicate is always false for NPBs, as that
predicate applies only to material that intervenes between the current modifier and the head.
24 Interestingly, unlike in the regular model, punctuation that occurs to the left of the head is generated
when it occurs within an NPB. Thus, this particular—albeit small—deficiency of Collins’ punctuation
handling does not apply to the base NP model.
</footnote>
<page confidence="0.997244">
496
</page>
<note confidence="0.657368">
Bikel Intricacies of Collins’ Parsing Model
</note>
<bodyText confidence="0.9941245">
The prior of a lexicalized nonterminal M(w, t) is broken down into two separate
estimates using parameters from two new classes, Ppriorw and PpriorNT:
</bodyText>
<equation confidence="0.568207">
Pprior(M(w, t)) = Ppriorw(w, t) · PpriorNT(M  |w, t)
</equation>
<bodyText confidence="0.9990885">
where ˆp(M  |w, t) is smoothed with ˆp(M  |t) and estimates using the parameters of the
Ppriorw class are unsmoothed.
</bodyText>
<subsectionHeader confidence="0.999381">
6.8 Smoothing Weights
</subsectionHeader>
<bodyText confidence="0.999973941176471">
Many of the parameter classes in Collins’ model—and indeed, in most statistical pars-
ing models—define conditional probabilities with very large conditioning contexts. In
this case, the conditioning contexts represent some subset of the history of the gen-
erative process. Even if there were orders of magnitude more training data available,
the large size of these contexts would cause horrendous sparse-data problems. The
solution is to smooth these distributions that are made rough primarily by the abun-
dance of zeros. Collins uses the technique of deleted interpolation, which smoothes
the distributions based on full contexts with those from coarser models that use less
of the context, by successively deleting elements from the context at each back-off
level. As a simple example, the head parameter class smoothes PH0(H  |P, wh, th) with
PH1(H  |P, th) and PH2(H  |P). For some conditional probability p(A  |B), let us call the
reduced context at the ith back-off level Oi(B), where typically O0(B) = B. Each esti-
mate in the back-off chain is computed via maximum-likelihood (ML) estimation, and
the overall smoothed estimate with n back-off levels is computed using n −1 smooth-
ing weights, denoted A0, ... , An−2. These weights are used in a recursive fashion: The
smoothed version ˜ei = ˜pi(A  |Oi(B)) of an unsmoothed ML estimate ei = ˆpi(A  |Oi(B)) at
back-off level i is computed via the formula
</bodyText>
<equation confidence="0.976485">
˜ei = Aiei + (1 − Ai)˜ei+1, 0 &lt; i &lt; n − 1, ˜en−1 = en−1 (15)
</equation>
<bodyText confidence="0.994868">
So, for example, with three levels of back-off, the overall smoothed estimate would be
defined as
</bodyText>
<equation confidence="0.959202571428571">
˜e0 = A0e0 + (1 − A0) [A1e1 + (1 − A1)e2] (16)
It is easy to prove by structural induction that if
0 &lt; Ai &lt; 1 and E ˆpi(A|Oi(B)) = 1, 0 &lt; i &lt; n − 1
A
then
E ˜p0(A  |O0(B)) = 1 (17)
A
</equation>
<bodyText confidence="0.9243925">
Each smoothing weight can be conceptualized as the confidence in the estimate
with which it is being multiplied. These confidence values can be derived in a number
of sensible ways; the technique used by Collins was adapted from that used in Bikel
et al. (1997), which makes use of a quantity called the diversity of the history context
(Witten and Bell 1991), which is equal to the number of unique futures observed in
training for that history context.
6.8.1 Deficient Model. As previously mentioned, n back-off levels require n−1 smooth-
ing weights. Collins’ parser effectively uses n weights, because the estimator always
</bodyText>
<page confidence="0.989363">
497
</page>
<note confidence="0.39275">
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.991853">
adds an extra, constant-valued estimate to the back-off chain. Collins’ parser hard-
codes this extra value to be a vanishingly small (but nonzero) “probability” of 10−19,
resulting in smoothed estimates of the form
</bodyText>
<equation confidence="0.998362">
˜e0 = A0e0 + (1 − A0) [A1e1 + (1 − A1) [A2e2 + (1 − A2) · 10−1911 (18)
</equation>
<bodyText confidence="0.9987104">
when there are three levels of back-off. The addition of this constant-valued en =
10−19 causes all estimates in the parser to be deficient, as it ends up throwing away
probability mass. More formally, the proof leading to equation (17) no longer holds:
The “distribution” sums to less than one (there is no history context in the model for
which there are 1019 possible outcomes).25
</bodyText>
<subsubsectionHeader confidence="0.645679">
6.8.2 Smoothing Factors and Smoothing Terms. The formula given in Collins’ thesis
</subsubsectionHeader>
<bodyText confidence="0.990182">
for computing smoothing weights is
</bodyText>
<equation confidence="0.990977">
Ai =
ci + 5ui
</equation>
<bodyText confidence="0.999894166666667">
where ci is the count of the history context Oi(B) and ui is the diversity of that context.26
The multiplicative constant five is used to give less weight to the back-off levels
with more context and was optimized by looking at overall parsing performance on
the development test set, Section 00 of the Penn Treebank. We call this constant the
smoothing factor and denote it as ff. As it happens, the actual formula for computing
smoothing weights in Collins’ implementation is
</bodyText>
<equation confidence="0.993250428571429">
ci
ciif ci &gt; 0
Ai = { +f +f
i
c; t fu
(19)
0 otherwise
</equation>
<bodyText confidence="0.9973165">
where ft is an unmentioned smoothing term. For every parameter class except the
subcat parameter class and Ppriorw, ft = 0 and ff = 5.0. For the subcat parameter class,
ft = 5.0 and ff = 0. For Ppriorw, ft = 1.0 and ff = 0.0. This curiously means that diversity
is not used at all when smoothing subcat-generation probabilities.27
The second case in (19) handles the situation in which the history context was never
observed in training, that is, where ci = ui = 0, which would yield an undefined value
25 Collins used this technique to ensure that even futures that were never seen with an observed history
context would still have some probability mass, albeit a vanishingly small one (Collins, personal
communication, January 2003). Another commonly used technique would be to back off to the uniform
distribution, which has the desirable property of not producing deficient estimates. As with all of the
treebank- or model-specific aspects of the Collins parser, our engine uses equation (16) or (18)
depending on the value of a particular run-time setting.
26 The smoothing weights can be viewed as confidence values for the probability estimates with which
they are multiplied. The Witten-Bell technique crucially makes use of the quantity ni = ui , the average
number of transitions from the history context Oi(B) to a possible future. With a little algebraic
manipulation, we have
</bodyText>
<equation confidence="0.7637265">
ni
λi =ni + 5
</equation>
<bodyText confidence="0.984168166666667">
a quantity that is at its maximum when ni = ci and at its minimum when ni = 1, that is, when every
future observed in training was unique. This latter case represents when the model is most
“uncertain,” in that the transition distribution from Oi(B) is uniform and poorly trained (one
observation per possible transition). Because these smoothing weights measure, in some sense, the
closeness of the observed distribution to uniform, they can be viewed as proxies for the entropy of the
distribution p(·  |Oi(B)).
</bodyText>
<footnote confidence="0.940278333333333">
27 As mentioned above, the Ppriorw parameters are unsmoothed. However, as a result of the deficient
estimation method, they still have an associated lambda value, the computation of which, just like the
subcat-generation probability estimates, does not make use of diversity.
</footnote>
<page confidence="0.995834">
498
</page>
<note confidence="0.845269">
Bikel Intricacies of Collins’ Parsing Model
</note>
<tableCaption confidence="0.99752">
Table 1
</tableCaption>
<bodyText confidence="0.660888">
Back-off levels for PLw/PRw, the modifier headword generation parameter classes. wLiand tLi
are, respectively, the headword and its part of speech of the nonterminal Li. This table is
basically a reproduction of the last column of Table 7.1 in Collins’ thesis.
</bodyText>
<equation confidence="0.4254844">
Back-off PLw(wLi ...)
level PRw(wRi ...)
0 ry(Li), tLi, coord, punc, α(P), ry(H), wh, th, OL, subcat
1 ry(Li), tLi, coord, punc, α(P), ry(H), th, OL, subcat
2 tLi
</equation>
<tableCaption confidence="0.611619">
Table 2
</tableCaption>
<bodyText confidence="0.438864">
Our new parameter class for the generation of headwords of modifying nonterminals.
</bodyText>
<equation confidence="0.6836595">
Back-off level PMw(wMi I ...)
0 ry(Mi), tMi, α(P), ry(H), wh, th, subcatside, vi(Mi), 6(Mi−1), side
1 ry(Mi), tMi, α(P), ry(H), th, subcatside, vi(Mi), 6(Mi−1), side
2 tMi
</equation>
<bodyText confidence="0.9958206">
when ft = 0. In such situations, making λi = 0 throws all remaining probability mass
to the smoothed back-off estimate, ˜ei+1. This is a crucial part of the way smoothing
is done: If a particular history context φi(B) has never been observed in training, the
smoothed estimate using less context, φi+1(B), is simply substituted as the “best guess”
for the estimate using more context; that is, ˜ei = ˜ei+1.28
</bodyText>
<subsectionHeader confidence="0.998625">
6.9 Modifier Head-Word Generation
</subsectionHeader>
<bodyText confidence="0.999740875">
As mentioned in Section 6.4, fully lexicalized modifying nonterminals are generated in
two steps. First, the label and part-of-speech tag are generated with an instance of PL
or PR. Next, the headword is generated via an instance of one of two parameter classes,
PLw or PRw. The back-off contexts for the smoothed estimates of these parameters are
specified in Table 1. Notice how the last level of back-off is markedly different from
the previous two levels in that it removes nearly all the elements of the history: In
the face of sparse data, the probability of generating the headword of a modifying
nonterminal is conditioned only on its part of speech.
</bodyText>
<subsubsectionHeader confidence="0.830912">
6.9.1 Smoothing and the Last Level of Back-Off. Table 1 is misleading, however. In
</subsubsectionHeader>
<bodyText confidence="0.99957">
order to capture the most data for the crucial last level of back-off, Collins uses words
that occur on either side of the headword, resulting in a general estimate ˆp(w I t), as
opposed to ˆpLw(wLi I tLi). Accordingly, in our emulation of Collins’ model, we replace
the left- and right-word parameter classes with a single modifier headword generation
parameter class that, as with (11), includes a boolean side component that is deleted
from the last level of back-off (see Table 2).
Even with this change, there is still a problem. Every headword in a lexicalized
parse tree is the modifier of some other headword—except the word that is the head of
the entire sentence (i.e., the headword of the root nonterminal). In order to properly
duplicate Collins’ model, an implementation must take care that the P(w I t) model
includes counts for these important headwords.29
</bodyText>
<footnote confidence="0.556638333333333">
28 This fact is crucial in understanding how little the Collins parsing model relies on bilexical statistics, as
described in Section 8.2 and the supporting experiment shown in Table 6.
29 In our implementation, we add such counts by having our trainer generate a “fake” modifier event in
</footnote>
<page confidence="0.997612">
499
</page>
<figure confidence="0.997811777777778">
Computational Linguistics Volume 30, Number 4
S(sat–VBD)
❍
✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍
NP-A(Fido–NNP)
NPB(Fido–NNP)
✟✟ ❍
❍
JJ
Faithful
ADVP(faithfully–RB)
RB
faithfully
VP(sat–VBD)
VBD
sat
NNP
Fido
</figure>
<figureCaption confidence="0.990992">
Figure 11
</figureCaption>
<bodyText confidence="0.981380333333333">
The low-frequency word Fido is mapped to +UNKNOWN+, but only when it is generated, not
when it is conditioned upon. All the nonterminals have been lexicalized (except for
preterminals) to show where the heads are.
6.9.2 Unknown-Word Mapping. As mentioned above, instead of mapping every low-
frequency word in the training data to some special +UNKNOWN+ token, Collins’ trainer
instead leaves the training data untouched and selectively maps words that appear in
the back-off levels of the parameters from the PL. and PR. parameter classes. Rather
curiously, the trainer maps only words that appear in the futures of these parameters,
but never in the histories. Put another way, low-frequency words are generated as
+UNKNOWN+ but are left unchanged when they are conditioned upon. For example, in
Figure 11, where we assume Fido is a low-frequency word, the trainer would derive
counts for the smoothed parameter
</bodyText>
<equation confidence="0.6424555">
pL. (+UNKNOWN + I NP-A, NNP, coord = 0, punc = 0, S, VP, sat, VBD, ...) (20)
However, when collecting events that condition on Fido, such as the parameters
pL (JJ(JJ) I NPB, NNP, Fido�
pL. (Faithful I JJ, JJ, NPB, NNP, Fido
</equation>
<bodyText confidence="0.9897645">
the word would not be mapped.
This strange mapping scheme has some interesting consequences. First, imagine
what happens to words that are truly unknown, that never occurred in the training
data. Such words are mapped to the +UNKNOWN+ token outright before parsing. When-
ever the parser estimates a probability with such a truly unknown word in the history,
it will necessarily throw all probability mass to the backed-off estimate (˜e1 in our ear-
lier notation), since +UNKNOWN+ effectively never occurred in a history context during
training.
The second consequence is that the mapping scheme yields a “superficient”30
model, if all other parts of the model are probabilistically sound (which is actually
</bodyText>
<footnote confidence="0.979203">
which the observed lexicalized root nonterminal is considered a modifier of +TOP+, the hidden
nonterminal that is the parent of the observed root of every tree (see Section 6.10 for details on the
+TOP+ nonterminal).
30 The term deficient is used to denote a model in which one or more estimated distributions sums to less
than 1. We use the term superficient to denote a model in which one or more estimated distributions
sums to greater than 1.
</footnote>
<page confidence="0.984466">
500
</page>
<note confidence="0.879763">
Bikel Intricacies of Collins’ Parsing Model
</note>
<tableCaption confidence="0.982941">
Table 3
</tableCaption>
<bodyText confidence="0.6101175">
Back-off structure for PTOPNT and PTOPw, which estimate the probability of generating H(w, t) as
the root nonterminal of a parse tree. PTOPNT is unsmoothed. n/a: not applicable.
</bodyText>
<figure confidence="0.724761333333333">
Back-off level PTOPNT(H(t) ...) PTOPw(w I ...)
0 +TOP+ t,H,+TOP+
1 n/a t
</figure>
<bodyText confidence="0.998121153846154">
not the case here). With a parsing model such as Collins’ that uses bilexical dependen-
cies, generating words in the course of parsing is done very much as it is in a bigram
language model: Every word is generated conditioning on some previously generated
word, as well as some hidden material. The only difference is that the word being
conditioned upon is often not the immediately preceding word in the sentence. How-
ever, one could plausibly construct a consistent bigram language model that generates
words with the same dependencies as those in a statistical parser that uses bilexical
dependencies derived from head-lexicalization.
Collins (personal communication, January 2003) notes that his parser’s unknown-
word-mapping scheme could be made consistent if one were to add a parameter class
that estimated ˆp(w  |+UNKNOWN+), where w E VL U {+UNKNOWN+}. The values of these
estimates for a given sentence would be constant across all parses, meaning that the
“superficiency” of the model would be irrelevant when determining arg max P(T  |S).
</bodyText>
<equation confidence="0.557493">
T
</equation>
<subsectionHeader confidence="0.717574">
6.10 The TOP Parameter Classes
</subsectionHeader>
<bodyText confidence="0.9999531">
It is assumed that all trees that can be generated by the model have an implicit non-
terminal +TOP+ that is the parent of the observed root. The observed lexicalized root
nonterminal is generated conditioning on +TOP+ (which has a prior probability of 1.0)
using a parameter from the class PTOP. This special parameter class is mentioned in a
footnote in chapter 7 of Collins’ thesis. There are actually two parameter classes used
to generated observed roots, one for generating the partially lexicalized root nonter-
minal, which we call PTOPNT, and the other for generating the headword of the entire
sentence, which we call PTOPw. Table 3 gives the unpublished back-off structure of
these two additional parameter classes.
Note that PTOPw backs off to simply estimating ˆp(w  |t). Technically, it should be
estimating ˆpNT(w  |t), which is to say the probability of a word’s occurring with a tag
in the space of lexicalized nonterminals. This is different from the last level of back-off
in the modifier headword parameter classes, which is effectively estimating ˆp(w  |t) in
the space of lexicalized preterminals. The difference is that in the same sentence, the
same headword can occur with the same tag in multiple nodes, such as sat in Figure
8, which occurs with the tag VBD three times (instead of just once) in the tree shown
there. Despite this difference, Collins’ parser uses counts from the (shared) last level
of back-off of the PLw and PRw parameters when delivering e1 estimates for the PTOPw
parameters. Our parsing engine emulates this “count sharing” for PTOPw by default,
by sharing counts from our PMw parameter class.
</bodyText>
<sectionHeader confidence="0.883048" genericHeader="method">
7. Decoding
</sectionHeader>
<bodyText confidence="0.998252">
Parsing, or decoding, is performed via a probabilistic version of the CKY chart-
parsing algorithm. As with normal CKY, even though the model is defined in a top-
down, generative manner, decoding proceeds bottom-up. Collins’ thesis gives a pseu-
</bodyText>
<page confidence="0.973756">
501
</page>
<note confidence="0.867461333333333">
Computational Linguistics Volume 30, Number 4
docode version of his algorithm in an appendix. This section contains a few practical
details.
</note>
<subsectionHeader confidence="0.965766">
7.1 Chart Item Equality
</subsectionHeader>
<bodyText confidence="0.99999615">
Since the goal of the decoding process is to determine the maximally likely theory,
if during decoding a proposed chart item is equal (or, technically, equivalent) to an
item that is already in the chart, the one with the greater score survives. Chart item
equality is closely tied to the generative parameters used to construct theories: We want
to treat two chart items as unequal if they represent derivation forests that would be
considered unequal according to the output elements and conditioning contexts of the
parameters used to generate them, subject to the independence assumptions of the
model. For example, for two chart items to be considered equal, they must have the
same label (the label of the root of their respective derivation forests’ subtrees), the
same headword and tag, and the same left and right subcat. They must also have the
same head label (that is, label of the head-child).
If a chart item’s root label is an NP node, its head label is most often an NPB node,
given the “extra” NP levels that are added during preprocessing to ensure that NPB
nodes are always dominated by NP nodes. In such cases, the chart item will contain a
back pointer to the chart item that represents the base NP. Curiously, however, Collins’
implementation considers the head label of the NP chart item not to be NPB, but rather
the head label of the NPB chart item. In other words, to get the head label of an NP chart
item, one must “peek through” the NPB and get at the NPB’s head label. Presumably,
this was done as a consideration for the NPB nodes’ being “extra” nodes, in some sense.
It appears to have little effect on overall parsing accuracy, however.
</bodyText>
<subsectionHeader confidence="0.999343">
7.2 Pruning
</subsectionHeader>
<bodyText confidence="0.99993925">
Ideally, every parse theory could be kept in the chart, and when the root symbol has
been generated for all theories, the top-ranked one would “win.” In order to speed
things up, Collins employs three different types of pruning. The first form of pruning
is to use a beam: The chart memoizes the highest-scoring theory in each span, and if a
proposed chart item for that span is not within a certain factor of the top-scoring item,
it is not added to the chart. Collins reports in his thesis that he uses a beam width of
105. As it happens, the beam width for his thesis experiments was 104. Interestingly,
there is a negligible difference in overall parsing accuracy when this wider beam is
used (see Table 5). An interesting modification to the standard beam in Collins’ parser
is that for chart items representing NP or NP-A derivations with more than one child,
the beam is expanded to be 104 · e3. We suspect that Collins made this modification
after he added the base NP model, to handle the greater perplexity associated with
NPs.
The second form of pruning employed is a comma constraint. Collins observed
that in the Penn Treebank data, 96% of the time, when a constituent contained a
comma, the word immediately following the end of the constituent’s span was either
a comma or the end of the sentence. So for speed reasons, the decoder rejects all
theories that would generate constituents that violate this comma constraint.31 There
is a subtlety to Collins’ implementation of this form of pruning, however. Commas
are quite common within parenthetical phrases. Accordingly, if a comma in an input
</bodyText>
<footnote confidence="0.8905645">
31 If one generates commas as first-class words, as we have done, one must take great care in applying
this comma constraint, for otherwise, chart items that represent partially completed constituents (i.e.,
constituents for which not all modifiers have been generated) may be incorrectly rejected. This is
especially important for NPB constituents.
</footnote>
<page confidence="0.984459">
502
</page>
<note confidence="0.887456">
Bikel Intricacies of Collins’ Parsing Model
</note>
<tableCaption confidence="0.99384">
Table 4
</tableCaption>
<bodyText confidence="0.894836714285714">
Overall parsing results using only details found in Collins (1997, 1999). The first two lines
show the results of Collins’ parser and those of our parser in its “complete” emulation mode
(i.e., including unpublished details). All reported scores are for sentences of length &lt; 40
words. LR (labeled recall) and LP (labeled precision) are the primary scoring metrics. CBs is
the number of crossing brackets. 0 CBs and &lt; 2 CBs are the percentages of sentences with 0
and &lt; 2 crossing brackets, respectively. F (the F-measure) is the evenly weighted harmonic
mean of precision and recall, or 1 LP·LR
</bodyText>
<table confidence="0.930704166666667">
2 (LP+LR)
Performance on Section 00
Model LR LP CBs 0 CBs &lt; 2 CBs F
Collins’ Model 2 89.75 90.19 0.77 69.10 88.31 89.97
Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01
Clean-room Model 2 88.85 88.97 0.92 65.55 87.06 88.91
</table>
<bodyText confidence="0.997895866666667">
sentence occurs after an opening parenthesis and before a closing parenthesis or the
end of the sentence, it is not considered a comma for the purposes of the comma
constraint. Another subtlety is that the comma constraint should effectively not be
employed when pursuing theories of an NPB subtree. As it turns out, using the comma
constraint also affects accuracy, as shown in Section 8.1.
The final form of pruning employed is rather subtle: Within each cell of the chart
that contains items covering some span of the sentence, Collins’ parser uses buckets
of items that share the same root nonterminal label for their respective derivations.
Only 100 of the top-scoring items covering the same span with the same nonterminal
label are kept in a particular bucket, meaning that if a new item is proposed and there
are already 100 items covering the same span with the same label in the chart, then it
will be compared to the lowest-scoring item in the bucket. If it has a higher score, it
will be added to the bucket and the lowest-scoring item will be removed; otherwise,
it will not be added. Apparently, this type of pruning has little effect, and so we have
not duplicated it in our engine.32
</bodyText>
<subsectionHeader confidence="0.999755">
7.3 Unknown Words and Parts of Speech
</subsectionHeader>
<bodyText confidence="0.999952166666667">
When the parser encounters an unknown word, the first-best tag delivered by Ratna-
parkhi’s (1996) tagger is used. As it happens, the tag dictionary built up when training
contains entries for every word observed, even low-frequency words. This means that
during decoding, the output of the tagger is used only for those words that are truly
unknown, that is, that were never observed in training. For all other words, the chart
is seeded with a separate item for each tag observed with that word in training.
</bodyText>
<sectionHeader confidence="0.959523" genericHeader="evaluation">
8. Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999947">
8.1 Effects of Unpublished Details
</subsectionHeader>
<bodyText confidence="0.999723333333333">
In this section we present the results of effectively doing a “clean-room” implemen-
tation of Collins’ parsing model, that is, using only information available in (Collins
1997, 1999), as shown in Table 4.
The clean-room model has a 10.6% increase in F-measure error compared to
Collins’ parser and an 11.0% increase in F-measure error compared to our engine
in its complete emulation of Collins’ Model 2. This is comparable to the increase in
</bodyText>
<footnote confidence="0.6759515">
32 Although we have implemented a version of this type of pruning that limits the number of items that
can be collected in any one cell, that is, the maximum number of items that cover a particular span.
</footnote>
<page confidence="0.988831">
503
</page>
<table confidence="0.479488">
Computational Linguistics Volume 30, Number 4
</table>
<tableCaption confidence="0.995969">
Table 5
</tableCaption>
<table confidence="0.957750777777778">
Effects of independently removing or changing individual details on overall parsing
performance. All reported scores are for sentences of length &lt; 40 words. †With beam width =
105, processing time was 3.36 times longer than with standard beam (104). $No count sharing
was performed for PTOPw (see Section 6.10), and p(w  |t) estimates were side-specific (see
Section 6.9.1). See Table 4 for definitions of column headings.
Model description LR Performance on Section 00 F
LP CBs 0 CBs &lt; 2 CBs
Collins’ Model 2 89.75 90.19 0.77 69.10 88.31 89.97
Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01
Unknown word threshold = 5 and unknown 89.94 90.22 0.77 68.99 89.27 90.08
words handled in uniform way (see Sec-
tion 5.3)
No training trees skipped (see Section 5.2) 89.85 90.12 0.78 68.71 89.16 89.98
Beam width = 105† 89.90 90.14 0.78 68.93 89.16 90.02
Nondeficient estimation (see Section 6.8.1) 89.75 90.00 0.80 68.82 88.88 89.87
No comma constraint (see Section 7.2) 89.52 89.80 0.84 68.09 88.20 89.66
No universal p(w  |t) model$ 89.40 89.17 0.88 66.14 87.92 89.28
Clean-room Model 2 88.85 88.97 0.92 65.55 87.06 88.91
</table>
<bodyText confidence="0.99950375">
error seen when removing such published features as the verb-intervening component
of the distance metric, which results in an F-measure error increase of 9.86%, or the
subcat feature, which results in a 7.62% increase in F-measure error.33 Therefore, while
the collection of unpublished details presented in Sections 4–7 is disparate, in toto
those details are every bit as important to overall parsing performance as certain of
the published features.
This does not mean that all the details are equally important. Table 5 shows the
effect on overall parsing performance of independently removing or changing certain
of the more than 30 unpublished details.34 Often, the detrimental effect of a particular
change is quite insignificant, even by the standards of the performance-obsessed world
of statistical parsing, and occasionally, the effect of a change is not even detrimental at
all. That is why we do not claim the importance of any single unpublished detail,
but rather that of their totality, given that several of the unpublished details are,
most likely, interacting. However, we note that certain individual details, such as the
universal p(w It) model, do appear to have a much more marked effect on overall
parsing accuracy than others.
</bodyText>
<subsectionHeader confidence="0.997483">
8.2 Bilexical Dependencies
</subsectionHeader>
<bodyText confidence="0.988171666666667">
The previous section accounts for the noticeable effects of all the unpublished details
of Collins’ model. But what of the details that were published? In chapter 8 of his
thesis, Collins gives an account on the motivation of various features of his model,
including the distance metric, the model’s use of subcats (and their interaction with the
distance metric), and structural versus semantic preferences. In the discussion of this
last issue, Collins points to the fact that structural preferences—which, in his model, are
33 These F-measures and the differences between them were calculated from experiments presented in
Collins (1999, page 201); these experiments, unlike those on which our reported numbers are based,
were on all sentences, not just those of length ≤ 40 words. As Collins notes, removing both the distance
metric and subcat features results in a gigantic drop in performance, since without both of these
features, the model has no way to encode the fact that flatter structures should be avoided in several
crucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children.
34 As a reviewer pointed out, the use of the comma constraint is a “published” detail. However, the
specifics of how certain commas do not apply to the constraint is an “unpublished detail,” as
mentioned in Section 7.2.
</bodyText>
<page confidence="0.995363">
504
</page>
<note confidence="0.878479">
Bikel Intricacies of Collins’ Parsing Model
</note>
<tableCaption confidence="0.983517">
Table 6
</tableCaption>
<bodyText confidence="0.748522">
Number of times our parsing engine was able to deliver a probability for the various levels of
back-off of the modifier-word generation model, PMw, when testing on Section 00, having
trained on Sections 02–21. In other words, this table reports how often a context in the
back-off chain of PMw that was needed during decoding was observed in training.
</bodyText>
<table confidence="0.9843248">
Back-off level Number of accesses Percentage
0 3,257,309 1.5
1 24,294,084 11.0
2 191,527,387 87.4
Total 219,078,780 100.0
</table>
<bodyText confidence="0.999806911764706">
modeled primarily by the PL and PR parameters—often provide the right information
for disambiguating competing analyses, but that these structural preferences may be
“overridden” by semantic preferences. Bilexical statistics (Eisner 1996), as represented
by the maximal context of the PLw and PRw parameters, serve as a proxy for such
semantic preferences, where the actual modifier word (as opposed to, say, merely its
part of speech) indicates the particular semantics of its head. Indeed, such bilexical
statistics were widely assumed for some time to be a source of great discriminative
power for several different parsing models, including that of Collins.
However, Gildea (2001) reimplemented Collins’ Model 1 (essentially Model 2 but
without subcats) and altered the PLw and PRw parameters so that they no longer had
the top level of context that included the headword (he removed back-off level 0,
as depicted in Table 1). In other words, Gildea removed all bilexical statistics from
the overall model. Surprisingly, this resulted in only a 0.45% absolute reduction in
F-measure (3.3% relative increase in error). Unfortunately, this result was not entirely
conclusive, in that Gildea was able to reimplement Collins’ baseline model only par-
tially, and the performance of his partial reimplementation was not quite as good as
that of Collins’ parser.35
Training on Sections 02–21, we have duplicated Gildea’s bigram-removal exper-
iment, except that our chosen test set is Section 00 instead of Section 23 and our
chosen model is the more widely used Model 2. Using the mode that most closely
emulates Collins’ Model 2, with bigrams, our engine obtains a recall of 89.89% and a
precision of 90.14% on sentences of length ≤ 40 words (see Table 8, Model Mtw,tw).
Without bigrams, performance drops only to 89.49% on recall, 89.95% on precision—
an exceedingly small drop in performance (see Table 8, Model Mtw,t). In an additional
experiment, we have examined the number of times that the parser is able, while de-
coding Section 00, to deliver a requested probability for the modifier-word generation
model using the increasingly less-specific contexts of the three back-off levels. The
results are presented in Table 6. Back-off level 0 indicates the use of the full history
context, which contains the head-child’s headword. Note that probabilities making use
of this full context, that is, making use of bilexical dependencies, are available only
1.49% of the time. Combined with the results from the previous experiment, this sug-
gests rather convincingly that such statistics are far less significant than once thought
to the overall discriminative power of Collins’ models, confirming Gildea’s result for
Model 2.36
</bodyText>
<footnote confidence="0.83272575">
35 The reimplementation was necessarily only partial, as Gildea did not have access to all the
unpublished details of Collins’ models that are presented in this article.
36 On a separate note, it may come as a surprise that the decoder needs to access more than 219 million
probabilities during the course of parsing the 1,917 sentences of Section 00. Among other things, this
</footnote>
<page confidence="0.98235">
505
</page>
<table confidence="0.459473">
Computational Linguistics Volume 30, Number 4
</table>
<tableCaption confidence="0.992996">
Table 7
</tableCaption>
<table confidence="0.9498">
Results on Section 00 with simplified head rules. The baseline model is our engine in its
closest possible emulation of Collins’ Model 2. See Table 4 for definitions of column headings.
LR LP CBs 0 CBs ≤ 2 CBs F
Collins’ Model 2 89.75 90.19 0.77 69.10 88.31 89.97
Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01
Simplified head rules 88.55 88.80 0.86 67.25 87.42 88.67
</table>
<subsectionHeader confidence="0.998937">
8.3 Choice of Heads
</subsectionHeader>
<bodyText confidence="0.99999772">
If not bilexical statistics, then surely, one might think, head-choice is critical to the
performance of a head-driven lexicalized statistical parsing model. Partly to this end,
in Chiang and Bikel (2002), we explored methods for recovering latent information
in treebanks. The second half of that paper focused on a use of the Inside–Outside
algorithm to reestimate the parameters of a model defined over an augmented tree
space, where the observed data were considered to be the gold-standard labeled brack-
etings found in the treebank, and the hidden data were considered to be the head-
lexicalizations, one of the most notable tree augmentations performed by modern
statistical parsers. These expectation maximization (EM) experiments were motivated
by the desire to overcome the limitations imposed by the heuristics that have been
heretofore used to perform head-lexicalization in treebanks. In particular, it appeared
that the head rules used in Collins’ parser had been tweaked specifically for the En-
glish Penn Treebank. Using EM would mean that very little effort would need to be
spent on developing head rules, since EM could take an initial model that used simple
heuristics and optimize it appropriately to maximize the likelihood of the unlexical-
ized (observed) training trees. To test this, we performed experiments with an initial
model trained using an extremely simplified head-rule set in which all rules were of
the form “if the parent is X, then choose the left/rightmost child.” A surprising side
result was that even with this simplified set of head-rules, overall parsing performance
still remained quite high. Using our simplified head-rule set for English, our engine in
its “Model 2 emulation mode” achieved a recall of 88.55% and a precision of 88.80%
for sentences of length ≤40 words in Section 00 (see Table 7). So contrary to our ex-
pectations, the lack of careful head-choice is not crippling in allowing the parser to
disambiguate competing theories and is a further indication that semantic preferences,
as represented by conditioning on a headword, rarely override structural ones.
</bodyText>
<subsectionHeader confidence="0.995619">
8.4 Lexical Dependencies Matter
</subsectionHeader>
<bodyText confidence="0.991448727272727">
Given that bilexical dependencies are almost never used and have a surprisingly small
effect on overall parsing performance, and given that the choice of head is not terribly
critical either, one might wonder what power, if any, head-lexicalization is providing.
The answer is that even when one removes bilexical dependencies from the model,
there are still plenty of lexico-structural dependencies, that is, structures being gen-
erated conditioning on headwords and headwords being generated conditioning on
structures.
To test the effect of such lexicostructural dependencies in our lexicalized PCFG-
style formalism, we experimented with the removal of the head tag th and/or the
head word wh from the conditioning contexts of the PMw and PM parameters. The re-
certainly points to the utility of caching probabilities (the 219 million are tokens, not types).
</bodyText>
<page confidence="0.995337">
506
</page>
<note confidence="0.87878">
Bikel Intricacies of Collins’ Parsing Model
</note>
<tableCaption confidence="0.991691">
Table 8
</tableCaption>
<bodyText confidence="0.856607285714286">
Parsing performance with various models on Section 00 of the Penn Treebank. PM is the
parameter class for generating partially lexicalized modifying nonterminals (a nonterminal
label and part of speech). PMw is the parameter class that generates the headword of a
modifying nonterminal. Together, PM and PMw generate a fully lexicalized modifying
nonterminal. The check marks indicate the inclusion of the headword wh and its part of
speech th of the lexicalized head nonterminal H(th,wh) in the conditioning contexts of PM and
PMw. See Table 4 for definitions of the remaining column headings.
</bodyText>
<table confidence="0.998917375">
Parameter class PM wh PMw LR LP CBs Score ≤ 2 CBs F
Conditioning on th th wh 0 CBs
Model Mtw,tw ✓ ✓ ✓ ✓ 89.89 90.14 0.78 68.82 89.21 90.01
Mtw,t ✓ ✓ ✓ 89.49 89.95 0.80 67.98 88.82 89.72
Mt,t ✓ ✓ 88.20 88.89 0.91 65.00 87.13 88.54
Mtw,φ ✓ ✓ 89.24 89.86 0.81 66.80 88.76 89.55
Mt,φ ✓ 88.01 88.96 0.91 63.93 86.91 88.48
Mφ,φ 87.01 88.75 0.96 61.08 86.00 87.87
</table>
<bodyText confidence="0.999780705882353">
sults are shown in Table 8. Model Mtw,tw shows our baseline, and Model Mφ,φ shows
the effect of removing all dependence on the headword and its part of speech, with the
other models illustrating varying degrees of removing elements from the two parame-
ter classes’ conditioning contexts. Notably, including the headword wh in or removing
it from the PM contexts appears to have a significant effect on overall performance, as
shown by moving from Model Mtw,t to Model Mt,t and from Model Mtw,φ to Model
Mt,φ. This reinforces the notion that particular headwords have structural preferences,
so that making the PM parameters dependent on headwords would capture such pref-
erences. As for effects involving dependence on the head tag th, observe that moving
from Model Mtw,t to Model Mtw,φ results in a small drop in both recall and precision,
whereas making an analogous move from Model Mt,t to Model Mt,φ results in a drop
in recall, but a slight gain in precision (the two moves are analogous in that in both
cases, th is dropped from the context of PMw). It is not evident why these two moves
do not produce similar performance losses, but in both cases, the performance drops
are small relative to those observed when eliminating wh from the conditioning con-
texts, indicating that headwords matter far more than parts of speech for determining
structural preferences, as one would expect.
</bodyText>
<sectionHeader confidence="0.99378" genericHeader="evaluation">
9. Conclusion
</sectionHeader>
<bodyText confidence="0.999978928571429">
We have documented what we believe is the complete set of heretofore unpublished
details Collins used in his parser, such that, along with Collins’ (1999) thesis, thi
s article contains all information necessary to duplicate Collins’ benchmark results.
Indeed, these as-yet-unpublished details account for an 11% relative increase in er-
ror from an implementation including all details to a clean-room implementation of
Collins’ model. We have also shown a cleaner and equally well-performing method
for the handling of punctuation and conjunction, and we have revealed certain other
probabilistic oddities about Collins’ parser. We have not only analyzed the effect of
the unpublished details but also reanalyzed the effect of certain well-known details,
revealing that bilexical dependencies are barely used by the model and that head
choice is not nearly as important to overall parsing performance as once thought. Fi-
nally, we have performed experiments that show that the true discriminative power
of lexicalization appears to lie in the fact that unlexicalized syntactic structures are
generated conditioning on the headword and head tag. These results regarding the
</bodyText>
<page confidence="0.970832">
507
</page>
<note confidence="0.459579">
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.999504142857143">
lack of reliance on bilexical statistics suggest that generative models still have room
for improvement through the employment of bilexical-class statistics, that is, depen-
dencies among head-modifier word classes, where such classes may be defined by, say,
WordNet synsets. Such dependencies might finally be able to capture the semantic
preferences that were thought to be captured by standard bilexical statistics, as well
as to alleviate the sparse-data problems associated with standard bilexical statistics.
This is the subject of our current research.
</bodyText>
<sectionHeader confidence="0.915543" genericHeader="conclusions">
Appendix: Complete List of Parameter Classes
</sectionHeader>
<bodyText confidence="0.998616363636364">
This section contains tables for all parameter classes in Collins’ Model 3, with appro-
priate modifications and additions from the tables presented in Collins’ thesis. The
notation is that used throughout this article. In particular, for notational brevity we
use M(w, t)i to refer to the three items Mi, tMi, and wMi that constitute some fully lexi-
calized modifying nonterminal and similarly M(t)i to refer to the two items Mi and tMi
that constitute some partially lexicalized modifying nonterminal. The (unlexicalized)
nonterminal-mapping functions alpha and gamma are defined in Section 6.1. As a
shorthand, y(M(t)i) = y(Mi),tMi.
The head-generation parameter class, PH, gap-generation parameter class, PG, and
subcat-generation parameter classes, PsubcatL and PsubcatR, have back-off structures as
follows:
</bodyText>
<equation confidence="0.986886333333333">
Back-off level PH(H |...) PG(G |...)
PsubcatL(subcatL |...)
PsubcatR(subcatR |...)
0 y(P), wh, th α(y(P)), α(y(H)), wh, th
1 y(P), th α(y(P)), α(y(H)), th
2 y(P) α(y(P)), α(y(H))
</equation>
<bodyText confidence="0.99986675">
The two parameter classes for generating modifying nonterminals that are not
dominated by a base NP, PM and PMw, have the following back-off structures. Recall
that back-off level 2 of the PMw parameters includes words that are the heads of the
observed roots of sentences (that is, the headword of the entire sentence).
</bodyText>
<equation confidence="0.9405489">
(M(t)i, coord, punc  |...)
Back-off level PM
0 α(P), y(H), wh, th, ∆side, subcatside, side
1 α(P), y(H), th, ∆side, subcatside, side
2 α(P), y(H), ∆side, subcatside, side
(wMi |...)
Back-off level PMw
0 y(M(t)i), coord, punc, α(P), y(H), wh, th, ∆side, subcatside, side
1 y(M(t)i), coord, punc, α(P), y(H), th, ∆side, subcatside, side
2 tMi
</equation>
<bodyText confidence="0.999334875">
The two parameter classes for generating modifying nonterminals that are children
of base NPs (NPB nodes), PM,NPB and PMw,NPB, have the following back-off structures.
Back-off level 2 of the PMw,NPB parameters includes words that are the heads of the
observed roots of sentences (that is, the headword of the entire sentence). Also, note
that there is no coord flag, as coordinating conjunctions are generated in the same
way as regular modifying nonterminals when they are dominated by NPB. Finally, we
define M0 = H, that is, the head nonterminal label of the base NP that was generated
using a PH parameter.
</bodyText>
<page confidence="0.963209">
508
</page>
<note confidence="0.395738">
Bikel Intricacies of Collins’ Parsing Model
</note>
<equation confidence="0.861661">
(wMi |. . .)
Back-off level PM,NPB (M(t)i, punc |...) PMw,NPB
0 P, M(w, t)i−1, side Mi, tMi, punc, P, M(w, t)i−1, side
1 P, M(t)i−1, side Mi, tMi, punc, P, M(t)i−1, side
2 P, Mi−1, side tMi
</equation>
<bodyText confidence="0.997419666666667">
The two parameter classes for generating punctuation and coordinating conjunc-
tions, Ppunc and Pcoord, have the following back-off structures (Collins, personal com-
munication, October 2001), where
</bodyText>
<listItem confidence="0.990424125">
• type is a flag that obtains the value p in the history contexts of Ppunc
parameters and c in the history contexts of Pcoord parameters;
• M(w, t)i is the modifying preterminal that is being conjoined to the
head-child;
• tp or tc is the particular preterminal (part-of-speech tag) that is conjoining
the modifier to the head-child (such as CC or :);
• wp or wc is the particular word that is conjoining the modifier to the
head-child (such as and or :).
</listItem>
<equation confidence="0.810215">
Back-off level Pcoord(tc |. . .) Pcoordw(wc |. . .)
...)
Ppunc(tp ...) Ppuncw(wp
0 wh, th, P, H, M(w,t)i, type ttype, wh, th, P, H, M(w,t)i, type
1 th, P, H, M(t)i, type ttype, th, P, H, M(t)i, type
</equation>
<bodyText confidence="0.83299">
2 type ttype
The parameter classes for generating fully lexicalized root nonterminals given the
hidden root +TOP+, PTOP and PTOPw, have the following back-off structures (identical
to Table 3; n/a: not applicable).
</bodyText>
<figure confidence="0.557314333333333">
Back-off level PTOPNT(H(t)  |. . .) PTOPw(w  |...)
0 +TOP+ t,H,+TOP+
1 n/a t
</figure>
<bodyText confidence="0.9979355">
The parameter classes for generating prior probabilities on lexicalized nontermi-
nals M(w, t), Ppriorw and PpriorNT, have the following back-off structures, where prior is a
dummy variable to indicate that Ppriorwis not smoothed (although the Ppriorw parameters
still have an associated smoothing weight; see note 27).
</bodyText>
<figure confidence="0.81371">
Back-off level Ppriorw(w, t |...) PpriorNT(M |...)
0 prior w, t
1 prior t
</figure>
<sectionHeader confidence="0.987138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.82280625">
I would especially like to thank Mike
Collins for his invaluable assistance and
great generosity while I was replicating his
thesis results and for his comments on a
prerelease draft of this article. Many thanks
to David Chiang and Dan Gildea for the
many valuable discussions during the
course of this work. Also, thanks to the
anonymous reviewers for their helpful and
astute observations. Finally, thanks to my
Ph.D. advisor Mitch Marcus, who during
the course of this work was, as ever, a
source of keen insight and unbridled
optimism. This work was supported in part
by NSF grant no. SBR-89-20239 and DARPA
grant no. N66001-00-1-8915.
</bodyText>
<page confidence="0.99244">
509
</page>
<figure confidence="0.345061">
Computational Linguistics Volume 30, Number 4
</figure>
<sectionHeader confidence="0.858005" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842615384616">
Baker, J. K. 1979. Trainable grammars for
speech recognition. In Spring Conference of
the Acoustical Society of America, pages
547–550, Boston.
Bies, A. 1995. Bracketing guidelines for
Treebank II style Penn Treebank Project.
Available at ftp://ftp.cis.upenn.edu/pub/
treebank/doc/manual/root.ps.gz.
Bikel, Daniel M. 2000. A statistical model for
parsing and word-sense disambiguation.
In Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora, Hong Kong, October.
Bikel, Daniel M. 2002. Design of a
multi-lingual, parallel-processing
statistical parsing engine. In Proceedings of
HLT2002, San Diego.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied to
the Chinese Treebank. In Martha Palmer,
Mitch Marcus, Aravind Joshi, and Fei Xia,
editors, Proceedings of the Second Chinese
Language Processing Workshop, pages 1–6,
Hong Kong.
Bikel, Daniel M., Richard Schwartz, Ralph
Weischedel, and Scott Miller. 1997.
Nymble: A high-performance learning
name-finder. In Fifth Conference on Applied
Natural Language Processing, pages
194–201, Washington, DC.
Black, Ezra, Frederick Jelinek, John Lafferty,
David Magerman, Robert Mercer, and
Salim Roukos. 1992. Towards
history-based grammars: Using richer
models for probabilistic parsing. In
Proceedings of the Fifth DARPA Speech and
Natural Language Workshop, Harriman, NY.
Booth, T. L. and R. A. Thompson. 1973.
Applying probability measures to abstract
languages. IEEE Transactions on Computers,
volume C-22: 442–450.
Chiang, David and Daniel M. Bikel. 2002.
Recovering latent information in
treebanks. In Proceedings of COLING’02,
Taipei.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. MIT Press,
Cambridge, MA.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 184–191,
Santa Cruz, CA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL-EACL ’97, pages
16–23, Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing. In
International Conference on Machine
Learning, Stanford University, Stanford, CA.
Collins, Michael and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In Proceedings of
ACL-02, pages 263–270, Philadelphia.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
Linguistics (COLING-96), pages 340–345,
Copenhagen, August.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods in
Natural Language Processing, Pittsburgh.
Gildea, Daniel and Daniel Jurafsky. 2000.
Automatic labeling of semantic roles. In
Proceedings of ACL 2000, Hong Kong.
Gildea, Daniel and Martha Palmer. 2002.
The necessity of parsing for predicate
argument recognition. In Proceedings of
ACL 2002, Philadelphia.
Goodman, Joshua. 1997. Global thresholding
and multiple-pass parsing. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
Brown University, Providence, RI.
Henderson, John C. and Eric Brill. 1999.
Exploiting diversity in natural language
processing: Combining parsers. In
Proceedings of the Fourth Conference on
Empirical Methods in Natural Language
Processing, College Park, MD.
Hwa, Rebecca. 2001. On minimizing
training corpus for parser acquisition. In
Proceedings of the Fifth Computational
Natural Language Learning Workshop,
Toulouse, France, July.
Hwa, Rebecca, Philip Resnik, and Amy
Weinberg. 2002. Breaking the resource
bottleneck for multilingual parsing. In
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, Third
International Conference on Language
Resources and Evaluation (LREC-2002), Las
Palmas, Canary Islands, June.
Lari, K. and S. J. Young. 1990. The
estimation of stochastic context-free
grammars using the Inside-Outside
algorithm. Computer Speech and Language,
4:35–56.
</reference>
<page confidence="0.934047">
510
</page>
<note confidence="0.514472">
Bikel Intricacies of Collins’ Parsing Model
</note>
<reference confidence="0.988252133333333">
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313–330.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Conference on Empirical Methods in
Natural Language Processing, University of
Pennsylvania, Philadelphia, May.
Witten, I. T. and T. C. Bell. 1991. The
zero-frequency problem: Estimating the
probabilities of novel events in adaptive
text compression. IEEE Transactions on
Information Theory 37: 1085–1094.
</reference>
<page confidence="0.997648">
511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455401">
<title confidence="0.978286">Intricacies of Collins’ Parsing Model</title>
<abstract confidence="0.921248642857143">M. University of Pennsylvania This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results. Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston.</location>
<marker>Baker, 1979</marker>
<rawString>Baker, J. K. 1979. Trainable grammars for speech recognition. In Spring Conference of the Acoustical Society of America, pages 547–550, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bies</author>
</authors>
<title>Bracketing guidelines for Treebank II style Penn Treebank Project. Available at ftp://ftp.cis.upenn.edu/pub/ treebank/doc/manual/root.ps.gz.</title>
<date>1995</date>
<contexts>
<context position="11657" citStr="Bies 1995" startWordPosition="1795" endWordPosition="1796">the head-child is truly conjoined to some other phrase, as opposed to a phrase in which, say, there is an initial CC, such as an S that begins with the conjunction but. 4.2 Pruning of Unnecessary Nodes As a preprocessing step, pruning of unnecessary nodes simply removes preterminals that should have little or no bearing on parser performance. In the case of the English Treebank, the pruned subtrees are all preterminal subtrees whose root label is one of {‘‘, ’’, .}. There are two reasons to remove these types of subtrees when parsing the English Treebank: First, in the treebanking guidelines (Bies 1995), quotation marks were given the lowest possible priority and thus cannot be expected to appear within constituent boundaries in any kind of consistent way, and second, neither of these types of preterminals—nor any punctuation marks, for that matter—counts towards the parsing score. 4.3 Adding Base NP Nodes An NP is basal when it does not itself dominate an NP; such NP nodes are relabeled NPB. More accurately, an NP is basal when it dominates no other NPs except possessive NPs, where a possessive NP is an NP that dominates POS, the preterminal possessive 4 Our positional descriptions here, su</context>
</contexts>
<marker>Bies, 1995</marker>
<rawString>Bies, A. 1995. Bracketing guidelines for Treebank II style Penn Treebank Project. Available at ftp://ftp.cis.upenn.edu/pub/ treebank/doc/manual/root.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>A statistical model for parsing and word-sense disambiguation.</title>
<date>2000</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="2118" citStr="Bikel 2000" startWordPosition="311" endWordPosition="312">n future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2. This task proved more difficult than it initially appeared. Starting with Collins’ (1999) thesis, we reproduced all the parameters described but did not achieve nearly the same high performance on the well-established development test set of Section 00 of ∗ Departmen</context>
</contexts>
<marker>Bikel, 2000</marker>
<rawString>Bikel, Daniel M. 2000. A statistical model for parsing and word-sense disambiguation. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Design of a multi-lingual, parallel-processing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT2002,</booktitle>
<location>San Diego.</location>
<contexts>
<context position="2354" citStr="Bikel 2002" startWordPosition="346" endWordPosition="347">es (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2. This task proved more difficult than it initially appeared. Starting with Collins’ (1999) thesis, we reproduced all the parameters described but did not achieve nearly the same high performance on the well-established development test set of Section 00 of ∗ Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104. E-mail: dbikel@linc.cis.upenn.edu 1 This engine is publicly available at http://www.cis.upenn.edu/∼dbikel/software.html Submission received: 18 January 2</context>
<context position="49124" citStr="Bikel (2002)" startWordPosition="8045" endWordPosition="8046">y for the outside probability of a chart item (see Baker [1979] and Lari and Young [1990] for full descriptions of the Inside–Outside algorithm). Previous work (Goodman 1997) has shown that the inside probability alone is an insufficient scoring metric when comparing chart items covering the same span during decoding and that some estimate of the outside probability of a chart item should be factored into the score. A prior on the root (lexicalized) nonterminal label of the derivation forest represented by a particular chart item is used for this purpose in Collins’ parser. 22 As described in Bikel (2002), our parsing engine allows easy experimentation with a wide variety of different generative models, including the ability to construct history contexts from arbitrary numbers of previously generated modifiers. The mapping function delta and the transition function tau presented in this section are just two examples of this capability. 23 This is the main reason that the cv (“contains verb”) predicate is always false for NPBs, as that predicate applies only to material that intervenes between the current modifier and the head. 24 Interestingly, unlike in the regular model, punctuation that occ</context>
<context position="80371" citStr="Bikel (2002)" startWordPosition="13241" endWordPosition="13242"> 7 Results on Section 00 with simplified head rules. The baseline model is our engine in its closest possible emulation of Collins’ Model 2. See Table 4 for definitions of column headings. LR LP CBs 0 CBs ≤ 2 CBs F Collins’ Model 2 89.75 90.19 0.77 69.10 88.31 89.97 Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01 Simplified head rules 88.55 88.80 0.86 67.25 87.42 88.67 8.3 Choice of Heads If not bilexical statistics, then surely, one might think, head-choice is critical to the performance of a head-driven lexicalized statistical parsing model. Partly to this end, in Chiang and Bikel (2002), we explored methods for recovering latent information in treebanks. The second half of that paper focused on a use of the Inside–Outside algorithm to reestimate the parameters of a model defined over an augmented tree space, where the observed data were considered to be the gold-standard labeled bracketings found in the treebank, and the hidden data were considered to be the headlexicalizations, one of the most notable tree augmentations performed by modern statistical parsers. These expectation maximization (EM) experiments were motivated by the desire to overcome the limitations imposed by</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>Bikel, Daniel M. 2002. Design of a multi-lingual, parallel-processing statistical parsing engine. In Proceedings of HLT2002, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<editor>In Martha Palmer, Mitch Marcus, Aravind Joshi, and Fei Xia, editors,</editor>
<location>Hong Kong.</location>
<contexts>
<context position="2212" citStr="Bikel and Chiang 2000" startWordPosition="322" endWordPosition="325">asis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2. This task proved more difficult than it initially appeared. Starting with Collins’ (1999) thesis, we reproduced all the parameters described but did not achieve nearly the same high performance on the well-established development test set of Section 00 of ∗ Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104. E-mail: dbi</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Bikel, Daniel M. and David Chiang. 2000. Two statistical parsing models applied to the Chinese Treebank. In Martha Palmer, Mitch Marcus, Aravind Joshi, and Fei Xia, editors, Proceedings of the Second Chinese Language Processing Workshop, pages 1–6, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
<author>Scott Miller</author>
</authors>
<title>Nymble: A high-performance learning name-finder.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<location>Washington, DC.</location>
<contexts>
<context position="52304" citStr="Bikel et al. (1997)" startWordPosition="8582" endWordPosition="8585"> via the formula ˜ei = Aiei + (1 − Ai)˜ei+1, 0 &lt; i &lt; n − 1, ˜en−1 = en−1 (15) So, for example, with three levels of back-off, the overall smoothed estimate would be defined as ˜e0 = A0e0 + (1 − A0) [A1e1 + (1 − A1)e2] (16) It is easy to prove by structural induction that if 0 &lt; Ai &lt; 1 and E ˆpi(A|Oi(B)) = 1, 0 &lt; i &lt; n − 1 A then E ˜p0(A |O0(B)) = 1 (17) A Each smoothing weight can be conceptualized as the confidence in the estimate with which it is being multiplied. These confidence values can be derived in a number of sensible ways; the technique used by Collins was adapted from that used in Bikel et al. (1997), which makes use of a quantity called the diversity of the history context (Witten and Bell 1991), which is equal to the number of unique futures observed in training for that history context. 6.8.1 Deficient Model. As previously mentioned, n back-off levels require n−1 smoothing weights. Collins’ parser effectively uses n weights, because the estimator always 497 Computational Linguistics Volume 30, Number 4 adds an extra, constant-valued estimate to the back-off chain. Collins’ parser hardcodes this extra value to be a vanishingly small (but nonzero) “probability” of 10−19, resulting in smo</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, Miller, 1997</marker>
<rawString>Bikel, Daniel M., Richard Schwartz, Ralph Weischedel, and Scott Miller. 1997. Nymble: A high-performance learning name-finder. In Fifth Conference on Applied Natural Language Processing, pages 194–201, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>David Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth DARPA Speech and Natural Language Workshop,</booktitle>
<location>Harriman, NY.</location>
<contexts>
<context position="25798" citStr="Black et al. 1992" startWordPosition="4286" endWordPosition="4289"> VP, the vi predicate is true, because one of the previously generated modifiers on the right side of the head dominates a verb, continue.12 More formally, this feature is most easily defined in terms of a recursively defined cv (“contains verb”) predicate, which is true if and only if a node dominates a verb: V cv(M) if M is not a preterminal M child of P true if P is a verb preterminal false otherwise cv(P) = I (2) 12 Note that any word in the surface strings dominated by the previously generated modifiers will trigger the vi predicate. This is possible because in a history-based model (cf. Black et al. 1992), anything previously generated—that is, anything in the history—can appear in the conditioning context. 488 Bikel Intricacies of Collins’ Parsing Model Referring to (2), we define the verb-intervening predicate recursively on the first-order Markov process generating modifying nonterminals: �cv(Li−1) ∨ vi(Li−2) false if i ≤ 1 vi(Li) = if i &gt; 1 (3) and similarly for right modifiers. What is considered to be a verb? While this is not spelled out, as it happens, a verb is any word whose part-of-speech tag is one of {VB, VBD, VBG, VBN, VBP, VBZ}. That is, the cv predicate returns true only for th</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1992</marker>
<rawString>Black, Ezra, Frederick Jelinek, John Lafferty, David Magerman, Robert Mercer, and Salim Roukos. 1992. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceedings of the Fifth DARPA Speech and Natural Language Workshop, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<pages>442--450</pages>
<contexts>
<context position="41325" citStr="Booth and Thompson 1973" startWordPosition="6758" endWordPosition="6761">irst-class words, but only as triggered from these punc and coord flags, meaning that the number of such intervening conjunctive items (and the order in which they are to be generated) is not specified. So for a given sentence/tree pair containing a conjunction and/or a punctuation mark, there is an infinite number of similar sentence/tree pairs with arbitrary amounts of “conjunctive” material between the same two nodes. Because all of these trees have the same, nonzero probability, the sum ETP(T), where T is a possible tree generated by the model, diverges, meaning the model is inconsistent (Booth and Thompson 1973). Another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 In fact, if punctuation occurs before the head, it is not generated at all—a deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of Collins (1997). 20 In (9), for clarity we have left out subcat generation and the use of Collins’ distance metric in the conditioning contexts. We have also glossed over the fact that lexicalized modifying nonterminals are actually generated in two steps, using two differently smoothed par</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Booth, T. L. and R. A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, volume C-22: 442–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING’02,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="80371" citStr="Chiang and Bikel (2002)" startWordPosition="13239" endWordPosition="13242">ber 4 Table 7 Results on Section 00 with simplified head rules. The baseline model is our engine in its closest possible emulation of Collins’ Model 2. See Table 4 for definitions of column headings. LR LP CBs 0 CBs ≤ 2 CBs F Collins’ Model 2 89.75 90.19 0.77 69.10 88.31 89.97 Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01 Simplified head rules 88.55 88.80 0.86 67.25 87.42 88.67 8.3 Choice of Heads If not bilexical statistics, then surely, one might think, head-choice is critical to the performance of a head-driven lexicalized statistical parsing model. Partly to this end, in Chiang and Bikel (2002), we explored methods for recovering latent information in treebanks. The second half of that paper focused on a use of the Inside–Outside algorithm to reestimate the parameters of a model defined over an augmented tree space, where the observed data were considered to be the gold-standard labeled bracketings found in the treebank, and the hidden data were considered to be the headlexicalizations, one of the most notable tree augmentations performed by modern statistical parsers. These expectation maximization (EM) experiments were motivated by the desire to overcome the limitations imposed by</context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>Chiang, David and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In Proceedings of COLING’02, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="45989" citStr="Cohen 1995" startWordPosition="7549" endWordPosition="7550">tional mechanism that attempted to generate punctuation and conjunctions with conditional independence. One of our reviewers astutely pointed out that the mechanism led to a deficient model (the very thing we have been trying to avoid), and so we have subsequently removed it from our model. The removal leads to a 0.05% absolute reduction in F-measure (which in this case is also a 0.05% relative increase in error) on sentences of length ≤ 40 words in Section 00 of the Penn Treebank. As this difference is not at all statistically significant (according to a randomized stratified shuffling test [Cohen 1995]), all evaluations reported in this article are with the original model. 495 Computational Linguistics Volume 30, Number 4 As shown in Section 8.1, this new parameterization yields virtually identical performance to that of the Collins model.22 6.6 The Base NP Model: A Model unto Itself As we have already seen, there are several ways in which base NPs are exceptional in Collins’ parsing model. This is partly because the flat structure of base NPs in the Penn Treebank suggested the use of a completely different model by which to generate them. Essentially, the model for generating children of </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Cohen, Paul R. 1995. Empirical Methods for Artificial Intelligence. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<location>Santa Cruz, CA.</location>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 184–191, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-EACL ’97,</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="41649" citStr="Collins (1997)" startWordPosition="6812" endWordPosition="6813">ce/tree pairs with arbitrary amounts of “conjunctive” material between the same two nodes. Because all of these trees have the same, nonzero probability, the sum ETP(T), where T is a possible tree generated by the model, diverges, meaning the model is inconsistent (Booth and Thompson 1973). Another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 In fact, if punctuation occurs before the head, it is not generated at all—a deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of Collins (1997). 20 In (9), for clarity we have left out subcat generation and the use of Collins’ distance metric in the conditioning contexts. We have also glossed over the fact that lexicalized modifying nonterminals are actually generated in two steps, using two differently smoothed parameters. 493 Computational Linguistics Volume 30, Number 4 NP ✘✘✘✘✘✘✘ ✟✟✟✟ ❍❍ � � � � � � � NP ❍ ❍ (a) NPB NN fire , , CC and , ADVP... , RB ultimately (b) NP ✏✏✏✏✏✏✏ � �� � ❅ � � � � ❅ � NP , CC ADVP... NPB , and RB NN ultimately (c) fire NP ✏✏✏✏✏ � � �� ❅ ❅ � � � NP NPB NN CC and , ADVP... , RB ultimately fire Figure 10 </context>
<context position="69040" citStr="Collins (1997" startWordPosition="11381" endWordPosition="11382">s’ implementation of this form of pruning, however. Commas are quite common within parenthetical phrases. Accordingly, if a comma in an input 31 If one generates commas as first-class words, as we have done, one must take great care in applying this comma constraint, for otherwise, chart items that represent partially completed constituents (i.e., constituents for which not all modifiers have been generated) may be incorrectly rejected. This is especially important for NPB constituents. 502 Bikel Intricacies of Collins’ Parsing Model Table 4 Overall parsing results using only details found in Collins (1997, 1999). The first two lines show the results of Collins’ parser and those of our parser in its “complete” emulation mode (i.e., including unpublished details). All reported scores are for sentences of length &lt; 40 words. LR (labeled recall) and LP (labeled precision) are the primary scoring metrics. CBs is the number of crossing brackets. 0 CBs and &lt; 2 CBs are the percentages of sentences with 0 and &lt; 2 crossing brackets, respectively. F (the F-measure) is the evenly weighted harmonic mean of precision and recall, or 1 LP·LR 2 (LP+LR) Performance on Section 00 Model LR LP CBs 0 CBs &lt; 2 CBs F C</context>
<context position="71779" citStr="Collins 1997" startWordPosition="11850" endWordPosition="11851">appens, the tag dictionary built up when training contains entries for every word observed, even low-frequency words. This means that during decoding, the output of the tagger is used only for those words that are truly unknown, that is, that were never observed in training. For all other words, the chart is seeded with a separate item for each tag observed with that word in training. 8. Evaluation 8.1 Effects of Unpublished Details In this section we present the results of effectively doing a “clean-room” implementation of Collins’ parsing model, that is, using only information available in (Collins 1997, 1999), as shown in Table 4. The clean-room model has a 10.6% increase in F-measure error compared to Collins’ parser and an 11.0% increase in F-measure error compared to our engine in its complete emulation of Collins’ Model 2. This is comparable to the increase in 32 Although we have implemented a version of this type of pruning that limits the number of items that can be collected in any one cell, that is, the maximum number of items that cover a particular span. 503 Computational Linguistics Volume 30, Number 4 Table 5 Effects of independently removing or changing individual details on ov</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of ACL-EACL ’97, pages 16–23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="75358" citStr="Collins (1999" startWordPosition="12430" endWordPosition="12431">nts for the noticeable effects of all the unpublished details of Collins’ model. But what of the details that were published? In chapter 8 of his thesis, Collins gives an account on the motivation of various features of his model, including the distance metric, the model’s use of subcats (and their interaction with the distance metric), and structural versus semantic preferences. In the discussion of this last issue, Collins points to the fact that structural preferences—which, in his model, are 33 These F-measures and the differences between them were calculated from experiments presented in Collins (1999, page 201); these experiments, unlike those on which our reported numbers are based, were on all sentences, not just those of length ≤ 40 words. As Collins notes, removing both the distance metric and subcat features results in a gigantic drop in performance, since without both of these features, the model has no way to encode the fact that flatter structures should be avoided in several crucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children. 34 As a reviewer pointed out, the use of the comma constraint is a “published” detail. However, the speci</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<location>Stanford University, Stanford, CA.</location>
<contexts>
<context position="1534" citStr="Collins 2000" startWordPosition="225" endWordPosition="226">sing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. 1. Introduction Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing. Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study o</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Collins, Michael. 2000. Discriminative reranking for natural language parsing. In International Conference on Machine Learning, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02,</booktitle>
<pages>263--270</pages>
<location>Philadelphia.</location>
<contexts>
<context position="1559" citStr="Collins and Duffy 2002" startWordPosition="227" endWordPosition="230">ce as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. 1. Introduction Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing. Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Collins, Michael and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL-02, pages 263–270, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<location>Copenhagen,</location>
<contexts>
<context position="76841" citStr="Eisner 1996" startWordPosition="12669" endWordPosition="12670">off of the modifier-word generation model, PMw, when testing on Section 00, having trained on Sections 02–21. In other words, this table reports how often a context in the back-off chain of PMw that was needed during decoding was observed in training. Back-off level Number of accesses Percentage 0 3,257,309 1.5 1 24,294,084 11.0 2 191,527,387 87.4 Total 219,078,780 100.0 modeled primarily by the PL and PR parameters—often provide the right information for disambiguating competing analyses, but that these structural preferences may be “overridden” by semantic preferences. Bilexical statistics (Eisner 1996), as represented by the maximal context of the PLw and PRw parameters, serve as a proxy for such semantic preferences, where the actual modifier word (as opposed to, say, merely its part of speech) indicates the particular semantics of its head. Indeed, such bilexical statistics were widely assumed for some time to be a source of great discriminative power for several different parsing models, including that of Collins. However, Gildea (2001) reimplemented Collins’ Model 1 (essentially Model 2 but without subcats) and altered the PLw and PRw parameters so that they no longer had the top level </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Eisner, Jason. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340–345, Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="1759" citStr="Gildea 2001" startWordPosition="259" endWordPosition="260"> the headword and its part of speech. 1. Introduction Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing. Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As</context>
<context position="4064" citStr="Gildea 2001" startWordPosition="591" endWordPosition="592">ing performance, as well as some comparative analysis of the effects of published features.3 In particular, implementing Collins’ model using only the published details causes an 11% increase in relative error over Collins’ own published results. That is, taken together, all the unpublished details have a significant effect on overall parsing performance. In addition to the effects of the unpublished details, we also have new evidence to show that the discriminative power of Collins’ model does not lie where once thought: Bilexical dependencies play an extremely small role in Collins’ models (Gildea 2001), and head choice is not nearly as critical as once thought. This article also discusses the rationale for various parameter choices. In general, we will limit our discussion to Collins’ Model 2, but we make occasional reference to Model 3, as well. 2. Motivation There are three primary motivations for this work. First, Collins’ parsing model represents a widely used and cited parsing model. As such, if it is not desirable to use it as a black box (it has only recently been made publicly available), then it should be possible to replicate the model in full, providing a necessary consistency am</context>
<context position="6342" citStr="Gildea (2001)" startWordPosition="968" endWordPosition="969">andard evaluation metrics, this work aims to provide a more thorough understanding of the nature of a model’s features. This understanding not only is useful in its own right but should help point the way toward newer features to model or better modeling techniques, for we are in the best position for advancement when we understand existing strengths and limitations. 2 In the course of replicating Collins’ results, it was brought to our attention that several other researchers had also tried to do this and had also gotten performance that fell short of Collins’ published results. For example, Gildea (2001) reimplemented Collins’ Model 1 but obtained results with roughly 16.7% more relative error than Collins’ reported results using that model. 3 Discovering these details and features involved a great deal of reverse engineering, and ultimately, much discussion with Collins himself and perusal of his code. Many thanks to Mike Collins for his generosity. As a word of caution, this article is exhaustive in its presentation of all such details and features, and we cannot guarantee that every reader will find every detail interesting. 480 Bikel Intricacies of Collins’ Parsing Model 3. Model Overview</context>
<context position="77287" citStr="Gildea (2001)" startWordPosition="12740" endWordPosition="12741">nformation for disambiguating competing analyses, but that these structural preferences may be “overridden” by semantic preferences. Bilexical statistics (Eisner 1996), as represented by the maximal context of the PLw and PRw parameters, serve as a proxy for such semantic preferences, where the actual modifier word (as opposed to, say, merely its part of speech) indicates the particular semantics of its head. Indeed, such bilexical statistics were widely assumed for some time to be a source of great discriminative power for several different parsing models, including that of Collins. However, Gildea (2001) reimplemented Collins’ Model 1 (essentially Model 2 but without subcats) and altered the PLw and PRw parameters so that they no longer had the top level of context that included the headword (he removed back-off level 0, as depicted in Table 1). In other words, Gildea removed all bilexical statistics from the overall model. Surprisingly, this resulted in only a 0.45% absolute reduction in F-measure (3.3% relative increase in error). Unfortunately, this result was not entirely conclusive, in that Gildea was able to reimplement Collins’ baseline model only partially, and the performance of his </context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, Daniel. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="1962" citStr="Gildea and Jurafsky 2000" startWordPosition="283" endWordPosition="286">they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2. This task proved more difficult than it initially appeared. Starting with Collins’ (1999) thesis, we reproduced</context>
</contexts>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2000. Automatic labeling of semantic roles. In Proceedings of ACL 2000, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="1987" citStr="Gildea and Palmer 2002" startWordPosition="287" endWordPosition="290">ce benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2. This task proved more difficult than it initially appeared. Starting with Collins’ (1999) thesis, we reproduced all the parameters descr</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Gildea, Daniel and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of ACL 2002, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="48686" citStr="Goodman 1997" startWordPosition="7973" endWordPosition="7974">.e., nonpunctuation) preterminal.24 Base NPs are also exceptional with respect to determining chart item equality, the comma-pruning rule, and general beam pruning (see Section 7.2 for details). 6.7 Parameter Classes for Priors on Lexicalized Nonterminals Two parameter classes that make their appearance only in Appendix E of Collins’ thesis are those that compute priors on lexicalized nonterminals. These priors are used as a crude proxy for the outside probability of a chart item (see Baker [1979] and Lari and Young [1990] for full descriptions of the Inside–Outside algorithm). Previous work (Goodman 1997) has shown that the inside probability alone is an insufficient scoring metric when comparing chart items covering the same span during decoding and that some estimate of the outside probability of a chart item should be factored into the score. A prior on the root (lexicalized) nonterminal label of the derivation forest represented by a particular chart item is used for this purpose in Collins’ parser. 22 As described in Bikel (2002), our parsing engine allows easy experimentation with a wide variety of different generative models, including the ability to construct history contexts from arbi</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Goodman, Joshua. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="1659" citStr="Henderson and Brill 1999" startWordPosition="244" endWordPosition="247">f lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. 1. Introduction Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing. Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that i</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>Henderson, John C. and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>On minimizing training corpus for parser acquisition.</title>
<date>2001</date>
<booktitle>In Proceedings of the Fifth Computational Natural Language Learning Workshop,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="1788" citStr="Hwa 2001" startWordPosition="263" endWordPosition="264">eech. 1. Introduction Michael Collins’ (1996, 1997, 1999) parsing models have been quite influential in the field of natural language processing. Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline mode</context>
</contexts>
<marker>Hwa, 2001</marker>
<rawString>Hwa, Rebecca. 2001. On minimizing training corpus for parser acquisition. In Proceedings of the Fifth Computational Natural Language Learning Workshop, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
</authors>
<title>Breaking the resource bottleneck for multilingual parsing.</title>
<date>2002</date>
<booktitle>In Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data, Third International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<location>Las Palmas, Canary Islands,</location>
<marker>Hwa, Resnik, Weinberg, 2002</marker>
<rawString>Hwa, Rebecca, Philip Resnik, and Amy Weinberg. 2002. Breaking the resource bottleneck for multilingual parsing. In Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data, Third International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<marker>Lari, Young, 1990</marker>
<rawString>Lari, K. and S. J. Young. 1990. The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia,</location>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, Adwait. 1996. A maximum entropy model for part-of-speech tagging. In Conference on Empirical Methods in Natural Language Processing, University of Pennsylvania, Philadelphia, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I T Witten</author>
<author>T C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory</journal>
<volume>37</volume>
<pages>1085--1094</pages>
<contexts>
<context position="52402" citStr="Witten and Bell 1991" startWordPosition="8599" endWordPosition="8602">ith three levels of back-off, the overall smoothed estimate would be defined as ˜e0 = A0e0 + (1 − A0) [A1e1 + (1 − A1)e2] (16) It is easy to prove by structural induction that if 0 &lt; Ai &lt; 1 and E ˆpi(A|Oi(B)) = 1, 0 &lt; i &lt; n − 1 A then E ˜p0(A |O0(B)) = 1 (17) A Each smoothing weight can be conceptualized as the confidence in the estimate with which it is being multiplied. These confidence values can be derived in a number of sensible ways; the technique used by Collins was adapted from that used in Bikel et al. (1997), which makes use of a quantity called the diversity of the history context (Witten and Bell 1991), which is equal to the number of unique futures observed in training for that history context. 6.8.1 Deficient Model. As previously mentioned, n back-off levels require n−1 smoothing weights. Collins’ parser effectively uses n weights, because the estimator always 497 Computational Linguistics Volume 30, Number 4 adds an extra, constant-valued estimate to the back-off chain. Collins’ parser hardcodes this extra value to be a vanishingly small (but nonzero) “probability” of 10−19, resulting in smoothed estimates of the form ˜e0 = A0e0 + (1 − A0) [A1e1 + (1 − A1) [A2e2 + (1 − A2) · 10−1911 (18)</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Witten, I. T. and T. C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory 37: 1085–1094.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>