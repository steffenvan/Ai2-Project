<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000179">
<title confidence="0.992912">
Estimating the Reliability of MDP Policies: A Confidence Interval Approach
</title>
<author confidence="0.987966">
Joel R. Tetreault
</author>
<affiliation confidence="0.977378">
University of Pittsburgh
</affiliation>
<note confidence="0.637633">
LRDC
</note>
<address confidence="0.71866">
Pittsburgh PA, 15260, USA
</address>
<email confidence="0.997164">
tetreaul@pitt.edu
</email>
<author confidence="0.992302">
Dan Bohus
</author>
<affiliation confidence="0.9979605">
Carnegie Mellon University
Dept. of Computer Science
</affiliation>
<address confidence="0.604623">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.998872">
dbohus@cs.cmu.edu
</email>
<author confidence="0.925377">
Diane J. Litman
</author>
<affiliation confidence="0.9730165">
University of Pittsburgh
Dept. of Computer Science
</affiliation>
<address confidence="0.5382625">
LRDC
Pittsburgh PA, 15260, USA
</address>
<email confidence="0.998781">
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941681818182">
Past approaches for using reinforcement
learning to derive dialog control policies
have assumed that there was enough col-
lected data to derive a reliable policy. In
this paper we present a methodology for
numerically constructing confidence inter-
vals for the expected cumulative reward
for a learned policy. These intervals are
used to (1) better assess the reliability
of the expected cumulative reward, and
(2) perform a refined comparison between
policies derived from different Markov
Decision Processes (MDP) models. We
applied this methodology to a prior ex-
periment where the goal was to select the
best features to include in the MDP state-
space. Our results show that while some
of the policies developed in the prior work
exhibited very large confidence intervals,
the policy developed from the best feature
set had a much smaller confidence interval
and thus showed very high reliability.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848261904762">
NLP researchers frequently have to deal with issues
of data sparsity. Whether the task is machine transla-
tion or named-entity recognition, the amount of data
one has to train or test with can greatly impact the re-
liability and robustness of one’s models, results and
conclusions.
One research area that is particularly sensitive to
the data sparsity issue is machine learning, specifi-
cally in using Reinforcement Learning (RL) to learn
the optimal action for a dialogue system to make
given any user state. Typically this involves learn-
ing from previously collected data or interacting in
real-time with real users or user simulators. One of
the biggest advantages to this machine learning ap-
proach is that it can be used to generate optimal poli-
cies for every possible state. However, this method
requires a thorough exploration of the state-space to
make reliable conclusions on what the best actions
are. States that are infrequently visited in the train-
ing set could be assigned sub-optimal actions, and
therefore the resulting dialogue manager may not
provide the best interaction for the user.
In this work, we present an approach for esti-
mating the reliability of a policy derived from col-
lected training data. The key idea is to take into ac-
count the uncertainty in the model parameters (MDP
transition probabilities), and use that information to
numerically construct a confidence interval for the
expected cumulative reward for the learned policy.
This confidence interval approach allows us to: (1)
better assess the reliability of the expected cumula-
tive reward for a given policy, and (2) perform a re-
fined comparison between policies derived from dif-
ferent MDP models.
We apply the proposed approach to our previous
work (Tetreault and Litman, 2006) in using RL to
improve a spoken dialogue tutoring system. In that
work, a dataset of 100 dialogues was used to de-
velop a methodology for selecting which user state
features should be included in the MDP state-space.
But are 100 dialogues enough to generate reliable
policies? In this paper we apply our confidence in-
</bodyText>
<page confidence="0.965086">
276
</page>
<note confidence="0.799028">
Proceedings of NAACL HLT 2007, pages 276–283,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999883888888889">
terval approach to the same dataset in an effort to in-
vestigate how reliable our previous conclusions are,
given the amount of available training data.
In the following section, we discuss the prior
work and its data sparsity issue. In section 3, we
describe in detail our confidence interval methodol-
ogy. In section 4, we show how this methodology
works by applying it to the prior work. In sections 5
and 6, we present our conclusions and future work.
</bodyText>
<sectionHeader confidence="0.995579" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.998123901960785">
Past research into using RL to improve spoken di-
alogue systems has commonly used Markov Deci-
sion Processes (MDP’s) (Sutton and Barto, 1998)
to model a dialogue (such as (Levin and Pieraccini,
1997) and (Singh et al., 1999)).
A MDP is defined by a set of states {si}i=1..n,
a set of actions {ak}k=1..p, and a set of transition
probabilities which reflect the dynamics of the en-
vironment {p(si|sj,ak)}k=1..p
i,j=1..n: if the model is at
time t in state sj and takes action ak, then it will
transition to state si with probability p(si|sj,ak).
Additionally, an expected reward r(si, sj, ak) is de-
fined for each transition. Once these model parame-
ters are known, a simple dynamic programming ap-
proach can be used to learn the optimal control pol-
icy 7r*, i.e. the set of actions the model should take
at each state, to maximize its expected cumulative
reward.
The dialog control problem can be naturally cast
in this formalism: the states {si}i=1..n in the MDP
correspond to the dialog states (or an abstraction
thereof), the actions {ak}k=1..p correspond to the
particular actions the dialog manager might take,
and the rewards r(si, sj, ak) are defined to reflect
a particular dialog performance metric. Once the
MDP structure has been defined, the model param-
eters {p(si|sj, ak)}k=1..p
i,j=1..n are estimated from a cor-
pus of dialogs (either real or simulated), and, based
on them, the policy which maximizes the expected
cumulative reward is computed.
While most work in this area has focused on de-
veloping the best policy (such as (Walker, 2000),
(Henderson et al., 2005)), there has been relatively
little work done with respect to selecting the best
features to include in the MDP state-space. For in-
stance, Singh et al. (1999) showed that dialogue
length was a useful state feature and Frampton and
Lemon (2005) showed that the user’s last dialogue
act was also useful. In our previous work, we com-
pare the worth of several features. In addition, Paek
and Chickering’s (2005) work showed how a state-
space can be reduced by only selecting features that
are relevant to maximizing the reward function.
The motivation for this line of research is that if
one can properly select the most informative fea-
tures, one develops better policies, and thus a bet-
ter dialogue system. In the following sections we
summarize our past data, approach, results, and is-
sue with policy reliability.
</bodyText>
<subsectionHeader confidence="0.997295">
2.1 MDP Structure
</subsectionHeader>
<bodyText confidence="0.999308272727273">
For this study, we used an annotated corpus of
human-computer spoken dialogue tutoring sessions.
The fixed-policy corpus contains data collected from
20 students interacting with the system for five prob-
lems (for a total of 100 dialogues of roughly 50 turns
each). The corpus was annotated with 5 state fea-
tures (Table 1). It should be noted that two of the
features, Certainty and Frustration, were manually
annotated while the other three were done automat-
ically. All features are binary except for Certainty
which has three values.
</bodyText>
<table confidence="0.98531825">
State Values
Correctness Student is correct or incorrect
in the current turn
Certainty Student is certain, neutral
or uncertain in the current turn
Concept Repetition A particular concept is either new
or repeated
Frustration Student is frustrated or not
in the current turn
Percent Correct Student answers over 66% of
questions correctly in dialogue
so far, or less
</table>
<tableCaption confidence="0.999556">
Table 1: State Features in Tutoring Corpus
</tableCaption>
<bodyText confidence="0.999086666666666">
For the action set {ak}k=1..p, we looked at what
type of question the system could ask the student
given the previous state. There are a total of four
possible actions: ask a short answer question (one
that requires a simple one word response), a com-
plex answer question (one that requires a longer,
deeper response), ask both a simple and complex
question in the same turn, or do not ask a question
at all (give a hint). The reward function r was the
</bodyText>
<page confidence="0.993241">
277
</page>
<bodyText confidence="0.9999179">
learning gain of each student based on a pair of tests
before and after the entire session of 5 dialogues.
The 20 students were split into two groups (high
and low learners) based on their learning gain, so
10 students and their respective five dialogues were
given a positive reward of +100, while the remain-
der were assigned a negative reward of -100. The
rewards were assigned in the final dialogue state, a
common approach when applying RL in spoken di-
alogue systems.
</bodyText>
<subsectionHeader confidence="0.999764">
2.2 Approach and Results
</subsectionHeader>
<bodyText confidence="0.998101266666667">
To investigate the usefulness of different features,
we took the following approach. We started with
two baseline MDPs. The first model (Baseline 1)
used only the Correctness feature in the state-space.
The second model (Baseline 2) included both the
Correctness and Certainty features. Next we con-
structed 3 new models by adding each of the remain-
ing three features (Frustration, Percent Correct and
Concept Repetition) to the Baseline 2 model.
We defined three metrics to compare the policies
derived from these MDPs: (1) Diff’s: the number of
states whose policy differs from the Baseline 2 pol-
icy, (2) Percent Policy change (P.C.): the weighted
amount of change between the two policies (100%
indicates total change), and (3) Expected Cumula-
tive Reward (or ECR) which is the average reward
one would expect in that MDP when in the state-
space.
The intuition is that if a new feature were rele-
vant, the corresponding model would lead to a dif-
ferent policy and a better expected cumulative re-
ward (when compared to the baseline models). Con-
versely, if the features were not useful, one would
expect that the new policies would look similar
(specifically, the Diff’s count and % Policy Change
would be low) or produce similar expected cumula-
tive rewards to the original baseline policy.
The results of this analysis are shown in Table 2 1
The Diff’s and Policy Change metrics are undefined
for the two baselines since we only use these two
metrics to compare the other three features to Base-
&apos;Please note that to due to refinements in code, there is a
slight difference between the ECR’s reported in this work and
the ECR’s reported in the previous work, for the three features
added to Baseline 2. These changes did not alter the rankings
of these models, or the conclusions of the previous work.
line 2. All three metrics show that the best feature
to add to the Baseline 2 model is Concept Repetition
since it results in the most change over the Baseline
2 policy, and also the expected reward is the highest
as well. For the remainder of this paper, when we
refer to Concept Repetition, Frustration, or Percent
Correctness, we are referring to the model that in-
cludes that feature as well as the Baseline 2 features
Correctness and Certainty.
</bodyText>
<table confidence="0.999161">
State Feature # Diff’s % P.C. ECR
Baseline 1 N/A N/A 6.15
Baseline 2 N/A N/A 31.92
B2 + Concept Repetition 10 80.2% 42.56
B2 + Frustration 8 66.4% 32.99
B2 + Percent Correctness 4 44.3% 28.50
</table>
<tableCaption confidence="0.981608">
Table 2: Feature Comparison Results
</tableCaption>
<subsectionHeader confidence="0.999505">
2.3 Problem with Reliability
</subsectionHeader>
<bodyText confidence="0.999789037037037">
However, the approach discussed above assumes
that given the size of the data set, the ECR and poli-
cies are reliable. If the MDP model were very frag-
ile, that is the policy and expected cumulative reward
were very sensitive to the quality of the transition
probability estimates, then the metrics could reveal
quite different rankings. Previously, we used a qual-
itative approach of tracking how the worth of each
state (V-value) changed over time. The V-values
indicate how much reward one would expect from
starting in that state to get to a final state. We hy-
pothesized that if the V-values stabilized as data in-
creased, then the learned policy would be more reli-
able.
So is this V-value methodology adequate for as-
sessing if there is enough data to determine a sta-
ble policy, and also for assessing if one model is
better than another? Since our approach for state-
space selection is based on comparing a new pol-
icy with a baseline policy, having a stable policy is
extremely important since instability could lead to
different conclusions. For example, in one compar-
ison, a new policy could differ with the baseline in
8 out of 10 states. But if the MDP were unstable,
adding just a little more data could result in a differ-
ence of only 4 out of 10 states. Is there an approach
that can categorize whether given a certain data size,
</bodyText>
<page confidence="0.987504">
278
</page>
<bodyText confidence="0.999840714285714">
that the expected cumulative reward (and thus the
policy) is reliable? In the next section we present a
new methodology for numerically constructing con-
fidence intervals for these value function estimates.
Then, in the following section, we reevaluate our
prior work with this methodology and discuss the
results.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="method">
3 Confidence Interval Methodology
</sectionHeader>
<subsectionHeader confidence="0.9870175">
3.1 Policy Evaluation with Confidence
Intervals
</subsectionHeader>
<bodyText confidence="0.999966166666667">
The starting point for the proposed methodology
is the observation that for each state sj and ac-
tion ak in the MDP, the set of transition probabili-
ties {p(si|sj, ak)}i=1..n are modeled as multinomial
distributions that are estimated from the transition
counts in the training data:
</bodyText>
<equation confidence="0.9960605">
c(si, sj, ak)
ˆp(si|sj, ak) = Pn i=1 c(si, sj, ak) (1)
</equation>
<bodyText confidence="0.998912142857143">
where n is the number of states in the model, and
c(si, sj, ak) is the number of times the system was
in state sj, took action ak, and transitioned to state
si in the training data.
It is important to note that these parameters are
just estimates. The reliability of these estimates
clearly depends on the amount of training data, more
specifically on the transition counts c(si, sj, ak). For
instance, consider a model with 3 states and 2 ac-
tions. Say the model was in state s1 and took action
a1 ten times. Out of these, three times the model
transitioned back to state s1, two times it transi-
tioned to state s2, and five times to state s3. Then
we have:
</bodyText>
<equation confidence="0.963701">
ˆp(si|s1, a1) = (0.3; 0.2; 0.5) = ( 310; 210 ; 510) (2)
</equation>
<bodyText confidence="0.9990905">
Additionally, let’s say the same model was in state
s2 and took action a2 1000 times. Following that ac-
tion, it transitioned 300 times to state s1, 200 times
to state s2, and 500 times to state s3.
</bodyText>
<equation confidence="0.9850355">
ˆp(si|s2, a2) = (0.3; 0.2; 0.5) = ( 300 200 500 ) (3)
1000 1000 1000
</equation>
<bodyText confidence="0.9971101875">
While both sets of transition parameters have the
same value, the second set of estimates is more reli-
able. The central idea of the proposed approach is to
model this uncertainty in the system parameters, and
use it to numerically construct confidence intervals
for the value of the optimal policy.
Formally, each set of transition probabilities
{p(si|sj, ak)}i=1..n is modeled as amultinomial dis-
tribution, estimated from data2. The uncertainty of
multinomial estimates are commonly modeled by
means of a Dirichlet distribution. The Dirichlet dis-
tribution is characterized by a set of parameters α1,
α2, ..., αn, which in this case correspond to the
counts {c(si, sj, ak)}i=1..n. For any given j, the
likelihood of the set of multinomial transition pa-
rameters {p(si|sj, ak)}i=1..n is then given by:
</bodyText>
<equation confidence="0.993495333333333">
P({p(si|sj, ak)}i=1..n|D) =
Qn
1
= i=1 p(si|sj, ak)αi−1 (4)
Z(D)
Qn
</equation>
<bodyText confidence="0.8863384">
where Z(D) = Γ(αi)
Γ(P 1 αi) and αi = c(si, sj, ak).
Note that the maximum likelihood estimates for the
formula above correspond to the frequency count
formula we have already described:
</bodyText>
<equation confidence="0.87352875">
αi c(si,sj, ak)
ˆpML(si|sj, ak) = Pn = Pn
i=1 αi i=1 c(si, sj, ak)
(5)
</equation>
<bodyText confidence="0.954744409090909">
To capture the uncertainty in the model parame-
ters, we therefore simply need to store the counts
of the observed transitions c(si, sj, ak). Based on
this model of uncertainty, we can numerically con-
struct a confidence interval for the value of the opti-
mal policy π*. Instead of computing the value of the
policy based on the maximum likelihood transition
estimates ˆTML = {ˆpML(si|sj, ak)}k=1..p
i,j=1..n, we gen-
erate a large number of transition matrices ˆT1, ˆT1,
... ˆTm by sampling from the Dirichlet distributions
corresponding to the counts observed in the train-
ing data (in the experiments reported in this paper,
we used m = 1000). We then compute the value
of the optimal policy π* in each of these models
{Vπ*( ˆTi)}i=1..m. Finally, we numerically construct
the 95% confidence interval for the value function
based on the resulting value estimates: the bounds
for the confidence interval are set at the lowest and
highest 2.5 percentile of the resulting distribution of
the values for the optimal policy {Vπ*( ˆTi)}i=1..m.
The algorithm is outlined below:
</bodyText>
<footnote confidence="0.764006">
2By p we will denote the true model parameters; by pˆ we
will denote data-driven estimates for these parameters
</footnote>
<page confidence="0.995083">
279
</page>
<listItem confidence="0.935981">
1. compute transition counts from the training set:
</listItem>
<equation confidence="0.948974">
C = {c(si, sj, ak)}k=1..p (6)
i,j=1..n
</equation>
<bodyText confidence="0.5234355">
2. compute maximum likelihood estimates for
transition probability matrix:
</bodyText>
<equation confidence="0.993837">
TML = {�pML(si|sj,ak)}k=1..p
i,j=1..n (7)
</equation>
<listItem confidence="0.8707505">
3. use dynamic programming to compute the op-
timal policy 7r* for model TML
4. sample m transition matrices { �Tk}k=1..m, us-
ing the Dirichlet distribution for each row:
</listItem>
<equation confidence="0.989682">
{�pi(si|sj,ak)}i=1..n =
= Dir({c(si, sj, ak)}i=1..n) (8)
</equation>
<listItem confidence="0.97655425">
5. evaluate the optimal policy 7r* in each of these
m models, and obtain Vπ.( Ti)
6. numerically build the 95% confidence interval
for Vπ. from these estimates.
</listItem>
<bodyText confidence="0.999992">
To summarize, the central idea is to take into ac-
count the reliability of the transition probability esti-
mates and construct a confidence interval for the ex-
pected cumulative reward for the learned policy. In
the standard approach, we would compute an esti-
mate for the expected cumulative reward, by simply
using the transition probabilities derived from the
training set. Note that these transition probabilities
are simply estimates which are more or less accu-
rate, depending on how much data is available. The
proposed methodology does not fully trust these es-
timates, and asks the question: given that the real
world (i.e. real transition probabilities) might actu-
ally be a bit different than we think it is, how well
can we expect the learned policy to perform? Note
that the confidence interval we construct, and there-
fore the conclusions we draw, are with respect to the
policy learned from the current estimates, i.e. from
the current training set. If more data becomes avail-
able, a different optimal policy might emerge, about
which we cannot say much.
</bodyText>
<subsectionHeader confidence="0.788482">
3.2 Related Work
</subsectionHeader>
<bodyText confidence="0.999886777777778">
Given the stochastic nature of the models, confi-
dence intervals are often used to estimate the reli-
ability of results in machine learning experiments,
e.g. (Rivals and Personnaz, 2002), (Schapire, 2002)
and (Dumais et al., 1998). In this work we use a
confidence interval methodology in the context of
MDPs. The idea of modeling the uncertainty of
the transition probability estimates using Dirichlet
models also appears in (Jaulmes et al., 2005). In
that work, the authors used the uncertainty in model
parameters to develop active learning strategies for
partially observable MDPs, a topic not previously
addressed in the literature. In our work we rely on
the same model of uncertainty for the transition ma-
trix, but use it to derive confidence intervals for the
expected cumulative reward for the learned optimal
policy, in an effort to assess the reliability of this
policy.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999828066666667">
Our previous results indicated that Concept Repe-
tition was the best feature to add to the Baseline 2
state-space model, but also that Percent Correctness
and Frustration (when added to Baseline 2) offered
an improvement over the Baseline MDP’s. How-
ever, these conclusions were based on a very quali-
tative approach for determining if a policy is reliable
or not. In the following subsection, we apply our ap-
proach of confidence intervals to empirically deter-
mine if given this data set of 100 dialogues, whether
the estimates of the ECR are reliable, and whether
the original rankings and conclusions hold up under
this refined analysis. In subsection 4.2, we provide
a methodology for pinpointing when one model is
better than another.
</bodyText>
<subsectionHeader confidence="0.999233">
4.1 Quantitative Analysis of ECR Reliability
</subsectionHeader>
<bodyText confidence="0.999795461538461">
For our first investigation, we look at the confidence
intervals of each MDP’s ECR over the entire data set
of 20 students (later in this section we show plots for
the confidence intervals as data increases). Table 3
shows the upper and lower bounds for the ECR orig-
inally reported in Table 2. The first column shows
the original, estimated ECR of the MDP and the last
column is the width of the bound (the difference be-
tween the upper and lower bound).
So what conclusions can we make about the reli-
ability of the ECR, and hence of the learned policies
for the different MDP’s, given this amount of train-
ing data? The confidence interval for the ECR for
</bodyText>
<page confidence="0.988191">
280
</page>
<table confidence="0.999287333333333">
State Feature ECR Lower Bound Upper Bound Width
Baseline 1 6.15 0.21 23.73 23.52
Baseline 2 (B2) 31.92 -5.31 60.48 65.79
B2 + Concept Repetition 42.56 28.37 59.29 30.92
B2 + Frustration 32.99 -4.12 61.30 65.42
B2 + Percent Correctness 28.50 -5.89 57.82 63.71
</table>
<tableCaption confidence="0.999733">
Table 3: Confidence Intervals with complete dataset
</tableCaption>
<bodyText confidence="0.999985455882353">
the Baseline 1 model ranges from 0.21 to 23.73. Re-
call that the final states are capped at +100 and -100,
and are thus the maximum and minimum bounds
that one can see in this experiment. These bounds
tell us that, if we take into account the uncertainty
in the model estimates (given the small training set
size), with probability 0.95 the actual true ECR for
this policy will be greater than 0.21 and smaller than
23.73. The width of this confidence interval is 23.52.
For the Baseline 2 model, the bounds are much
wider: from -5.31 to 60.48, for a total width of
65.79. While the ECR estimate is 31.92 (which
is seemingly larger than 6.15 for the Baseline 1
model), the wide confidence interval tells us that this
estimate is not very reliable. It is possible that the
policy derived from this model with this amount of
data could perform poorly, and even get a negative
reward. From the dialogue system designer’s stand-
point, a model like this is best avoided.
Of the remaining three models – Concept Repeti-
tion, Frustration, and Percent Correctness, the first
one exhibits a tighter confidence interval, indicat-
ing that the estimated expected cumulative reward
(42.56) is fairly reliable: with 95% probability of
being between 28.37 and 59.29. The ECR for the
other two models (Frustration and Percent Correct-
ness) again shows a wide confidence interval once
we take into account the uncertainty in the model
parameters.
These results shed more light on the shortcom-
ings of the ECR metric used to evaluate the models
in prior work. This estimate does not take into ac-
count the uncertainty of the model parameters. For
example, a model can have an optimal policy with
a very high ECR value, but have very wide confi-
dence bounds reaching even into negative rewards.
On the other hand, another model can have a rela-
tively lower ECR but if its bounds are tighter (and
the lower bound is not negative), one can know that
that policy is less affected by poor parameter esti-
mates stemming from data sparsity issues. Using the
confidence intervals associated with the ECR gives a
much more refined, quantitative estimate of the reli-
ability of the reward, and hence of the policy derived
from that data.
An extension of this result is that confidence in-
tervals can also allow us to make refined judgments
about the comparative utility of different features,
the original motivation of our prior study. Basi-
cally, a model (M1) is better than another (M2) if
M1’s lower bound is greater than the upper bound of
M2. That is, one knows that 95% of the time, the
worst case situation of M1 (the lower bound) will
always yield a higher reward than the best case of
M2. In our data, this happens only once, with Con-
cept Repetition being empirically better than Base-
line 1, since the lower bound of Concept Repetition
is 28.37 and the upper bound of Baseline 1 is 23.73.
Given this situation, Concept Repetition is a useful
feature which, when included in the model, leads to
a better policy than simply using Correctness. We
cannot draw any conclusions about the other fea-
tures, since their bounds are generally quite wide.
Given this amount of training data, we cannot say
whether Percent Correctness and Frustration are bet-
ter features than the Baseline MDP’s. Although their
ECR’s are higher, there is too much uncertainty to
definitely conclude they are better.
</bodyText>
<subsectionHeader confidence="0.997277">
4.2 Pinpointing Model Cross-over
</subsectionHeader>
<bodyText confidence="0.999529444444444">
The previous analysis focused on a quantitative
method of (1) determining the reliability of the MDP
ECR estimate and policy, as well as (2) assessing
whether one model is better than another. In this
section, we present an extension to the second con-
tribution by answering the question: given that one
model is more reliable than another, is it possible
to determine at which point one model’s estimates
become more reliable than another model’s? In our
</bodyText>
<page confidence="0.988497">
281
</page>
<figure confidence="0.997162628571429">
Baseline 1 Baseline 2 +Concept Repetition
100
100
−60
−60
−80
−80
ECR
−20
−40
80
60
40
20
0
ECR
−20
−40
80
60
40
20
0
Confidence Bounds
Calculated ECR
Confidence Bounds
Calculated ECR
−100
0 2 4 6 8 10
# of students
−100
0 2 4 6 8 10
# of students
12 14 16 18 20
12 14 16 18 20
</figure>
<figureCaption confidence="0.999955">
Figure 1: Confidence Interval Plots
</figureCaption>
<bodyText confidence="0.999439452380953">
case, we want to know at what point Concept Rep-
etition becomes more reliable than Baseline 1. To
do this, we investigate how the confidence interval
changes as the amount of training data increases in-
stead of looking at the reliability estimate at only one
particular data size.
We incrementally increase the amount of train-
ing data (adding the data from one new student at a
time), and calculate the corresponding optimal pol-
icy and confidence interval for the expected cumula-
tive reward for that policy. Figure 1 shows the con-
fidence interval plots as data is added to the MDP
for the Baseline 1 and Concept Repetition MDP’s.
For reference, Baseline 2, Percent Correctness and
Frustration plots did not exhibit the same converg-
ing behavior as these two, which is not surprising
given how wide the final bounds are. For each plot,
the bold lines represent the upper and lower bounds,
and the dotted line represents the calculated ECR.
Analyzing the two MDP’s, we find that the confi-
dence intervals for Baseline 1 and Concept Repeti-
tion converge as more data is added, which is an ex-
pected trend. One useful result from observing the
change in confidence intervals is that one can de-
termine the point in one which one model becomes
empirically better than another. Superimposing the
upper and lower bounds (Figure 2) reveals that after
we include the data from the first 13 students, the
lower bound of Concept Repetition crosses over the
upper bound of Baseline 1.
Observing this behavior is especially useful for
performing model switching. In automatic model
switching, a dialogue manager runs in real time and
as it collects data, it can switch from using a sim-
ple dialogue model to a complex model. Confidence
intervals can be used to determine when to switch
from one model to the next by checking if a complex
model’s bounds cross over the bounds of the current
model. Basically, the dialogue manager switches
when it can be sure that the more complex model’s
ECR is not only higher, but statistically significantly
so.
</bodyText>
<figure confidence="0.577486">
Baseline 1 and Concept Repetition Superimposed
# of students
</figure>
<figureCaption confidence="0.957504">
Figure 2: Baseline 1 and Concept Repetition Bounds
</figureCaption>
<sectionHeader confidence="0.999518" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999700571428571">
Past work in using MDP’s to improve spoken dia-
logue systems have usually glossed over the issue of
whether or not there was enough training data to de-
velop reliable policies. In this work, we present a
numerical method for building confidence intervals
for the expected cumulative reward for a learned pol-
icy. The proposed approach allows one to (1) better
</bodyText>
<figure confidence="0.991687555555556">
ECR
2 14 16 18 20
−50
0 2 4 6 8 10 1
100
Baseline 1
B2 + Concept Repetition
50
0
</figure>
<page confidence="0.990268">
282
</page>
<bodyText confidence="0.999894565217391">
assess the reliability of the expected cumulative re-
ward for a given policy, and (2) perform a refined
comparison between policies derived from different
MDP models.
We applied this methodology to a prior experi-
ment where the objective was to select the best fea-
tures to include in the MDP state-space. Our results
show that policies constructed from the Baseline 1
and Concept Repetition models are more reliable,
given the amount of data available for training. The
Concept Repetition model (which is composed of
the Concept Repetition, Certainty and Correctness
features) was especially useful, as it led to a policy
that outperformed the Baseline 1 model, even when
we take into account the uncertainty in the model
estimates caused by data sparsity. In contrast, for
the Baseline 2, Percent Correctness, and Frustration
models, the estimates for the expected cumulative
reward are much less reliable, and no conclusion can
be reliably drawn about the usefulness of these fea-
tures. In addition, we showed that our confidence
interval approach has applications in another MDP
problem: model switching.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999976">
As an extension of this work, we are currently inves-
tigating in more detail what makes some MDP’s reli-
able or unreliable for a certain data size (such as the
case where Baseline 2 does not converge but a more
complicated model does, such as Concept Repeti-
tion). Our initial findings indicate that, as more data
becomes available the bounds tighten for most pa-
rameters in the transition matrix. However, for some
of the parameters the bounds can remain wide, and
that is enough to keep the confidence interval for the
expected cumulative reward from converging.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996775">
We would like to thank Jeff Schneider, Drew Bag-
nell, Pam Jordan, as well as the ITSPOKE and Pitt
NLP groups, and the Dialog on Dialogs group for
their help and comments. Finally, we would like to
thank the four anonymous reviewers for their com-
ments on the initial version of this paper. Support for
this research was provided by NSF grants #0325054
and #0328431.
</bodyText>
<sectionHeader confidence="0.998474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999437631578947">
S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998.
Inductive learning algorithms and representations for
text categorization. In Conference on Information and
Knowledge Management.
M. Frampton and O. Lemon. 2005. Reinforcement learn-
ing of dialogue strategies using the user’s last dialogue
act. In IJCAI Wkshp. on K&amp;R in Practical Dialogue
Systems.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid
reinforcement/supervised learning for dialogue poli-
cies from communicator data. In IJCAI Wkshp. on
K&amp;R in Practical Dialogue Systems.
R. Jaulmes, J. Pineau, and D. Precup. 2005. Active learn-
ing in partially observable markov decision processes.
In European Conference on Machine Learning.
E. Levin and R. Pieraccini. 1997. A stochastic model of
computer-human interaction for learning dialogues. In
Proc. ofEUROSPEECH’97.
T. Paek and D. Chickering. 2005. The markov assump-
tion in spoken dialogue management. In 6th SIGDial
Workshop on Discourse and Dialogue.
I. Rivals and L. Personnaz. 2002. Construction of con-
fidence intervals for neural networks based on least
squares estimation. In Neural Networks.
R. Schapire. 2002. The boosting approach to machine
learning: An overview. In MSRI Workshop on Nonlin-
ear Estimation and Classification.
S. Singh, M. Kearns, D. Litman, and M. Walker. 1999.
Reinforcement learning for spoken dialogue systems.
In Proc. NIPS ’99.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
The MIT Press.
J. Tetreault and D. Litman. 2006. Comparing the utility
of state features in spoken dialogue using reinforce-
ment learning. In NAACL.
M. Walker. 2000. An application of reinforcement learn-
ing to dialogue strategy selection in a spoken dialogue
system for email. JAIR, 12.
</reference>
<page confidence="0.998954">
283
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960086">
<title confidence="0.999857">Estimating the Reliability of MDP Policies: A Confidence Interval Approach</title>
<author confidence="0.99995">R Joel</author>
<affiliation confidence="0.999881">University of</affiliation>
<address confidence="0.990412">Pittsburgh PA, 15260,</address>
<email confidence="0.997792">tetreaul@pitt.edu</email>
<author confidence="0.999255">Dan Bohus</author>
<affiliation confidence="0.999673">Carnegie Mellon University Dept. of Computer Science</affiliation>
<address confidence="0.996361">Pittsburgh, PA, 15213, USA</address>
<email confidence="0.999708">dbohus@cs.cmu.edu</email>
<author confidence="0.99275">J Diane</author>
<affiliation confidence="0.999942">University of Dept. of Computer</affiliation>
<address confidence="0.99573">Pittsburgh PA, 15260,</address>
<email confidence="0.998093">litman@cs.pitt.edu</email>
<abstract confidence="0.999547304347826">Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy. In this paper we present a methodology for numerically constructing confidence intervals for the expected cumulative reward for a learned policy. These intervals are used to (1) better assess the reliability of the expected cumulative reward, and (2) perform a refined comparison between policies derived from different Markov Decision Processes (MDP) models. We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace. Our results show that while some of the policies developed in the prior work exhibited very large confidence intervals, the policy developed from the best feature set had a much smaller confidence interval and thus showed very high reliability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>J Platt</author>
<author>D Heckerman</author>
<author>M Sahami</author>
</authors>
<title>Inductive learning algorithms and representations for text categorization.</title>
<date>1998</date>
<booktitle>In Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="18054" citStr="Dumais et al., 1998" startWordPosition="3029" endWordPosition="3032">rent than we think it is, how well can we expect the learned policy to perform? Note that the confidence interval we construct, and therefore the conclusions we draw, are with respect to the policy learned from the current estimates, i.e. from the current training set. If more data becomes available, a different optimal policy might emerge, about which we cannot say much. 3.2 Related Work Given the stochastic nature of the models, confidence intervals are often used to estimate the reliability of results in machine learning experiments, e.g. (Rivals and Personnaz, 2002), (Schapire, 2002) and (Dumais et al., 1998). In this work we use a confidence interval methodology in the context of MDPs. The idea of modeling the uncertainty of the transition probability estimates using Dirichlet models also appears in (Jaulmes et al., 2005). In that work, the authors used the uncertainty in model parameters to develop active learning strategies for partially observable MDPs, a topic not previously addressed in the literature. In our work we rely on the same model of uncertainty for the transition matrix, but use it to derive confidence intervals for the expected cumulative reward for the learned optimal policy, in </context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998. Inductive learning algorithms and representations for text categorization. In Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frampton</author>
<author>O Lemon</author>
</authors>
<title>Reinforcement learning of dialogue strategies using the user’s last dialogue act.</title>
<date>2005</date>
<booktitle>In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="5804" citStr="Frampton and Lemon (2005)" startWordPosition="938" endWordPosition="941">nce metric. Once the MDP structure has been defined, the model parameters {p(si|sj, ak)}k=1..p i,j=1..n are estimated from a corpus of dialogs (either real or simulated), and, based on them, the policy which maximizes the expected cumulative reward is computed. While most work in this area has focused on developing the best policy (such as (Walker, 2000), (Henderson et al., 2005)), there has been relatively little work done with respect to selecting the best features to include in the MDP state-space. For instance, Singh et al. (1999) showed that dialogue length was a useful state feature and Frampton and Lemon (2005) showed that the user’s last dialogue act was also useful. In our previous work, we compare the worth of several features. In addition, Paek and Chickering’s (2005) work showed how a statespace can be reduced by only selecting features that are relevant to maximizing the reward function. The motivation for this line of research is that if one can properly select the most informative features, one develops better policies, and thus a better dialogue system. In the following sections we summarize our past data, approach, results, and issue with policy reliability. 2.1 MDP Structure For this stud</context>
</contexts>
<marker>Frampton, Lemon, 2005</marker>
<rawString>M. Frampton and O. Lemon. 2005. Reinforcement learning of dialogue strategies using the user’s last dialogue act. In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>O Lemon</author>
<author>K Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning for dialogue policies from communicator data.</title>
<date>2005</date>
<booktitle>In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="5561" citStr="Henderson et al., 2005" startWordPosition="897" endWordPosition="900">the MDP correspond to the dialog states (or an abstraction thereof), the actions {ak}k=1..p correspond to the particular actions the dialog manager might take, and the rewards r(si, sj, ak) are defined to reflect a particular dialog performance metric. Once the MDP structure has been defined, the model parameters {p(si|sj, ak)}k=1..p i,j=1..n are estimated from a corpus of dialogs (either real or simulated), and, based on them, the policy which maximizes the expected cumulative reward is computed. While most work in this area has focused on developing the best policy (such as (Walker, 2000), (Henderson et al., 2005)), there has been relatively little work done with respect to selecting the best features to include in the MDP state-space. For instance, Singh et al. (1999) showed that dialogue length was a useful state feature and Frampton and Lemon (2005) showed that the user’s last dialogue act was also useful. In our previous work, we compare the worth of several features. In addition, Paek and Chickering’s (2005) work showed how a statespace can be reduced by only selecting features that are relevant to maximizing the reward function. The motivation for this line of research is that if one can properly</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid reinforcement/supervised learning for dialogue policies from communicator data. In IJCAI Wkshp. on K&amp;R in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jaulmes</author>
<author>J Pineau</author>
<author>D Precup</author>
</authors>
<title>Active learning in partially observable markov decision processes.</title>
<date>2005</date>
<booktitle>In European Conference on Machine Learning.</booktitle>
<contexts>
<context position="18272" citStr="Jaulmes et al., 2005" startWordPosition="3064" endWordPosition="3067">rrent estimates, i.e. from the current training set. If more data becomes available, a different optimal policy might emerge, about which we cannot say much. 3.2 Related Work Given the stochastic nature of the models, confidence intervals are often used to estimate the reliability of results in machine learning experiments, e.g. (Rivals and Personnaz, 2002), (Schapire, 2002) and (Dumais et al., 1998). In this work we use a confidence interval methodology in the context of MDPs. The idea of modeling the uncertainty of the transition probability estimates using Dirichlet models also appears in (Jaulmes et al., 2005). In that work, the authors used the uncertainty in model parameters to develop active learning strategies for partially observable MDPs, a topic not previously addressed in the literature. In our work we rely on the same model of uncertainty for the transition matrix, but use it to derive confidence intervals for the expected cumulative reward for the learned optimal policy, in an effort to assess the reliability of this policy. 4 Results Our previous results indicated that Concept Repetition was the best feature to add to the Baseline 2 state-space model, but also that Percent Correctness an</context>
</contexts>
<marker>Jaulmes, Pineau, Precup, 2005</marker>
<rawString>R. Jaulmes, J. Pineau, and D. Precup. 2005. Active learning in partially observable markov decision processes. In European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>A stochastic model of computer-human interaction for learning dialogues.</title>
<date>1997</date>
<booktitle>In Proc. ofEUROSPEECH’97.</booktitle>
<contexts>
<context position="4187" citStr="Levin and Pieraccini, 1997" startWordPosition="666" endWordPosition="669">n effort to investigate how reliable our previous conclusions are, given the amount of available training data. In the following section, we discuss the prior work and its data sparsity issue. In section 3, we describe in detail our confidence interval methodology. In section 4, we show how this methodology works by applying it to the prior work. In sections 5 and 6, we present our conclusions and future work. 2 Previous Work Past research into using RL to improve spoken dialogue systems has commonly used Markov Decision Processes (MDP’s) (Sutton and Barto, 1998) to model a dialogue (such as (Levin and Pieraccini, 1997) and (Singh et al., 1999)). A MDP is defined by a set of states {si}i=1..n, a set of actions {ak}k=1..p, and a set of transition probabilities which reflect the dynamics of the environment {p(si|sj,ak)}k=1..p i,j=1..n: if the model is at time t in state sj and takes action ak, then it will transition to state si with probability p(si|sj,ak). Additionally, an expected reward r(si, sj, ak) is defined for each transition. Once these model parameters are known, a simple dynamic programming approach can be used to learn the optimal control policy 7r*, i.e. the set of actions the model should take a</context>
</contexts>
<marker>Levin, Pieraccini, 1997</marker>
<rawString>E. Levin and R. Pieraccini. 1997. A stochastic model of computer-human interaction for learning dialogues. In Proc. ofEUROSPEECH’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>D Chickering</author>
</authors>
<title>The markov assumption in spoken dialogue management.</title>
<date>2005</date>
<booktitle>In 6th SIGDial Workshop on Discourse and Dialogue.</booktitle>
<marker>Paek, Chickering, 2005</marker>
<rawString>T. Paek and D. Chickering. 2005. The markov assumption in spoken dialogue management. In 6th SIGDial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Rivals</author>
<author>L Personnaz</author>
</authors>
<title>Construction of confidence intervals for neural networks based on least squares estimation. In Neural Networks.</title>
<date>2002</date>
<contexts>
<context position="18010" citStr="Rivals and Personnaz, 2002" startWordPosition="3022" endWordPosition="3025">sition probabilities) might actually be a bit different than we think it is, how well can we expect the learned policy to perform? Note that the confidence interval we construct, and therefore the conclusions we draw, are with respect to the policy learned from the current estimates, i.e. from the current training set. If more data becomes available, a different optimal policy might emerge, about which we cannot say much. 3.2 Related Work Given the stochastic nature of the models, confidence intervals are often used to estimate the reliability of results in machine learning experiments, e.g. (Rivals and Personnaz, 2002), (Schapire, 2002) and (Dumais et al., 1998). In this work we use a confidence interval methodology in the context of MDPs. The idea of modeling the uncertainty of the transition probability estimates using Dirichlet models also appears in (Jaulmes et al., 2005). In that work, the authors used the uncertainty in model parameters to develop active learning strategies for partially observable MDPs, a topic not previously addressed in the literature. In our work we rely on the same model of uncertainty for the transition matrix, but use it to derive confidence intervals for the expected cumulativ</context>
</contexts>
<marker>Rivals, Personnaz, 2002</marker>
<rawString>I. Rivals and L. Personnaz. 2002. Construction of confidence intervals for neural networks based on least squares estimation. In Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schapire</author>
</authors>
<title>The boosting approach to machine learning: An overview.</title>
<date>2002</date>
<booktitle>In MSRI Workshop on Nonlinear Estimation and Classification.</booktitle>
<contexts>
<context position="18028" citStr="Schapire, 2002" startWordPosition="3026" endWordPosition="3027">tually be a bit different than we think it is, how well can we expect the learned policy to perform? Note that the confidence interval we construct, and therefore the conclusions we draw, are with respect to the policy learned from the current estimates, i.e. from the current training set. If more data becomes available, a different optimal policy might emerge, about which we cannot say much. 3.2 Related Work Given the stochastic nature of the models, confidence intervals are often used to estimate the reliability of results in machine learning experiments, e.g. (Rivals and Personnaz, 2002), (Schapire, 2002) and (Dumais et al., 1998). In this work we use a confidence interval methodology in the context of MDPs. The idea of modeling the uncertainty of the transition probability estimates using Dirichlet models also appears in (Jaulmes et al., 2005). In that work, the authors used the uncertainty in model parameters to develop active learning strategies for partially observable MDPs, a topic not previously addressed in the literature. In our work we rely on the same model of uncertainty for the transition matrix, but use it to derive confidence intervals for the expected cumulative reward for the l</context>
</contexts>
<marker>Schapire, 2002</marker>
<rawString>R. Schapire. 2002. The boosting approach to machine learning: An overview. In MSRI Workshop on Nonlinear Estimation and Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>M Kearns</author>
<author>D Litman</author>
<author>M Walker</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proc. NIPS ’99.</booktitle>
<contexts>
<context position="4212" citStr="Singh et al., 1999" startWordPosition="671" endWordPosition="674">ble our previous conclusions are, given the amount of available training data. In the following section, we discuss the prior work and its data sparsity issue. In section 3, we describe in detail our confidence interval methodology. In section 4, we show how this methodology works by applying it to the prior work. In sections 5 and 6, we present our conclusions and future work. 2 Previous Work Past research into using RL to improve spoken dialogue systems has commonly used Markov Decision Processes (MDP’s) (Sutton and Barto, 1998) to model a dialogue (such as (Levin and Pieraccini, 1997) and (Singh et al., 1999)). A MDP is defined by a set of states {si}i=1..n, a set of actions {ak}k=1..p, and a set of transition probabilities which reflect the dynamics of the environment {p(si|sj,ak)}k=1..p i,j=1..n: if the model is at time t in state sj and takes action ak, then it will transition to state si with probability p(si|sj,ak). Additionally, an expected reward r(si, sj, ak) is defined for each transition. Once these model parameters are known, a simple dynamic programming approach can be used to learn the optimal control policy 7r*, i.e. the set of actions the model should take at each state, to maximize</context>
<context position="5719" citStr="Singh et al. (1999)" startWordPosition="924" endWordPosition="927">d the rewards r(si, sj, ak) are defined to reflect a particular dialog performance metric. Once the MDP structure has been defined, the model parameters {p(si|sj, ak)}k=1..p i,j=1..n are estimated from a corpus of dialogs (either real or simulated), and, based on them, the policy which maximizes the expected cumulative reward is computed. While most work in this area has focused on developing the best policy (such as (Walker, 2000), (Henderson et al., 2005)), there has been relatively little work done with respect to selecting the best features to include in the MDP state-space. For instance, Singh et al. (1999) showed that dialogue length was a useful state feature and Frampton and Lemon (2005) showed that the user’s last dialogue act was also useful. In our previous work, we compare the worth of several features. In addition, Paek and Chickering’s (2005) work showed how a statespace can be reduced by only selecting features that are relevant to maximizing the reward function. The motivation for this line of research is that if one can properly select the most informative features, one develops better policies, and thus a better dialogue system. In the following sections we summarize our past data, </context>
</contexts>
<marker>Singh, Kearns, Litman, Walker, 1999</marker>
<rawString>S. Singh, M. Kearns, D. Litman, and M. Walker. 1999. Reinforcement learning for spoken dialogue systems. In Proc. NIPS ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4129" citStr="Sutton and Barto, 1998" startWordPosition="656" endWordPosition="659">l Linguistics terval approach to the same dataset in an effort to investigate how reliable our previous conclusions are, given the amount of available training data. In the following section, we discuss the prior work and its data sparsity issue. In section 3, we describe in detail our confidence interval methodology. In section 4, we show how this methodology works by applying it to the prior work. In sections 5 and 6, we present our conclusions and future work. 2 Previous Work Past research into using RL to improve spoken dialogue systems has commonly used Markov Decision Processes (MDP’s) (Sutton and Barto, 1998) to model a dialogue (such as (Levin and Pieraccini, 1997) and (Singh et al., 1999)). A MDP is defined by a set of states {si}i=1..n, a set of actions {ak}k=1..p, and a set of transition probabilities which reflect the dynamics of the environment {p(si|sj,ak)}k=1..p i,j=1..n: if the model is at time t in state sj and takes action ak, then it will transition to state si with probability p(si|sj,ak). Additionally, an expected reward r(si, sj, ak) is defined for each transition. Once these model parameters are known, a simple dynamic programming approach can be used to learn the optimal control p</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. Sutton and A. Barto. 1998. Reinforcement Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>D Litman</author>
</authors>
<title>Comparing the utility of state features in spoken dialogue using reinforcement learning.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3077" citStr="Tetreault and Litman, 2006" startWordPosition="479" endWordPosition="482"> for estimating the reliability of a policy derived from collected training data. The key idea is to take into account the uncertainty in the model parameters (MDP transition probabilities), and use that information to numerically construct a confidence interval for the expected cumulative reward for the learned policy. This confidence interval approach allows us to: (1) better assess the reliability of the expected cumulative reward for a given policy, and (2) perform a refined comparison between policies derived from different MDP models. We apply the proposed approach to our previous work (Tetreault and Litman, 2006) in using RL to improve a spoken dialogue tutoring system. In that work, a dataset of 100 dialogues was used to develop a methodology for selecting which user state features should be included in the MDP state-space. But are 100 dialogues enough to generate reliable policies? In this paper we apply our confidence in276 Proceedings of NAACL HLT 2007, pages 276–283, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics terval approach to the same dataset in an effort to investigate how reliable our previous conclusions are, given the amount of available training data. In th</context>
</contexts>
<marker>Tetreault, Litman, 2006</marker>
<rawString>J. Tetreault and D. Litman. 2006. Comparing the utility of state features in spoken dialogue using reinforcement learning. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
</authors>
<title>An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email.</title>
<date>2000</date>
<journal>JAIR,</journal>
<volume>12</volume>
<contexts>
<context position="5535" citStr="Walker, 2000" startWordPosition="895" endWordPosition="896">s {si}i=1..n in the MDP correspond to the dialog states (or an abstraction thereof), the actions {ak}k=1..p correspond to the particular actions the dialog manager might take, and the rewards r(si, sj, ak) are defined to reflect a particular dialog performance metric. Once the MDP structure has been defined, the model parameters {p(si|sj, ak)}k=1..p i,j=1..n are estimated from a corpus of dialogs (either real or simulated), and, based on them, the policy which maximizes the expected cumulative reward is computed. While most work in this area has focused on developing the best policy (such as (Walker, 2000), (Henderson et al., 2005)), there has been relatively little work done with respect to selecting the best features to include in the MDP state-space. For instance, Singh et al. (1999) showed that dialogue length was a useful state feature and Frampton and Lemon (2005) showed that the user’s last dialogue act was also useful. In our previous work, we compare the worth of several features. In addition, Paek and Chickering’s (2005) work showed how a statespace can be reduced by only selecting features that are relevant to maximizing the reward function. The motivation for this line of research i</context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>M. Walker. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. JAIR, 12.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>