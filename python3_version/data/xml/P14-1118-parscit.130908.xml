<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.984996">
Opinion Mining on YouTube
</title>
<author confidence="0.972236">
Aliaksei Severyn1, Alessandro Moschitti3,1,
</author>
<affiliation confidence="0.834930333333333">
Olga Uryupina1, Barbara Plank2, Katja Filippova4
1DISI - University of Trento, 2CLT - University of Copenhagen,
3Qatar Computing Research Institute, 4Google Inc.
</affiliation>
<email confidence="0.945349">
severyn@disi.unitn.it, amoschitti@qf.org.qa,
uryupina@gmail.com, bplank@cst.dk, katjaf@google.com
</email>
<sectionHeader confidence="0.993822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999708">
This paper defines a systematic approach
to Opinion Mining (OM) on YouTube
comments by (i) modeling classifiers for
predicting the opinion polarity and the
type of comment and (ii) proposing ro-
bust shallow syntactic structures for im-
proving model adaptability. We rely on the
tree kernel technology to automatically ex-
tract and learn features with better gener-
alization power than bag-of-words. An ex-
tensive empirical evaluation on our manu-
ally annotated YouTube comments corpus
shows a high classification accuracy and
highlights the benefits of structural mod-
els in a cross-domain setting.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999028096774194">
Social media such as Twitter, Facebook or
YouTube contain rapidly changing information
generated by millions of users that can dramati-
cally affect the reputation of a person or an orga-
nization. This raises the importance of automatic
extraction of sentiments and opinions expressed in
social media.
YouTube is a unique environment, just like
Twitter, but probably even richer: multi-modal,
with a social graph, and discussions between peo-
ple sharing an interest. Hence, doing sentiment
research in such an environment is highly relevant
for the community. While the linguistic conven-
tions used on Twitter and YouTube indeed show
similarities (Baldwin et al., 2013), focusing on
YouTube allows to exploit context information,
possibly also multi-modal information, not avail-
able in isolated tweets, thus rendering it a valuable
resource for the future research.
Nevertheless, there is almost no work showing
effective OM on YouTube comments. To the best
of our knowledge, the only exception is given by
the classification system of YouTube comments
proposed by Siersdorfer et al. (2010).
While previous state-of-the-art models for opin-
ion classification have been successfully applied
to traditional corpora (Pang and Lee, 2008),
YouTube comments pose additional challenges:
(i) polarity words can refer to either video or prod-
uct while expressing contrasting sentiments; (ii)
many comments are unrelated or contain spam;
and (iii) learning supervised models requires train-
ing data for each different YouTube domain, e.g.,
tablets, automobiles, etc. For example, consider a
typical comment on a YouTube review video about
a Motorola Xoom tablet:
this guy really puts a negative spin on
this, and I ’m not sure why, this seems
crazy fast , and I ’m not entirely sure
why his pinch to zoom his laggy all the
other xoom reviews
The comment contains a product name xoom and
some negative expressions, thus, a bag-of-words
model would derive a negative polarity for this
product. In contrast, the opinion towards the prod-
uct is neutral as the negative sentiment is ex-
pressed towards the video. Similarly, the follow-
ing comment:
iPad 2 is better. the superior apps just
destroy the xoom.
contains two positive and one negative word, yet
the sentiment towards the product is negative (the
negative word destroy refers to Xoom). Clearly,
the bag-of-words lacks the structural information
linking the sentiment with the target product.
In this paper, we carry out a systematic study on
OM targeting YouTube comments; its contribution
is three-fold: firstly, to solve the problems outlined
above, we define a classification schema, which
separates spam and not related comments from the
informative ones, which are, in turn, further cate-
gorized into video- or product-related comments
</bodyText>
<page confidence="0.940898">
1252
</page>
<note confidence="0.830225">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1252–1261,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999915948717949">
(type classification). At the final stage, differ-
ent classifiers assign polarity (positive, negative or
neutral) to each type of a meaningful comment.
This allows us to filter out irrelevant comments,
providing accurate OM distinguishing comments
about the video and the target product.
The second contribution of the paper is the cre-
ation and annotation (by an expert coder) of a
comment corpus containing 35k manually labeled
comments for two product YouTube domains:
tablets and automobiles.1 It is the first manu-
ally annotated corpus that enables researchers to
use supervised methods on YouTube for comment
classification and opinion analysis. The comments
from different product domains exhibit different
properties (cf. Sec. 5.2), which give the possibility
to study the domain adaptability of the supervised
models by training on one category and testing on
the other (and vice versa).
The third contribution of the paper is a novel
structural representation, based on shallow syn-
tactic trees enriched with conceptual information,
i.e., tags generalizing the specific topic of the
video, e.g., iPad, Kindle, Toyota Camry. Given the
complexity and the novelty of the task, we exploit
structural kernels to automatically engineer novel
features. In particular, we define an efficient tree
kernel derived from the Partial Tree Kernel, (Mos-
chitti, 2006a), suitable for encoding structural rep-
resentation of comments into Support Vector Ma-
chines (SVMs). Finally, our results show that our
models are adaptable, especially when the struc-
tural information is used. Structural models gen-
erally improve on both tasks – polarity and type
classification – yielding up to 30% of relative im-
provement, when little data is available. Hence,
the impractical task of annotating data for each
YouTube category can be mitigated by the use of
models that adapt better across domains.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999697285714286">
Most prior work on more general OM has been
carried out on more standardized forms of text,
such as consumer reviews or newswire. The most
commonly used datasets include: the MPQA cor-
pus of news documents (Wilson et al., 2005), web
customer review data (Hu and Liu, 2004), Ama-
zon review data (Blitzer et al., 2007), the JDPA
</bodyText>
<footnote confidence="0.985420666666667">
1The corpus and the annotation guidelines are pub-
licly available at: http://projects.disi.unitn.
it/iKernels/projects/sentube/
</footnote>
<bodyText confidence="0.999924725490196">
corpus of blogs (Kessler et al., 2010), etc. The
aforementioned corpora are, however, only par-
tially suitable for developing models on social
media, since the informal text poses additional
challenges for Information Extraction and Natu-
ral Language Processing. Similar to Twitter, most
YouTube comments are very short, the language
is informal with numerous accidental and deliber-
ate errors and grammatical inconsistencies, which
makes previous corpora less suitable to train mod-
els for OM on YouTube. A recent study focuses on
sentiment analysis for Twitter (Pak and Paroubek,
2010), however, their corpus was compiled auto-
matically by searching for emoticons expressing
positive and negative sentiment only.
Siersdorfer et al. (2010) focus on exploiting user
ratings (counts of ‘thumbs up/down’ as flagged by
other users) of YouTube video comments to train
classifiers to predict the community acceptance of
new comments. Hence, their goal is different: pre-
dicting comment ratings, rather than predicting the
sentiment expressed in a YouTube comment or its
information content. Exploiting the information
from user ratings is a feature that we have not ex-
ploited thus far, but we believe that it is a valuable
feature to use in future work.
Most of the previous work on supervised senti-
ment analysis use feature vectors to encode doc-
uments. While a few successful attempts have
been made to use more involved linguistic anal-
ysis for opinion mining, such as dependency
trees with latent nodes (T¨ackstr¨om and McDonald,
2011) and syntactic parse trees with vectorized
nodes (Socher et al., 2011), recently, a comprehen-
sive study by Wang and Manning (2012) showed
that a simple model using bigrams and SVMs per-
forms on par with more complex models.
In contrast, we show that adding structural fea-
tures from syntactic trees is particularly useful for
the cross-domain setting. They help to build a sys-
tem that is more robust across domains. Therefore,
rather than trying to build a specialized system
for every new target domain, as it has been done
in most prior work on domain adaptation (Blitzer
et al., 2007; Daum´e, 2007), the domain adapta-
tion problem boils down to finding a more robust
system (Søgaard and Johannsen, 2012; Plank and
Moschitti, 2013). This is in line with recent ad-
vances in parsing the web (Petrov and McDonald,
2012), where participants where asked to build a
single system able to cope with different yet re-
</bodyText>
<page confidence="0.970564">
1253
</page>
<bodyText confidence="0.997710076923077">
lated domains.
Our approach relies on robust syntactic struc-
tures to automatically generate patterns that adapt
better. These representations have been inspired
by the semantic models developed for Ques-
tion Answering (Moschitti, 2008; Severyn and
Moschitti, 2012; Severyn and Moschitti, 2013)
and Semantic Textual Similarity (Severyn et al.,
2013). Moreover, we introduce additional tags,
e.g., video concepts, polarity and negation words,
to achieve better generalization across different
domains where the word distribution and vocab-
ulary changes.
</bodyText>
<sectionHeader confidence="0.990442" genericHeader="method">
3 Representations and models
</sectionHeader>
<bodyText confidence="0.999982857142857">
Our approach to OM on YouTube relies on the
design of classifiers to predict comment type and
opinion polarity. Such classifiers are traditionally
based on bag-of-words and more advanced fea-
tures. In the next sections, we define a baseline
feature vector model and a novel structural model
based on kernel methods.
</bodyText>
<subsectionHeader confidence="0.999441">
3.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.9425418">
We enrich the traditional bag-of-word representa-
tion with features from a sentiment lexicon and
features quantifying the negation present in the
comment. Our model (FVEC) encodes each docu-
ment using the following feature groups:
- word n-grams: we compute unigrams and
bigrams over lower-cased word lemmas where
binary values are used to indicate the pres-
ence/absence of a given item.
- lexicon: a sentiment lexicon is a collection of
words associated with a positive or negative senti-
ment. We use two manually constructed sentiment
lexicons that are freely available: the MPQA Lex-
icon (Wilson et al., 2005) and the lexicon of Hu
and Liu (2004). For each of the lexicons, we use
the number of words found in the comment that
have positive and negative sentiment as a feature.
- negation: the count of negation words, e.g.,
{don’t, never, not, etc.}, found in a comment.2
Our structural representation (defined next) en-
ables a more involved treatment of negation.
- video concept: cosine similarity between a com-
ment and the title/description of the video. Most
of the videos come with a title and a short descrip-
tion, which can be used to encode the topicality of
</bodyText>
<footnote confidence="0.9878555">
2The list of negation words is adopted from
http://sentiment.christopherpotts.net/lingstruc.html
</footnote>
<bodyText confidence="0.878288">
each comment by looking at their overlap.
</bodyText>
<subsectionHeader confidence="0.997612">
3.2 Structural model
</subsectionHeader>
<bodyText confidence="0.999994088888889">
We go beyond traditional feature vectors by em-
ploying structural models (STRUCT), which en-
code each comment into a shallow syntactic tree.
These trees are input to tree kernel functions
for generating structural features. Our struc-
tures are specifically adapted to the noisy user-
generated texts and encode important aspects of
the comments, e.g., words from the sentiment lexi-
cons, product concepts and negation words, which
specifically targets the sentiment and comment
type classification tasks.
In particular, our shallow tree structure is a
two-level syntactic hierarchy built from word lem-
mas (leaves) and part-of-speech tags that are fur-
ther grouped into chunks (Fig. 1). As full syn-
tactic parsers such as constituency or dependency
tree parsers would significantly degrade in perfor-
mance on noisy texts, e.g., Twitter or YouTube
comments, we opted for shallow structures, which
rely on simpler and more robust components: a
part-of-speech tagger and a chunker. Moreover,
such taggers have been recently updated with
models (Ritter et al., 2011; Gimpel et al., 2011)
trained specifically to process noisy texts show-
ing significant reductions in the error rate on user-
generated texts, e.g., Twitter. Hence, we use the
CMU Twitter pos-tagger (Gimpel et al., 2011;
Owoputi et al., 2013) to obtain the part-of-speech
tags. Our second component – chunker – is taken
from (Ritter et al., 2011), which also comes with a
model trained on Twitter data3 and shown to per-
form better on noisy data such as user comments.
To address the specifics of OM tasks on
YouTube comments, we enrich syntactic trees
with semantic tags to encode: (i) central con-
cepts of the video, (ii) sentiment-bearing words
expressing positive or negative sentiment and (iii)
negation words. To automatically identify con-
cept words of the video we use context words (to-
kens detected as nouns by the part-of-speech tag-
ger) from the video title and video description and
match them in the tree. For the matched words,
we enrich labels of their parent nodes (part-of-
speech and chunk) with the PRODUCT tag. Sim-
ilarly, the nodes associated with words found in
</bodyText>
<footnote confidence="0.9724165">
3The chunker from (Ritter et al., 2011) relies on its own
POS tagger, however, in our structural representations we fa-
vor the POS tags from the CMU Twitter tagger and take only
the chunk tags from the chunker.
</footnote>
<page confidence="0.989816">
1254
</page>
<figureCaption confidence="0.992577">
Figure 1: Shallow tree representation of the example comment (labeled with product type and
</figureCaption>
<bodyText confidence="0.998261666666666">
negative sentiment): “iPad 2 is better. the superior apps just destroy the xoom.” (lemmas are replaced
with words for readability) taken from the video “Motorola Xoom Review”. We introduce additional tags
in the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words
(better, superior, destroy) directly in the tree nodes. For the former we add a PRODUCT tag on the chunk
and part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter. Two
sentences are split into separate root nodes S.
the sentiment lexicon are enriched with a polar-
ity tag (either positive or negative), while nega-
tion words are labeled with the NEG tag. It should
be noted that vector-based (FVEC) model relies
only on feature counts whereas the proposed tree
encodes powerful contextual syntactic features in
terms of tree fragments. The latter are automati-
cally generated and learned by SVMs with expres-
sive tree kernels.
For example, the comment in Figure 1 shows
two positive and one negative word from the senti-
ment lexicon. This would strongly bias the FVEC
sentiment classifier to assign a positive label
to the comment. In contrast, the STRUCT model
relies on the fact that the negative word, destroy,
refers to the PRODUCT (xoom) since they form a
verbal phase (VP). In other words, the tree frag-
ment: [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP [PRODUCT-N
[xoom]]]] is a strong feature (induced
by tree kernels) to help the classifier to dis-
criminate such hard cases. Moreover, tree
kernels generate all possible subtrees, thus
producing generalized (back-off) features,
e.g., [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] or [S
[negative-VP [PRODUCT-NP]]]].
</bodyText>
<subsectionHeader confidence="0.998958">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.999915225806452">
We perform OM on YouTube using supervised
methods, e.g., SVM. Our goal is to learn a model
to automatically detect the sentiment and type of
each comment. For this purpose, we build a multi-
class classifier using the one-vs-all scheme. A bi-
nary classifier is trained for each of the classes
and the predicted class is obtained by taking a
class from the classifier with a maximum predic-
tion score. Our back-end binary classifier is SVM-
light-TK4, which encodes structural kernels in the
SVM-light (Joachims, 2002) solver. We define a
novel and efficient tree kernel function, namely,
Shallow syntactic Tree Kernel (SHTK), which is
as expressive as the Partial Tree Kernel (PTK)
(Moschitti, 2006a) to handle feature engineering
over the structural representations of the STRUCT
model. A polynomial kernel of degree 3 is applied
to feature vectors (FVEC).
Combining structural and vector models. A
typical kernel machine, e.g., SVM, classifies a
test input x using the following prediction func-
tion: h(x) = Ei αiyiK(x,xi), where αi are
the model parameters estimated from the training
data, yi are target variables, xi are support vec-
tors, and K(·, ·) is a kernel function. The latter
computes the similarity between two comments.
The STRUCT model treats each comment as a tu-
ple x = (T ,v) composed of a shallow syntactic
tree T and a feature vector v. Hence, for each pair
of comments x1 and x2, we define the following
comment similarity kernel:
</bodyText>
<equation confidence="0.997901">
K(x1,x2) = KTK(T 1,T 2) + Kv(v1,v2), (1)
</equation>
<bodyText confidence="0.9970068">
where KTK computes SHTK (defined next), and
Kv is a kernel over feature vectors, e.g., linear,
polynomial, Gaussian, etc.
Shallow syntactic tree kernel. Following the
convolution kernel framework, we define the new
</bodyText>
<footnote confidence="0.970401">
4http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<page confidence="0.985775">
1255
</page>
<bodyText confidence="0.975027833333333">
SHTK function from Eq. 1 to compute the similar-
ity between tree structures. It counts the number of
common substructures between two trees T1 and
T2 without explicitly considering the whole frag-
ment space. The general equations for Convolu-
tion Tree Kernels is:
</bodyText>
<equation confidence="0.983055">
E
TK(T1, T2) =
n1∈NT1
</equation>
<bodyText confidence="0.999994727272727">
where NT1 and NT2 are the sets of the T1’s and
T2’s nodes, respectively and A(n1, n2) is equal to
the number of common fragments rooted in the n1
and n2 nodes, according to several possible defini-
tion of the atomic fragments.
To improve the speed computation of TK, we
consider pairs of nodes (n1, n2) belonging to the
same tree level. Thus, given H, the height of the
STRUCT trees, where each level h contains nodes
of the same type, i.e., chunk, POS, and lexical
nodes, we define SHTK as the following5:
</bodyText>
<equation confidence="0.772452">
SHTK(T1, T2) =
</equation>
<bodyText confidence="0.998838">
where NhT1 and NhT2 are sets of nodes at height h.
The above equation can be applied with any A
function. To have a more general and expressive
kernel, we use A previously defined for PTK.
More formally: if n1 and n2 are leaves then
</bodyText>
<equation confidence="0.877638666666667">
A(n1, n2) = µλ(n1, n2); else A(n1, n2) =
µ(λ2 +Eλd(�I1)+d(�I2) � |~I1 |�A(cn1(~I1j), cn2(~I2j)) ,
~I1,~I2,|~I1|=|~I2 |j=1
</equation>
<bodyText confidence="0.999950142857143">
where λ, µ E [0, 1] are decay factors; the large
sum is adopted from a definition of the sub-
sequence kernel (Shawe-Taylor and Cristianini,
2004) to generate children subsets with gaps,
which are then used in a recursive call to A. Here,
cn1(i) is the ith child of the node n1; ~I1 and ~I2 are
two sequences of indexes that enumerate subsets
</bodyText>
<equation confidence="0.980947">
I~ = (i1, i2, .., |I|), with
1 ≤ i1 &lt; i2 &lt; .. &lt; i|I|; and d(~I1) = ~I1l(~I1) − ~I11 + 1
</equation>
<bodyText confidence="0.984241166666667">
and d(~I2) = ~I2l(~I2) − ~I21 + 1, which penalizes
subsequences with larger gaps.
It should be noted that: firstly, the use of a
subsequence kernel makes it possible to generate
child subsets of the two nodes, i.e., it allows for
gaps, which makes matching of syntactic patterns
</bodyText>
<footnote confidence="0.660409">
5To have a similarity score between 0 and 1, a normaliza-
tion in the kernel space, i.e. SHTK(T1,T2) is
</footnote>
<equation confidence="0.911736">
√SHT K (T1,T1) × SHT K (T2 ,T2 )
applied.
</equation>
<bodyText confidence="0.989238590909091">
less rigid. Secondly, the resulting SHTK is essen-
tially a special case of PTK (Moschitti, 2006a),
adapted to the shallow structural representation
STRUCT (see Sec. 3.2). When applied to STRUCT
trees, SHTK exactly computes the same feature
space as PTK, but in faster time (on average). In-
deed, SHTK required to be only applied to node
pairs from the same level (see Eq. 3), where the
node labels can match – chunk, POS or lexicals.
This reduces the time for selecting the matching-
node pairs carried out in PTK (Moschitti, 2006a;
Moschitti, 2006b). The fragment space is obvi-
ously the same, as the node labels of different
levels in STRUCT are different and will not be
matched by PTK either.
Finally, given its recursive definition in Eq. 3
and the use of subsequence (with gaps), SHTK can
derive useful dependencies between its elements.
For example, it will generate the following subtree
fragments: [positive-NP [positive-A
N]], [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] and so on.
</bodyText>
<sectionHeader confidence="0.985306" genericHeader="method">
4 YouTube comments corpus
</sectionHeader>
<bodyText confidence="0.998130230769231">
To build a corpus of YouTube comments, we fo-
cus on a particular set of videos (technical reviews
and advertisings) featuring commercial products.
In particular, we chose two product categories:
automobiles (AUTO) and tablets (TABLETS). To
collect the videos, we compiled a list of prod-
ucts and queried the YouTube gData API6 to re-
trieve the videos. We then manually excluded
irrelevant videos. For each video, we extracted
all available comments (limited to maximum 1k
comments per video) and manually annotated each
comment with its type and polarity. We distin-
guish between the following types:
product: discuss the topic product in general or
some features of the product;
video: discuss the video or some of its details;
spam: provide advertising and malicious links; and
off-topic: comments that have almost no content
(“lmao”) or content that is not related to the video
(“Thank you!”).
Regarding the polarity, we distinguish between
{positive, negative, neutral} sentiments with re-
spect to the product and the video. If the comment
contains several statements of different polarities,
it is annotated as both positive and negative: “Love
the video but waiting for iPad 4”. In total we have
</bodyText>
<footnote confidence="0.607727">
6https://developers.google.com/youtube/v3/
</footnote>
<table confidence="0.4396976">
E A(n1, n2), (2)
n2∈NT2
EH E E A(n1, n2), (3)
h=1 n1∈NhT1 n2∈NhT2
of children with gaps, i.e.,
</table>
<page confidence="0.900116">
1256
</page>
<bodyText confidence="0.996626923076923">
annotated 208 videos with around 35k comments
(128 videos TABLETS and 80 for AUTO).
To evaluate the quality of the produced labels,
we asked 5 annotators to label a sample set of one
hundred comments and measured the agreement.
The resulting annotator agreement α value (Krip-
pendorf, 2004; Artstein and Poesio, 2008) scores
are 60.6 (AUTO), 72.1 (TABLETS) for the senti-
ment task and 64.1 (AUTO), 79.3 (TABLETS) for
the type classification task. For the rest of the
comments, we assigned the entire annotation task
to a single coder. Further details on the corpus can
be found in Uryupina et al. (2014).
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99993375">
This section reports: (i) experiments on individ-
ual subtasks of opinion and type classification; (ii)
the full task of predicting type and sentiment; (iii)
study on the adaptability of our system by learn-
ing on one domain and testing on the other; (iv)
learning curves that provide an indication on the
required amount and type of data and the scalabil-
ity to other domains.
</bodyText>
<subsectionHeader confidence="0.988902">
5.1 Task description
</subsectionHeader>
<bodyText confidence="0.999867538461538">
Sentiment classification. We treat each com-
ment as expressing positive, negative or
neutral sentiment. Hence, the task is a three-
way classification.
Type classification. One of the challenging as-
pects of sentiment analysis of YouTube data is that
the comments may express the sentiment not only
towards the product shown in the video, but
also the video itself, i.e., users may post posi-
tive comments to the video while being generally
negative about the product and vice versa. Hence,
it is of crucial importance to distinguish between
these two types of comments. Additionally, many
comments are irrelevant for both the product and
the video (off-topic) or may even contain
spam. Given that the main goal of sentiment
analysis is to select sentiment-bearing comments
and identify their polarity, distinguishing between
off-topic and spam categories is not critical.
Thus, we merge the spam and off-topic into
a single uninformative category. Similar to
the opinion classification task, comment type clas-
sification is a multi-class classification with three
classes: video, product and uninform.
Full task. While the previously discussed sen-
timent and type identification tasks are useful to
</bodyText>
<table confidence="0.999819">
Task class AUTO TABLETS
TRAIN TEST TRAIN TEST
positive 2005 (36%) 807 (27%) 2393 (27%) 1872 (27%)
neutral 2649 (48%) 1413 (47%) 4683 (53%) 3617 (52%)
Sentiment negative 878 (16%) 760 (26%) 1698 (19%) 1471 (21%)
total 5532 2980 8774 6960
product 2733 (33%) 1761 (34%) 7180 (59%) 5731 (61%)
video 3008 (36%) 1369 (26%) 2088 (17%) 1674 (18%)
Type off-topic 2638 (31%) 2045 (39%) 2334 (19%) 1606 (17%)
spam 26 (&gt;1%) 17 (&gt;1%) 658 (5%) 361 (4%)
total 8405 5192 12260 9372
product-pos. 1096 (13%) 517 (10%) 1648 (14%) 1278 (14%)
product-neu. 908 (11%) 729 (14%) 3681 (31%) 2844 (32%)
Full product-neg. 554 (7%) 370 (7%) 1404 (12%) 1209 (14%)
video-pos. 909 (11%) 290 (6%) 745 (6%) 594 (7%)
video-neu. 1741 (21%) 683 (14%) 1002 (9%) 773 (9%)
video-neg. 324 (4%) 390 (8%) 294 (2%) 262 (3%)
off-topic 2638 (32%) 2045 (41%) 2334 (20%) 1606 (18%)
spam 26 (&gt;1%) 17 (&gt;1%) 658 (6%) 361 (4%)
total 8196 5041 11766 8927
</table>
<tableCaption confidence="0.99883">
Table 1: Summary of YouTube comments data
</tableCaption>
<bodyText confidence="0.957957470588235">
used in the sentiment, type and full classification
tasks. The comments come from two product cate-
gories: AUTO and TABLETS. Numbers in paren-
thesis show proportion w.r.t. to the total number of
comments used in a task.
model and study in their own right, our end goal is:
given a stream of comments, to jointly predict both
the type and the sentiment of each comment. We
cast this problem as a single multi-class classifica-
tion task with seven classes: the Cartesian product
between {product, video} type labels and
{positive, neutral, negative} senti-
ment labels plus the uninformative category
(spam and off-topic). Considering a real-life ap-
plication, it is important not only to detect the po-
larity of the comment, but to also identify if it is
expressed towards the product or the video.7
</bodyText>
<subsectionHeader confidence="0.990503">
5.2 Data
</subsectionHeader>
<bodyText confidence="0.969392631578947">
We split all the videos 50% between training
set (TRAIN) and test set (TEST), where each
video contains all its comments. This ensures
that all comments from the same video appear
either in TRAIN or in TEST. Since the number
of comments per video varies, the resulting sizes
of each set are different (we use the larger split
for TRAIN). Table 1 shows the data distribution
across the task-specific classes – sentiment and
type classification. For the sentiment task we ex-
clude off-topic and spam comments as well
as comments with ambiguous sentiment, i.e., an-
7We exclude comments annotated as both video and
product. This enables the use of a simple flat multi-
classifiers with seven categories for the full task, instead of
a hierarchical multi-label classifiers (i.e., type classification
first and then opinion polarity). The number of comments as-
signed to both product and video is relatively small (8%
for TABLETS and 4% for AUTO).
</bodyText>
<page confidence="0.985615">
1257
</page>
<bodyText confidence="0.999982875">
notated as both positive and negative.
For the sentiment task about 50% of the
comments have neutral polarity, while the
negative class is much less frequent. Inter-
estingly, the ratios between polarities expressed
in comments from AUTO and TABLETS are very
similar across both TRAIN and TEST. Conversely,
for the type task, we observe that comments from
AUTO are uniformly distributed among the three
classes, while for the TABLETS the majority of
comments are product related. It is likely due
to the nature of the TABLETS videos, that are
more geek-oriented, where users are more prone
to share their opinions and enter involved discus-
sions about a product. Additionally, videos from
the AUTO category (both commercials and user
reviews) are more visually captivating and, be-
ing generally oriented towards a larger audience,
generate more video-related comments. Regard-
ing the full setting, where the goal is to have
a joint prediction of the comment sentiment and
type, we observe that video-negative and
video-positive are the most scarce classes,
which makes them the most difficult to predict.
</bodyText>
<subsectionHeader confidence="0.68748">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999988555555556">
We start off by presenting the results for the tradi-
tional in-domain setting, where both TRAIN and
TEST come from the same domain, e.g., AUTO or
TABLETS. Next, we show the learning curves to
analyze the behavior of FVEC and STRUCT mod-
els according to the training size. Finally, we per-
form a set of cross-domain experiments that de-
scribe the enhanced adaptability of the patterns
generated by the STRUCT model.
</bodyText>
<subsectionHeader confidence="0.55697">
5.3.1 In-domain experiments
</subsectionHeader>
<bodyText confidence="0.999988219512195">
We compare FVEC and STRUCT models on three
tasks described in Sec. 5.1: sentiment, type and
full. Table 2 reports the per-class performance
and the overall accuracy of the multi-class clas-
sifier. Firstly, we note that the performance on
TABLETS is much higher than on AUTO across
all tasks. This can be explained by the follow-
ing: (i) TABLETS contains more training data and
(ii) videos from AUTO and TABLETS categories
draw different types of audiences – well-informed
users and geeks expressing better-motivated opin-
ions about a product for the former vs. more gen-
eral audience for the latter. This results in the
different quality of comments with the AUTO be-
ing more challenging to analyze. Secondly, we
observe that the STRUCT model provides 1-3%
of absolute improvement in accuracy over FVEC
for every task. For individual categories the F1
scores are also improved by the STRUCT model
(except for the negative classes for AUTO, where
we see a small drop). We conjecture that sentiment
prediction for AUTO category is largely driven
by one-shot phrases and statements where it is
hard to improve upon the bag-of-words and senti-
ment lexicon features. In contrast, comments from
TABLETS category tend to be more elaborated
and well-argumented, thus, benefiting from the ex-
pressiveness of the structural representations.
Considering per-class performance, correctly
predicting negative sentiment is most difficult
for both AUTO and TABLETS, which is proba-
bly caused by the smaller proportion of the neg-
ative comments in the training set. For the type
task, video-related class is substantially more dif-
ficult than product-related for both categories. For
the full task, the class video-negative ac-
counts for the largest error. This is confirmed by
the results from the previous sentiment and type
tasks, where we saw that handling negative sen-
timent and detecting video-related comments are
most difficult.
</bodyText>
<subsectionHeader confidence="0.986896">
5.3.2 Learning curves
</subsectionHeader>
<bodyText confidence="0.999982666666667">
The learning curves depict the behavior of FVEC
and STRUCT models as we increase the size of
the training set. Intuitively, the STRUCT model
relies on more general syntactic patterns and may
overcome the sparseness problems incurred by the
FVEC model when little training data is available.
Nevertheless, as we see in Figure 2, the learning
curves for sentiment and type classification tasks
across both product categories do not confirm this
intuition. The STRUCT model consistently outper-
forms the FVEC across all training sizes, but the
gap in the performance does not increase when we
move to smaller training sets. As we will see next,
this picture changes when we perform the cross-
domain study.
</bodyText>
<subsectionHeader confidence="0.751531">
5.3.3 Cross-domain experiments
</subsectionHeader>
<bodyText confidence="0.999249571428572">
To understand the performance of our classifiers
on other YouTube domains, we perform a set of
cross-domain experiments by training on the data
from one product category and testing on the other.
Table 3 reports the accuracy for three tasks
when we use all comments (TRAIN + TEST) from
AUTO to predict on the TEST from TABLETS
</bodyText>
<page confidence="0.961949">
1258
</page>
<table confidence="0.9999297">
Task class AUTO TABLETS
P FVEC F1 STRUCT P FVEC F1 STRUCT
R P R F1 R P R F1
positive 49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3
neutral 68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8
Sent
negative 42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5
Acc 54.7 55.7 68.6 70.5
product 66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6
video 45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4
Type uninform 59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0
Acc 57.4 59.4 77.2 78.6
product-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4
product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0
product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4
Full video-pos 23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4
video-neu 26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3
video-neg 21.9 3.7 6.0 17.7 2.3 4.8 39.0 17.5 23.9 39.5 6.1 11.5
uninform 56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9
Acc 40.0 41.5 57.6 60.3
</table>
<tableCaption confidence="0.907855">
Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The
results are reported for sentiment, type and full classification tasks. The metrics used are precision (P),
recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).
</tableCaption>
<figure confidence="0.95885125">
training size
(a) Sentiment classification
training size
(b) Type classification
</figure>
<figureCaption confidence="0.942997">
Figure 2: In-domain learning curves. ALL refers
to the entire TRAIN set for a given product cate-
gory, i.e., AUTO and TABLETS (see Table 1)
</figureCaption>
<bodyText confidence="0.977954333333333">
and in the opposite direction (TABLETS→AUTO).
When using AUTO as a source domain, STRUCT
model provides additional 1-3% of absolute im-
</bodyText>
<table confidence="0.998175428571429">
Source Target Task FVEC STRUCT
Sent 66.1 66.6
AUTO TABLETS Type 59.9 64.1†
Full 35.6 38.3†
Sent 60.4 61.9†
TABLETS AUTO Type 54.2 55.6†
Full 43.4 44.7†
</table>
<tableCaption confidence="0.999811">
Table 3: Cross-domain experiment. Ac-
</tableCaption>
<bodyText confidence="0.9807224">
curacy using FVEC and STRUCT models
when trained/tested in both directions, i.e.
AUTO→TABLETS and TABLETS→AUTO. † de-
notes results statistically significant at 95% level
(via pairwise t-test).
provement, except for the sentiment task.
Similar to the in-domain experiments, we stud-
ied the effect of the source domain size on the tar-
get test performance. This is useful to assess the
adaptability of features exploited by the FVEC and
STRUCT models with the change in the number
of labeled examples available for training. Addi-
tionally, we considered a setting including a small
amount of training data from the target data (i.e.,
supervised domain adaptation).
For this purpose, we drew the learning curves of
the FVEC and STRUCT models applied to the sen-
timent and type tasks (Figure 3): AUTO is used
as the source domain to train models, which are
tested on TABLETS.8 The plot shows that when
</bodyText>
<footnote confidence="0.963565">
8The results for the other direction (TABLETS→AUTO)
show similar behavior.
</footnote>
<figure confidence="0.986536694444444">
1k 2k 3k 4k 5k ALL
Accuracy
70
65
60
55
AUTOSTRUCT
AUTOFVEC
TABLETSSTRUCT
TABLETSFVEC
1k 2k 3k 4k 5k ALL
Accuracy
80
75
70
65
60
55
50
45
40
AUTOSTRUCT
AUTOFVEC
TABLETSSTRUCT
TABLETSFVEC
1259
70
65
60
55
50
45
40
35
30
(b) Type classification
</figure>
<figureCaption confidence="0.71221425">
Figure 3: Learning curves for the cross-domain
setting (AUTO→TABLETS). Shaded area refers to
adding a small portion of comments from the same
domain as the target test data to the training.
</figureCaption>
<bodyText confidence="0.999832416666667">
little training data is available, the features gener-
ated by the STRUCT model exhibit better adapt-
ability (up to 10% of improvement over FVEC).
The bag-of-words model seems to be affected by
the data sparsity problem which becomes a crucial
issue when only a small training set is available.
This difference becomes smaller as we add data
from the same domain. This is an important ad-
vantage of our structural approach, since we can-
not realistically expect to obtain manual annota-
tions for 10k+ comments for each (of many thou-
sands) product domains present on YouTube.
</bodyText>
<subsectionHeader confidence="0.726035">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999977363636364">
Our STRUCT model is more accurate since it is
able to induce structural patterns of sentiment.
Consider the following comment: optimus pad
is better. this xoom is just to bulky but optimus
pad offers better functionality. The FVEC bag-
of-words model misclassifies it to be positive,
since it contains two positive expressions (better,
better functionality) that outweigh a single nega-
tive expression (bulky). The structural model, in
contrast, is able to identify the product of interest
(xoom) and associate it with the negative expres-
sion through a structural feature and thus correctly
classify the comment as negative.
Some issues remain problematic even for the
structural model. The largest group of errors are
implicit sentiments. Thus, some comments do not
contain any explicit positive or negative opinions,
but provide detailed and well-argumented criti-
cism, for example, this phone is heavy. Such com-
ments might also include irony. To account for
these cases, a deep understanding of the product
domain is necessary.
</bodyText>
<sectionHeader confidence="0.999076" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99999568">
We carried out a systematic study on OM from
YouTube comments by training a set of su-
pervised multi-class classifiers distinguishing be-
tween video and product related opinions. We
use standard feature vectors augmented by shallow
syntactic trees enriched with additional conceptual
information.
This paper makes several contributions: (i) it
shows that effective OM can be carried out with
supervised models trained on high quality annota-
tions; (ii) it introduces a novel annotated corpus
of YouTube comments, which we make available
for the research community; (iii) it defines novel
structural models and kernels, which can improve
on feature vectors, e.g., up to 30% of relative im-
provement in type classification, when little data
is available, and demonstrates that the structural
model scales well to other domains.
In the future, we plan to work on a joint model
to classify all the comments of a given video, s.t. it
is possible to exploit latent dependencies between
entities and the sentiments of the comment thread.
Additionally, we plan to experiment with hierar-
chical multi-label classifiers for the full task (in
place of a flat multi-class learner).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995748">
The authors are supported by a Google Fac-
ulty Award 2011, the Google Europe Fellowship
Award 2013 and the European Community’s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grant #288024: LIMOSINE.
</bodyText>
<figure confidence="0.99750232">
training size (ALL)
Accuracy
68
67
66
65
64
63
62
1k 2k 3k 4k 5k 8.5k
Source +Target
STRUCT
FVEC
100 500 1k
Accuracy
(a) Sentiment classification
1k 2k 3k 4k 5k 8.5k
Source +Target
training size
(TRAIN)
STRUCT
FVEC
13k
(ALL)
100 500 1k
</figure>
<page confidence="0.940086">
1260
</page>
<sectionHeader confidence="0.990661" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945791666667">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596, December.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources? In
IJCNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Hal Daum´e, III. 2007. Frustratingly easy domain
adaptation. ACL.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Alessandro Moschitti. 2006a. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML.
Alessandro Moschitti. 2006b. Making tree kernels
practical for natural language learning. In EACL,
pages 113–120.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In EMNLP.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual simi-
larity with structural representations. In ACL.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,
and Jose San Pedro. 2010. How useful are your
comments?: Analyzing and predicting YouTube
comments and comment ratings. In WWW.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP.
Anders Søgaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In ACL.
Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
SenTube: A corpus for sentiment analysis on
YouTube social media. In LREC.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
</reference>
<page confidence="0.99143">
1261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488173">
<title confidence="0.999466">Opinion Mining on YouTube</title>
<author confidence="0.995731">Alessandro</author>
<affiliation confidence="0.9999365">University of Trento, - University of Computing Research Institute,</affiliation>
<email confidence="0.7507805">severyn@disi.unitn.it,uryupina@gmail.com,bplank@cst.dk,katjaf@google.com</email>
<abstract confidence="0.99838175">This paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing robust shallow syntactic structures for improving model adaptability. We rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words. An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="21560" citStr="Artstein and Poesio, 2008" startWordPosition="3445" endWordPosition="3448"> comment contains several statements of different polarities, it is annotated as both positive and negative: “Love the video but waiting for iPad 4”. In total we have 6https://developers.google.com/youtube/v3/ E A(n1, n2), (2) n2∈NT2 EH E E A(n1, n2), (3) h=1 n1∈NhT1 n2∈NhT2 of children with gaps, i.e., 1256 annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO). To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in Uryupina et al. (2014). 5 Experiments This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves that provide an indication on </context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Paul Cook</author>
<author>Marco Lui</author>
<author>Andrew MacKinlay</author>
<author>Li Wang</author>
</authors>
<title>How noisy social media text, how diffrnt social media sources?</title>
<date>2013</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="1610" citStr="Baldwin et al., 2013" startWordPosition="227" endWordPosition="230">ouTube contain rapidly changing information generated by millions of users that can dramatically affect the reputation of a person or an organization. This raises the importance of automatic extraction of sentiments and opinions expressed in social media. YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with a social graph, and discussions between people sharing an interest. Hence, doing sentiment research in such an environment is highly relevant for the community. While the linguistic conventions used on Twitter and YouTube indeed show similarities (Baldwin et al., 2013), focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research. Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by the classification system of YouTube comments proposed by Siersdorfer et al. (2010). While previous state-of-the-art models for opinion classification have been successfully applied to traditional corpora (Pang and Lee, 2008), YouTube comments pose additional ch</context>
</contexts>
<marker>Baldwin, Cook, Lui, MacKinlay, Wang, 2013</marker>
<rawString>Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text, how diffrnt social media sources? In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6125" citStr="Blitzer et al., 2007" startWordPosition="931" endWordPosition="934">ls generally improve on both tasks – polarity and type classification – yielding up to 30% of relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 2 Related work Most prior work on more general OM has been carried out on more standardized forms of text, such as consumer reviews or newswire. The most commonly used datasets include: the MPQA corpus of news documents (Wilson et al., 2005), web customer review data (Hu and Liu, 2004), Amazon review data (Blitzer et al., 2007), the JDPA 1The corpus and the annotation guidelines are publicly available at: http://projects.disi.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suit</context>
<context position="8373" citStr="Blitzer et al., 2007" startWordPosition="1284" endWordPosition="1287">des (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn </context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<publisher>ACL.</publisher>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e, III. 2007. Frustratingly easy domain adaptation. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: annotation, features, and experiments. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="6082" citStr="Hu and Liu, 2004" startWordPosition="923" endWordPosition="926">al information is used. Structural models generally improve on both tasks – polarity and type classification – yielding up to 30% of relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 2 Related work Most prior work on more general OM has been carried out on more standardized forms of text, such as consumer reviews or newswire. The most commonly used datasets include: the MPQA corpus of news documents (Wilson et al., 2005), web customer review data (Hu and Liu, 2004), Amazon review data (Blitzer et al., 2007), the JDPA 1The corpus and the annotation guidelines are publicly available at: http://projects.disi.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistenc</context>
<context position="10255" citStr="Hu and Liu (2004)" startWordPosition="1579" endWordPosition="1582">ag-of-word representation with features from a sentiment lexicon and features quantifying the negation present in the comment. Our model (FVEC) encodes each document using the following feature groups: - word n-grams: we compute unigrams and bigrams over lower-cased word lemmas where binary values are used to indicate the presence/absence of a given item. - lexicon: a sentiment lexicon is a collection of words associated with a positive or negative sentiment. We use two manually constructed sentiment lexicons that are freely available: the MPQA Lexicon (Wilson et al., 2005) and the lexicon of Hu and Liu (2004). For each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature. - negation: the count of negation words, e.g., {don’t, never, not, etc.}, found in a comment.2 Our structural representation (defined next) enables a more involved treatment of negation. - video concept: cosine similarity between a comment and the title/description of the video. Most of the videos come with a title and a short description, which can be used to encode the topicality of 2The list of negation words is adopted from http://sentiment.christopherpotts.ne</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="15627" citStr="Joachims, 2002" startWordPosition="2448" endWordPosition="2449">[S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]] or [S [negative-VP [PRODUCT-NP]]]]. 3.3 Learning We perform OM on YouTube using supervised methods, e.g., SVM. Our goal is to learn a model to automatically detect the sentiment and type of each comment. For this purpose, we build a multiclass classifier using the one-vs-all scheme. A binary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. Our back-end binary classifier is SVMlight-TK4, which encodes structural kernels in the SVM-light (Joachims, 2002) solver. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK) (Moschitti, 2006a) to handle feature engineering over the structural representations of the STRUCT model. A polynomial kernel of degree 3 is applied to feature vectors (FVEC). Combining structural and vector models. A typical kernel machine, e.g., SVM, classifies a test input x using the following prediction function: h(x) = Ei αiyiK(x,xi), where αi are the model parameters estimated from the training data, yi are target variables,</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Kessler</author>
<author>Miriam Eckert</author>
<author>Lyndsie Clark</author>
<author>Nicolas Nicolov</author>
</authors>
<title>ICWSM JDPA sentiment corpus for the automotive domain.</title>
<date>2010</date>
<booktitle>In ICWSM-DWC.</booktitle>
<publisher>The</publisher>
<contexts>
<context position="6301" citStr="Kessler et al., 2010" startWordPosition="953" endWordPosition="956">f annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 2 Related work Most prior work on more general OM has been carried out on more standardized forms of text, such as consumer reviews or newswire. The most commonly used datasets include: the MPQA corpus of news documents (Wilson et al., 2005), web customer review data (Hu and Liu, 2004), Amazon review data (Blitzer et al., 2007), the JDPA 1The corpus and the annotation guidelines are publicly available at: http://projects.disi.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube. A recent study focuses on sentiment analysis for Twitter (Pak and Paroubek, 2010), however, their corpus was compiled automatically by s</context>
</contexts>
<marker>Kessler, Eckert, Clark, Nicolov, 2010</marker>
<rawString>Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and Nicolas Nicolov. 2010. The 2010 ICWSM JDPA sentiment corpus for the automotive domain. In ICWSM-DWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorf</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology,</title>
<date>2004</date>
<location>Thousand Oaks, CA.</location>
<note>second edition, chapter 11. Sage,</note>
<contexts>
<context position="21532" citStr="Krippendorf, 2004" startWordPosition="3442" endWordPosition="3444">d the video. If the comment contains several statements of different polarities, it is annotated as both positive and negative: “Love the video but waiting for iPad 4”. In total we have 6https://developers.google.com/youtube/v3/ E A(n1, n2), (2) n2∈NT2 EH E E A(n1, n2), (3) h=1 n1∈NhT1 n2∈NhT2 of children with gaps, i.e., 1256 annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO). To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in Uryupina et al. (2014). 5 Experiments This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves th</context>
</contexts>
<marker>Krippendorf, 2004</marker>
<rawString>Klaus Krippendorf, 2004. Content Analysis: An Introduction to Its Methodology, second edition, chapter 11. Sage, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="5279" citStr="Moschitti, 2006" startWordPosition="791" endWordPosition="793">ch give the possibility to study the domain adaptability of the supervised models by training on one category and testing on the other (and vice versa). The third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g., iPad, Kindle, Toyota Camry. Given the complexity and the novelty of the task, we exploit structural kernels to automatically engineer novel features. In particular, we define an efficient tree kernel derived from the Partial Tree Kernel, (Moschitti, 2006a), suitable for encoding structural representation of comments into Support Vector Machines (SVMs). Finally, our results show that our models are adaptable, especially when the structural information is used. Structural models generally improve on both tasks – polarity and type classification – yielding up to 30% of relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 2 Related work Most prior work on more general OM has been carried out on more stan</context>
<context position="15808" citStr="Moschitti, 2006" startWordPosition="2476" endWordPosition="2477">learn a model to automatically detect the sentiment and type of each comment. For this purpose, we build a multiclass classifier using the one-vs-all scheme. A binary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. Our back-end binary classifier is SVMlight-TK4, which encodes structural kernels in the SVM-light (Joachims, 2002) solver. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK) (Moschitti, 2006a) to handle feature engineering over the structural representations of the STRUCT model. A polynomial kernel of degree 3 is applied to feature vectors (FVEC). Combining structural and vector models. A typical kernel machine, e.g., SVM, classifies a test input x using the following prediction function: h(x) = Ei αiyiK(x,xi), where αi are the model parameters estimated from the training data, yi are target variables, xi are support vectors, and K(·, ·) is a kernel function. The latter computes the similarity between two comments. The STRUCT model treats each comment as a tuple x = (T ,v) compos</context>
<context position="18974" citStr="Moschitti, 2006" startWordPosition="3032" endWordPosition="3033">e subsets I~ = (i1, i2, .., |I|), with 1 ≤ i1 &lt; i2 &lt; .. &lt; i|I|; and d(~I1) = ~I1l(~I1) − ~I11 + 1 and d(~I2) = ~I2l(~I2) − ~I21 + 1, which penalizes subsequences with larger gaps. It should be noted that: firstly, the use of a subsequence kernel makes it possible to generate child subsets of the two nodes, i.e., it allows for gaps, which makes matching of syntactic patterns 5To have a similarity score between 0 and 1, a normalization in the kernel space, i.e. SHTK(T1,T2) is √SHT K (T1,T1) × SHT K (T2 ,T2 ) applied. less rigid. Secondly, the resulting SHTK is essentially a special case of PTK (Moschitti, 2006a), adapted to the shallow structural representation STRUCT (see Sec. 3.2). When applied to STRUCT trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average). Indeed, SHTK required to be only applied to node pairs from the same level (see Eq. 3), where the node labels can match – chunk, POS or lexicals. This reduces the time for selecting the matchingnode pairs carried out in PTK (Moschitti, 2006a; Moschitti, 2006b). The fragment space is obviously the same, as the node labels of different levels in STRUCT are different and will not be matched by PTK either. Fi</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006a. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="5279" citStr="Moschitti, 2006" startWordPosition="791" endWordPosition="793">ch give the possibility to study the domain adaptability of the supervised models by training on one category and testing on the other (and vice versa). The third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g., iPad, Kindle, Toyota Camry. Given the complexity and the novelty of the task, we exploit structural kernels to automatically engineer novel features. In particular, we define an efficient tree kernel derived from the Partial Tree Kernel, (Moschitti, 2006a), suitable for encoding structural representation of comments into Support Vector Machines (SVMs). Finally, our results show that our models are adaptable, especially when the structural information is used. Structural models generally improve on both tasks – polarity and type classification – yielding up to 30% of relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 2 Related work Most prior work on more general OM has been carried out on more stan</context>
<context position="15808" citStr="Moschitti, 2006" startWordPosition="2476" endWordPosition="2477">learn a model to automatically detect the sentiment and type of each comment. For this purpose, we build a multiclass classifier using the one-vs-all scheme. A binary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. Our back-end binary classifier is SVMlight-TK4, which encodes structural kernels in the SVM-light (Joachims, 2002) solver. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK) (Moschitti, 2006a) to handle feature engineering over the structural representations of the STRUCT model. A polynomial kernel of degree 3 is applied to feature vectors (FVEC). Combining structural and vector models. A typical kernel machine, e.g., SVM, classifies a test input x using the following prediction function: h(x) = Ei αiyiK(x,xi), where αi are the model parameters estimated from the training data, yi are target variables, xi are support vectors, and K(·, ·) is a kernel function. The latter computes the similarity between two comments. The STRUCT model treats each comment as a tuple x = (T ,v) compos</context>
<context position="18974" citStr="Moschitti, 2006" startWordPosition="3032" endWordPosition="3033">e subsets I~ = (i1, i2, .., |I|), with 1 ≤ i1 &lt; i2 &lt; .. &lt; i|I|; and d(~I1) = ~I1l(~I1) − ~I11 + 1 and d(~I2) = ~I2l(~I2) − ~I21 + 1, which penalizes subsequences with larger gaps. It should be noted that: firstly, the use of a subsequence kernel makes it possible to generate child subsets of the two nodes, i.e., it allows for gaps, which makes matching of syntactic patterns 5To have a similarity score between 0 and 1, a normalization in the kernel space, i.e. SHTK(T1,T2) is √SHT K (T1,T1) × SHT K (T2 ,T2 ) applied. less rigid. Secondly, the resulting SHTK is essentially a special case of PTK (Moschitti, 2006a), adapted to the shallow structural representation STRUCT (see Sec. 3.2). When applied to STRUCT trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average). Indeed, SHTK required to be only applied to node pairs from the same level (see Eq. 3), where the node labels can match – chunk, POS or lexicals. This reduces the time for selecting the matchingnode pairs carried out in PTK (Moschitti, 2006a; Moschitti, 2006b). The fragment space is obviously the same, as the node labels of different levels in STRUCT are different and will not be matched by PTK either. Fi</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006b. Making tree kernels practical for natural language learning. In EACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="8934" citStr="Moschitti, 2008" startWordPosition="1375" endWordPosition="1376"> prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="12225" citStr="Owoputi et al., 2013" startWordPosition="1889" endWordPosition="1892">(Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models (Ritter et al., 2011; Gimpel et al., 2011) trained specifically to process noisy texts showing significant reductions in the error rate on usergenerated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) to obtain the part-of-speech tags. Our second component – chunker – is taken from (Ritter et al., 2011), which also comes with a model trained on Twitter data3 and shown to perform better on noisy data such as user comments. To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts of the video, (ii) sentiment-bearing words expressing positive or negative sentiment and (iii) negation words. To automatically identify concept words of the video we use context words (tokens detected as nouns by the part-of-speech tagger</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="6846" citStr="Pak and Paroubek, 2010" startWordPosition="1034" endWordPosition="1037">si.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube. A recent study focuses on sentiment analysis for Twitter (Pak and Paroubek, 2010), however, their corpus was compiled automatically by searching for emoticons expressing positive and negative sentiment only. Siersdorfer et al. (2010) focus on exploiting user ratings (counts of ‘thumbs up/down’ as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Hence, their goal is different: predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we b</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="2173" citStr="Pang and Lee, 2008" startWordPosition="310" endWordPosition="313"> YouTube indeed show similarities (Baldwin et al., 2013), focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research. Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by the classification system of YouTube comments proposed by Siersdorfer et al. (2010). While previous state-of-the-art models for opinion classification have been successfully applied to traditional corpora (Pang and Lee, 2008), YouTube comments pose additional challenges: (i) polarity words can refer to either video or product while expressing contrasting sentiments; (ii) many comments are unrelated or contain spam; and (iii) learning supervised models requires training data for each different YouTube domain, e.g., tablets, automobiles, etc. For example, consider a typical comment on a YouTube review video about a Motorola Xoom tablet: this guy really puts a negative spin on this, and I ’m not sure why, this seems crazy fast , and I ’m not entirely sure why his pinch to zoom his laggy all the other xoom reviews The</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="8604" citStr="Petrov and McDonald, 2012" startWordPosition="1323" endWordPosition="1326"> on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8519" citStr="Plank and Moschitti, 2013" startWordPosition="1307" endWordPosition="1310">by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarit</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11986" citStr="Ritter et al., 2011" startWordPosition="1850" endWordPosition="1853">cally targets the sentiment and comment type classification tasks. In particular, our shallow tree structure is a two-level syntactic hierarchy built from word lemmas (leaves) and part-of-speech tags that are further grouped into chunks (Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models (Ritter et al., 2011; Gimpel et al., 2011) trained specifically to process noisy texts showing significant reductions in the error rate on usergenerated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) to obtain the part-of-speech tags. Our second component – chunker – is taken from (Ritter et al., 2011), which also comes with a model trained on Twitter data3 and shown to perform better on noisy data such as user comments. To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts </context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: an experimental study. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="8963" citStr="Severyn and Moschitti, 2012" startWordPosition="1377" endWordPosition="1380">main adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic feature engineering for answer selection and extraction.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8993" citStr="Severyn and Moschitti, 2013" startWordPosition="1381" endWordPosition="1384">l., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods</context>
</contexts>
<marker>Severyn, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic feature engineering for answer selection and extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning semantic textual similarity with structural representations.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9048" citStr="Severyn et al., 2013" startWordPosition="1389" endWordPosition="1392">own to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods. 3.1 Feature Set We enrich the traditional bag-of-word</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013. Learning semantic textual similarity with structural representations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18172" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2876" endWordPosition="2879">ere each level h contains nodes of the same type, i.e., chunk, POS, and lexical nodes, we define SHTK as the following5: SHTK(T1, T2) = where NhT1 and NhT2 are sets of nodes at height h. The above equation can be applied with any A function. To have a more general and expressive kernel, we use A previously defined for PTK. More formally: if n1 and n2 are leaves then A(n1, n2) = µλ(n1, n2); else A(n1, n2) = µ(λ2 +Eλd(�I1)+d(�I2) � |~I1 |�A(cn1(~I1j), cn2(~I2j)) , ~I1,~I2,|~I1|=|~I2 |j=1 where λ, µ E [0, 1] are decay factors; the large sum is adopted from a definition of the subsequence kernel (Shawe-Taylor and Cristianini, 2004) to generate children subsets with gaps, which are then used in a recursive call to A. Here, cn1(i) is the ith child of the node n1; ~I1 and ~I2 are two sequences of indexes that enumerate subsets I~ = (i1, i2, .., |I|), with 1 ≤ i1 &lt; i2 &lt; .. &lt; i|I|; and d(~I1) = ~I1l(~I1) − ~I11 + 1 and d(~I2) = ~I2l(~I2) − ~I21 + 1, which penalizes subsequences with larger gaps. It should be noted that: firstly, the use of a subsequence kernel makes it possible to generate child subsets of the two nodes, i.e., it allows for gaps, which makes matching of syntactic patterns 5To have a similarity score between </context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Siersdorfer</author>
<author>Sergiu Chelaru</author>
<author>Wolfgang Nejdl</author>
<author>Jose San Pedro</author>
</authors>
<title>How useful are your comments?: Analyzing and predicting YouTube comments and comment ratings.</title>
<date>2010</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="2031" citStr="Siersdorfer et al. (2010)" startWordPosition="291" endWordPosition="294">t. Hence, doing sentiment research in such an environment is highly relevant for the community. While the linguistic conventions used on Twitter and YouTube indeed show similarities (Baldwin et al., 2013), focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research. Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by the classification system of YouTube comments proposed by Siersdorfer et al. (2010). While previous state-of-the-art models for opinion classification have been successfully applied to traditional corpora (Pang and Lee, 2008), YouTube comments pose additional challenges: (i) polarity words can refer to either video or product while expressing contrasting sentiments; (ii) many comments are unrelated or contain spam; and (iii) learning supervised models requires training data for each different YouTube domain, e.g., tablets, automobiles, etc. For example, consider a typical comment on a YouTube review video about a Motorola Xoom tablet: this guy really puts a negative spin on </context>
<context position="6998" citStr="Siersdorfer et al. (2010)" startWordPosition="1055" endWordPosition="1058">for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube. A recent study focuses on sentiment analysis for Twitter (Pak and Paroubek, 2010), however, their corpus was compiled automatically by searching for emoticons expressing positive and negative sentiment only. Siersdorfer et al. (2010) focus on exploiting user ratings (counts of ‘thumbs up/down’ as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Hence, their goal is different: predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode doc</context>
</contexts>
<marker>Siersdorfer, Chelaru, Nejdl, Pedro, 2010</marker>
<rawString>Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl, and Jose San Pedro. 2010. How useful are your comments?: Analyzing and predicting YouTube comments and comment ratings. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7859" citStr="Socher et al., 2011" startWordPosition="1195" endWordPosition="1198">atings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees with latent nodes (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust sys</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Anders Johannsen</author>
</authors>
<title>Robust learning in random subspaces: Equipping nlp for oov effects.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="8491" citStr="Søgaard and Johannsen, 2012" startWordPosition="1303" endWordPosition="1306">ently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e</context>
</contexts>
<marker>Søgaard, Johannsen, 2012</marker>
<rawString>Anders Søgaard and Anders Johannsen. 2012. Robust learning in random subspaces: Equipping nlp for oov effects. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Semisupervised latent variable models for sentence-level sentiment analysis.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semisupervised latent variable models for sentence-level sentiment analysis. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Barbara Plank</author>
<author>Aliaksei Severyn</author>
<author>Agata Rotondi</author>
<author>Alessandro Moschitti</author>
</authors>
<title>SenTube: A corpus for sentiment analysis on YouTube social media.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="21845" citStr="Uryupina et al. (2014)" startWordPosition="3495" endWordPosition="3498">ith gaps, i.e., 1256 annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO). To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in Uryupina et al. (2014). 5 Experiments This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves that provide an indication on the required amount and type of data and the scalability to other domains. 5.1 Task description Sentiment classification. We treat each comment as expressing positive, negative or neutral sentiment. Hence, the task is a threeway classification. Type classification. One of the challeng</context>
</contexts>
<marker>Uryupina, Plank, Severyn, Rotondi, Moschitti, 2014</marker>
<rawString>Olga Uryupina, Barbara Plank, Aliaksei Severyn, Agata Rotondi, and Alessandro Moschitti. 2014. SenTube: A corpus for sentiment analysis on YouTube social media. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7919" citStr="Wang and Manning (2012)" startWordPosition="1205" endWordPosition="1208"> a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees with latent nodes (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013)</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6037" citStr="Wilson et al., 2005" startWordPosition="915" endWordPosition="918">dels are adaptable, especially when the structural information is used. Structural models generally improve on both tasks – polarity and type classification – yielding up to 30% of relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 2 Related work Most prior work on more general OM has been carried out on more standardized forms of text, such as consumer reviews or newswire. The most commonly used datasets include: the MPQA corpus of news documents (Wilson et al., 2005), web customer review data (Hu and Liu, 2004), Amazon review data (Blitzer et al., 2007), the JDPA 1The corpus and the annotation guidelines are publicly available at: http://projects.disi.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and d</context>
<context position="10218" citStr="Wilson et al., 2005" startWordPosition="1571" endWordPosition="1574"> Feature Set We enrich the traditional bag-of-word representation with features from a sentiment lexicon and features quantifying the negation present in the comment. Our model (FVEC) encodes each document using the following feature groups: - word n-grams: we compute unigrams and bigrams over lower-cased word lemmas where binary values are used to indicate the presence/absence of a given item. - lexicon: a sentiment lexicon is a collection of words associated with a positive or negative sentiment. We use two manually constructed sentiment lexicons that are freely available: the MPQA Lexicon (Wilson et al., 2005) and the lexicon of Hu and Liu (2004). For each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature. - negation: the count of negation words, e.g., {don’t, never, not, etc.}, found in a comment.2 Our structural representation (defined next) enables a more involved treatment of negation. - video concept: cosine similarity between a comment and the title/description of the video. Most of the videos come with a title and a short description, which can be used to encode the topicality of 2The list of negation words is adopted from</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>