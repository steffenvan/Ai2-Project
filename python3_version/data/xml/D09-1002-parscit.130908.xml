<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000110">
<title confidence="0.940671">
Graph Alignment for Semi-Supervised Semantic Role Labeling
</title>
<author confidence="0.997178">
Hagen Firstenau
</author>
<affiliation confidence="0.996743">
Dept. of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.65121">
Saarbr¨ucken, Germany
</address>
<email confidence="0.970158">
hagenf@coli.uni-saarland.de
</email>
<author confidence="0.977768">
Mirella Lapata
</author>
<affiliation confidence="0.859404333333333">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.982682">
mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.984146" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991185">
Unknown lexical items present a major
obstacle to the development of broad-
coverage semantic role labeling systems.
We address this problem with a semi-
supervised learning approach which ac-
quires training instances for unseen verbs
from an unlabeled corpus. Our method re-
lies on the hypothesis that unknown lexical
items will be structurally and semantically
similar to known items for which annota-
tions are available. Accordingly, we rep-
resent known and unknown sentences as
graphs, formalize the search for the most
similar verb as a graph alignment prob-
lem and solve the optimization using inte-
ger linear programming. Experimental re-
sults show that role labeling performance
for unknown lexical items improves with
training data produced automatically by
our method.
</bodyText>
<sectionHeader confidence="0.992541" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998687090909091">
Semantic role labeling, the task of automatically
identifying the semantic roles conveyed by sen-
tential constituents, has recently attracted much at-
tention in the literature. The ability to express the
relations between predicates and their arguments
while abstracting over surface syntactic configu-
rations holds promise for many applications that
require broad coverage semantic processing. Ex-
amples include information extraction (Surdeanu
et al., 2003), question answering (Narayanan
and Harabagiu, 2004), machine translation (Boas,
2005), and summarization (Melli et al., 2005).
Much progress in the area of semantic role la-
beling is due to the creation of resources like
FrameNet (Fillmore et al., 2003), which document
the surface realization of semantic roles in real
world corpora. Such data is paramount for de-
veloping semantic role labelers which are usually
based on supervised learning techniques and thus
require training on role-annotated data. Examples
of the training instances provided in FrameNet are
given below:
</bodyText>
<listItem confidence="0.8661645">
(1) a. If [you]Agent [carelessly]Manner
chance going back there, you
deserve what you get.
b. Only [one winner]Buyer purchased
[the paintings]Goods
c. [Rachel]Agent injured [her
friend]Victim [by closing the car
door on his left hand]Means.
</listItem>
<bodyText confidence="0.999202888888889">
Each verb in the example sentences evokes a frame
which is situation-specific. For instance, chance
evokes the Daring frame, purchased the Com-
merce buy frame, and injured the Cause harm
frame. In addition, frames are associated with
semantic roles corresponding to salient entities
present in the situation evoked by the predicate.
The semantic roles for the frame Daring are Agent
and Manner, whereas for Commerce buy these are
Buyer and Goods. A system trained on large
amounts of such hand-annotated sentences typi-
cally learns to identify the boundaries of the argu-
ments of the verb predicate (argument identifica-
tion) and label them with semantic roles (argument
classification).
A variety of methods have been developed for
semantic role labeling with reasonably good per-
formance (F1 measures in the low 80s on standard
test collections for English; we refer the interested
reader to the proceedings of the SemEval-2007
shared task (Baker et al., 2007) for an overview
of the state-of-the-art). Unfortunately, the reliance
on training data, which is both difficult and highly
expensive to produce, presents a major obstacle
to the widespread application of semantic role la-
beling across different languages and text gen-
res. The English FrameNet (version 1.3) is not
</bodyText>
<page confidence="0.664799">
11
</page>
<note confidence="0.999115">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11–20,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9997536">
a small resource — it contains 502 frames cov-
ering 5,866 lexical entries and 135,000 annotated
sentences. Nevertheless, by virtue of being un-
der development it is incomplete. Lexical items
(i.e., predicates evoking existing frames) are miss-
ing as well as frames and annotated sentences
(their number varies greatly across lexical items).
Considering how the performance of supervised
systems degrades on out-of-domain data (Baker
et al., 2007), not to mention unseen events, semi-
supervised or unsupervised methods seem to offer
the primary near-term hope for broad coverage se-
mantic role labeling.
In this work, we develop a semi-supervised
method for enhancing FrameNet with additional
annotations which could then be used for clas-
sifier training. We assume that an initial set of
labeled examples is available. Then, faced with
an unknown predicate, i.e., a predicate that does
not evoke any frame according to the FrameNet
database, we must decide (a) which frames it be-
longs to and (b) how to automatically annotate
example sentences containing the predicate. We
solve both problems jointly, using a graph align-
ment algorithm. Specifically, we view the task
of inferring annotations for new verbs as an in-
stance of a structural matching problem and fol-
low a graph-based formulation for pairwise global
network alignment (Klau, 2009). Labeled and un-
labeled sentences are represented as dependency-
graphs; we formulate the search for an optimal
alignment as an integer linear program where dif-
ferent graph alignments are scored using a func-
tion based on semantic and structural similarity.
We evaluate our algorithm in two ways. We assess
how accurate it is in predicting the frame for an
unknown verb and also evaluate whether the an-
notations we produce are useful for semantic role
labeling.
In the following section we provide an overview
of related work. Next, we describe our graph-
alignment model in more detail (Section 3) and
present the resources and evaluation methodology
used in our experiments (Section 4). We conclude
the paper by presenting and discussing our results.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999630890909091">
Much previous work has focused on creating
FrameNet-style annotations for languages other
than English. A common strategy is to exploit
parallel corpora and transfer annotations from
English sentences onto their translations (Pad´o
and Lapata, 2006; Johansson and Nugues, 2006).
Other work attempts to automatically augment the
English FrameNet in a monolingual setting either
by extending its coverage or by creating additional
training data.
There has been growing interest recently in
determining the frame membership for unknown
predicates. This is a challenging task, FrameNet
currently lists 502 frames with example sentences
which are simply too many (potentially related)
classes to consider for a hypothetical system.
Moreover, predicates may have to be assigned to
multiple frames, on account of lexical ambiguity.
Previous work has mainly used WordNet (Fell-
baum, 1998) to extend FrameNet. For example,
Burchardt et al. (2005) apply a word sense dis-
ambiguation system to annotate predicates with
a WordNet sense and hyponyms of these predi-
cates are then assumed to evoke the same frame.
Johansson and Nugues (2007) treat this problem
as an instance of supervised classification. Using
a feature representation based also on WordNet,
they learn a classifier for each frame which decides
whether an unseen word belongs to the frame or
not. Pennacchiotti et al. (2008) create “distribu-
tional profiles” for frames. Each frame is repre-
sented as a vector, the (weighted) centroid of the
vectors representing the meaning of the predicates
it evokes. Unknown predicates are then assigned
to the most similar frame. They also propose a
WordNet-based model that computes the similar-
ity between the synsets representing an unknown
predicate and those activated by the predicates of
a frame.
All the approaches described above are type-
based. They place more emphasis on extending
the lexicon rather than the annotations that come
with it. In our earlier work (F¨urstenau and Lapata,
2009) we acquire new training instances, by pro-
jecting annotations from existing FrameNet sen-
tences to new unseen ones. The proposed method
is token-based, however, it only produces annota-
tions for known verbs, i.e., verbs that FrameNet
lists as evoking a given frame.
In this paper we generalize the proposals of
Pennacchiotti et al. (2008) and F¨urstenau and Lap-
ata (2009) in a unified framework. We create train-
ing data for semantic role labeling of unknown
predicates by projection of annotations from la-
beled onto unlabeled data. This projection is con-
</bodyText>
<page confidence="0.841844">
12
</page>
<bodyText confidence="0.999898111111111">
ceptualized as a graph alignment problem where
we seek to find a globally optimal alignment sub-
ject to semantic and structural constraints. Instead
of predicting the same frame for each occurence of
an unknown predicate, we consider a set of candi-
date frames and allow projection from any labeled
predicate that can evoke one of these frames. This
allows us to make instance-based decisions and
thus account for predicate ambiguity.
</bodyText>
<sectionHeader confidence="0.985857" genericHeader="method">
3 Graph Alignment Method
</sectionHeader>
<bodyText confidence="0.9999956">
Our approach acquires annotations for an un-
known frame evoking verb by selecting sen-
tences featuring this verb from a large unlabeled
corpus (the expansion corpus). The choice is
based upon a measure of similarity between the
predicate-argument structure of the unknown verb
and those of similar verbs in a manually labeled
corpus (the seed corpus). We formulate the prob-
lem of finding the most similar verbs as the search
for an optimal graph alignment (we represent
labeled and unlabeled sentences as dependency
graphs). Conveniently, this allows us to create la-
beled training instances for the unknown verb by
projecting role labels from the most similar seed
instance. The annotations can be subsequently
used for training a semantic role labeler.
Given an unknown verb, the first step is to nar-
row down the number of frames it could poten-
tially evoke. FrameNet provides definitions for
more than 500 frames, of which we entertain only
a small number. This is done using a method sim-
ilar to Pennacchiotti et al. (2008). Each frame
is represented in a semantic space as the cen-
troid of the vectors of all its known frame evoking
verbs. For an unknown verb we then consider as
frame candidates the k closest frames according to
a measure of distributional similarity (which we
compute between the unknown verb’s vector and
the frame centroid vector). We provide details of
the semantic space we used in our experiments in
Section 4.
Next, we compare each sentence featuring the
unknown verb in question to labeled sentences fea-
turing known verbs which according to FrameNet
evoke any of the k candidate frames. If sufficiently
similar seeds exist, the unlabeled sentence is anno-
tated by projecting role labels from the most sim-
ilar one. The similarity score of this best match is
recorded as a measure of the quality (or reliability)
of the new instance. After carrying out this pro-
</bodyText>
<figure confidence="0.987175571428571">
� —Body movement
I
�
FEE i
i
�
Body part
nod
�
MOD
v
wisely
V
his
</figure>
<figureCaption confidence="0.999659">
Figure 1: Annotated dependency graph for the
</figureCaption>
<bodyText confidence="0.970774545454546">
sentence Old Herkimer blinked his eye and nodded
wisely. The alignment domain is indicated in bold
face. Labels in italics denote frame roles, whereas
grammatical roles are rendered in small capitals.
The verb blink evokes the frame Body Movement.
cedure for all sentences in the expansion corpus
featuring an unknown verb, we collect the highest
scoring new instances and add them back to our
seed corpus as new training items. In the follow-
ing we discuss in more detail how the similarity of
predicate-argument structures is assessed.
</bodyText>
<subsectionHeader confidence="0.999854">
3.1 Alignment Scoring
</subsectionHeader>
<bodyText confidence="0.999890954545455">
Let s be a semantically labeled dependency graph
in which node nFEE represents the frame evoking
verb. Here, we use the term “labeled” to indi-
cate that the graph contains semantic role labels
in addition to grammatical role labels (e.g., sub-
ject or object). Let g be an unlabeled graph
and ntarget a verbal node in it. The “unlabeled”
graph contains grammatical roles but no semantic
roles. We wish to find an alignment between the
predicate-argument structures of nFEE and ntarget,
respectively. Such an alignment takes the form of
a function σ from a set M of nodes of s (the align-
ment domain) to a set N of nodes of g (the align-
ment range). These two sets represent the rele-
vant predicate-argument structures within the two
graphs; nodes that are not members of these sets
are excluded from any further computations.
If there were no mismatches between (frame)
semantic arguments and syntactic arguments, we
would expect all roles in s to be instantiated by
syntactic dependents in nFEE. This is usually the
case but not always. We cannot therefore sim-
</bodyText>
<figure confidence="0.9852435">
blink
Herkimer
DOBJ
MOD
11
V
� �
•� �
eye
Old
DET
Agent �
�
�
�
�
i
i
and
�
�
SUBJ
V
CONJ �
�
Y �
i
�
CONJ
�
���
/
�
13
</figure>
<bodyText confidence="0.999444">
ply define M as the set of direct dependents of
the predicate, but also have to consider complex
paths between nFEE and role bearing nodes. An
example is given in Figure 1, where the role Agent
is filled by a node which is not dominated by the
frame evoking verb blink; instead, it is connected
to blink by the complex path (CONJ-1, SUBJ). For
a given seed s we build a list of all such complex
paths and also include all nodes of s connected
to nFEE by one of these paths. We thus define the
alignment domain M as:
</bodyText>
<listItem confidence="0.95426075">
1. the predicate node nFEE
2. all direct dependents of nFEE, except auxil-
iaries
3. all nodes on complex paths originating
in nFEE
4. single direct dependents of any preposition or
conjunction node which is in (2) or end-point
of a complex path covered in (3)
</listItem>
<bodyText confidence="0.999968066666667">
The last rule ensures that the semantic heads
of prepositional phrases and conjunctions are in-
cluded in the alignment domain.
The alignment range N is defined in a similar
way. However, we cannot extract complex paths
from the unlabeled graph g, as it does not con-
tain semantic role information. Therefore, we use
the same list of complex paths extracted from s.
Note that this introduces an unavoidable asymme-
try into our similarity computation.
An alignment is a function σ : M —* N U {ε}
which is injective for all values except ε,
i.e., σ(n1) = σ(n2) =� ε =&gt;. n1 = n2. We score the
similarity of two subgraphs expressed by an align-
ment function σ by the following term:
</bodyText>
<equation confidence="0.653792666666667">
∑ sem(n,σ(n))+αY syn(rn1 r n2,σ�n2j) (2)
nEM (n1,n2)EE(M)
σ(n)0ε (σ(n1),σ(n2))EE(N)
</equation>
<bodyText confidence="0.999315">
Here, sem represents a semantic similarity mea-
sure between graph nodes and syn a syntactic sim-
ilarity measure between the grammatical role la-
bels of graph edges. E(M) and E(N) are the sets
of all graph edges between nodes of M and nodes
of N, respectively, and rn1
n2 denotes the grammati-
cal relation between nodes n1 and n2.
Equation (2) expresses the similarity between
two predicate-argument structures in terms of the
sum of semantic similarity scores of aligned graph
nodes and the sum of syntactic similarity scores of
aligned graph edges. The relative weight of these
two sums is determined by the parameter α. Fig-
ure 2 shows an example of an alignment between
two dependency graphs. Here, the aligned node
pairs thud and thump, back and rest, against and
against, as well as wall and front contribute se-
mantic similarity scores, while the three edge pairs
SUBJ and SUBJ, IOBJ and IOBJ, as well as DOBJ
and DOBJ contribute syntactic similarity scores.
We normalize the resulting score so that it al-
ways falls within the interval [0,1]. To take into
account unaligned nodes in both the alignment do-
main and the alignment range, we divide Equa-
tion (2) by:
</bodyText>
<equation confidence="0.9696275">
VI VI
|M |� |N |+α |E(M) |- |E(N) |(3)
</equation>
<bodyText confidence="0.972369666666667">
A trivial alignment of a seed with itself where all
semantic and syntactic scores are 1 will thus re-
ceive a score of:
</bodyText>
<equation confidence="0.516311333333333">
|M |• 1+α• |E(M)|.1 = 1 (4)
VI VI
|M|2 +α E(M)2
</equation>
<bodyText confidence="0.997126625">
which is the largest possible similarity score. The
lowest possible score is obviously 0, assuming that
the semantic and syntactic scores cannot be nega-
tive.
Considerable latitude is available in selecting
the semantic and syntactic similarity measures.
With regard to semantic similarity, WordNet is a
prime contender and indeed has been previously
used to acquire new predicates in FrameNet (Pen-
nacchiotti et al., 2008; Burchardt et al., 2005; Jo-
hansson and Nugues, 2007). Syntactic similarity
may be operationalized in many ways, for exam-
ple by taking account a hierarchy of grammatical
relations (Keenan and Comrie, 1977). Our experi-
ments employed relatively simple instantiations of
these measures. We did not make use of Word-
Net, as we were interested in exploring the set-
ting where WordNet is not available or has limited
coverage. Therefore, we approximate the seman-
tic similarity between two nodes via distributional
similarity. We present the details of the semantic
space model we used in Section 4.
If n and n&apos; are both nouns, verbs or adjectives,
we set:
</bodyText>
<equation confidence="0.997969">
sem(n,n&apos;) := cos(~vn,~vn&apos;) (5)
</equation>
<bodyText confidence="0.9995275">
where ~vn and ~vn&apos; are the vectors representing the
lemmas of n and n&apos; respectively. If n and n&apos;
</bodyText>
<figure confidence="0.993679689655172">
14
Impactor
Impact—
Impactee
�
�
�
�
FEE
9
�
��
thump
k
I
I
I
I
I
Y ���
back
thud
SUBJ IOBJ
SUBJ IOBJ ������
a
against
�
i
I
Y
against
rest
7
DET
DET
DOBJ
DOBJ
IOBJ
V V V V
his wall the of front
DOBJ
DET
the
77
V
body
V
DET
V
the
IOBJ
of
DET V DOBJ
V
his cage
DET
V
the
</figure>
<figureCaption confidence="0.999158">
Figure 2: The dotted arrows show aligned nodes in the graphs for the two sentences His back thudded
</figureCaption>
<bodyText confidence="0.989651571428571">
against the wall. and The rest of his body thumped against the front of the cage. (Graph edges are also
aligned to each other.) The alignment domain and alignment range are indicated in bold face. The verb
thud evokes the frame Impact.
are identical prepositions or conjunctions we set
sem(n,n&apos;) := 1. In all other cases sem(n,n&apos;) := 0.
As far as syntactic similarity is concerned, we
chose the simplest metric possible and set:
</bodyText>
<equation confidence="0.857261">
syn r,r&apos;) := 1 if r = r&apos; (6)
0 otherwise
</equation>
<subsectionHeader confidence="0.997945">
3.2 Alignment Search
</subsectionHeader>
<bodyText confidence="0.999906666666667">
The problem of finding the best alignment ac-
cording to the scoring function presented in Equa-
tion (2) can be formulated as an integer linear pro-
gram. Let the binary variables xik indicate whether
node ni of graph s is aligned to node nk of graph g.
Since it is not only nodes but also graph edges
that must be aligned we further introduce binary
variables yijkl, where yijkl = 1 indicates that the
edge between nodes ni and nj of graph s is aligned
to the edge between nodes nk and nl of graph g.
This follows a general formulation of the graph
alignment problem based on maximum structural
matching (Klau, 2009). In order for the xik and
yijkl variables to represent a valid alignment, the
following constraints must hold:
</bodyText>
<listItem confidence="0.945572">
1. Each node of s is aligned to at most one node
of g: Ykxik &lt; 1
2. Each node of g is aligned to at most one node
of s: Yi xik &lt; 1
3. Two edges may only be aligned if their
adjacent nodes are aligned: yijkl &lt; xik and
yijkl &lt; xjl
</listItem>
<bodyText confidence="0.918466">
The scoring function then becomes:
</bodyText>
<equation confidence="0.531583">
syn (rnn;, rnnk) yijkl (7)
</equation>
<bodyText confidence="0.999872529411765">
We solve this optimization problem with a ver-
sion of the branch-and-bound algorithm (Land
and Doig, 1960). In general, this graph align-
ment problem is NP-hard (Klau, 2009) and usually
solved approximately following a procedure simi-
lar to beam search. However, the special structure
of constraints 1 to 3, originating from the required
injectivity of the alignment function, allows us to
solve the optimization exactly. Our implementa-
tion of the branch-and-bound algorithm does not
generally run in polynomial time, however, we
found that in practice we could efficiently com-
pute optimal alignments in almost all cases (less
than 0.1% of alignment pairs in our data could not
be solved in reasonable time). This relatively be-
nign behavior depends crucially on the fact that
we do not have to consider alignments between
</bodyText>
<equation confidence="0.931497666666667">
Y sem(ni,nk)xik + a · Y
i,k i, j,k,l
15
</equation>
<bodyText confidence="0.9877275">
full graphs, and the number of nodes in the aligned
subgraphs is limited.
</bodyText>
<sectionHeader confidence="0.995411" genericHeader="method">
4 Experimental Design
</sectionHeader>
<bodyText confidence="0.999321494736843">
In this section we present our experimental set-up
for assessing the performance of our method. We
give details on the data sets we used, describe the
baselines we adopted for comparison with our ap-
proach, and explain how our system output was
evaluated.
Data Our experiments used annotated sentences
from FrameNet as a seed corpus. These were
augmented with automatically labeled sentences
from the BNC which we used as our expan-
sion corpus. FrameNet sentences were parsed
with RASP (Briscoe et al., 2006). In addi-
tion to phrase structure trees, RASP delivers a
dependency-based representation of the sentence
which we used in our experiments. FrameNet role
annotations were mapped onto those dependency
graph nodes that corresponded most closely to the
annotated substring (see F¨urstenau (2008) for a de-
tailed description of the mapping algorithm). BNC
sentences were also parsed with RASP (Andersen
et al., 2008).
We randomly split the FrameNet corpus1
into 80% training set, 10% test set, and 10% de-
velopment set. Next, all frame evoking verbs in
the training set were ordered by their number of
occurrence and split into two groups, seen and un-
seen. Every other verb from the ordered list was
considered unseen. This quasi-random split covers
a broad range of predicates with a varying number
of annotations. Accordingly, the FrameNet sen-
tences in the training and test sets were divided
into the sets train seen, train unseen, test seen,
and test unseen. As we explain below, this was
necessary for evaluation purposes.
The train seen dataset consisted of 24,220 sen-
tences, with 1,238 distinct frame evoking verbs,
whereas train unseen contained 24,315 sentences
with the same number of frame evoking verbs.
Analogously, test seen had 2,990 sentences and
817 unique frame evoking verbs; the number
of sentences in test unseen was 3,064 (with
847 unique frame evoking verbs).
Model Parameters The alignment model pre-
sented in Section 3 crucially relies on the similar-
1Here, we consider only FrameNet example sentences
featuring verbal predicates.
ity function that scores potential alignments (see
Equation (2)). This function has a free parameter,
the weight a for determining the relative contri-
bution of semantic and syntactic similarity. We
tuned a using leave-one-out cross-validation on
the development set. For each annotated sentence
in this set we found its most similar other sentence
and determined the best alignment between the
two dependency graphs representing them. Since
the true annotations for each sentence were avail-
able, it was possible to evaluate the accuracy of our
method for any a value. We did this by compar-
ing the true annotation of a sentence to the anno-
tation its nearest neighbor would have induced by
projection. Following this procedure, we obtained
best results with a = 0.2.
The semantic similarity measure relies on a se-
mantic space model which we built on a lemma-
tized version of the BNC. Our implementation fol-
lowed closely the model presented in F¨urstenau
and Lapata (2009) as it was used in a similar
task and obtained good results. Specifically, we
used a context window of five words on either
side of the target word, and 2,000 vector dimen-
sions. These were the common context words in
the BNC. Their values were set to the ratio of the
probability of the context word given the target
word to the probability of the context word over-
all. Semantic similarity was measured using the
cosine of the angle between the vectors represent-
ing any two words. The same semantic space was
used to create the distributional profile of a frame
(which is the centroid of the vectors of its verbs).
For each unknown verb, we consider the k most
similar frame candidates (again similarity is mea-
sured via cosine). Our experiments explored dif-
ferent values of k ranging from 1 to 10.
Evaluation Our evaluation assessed the perfor-
mance of a semantic frame and role labeler with
and without the annotations produced by our
method. The labeler followed closely the im-
plementation described in Johansson and Nugues
(2008). We extracted features from dependency
parses corresponding to those routinely used in
the semantic role labeling literature (see Baker
et al. (2007) for an overview). SVM classifiers
were trained2 with the LIBLINEAR library (Fan
et al., 2008) and learned to predict the frame
name, role spans, and role labels. We followed
</bodyText>
<figure confidence="0.5339415">
2The regularization parameter C was set to 0.1.
16
</figure>
<figureCaption confidence="0.898173">
Figure 3: Frame labeling accuracy on high,
</figureCaption>
<bodyText confidence="0.979787285714286">
medium and low frequency verbs, before and af-
ter applying our expansion method; the labeler de-
cides among k = 1,...,10 candidate frames.
Figure 4: Role labeling F1 for high, medium, and
low frequency verbs (roles of mislabeled frames
are counted as wrong); the labeler decides among
k = 1,...,10 candidate frames.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99995">
the one-versus-one strategy for multi-class classi-
fication (Friedman, 1996).
Specifically, the labeler was trained on the
train seen data set without any access to training
instances representative of the “unknown” verbs in
test unseen. We then trained the labeler on a larger
set containing train seen and new training exam-
ples obtained with our method. To do this, we used
train seen as the seed corpus and the BNC as the
expansion corpus. For each “unknown” verb in
train unseen we obtained BNC sentences with an-
notations projected from their most similar seeds.
The quality of these sentences as training instances
varies depending on their similarity to the seed.
In our experiments we added to the training set
the 20 highest scoring BNC sentences per verb
(adding less or more instances led to worse per-
formance).
The average number of frames which can be
evoked by a verb token in the set test unseen
was 1.96. About half of them (1,522 instances)
can evoke only one frame, 22% can evoke two
frames, and 14 instances can evoke up to 11 differ-
ent frames. Finally, there are 120 instances (4%)
in test unseen for which the correct frame is not
annotated on any sentence in train seen.
We first examine how well our method performs
at frame labeling. We partitioned the frame evok-
ing verbs in our data set into three bands (High,
Medium, and Low) based on an equal division
of the range of their occurrence frequency in the
BNC. As frequency is strongly correlated with
polysemy, the division allows us to assess how
well our method is performing at different degrees
of ambiguity. Figure 3 summarizes our results for
High, Medium, and Low frequency verbs. The
number of verbs in each band are 282, 282, and
283, respectively. We compare the frame accuracy
of a labeler trained solely on the annotations avail-
able in FrameNet (Without expansion) against a
labeler that also uses annotations created with our
method (After expansion). Both classifiers were
employed in a setting where they had to decide
among k candidate frames. These were the k most
similar frames to the unknown verb in question.
We also show the accuracy of a simple baseline
labeler, which randomly chooses one of the k can-
didate frames.
The graphs in Figure 3 show that for verbs in the
Medium and Low frequency bands, both classi-
fiers (with and without expansion) outperform the
baseline of randomly choosing among k candidate
frames. Interestingly, rather than defaulting to the
most similar frame (k = 1), we observe that ac-
</bodyText>
<page confidence="0.749222">
17
</page>
<figureCaption confidence="0.99425625">
Figure 5: Hybrid frame labeling accuracy (k = 1
for High frequency verbs).
Figure 6: Hybrid role labeling F1 (k = 1 for High
frequency verbs).
</figureCaption>
<bodyText confidence="0.999992696428572">
curacy improves when frame selection is viewed
as a classification task. The classifier trained on
the expanded training set consistently outperforms
the one trained on the original training set. While
this is also true for the verbs in the High frequency
band, labeling accuracy peaks at k = 1 and does
not improve when more candidate frames are con-
sidered. This is presumably due to the skewed
sense distributions of high frequency verbs, and
defaulting to the most likely sense achieves rela-
tively good performance.
Next, we evaluated our method on role label-
ing, again by comparing the performance of our
role labeler on the expanded and original train-
ing set. Since role and frame labeling are inter-
dependent, we count all predicted roles of an in-
correctly predicted frame as wrong. This unavoid-
ably results in low role labeling scores, but allows
us to directly compare performance across differ-
ent settings (e.g., different number of candidate
frames, with or without expansion). Figure 4 re-
ports labeled F1 for verbs in the High, Medium
and Low frequency bands. The results are simi-
lar to those obtained for frame labeling; the role
labeler trained on the the expanded training set
consistently outperforms the labeler trained on the
unexpanded one. (There is no obvious baseline
for role labeling, which is a complex task involv-
ing the prediction of frame labels, identification of
the role bearing elements, and assignment of role
labels.) Again, for High frequency verbs simply
defaulting to k = 1 performs best.
Taken together, our results on frame and role
labeling indicate that our method is not very effec-
tive for High frequency verbs (which in practice
should be still annotated manually). We there-
fore also experimented with a hybrid approach
that lets the classifier choose among k candi-
dates for Medium and Low frequency verbs and
defaults to the most similar candidate for High
frequency verbs. Results for this approach are
shown in Figures 5 and 6. All differences be-
tween the expanded and the unexpanded classi-
fier when choosing between the same k &gt; 1 can-
didates are significant according to McNemar’s
test (p &lt; .05). The best frame labeling accu-
racy (26.3%) is achieved by the expanded classi-
fier when deciding among k = 6 candidate frames.
This is significantly better (p &lt; .01) than the best
performance of the unexpanded classifier (25.0%),
which is achieved at k = 2. Role labeling results
follow a similar pattern. The best expanded classi-
fier (F1=14.9% at k = 6) outperforms the best un-
expanded one (F1=14.1% at k = 2). The difference
in performance as significant at p &lt; 0.05, using
stratified shuffling (Noreen, 1989).
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999994928571429">
This paper presents a novel semi-supervised ap-
proach for reducing the annotation effort involved
in creating resources for semantic role labeling.
Our method acquires training instances for un-
known verbs (i.e., verbs that are not evoked by
existing FrameNet frames) from an unlabeled cor-
pus. A key assumption underlying our work is
that verbs with similar meanings will have sim-
ilar argument structures. Our task then amounts
to finding the seen instances that resemble the un-
seen instances most, and projecting their annota-
tions. We represent this task as a graph alignment
problem, and formalize the search for an optimal
alignment as an integer linear program under an
</bodyText>
<page confidence="0.832697">
18
</page>
<bodyText confidence="0.999925303030303">
objective function that takes semantic and struc-
tural similarity into account.
Experimental results show that our method im-
proves frame and role labeling accuracy, espe-
cially for Medium and Low frequency verbs. The
overall frame labeling accuracy may seem low.
There are at least two reasons for this. Firstly, the
unknown verb might have a frame for which no
manual annotation exists. And secondly, many er-
rors are due to near-misses, i.e., we assign the un-
known verb a wrong frame which is nevertheless
very similar to the right one. In this case, accuracy
will not give us any credit.
An obvious direction for future work concerns
improving our scoring function. Pennacchiotti
et al. (2008) show that WordNet-based similarity
measures outperform their simpler distributional
alternatives. An interesting question is whether the
incorporation of WordNet-based similarity would
lead to similar improvements in our case. Also
note that currently our method assigns unknown
lexical items to existing frames. A better alterna-
tive would be to decide first whether the unknown
item can be classified at all (because it evokes a
known frame) or whether it represents a genuinely
novel frame for which manual annotation must be
provided.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC (grant
GR/T04540/01). We are grateful to Richard Jo-
hansson for his help with the re-implementation of
his semantic role labeler. Special thanks to Man-
fred Pinkal for valuable feedback on this work.
</bodyText>
<sectionHeader confidence="0.994289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990468115942">
Øistein E. Andersen, Julien Nioche, Ted Briscoe,
and John Carroll. 2008. The BNC Parsed with
RASP4UIMA. In Proceedings of the 6th Interna-
tional Language Resources and Evaluation Confer-
ence, pages 865–869, Marrakech, Morocco.
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99–104, Prague, Czech Republic.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases.
International Journal of Lexicography, 18(4):445–
478.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77–80, Sydney, Australia.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In Proceed-
ings of the GLDV 200 Workshop GermaNet II, Bonn,
Germany.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. Journal of
Machine Learning Research, 9:1871–1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235–250.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Hagen F¨urstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 220–228, Athens, Greece.
Hagen F¨urstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, pages 1478–1484, Marrakech, Morocco.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436–443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Richard
Johansson and Pierre Nugues, editors, FRAME
2007: Building Frame Semantics Resources for
Scandinavian and Baltic Languages, pages 27–30,
Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393–400, Manchester, UK.
E. Keenan and B. Comrie. 1977. Noun phrase acces-
sibility and universal grammar. Linguistic Inquiry,
8:62–100.
Gunnar W. Klau. 2009. A new graph-based method
for pairwise global network alignment. BMC Bioin-
formatics, 10 (Suppl 1).
A.H. Land and A.G. Doig. 1960. An automatic
method for solving discrete programming problems.
Econometrica, 28:497–520.
</reference>
<page confidence="0.597393">
19
</page>
<reference confidence="0.999041470588236">
Gabor Melli, Yang Wang, Yurdong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of
SQUASH, the SFU question answering summary
handler for the duc-2005 summarization task. In
Proceedings of the HLT/EMNLP Document Under-
standing Workshop, Vancouver, Canada.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693–701, Geneva,
Switzerland.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pad´o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161–1168, Sydney,
Australia.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457–465, Honolulu,
Hawaii.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8–15, Sap-
poro, Japan.
</reference>
<page confidence="0.619498">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256338">
<title confidence="0.999849">Graph Alignment for Semi-Supervised Semantic Role Labeling</title>
<author confidence="0.975972">Hagen</author>
<affiliation confidence="0.999612">Dept. of Computational</affiliation>
<address confidence="0.458613">Saarland</address>
<email confidence="0.74925">Saarbr¨ucken,hagenf@coli.uni-saarland.de</email>
<author confidence="0.920232">Mirella</author>
<affiliation confidence="0.974835666666667">School of University of Edinburgh,</affiliation>
<email confidence="0.999255">mlap@inf.ed.ac.uk</email>
<abstract confidence="0.998952476190476">Unknown lexical items present a major obstacle to the development of broadcoverage semantic role labeling systems. We address this problem with a semisupervised learning approach which acquires training instances for unseen verbs from an unlabeled corpus. Our method relies on the hypothesis that unknown lexical items will be structurally and semantically similar to known items for which annotations are available. Accordingly, we represent known and unknown sentences as graphs, formalize the search for the most similar verb as a graph alignment problem and solve the optimization using integer linear programming. Experimental results show that role labeling performance for unknown lexical items improves with training data produced automatically by our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Øistein E Andersen</author>
<author>Julien Nioche</author>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>The BNC Parsed with RASP4UIMA.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Language Resources and Evaluation Conference,</booktitle>
<pages>865--869</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="20431" citStr="Andersen et al., 2008" startWordPosition="3408" endWordPosition="3411">tences from FrameNet as a seed corpus. These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus. FrameNet sentences were parsed with RASP (Briscoe et al., 2006). In addition to phrase structure trees, RASP delivers a dependency-based representation of the sentence which we used in our experiments. FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see F¨urstenau (2008) for a detailed description of the mapping algorithm). BNC sentences were also parsed with RASP (Andersen et al., 2008). We randomly split the FrameNet corpus1 into 80% training set, 10% test set, and 10% development set. Next, all frame evoking verbs in the training set were ordered by their number of occurrence and split into two groups, seen and unseen. Every other verb from the ordered list was considered unseen. This quasi-random split covers a broad range of predicates with a varying number of annotations. Accordingly, the FrameNet sentences in the training and test sets were divided into the sets train seen, train unseen, test seen, and test unseen. As we explain below, this was necessary for evaluation</context>
</contexts>
<marker>Andersen, Nioche, Briscoe, Carroll, 2008</marker>
<rawString>Øistein E. Andersen, Julien Nioche, Ted Briscoe, and John Carroll. 2008. The BNC Parsed with RASP4UIMA. In Proceedings of the 6th International Language Resources and Evaluation Conference, pages 865–869, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval-2007 Task 19: Frame Semantic Structure Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>99--104</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3295" citStr="Baker et al., 2007" startWordPosition="484" endWordPosition="487">mantic roles for the frame Daring are Agent and Manner, whereas for Commerce buy these are Buyer and Goods. A system trained on large amounts of such hand-annotated sentences typically learns to identify the boundaries of the arguments of the verb predicate (argument identification) and label them with semantic roles (argument classification). A variety of methods have been developed for semantic role labeling with reasonably good performance (F1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al., 2007) for an overview of the state-of-the-art). Unfortunately, the reliance on training data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres. The English FrameNet (version 1.3) is not 11 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11–20, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP a small resource — it contains 502 frames covering 5,866 lexical entries and 135,000 annotated sentences. Nevertheless, by virtue of </context>
<context position="23717" citStr="Baker et al. (2007)" startWordPosition="3952" endWordPosition="3955">(which is the centroid of the vectors of its verbs). For each unknown verb, we consider the k most similar frame candidates (again similarity is measured via cosine). Our experiments explored different values of k ranging from 1 to 10. Evaluation Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method. The labeler followed closely the implementation described in Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained2 with the LIBLINEAR library (Fan et al., 2008) and learned to predict the frame name, role spans, and role labels. We followed 2The regularization parameter C was set to 0.1. 16 Figure 3: Frame labeling accuracy on high, medium and low frequency verbs, before and after applying our expansion method; the labeler decides among k = 1,...,10 candidate frames. Figure 4: Role labeling F1 for high, medium, and low frequency verbs (roles of mislabeled frames are counted as wrong); the labeler decides among k = 1,...,10 candidate frames. 5 Results the one</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin F. Baker, Michael Ellsworth, and Katrin Erk. 2007. SemEval-2007 Task 19: Frame Semantic Structure Extraction. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 99–104, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans C Boas</author>
</authors>
<title>Semantic frames as interlingual representations for multilingual lexical databases.</title>
<date>2005</date>
<journal>International Journal of Lexicography,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>478</pages>
<contexts>
<context position="1606" citStr="Boas, 2005" startWordPosition="222" endWordPosition="223">th training data produced automatically by our method. 1 Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually based on supervised learning techniques and thus require training on role-annotated data. Examples of the training instances provided in FrameNet are given below: (1) a. If [you]Agent [carelessly]Manner chance going back there, you deserve what you get. b. Only [one winne</context>
</contexts>
<marker>Boas, 2005</marker>
<rawString>Hans C. Boas. 2005. Semantic frames as interlingual representations for multilingual lexical databases. International Journal of Lexicography, 18(4):445– 478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The Second Release of the RASP System.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<pages>77--80</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="20021" citStr="Briscoe et al., 2006" startWordPosition="3346" endWordPosition="3349"> Y i,k i, j,k,l 15 full graphs, and the number of nodes in the aligned subgraphs is limited. 4 Experimental Design In this section we present our experimental set-up for assessing the performance of our method. We give details on the data sets we used, describe the baselines we adopted for comparison with our approach, and explain how our system output was evaluated. Data Our experiments used annotated sentences from FrameNet as a seed corpus. These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus. FrameNet sentences were parsed with RASP (Briscoe et al., 2006). In addition to phrase structure trees, RASP delivers a dependency-based representation of the sentence which we used in our experiments. FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see F¨urstenau (2008) for a detailed description of the mapping algorithm). BNC sentences were also parsed with RASP (Andersen et al., 2008). We randomly split the FrameNet corpus1 into 80% training set, 10% test set, and 10% development set. Next, all frame evoking verbs in the training set were ordered by their number of occur</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The Second Release of the RASP System. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77–80, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
</authors>
<title>A WordNet Detour to FrameNet.</title>
<date>2005</date>
<booktitle>In Proceedings of the GLDV 200 Workshop GermaNet II,</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="6797" citStr="Burchardt et al. (2005)" startWordPosition="1027" endWordPosition="1030">ly augment the English FrameNet in a monolingual setting either by extending its coverage or by creating additional training data. There has been growing interest recently in determining the frame membership for unknown predicates. This is a challenging task, FrameNet currently lists 502 frames with example sentences which are simply too many (potentially related) classes to consider for a hypothetical system. Moreover, predicates may have to be assigned to multiple frames, on account of lexical ambiguity. Previous work has mainly used WordNet (Fellbaum, 1998) to extend FrameNet. For example, Burchardt et al. (2005) apply a word sense disambiguation system to annotate predicates with a WordNet sense and hyponyms of these predicates are then assumed to evoke the same frame. Johansson and Nugues (2007) treat this problem as an instance of supervised classification. Using a feature representation based also on WordNet, they learn a classifier for each frame which decides whether an unseen word belongs to the frame or not. Pennacchiotti et al. (2008) create “distributional profiles” for frames. Each frame is represented as a vector, the (weighted) centroid of the vectors representing the meaning of the predi</context>
<context position="15932" citStr="Burchardt et al., 2005" startWordPosition="2606" endWordPosition="2609"> |E(N) |(3) A trivial alignment of a seed with itself where all semantic and syntactic scores are 1 will thus receive a score of: |M |• 1+α• |E(M)|.1 = 1 (4) VI VI |M|2 +α E(M)2 which is the largest possible similarity score. The lowest possible score is obviously 0, assuming that the semantic and syntactic scores cannot be negative. Considerable latitude is available in selecting the semantic and syntactic similarity measures. With regard to semantic similarity, WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet (Pennacchiotti et al., 2008; Burchardt et al., 2005; Johansson and Nugues, 2007). Syntactic similarity may be operationalized in many ways, for example by taking account a hierarchy of grammatical relations (Keenan and Comrie, 1977). Our experiments employed relatively simple instantiations of these measures. We did not make use of WordNet, as we were interested in exploring the setting where WordNet is not available or has limited coverage. Therefore, we approximate the semantic similarity between two nodes via distributional similarity. We present the details of the semantic space model we used in Section 4. If n and n&apos; are both nouns, verbs</context>
</contexts>
<marker>Burchardt, Erk, Frank, 2005</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005. A WordNet Detour to FrameNet. In Proceedings of the GLDV 200 Workshop GermaNet II, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="23811" citStr="Fan et al., 2008" startWordPosition="3967" endWordPosition="3970">st similar frame candidates (again similarity is measured via cosine). Our experiments explored different values of k ranging from 1 to 10. Evaluation Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method. The labeler followed closely the implementation described in Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained2 with the LIBLINEAR library (Fan et al., 2008) and learned to predict the frame name, role spans, and role labels. We followed 2The regularization parameter C was set to 0.1. 16 Figure 3: Frame labeling accuracy on high, medium and low frequency verbs, before and after applying our expansion method; the labeler decides among k = 1,...,10 candidate frames. Figure 4: Role labeling F1 for high, medium, and low frequency verbs (roles of mislabeled frames are counted as wrong); the labeler decides among k = 1,...,10 candidate frames. 5 Results the one-versus-one strategy for multi-class classification (Friedman, 1996). Specifically, the labele</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--235</pages>
<contexts>
<context position="1773" citStr="Fillmore et al., 2003" startWordPosition="249" endWordPosition="252">yed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually based on supervised learning techniques and thus require training on role-annotated data. Examples of the training instances provided in FrameNet are given below: (1) a. If [you]Agent [carelessly]Manner chance going back there, you deserve what you get. b. Only [one winner]Buyer purchased [the paintings]Goods c. [Rachel]Agent injured [her friend]Victim [by closing the car door on his left hand]Means. Each verb in the example sentences </context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam R. L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Another approach to polychotomous classification.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Department of Statistics, Stanford University.</institution>
<contexts>
<context position="24385" citStr="Friedman, 1996" startWordPosition="4062" endWordPosition="4063"> the LIBLINEAR library (Fan et al., 2008) and learned to predict the frame name, role spans, and role labels. We followed 2The regularization parameter C was set to 0.1. 16 Figure 3: Frame labeling accuracy on high, medium and low frequency verbs, before and after applying our expansion method; the labeler decides among k = 1,...,10 candidate frames. Figure 4: Role labeling F1 for high, medium, and low frequency verbs (roles of mislabeled frames are counted as wrong); the labeler decides among k = 1,...,10 candidate frames. 5 Results the one-versus-one strategy for multi-class classification (Friedman, 1996). Specifically, the labeler was trained on the train seen data set without any access to training instances representative of the “unknown” verbs in test unseen. We then trained the labeler on a larger set containing train seen and new training examples obtained with our method. To do this, we used train seen as the seed corpus and the BNC as the expansion corpus. For each “unknown” verb in train unseen we obtained BNC sentences with annotations projected from their most similar seeds. The quality of these sentences as training instances varies depending on their similarity to the seed. In our</context>
</contexts>
<marker>Friedman, 1996</marker>
<rawString>Jerome H. Friedman. 1996. Another approach to polychotomous classification. Technical report, Department of Statistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Semisupervised semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>220--228</pages>
<location>Athens, Greece.</location>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>Hagen F¨urstenau and Mirella Lapata. 2009. Semisupervised semantic role labeling. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–228, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
</authors>
<title>Enriching frame semantic resources with dependency graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Language Resources and Evaluation Conference,</booktitle>
<pages>1478--1484</pages>
<location>Marrakech, Morocco.</location>
<marker>F¨urstenau, 2008</marker>
<rawString>Hagen F¨urstenau. 2008. Enriching frame semantic resources with dependency graphs. In Proceedings of the 6th Language Resources and Evaluation Conference, pages 1478–1484, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>A FrameNet-based semantic role labeler for Swedish.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>436--443</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6138" citStr="Johansson and Nugues, 2006" startWordPosition="929" endWordPosition="932">roduce are useful for semantic role labeling. In the following section we provide an overview of related work. Next, we describe our graphalignment model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results. 2 Related Work Much previous work has focused on creating FrameNet-style annotations for languages other than English. A common strategy is to exploit parallel corpora and transfer annotations from English sentences onto their translations (Pad´o and Lapata, 2006; Johansson and Nugues, 2006). Other work attempts to automatically augment the English FrameNet in a monolingual setting either by extending its coverage or by creating additional training data. There has been growing interest recently in determining the frame membership for unknown predicates. This is a challenging task, FrameNet currently lists 502 frames with example sentences which are simply too many (potentially related) classes to consider for a hypothetical system. Moreover, predicates may have to be assigned to multiple frames, on account of lexical ambiguity. Previous work has mainly used WordNet (Fellbaum, 199</context>
</contexts>
<marker>Johansson, Nugues, 2006</marker>
<rawString>Richard Johansson and Pierre Nugues. 2006. A FrameNet-based semantic role labeler for Swedish. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436–443, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Using WordNet to extend FrameNet coverage.</title>
<date>2007</date>
<booktitle>FRAME 2007: Building Frame Semantics Resources for Scandinavian and Baltic Languages,</booktitle>
<pages>27--30</pages>
<editor>In Richard Johansson and Pierre Nugues, editors,</editor>
<location>Tartu, Estonia.</location>
<contexts>
<context position="6985" citStr="Johansson and Nugues (2007)" startWordPosition="1059" endWordPosition="1062">ng the frame membership for unknown predicates. This is a challenging task, FrameNet currently lists 502 frames with example sentences which are simply too many (potentially related) classes to consider for a hypothetical system. Moreover, predicates may have to be assigned to multiple frames, on account of lexical ambiguity. Previous work has mainly used WordNet (Fellbaum, 1998) to extend FrameNet. For example, Burchardt et al. (2005) apply a word sense disambiguation system to annotate predicates with a WordNet sense and hyponyms of these predicates are then assumed to evoke the same frame. Johansson and Nugues (2007) treat this problem as an instance of supervised classification. Using a feature representation based also on WordNet, they learn a classifier for each frame which decides whether an unseen word belongs to the frame or not. Pennacchiotti et al. (2008) create “distributional profiles” for frames. Each frame is represented as a vector, the (weighted) centroid of the vectors representing the meaning of the predicates it evokes. Unknown predicates are then assigned to the most similar frame. They also propose a WordNet-based model that computes the similarity between the synsets representing an un</context>
<context position="15961" citStr="Johansson and Nugues, 2007" startWordPosition="2610" endWordPosition="2614">ignment of a seed with itself where all semantic and syntactic scores are 1 will thus receive a score of: |M |• 1+α• |E(M)|.1 = 1 (4) VI VI |M|2 +α E(M)2 which is the largest possible similarity score. The lowest possible score is obviously 0, assuming that the semantic and syntactic scores cannot be negative. Considerable latitude is available in selecting the semantic and syntactic similarity measures. With regard to semantic similarity, WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet (Pennacchiotti et al., 2008; Burchardt et al., 2005; Johansson and Nugues, 2007). Syntactic similarity may be operationalized in many ways, for example by taking account a hierarchy of grammatical relations (Keenan and Comrie, 1977). Our experiments employed relatively simple instantiations of these measures. We did not make use of WordNet, as we were interested in exploring the setting where WordNet is not available or has limited coverage. Therefore, we approximate the semantic similarity between two nodes via distributional similarity. We present the details of the semantic space model we used in Section 4. If n and n&apos; are both nouns, verbs or adjectives, we set: sem(n</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Using WordNet to extend FrameNet coverage. In Richard Johansson and Pierre Nugues, editors, FRAME 2007: Building Frame Semantics Resources for Scandinavian and Baltic Languages, pages 27–30, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>The effect of syntactic representation on semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>393--400</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="23567" citStr="Johansson and Nugues (2008)" startWordPosition="3930" endWordPosition="3933">sing the cosine of the angle between the vectors representing any two words. The same semantic space was used to create the distributional profile of a frame (which is the centroid of the vectors of its verbs). For each unknown verb, we consider the k most similar frame candidates (again similarity is measured via cosine). Our experiments explored different values of k ranging from 1 to 10. Evaluation Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method. The labeler followed closely the implementation described in Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained2 with the LIBLINEAR library (Fan et al., 2008) and learned to predict the frame name, role spans, and role labels. We followed 2The regularization parameter C was set to 0.1. 16 Figure 3: Frame labeling accuracy on high, medium and low frequency verbs, before and after applying our expansion method; the labeler decides among k = 1,...,10 candidate frames. Figure 4: Role labeling F1 for high, medium,</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 393–400, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Keenan</author>
<author>B Comrie</author>
</authors>
<title>Noun phrase accessibility and universal grammar. Linguistic Inquiry,</title>
<date>1977</date>
<pages>8--62</pages>
<contexts>
<context position="16113" citStr="Keenan and Comrie, 1977" startWordPosition="2634" endWordPosition="2637">hich is the largest possible similarity score. The lowest possible score is obviously 0, assuming that the semantic and syntactic scores cannot be negative. Considerable latitude is available in selecting the semantic and syntactic similarity measures. With regard to semantic similarity, WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet (Pennacchiotti et al., 2008; Burchardt et al., 2005; Johansson and Nugues, 2007). Syntactic similarity may be operationalized in many ways, for example by taking account a hierarchy of grammatical relations (Keenan and Comrie, 1977). Our experiments employed relatively simple instantiations of these measures. We did not make use of WordNet, as we were interested in exploring the setting where WordNet is not available or has limited coverage. Therefore, we approximate the semantic similarity between two nodes via distributional similarity. We present the details of the semantic space model we used in Section 4. If n and n&apos; are both nouns, verbs or adjectives, we set: sem(n,n&apos;) := cos(~vn,~vn&apos;) (5) where ~vn and ~vn&apos; are the vectors representing the lemmas of n and n&apos; respectively. If n and n&apos; 14 Impactor Impact— Impactee </context>
</contexts>
<marker>Keenan, Comrie, 1977</marker>
<rawString>E. Keenan and B. Comrie. 1977. Noun phrase accessibility and universal grammar. Linguistic Inquiry, 8:62–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunnar W Klau</author>
</authors>
<title>A new graph-based method for pairwise global network alignment.</title>
<date>2009</date>
<journal>BMC Bioinformatics,</journal>
<volume>10</volume>
<contexts>
<context position="5098" citStr="Klau, 2009" startWordPosition="767" endWordPosition="768">or classifier training. We assume that an initial set of labeled examples is available. Then, faced with an unknown predicate, i.e., a predicate that does not evoke any frame according to the FrameNet database, we must decide (a) which frames it belongs to and (b) how to automatically annotate example sentences containing the predicate. We solve both problems jointly, using a graph alignment algorithm. Specifically, we view the task of inferring annotations for new verbs as an instance of a structural matching problem and follow a graph-based formulation for pairwise global network alignment (Klau, 2009). Labeled and unlabeled sentences are represented as dependencygraphs; we formulate the search for an optimal alignment as an integer linear program where different graph alignments are scored using a function based on semantic and structural similarity. We evaluate our algorithm in two ways. We assess how accurate it is in predicting the frame for an unknown verb and also evaluate whether the annotations we produce are useful for semantic role labeling. In the following section we provide an overview of related work. Next, we describe our graphalignment model in more detail (Section 3) and pr</context>
<context position="18157" citStr="Klau, 2009" startWordPosition="3025" endWordPosition="3026">he problem of finding the best alignment according to the scoring function presented in Equation (2) can be formulated as an integer linear program. Let the binary variables xik indicate whether node ni of graph s is aligned to node nk of graph g. Since it is not only nodes but also graph edges that must be aligned we further introduce binary variables yijkl, where yijkl = 1 indicates that the edge between nodes ni and nj of graph s is aligned to the edge between nodes nk and nl of graph g. This follows a general formulation of the graph alignment problem based on maximum structural matching (Klau, 2009). In order for the xik and yijkl variables to represent a valid alignment, the following constraints must hold: 1. Each node of s is aligned to at most one node of g: Ykxik &lt; 1 2. Each node of g is aligned to at most one node of s: Yi xik &lt; 1 3. Two edges may only be aligned if their adjacent nodes are aligned: yijkl &lt; xik and yijkl &lt; xjl The scoring function then becomes: syn (rnn;, rnnk) yijkl (7) We solve this optimization problem with a version of the branch-and-bound algorithm (Land and Doig, 1960). In general, this graph alignment problem is NP-hard (Klau, 2009) and usually solved approx</context>
</contexts>
<marker>Klau, 2009</marker>
<rawString>Gunnar W. Klau. 2009. A new graph-based method for pairwise global network alignment. BMC Bioinformatics, 10 (Suppl 1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A H Land</author>
<author>A G Doig</author>
</authors>
<title>An automatic method for solving discrete programming problems.</title>
<date>1960</date>
<journal>Econometrica,</journal>
<pages>28--497</pages>
<contexts>
<context position="18665" citStr="Land and Doig, 1960" startWordPosition="3123" endWordPosition="3126">s follows a general formulation of the graph alignment problem based on maximum structural matching (Klau, 2009). In order for the xik and yijkl variables to represent a valid alignment, the following constraints must hold: 1. Each node of s is aligned to at most one node of g: Ykxik &lt; 1 2. Each node of g is aligned to at most one node of s: Yi xik &lt; 1 3. Two edges may only be aligned if their adjacent nodes are aligned: yijkl &lt; xik and yijkl &lt; xjl The scoring function then becomes: syn (rnn;, rnnk) yijkl (7) We solve this optimization problem with a version of the branch-and-bound algorithm (Land and Doig, 1960). In general, this graph alignment problem is NP-hard (Klau, 2009) and usually solved approximately following a procedure similar to beam search. However, the special structure of constraints 1 to 3, originating from the required injectivity of the alignment function, allows us to solve the optimization exactly. Our implementation of the branch-and-bound algorithm does not generally run in polynomial time, however, we found that in practice we could efficiently compute optimal alignments in almost all cases (less than 0.1% of alignment pairs in our data could not be solved in reasonable time).</context>
</contexts>
<marker>Land, Doig, 1960</marker>
<rawString>A.H. Land and A.G. Doig. 1960. An automatic method for solving discrete programming problems. Econometrica, 28:497–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabor Melli</author>
<author>Yang Wang</author>
<author>Yurdong Liu</author>
<author>Mehdi M Kashani</author>
<author>Zhongmin Shi</author>
<author>Baohua Gu</author>
<author>Anoop Sarkar</author>
<author>Fred Popowich</author>
</authors>
<title>Description of SQUASH, the SFU question answering summary handler for the duc-2005 summarization task.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT/EMNLP Document Understanding Workshop,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1646" citStr="Melli et al., 2005" startWordPosition="226" endWordPosition="229">tically by our method. 1 Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually based on supervised learning techniques and thus require training on role-annotated data. Examples of the training instances provided in FrameNet are given below: (1) a. If [you]Agent [carelessly]Manner chance going back there, you deserve what you get. b. Only [one winner]Buyer purchased [the paintings]Goods c</context>
</contexts>
<marker>Melli, Wang, Liu, Kashani, Shi, Gu, Sarkar, Popowich, 2005</marker>
<rawString>Gabor Melli, Yang Wang, Yurdong Liu, Mehdi M. Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar, and Fred Popowich. 2005. Description of SQUASH, the SFU question answering summary handler for the duc-2005 summarization task. In Proceedings of the HLT/EMNLP Document Understanding Workshop, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>693--701</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1572" citStr="Narayanan and Harabagiu, 2004" startWordPosition="216" endWordPosition="219">ing performance for unknown lexical items improves with training data produced automatically by our method. 1 Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually based on supervised learning techniques and thus require training on role-annotated data. Examples of the training instances provided in FrameNet are given below: (1) a. If [you]Agent [carelessly]Manner chance going back there, you deserv</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question answering based on semantic structures. In Proceedings of the 20th International Conference on Computational Linguistics, pages 693–701, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Noreen</author>
</authors>
<title>Computer-intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley and Sons Inc.</publisher>
<contexts>
<context position="29610" citStr="Noreen, 1989" startWordPosition="4955" endWordPosition="4956">r when choosing between the same k &gt; 1 candidates are significant according to McNemar’s test (p &lt; .05). The best frame labeling accuracy (26.3%) is achieved by the expanded classifier when deciding among k = 6 candidate frames. This is significantly better (p &lt; .01) than the best performance of the unexpanded classifier (25.0%), which is achieved at k = 2. Role labeling results follow a similar pattern. The best expanded classifier (F1=14.9% at k = 6) outperforms the best unexpanded one (F1=14.1% at k = 2). The difference in performance as significant at p &lt; 0.05, using stratified shuffling (Noreen, 1989). 6 Conclusions This paper presents a novel semi-supervised approach for reducing the annotation effort involved in creating resources for semantic role labeling. Our method acquires training instances for unknown verbs (i.e., verbs that are not evoked by existing FrameNet frames) from an unlabeled corpus. A key assumption underlying our work is that verbs with similar meanings will have similar argument structures. Our task then amounts to finding the seen instances that resemble the unseen instances most, and projecting their annotations. We represent this task as a graph alignment problem, </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. Noreen. 1989. Computer-intensive Methods for Testing Hypotheses: An Introduction. John Wiley and Sons Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Optimal constituent alignment with edge covers for semantic projection.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1161--1168</pages>
<location>Sydney, Australia.</location>
<marker>Pad´o, Lapata, 2006</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2006. Optimal constituent alignment with edge covers for semantic projection. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1161–1168, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Diego De Cao</author>
<author>Roberto Basili</author>
<author>Danilo Croce</author>
<author>Michael Roth</author>
</authors>
<title>Automatic induction of FrameNet lexical units.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>457--465</pages>
<location>Honolulu, Hawaii.</location>
<marker>Pennacchiotti, De Cao, Basili, Croce, Roth, 2008</marker>
<rawString>Marco Pennacchiotti, Diego De Cao, Roberto Basili, Danilo Croce, and Michael Roth. 2008. Automatic induction of FrameNet lexical units. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 457–465, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1520" citStr="Surdeanu et al., 2003" startWordPosition="210" endWordPosition="213">g. Experimental results show that role labeling performance for unknown lexical items improves with training data produced automatically by our method. 1 Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually based on supervised learning techniques and thus require training on role-annotated data. Examples of the training instances provided in FrameNet are given below: (1) a. If [you]Agent [c</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8–15, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>