<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.978316">
Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems
</title>
<author confidence="0.996437">
Simone Paolo Ponzetto Roberto Navigli
</author>
<affiliation confidence="0.996412">
Department of Computational Linguistics Dipartimento di Informatica
Heidelberg University Sapienza Universit`a di Roma
</affiliation>
<email confidence="0.98294">
ponzetto@cl.uni-heidelberg.de navigli@di.uniroma1.it
</email>
<sectionHeader confidence="0.993393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998860666666667">
One of the main obstacles to high-
performance Word Sense Disambigua-
tion (WSD) is the knowledge acquisi-
tion bottleneck. In this paper, we present
a methodology to automatically extend
WordNet with large amounts of seman-
tic relations from an encyclopedic re-
source, namely Wikipedia. We show
that, when provided with a vast amount
of high-quality semantic relations, sim-
ple knowledge-lean disambiguation algo-
rithms compete with state-of-the-art su-
pervised WSD systems in a coarse-grained
all-words setting and outperform them on
gold-standard domain-specific datasets.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982122807018">
Knowledge lies at the core of Word Sense Dis-
ambiguation (WSD), the task of computation-
ally identifying the meanings of words in context
(Navigli, 2009b). In the recent years, two main
approaches have been studied that rely on a fixed
sense inventory, i.e., supervised and knowledge-
based methods. In order to achieve high perfor-
mance, supervised approaches require large train-
ing sets where instances (target words in con-
text) are hand-annotated with the most appropri-
ate word senses. Producing this kind of knowl-
edge is extremely costly: at a throughput of one
sense annotation per minute (Edmonds, 2000)
and tagging one thousand examples per word,
dozens of person-years would be required for en-
abling a supervised classifier to disambiguate all
the words in the English lexicon with high accu-
racy. In contrast, knowledge-based approaches ex-
ploit the information contained in wide-coverage
lexical resources, such as WordNet (Fellbaum,
1998). However, it has been demonstrated that
the amount of lexical and semantic information
contained in such resources is typically insuffi-
cient for high-performance WSD (Cuadros and
Rigau, 2006). Several methods have been pro-
posed to automatically extend existing resources
(cf. Section 2) and it has been shown that highly-
interconnected semantic networks have a great im-
pact on WSD (Navigli and Lapata, 2010). How-
ever, to date, the real potential of knowledge-rich
WSD systems has been shown only in the presence
of either a large manually-developed extension of
WordNet (Navigli and Velardi, 2005) or sophisti-
cated WSD algorithms (Agirre et al., 2009).
The contributions of this paper are two-fold.
First, we relieve the knowledge acquisition bot-
tleneck by developing a methodology to extend
WordNet with millions of semantic relations. The
relations are harvested from an encyclopedic re-
source, namely Wikipedia. Wikipedia pages are
automatically associated with WordNet senses,
and topical, semantic associative relations from
Wikipedia are transferred to WordNet, thus pro-
ducing a much richer lexical resource. Sec-
ond, two simple knowledge-based algorithms that
exploit our extended WordNet are applied to
standard WSD datasets. The results show that
the integration of vast amounts of semantic re-
lations in knowledge-based systems yields per-
formance competitive with state-of-the-art super-
vised approaches on open-text WSD. In addition,
we support previous findings from Agirre et al.
(2009) that in a domain-specific WSD scenario
knowledge-based systems perform better than su-
pervised ones, and we show that, given enough
knowledge, simple algorithms perform better than
more sophisticated ones.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999585">
In the last three decades, a large body of work
has been presented that concerns the develop-
ment of automatic methods for the enrichment of
existing resources such as WordNet. These in-
</bodyText>
<page confidence="0.947463">
1522
</page>
<note confidence="0.9427745">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999959180722892">
clude proposals to extract semantic information
from dictionaries (e.g. Chodorow et al. (1985)
and Rigau et al. (1998)), approaches using lexico-
syntactic patterns (Hearst, 1992; Cimiano et al.,
2004; Girju et al., 2006), heuristic methods based
on lexical and semantic regularities (Harabagiu et
al., 1999), taxonomy-based ontologization (Pen-
nacchiotti and Pantel, 2006; Snow et al., 2006).
Other approaches include the extraction of seman-
tic preferences from sense-annotated (Agirre and
Martinez, 2001) and raw corpora (McCarthy and
Carroll, 2003), as well as the disambiguation of
dictionary glosses based on cyclic graph patterns
(Navigli, 2009a). Other works rely on the dis-
ambiguation of collocations, either obtained from
specialized learner’s dictionaries (Navigli and Ve-
lardi, 2005) or extracted by means of statistical
techniques (Cuadros and Rigau, 2008), e.g. based
on the method proposed by Agirre and de Lacalle
(2004). But while most of these methods represent
state-of-the-art proposals for enriching lexical and
taxonomic resources, none concentrates on aug-
menting WordNet with associative semantic rela-
tions for many domains on a very large scale. To
overcome this limitation, we exploit Wikipedia, a
collaboratively generated Web encyclopedia.
The use of collaborative contributions from vol-
unteers has been previously shown to be beneficial
in the Open Mind Word Expert project (Chklovski
and Mihalcea, 2002). However, its current status
indicates that the project remains a mainly aca-
demic attempt. In contrast, due to its low en-
trance barrier and vast user base, Wikipedia pro-
vides large amounts of information at practically
no cost. Previous work aimed at transforming
its content into a knowledge base includes open-
domain relation extraction (Wu and Weld, 2007),
the acquisition of taxonomic (Ponzetto and Strube,
2007a; Suchanek et al., 2008; Wu and Weld, 2008)
and other semantic relations (Nastase and Strube,
2008), as well as lexical reference rules (Shnarch
et al., 2009). Applications using the knowledge
contained in Wikipedia include, among others,
text categorization (Gabrilovich and Markovitch,
2006), computing semantic similarity of texts
(Gabrilovich and Markovitch, 2007; Ponzetto and
Strube, 2007b; Milne and Witten, 2008a), coref-
erence resolution (Ponzetto and Strube, 2007b),
multi-document summarization (Nastase, 2008),
and text generation (Sauper and Barzilay, 2009).
In our work we follow this line of research and
show that knowledge harvested from Wikipedia
can be used effectively to improve the perfor-
mance of a WSD system. Our proposal builds on
previous insights from Bunescu and Pas¸ca (2006)
and Mihalcea (2007) that pages in Wikipedia can
be taken as word senses. Mihalcea (2007) manu-
ally maps Wikipedia pages to WordNet senses to
perform lexical-sample WSD. We extend her pro-
posal in three important ways: (1) we fully autom-
atize the mapping between Wikipedia pages and
WordNet senses; (2) we use the mappings to en-
rich an existing resource, i.e. WordNet, rather than
annotating text with sense labels; (3) we deploy
the knowledge encoded by this mapping to per-
form unrestricted WSD, rather than apply it to a
lexical sample setting.
Knowledge from Wikipedia is injected into a
WSD system by means of a mapping to Word-
Net. Previous efforts aimed at automatically link-
ing Wikipedia to WordNet include full use of the
first WordNet sense heuristic (Suchanek et al.,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), a model based on vector spaces (Ruiz-
Casado et al., 2005) and a supervised approach
using keyword extraction (Reiter et al., 2008).
These latter methods rely only on text overlap
techniques and neither they take advantage of the
input from Wikipedia being semi-structured, e.g.
hyperlinked, nor they propose a high-performing
probabilistic formulation of the mapping problem,
a task to which we turn in the next section.
</bodyText>
<sectionHeader confidence="0.978988" genericHeader="method">
3 Extending WordNet
</sectionHeader>
<bodyText confidence="0.999977">
Our approach consists of two main phases: first,
a mapping is automatically established between
Wikipedia pages and WordNet senses; second, the
relations connecting Wikipedia pages are trans-
ferred to WordNet. As a result, an extended ver-
sion of WordNet is produced, that we call Word-
Net++. We present the two resources used in our
methodology in Section 3.1. Sections 3.2 and 3.3
illustrate the two phases of our approach.
</bodyText>
<subsectionHeader confidence="0.998894">
3.1 Knowledge Resources
</subsectionHeader>
<bodyText confidence="0.998569">
WordNet. Being the most widely used compu-
tational lexicon of English in Natural Language
Processing, WordNet is an essential resource for
WSD. A concept in WordNet is represented as a
synonym set, or synset, i.e. the set of words which
share a common meaning. For instance, the con-
</bodyText>
<page confidence="0.950662">
1523
</page>
<bodyText confidence="0.980113037037037">
cept of soda drink is expressed as:
{ popn, soda��, soda pops, soda water��, tonic�� }
where each word’s subscripts and superscripts in-
dicate their parts of speech (e.g. n stands for noun)
and sense number1, respectively. For each synset,
WordNet provides a textual definition, or gloss.
For example, the gloss of the above synset is: “a
sweet drink containing carbonated water and fla-
voring”.
Wikipedia. Our second resource, Wikipedia, is
a collaborative Web encyclopedia composed of
pages2. A Wikipedia page (henceforth, Wikipage)
presents the knowledge about a specific concept
(e.g. SODA (SOFT DRINK)) or named entity (e.g.
FOOD STANDARDS AGENCY). The page typi-
cally contains hypertext linked to other relevant
Wikipages. For instance, SODA (SOFT DRINK)
is linked to COLA, FLAVORED WATER, LEMON-
ADE, and many others. The title of a Wikipage
(e.g. SODA (SOFT DRINK)) is composed of the
lemma of the concept defined (e.g. soda) plus
an optional label in parentheses which specifies
its meaning in case the lemma is ambiguous
(e.g. SOFT DRINK vs. SODIUM CARBONATE). Fi-
nally, some Wikipages are redirections to other
pages, e.g. SODA (SODIUM CARBONATE) redirects
to SODIUM CARBONATE.
</bodyText>
<subsectionHeader confidence="0.999871">
3.2 Mapping Wikipedia to WordNet
</subsectionHeader>
<bodyText confidence="0.9999622">
During the first phase of our methodology we aim
to establish links between Wikipages and Word-
Net senses. Formally, given the entire set of pages
SensesWiki and WordNet senses SensesWN, we aim
to acquire a mapping:
</bodyText>
<equation confidence="0.9761422">
µ : SensesWiki → SensesWN,
such that, for each Wikipage w E SensesWiki:
I s E SensesWN(w) if a link can be
established,
e otherwise,
</equation>
<bodyText confidence="0.98281">
where SensesWN(w) is the set of senses of the
lemma of w in WordNet. For example, if our
</bodyText>
<footnote confidence="0.998565625">
1We use WordNet version 3.0. We use word senses to un-
ambiguously denote the corresponding synsets (e.g. planes
for { airplanes, aeroplane��, plane�� }).
2http://download.wikipedia.org. We use the
English Wikipedia database dump from November 3, 2009,
which includes 3,083,466 articles. Throughout this paper, we
use Sans Serif for words, SMALL CAPS for Wikipedia pages
and CAPITALS for Wikipedia categories.
</footnote>
<bodyText confidence="0.974742666666666">
mapping methodology linked SODA (SOFT DRINK)
to the corresponding WordNet sense soda��, we
would have µ(SODA (SOFT DRINK)) = sodan.
In order to establish a mapping between the
two resources, we first identify different kinds of
disambiguation contexts for Wikipages (Section
3.2.1) and WordNet senses (Section 3.2.2). Next,
we intersect these contexts to perform the mapping
(see Section 3.2.3).
</bodyText>
<subsectionHeader confidence="0.810153">
3.2.1 Disambiguation Context of a Wikipage
</subsectionHeader>
<bodyText confidence="0.99578">
Given a target Wikipage w which we aim to map
to a WordNet sense of w, we use the following
information as a disambiguation context:
</bodyText>
<listItem confidence="0.988099210526316">
• Sense labels: e.g. given the page SODA (SOFT
DRINK), the words soft and drink are added to
the disambiguation context.
• Links: the titles’ lemmas of the pages linked
from the Wikipage w (outgoing links). For in-
stance, the links in the Wikipage SODA (SOFT
DRINK) include soda, lemonade, sugar, etc.
• Categories: Wikipages are classified accord-
ing to one or more categories, which repre-
sent meta-information used to categorize them.
For instance, the Wikipage SODA (SOFT DRINK)
is categorized as SOFT DRINKS. Since many
categories are very specific and do not appear in
WordNet (e.g., SWEDISH WRITERS or SCI-
ENTISTS WHO COMMITTED SUICIDE),
we use the lemmas of their syntactic heads as
disambiguation context (i.e. writer and scien-
tist). To this end, we use the category heads
provided by Ponzetto and Navigli (2009).
</listItem>
<bodyText confidence="0.988128">
Given a Wikipage w, we define its disambiguation
context Ctx(w) as the set of words obtained from
some or all of the three sources above.
</bodyText>
<subsectionHeader confidence="0.7147145">
3.2.2 Disambiguation Context of a WordNet
Sense
</subsectionHeader>
<bodyText confidence="0.9153305">
Given a WordNet sense s and its synset S, we use
the following information as disambiguation con-
text to provide evidence for a potential link in our
mapping µ:
</bodyText>
<listItem confidence="0.95674225">
• Synonymy: all synonyms of s in synset S. For
instance, given the synset of soda��, all its syn-
onyms are included in the context (that is, tonic,
soda pop, pop, etc.).
</listItem>
<equation confidence="0.973684">
µ(w) =
</equation>
<page confidence="0.932355">
1524
</page>
<listItem confidence="0.976319315789474">
• Hypernymy/Hyponymy: all synonyms in the
synsets H such that H is either a hypernym
(i.e., a generalization) or a hyponym (i.e., a spe-
cialization) of S. For example, given soda2n,
we include the words from its hypernym { soft
drink1n }.
• Sisterhood: words from the sisters of S. A sister
synset S&apos; is such that S and S&apos; have a common
direct hypernym. For example, given soda2n, it
can be found that bitter lemon1 n and soda2n are
sisters. Thus the words bitter and lemon are in-
cluded in the disambiguation context of s.
• Gloss: the set of lemmas of the content words
occurring within the gloss of s. For instance,
given s = soda2n, defined as “a sweet drink
containing carbonated water and flavoring”, we
add to the disambiguation context of s the fol-
lowing lemmas: sweet, drink, contain, carbon-
ated, water, flavoring.
</listItem>
<bodyText confidence="0.853343666666667">
Given a WordNet sense s, we define its disam-
biguation context Ctx(s) as the set of words ob-
tained from some or all of the four sources above.
</bodyText>
<subsectionHeader confidence="0.985758">
3.2.3 Mapping Algorithm
</subsectionHeader>
<bodyText confidence="0.9999325">
In order to link each Wikipedia page to a Word-
Net sense, we developed a novel algorithm, whose
pseudocode is shown in Algorithm 1. The follow-
ing steps are performed:
</bodyText>
<listItem confidence="0.959612888888889">
• Initially (lines 1-2), our mapping µ is empty, i.e.
it links each Wikipage w to e.
• For each Wikipage w whose lemma is monose-
mous both in Wikipedia and WordNet (i.e.
|SensesWiki(w) |= |SensesWN(w) |= 1) we map
w to its only WordNet sense w1n (lines 3-5).
• Finally, for each remaining Wikipage w for
which no mapping was previously found (i.e.,
µ(w) = e, line 7), we do the following:
</listItem>
<bodyText confidence="0.998797">
– lines 8-10: for each Wikipage d which is a
redirection to w, for which a mapping was
previously found (i.e. µ(d) =� e, that is, d is
monosemous in both Wikipedia and Word-
Net) and such that it maps to a sense µ(d) in
a synset S that also contains a sense of w, we
map w to the corresponding sense in S.
– lines 11-14: if a Wikipage w has not been
linked yet, we assign the most likely sense
to w based on the maximization of the con-
ditional probabilities p(s|w) over the senses
</bodyText>
<construct confidence="0.481648">
Algorithm 1 The mapping algorithm
Input: SensesWiki, SensesWN
Output: a mapping µ : SensesWiki --+ SensesWN
</construct>
<listItem confidence="0.973647818181818">
1: for each w E SensesWiki
2: µ(w) := E
3: for each w E SensesWiki
4: if |SensesWiki�w) |= |SensesWN (w) |= 1 then
5: µ(w) := wn
6: for each w E SensesWiki
7: if µ(w) = E then
8: for each d E SensesWiki s.t. d redirects to w
9: if µ(d) =� E and µ(d) is in a synset of w then
10: µ(w) := sense of w in synset of µ(d); break
11: for each w E SensesWiki
</listItem>
<equation confidence="0.849012833333333">
12: if µ(w) = E then
13: if no tie occurs then
14: µ(w) := argmax
sESensesWN(w)
15: return µ
s E SensesWN(w) (no mapping is established
</equation>
<bodyText confidence="0.994784875">
if a tie occurs, line 13).
As a result of the execution of the algorithm, the
mapping µ is returned (line 15). At the heart of the
mapping algorithm lies the calculation of the con-
ditional probability p(s|w) of selecting the Word-
Net sense s given the Wikipage w. The sense s
which maximizes this probability can be obtained
as follows:
</bodyText>
<equation confidence="0.991767">
µ(w) = argmax
sESensesWN(w)
= argmax p(s, w)
s
</equation>
<bodyText confidence="0.999847833333333">
The latter formula is obtained by observing that
p(w) does not influence our maximization, as it is
a constant independent of s. As a result, the most
appropriate sense s is determined by maximizing
the joint probability p(s, w) of sense s and page w.
We estimate p(s, w) as:
</bodyText>
<equation confidence="0.9934725">
score(s, w)
p(s, w) =
s&apos;ESensesWN(w),
w&apos;ESensesWiki(w)
</equation>
<bodyText confidence="0.999352666666667">
where score(s, w) = |Ctx(s)nCtx(w)|+1 (we add
1 as a smoothing factor). Thus, in our algorithm
we determine the best sense s by computing the in-
tersection of the disambiguation contexts of s and
w, and normalizing by the scores summed over all
senses of w in Wikipedia and WordNet.
</bodyText>
<subsectionHeader confidence="0.518587">
3.2.4 Example
</subsectionHeader>
<bodyText confidence="0.999427">
We illustrate the execution of our mapping algo-
rithm by way of an example. Let us focus on the
</bodyText>
<equation confidence="0.8608378">
p(s|w)
p(s|w) = argmax p(s, w)
s p(w)
,
score(s&apos;, w&apos;)
</equation>
<page confidence="0.91449">
1525
</page>
<bodyText confidence="0.998755565217392">
Wikipage SODA (SOFT DRINK). The word soda
is polysemous both in Wikipedia and WordNet,
thus lines 3–5 of the algorithm do not concern
this Wikipage. Lines 6–14 aim to find a mapping
µ(SODA (SOFT DRINK)) to an appropriate WordNet
sense of the word. First, we check whether a redi-
rection exists to SODA (SOFT DRINK) that was pre-
viously disambiguated (lines 8–10). Next, we con-
struct the disambiguation context for the Wikipage
by including words from its label, links and cate-
gories (cf. Section 3.2.1). The context includes,
among others, the following words: soft, drink,
cola, sugar. We now construct the disambiguation
context for the two WordNet senses of soda (cf.
Section 3.2.2), namely the sodium carbonate (#1)
and the drink (#2) senses. To do so, we include
words from their synsets, hypernyms, hyponyms,
sisters, and glosses. The context for soda1 n in-
cludes: salt, acetate, chlorate, benzoate. The
context for soda2 n contains instead: soft, drink,
cola, bitter, etc. The sense with the largest inter-
section is #2, so the following mapping is estab-
lished: µ(SODA (SOFT DRINK)) = soda2n.
</bodyText>
<subsectionHeader confidence="0.998773">
3.3 Transferring Semantic Relations
</subsectionHeader>
<bodyText confidence="0.999873391304348">
The output of the algorithm presented in the previ-
ous section is a mapping between Wikipages and
WordNet senses (that is, implicitly, synsets). Our
insight is to use this alignment to enable the trans-
fer of semantic relations from Wikipedia to Word-
Net. In fact, given a Wikipage w we can collect
all Wikipedia links occurring in that page. For
any such link from w to w&apos;, if the two Wikipages
are mapped to WordNet senses (i.e., µ(w) # c
and µ(w&apos;) # c), we can transfer the correspond-
ing edge (µ(w), µ(w&apos;)) to WordNet. Note that µ(w)
and µ(w&apos;) are noun senses, as Wikipages describe
nominal concepts or named entities. We refer to
this extended resource as WordNet++.
For instance, consider the Wikipage SODA
(SOFT DRINK). This page contains, among oth-
ers, a link to the Wikipage SYRUP. Assuming
µ(SODA (SODA DRINK)) = soda2 n and µ(SYRUP) =
syrup1n, we can add the corresponding semantic
relation (soda2n, syrup1n) to WordNet3.
Thus, WordNet++ represents an extension of
WordNet which includes semantic associative re-
lations between synsets. These are originally
</bodyText>
<footnote confidence="0.91930875">
3Note that such relations are unlabeled. However, for our
purposes this has no impact, since our algorithms do not dis-
tinguish between is-a and other kinds of relations in the lexi-
cal knowledge base (cf. Section 4.2).
</footnote>
<bodyText confidence="0.999531875">
found in Wikipedia and then integrated into Word-
Net by means of our mapping. In turn, Word-
Net++ represents the English-only subset of a
larger multilingual resource, BabelNet (Navigli
and Ponzetto, 2010), where lexicalizations of the
synsets are harvested for many languages using
the so-called Wikipedia inter-language links and
applying a machine translation system.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999731">
We perform two sets of experiments: we first eval-
uate the intrinsic quality of our mapping (Section
4.1) and then quantify the impact of WordNet++
for coarse-grained (Section 4.2) and domain-
specific WSD (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.995139">
4.1 Evaluation of the Mapping
</subsectionHeader>
<bodyText confidence="0.9999795">
Experimental setting. We first conducted an
evaluation of the mapping quality. To create
a gold standard for evaluation, we started from
the set of all lemmas contained both in Word-
Net and Wikipedia: the intersection between the
two resources includes 80,295 lemmas which cor-
respond to 105,797 WordNet senses and 199,735
Wikipedia pages. The average polysemy is 1.3 and
2.5 for WordNet senses and Wikipages, respec-
tively (2.8 and 4.7 when excluding monosemous
words). We selected a random sample of 1,000
Wikipages and asked an annotator with previous
experience in lexicographic annotation to provide
the correct WordNet sense for each page title (an
empty sense label was given if no correct mapping
was possible). 505 non-empty mappings were
found, i.e. Wikipedia pages with a corresponding
WordNet sense. In order to quantify the quality
of the annotations and the difficulty of the task,
a second annotator sense tagged a subset of 200
pages from the original sample. We computed the
inter-annotator agreement using the kappa coeffi-
cient (Carletta, 1996) and found out that our anno-
tators achieved an agreement coefficient r. of 0.9,
indicating almost perfect agreement.
Table 1 summarizes the performance of our dis-
ambiguation algorithm against the manually anno-
tated dataset. Evaluation is performed in terms of
standard measures of precision (the ratio of cor-
rect sense labels to the non-empty labels output
by the mapping algorithm), recall (the ratio of
correct sense labels to the total of non-empty la-
bels in the gold standard) and F1-measure ( 2PR P+R).
We also calculate accuracy, which accounts for
</bodyText>
<page confidence="0.921947">
1526
</page>
<table confidence="0.999735666666667">
P R F1 A
Structure 82.2 68.1 74.5 81.1
Gloss 81.1 64.2 71.7 78.8
Structure + Gloss 81.9 77.5 79.6 84.4
MFS BL 24.3 47.8 32.2 24.3
Random BL 23.8 46.8 31.6 23.9
</table>
<tableCaption confidence="0.999921">
Table 1: Performance of the mapping algorithm.
</tableCaption>
<bodyText confidence="0.996469766666667">
empty sense labels (that is, calculated on all 1,000
test instances). As baseline we use the most fre-
quent WordNet sense (MFS), as well as a ran-
dom sense assignment. We evaluate the map-
ping methodology described in Section 3.2 against
different disambiguation contexts for the Word-
Net senses (cf. Section 3.2.2), i.e. structure-based
(including synonymy, hypernymy/hyponymy and
sisterhood), gloss-derived evidence, and a combi-
nation of the two. As disambiguation context of
a Wikipage (Section 3.2.1) we use all information
available, i.e. sense labels, links and categories4.
Results and discussion. The results show that
our method improves on the baseline by a large
margin and that higher performance can be
achieved by using more disambiguation informa-
tion. That is, using a richer disambiguation con-
text helps to better choose the most appropriate
WordNet sense for a Wikipedia page. The combi-
nation of structural and gloss information attains a
slight variation in terms of precision (−0.3% and
+0.8% compared to Structure and Gloss respec-
tively), but a significantly high increase in recall
(+9.4% and +13.3%). This implies that the differ-
ent disambiguation contexts only partially overlap
and, when used separately, each produces differ-
ent mappings with a similar level of precision. In
the joint approach, the harmonic mean of preci-
sion and recall, i.e. F1, is in fact 5 and 8 points
higher than when separately using structural and
gloss information, respectively.
As for the baselines, the most frequent sense is
just 0.6% and 0.4% above the random baseline in
terms of F1 and accuracy, respectively. A X2 test
reveals in fact no statistically significant difference
at p &lt; 0.05. This is related to the random distri-
bution of senses in our dataset and the Wikipedia
unbiased coverage of WordNet senses. So select-
4We leave out the evaluation of different contexts for a
Wikipage for the sake of brevity. During prototyping we
found that the best results were given by using the largest
context available, as reported in Table 1.
ing the most frequent sense rather than any other
sense for each target page represents a choice as
arbitrary as picking a sense at random.
The final mapping contains 81,533 pairs of
Wikipages and word senses they map to, covering
55.7% of the noun senses in WordNet.
Using our best performing mapping we are
able to extend WordNet with 1,902,859 semantic
edges: of these, 97.93% are deemed novel, i.e. no
direct edge could previously be found between the
synsets. In addition, we performed a stricter eval-
uation of the novelty of our relations by check-
ing whether these can still be found indirectly by
searching for a connecting path between the two
synsets of interest. Here we found that 91.3%,
87.2% and 78.9% of the relations are novel to
WordNet when performing a graph search of max-
imum depth of 2, 3 and 4, respectively.
</bodyText>
<subsectionHeader confidence="0.89155">
4.2 Coarse-grained WSD
</subsectionHeader>
<bodyText confidence="0.9998735">
Experimental setting. We extrinsically evalu-
ate the impact of WordNet++ on the Semeval-
2007 coarse-grained all-words WSD task (Nav-
igli et al., 2007). Performing experiments in a
coarse-grained setting is a natural choice for sev-
eral reasons: first, it has been argued that the fine
granularity of WordNet is one of the main obsta-
cles to accurate WSD (cf. the discussion in Nav-
igli (2009b)); second, the meanings of Wikipedia
pages are intuitively coarser than those in Word-
Net5. For instance, mapping TRAVEL to the first
or the second sense in WordNet is an arbitrary
choice, as the Wikipage refers to both senses. Fi-
nally, given their different nature, WordNet and
Wikipedia do not fully overlap. Accordingly,
we expect the transfer of semantic relations from
Wikipedia to WordNet to have sometimes the side
effect to penalize some fine-grained senses of a
word.
We experiment with two simple knowledge-
based algorithms that are set to perform coarse-
grained WSD on a sentence-by-sentence basis:
</bodyText>
<listItem confidence="0.972277">
• Simplified Extended Lesk (ExtLesk): The first
algorithm is a simplified version of the Lesk
</listItem>
<footnote confidence="0.681416444444444">
5Note that our polysemy rates from Section 4.1 also in-
clude Wikipages whose lemma is contained in WordNet, but
which have out-of-domain meanings, i.e. encyclopedic en-
tries referring to specialized named entities such as e.g., DIS-
COVERY (SPACE SHUTTLE) or FIELD ARTILLERY (MAGA-
ZINE). We computed the polysemy rate for a random sample
of 20 polysemous words by manually removing these NEs
and found that Wikipedia’s polysemy rate is indeed lower
than that of WordNet – i.e. average polysemy of 2.1 vs. 2.8.
</footnote>
<page confidence="0.993736">
1527
</page>
<bodyText confidence="0.999892368421053">
algorithm (Lesk, 1986), that performs WSD
based on the overlap between the context sur-
rounding the target word to be disambiguated
and the definitions of its candidate senses (Kil-
garriff and Rosenzweig, 2000). Given a tar-
get word w, this method assigns to w the
sense whose gloss has the highest overlap (i.e.
most words in common) with the context of w,
namely the set of content words co-occurring
with it in a pre-defined window (a sentence in
our case). Due to the limited context provided
by the WordNet glosses, we follow Banerjee
and Pedersen (2003) and expand the gloss of
each sense s to include words from the glosses
of those synsets in a semantic relation with s.
These include all WordNet synsets which are
directly connected to s, either by means of the
semantic pointers found in WordNet or through
the unlabeled links found in WordNet++.
</bodyText>
<listItem confidence="0.9431489">
• Degree Centrality (Degree): The second algo-
rithm is a graph-based approach that relies on
the notion of vertex degree (Navigli and Lap-
ata, 2010). Starting from each sense s of the tar-
get word, it performs a depth-first search (DFS)
of the WordNet(++) graph and collects all the
paths connecting s to senses of other words in
context. As a result, a sentence graph is pro-
duced. A maximum search depth is established
to limit the size of this graph. The sense of the
</listItem>
<bodyText confidence="0.984020136363636">
target word with the highest vertex degree is se-
lected. We follow Navigli and Lapata (2010)
and run Degree in a weakly supervised setting
where the system attempts no sense assignment
if the highest degree score is below a certain
(empirically estimated) threshold. The optimal
threshold and maximum search depth are es-
timated by maximizing Degree’s Fl on a de-
velopment set of 1,000 randomly chosen noun
instances from the SemCor corpus (Miller et
al., 1993). Experiments on the development
dataset using Degree on WordNet++ revealed
a performance far lower than expected. Error
analysis showed that many instances were in-
correctly disambiguated, due to the noise from
weak semantic links, e.g. the links from SODA
(SOFT DRINK) to EUROPE or AUSTRALIA. Ac-
cordingly, in order to improve the disambigua-
tion performance, we developed a filter to rule
out weak semantic relations from WordNet++.
Given a WordNet++ edge (µ(w), µ(w&apos;)) where
w and w&apos; are both Wikipages and w links to w&apos;,
</bodyText>
<table confidence="0.9996551">
Resource Algorithm Nouns only
P R F1
WordNet ExtLesk 83.6 57.7 68.3
Degree 86.3 65.5 74.5
Wikipedia ExtLesk 82.3 64.1 72.0
Degree 96.2 40.1 57.4
WordNet++ ExtLesk 82.7 69.2 75.4
Degree 87.3 72.7 79.4
MFS BL 77.4 77.4 77.4
Random BL 63.5 63.5 63.5
</table>
<tableCaption confidence="0.8828505">
Table 2: Performance on Semeval-2007 coarse-
grained all-words WSD (nouns only subset).
</tableCaption>
<bodyText confidence="0.999002583333333">
we first collect all words from the category la-
bels of w and w&apos; into two bags of words. We re-
move stopwords and lemmatize the remaining
words. We then compute the degree of overlap
between the two sets of categories as the num-
ber of words in common between the two bags
of words, normalized in the [0, 1] interval. We fi-
nally retain the link for the DFS if such score is
above an empirically determined threshold. The
optimal value for this category overlap thresh-
old was again estimated by maximizing De-
gree’s Fl on the development set. The final
graph used by Degree consists of WordNet, to-
gether with 152,944 relations from our semantic
relation enrichment method (cf. Section 3.3).
Results and discussion. We report our results in
terms of precision, recall and Fl-measure on the
Semeval-2007 coarse-grained all-words dataset
(Navigli et al., 2007). We first evaluated ExtLesk
and Degree using three different resources: (1)
WordNet only; (2) Wikipedia only, i.e. only those
relations harvested from the links found within
Wikipedia pages; (3) their union, i.e. WordNet++.
In Table 2 we report the results on nouns only. As
common practice, we compare with random sense
assignment and the most frequent sense (MFS)
from SemCor as baselines. Enriching WordNet
with encyclopedic relations from Wikipedia yields
a consistent improvement against using WordNet
(+7.1% and +4.9% Fl for ExtLesk and Degree)
or Wikipedia (+3.4% and +22.0%) alone. The
best results are obtained by using Degree with
WordNet++. The better performance of Wikipedia
against WordNet when using ExtLesk (+3.7%)
highlights the quality of the relations extracted.
However, no such improvement is found with De-
</bodyText>
<page confidence="0.997602">
1528
</page>
<tableCaption confidence="0.912957666666667">
Table 3: Performance on Semeval-2007 coarse-
grained all-words WSD with MFS as a back-off
strategy when no sense assignment is attempted.
</tableCaption>
<table confidence="0.999798263157895">
Algorithm Nouns only All words
P/R/F1 P/R/F1
ExtLesk 81.0 79.1
Degree 85.5 81.7
SUSSX-FR 81.1 77.0
TreeMatch N/A 73.6
NUS-PT 82.3 82.5
SSI 84.1 83.2
MFS BL 77.4 78.9
Random BL 63.5 62.7
Algorithm Sports Finance
P/R/F1 P/R/F1
k-NN † 30.3 43.4
Static PR † 20.1 39.6
Personalized PR † 35.6 46.9
ExtLesk 40.1 45.6
Degree 42.0 47.8
MFS BL 19.6 37.1
Random BL 19.5 19.6
</table>
<tableCaption confidence="0.963963666666667">
Table 4: Performance on the Sports and Finance
sections of the dataset from Koeling et al. (2005):
† indicates results from Agirre et al. (2009).
</tableCaption>
<bodyText confidence="0.999978882352941">
gree, due to its lower recall. Interestingly, Degree
on WordNet++ beats the MFS baseline, which is
notably a difficult competitor for unsupervised and
knowledge-lean systems.
We finally compare our two algorithms using
WordNet++ with state-of-the-art WSD systems,
namely the best unsupervised (Koeling and Mc-
Carthy, 2007, SUSSX-FR) and supervised (Chan
et al., 2007, NUS-PT) systems participating in
the Semeval-2007 coarse-grained all-words task.
We also compare with SSI (Navigli and Velardi,
2005) – a knowledge-based system that partici-
pated out of competition – and the unsupervised
proposal from Chen et al. (2009, TreeMatch). Ta-
ble 3 shows the results for nouns (1,108) and
all words (2,269 words): we use the MFS as a
back-off strategy when no sense assignment is at-
tempted. Degree with WordNet++ achieves the
best performance in the literature6. On the noun-
only subset of the data, its performance is com-
parable with SSI and significantly better than the
best supervised and unsupervised systems (+3.2%
and +4.4% Fl against NUS-PT and SUSSX-FR).
On the entire dataset, it outperforms SUSSX-FR
and TreeMatch (+4.7% and +8.1%) and its re-
call is not statistically different from that of SSI
and NUS-PT. This result is particularly interest-
ing, given that WordNet++ is extended only with
relations between nominals, and, in contrast to
SSI, it does not rely on a costly annotation effort
to engineer the set of semantic relations. Last but
not least, we achieve state-of-the-art performance
with a much simpler algorithm that is based on the
notion of vertex degree in a graph.
</bodyText>
<footnote confidence="0.697173">
6The differences between the results in bold in each col-
umn of the table are not statistically significant at P &lt; 0.05.
</footnote>
<subsectionHeader confidence="0.998493">
4.3 Domain WSD
</subsectionHeader>
<bodyText confidence="0.99982609375">
The main strength of Wikipedia is to provide wide
coverage for many specific domains. Accord-
ingly, on the Semeval dataset our system achieves
the best performance on a domain-specific text,
namely d004, a document on computer science
where we achieve 82.9% Fl (+6.8% when com-
pared with the best supervised system, namely
NUS-PT). To test whether our performance on the
Semeval dataset is an artifact of the data, i.e. d004
coming from Wikipedia itself, we evaluated our
system on the Sports and Finance sections of the
domain corpora from Koeling et al. (2005). In Ta-
ble 4 we report our results on these datasets and
compare them with Personalized PageRank, the
state-of-the-art system from Agirre et al. (2009)7,
as well as Static PageRank and a k-NN supervised
WSD system trained on SemCor.
The results we obtain on the two domains with
our best configuration (Degree using WordNet++)
outperform by a large margin k-NN, thus sup-
porting the findings from Agirre et al. (2009)
that knowledge-based systems exhibit a more ro-
bust performance than their supervised alterna-
tives when evaluated across different domains. In
addition, our system achieves better results than
Static and Personalized PageRank, indicating that
competitive disambiguation performance can still
be achieved by a less sophisticated knowledge-
based WSD algorithm when provided with a rich
amount of high-quality knowledge. Finally, the
results show that WordNet++ enables competitive
performance also in a fine-grained domain setting.
</bodyText>
<footnote confidence="0.60996525">
7We compare only with those system configurations per-
forming token-based WSD, i.e. disambiguating each instance
of a target word separately, since our aim is not to perform
type-based disambiguation.
</footnote>
<page confidence="0.996156">
1529
</page>
<sectionHeader confidence="0.999452" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999979485714286">
In this paper, we have presented a large-scale
method for the automatic enrichment of a com-
putational lexicon with encyclopedic relational
knowledge8. Our experiments show that the large
amount of knowledge injected into WordNet is of
high quality and, more importantly, it enables sim-
ple knowledge-based WSD systems to perform as
well as the highest-performing supervised ones in
a coarse-grained setting and to outperform them
on domain-specific text. Thus, our results go
one step beyond previous findings (Cuadros and
Rigau, 2006; Agirre et al., 2009; Navigli and La-
pata, 2010) and prove that knowledge-rich dis-
ambiguation is a competitive alternative to super-
vised systems, even when relying on a simple al-
gorithm. We note, however, that the present con-
tribution does not show which knowledge-rich al-
gorithm performs best with WordNet++. In fact,
more sophisticated approaches, such as Personal-
ized PageRank (Agirre and Soroa, 2009), could be
still applied to yield even higher performance. We
leave such exploration to future work. Moreover,
while the mapping has been used to enrich Word-
Net with a large amount of semantic edges, the
method can be reversed and applied to the ency-
clopedic resource itself, that is Wikipedia, to per-
form disambiguation with the corresponding sense
inventory (cf. the task of wikification proposed
by Mihalcea and Csomai (2007) and Milne and
Witten (2008b)). In this paper, we focused on
English Word Sense Disambiguation. However,
since WordNet++ is part of a multilingual seman-
tic network (Navigli and Ponzetto, 2010), we plan
to explore the impact of this knowledge in a mul-
tilingual setting.
</bodyText>
<sectionHeader confidence="0.997781" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.970911">
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proc. of LREC ’04.
Eneko Agirre and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proceed-
ings of CoNLL-01, pages 15–22.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Proc.
of EACL-09, pages 33–41.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
</reference>
<footnote confidence="0.979816">
8The resulting resource, WordNet++, is freely available at
http://lcl.uniroma1.it/wordnetplusplus for
research purposes.
</footnote>
<reference confidence="0.984480465517241">
performing better than generic supervised WSD. In
Proc. of IJCAI-09, pages 1501–1506.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805–810.
Razvan Bunescu and Marius Pas¸ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9–16.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249–254.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-ML: Exploiting parallel texts for Word Sense
Disambiguation in the English all-words tasks. In
Proc. of SemEval-2007, pages 253–256.
Ping Chen, Wei Ding, Chris Bowes, and David Brown.
2009. A fully unsupervised Word Sense Disam-
biguation method using dependency knowledge. In
Proc. of NAACL-HLT-09, pages 28–36.
Tim Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Expert.
In Proceedings of the ACL-02 Workshop on WSD:
Recent Successes and Future Directions at ACL-02.
Martin Chodorow, Roy Byrd, and George E. Heidorn.
1985. Extracting semantic hierarchies from a large
on-line dictionary. In Proc. of ACL-85, pages 299–
304.
Philipp Cimiano, Siegfried Handschuh, and Steffen
Staab. 2004. Towards the self-annotating Web. In
Proc. of WWW-04, pages 462–471.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proc. of EMNLP-06, pages 534–541.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web. In
Proc. of COLING-08, pages 161–168.
Philip Edmonds. 2000. Designing a task for
SENSEVAL-2. Technical report, University of
Brighton, U.K.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proc. of AAAI-06,
pages 1301–1306.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proc. of IJCAI-
07, pages 1606–1611.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Sanda M. Harabagiu, George A. Miller, and Dan I.
Moldovan. 1999. WordNet 2 – a morphologically
and semantically enhanced resource. In Proceed-
ings of the SIGLEX99 Workshop on Standardizing
Lexical Resources, pages 1–8.
</reference>
<page confidence="0.747596">
1530
</page>
<reference confidence="0.999963297520661">
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proc. of
COLING-92, pages 539–545.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for English SENSEVAL.
Computers and the Humanities, 34(1-2).
Rob Koeling and Diana McCarthy. 2007. Sussx: WSD
using automatically acquired predominant senses.
In Proc. of SemEval-2007, pages 314–317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proc. of HLT-
EMNLP-05, pages 419–426.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th Annual Conference on Systems Documen-
tation, Toronto, Ontario, Canada, pages 24–26.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639–654.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
Linking documents to encyclopedic knowledge. In
Proc. of CIKM-07, pages 233–242.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proc. of NAACL-
HLT-07, pages 196–203.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human
Language Technology, pages 303–308, Plainsboro,
N.J.
David Milne and Ian H. Witten. 2008a. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the Work-
shop on Wikipedia and Artificial Intelligence: An
Evolving Synergy at AAAI-08, pages 25–30.
David Milne and Ian H. Witten. 2008b. Learning to
link with Wikipedia. In Proc. of CIKM-08, pages
509–518.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion. In Proc. of AAAI-08, pages 1219–1224.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
activation spreading. In Proc. of EMNLP-08, pages
763–772.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study on graph connectivity for unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Anaylsis and Machine Intelligence,
32(4):678–692.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual seman-
tic network. In Proc. ofACL-10.
Roberto Navigli and Paola Velardi. 2005. Struc-
tural Semantic Interconnections: a knowledge-based
approach to Word Sense Disambiguation. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 27(7):1075–1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. In Proc. of SemEval-
2007, pages 30–35.
Roberto Navigli. 2009a. Using cycles and quasi-
cycles to disambiguate dictionary glosses. In Proc.
of EACL-09, pages 594–602.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1–69.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proc. of COLING-
ACL-06, pages 793–800.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proc. of IJCAI-09,
pages 2083–2088.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440–1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181–212.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381–387. College Publications, Lon-
don, England.
German Rigau, Horacio Rodriguez, and Eneko Agirre.
1998. Building accurate semantic taxonomies from
monolingual MRDs. In Proc. of COLING-ACL-98,
pages 1103–1109.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lecture
Notes in Computer Science. Springer Verlag.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proc. ofACL-IJCNLP-09, pages
208–216.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL-IJCNLP-09, pages 450–458.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proc. of COLING-ACL-06, pages 801–
808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203–217.
Fei Wu and Daniel Weld. 2007. Automatically se-
mantifying Wikipedia. In Proc. of CIKM-07, pages
41–50.
Fei Wu and Daniel Weld. 2008. Automatically refining
the Wikipedia infobox ontology. In Proc. of WWW-
08, pages 635–644.
</reference>
<page confidence="0.992739">
1531
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738836">
<title confidence="0.999481">Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems</title>
<author confidence="0.99989">Simone Paolo Ponzetto Roberto Navigli</author>
<affiliation confidence="0.9890575">Department of Computational Linguistics Dipartimento di Informatica Heidelberg University Sapienza Universit`a di Roma</affiliation>
<email confidence="0.770039">ponzetto@cl.uni-heidelberg.denavigli@di.uniroma1.it</email>
<abstract confidence="0.9989323125">One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Eneko Agirre and Oier Lopez de Lacalle.</title>
<date>2004</date>
<booktitle>In Proc. of LREC ’04.</booktitle>
<contexts>
<context position="4835" citStr="(2004)" startWordPosition="707" endWordPosition="707">tologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast,</context>
</contexts>
<marker>2004</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly available topic signatures for all WordNet nominal senses. In Proc. of LREC ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Learning class-to-class selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL-01,</booktitle>
<pages>15--22</pages>
<contexts>
<context position="4407" citStr="Agirre and Martinez, 2001" startWordPosition="639" endWordPosition="642">utational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associati</context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>Eneko Agirre and David Martinez. 2001. Learning class-to-class selectional preferences. In Proceedings of CoNLL-01, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of EACL-09,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="35067" citStr="Agirre and Soroa, 2009" startWordPosition="5720" endWordPosition="5723">ms to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text. Thus, our results go one step beyond previous findings (Cuadros and Rigau, 2006; Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm. We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++. In fact, more sophisticated approaches, such as Personalized PageRank (Agirre and Soroa, 2009), could be still applied to yield even higher performance. We leave such exploration to future work. Moreover, while the mapping has been used to enrich WordNet with a large amount of semantic edges, the method can be reversed and applied to the encyclopedic resource itself, that is Wikipedia, to perform disambiguation with the corresponding sense inventory (cf. the task of wikification proposed by Mihalcea and Csomai (2007) and Milne and Witten (2008b)). In this paper, we focused on English Word Sense Disambiguation. However, since WordNet++ is part of a multilingual semantic network (Navigli</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proc. of EACL-09, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Knowledge-based WSD on specific domains: performing better than generic supervised WSD.</title>
<date>2009</date>
<booktitle>In Proc. of IJCAI-09,</booktitle>
<pages>1501--1506</pages>
<marker>Agirre, de Lacalle, Soroa, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009. Knowledge-based WSD on specific domains: performing better than generic supervised WSD. In Proc. of IJCAI-09, pages 1501–1506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlap as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proc. of IJCAI-03,</booktitle>
<pages>805--810</pages>
<contexts>
<context position="26278" citStr="Banerjee and Pedersen (2003)" startWordPosition="4291" endWordPosition="4294">lower than that of WordNet – i.e. average polysemy of 2.1 vs. 2.8. 1527 algorithm (Lesk, 1986), that performs WSD based on the overlap between the context surrounding the target word to be disambiguated and the definitions of its candidate senses (Kilgarriff and Rosenzweig, 2000). Given a target word w, this method assigns to w the sense whose gloss has the highest overlap (i.e. most words in common) with the context of w, namely the set of content words co-occurring with it in a pre-defined window (a sentence in our case). Due to the limited context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand the gloss of each sense s to include words from the glosses of those synsets in a semantic relation with s. These include all WordNet synsets which are directly connected to s, either by means of the semantic pointers found in WordNet or through the unlabeled links found in WordNet++. • Degree Centrality (Degree): The second algorithm is a graph-based approach that relies on the notion of vertex degree (Navigli and Lapata, 2010). Starting from each sense s of the target word, it performs a depth-first search (DFS) of the WordNet(++) graph and collects all the paths connecting s to </context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlap as a measure of semantic relatedness. In Proc. of IJCAI-03, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-06,</booktitle>
<pages>9--16</pages>
<marker>Bunescu, Pas¸ca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pas¸ca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proc. of EACL-06, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="20464" citStr="Carletta, 1996" startWordPosition="3328" endWordPosition="3329">ding monosemous words). We selected a random sample of 1,000 Wikipages and asked an annotator with previous experience in lexicographic annotation to provide the correct WordNet sense for each page title (an empty sense label was given if no correct mapping was possible). 505 non-empty mappings were found, i.e. Wikipedia pages with a corresponding WordNet sense. In order to quantify the quality of the annotations and the difficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample. We computed the inter-annotator agreement using the kappa coefficient (Carletta, 1996) and found out that our annotators achieved an agreement coefficient r. of 0.9, indicating almost perfect agreement. Table 1 summarizes the performance of our disambiguation algorithm against the manually annotated dataset. Evaluation is performed in terms of standard measures of precision (the ratio of correct sense labels to the non-empty labels output by the mapping algorithm), recall (the ratio of correct sense labels to the total of non-empty labels in the gold standard) and F1-measure ( 2PR P+R). We also calculate accuracy, which accounts for 1526 P R F1 A Structure 82.2 68.1 74.5 81.1 G</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>Zhi Zhong</author>
</authors>
<title>NUS-ML: Exploiting parallel texts for Word Sense Disambiguation in the English all-words tasks.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval-2007,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="31053" citStr="Chan et al., 2007" startWordPosition="5084" endWordPosition="5087"> 39.6 Personalized PR † 35.6 46.9 ExtLesk 40.1 45.6 Degree 42.0 47.8 MFS BL 19.6 37.1 Random BL 19.5 19.6 Table 4: Performance on the Sports and Finance sections of the dataset from Koeling et al. (2005): † indicates results from Agirre et al. (2009). gree, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated out of competition – and the unsupervised proposal from Chen et al. (2009, TreeMatch). Table 3 shows the results for nouns (1,108) and all words (2,269 words): we use the MFS as a back-off strategy when no sense assignment is attempted. Degree with WordNet++ achieves the best performance in the literature6. On the nounonly subset of the data, its performance is comparable with SSI and significantly better than the b</context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007. NUS-ML: Exploiting parallel texts for Word Sense Disambiguation in the English all-words tasks. In Proc. of SemEval-2007, pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Chen</author>
<author>Wei Ding</author>
<author>Chris Bowes</author>
<author>David Brown</author>
</authors>
<title>A fully unsupervised Word Sense Disambiguation method using dependency knowledge.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT-09,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="31307" citStr="Chen et al. (2009" startWordPosition="5122" endWordPosition="5125">ee, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated out of competition – and the unsupervised proposal from Chen et al. (2009, TreeMatch). Table 3 shows the results for nouns (1,108) and all words (2,269 words): we use the MFS as a back-off strategy when no sense assignment is attempted. Degree with WordNet++ achieves the best performance in the literature6. On the nounonly subset of the data, its performance is comparable with SSI and significantly better than the best supervised and unsupervised systems (+3.2% and +4.4% Fl against NUS-PT and SUSSX-FR). On the entire dataset, it outperforms SUSSX-FR and TreeMatch (+4.7% and +8.1%) and its recall is not statistically different from that of SSI and NUS-PT. This resul</context>
</contexts>
<marker>Chen, Ding, Bowes, Brown, 2009</marker>
<rawString>Ping Chen, Wei Ding, Chris Bowes, and David Brown. 2009. A fully unsupervised Word Sense Disambiguation method using dependency knowledge. In Proc. of NAACL-HLT-09, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with Open Mind Word Expert.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on WSD: Recent Successes and Future Directions at ACL-02.</booktitle>
<contexts>
<context position="5331" citStr="Chklovski and Mihalcea, 2002" startWordPosition="776" endWordPosition="779">acted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the k</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Tim Chklovski and Rada Mihalcea. 2002. Building a sense tagged corpus with Open Mind Word Expert. In Proceedings of the ACL-02 Workshop on WSD: Recent Successes and Future Directions at ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Roy Byrd</author>
<author>George E Heidorn</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.</title>
<date>1985</date>
<booktitle>In Proc. of ACL-85,</booktitle>
<pages>299--304</pages>
<contexts>
<context position="3998" citStr="Chodorow et al. (1985)" startWordPosition="581" endWordPosition="584"> better than supervised ones, and we show that, given enough knowledge, simple algorithms perform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of c</context>
</contexts>
<marker>Chodorow, Byrd, Heidorn, 1985</marker>
<rawString>Martin Chodorow, Roy Byrd, and George E. Heidorn. 1985. Extracting semantic hierarchies from a large on-line dictionary. In Proc. of ACL-85, pages 299– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Siegfried Handschuh</author>
<author>Steffen Staab</author>
</authors>
<title>Towards the self-annotating Web. In</title>
<date>2004</date>
<booktitle>Proc. of WWW-04,</booktitle>
<pages>462--471</pages>
<contexts>
<context position="4102" citStr="Cimiano et al., 2004" startWordPosition="597" endWordPosition="600">han more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extr</context>
</contexts>
<marker>Cimiano, Handschuh, Staab, 2004</marker>
<rawString>Philipp Cimiano, Siegfried Handschuh, and Steffen Staab. 2004. Towards the self-annotating Web. In Proc. of WWW-04, pages 462–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>Quality assessment of large scale knowledge resources.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06,</booktitle>
<pages>534--541</pages>
<contexts>
<context position="2002" citStr="Cuadros and Rigau, 2006" startWordPosition="284" endWordPosition="287">wledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the words in the English lexicon with high accuracy. In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet (Fellbaum, 1998). However, it has been demonstrated that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006). Several methods have been proposed to automatically extend existing resources (cf. Section 2) and it has been shown that highlyinterconnected semantic networks have a great impact on WSD (Navigli and Lapata, 2010). However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet (Navigli and Velardi, 2005) or sophisticated WSD algorithms (Agirre et al., 2009). The contributions of this paper are two-fold. First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend </context>
<context position="34663" citStr="Cuadros and Rigau, 2006" startWordPosition="5656" endWordPosition="5659">separately, since our aim is not to perform type-based disambiguation. 1529 5 Conclusions In this paper, we have presented a large-scale method for the automatic enrichment of a computational lexicon with encyclopedic relational knowledge8. Our experiments show that the large amount of knowledge injected into WordNet is of high quality and, more importantly, it enables simple knowledge-based WSD systems to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text. Thus, our results go one step beyond previous findings (Cuadros and Rigau, 2006; Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm. We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++. In fact, more sophisticated approaches, such as Personalized PageRank (Agirre and Soroa, 2009), could be still applied to yield even higher performance. We leave such exploration to future work. Moreover, while the mapping has been used to enrich WordNet with a large amount of semantic edg</context>
</contexts>
<marker>Cuadros, Rigau, 2006</marker>
<rawString>Montse Cuadros and German Rigau. 2006. Quality assessment of large scale knowledge resources. In Proc. of EMNLP-06, pages 534–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>KnowNet: building a large net of knowledge from the Web. In</title>
<date>2008</date>
<booktitle>Proc. of COLING-08,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="4768" citStr="Cuadros and Rigau, 2008" startWordPosition="692" endWordPosition="695">ased on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicat</context>
</contexts>
<marker>Cuadros, Rigau, 2008</marker>
<rawString>Montse Cuadros and German Rigau. 2008. KnowNet: building a large net of knowledge from the Web. In Proc. of COLING-08, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
</authors>
<title>Designing a task for SENSEVAL-2.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>University of Brighton, U.K.</institution>
<contexts>
<context position="1472" citStr="Edmonds, 2000" startWordPosition="208" endWordPosition="209">roduction Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context (Navigli, 2009b). In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledgebased methods. In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses. Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the words in the English lexicon with high accuracy. In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet (Fellbaum, 1998). However, it has been demonstrated that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006). Several methods have been proposed to automatically extend existing </context>
</contexts>
<marker>Edmonds, 2000</marker>
<rawString>Philip Edmonds. 2000. Designing a task for SENSEVAL-2. Technical report, University of Brighton, U.K.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4022" citStr="(1998)" startWordPosition="589" endWordPosition="589">w that, given enough knowledge, simple algorithms perform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obta</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge.</title>
<date>2006</date>
<booktitle>In Proc. of AAAI-06,</booktitle>
<pages>1301--1306</pages>
<contexts>
<context position="6040" citStr="Gabrilovich and Markovitch, 2006" startWordPosition="884" endWordPosition="887">ademic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2006</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2006. Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge. In Proc. of AAAI-06, pages 1301–1306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI07,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="6114" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="893" endWordPosition="896">base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proc. of IJCAI07, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="4123" citStr="Girju et al., 2006" startWordPosition="601" endWordPosition="604"> ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of sta</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>George A Miller</author>
<author>Dan I Moldovan</author>
</authors>
<title>WordNet 2 – a morphologically and semantically enhanced resource.</title>
<date>1999</date>
<booktitle>In Proceedings of the SIGLEX99 Workshop on Standardizing Lexical Resources,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4210" citStr="Harabagiu et al., 1999" startWordPosition="613" endWordPosition="616">ented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agi</context>
</contexts>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>Sanda M. Harabagiu, George A. Miller, and Dan I. Moldovan. 1999. WordNet 2 – a morphologically and semantically enhanced resource. In Proceedings of the SIGLEX99 Workshop on Standardizing Lexical Resources, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of COLING-92,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="4080" citStr="Hearst, 1992" startWordPosition="595" endWordPosition="596">rform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of COLING-92, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<date>2000</date>
<booktitle>Framework and results for English SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="25930" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="4227" endWordPosition="4231">ontained in WordNet, but which have out-of-domain meanings, i.e. encyclopedic entries referring to specialized named entities such as e.g., DISCOVERY (SPACE SHUTTLE) or FIELD ARTILLERY (MAGAZINE). We computed the polysemy rate for a random sample of 20 polysemous words by manually removing these NEs and found that Wikipedia’s polysemy rate is indeed lower than that of WordNet – i.e. average polysemy of 2.1 vs. 2.8. 1527 algorithm (Lesk, 1986), that performs WSD based on the overlap between the context surrounding the target word to be disambiguated and the definitions of its candidate senses (Kilgarriff and Rosenzweig, 2000). Given a target word w, this method assigns to w the sense whose gloss has the highest overlap (i.e. most words in common) with the context of w, namely the set of content words co-occurring with it in a pre-defined window (a sentence in our case). Due to the limited context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand the gloss of each sense s to include words from the glosses of those synsets in a semantic relation with s. These include all WordNet synsets which are directly connected to s, either by means of the semantic pointers found in WordNet or th</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for English SENSEVAL. Computers and the Humanities, 34(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
</authors>
<title>Sussx: WSD using automatically acquired predominant senses.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval-2007,</booktitle>
<pages>314--317</pages>
<contexts>
<context position="31008" citStr="Koeling and McCarthy, 2007" startWordPosition="5076" endWordPosition="5080">inance P/R/F1 P/R/F1 k-NN † 30.3 43.4 Static PR † 20.1 39.6 Personalized PR † 35.6 46.9 ExtLesk 40.1 45.6 Degree 42.0 47.8 MFS BL 19.6 37.1 Random BL 19.5 19.6 Table 4: Performance on the Sports and Finance sections of the dataset from Koeling et al. (2005): † indicates results from Agirre et al. (2009). gree, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated out of competition – and the unsupervised proposal from Chen et al. (2009, TreeMatch). Table 3 shows the results for nouns (1,108) and all words (2,269 words): we use the MFS as a back-off strategy when no sense assignment is attempted. Degree with WordNet++ achieves the best performance in the literature6. On the nounonly subset of the data, its performance is comparable</context>
</contexts>
<marker>Koeling, McCarthy, 2007</marker>
<rawString>Rob Koeling and Diana McCarthy. 2007. Sussx: WSD using automatically acquired predominant senses. In Proc. of SemEval-2007, pages 314–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP-05,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="30639" citStr="Koeling et al. (2005)" startWordPosition="5023" endWordPosition="5026">nt is found with De1528 Table 3: Performance on Semeval-2007 coarsegrained all-words WSD with MFS as a back-off strategy when no sense assignment is attempted. Algorithm Nouns only All words P/R/F1 P/R/F1 ExtLesk 81.0 79.1 Degree 85.5 81.7 SUSSX-FR 81.1 77.0 TreeMatch N/A 73.6 NUS-PT 82.3 82.5 SSI 84.1 83.2 MFS BL 77.4 78.9 Random BL 63.5 62.7 Algorithm Sports Finance P/R/F1 P/R/F1 k-NN † 30.3 43.4 Static PR † 20.1 39.6 Personalized PR † 35.6 46.9 ExtLesk 40.1 45.6 Degree 42.0 47.8 MFS BL 19.6 37.1 Random BL 19.5 19.6 Table 4: Performance on the Sports and Finance sections of the dataset from Koeling et al. (2005): † indicates results from Agirre et al. (2009). gree, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated out o</context>
<context position="32966" citStr="Koeling et al. (2005)" startWordPosition="5401" endWordPosition="5404">re not statistically significant at P &lt; 0.05. 4.3 Domain WSD The main strength of Wikipedia is to provide wide coverage for many specific domains. Accordingly, on the Semeval dataset our system achieves the best performance on a domain-specific text, namely d004, a document on computer science where we achieve 82.9% Fl (+6.8% when compared with the best supervised system, namely NUS-PT). To test whether our performance on the Semeval dataset is an artifact of the data, i.e. d004 coming from Wikipedia itself, we evaluated our system on the Sports and Finance sections of the domain corpora from Koeling et al. (2005). In Table 4 we report our results on these datasets and compare them with Personalized PageRank, the state-of-the-art system from Agirre et al. (2009)7, as well as Static PageRank and a k-NN supervised WSD system trained on SemCor. The results we obtain on the two domains with our best configuration (Degree using WordNet++) outperform by a large margin k-NN, thus supporting the findings from Agirre et al. (2009) that knowledge-based systems exhibit a more robust performance than their supervised alternatives when evaluated across different domains. In addition, our system achieves better resu</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proc. of HLTEMNLP-05, pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual Conference on Systems Documentation,</booktitle>
<pages>24--26</pages>
<location>Toronto, Ontario, Canada,</location>
<contexts>
<context position="25744" citStr="Lesk, 1986" startWordPosition="4200" endWordPosition="4201">tended Lesk (ExtLesk): The first algorithm is a simplified version of the Lesk 5Note that our polysemy rates from Section 4.1 also include Wikipages whose lemma is contained in WordNet, but which have out-of-domain meanings, i.e. encyclopedic entries referring to specialized named entities such as e.g., DISCOVERY (SPACE SHUTTLE) or FIELD ARTILLERY (MAGAZINE). We computed the polysemy rate for a random sample of 20 polysemous words by manually removing these NEs and found that Wikipedia’s polysemy rate is indeed lower than that of WordNet – i.e. average polysemy of 2.1 vs. 2.8. 1527 algorithm (Lesk, 1986), that performs WSD based on the overlap between the context surrounding the target word to be disambiguated and the definitions of its candidate senses (Kilgarriff and Rosenzweig, 2000). Given a target word w, this method assigns to w the sense whose gloss has the highest overlap (i.e. most words in common) with the context of w, namely the set of content words co-occurring with it in a pre-defined window (a sentence in our case). Due to the limited context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand the gloss of each sense s to include words from the gl</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual Conference on Systems Documentation, Toronto, Ontario, Canada, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="4452" citStr="McCarthy and Carroll, 2003" startWordPosition="646" endWordPosition="649">ala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a v</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify! Linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM-07,</booktitle>
<pages>233--242</pages>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify! Linking documents to encyclopedic knowledge. In Proc. of CIKM-07, pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACLHLT-07,</booktitle>
<pages>196--203</pages>
<contexts>
<context position="6570" citStr="Mihalcea (2007)" startWordPosition="964" endWordPosition="965">kipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mappin</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea. 2007. Using Wikipedia for automatic Word Sense Disambiguation. In Proc. of NAACLHLT-07, pages 196–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, N.J.</location>
<contexts>
<context position="27501" citStr="Miller et al., 1993" startWordPosition="4504" endWordPosition="4507">ses of other words in context. As a result, a sentence graph is produced. A maximum search depth is established to limit the size of this graph. The sense of the target word with the highest vertex degree is selected. We follow Navigli and Lapata (2010) and run Degree in a weakly supervised setting where the system attempts no sense assignment if the highest degree score is below a certain (empirically estimated) threshold. The optimal threshold and maximum search depth are estimated by maximizing Degree’s Fl on a development set of 1,000 randomly chosen noun instances from the SemCor corpus (Miller et al., 1993). Experiments on the development dataset using Degree on WordNet++ revealed a performance far lower than expected. Error analysis showed that many instances were incorrectly disambiguated, due to the noise from weak semantic links, e.g. the links from SODA (SOFT DRINK) to EUROPE or AUSTRALIA. Accordingly, in order to improve the disambiguation performance, we developed a filter to rule out weak semantic relations from WordNet++. Given a WordNet++ edge (µ(w), µ(w&apos;)) where w and w&apos; are both Wikipages and w links to w&apos;, Resource Algorithm Nouns only P R F1 WordNet ExtLesk 83.6 57.7 68.3 Degree 86</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross Bunker. 1993. A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 303–308, Plainsboro, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective, low-cost measure of semantic relatedness obtained from Wikipedia links.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy at AAAI-08,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="6166" citStr="Milne and Witten, 2008" startWordPosition="901" endWordPosition="904">ically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways:</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008a. An effective, low-cost measure of semantic relatedness obtained from Wikipedia links. In Proceedings of the Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy at AAAI-08, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of CIKM-08,</booktitle>
<pages>509--518</pages>
<contexts>
<context position="6166" citStr="Milne and Witten, 2008" startWordPosition="901" endWordPosition="904">ically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways:</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008b. Learning to link with Wikipedia. In Proc. of CIKM-08, pages 509–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Decoding Wikipedia category names for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI-08,</booktitle>
<pages>1219--1224</pages>
<contexts>
<context position="5846" citStr="Nastase and Strube, 2008" startWordPosition="858" endWordPosition="861">has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve </context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>Vivi Nastase and Michael Strube. 2008. Decoding Wikipedia category names for knowledge acquisition. In Proc. of AAAI-08, pages 1219–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Topic-driven multi-document summarization with encyclopedic knowledge and activation spreading.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP-08,</booktitle>
<pages>763--772</pages>
<contexts>
<context position="6267" citStr="Nastase, 2008" startWordPosition="914" endWordPosition="915">ation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappi</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Topic-driven multi-document summarization with encyclopedic knowledge and activation spreading. In Proc. of EMNLP-08, pages 763–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study on graph connectivity for unsupervised Word Sense Disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Anaylsis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="2217" citStr="Navigli and Lapata, 2010" startWordPosition="319" endWordPosition="322">er to disambiguate all the words in the English lexicon with high accuracy. In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet (Fellbaum, 1998). However, it has been demonstrated that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006). Several methods have been proposed to automatically extend existing resources (cf. Section 2) and it has been shown that highlyinterconnected semantic networks have a great impact on WSD (Navigli and Lapata, 2010). However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet (Navigli and Velardi, 2005) or sophisticated WSD algorithms (Agirre et al., 2009). The contributions of this paper are two-fold. First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend WordNet with millions of semantic relations. The relations are harvested from an encyclopedic resource, namely Wikipedia. Wikipedia pages are automatically associated with WordNet senses, and topical, semantic assoc</context>
<context position="26722" citStr="Navigli and Lapata, 2010" startWordPosition="4367" endWordPosition="4371">ntent words co-occurring with it in a pre-defined window (a sentence in our case). Due to the limited context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand the gloss of each sense s to include words from the glosses of those synsets in a semantic relation with s. These include all WordNet synsets which are directly connected to s, either by means of the semantic pointers found in WordNet or through the unlabeled links found in WordNet++. • Degree Centrality (Degree): The second algorithm is a graph-based approach that relies on the notion of vertex degree (Navigli and Lapata, 2010). Starting from each sense s of the target word, it performs a depth-first search (DFS) of the WordNet(++) graph and collects all the paths connecting s to senses of other words in context. As a result, a sentence graph is produced. A maximum search depth is established to limit the size of this graph. The sense of the target word with the highest vertex degree is selected. We follow Navigli and Lapata (2010) and run Degree in a weakly supervised setting where the system attempts no sense assignment if the highest degree score is below a certain (empirically estimated) threshold. The optimal t</context>
<context position="34711" citStr="Navigli and Lapata, 2010" startWordPosition="5664" endWordPosition="5668">pe-based disambiguation. 1529 5 Conclusions In this paper, we have presented a large-scale method for the automatic enrichment of a computational lexicon with encyclopedic relational knowledge8. Our experiments show that the large amount of knowledge injected into WordNet is of high quality and, more importantly, it enables simple knowledge-based WSD systems to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text. Thus, our results go one step beyond previous findings (Cuadros and Rigau, 2006; Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm. We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++. In fact, more sophisticated approaches, such as Personalized PageRank (Agirre and Soroa, 2009), could be still applied to yield even higher performance. We leave such exploration to future work. Moreover, while the mapping has been used to enrich WordNet with a large amount of semantic edges, the method can be reversed and applied to th</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study on graph connectivity for unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Anaylsis and Machine Intelligence, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: Building a very large multilingual semantic network. In</title>
<date>2010</date>
<booktitle>Proc. ofACL-10.</booktitle>
<contexts>
<context position="18977" citStr="Navigli and Ponzetto, 2010" startWordPosition="3094" endWordPosition="3097">add the corresponding semantic relation (soda2n, syrup1n) to WordNet3. Thus, WordNet++ represents an extension of WordNet which includes semantic associative relations between synsets. These are originally 3Note that such relations are unlabeled. However, for our purposes this has no impact, since our algorithms do not distinguish between is-a and other kinds of relations in the lexical knowledge base (cf. Section 4.2). found in Wikipedia and then integrated into WordNet by means of our mapping. In turn, WordNet++ represents the English-only subset of a larger multilingual resource, BabelNet (Navigli and Ponzetto, 2010), where lexicalizations of the synsets are harvested for many languages using the so-called Wikipedia inter-language links and applying a machine translation system. 4 Experiments We perform two sets of experiments: we first evaluate the intrinsic quality of our mapping (Section 4.1) and then quantify the impact of WordNet++ for coarse-grained (Section 4.2) and domainspecific WSD (Section 4.3). 4.1 Evaluation of the Mapping Experimental setting. We first conducted an evaluation of the mapping quality. To create a gold standard for evaluation, we started from the set of all lemmas contained bot</context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a very large multilingual semantic network. In Proc. ofACL-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural Semantic Interconnections: a knowledge-based approach to Word Sense Disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="2407" citStr="Navigli and Velardi, 2005" startWordPosition="349" endWordPosition="352">h as WordNet (Fellbaum, 1998). However, it has been demonstrated that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006). Several methods have been proposed to automatically extend existing resources (cf. Section 2) and it has been shown that highlyinterconnected semantic networks have a great impact on WSD (Navigli and Lapata, 2010). However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet (Navigli and Velardi, 2005) or sophisticated WSD algorithms (Agirre et al., 2009). The contributions of this paper are two-fold. First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend WordNet with millions of semantic relations. The relations are harvested from an encyclopedic resource, namely Wikipedia. Wikipedia pages are automatically associated with WordNet senses, and topical, semantic associative relations from Wikipedia are transferred to WordNet, thus producing a much richer lexical resource. Second, two simple knowledge-based algorithms that exploit our extended WordNet are</context>
<context position="4694" citStr="Navigli and Velardi, 2005" startWordPosition="680" endWordPosition="684">Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert p</context>
<context position="31188" citStr="Navigli and Velardi, 2005" startWordPosition="5102" endWordPosition="5105"> on the Sports and Finance sections of the dataset from Koeling et al. (2005): † indicates results from Agirre et al. (2009). gree, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated out of competition – and the unsupervised proposal from Chen et al. (2009, TreeMatch). Table 3 shows the results for nouns (1,108) and all words (2,269 words): we use the MFS as a back-off strategy when no sense assignment is attempted. Degree with WordNet++ achieves the best performance in the literature6. On the nounonly subset of the data, its performance is comparable with SSI and significantly better than the best supervised and unsupervised systems (+3.2% and +4.4% Fl against NUS-PT and SUSSX-FR). On the entire dataset, it outperforms SUSSX-F</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural Semantic Interconnections: a knowledge-based approach to Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1075–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 07: Coarsegrained English all-words task.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval2007,</booktitle>
<pages>30--35</pages>
<contexts>
<context position="24271" citStr="Navigli et al., 2007" startWordPosition="3956" endWordPosition="3960"> are deemed novel, i.e. no direct edge could previously be found between the synsets. In addition, we performed a stricter evaluation of the novelty of our relations by checking whether these can still be found indirectly by searching for a connecting path between the two synsets of interest. Here we found that 91.3%, 87.2% and 78.9% of the relations are novel to WordNet when performing a graph search of maximum depth of 2, 3 and 4, respectively. 4.2 Coarse-grained WSD Experimental setting. We extrinsically evaluate the impact of WordNet++ on the Semeval2007 coarse-grained all-words WSD task (Navigli et al., 2007). Performing experiments in a coarse-grained setting is a natural choice for several reasons: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in Navigli (2009b)); second, the meanings of Wikipedia pages are intuitively coarser than those in WordNet5. For instance, mapping TRAVEL to the first or the second sense in WordNet is an arbitrary choice, as the Wikipage refers to both senses. Finally, given their different nature, WordNet and Wikipedia do not fully overlap. Accordingly, we expect the transfer of semantic re</context>
<context position="29209" citStr="Navigli et al., 2007" startWordPosition="4792" endWordPosition="4795">f words in common between the two bags of words, normalized in the [0, 1] interval. We finally retain the link for the DFS if such score is above an empirically determined threshold. The optimal value for this category overlap threshold was again estimated by maximizing Degree’s Fl on the development set. The final graph used by Degree consists of WordNet, together with 152,944 relations from our semantic relation enrichment method (cf. Section 3.3). Results and discussion. We report our results in terms of precision, recall and Fl-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al., 2007). We first evaluated ExtLesk and Degree using three different resources: (1) WordNet only; (2) Wikipedia only, i.e. only those relations harvested from the links found within Wikipedia pages; (3) their union, i.e. WordNet++. In Table 2 we report the results on nouns only. As common practice, we compare with random sense assignment and the most frequent sense (MFS) from SemCor as baselines. Enriching WordNet with encyclopedic relations from Wikipedia yields a consistent improvement against using WordNet (+7.1% and +4.9% Fl for ExtLesk and Degree) or Wikipedia (+3.4% and +22.0%) alone. The best </context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. Semeval-2007 task 07: Coarsegrained English all-words task. In Proc. of SemEval2007, pages 30–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Using cycles and quasicycles to disambiguate dictionary glosses.</title>
<date>2009</date>
<booktitle>In Proc. of EACL-09,</booktitle>
<pages>594--602</pages>
<contexts>
<context position="1018" citStr="Navigli, 2009" startWordPosition="135" endWordPosition="136"> In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets. 1 Introduction Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context (Navigli, 2009b). In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledgebased methods. In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses. Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the</context>
<context position="4551" citStr="Navigli, 2009" startWordPosition="663" endWordPosition="664">c information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web</context>
<context position="7425" citStr="Navigli, 2009" startWordPosition="1107" endWordPosition="1109">Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations conn</context>
<context position="12050" citStr="Navigli (2009)" startWordPosition="1853" endWordPosition="1854">s). For instance, the links in the Wikipage SODA (SOFT DRINK) include soda, lemonade, sugar, etc. • Categories: Wikipages are classified according to one or more categories, which represent meta-information used to categorize them. For instance, the Wikipage SODA (SOFT DRINK) is categorized as SOFT DRINKS. Since many categories are very specific and do not appear in WordNet (e.g., SWEDISH WRITERS or SCIENTISTS WHO COMMITTED SUICIDE), we use the lemmas of their syntactic heads as disambiguation context (i.e. writer and scientist). To this end, we use the category heads provided by Ponzetto and Navigli (2009). Given a Wikipage w, we define its disambiguation context Ctx(w) as the set of words obtained from some or all of the three sources above. 3.2.2 Disambiguation Context of a WordNet Sense Given a WordNet sense s and its synset S, we use the following information as disambiguation context to provide evidence for a potential link in our mapping µ: • Synonymy: all synonyms of s in synset S. For instance, given the synset of soda��, all its synonyms are included in the context (that is, tonic, soda pop, pop, etc.). µ(w) = 1524 • Hypernymy/Hyponymy: all synonyms in the synsets H such that H is eith</context>
<context position="24509" citStr="Navigli (2009" startWordPosition="4000" endWordPosition="4002">cting path between the two synsets of interest. Here we found that 91.3%, 87.2% and 78.9% of the relations are novel to WordNet when performing a graph search of maximum depth of 2, 3 and 4, respectively. 4.2 Coarse-grained WSD Experimental setting. We extrinsically evaluate the impact of WordNet++ on the Semeval2007 coarse-grained all-words WSD task (Navigli et al., 2007). Performing experiments in a coarse-grained setting is a natural choice for several reasons: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in Navigli (2009b)); second, the meanings of Wikipedia pages are intuitively coarser than those in WordNet5. For instance, mapping TRAVEL to the first or the second sense in WordNet is an arbitrary choice, as the Wikipage refers to both senses. Finally, given their different nature, WordNet and Wikipedia do not fully overlap. Accordingly, we expect the transfer of semantic relations from Wikipedia to WordNet to have sometimes the side effect to penalize some fine-grained senses of a word. We experiment with two simple knowledgebased algorithms that are set to perform coarsegrained WSD on a sentence-by-sentenc</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009a. Using cycles and quasicycles to disambiguate dictionary glosses. In Proc. of EACL-09, pages 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1018" citStr="Navigli, 2009" startWordPosition="135" endWordPosition="136"> In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets. 1 Introduction Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context (Navigli, 2009b). In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledgebased methods. In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses. Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the</context>
<context position="4551" citStr="Navigli, 2009" startWordPosition="663" endWordPosition="664">c information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web</context>
<context position="7425" citStr="Navigli, 2009" startWordPosition="1107" endWordPosition="1109">Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations conn</context>
<context position="12050" citStr="Navigli (2009)" startWordPosition="1853" endWordPosition="1854">s). For instance, the links in the Wikipage SODA (SOFT DRINK) include soda, lemonade, sugar, etc. • Categories: Wikipages are classified according to one or more categories, which represent meta-information used to categorize them. For instance, the Wikipage SODA (SOFT DRINK) is categorized as SOFT DRINKS. Since many categories are very specific and do not appear in WordNet (e.g., SWEDISH WRITERS or SCIENTISTS WHO COMMITTED SUICIDE), we use the lemmas of their syntactic heads as disambiguation context (i.e. writer and scientist). To this end, we use the category heads provided by Ponzetto and Navigli (2009). Given a Wikipage w, we define its disambiguation context Ctx(w) as the set of words obtained from some or all of the three sources above. 3.2.2 Disambiguation Context of a WordNet Sense Given a WordNet sense s and its synset S, we use the following information as disambiguation context to provide evidence for a potential link in our mapping µ: • Synonymy: all synonyms of s in synset S. For instance, given the synset of soda��, all its synonyms are included in the context (that is, tonic, soda pop, pop, etc.). µ(w) = 1524 • Hypernymy/Hyponymy: all synonyms in the synsets H such that H is eith</context>
<context position="24509" citStr="Navigli (2009" startWordPosition="4000" endWordPosition="4002">cting path between the two synsets of interest. Here we found that 91.3%, 87.2% and 78.9% of the relations are novel to WordNet when performing a graph search of maximum depth of 2, 3 and 4, respectively. 4.2 Coarse-grained WSD Experimental setting. We extrinsically evaluate the impact of WordNet++ on the Semeval2007 coarse-grained all-words WSD task (Navigli et al., 2007). Performing experiments in a coarse-grained setting is a natural choice for several reasons: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in Navigli (2009b)); second, the meanings of Wikipedia pages are intuitively coarser than those in WordNet5. For instance, mapping TRAVEL to the first or the second sense in WordNet is an arbitrary choice, as the Wikipage refers to both senses. Finally, given their different nature, WordNet and Wikipedia do not fully overlap. Accordingly, we expect the transfer of semantic relations from Wikipedia to WordNet to have sometimes the side effect to penalize some fine-grained senses of a word. We experiment with two simple knowledgebased algorithms that are set to perform coarsegrained WSD on a sentence-by-sentenc</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009b. Word Sense Disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Ontologizing semantic relations.</title>
<date>2006</date>
<booktitle>In Proc. of COLINGACL-06,</booktitle>
<pages>793--800</pages>
<contexts>
<context position="4273" citStr="Pennacchiotti and Pantel, 2006" startWordPosition="619" endWordPosition="623"> for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods repr</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2006</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2006. Ontologizing semantic relations. In Proc. of COLINGACL-06, pages 793–800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Large-scale taxonomy mapping for restructuring and integrating Wikipedia.</title>
<date>2009</date>
<booktitle>In Proc. of IJCAI-09,</booktitle>
<pages>2083--2088</pages>
<contexts>
<context position="7425" citStr="Ponzetto and Navigli, 2009" startWordPosition="1105" endWordPosition="1109">ping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations conn</context>
<context position="12050" citStr="Ponzetto and Navigli (2009)" startWordPosition="1851" endWordPosition="1854">outgoing links). For instance, the links in the Wikipage SODA (SOFT DRINK) include soda, lemonade, sugar, etc. • Categories: Wikipages are classified according to one or more categories, which represent meta-information used to categorize them. For instance, the Wikipage SODA (SOFT DRINK) is categorized as SOFT DRINKS. Since many categories are very specific and do not appear in WordNet (e.g., SWEDISH WRITERS or SCIENTISTS WHO COMMITTED SUICIDE), we use the lemmas of their syntactic heads as disambiguation context (i.e. writer and scientist). To this end, we use the category heads provided by Ponzetto and Navigli (2009). Given a Wikipage w, we define its disambiguation context Ctx(w) as the set of words obtained from some or all of the three sources above. 3.2.2 Disambiguation Context of a WordNet Sense Given a WordNet sense s and its synset S, we use the following information as disambiguation context to provide evidence for a potential link in our mapping µ: • Synonymy: all synonyms of s in synset S. For instance, given the synset of soda��, all its synonyms are included in the context (that is, tonic, soda pop, pop, etc.). µ(w) = 1524 • Hypernymy/Hyponymy: all synonyms in the synsets H such that H is eith</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2009. Large-scale taxonomy mapping for restructuring and integrating Wikipedia. In Proc. of IJCAI-09, pages 2083–2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of AAAI-07,</booktitle>
<pages>1440--1445</pages>
<contexts>
<context position="5746" citStr="Ponzetto and Strube, 2007" startWordPosition="842" endWordPosition="845"> collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this l</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007a. Deriving a large scale taxonomy from Wikipedia. In Proc. of AAAI-07, pages 1440–1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Knowledge derived from Wikipedia for computing semantic relatedness.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>30--181</pages>
<contexts>
<context position="5746" citStr="Ponzetto and Strube, 2007" startWordPosition="842" endWordPosition="845"> collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this l</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007b. Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Reiter</author>
<author>Matthias Hartung</author>
<author>Anette Frank</author>
</authors>
<title>A resource-poor approach for linking ontology classes to Wikipedia articles.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing,</booktitle>
<volume>1</volume>
<pages>381--387</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications,</publisher>
<location>London, England.</location>
<contexts>
<context position="7556" citStr="Reiter et al., 2008" startWordPosition="1128" endWordPosition="1131">ing text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations connecting Wikipedia pages are transferred to WordNet. As a result, an extended version of WordNet is produced, that we call WordNet++.</context>
</contexts>
<marker>Reiter, Hartung, Frank, 2008</marker>
<rawString>Nils Reiter, Matthias Hartung, and Anette Frank. 2008. A resource-poor approach for linking ontology classes to Wikipedia articles. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing, volume 1 of Research in Computational Semantics, pages 381–387. College Publications, London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>German Rigau</author>
<author>Horacio Rodriguez</author>
<author>Eneko Agirre</author>
</authors>
<title>Building accurate semantic taxonomies from monolingual MRDs.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL-98,</booktitle>
<pages>1103--1109</pages>
<contexts>
<context position="4022" citStr="Rigau et al. (1998)" startWordPosition="586" endWordPosition="589">s, and we show that, given enough knowledge, simple algorithms perform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obta</context>
</contexts>
<marker>Rigau, Rodriguez, Agirre, 1998</marker>
<rawString>German Rigau, Horacio Rodriguez, and Eneko Agirre. 1998. Building accurate semantic taxonomies from monolingual MRDs. In Proc. of COLING-ACL-98, pages 1103–1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets.</title>
<date>2005</date>
<booktitle>In Advances in Web Intelligence,</booktitle>
<volume>3528</volume>
<publisher>Springer Verlag.</publisher>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets. In Advances in Web Intelligence, volume 3528 of Lecture Notes in Computer Science. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatically generating Wikipedia articles: A structureaware approach.</title>
<date>2009</date>
<booktitle>In Proc. ofACL-IJCNLP-09,</booktitle>
<pages>208--216</pages>
<contexts>
<context position="6316" citStr="Sauper and Barzilay, 2009" startWordPosition="919" endWordPosition="922"> the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet,</context>
</contexts>
<marker>Sauper, Barzilay, 2009</marker>
<rawString>Christina Sauper and Regina Barzilay. 2009. Automatically generating Wikipedia articles: A structureaware approach. In Proc. ofACL-IJCNLP-09, pages 208–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Libby Barak</author>
<author>Ido Dagan</author>
</authors>
<title>Extracting lexical reference rules from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP-09,</booktitle>
<pages>450--458</pages>
<contexts>
<context position="5905" citStr="Shnarch et al., 2009" startWordPosition="868" endWordPosition="871">d Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on pre</context>
</contexts>
<marker>Shnarch, Barak, Dagan, 2009</marker>
<rawString>Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting lexical reference rules from Wikipedia. In Proc. of ACL-IJCNLP-09, pages 450–458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogeneous evidence.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL-06,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="4293" citStr="Snow et al., 2006" startWordPosition="624" endWordPosition="627">resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-a</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogeneous evidence. In Proc. of COLING-ACL-06, pages 801– 808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from Wikipedia and WordNet.</title>
<date>2008</date>
<journal>Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="5770" citStr="Suchanek et al., 2008" startWordPosition="846" endWordPosition="849">eb encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show</context>
<context position="7330" citStr="Suchanek et al., 2008" startWordPosition="1091" endWordPosition="1094">ample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is a</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from Wikipedia and WordNet. Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel Weld</author>
</authors>
<title>Automatically semantifying Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM-07,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="5689" citStr="Wu and Weld, 2007" startWordPosition="834" endWordPosition="837"> overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation </context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel Weld. 2007. Automatically semantifying Wikipedia. In Proc. of CIKM-07, pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel Weld</author>
</authors>
<title>Automatically refining the Wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proc. of WWW08,</booktitle>
<pages>635--644</pages>
<contexts>
<context position="5790" citStr="Wu and Weld, 2008" startWordPosition="850" endWordPosition="853">e of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harv</context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>Fei Wu and Daniel Weld. 2008. Automatically refining the Wikipedia infobox ontology. In Proc. of WWW08, pages 635–644.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>