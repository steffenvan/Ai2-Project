<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000509">
<title confidence="0.948846">
Domain adaptive bootstrapping for named entity recognition
</title>
<note confidence="0.308484">
Dan Wui, Wee Sun Lee&apos;, Nan Yee Hai Leong Chieu
&apos;Singapore MIT Alliance DSO National Laboratories
&apos;Department of Computer Science chaileon@dso.org.sg
National University of Singapore
{dwu@,leews@comp,g0701171@}nus.edu.sg
</note>
<sectionHeader confidence="0.978042" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917153846154">
Bootstrapping is the process of improving
the performance of a trained classifier by
iteratively adding data that is labeled by
the classifier itself to the training set, and
retraining the classifier. It is often used
in situations where labeled training data is
scarce but unlabeled data is abundant. In
this paper, we consider the problem of do-
main adaptation: the situation where train-
ing data may not be scarce, but belongs to
a different domain from the target appli-
cation domain. As the distribution of un-
labeled data is different from the training
data, standard bootstrapping often has dif-
ficulty selecting informative data to add to
the training set. We propose an effective
domain adaptive bootstrapping algorithm
that selects unlabeled target domain data
that are informative about the target do-
main and easy to automatically label cor-
rectly. We call these instances bridges, as
they are used to bridge the source domain
to the target domain. We show that the
method outperforms supervised, transduc-
tive and bootstrapping algorithms on the
named entity recognition task.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962076923077">
Most recent researches on natural language pro-
cessing (NLP) problems are based on machine
learning algorithms. High performance can often
be achieved if the system is trained and tested on
data from the same domain. However, the perfor-
mance of NLP systems often degrades badly when
the test data is drawn from a source that is differ-
ent from the labeled data used to train the system.
For named entity recognition (NER), for example,
Ciaramita and Altun (2005) reported that a system
trained on a labeled Reuters corpus achieved an
F-measure of 91% on a Reuters test set, but only
64% on a Wall Street Journal test set.
The task of adapting a system trained on one do-
main (called the source domain) to a new domain
(called the target domain) is called domain adap-
tation. In domain adaptation, it is generally as-
sumed that we have labeled data in the source do-
main while labeled data may or may not be avail-
able in the target domain. Previous work in do-
main adaptation can be classified into two cate-
gories: [S+T+], where a small, labeled target do-
main data is available, e.g. (Blitzer et al., 2006;
Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and
Manning, 2009), or [S+T-], where no labeled tar-
get domain data is available, e.g. (Blitzer et al.,
2006; Jiang and Zhai, 2007). In both cases, and es-
pecially for [S+T-], domain adaptation can lever-
age on large amounts of unlabeled data in the tar-
get domain. In practice, it is often unreasonable
to expect labeled data for every new domain that
we come across, such as blogs, emails, a different
newspaper agency, or simply articles from a differ-
ent topic or period in time. Thus although [S+T+]
is easier to handle, [S+T-] is of higher practical
importance.
In this paper, we propose a domain adaptive
bootstrapping (DAB) approach to tackle the do-
main adaptation problem under the setting [S+T-].
Bootstrapping is an iterative process that uses a
trained classifier to label and select unlabeled in-
stances to add to the training set for retraining
the classifier. It is often used when labeled train-
ing data is scarce but unlabeled data is abundant.
In contrast, for domain adaptation problems, we
may have a lot of training data but the target ap-
plication domain has a different data distribution.
Standard bootstrapping usually selects instances
that are most confidently labeled from the unla-
beled data. In domain adaptation situations, usu-
ally the most confidently labeled instances are the
ones that are most similar to the source domain in-
</bodyText>
<page confidence="0.774616">
1523
</page>
<note confidence="0.9965465">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1523–1532,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999779304347826">
stances - these instances tend to contain very little
information about the target domain. For domain
adaptive bootstrapping, we propose a selection cri-
terion that selects instances that are informative
and easy to automatically label correctly. In addi-
tion, we propose a criterion for stopping the pro-
cess of bootstrapping before it adds uninformative
and incorrectly labeled instances that can reduce
performance.
Our approach leverages on instances in the tar-
get domain called bridges. These instances con-
tain domain-independent features, as well as fea-
tures specific to the target domain. As they contain
domain-independent features, they can be classi-
fied correctly by classifiers trained on the source
domain labeled data. We argue that these instances
act as a bridge between the source and the target
domain. We show that, on the NER task, DAB
outperforms supervised, transductive and standard
bootstrapping algorithms, as well as a bootstrap-
ping variant, called balanced bootstrapping (Jiang
and Zhai, 2007), that has recently been proposed
for domain adaptation.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999295">
One general class of approaches to domain adap-
tation is to consider that the instances from the
source and the target domain are drawn from dif-
ferent distributions. Bickel et al. (Bickel et al.,
2007) discriminatively learns a scaling factor for
source domain training data, so as to adapt the
source domain data distribution to resemble the
target domain data distribution, under the [S+T-]
setting. Daume III and Marcu (Daum´e III and
Marcu, 2006) considers that the data distribution is
a mixture distribution over general, source domain
and target domain data. They learn the underlying
mixture distribution using the conditional expec-
tation maximization algorithm, under the [S+T+]
setting. Jiang and Zhai (2007) proposed an in-
stance re-weighting framework that handles both
the [S+T+] and [S+T-] settings. For [S+T-], the
resulting algorithm is a balanced bootstrapping al-
gorithm, which was shown to outperform the stan-
dard bootstrapping algorithm. In this paper, we
assume the [S+T-] settings, and we show that the
approach proposed in this paper, domain adaptive
bootstrapping (DAB), outperforms the balanced
bootstrapping algorithm on NER.
Another class of approaches to domain adap-
tation is feature-based. Daume III (Daum´e III,
2007) divided features into three classes: domain-
independent features, source-domain features and
target-domain features. He assumed the existence
of training data in the target-domain (under the
setting [S+T+]), so that the three classes of fea-
tures can be jointly trained using source and target
domain labeled data. This cannot be done in the
setting [S+T-], where no training data is available
in the target domain. Using a different approach,
Blitzer et al. (2006) induces correspondences be-
tween feature spaces in different domains, by de-
tecting pivot features. Pivot features are features
that occur frequently and behave similarly in dif-
ferent domains. Pivot features are used to put
domain-specific features in correspondence. In
this paper, instead of pivot features, we attempt
to leverage on pivot instances that we call bridges,
which are instances that bridge the source and tar-
get domain. This will be illustrated in Section 3.
It is generally recognized that adding informa-
tive and correctly labeled instances is more useful
for learning. Active learning queries the user for
labels of most informative or relevant instances.
Active learning, which has been applied to the
problem of NER in (Shen et al., 2004), is used in
situations where a large amount of unlabeled data
exists and data labeling is expensive. It has also
been applied to the problem of domain adaptation
for word sense disambiguation in (Chan and Ng,
2007). However, active learning requires human
intervention. Here, we want to achieve the same
goal without human intervention.
</bodyText>
<sectionHeader confidence="0.995425" genericHeader="method">
3 Bootstrapping for domain adaptation
</sectionHeader>
<bodyText confidence="0.999805705882353">
We first define the notations used for domain adap-
tation in the [S+T-] setting. A set of training data
DS = {xi7 yi}1≤i≤|DS |is given in the source do-
main, where the notation |X |denotes the size of a
set X. Each instance xi in DS has been manually
annotated with a label, yi, from a given set of la-
bels Y . The objective of domain adaptation is to
label a set of unlabeled data, DT = {xi}1≤i≤|D, |
with labels from Y . A machine learning algorithm
will take a labeled data set (for e.g. DS) and out-
puts a classifier, which can then be used to classify
unlabeled data, i.e. assign labels to unlabeled in-
stances.
A special class of machine learning algorithms,
called transductive learning algorithms, is able to
take the unlabeled data DT into account during
the learning process (see e.g. (Joachims, 1999)).
</bodyText>
<page confidence="0.991564">
1524
</page>
<bodyText confidence="0.9995745">
However, such algorithms do not take into account
the shift in domain of the test data. Jiang and Zhai
(2007) recently proposed an instance re-weighting
framework to take domain shift into account. For
[S+T-], the resulting algorithm is a balanced boot-
strapping algorithm, which we describe below.
</bodyText>
<subsectionHeader confidence="0.999497">
3.1 Standard and balanced bootstrapping
</subsectionHeader>
<bodyText confidence="0.99874204">
We define a general bootstrapping algorithm in Al-
gorithm 1. The algorithm can be applied to any
machine learning algorithm that allows training in-
stances to be weighted, and that gives confidence
scores for the labels when used to classify test
data. The bootstrapping procedure iteratively im-
proves the performance of a classifier SCt over a
number of iterations. In Algorithm 1, we have left
a number of parameters unspecified. These param-
eters are (1) the selection-criterion for instances to
be added to the training data, (2) the termination-
criterion for the bootstrapping process, and (3) the
weights (wS, wT) given to the labeled and boot-
strapped training sets.
Standard bootstrapping: (Jiang and Zhai,
2007) the selection-criterion is based on selecting
the top k most-confidently labeled instances in Rt.
The weight wSt is equal to wTt . The value of k is a
parameter for the bootstrapping algorithm.
Balanced bootstrapping: (Jiang and Zhai,
2007) the selection-criterion is still based on se-
lecting the top k most-confidently labeled in-
stances in Rt. Balanced bootstrapping was for-
mulated for domain adaptation, and hence they set
the weights to satisfy the ratio wT = �iTtI This
</bodyText>
<equation confidence="0.691766">
t
</equation>
<bodyText confidence="0.998179636363637">
allows the small amount of target data added, Tt,
to have an equal weight to the large source domain
training set DS.
In this paper, we formulate a selection-criterion
and a termination-criterion which are better than
those used in standard and balanced bootstrap-
ping. Regarding the selection-criterion, standard
and balanced bootstrapping both select instances
which are confidently labeled by SCt to be used
for training SCt+1, in the hope of avoiding us-
ing wrongly labeled data in bootstrapping. How-
ever, instances that are already confidently labeled
by SCt may not contain sufficient information
which is not in DS, and using them to train SCt+1
may result in SCt+1 performing similarly to SCt.
This motivates us to select samples which are both
informative and easy to automatically label cor-
rectly. Regarding the termination-criterion, which
Algorithm 1 Bootstrapping algorithm
Input: labeled data DS, test data DT and a ma-
chine learning algorithm.
Output: the predicted labels of the set DT.
</bodyText>
<equation confidence="0.65556">
Set T0 = 0, R0 = DT, and t = 0
Repeat
</equation>
<listItem confidence="0.995514">
1. learn a classifier SCt with (DS, Tt) with
weights (wSt , wTt )
2. label the set Rt with SCt
3. select St C Rt based on selection-criterion
4. Tt+1 = Tt U St, and Rt+1 = Rt \ St.
</listItem>
<bodyText confidence="0.9770271">
Until termination-criterion
Output the predicted labels of DT by SCt.
is not mentioned in the paper (Jiang and Zhai,
2007), we assume that bootstrapping is simply run
for either a single iteration, or a small and fixed
number of iterations. However, it is known that
such simple criterion may result in stopping too
early or too late, leading to sub-optimal perfor-
mance. We propose a more effective termination-
criterion here.
</bodyText>
<subsectionHeader confidence="0.999499">
3.2 Domain adaptive bootstrapping (DAB)
</subsectionHeader>
<bodyText confidence="0.999787923076923">
Our selection-criterion relies on the observation
that in domain adaptation, instances (from the
source or the target domain) can be divided into
three types according to their information content:
generalists are instances that contain only domain-
independent information and are present in all do-
mains; specialists are instances containing only
domain-specific information and are present only
in their respective domains; bridges are instances
containing both domain-independent and domain-
specific information, also present only in their re-
spective domains but are useful as a “bridge” be-
tween the source and the target domains.
The implication of the above observation is
that when choosing unlabeled target domain data
for bootstrapping, we should exploit the bridges,
because the generalists are not likely to contain
much information not in DS due to their domain-
independence, and the specialists are difficult to be
labeled correctly due to their domain-specificity.
In contrast, the bridges are informative and eas-
ier to label correctly. Choosing confidently clas-
sified instances for bootstrapping, as in standard
bootstrapping and balanced bootstrapping, is sim-
ple, but results in choosing mostly generalists, and
is too conservative. We design a scoring function
</bodyText>
<page confidence="0.950931">
1525
</page>
<bodyText confidence="0.991454052631579">
on instances, which has high value when the in-
stance is informative and sufficiently likely to be
correctly labeled in order to identify correctly la-
beled bridges.
Intuitively, informativeness of an instance can
be measured by the prediction results of the ideal
classifier IS for the source domain and the ideal
classifier IT for the target domain. If IS and IT
are both probabilistic classifiers, IS should return
a noninformative distribution while IT should re-
turn an informative one. The ideal classifier for the
source domain is approximated with a source clas-
sifier SC trained on DS, while the ideal classifier
for the target domain is approximated by training a
classifier, TC, on target domain instances labeled
by the source classifier.
We also try to ensure that instances that are se-
lected are correctly classified. As the label used
is provided by the target classifier, we estimate
the precision of the target classification. The final
ranking function is constructed by combining this
estimate with the informativeness of the instance.
We show the algorithm for the instance selec-
tion in Algorithm 2. The notations used follow
those used in Algorithm 1. For simplicity, we as-
sume that wSt = wTt = 1 for all t. We expect
TC to be a reasonable classifier on DT due to the
presence of generalists and bridges. Note that the
target classifier is constructed by randomly split-
ting DT into two partitions, training a classifier
on each partition and using the prediction of the
trained classifier on the partition it is not trained
on. This is because classifiers tend to fit the data
that they have been trained on too well making the
probability estimates on their training data unreli-
able. Also, a random partition is used to ensure
that the data in each partition is representative of
Du.
</bodyText>
<subsectionHeader confidence="0.988005">
3.3 The scoring function: score(p(8), p(0)
</subsectionHeader>
<bodyText confidence="0.998954222222222">
The scoring function score(p(s), p(t)) in Algo-
rithm 2 is simply implemented as the product of
two components: a measure of the informative-
ness and the probability that SC’s label is correct.
We show how the intuitive ideas (described above)
behind these two components are formalized.
Informativeness of a distribution p on a set of
discrete labels Y is measured by its entropy h(p)
defined by
</bodyText>
<equation confidence="0.988039">
h(p) = − 1] p(y) log p(y)-
yEY
</equation>
<bodyText confidence="0.986441714285714">
Algorithm 2 Algorithm for selecting instances for
bootstrapping at iteration t
Input: Labeled source domain data DS, target do-
main training data Tt, remaining data Rt, the clas-
sifier SCt trained on DS U Tt, and a scoring func-
tion score(p(s), p(t))
Output: k instances for bootstrapping.
</bodyText>
<listItem confidence="0.998050333333333">
1. Label Rt with SCt, and to each instance xi E
Rt, SCt outputs a distribution p(s)
i (yi) over
its labels.
2. Randomly split Rt into two partitions, R0t
and R1t with their labels assigned by SCt.
3. Train each target classifier, TCxt with the data
Rxt , for x = 10, 11.
4. Label R(1−x) with the classifier TCx t , which
</listItem>
<equation confidence="0.8480775">
t
to each instance xi E Rt, outputs a distribu-
tion p(t)
i (yi) over its labels.
</equation>
<listItem confidence="0.9939378">
5. Score each instance from xi E Rt with the
function score (p s), t))
i pi
6. Select top k instances from Rt with the high-
est scores.
</listItem>
<bodyText confidence="0.99819362962963">
h(p) is nonnegative; h(p) = 0 if and only if p
has probability 1 on one of the labels; h(p) attains
its maximum value when the distribution p is uni-
form over all labels. Hence, an instance is clas-
sified with high confidence when the distribution
over its labels has low entropy.
We measure the informativeness of an instance
using h(p(s)) − h(p(t)), where p(s) and p(t) are as
in Algorithm 2. We argue that a larger value of this
expression implies that the instance is more likely
to be a bridge instance. This expression has a high
value when the source classifier is uncertain, and
the target classifier is certain. Uncertain classifi-
cation by the source classifier indicates that the in-
stance is unlikely to be a generalist. Moreover, if
the target classifier is certain on xi, it means that
instances similar to the instance xi are consistently
labeled with the same label by the source classifier
SCt, indicating that it is likely to be a bridge in-
stance.
The probability that TC’s label is correct can-
not be estimated directly because we do not have
labeled target domain data. Instead, we use the
source domain to give an estimate. We do this with
a simple pre-processing step: we split the data DS
into two partitions of equal size, train a classifier
on each partition, and test each classifier on the
</bodyText>
<page confidence="0.966883">
1526
</page>
<bodyText confidence="0.99989625">
other partition. We then measure the resulting ac-
curacy given each label:
Summarizing the above discussion, the scoring
function is as shown below.
</bodyText>
<equation confidence="0.9657905">
[ ]
score(p(s), p(t)) = �(y*) h(p(s))−h(p(t)) ,
where y* = arg maxp(s)(y)
yEY
</equation>
<bodyText confidence="0.999979666666667">
The scoring function has a high value when the
information content of the example is high and the
label has high precision.
</bodyText>
<subsectionHeader confidence="0.969431">
3.4 The termination criterion
</subsectionHeader>
<bodyText confidence="0.999972">
Intuitively, our algorithm terminates when there
are not enough informative instances. Formally,
we define the termination criterion as follows: we
terminate the bootstrapping process when, there
exists an instance xi in the top k instances satis-
fying the following condition:
</bodyText>
<equation confidence="0.9927385">
1. h(p(s)
i ) &lt; h(p(t)
i ), or
2. maxyEY p(s)
i (y) &gt; maxyEY p(t)
i (y)
</equation>
<bodyText confidence="0.9999514">
The second case is used to check for instances
where the classifier SCt is more confident than
the target classifiers TCt , on their respective pre-
dicted labels. This shows that the instance xi is
more of a generalist than a bridge.
</bodyText>
<sectionHeader confidence="0.951923" genericHeader="method">
4 NER task and implementation
</sectionHeader>
<bodyText confidence="0.9999868">
The algorithm described in Section 3 is not spe-
cific to any particular application. In this paper,
we apply it to the problem of named entity recog-
nition (NER). In this section, we describe the NER
classifier and the features used in our experiments.
</bodyText>
<subsectionHeader confidence="0.906449">
4.1 NER features
</subsectionHeader>
<bodyText confidence="0.99999675">
We used the features generated by the CRF pack-
age (Finkel et al., 2005). These features include
the word string feature, the case feature for the cur-
rent word, the context words for the current word
and their cases, the presence in dictionaries for the
current word, the position of the current word in
the sentence, prefix and suffix of the current word
as well as the case information of the multiple oc-
currences of the current word. We use the same
set of features for all classifiers used in the boot-
strapping process, and for all baselines used in the
experimental section.
</bodyText>
<subsectionHeader confidence="0.921294">
4.2 Machine learning algorithms
</subsectionHeader>
<bodyText confidence="0.999457085106383">
A base machine learning algorithm is required in
bootstrapping approaches. We describe the two
machine learning algorithms used in this paper.
We chose these algorithms for their good perfor-
mance on the NER task.
Maximum entropy classification (MaxEnt):
The MaxEnt approach, or logistic regression, is
one of the most competitive methods for named
entity recognition (Tjong and Meulder, 2003).
MaxEnt is a discriminative method that learns a
distribution, p(yi|xi), over the labels, yi, given
the vector of features, xi. We used the imple-
mentation of MaxEnt classifier described in (Man-
ning and Klein, 2003). For NER, each instance
represents a single word token within a sentence,
with the feature vector xi derived from the sen-
tence as described in the previous section. Max-
Ent is not designed for sequence classification. To
deal with sequences, each name-class (e.g. PER-
SON) is divided into sub-classes: first token (e.g.
PERSON-begin), unique token (e.g. PERSON-
unique), or subsequent tokens (e.g. PERSON-
continue) in the name-class. To ensure that the
results returned by MaxEnt is coherent, we de-
fine deterministic transition probabilities that dis-
allow transitions such as one from PERSON-begin
to LOCATION-continue. A Viterbi parse is used
to find the valid sequence of name-classes with the
highest probability.
Support vector machines (SVM): The basic
idea behind SVM for binary classification prob-
lems is to consider the data points in their fea-
ture space, and to separate the two classes with a
hyper-plane, by maximizing the shortest distance
between the data points and the hyper-plane. If
there exists no hyperplane that can split the two la-
bels, the soft margin version of SVM will choose
a hyperplane that splits the examples as cleanly as
possible, while still maximizing the distance to the
nearest cleanly split examples (Joachims, 2002).
We used the SVM&amp;quot;ght package for our experi-
ments (Joachims, 2002). For the multi-label NER
classification with N classes, we learn N SVM
classifiers, and use a softmax function to obtain
the distribution. Formally, denoting by s(y) the
confidence returned by the classifier for each label
y E Y , the probability of the label yi is given by
</bodyText>
<equation confidence="0.9750925">
exp(s(yi))
p(yi|xi) =
EyEY exp(s(y))
# correctly labeled instances of label y
P(y) =
�
</equation>
<bodyText confidence="0.814465">
# total instances of label y
</bodyText>
<page confidence="0.977436">
1527
</page>
<bodyText confidence="0.999672125">
Similarly to MaxEnt, we subdivide name-classes
into begin, continue, and unique sub-classes, and
use a Viterbi parse for the sequence of highest
probability. The SVM&amp;quot;ght package also imple-
ments a transductive version of the SVM algo-
rithm. We also compare our approach with the
transductive SVM (Joachims, 1999) in our experi-
mental results.
</bodyText>
<sectionHeader confidence="0.99594" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.999975048780488">
In this paper, we use the annotated data provided
by the Automatic Content Extraction (ACE) pro-
gram. The ACE data set is annotated for an Entity
Detection task, and the annotation consists of the
labeling of entity names (e.g. Powell) and men-
tions for each entity (e.g. pronouns such as he).
In this paper, we are interested in the problem of
recognition of the proper names (the named entity
recognition task), and hence use only entities la-
beled with the type NAM (LDC, 2005). Entities
are classified into seven types: Person entities are
humans mentioned in a document; Organization
entities are limited to established associations of
people; Geo-political entities are geographical ar-
eas defined by political and/or social groups; Lo-
cation entities are geographical items like land-
masses and bodies of water; Facility entities re-
fer to buildings and real estate improvements; Ve-
hicle entities are devices used for transportation;
and Weapon entities are devices used for harming
or destruction.
We compare performances of a few algorithms:
MaxEnt classifier (MaxEnt); MaxEnt classifier
with standard bootstrapping (MaxEnt-SB); bal-
anced bootstrapping based on MaxEnt classi-
fier (MaxEnt-BB); MaxEnt with DAB (MaxEnt-
DAB); SVM classifier (SVM); transductive SVM
classifier (SVM-Trans); and DAB based on SVM
classifier (SVM-DAB). No regularization is used
for MaxEnt classifiers. SVM classifiers use a
value of 10 for parameter C (trade-off between
training error and margin). Bootstrapping based
algorithms are run for 30 iterations and 100 in-
stances are selected in every iteration.
The evaluation measure used is the F-measure.
F-measure is the harmonic mean of precision and
recall, and is commonly used to evaluate NER
systems. We use the scorer for CONLL 2003
shared task (Tjong and Meulder, 2003) where the
F-measure is computed by averaging F-measures
for name-classes, weighted by the number of oc-
</bodyText>
<table confidence="0.999664">
Code Source Num docs
NW Newswire 81
BC Broadcast conversation 52
WL Weblog 114
CTS Conversational Telephone Speech 34
</table>
<tableCaption confidence="0.8653975">
Table 1: The sources, and the number of docu-
ments in each source, in the ACE 2005 data set.
</tableCaption>
<bodyText confidence="0.801969">
currences.
</bodyText>
<subsectionHeader confidence="0.996412">
5.1 Cross-source transfer
</subsectionHeader>
<bodyText confidence="0.998225">
The ACE 2005 data set consists of articles drawn
from a variety of sources. We use the four cate-
gories shown in Table 1. Each category is consid-
ered to be a domain, and we consider each pair of
categories as the source and the target domain in
turn.
Figure 1 compares the performance of MaxEnt-
SB, MaxEnt-BB and MaxEnt-DAB over multiple
iterations. Figure 2 compares the performance
of SVM, SVM-Trans and SVM-DAB. Each line
in the figures represents the average F-measure
across all the domains over many iterations. When
the termination condition is met for one domain,
its F-measure remains at the value of the final iter-
ation.
Despite a large number of iterations, both stan-
dard and balanced bootstrapping fail to improve
performance. Supervised learning performance on
each domain is shown in Table 3 (by 2-fold cross-
validation with random ordering) as a reference.
In Table 5, we compare the F-measures obtained
by different algorithms at the last iteration they
were run. We will discuss more on this in Sec-
tion 5.3.
</bodyText>
<subsectionHeader confidence="0.99968">
5.2 Cross-topic transfer
</subsectionHeader>
<bodyText confidence="0.998841933333333">
This data set is constructed from 175 articles from
the ACE 2005 corpus. The data set is used to eval-
uate transfer across topics. We manually classify
the articles into 4 categories: military operations
(MO), political relationship or politicians (POL),
terrorism-related (TER), and those which are not
in the above categories (OTH). A detailed break-
down of the number of documents in the each
topic is given in Table 2.
Supervised learning performance on each do-
main is shown in Table 4 (by 2-fold cross-
validation with random ordering) as a reference.
Experimental results on cross-topic evaluation are
shown in Table 6. Figure 3 compares the perfor-
mance of MaxEnt-SB, MaxEnt-BB and MaxEnt-
</bodyText>
<page confidence="0.974838">
1528
</page>
<figure confidence="0.996194">
0 5 10 15 20 25 30 35
Number of iterations
</figure>
<figureCaption confidence="0.9829025">
Figure 1: Average performance on the cross-
source transfer using MaxEnt classifier.
</figureCaption>
<figure confidence="0.996692">
0 5 10 15 20 25 30 35
Number of iterations
</figure>
<figureCaption confidence="0.9872185">
Figure 3: Average performance on the cross-topic
transfer using MaxEnt classifier.
</figureCaption>
<table confidence="0.9989086">
Topic Topic description # docs
MO Military operations 92
POL Political relationships 40
TER Terrorist-related 28
OTH None of the above 15
</table>
<tableCaption confidence="0.960451666666667">
Table 2: The topics, their descriptions, and the
number of training and test documents in each
topic.
</tableCaption>
<table confidence="0.9998408">
Domain MaxEnt SVM
NW 82.47 82.32
BC 78.21 77.91
WL 71.41 71.84
CTS 93.90 94.01
</table>
<tableCaption confidence="0.923386">
Table 3: F-measure of supervised learning on the
cross-source target domains.
</tableCaption>
<bodyText confidence="0.7930176">
DAB over multiple iterations. Figure 4 compares
the performance of SVM, SVM-Trans and SVM-
DAB. Similar to cross-source transfer, standard
and balanced bootstrapping perform badly. This
will be discussed in Section 5.3.
</bodyText>
<figure confidence="0.9346175">
1 2 3 4 5 6
Number of iterations
</figure>
<figureCaption confidence="0.975929">
Figure 2: Average performance on the cross-
source transfer using SVM classifier.
</figureCaption>
<figure confidence="0.9884225">
1 2 3 4
Number of iterations
</figure>
<figureCaption confidence="0.990071">
Figure 4: Average performance on the cross-topic
transfer using SVM classifier.
</figureCaption>
<table confidence="0.9996546">
Domain MaxEnt SVM
MO 80.52 80.6
POL 77.99 79.05
TER 81.74 82.12
OTH 71.33 72.08
</table>
<tableCaption confidence="0.9912155">
Table 4: F-measure of supervised learning on the
cross-topic target domains.
</tableCaption>
<subsectionHeader confidence="0.989766">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9996985">
We show in our experiments that DAB outper-
forms standard and balanced bootstrapping, as
well as the transductive SVM. We have also shown
DAB to be robust across two state-of-the-art clas-
sifiers, MaxEnt and SVM. Balanced bootstrapping
has been shown to be more effective for domain
adaptation than standard bootstrapping (Jiang and
Zhai, 2007) for named entity classification on a
subset of the dataset used here. In contrast, we
found that both methods perform poorly on do-
main adaptation for NER. In named entity clas-
sification, the names have already been segmented
out and only need to be classified with the appro-
priate class. However, for NER, the names also
</bodyText>
<figure confidence="0.99930884">
62.0
61.5
61.0
60.5
60.0
59.5
59.0
58.5
58.0
57.5
MaxEnt-DAB
MaxEnt-SB
MaxEnt-BB
MaxEnt-DAB
MaxEnt-SB
MaxEnt-BB
SVM-DAB
SVM
SVM-Trans
SVM-DAB
SVM
SVM-Trans
68.2
68.0
67.8
67.6
67.4
67.2
67.0
66.8
66.6
66.4
66.2
70.0
65.0
60.0
55.0
50.0
45.0
40.0
35.0
72.0
70.0
68.0
66.0
64.0
62.0
60.0
58.0
56.0
</figure>
<page confidence="0.977007">
1529
</page>
<table confidence="0.999853357142857">
Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB
BC CTS 74.26 74.19 74.16 81.03 72.47 43.27 75.43
BC NW 64.81 64.76 64.80 66.20 64.08 43.01 64.39
BC WL 47.81 47.80 47.76 49.52 47.98 36.58 47.93
CTS BC 46.19 46.12 46.40 54.62 46.02 40.44 49.64
CTS NW 54.25 54.15 54.26 53.07 55.63 23.61 58.99
CTS WL 40.42 40.43 40.72 41.27 39.96 29.05 42.04
NW BC 59.90 59.83 59.80 60.55 59.89 45.71 58.42
NW CTS 66.64 66.48 66.59 66.73 68.28 28.80 73.47
NW WL 52.52 52.53 52.47 53.44 52.19 36.39 52.30
WL BC 58.58 58.79 58.65 56.00 58.43 52.64 58.64
WL CTS 64.63 63.89 64.50 80.45 65.96 45.04 81.04
WL NW 67.79 67.72 67.92 68.46 68.38 43.40 69.33
Average 58.15 58.06 58.17 60.95 58.27 39.00 60.97
</table>
<tableCaption confidence="0.951499">
Table 5: F-measure of the cross-source transfer.
</tableCaption>
<table confidence="0.999948714285714">
Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB
MO OTH 81.70 81.48 81.57 81.95 81.78 75.68 81.94
MO POL 73.21 73.11 73.28 74.97 72.56 58.13 72.66
MO TER 68.13 68.07 68.24 69.89 69.40 65.02 69.38
OTH MO 63.30 63.80 63.94 63.91 64.18 61.03 65.45
OTH POL 67.96 68.05 67.86 69.13 68.29 56.50 70.67
OTH TER 45.34 44.82 45.30 51.06 45.71 48.77 52.87
POL MO 62.14 62.12 61.95 61.94 61.98 51.67 62.32
POL OTH 77.91 77.72 77.79 76.58 78.11 65.71 78.13
POL TER 66.55 66.38 66.08 66.38 66.44 51.29 67.24
TER MO 58.35 58.62 58.02 57.29 58.30 49.80 58.14
TER OTH 66.83 67.61 66.83 68.97 66.28 58.25 68.12
TER POL 67.34 66.94 67.16 72.00 67.54 50.55 70.65
Average 66.56 66.56 66.50 67.84 66.71 57.70 68.13
</table>
<tableCaption confidence="0.996949">
Table 6: F-measure of the cross-topic transfer.
</tableCaption>
<bodyText confidence="0.999752918367347">
need to be separated from not-a-name instances.
We find that the addition of not-a-name instances
changes the problem - the not-a-names form most
of the instances classified with high confidence.
As a result, we find that both standard and bal-
anced bootstrapping fail to improve performance:
the selection of the most confident instances no
longer provide sufficient new information to im-
prove performance.
We also find that transductive SVM performs
poorly on this task. This is because it assumes
that the unlabeled data comes from the same dis-
tribution as the labeled data. In general, apply-
ing semi-supervised learning methods directly to
[S+T-] type domain adaptation problems do not
work and appropriate modifications need to be
made to the methods.
The ACE 2005 data set also contains a set of
ariticles from the broadcast news (BN) source
which is written entirely in lower case. This makes
NER much more difficult. However, when BN is
the source domain, the capitalization information
can be discovered by DAB. Figures 5 and 6 show
the average performance when BN is used as the
source domain and all other domains in Table 1 as
the target domains.
The source domain classifier tends to have high
precision and low recall, DAB results in an in-
crease in recall, with a small decrease in precision.
Testing the significance of the F-measure is not
trivial because the named entities wrongly labeled
by two classifiers are not directly comparable. We
tested the labeling disagreements instead, using a
McNemar paired test. The significance test is per-
formed on the improvement of MaxEnt-DAB over
MaxEnt and SVM-DAB over SVM. In most of
the domains for the cross-source transfer, the im-
provements are significant at a significance level
of 0.05, using MaxEnt classifier. The exceptional
train-test pairs are NW-WL and WL-BC. In the
case of WL-BC, this means the slight decrement in
performance is not statistically significant. Similar
result is achieved for the cross-source transfer us-
ing SVM classifier. In the cross-topic transfer, the
source domain and the target domain are not very
different. When we have a large amount of train-
ing data and little testing data, the gain of DAB
can be not statistically significant, as in the case
when we train with MO and POL domains.
</bodyText>
<page confidence="0.931966">
1530
</page>
<figure confidence="0.9876695">
1 2 3 4 5 6 7 8 9
Number of iterations
</figure>
<figureCaption confidence="0.9906135">
Figure 5: Performance on recovering capitaliza-
tion using MaxEnt classifier.
</figureCaption>
<figure confidence="0.9899745">
1 2 3 4
Number of iterations
</figure>
<figureCaption confidence="0.9928805">
Figure 6: Performance on recovering capitaliza-
tion using SVM classifier.
</figureCaption>
<figure confidence="0.99856952173913">
MaxEnt-DAB
MaxEnt-SB
MaxEnt-BB
SVM-DAB
SVM
SVM-Trans
46.0
44.0
42.0
40.0
38.0
36.0
34.0
32.0
30.0
28.0
50.0
45.0
40.0
35.0
30.0
25.0
20.0
</figure>
<sectionHeader confidence="0.96128" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999524">
We proposed a bootstrapping approach for domain
adaptation, and we applied it to the named entity
recognition task. Our approach leverages on in-
stances that serve as bridges between the source
and target domain. Empirically, our method out-
performs baseline approaches including super-
vised, transductive and standard bootstrapping ap-
proaches. It also outperforms balanced bootstrap-
ping, an approach designed for domain adaptation
(Jiang and Zhai, 2007).
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999430629032259">
Steffen Bickel, Michael Br¨uckner, and Tobias Scheffer.
2007. Discriminative learning for differing training
and test distributions. In ICML ’07: Proceedings of
the 24th international conference on Machine learn-
ing, pages 81–88, New York, NY, USA. ACM Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney, Aus-
tralia.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49–56, Prague, Czech Republic,
June. Association for Computational Linguistics.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Advances in Structured
Learning for Text and Speech Processing Workshop.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101–126.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Compu-
tational Linguistics (ACL), Prague, Czech Republic.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Main Conference, New York
City, USA. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ’05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363–370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 264–271,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML ’99: Proceedings of the Sixteenth Interna-
tional Conference on Machine Learning, pages 200–
209, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
T. Joachims. 2002. Learning to Classify Text Using
Support Vector Machines – Methods, Theory, and
Algorithms. Kluwer/Springer.
Linguistic Data Consortium LDC. 2005. ACE
(Automatic Content Extraction) English Annotation
Guidelines for Entities.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In NAACL ’03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
</reference>
<page confidence="0.810282">
1531
</page>
<reference confidence="0.999062357142857">
Human Language Technology, pages 8–8, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based ac-
tive learning for named entity recognition. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 589–596, Barcelona, Spain, July.
Erik Tjong and Fien De Meulder. 2003. Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of Conference on Computational Natural Lan-
guage Learning.
</reference>
<page confidence="0.994264">
1532
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966007">
<title confidence="0.999115">Domain adaptive bootstrapping for named entity recognition</title>
<author confidence="0.998219">Wee Sun Nan Hai Leong</author>
<affiliation confidence="0.993143666666667">MIT DSO National Laboratories of Computer chaileon@dso.org.sg National University of Singapore</affiliation>
<abstract confidence="0.999516148148148">Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier. It is often used in situations where labeled training data is scarce but unlabeled data is abundant. In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain. As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set. We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data are about the target doto automatically label cor- We call these instances as they are used to bridge the source domain to the target domain. We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steffen Bickel</author>
<author>Michael Br¨uckner</author>
<author>Tobias Scheffer</author>
</authors>
<title>Discriminative learning for differing training and test distributions.</title>
<date>2007</date>
<booktitle>In ICML ’07: Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>81--88</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<marker>Bickel, Br¨uckner, Scheffer, 2007</marker>
<rawString>Steffen Bickel, Michael Br¨uckner, and Tobias Scheffer. 2007. Discriminative learning for differing training and test distributions. In ICML ’07: Proceedings of the 24th international conference on Machine learning, pages 81–88, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2483" citStr="Blitzer et al., 2006" startWordPosition="403" endWordPosition="406">system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set. The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation. In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain. Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007). In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain. In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time. Thus although [S+T+] is easier to handle, [S+T-] is of higher practical </context>
<context position="6837" citStr="Blitzer et al. (2006)" startWordPosition="1089" endWordPosition="1092">AB), outperforms the balanced bootstrapping algorithm on NER. Another class of approaches to domain adaptation is feature-based. Daume III (Daum´e III, 2007) divided features into three classes: domainindependent features, source-domain features and target-domain features. He assumed the existence of training data in the target-domain (under the setting [S+T+]), so that the three classes of features can be jointly trained using source and target domain labeled data. This cannot be done in the setting [S+T-], where no training data is available in the target domain. Using a different approach, Blitzer et al. (2006) induces correspondences between feature spaces in different domains, by detecting pivot features. Pivot features are features that occur frequently and behave similarly in different domains. Pivot features are used to put domain-specific features in correspondence. In this paper, instead of pivot features, we attempt to leverage on pivot instances that we call bridges, which are instances that bridge the source and target domain. This will be illustrated in Section 3. It is generally recognized that adding informative and correctly labeled instances is more useful for learning. Active learnin</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7809" citStr="Chan and Ng, 2007" startWordPosition="1246" endWordPosition="1249">t we call bridges, which are instances that bridge the source and target domain. This will be illustrated in Section 3. It is generally recognized that adding informative and correctly labeled instances is more useful for learning. Active learning queries the user for labels of most informative or relevant instances. Active learning, which has been applied to the problem of NER in (Shen et al., 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive. It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). However, active learning requires human intervention. Here, we want to achieve the same goal without human intervention. 3 Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting. A set of training data DS = {xi7 yi}1≤i≤|DS |is given in the source domain, where the notation |X |denotes the size of a set X. Each instance xi in DS has been manually annotated with a label, yi, from a given set of labels Y . The objective of domain adaptation is to label a set of unlabeled data, DT = {xi}1≤i≤|D, | with labels from Y . A machine learning </context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Named-entity recognition in novel domains with external lexical knowledge.</title>
<date>2005</date>
<booktitle>In Advances in Structured Learning for Text and Speech Processing Workshop.</booktitle>
<contexts>
<context position="1846" citStr="Ciaramita and Altun (2005)" startWordPosition="284" endWordPosition="287"> to the target domain. We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task. 1 Introduction Most recent researches on natural language processing (NLP) problems are based on machine learning algorithms. High performance can often be achieved if the system is trained and tested on data from the same domain. However, the performance of NLP systems often degrades badly when the test data is drawn from a source that is different from the labeled data used to train the system. For named entity recognition (NER), for example, Ciaramita and Altun (2005) reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set. The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation. In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain. Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is </context>
</contexts>
<marker>Ciaramita, Altun, 2005</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2005. Named-entity recognition in novel domains with external lexical knowledge. In Advances in Structured Learning for Text and Speech Processing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Conference of the Association for Computational Linguistics (ACL),</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Conference of the Association for Computational Linguistics (ACL), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA.</location>
<contexts>
<context position="2550" citStr="Finkel and Manning, 2009" startWordPosition="414" endWordPosition="417">re of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set. The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation. In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain. Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007). In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain. In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time. Thus although [S+T+] is easier to handle, [S+T-] is of higher practical importance. In this paper, we propose a domain adaptive bootstrappi</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, New York City, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19018" citStr="Finkel et al., 2005" startWordPosition="3142" endWordPosition="3145">maxyEY p(t) i (y) The second case is used to check for instances where the classifier SCt is more confident than the target classifiers TCt , on their respective predicted labels. This shows that the instance xi is more of a generalist than a bridge. 4 NER task and implementation The algorithm described in Section 3 is not specific to any particular application. In this paper, we apply it to the problem of named entity recognition (NER). In this section, we describe the NER classifier and the features used in our experiments. 4.1 NER features We used the features generated by the CRF package (Finkel et al., 2005). These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word. We use the same set of features for all classifiers used in the bootstrapping process, and for all baselines used in the experimental section. 4.2 Machine learning algorithms A base machine learning algorithm is required in bootstrappi</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2505" citStr="Jiang and Zhai, 2007" startWordPosition="407" endWordPosition="410">beled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set. The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation. In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain. Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007). In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain. In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time. Thus although [S+T+] is easier to handle, [S+T-] is of higher practical importance. In this pa</context>
<context position="5057" citStr="Jiang and Zhai, 2007" startWordPosition="815" endWordPosition="818"> reduce performance. Our approach leverages on instances in the target domain called bridges. These instances contain domain-independent features, as well as features specific to the target domain. As they contain domain-independent features, they can be classified correctly by classifiers trained on the source domain labeled data. We argue that these instances act as a bridge between the source and the target domain. We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. 2 Related work One general class of approaches to domain adaptation is to consider that the instances from the source and the target domain are drawn from different distributions. Bickel et al. (Bickel et al., 2007) discriminatively learns a scaling factor for source domain training data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting. Daume III and Marcu (Daum´e III and Marcu, 2006) considers that the data distribution is a mixture distribution over general, s</context>
<context position="8893" citStr="Jiang and Zhai (2007)" startWordPosition="1437" endWordPosition="1440">s Y . The objective of domain adaptation is to label a set of unlabeled data, DT = {xi}1≤i≤|D, | with labels from Y . A machine learning algorithm will take a labeled data set (for e.g. DS) and outputs a classifier, which can then be used to classify unlabeled data, i.e. assign labels to unlabeled instances. A special class of machine learning algorithms, called transductive learning algorithms, is able to take the unlabeled data DT into account during the learning process (see e.g. (Joachims, 1999)). 1524 However, such algorithms do not take into account the shift in domain of the test data. Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which we describe below. 3.1 Standard and balanced bootstrapping We define a general bootstrapping algorithm in Algorithm 1. The algorithm can be applied to any machine learning algorithm that allows training instances to be weighted, and that gives confidence scores for the labels when used to classify test data. The bootstrapping procedure iteratively improves the performance of a classifier SCt over a number of iterations. In Alg</context>
<context position="11655" citStr="Jiang and Zhai, 2007" startWordPosition="1894" endWordPosition="1897"> select samples which are both informative and easy to automatically label correctly. Regarding the termination-criterion, which Algorithm 1 Bootstrapping algorithm Input: labeled data DS, test data DT and a machine learning algorithm. Output: the predicted labels of the set DT. Set T0 = 0, R0 = DT, and t = 0 Repeat 1. learn a classifier SCt with (DS, Tt) with weights (wSt , wTt ) 2. label the set Rt with SCt 3. select St C Rt based on selection-criterion 4. Tt+1 = Tt U St, and Rt+1 = Rt \ St. Until termination-criterion Output the predicted labels of DT by SCt. is not mentioned in the paper (Jiang and Zhai, 2007), we assume that bootstrapping is simply run for either a single iteration, or a small and fixed number of iterations. However, it is known that such simple criterion may result in stopping too early or too late, leading to sub-optimal performance. We propose a more effective terminationcriterion here. 3.2 Domain adaptive bootstrapping (DAB) Our selection-criterion relies on the observation that in domain adaptation, instances (from the source or the target domain) can be divided into three types according to their information content: generalists are instances that contain only domainindepend</context>
<context position="27737" citStr="Jiang and Zhai, 2007" startWordPosition="4558" endWordPosition="4561">2 3 4 Number of iterations Figure 4: Average performance on the cross-topic transfer using SVM classifier. Domain MaxEnt SVM MO 80.52 80.6 POL 77.99 79.05 TER 81.74 82.12 OTH 71.33 72.08 Table 4: F-measure of supervised learning on the cross-topic target domains. 5.3 Discussion We show in our experiments that DAB outperforms standard and balanced bootstrapping, as well as the transductive SVM. We have also shown DAB to be robust across two state-of-the-art classifiers, MaxEnt and SVM. Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. In contrast, we found that both methods perform poorly on domain adaptation for NER. In named entity classification, the names have already been segmented out and only need to be classified with the appropriate class. However, for NER, the names also 62.0 61.5 61.0 60.5 60.0 59.5 59.0 58.5 58.0 57.5 MaxEnt-DAB MaxEnt-SB MaxEnt-BB MaxEnt-DAB MaxEnt-SB MaxEnt-BB SVM-DAB SVM SVM-Trans SVM-DAB SVM SVM-Trans 68.2 68.0 67.8 67.6 67.4 67.2 67.0 66.8 66.6 66.4 66.2 70.0 65.0 60.0 55.0 50.0 45.0 40.0 35.0 72.0 70.0 68.0 66.0 64.0 62</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In ICML ’99: Proceedings of the Sixteenth International Conference on Machine Learning,</booktitle>
<pages>200--209</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8776" citStr="Joachims, 1999" startWordPosition="1418" endWordPosition="1419">size of a set X. Each instance xi in DS has been manually annotated with a label, yi, from a given set of labels Y . The objective of domain adaptation is to label a set of unlabeled data, DT = {xi}1≤i≤|D, | with labels from Y . A machine learning algorithm will take a labeled data set (for e.g. DS) and outputs a classifier, which can then be used to classify unlabeled data, i.e. assign labels to unlabeled instances. A special class of machine learning algorithms, called transductive learning algorithms, is able to take the unlabeled data DT into account during the learning process (see e.g. (Joachims, 1999)). 1524 However, such algorithms do not take into account the shift in domain of the test data. Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which we describe below. 3.1 Standard and balanced bootstrapping We define a general bootstrapping algorithm in Algorithm 1. The algorithm can be applied to any machine learning algorithm that allows training instances to be weighted, and that gives confidence scores for the labels when used to classify test data. The</context>
<context position="22187" citStr="Joachims, 1999" startWordPosition="3659" endWordPosition="3660"> a softmax function to obtain the distribution. Formally, denoting by s(y) the confidence returned by the classifier for each label y E Y , the probability of the label yi is given by exp(s(yi)) p(yi|xi) = EyEY exp(s(y)) # correctly labeled instances of label y P(y) = � # total instances of label y 1527 Similarly to MaxEnt, we subdivide name-classes into begin, continue, and unique sub-classes, and use a Viterbi parse for the sequence of highest probability. The SVM&amp;quot;ght package also implements a transductive version of the SVM algorithm. We also compare our approach with the transductive SVM (Joachims, 1999) in our experimental results. 5 Experimental results In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program. The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he). In this paper, we are interested in the problem of recognition of the proper names (the named entity recognition task), and hence use only entities labeled with the type NAM (LDC, 2005). Entities are classified into seven types: Person entities are humans</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In ICML ’99: Proceedings of the Sixteenth International Conference on Machine Learning, pages 200– 209, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to Classify Text Using Support Vector Machines – Methods, Theory, and Algorithms.</title>
<date>2002</date>
<publisher>Kluwer/Springer.</publisher>
<contexts>
<context position="21414" citStr="Joachims, 2002" startWordPosition="3530" endWordPosition="3531"> used to find the valid sequence of name-classes with the highest probability. Support vector machines (SVM): The basic idea behind SVM for binary classification problems is to consider the data points in their feature space, and to separate the two classes with a hyper-plane, by maximizing the shortest distance between the data points and the hyper-plane. If there exists no hyperplane that can split the two labels, the soft margin version of SVM will choose a hyperplane that splits the examples as cleanly as possible, while still maximizing the distance to the nearest cleanly split examples (Joachims, 2002). We used the SVM&amp;quot;ght package for our experiments (Joachims, 2002). For the multi-label NER classification with N classes, we learn N SVM classifiers, and use a softmax function to obtain the distribution. Formally, denoting by s(y) the confidence returned by the classifier for each label y E Y , the probability of the label yi is given by exp(s(yi)) p(yi|xi) = EyEY exp(s(y)) # correctly labeled instances of label y P(y) = � # total instances of label y 1527 Similarly to MaxEnt, we subdivide name-classes into begin, continue, and unique sub-classes, and use a Viterbi parse for the sequence of </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning to Classify Text Using Support Vector Machines – Methods, Theory, and Algorithms. Kluwer/Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium LDC</author>
</authors>
<date>2005</date>
<journal>ACE (Automatic Content Extraction) English Annotation Guidelines for Entities.</journal>
<contexts>
<context position="22717" citStr="LDC, 2005" startWordPosition="3751" endWordPosition="3752">gorithm. We also compare our approach with the transductive SVM (Joachims, 1999) in our experimental results. 5 Experimental results In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program. The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he). In this paper, we are interested in the problem of recognition of the proper names (the named entity recognition task), and hence use only entities labeled with the type NAM (LDC, 2005). Entities are classified into seven types: Person entities are humans mentioned in a document; Organization entities are limited to established associations of people; Geo-political entities are geographical areas defined by political and/or social groups; Location entities are geographical items like landmasses and bodies of water; Facility entities refer to buildings and real estate improvements; Vehicle entities are devices used for transportation; and Weapon entities are devices used for harming or destruction. We compare performances of a few algorithms: MaxEnt classifier (MaxEnt); MaxEn</context>
</contexts>
<marker>LDC, 2005</marker>
<rawString>Linguistic Data Consortium LDC. 2005. ACE (Automatic Content Extraction) English Annotation Guidelines for Entities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Dan Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on</booktitle>
<contexts>
<context position="20165" citStr="Manning and Klein, 2003" startWordPosition="3328" endWordPosition="3332">rning algorithms A base machine learning algorithm is required in bootstrapping approaches. We describe the two machine learning algorithms used in this paper. We chose these algorithms for their good performance on the NER task. Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is one of the most competitive methods for named entity recognition (Tjong and Meulder, 2003). MaxEnt is a discriminative method that learns a distribution, p(yi|xi), over the labels, yi, given the vector of features, xi. We used the implementation of MaxEnt classifier described in (Manning and Klein, 2003). For NER, each instance represents a single word token within a sentence, with the feature vector xi derived from the sentence as described in the previous section. MaxEnt is not designed for sequence classification. To deal with sequences, each name-class (e.g. PERSON) is divided into sub-classes: first token (e.g. PERSON-begin), unique token (e.g. PERSONunique), or subsequent tokens (e.g. PERSONcontinue) in the name-class. To ensure that the results returned by MaxEnt is coherent, we define deterministic transition probabilities that disallow transitions such as one from PERSON-begin to LOC</context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without magic. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on</rawString>
</citation>
<citation valid="false">
<authors>
<author>Human Language Technology</author>
</authors>
<pages>8--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Technology, </marker>
<rawString>Human Language Technology, pages 8–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>Chew-Lim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>589--596</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7594" citStr="Shen et al., 2004" startWordPosition="1209" endWordPosition="1212">requently and behave similarly in different domains. Pivot features are used to put domain-specific features in correspondence. In this paper, instead of pivot features, we attempt to leverage on pivot instances that we call bridges, which are instances that bridge the source and target domain. This will be illustrated in Section 3. It is generally recognized that adding informative and correctly labeled instances is more useful for learning. Active learning queries the user for labels of most informative or relevant instances. Active learning, which has been applied to the problem of NER in (Shen et al., 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive. It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). However, active learning requires human intervention. Here, we want to achieve the same goal without human intervention. 3 Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting. A set of training data DS = {xi7 yi}1≤i≤|DS |is given in the source domain, where the notation |X |denotes the size of a set X. Each instance xi</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 589–596, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of Conference on Computational Natural Language Learning.</booktitle>
<marker>Tjong, De Meulder, 2003</marker>
<rawString>Erik Tjong and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of Conference on Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>