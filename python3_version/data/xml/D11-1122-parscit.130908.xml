<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004608">
<title confidence="0.994589">
Unsupervised Semantic Role Induction with Graph Partitioning
</title>
<author confidence="0.998215">
Joel Lang and Mirella Lapata
</author>
<affiliation confidence="0.999521">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.987984">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.99875">
J.Lang-3@sms.ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995847" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995907529411765">
In this paper we present a method for unsuper-
vised semantic role induction which we for-
malize as a graph partitioning problem. Ar-
gument instances of a verb are represented as
vertices in a graph whose edge weights quan-
tify their role-semantic similarity. Graph par-
titioning is realized with an algorithm that it-
eratively assigns vertices to clusters based on
the cluster assignments of neighboring ver-
tices. Our method is algorithmically and con-
ceptually simple, especially with respect to
how problem-specific knowledge is incorpo-
rated into the model. Experimental results on
the CoNLL 2008 benchmark dataset demon-
strate that our model is competitive with other
unsupervised approaches in terms of F1 whilst
attaining significantly higher cluster purity.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9905445">
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the se-
mantic relations that hold between a predicate and
its arguments (e.g., “who” did “what” to “whom”,
“when”, “where”, and “how”) abstracting over sur-
face syntactic configurations.
In the example sentences below, window occu-
pies different syntactic positions — it is the object of
broke in sentences (1a,b), and the subject in (1c) —
while bearing the same semantic role, i.e., the phys-
ical object affected by the breaking event. Analo-
gously, ball is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
</bodyText>
<figure confidence="0.4467985">
(1) a. [Jim]A0 broke the [window]A1 with a
[ball]A2.
b. The [ball]A2 broke the [window]A1.
c. The [window]A1 broke [last night]TMP.
</figure>
<bodyText confidence="0.999912772727273">
The semantic roles in the examples are labeled in
the style of PropBank (Palmer et al., 2005), a broad-
coverage human-annotated corpus of semantic roles
and their syntactic realizations. Under the Prop-
Bank annotation framework (which we will assume
throughout this paper) each predicate is associated
with a set of core roles (named A0, A1, A2, and so
on) whose interpretations are specific to that predi-
cate1 and a set of adjunct roles such as location or
time whose interpretation is common across predi-
cates (e.g., last night in sentence (1c)).
The availability of PropBank and related re-
sources (e.g., FrameNet; Ruppenhofer et al. (2006))
has sparked the development of great many seman-
tic role labeling systems most of which conceptu-
alize the task as a supervised learning problem and
rely on role-annotated data for model training. Most
of these systems implement a two-stage architec-
ture consisting of argument identification (determin-
ing the arguments of the verbal predicate) and ar-
gument classification (labeling these arguments with
semantic roles). Despite being relatively shallow, se-
</bodyText>
<footnote confidence="0.940394">
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
</footnote>
<page confidence="0.815358">
1320
</page>
<note confidence="0.958336">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999914574468085">
mantic role analysis has the potential of benefiting a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al., 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al., 2005).
Current approaches have high performance — a
system will recall around 81% of the arguments cor-
rectly and 95% of those will be assigned a cor-
rect semantic role (see M`arquez et al. (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al., 2008). Unfortu-
nately, the reliance on role-annotated data which is
expensive and time-consuming to produce for every
language and domain, presents a major bottleneck to
the widespread application of semantic role labeling.
In this paper we argue that unsupervised meth-
ods offer a promising yet challenging alternative. If
successful, such methods could lead to significant
savings in terms of annotation effort and ultimately
yield more portable semantic role labelers that re-
quire overall less engineering effort. Our approach
formalizes semantic role induction as a graph parti-
tioning problem. Given a verbal predicate, it con-
structs a weighted graph whose vertices correspond
to argument instances of the verb and whose edge
weights quantify the similarity between these in-
stances. The graph is partitioned into vertex clus-
ters representing semantic roles using a variant of
Chinese Whispers, a graph-clustering algorithm pro-
posed by Biemann (2006). The algorithm iteratively
assigns cluster labels to graph vertices by greedily
choosing the most common label amongst the neigh-
bors of the vertex being updated. Beyond extend-
ing Chinese Whispers to the semantic role induc-
tion task, we also show how it can be understood
as a type of Gibbs sampling when our graph is inter-
preted as a Markov random field.
Experimental results on the CoNLL 2008 bench-
mark dataset demonstrate that our method, de-
spite its simplicity, improves upon competitive ap-
proaches in terms of F1 and achieves significantly
higher cluster purity.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.991509173913043">
Although the bulk of previous work on semantic role
labeling has primarily focused on supervised meth-
ods (M`arquez et al., 2008), a few semi-supervised
and unsupervised approaches have been proposed
in the literature. The majority of semi-supervised
models have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(F¨urstenau and Lapata, 2009) or across different lan-
guages (Pad´o and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) pro-
pose to increase the coverage of PropBank to un-
seen verbs by finding syntactically similar (labeled)
verbs and using their annotations as surrogate train-
ing data.
Swier and Stevenson (2004) were the first to intro-
duce an unsupervised semantic role labeling system.
Their algorithm induces role labels following a boot-
strapping scheme where the set of labeled instances
is iteratively expanded using a classifier trained on
previously labeled instances. Their method starts
with a dataset containing no role annotations at all,
but crucially relies on VerbNet (Kipper et al., 2000)
for identifying the arguments of predicates and mak-
ing initial role assignments. VerbNet is a manually
constructed lexicon of verb classes each of which is
explicitly associated with argument realization and
semantic role specifications.
Subsequent work has focused on unsupervised
methods for argument identification and classifica-
tion. Abend et al. (2009) recognize the arguments of
predicates by relying solely on part of speech anno-
tations whereas Abend and Rappoport (2010) distin-
guish between core and adjunct roles, using an unsu-
pervised parser and part-of-speech tagger. Grenager
and Manning (2006) address the role induction prob-
lem and propose a directed graphical model which
relates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.
Following up on this work, Lang and Lapata
(2010) formulate role induction as the process of de-
</bodyText>
<page confidence="0.987185">
1321
</page>
<bodyText confidence="0.9999541875">
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that takes
advantage of the close relationship between syntac-
tic functions and semantic roles. More recently,
Lang and Lapata (2011) propose a clustering algo-
rithm which first splits the argument instances of
a verb into fine-grained clusters based on syntac-
tic cues and then executes a series of merge steps
(mainly) based on lexical cues. The split phase cre-
ates a large number of small clusters with high purity
but low collocation, i.e., while the instances in a par-
ticular cluster typically belong to the same role the
instances for a particular role are commonly scat-
tered amongst many clusters. The subsequent merge
phase conflates clusters with the same role in order
to increase collocation.
Like Grenager and Manning (2006) and Lang
and Lapata (2010; 2011), this paper describes an
unsupervised method for semantic role induction,
i.e., one that does not require any role annotated data
or additional semantic resources for training. Con-
trary to these previous approaches, we conceptualize
role induction in a novel way, as a graph partitioning
problem. Our method is simple, computationally ef-
ficient, and does not rely on hidden variables. More-
over, the graph-based representation for verbs and
their arguments affords greater modeling flexibility.
A wide range of methods exist for finding partitions
in graphs (Schaeffer, 2007), besides Chinese Whis-
pers (Biemann, 2006), which could be easily applied
to the semantic role induction problem. However,
we leave this to future work.
Graph-based methods are popular in natural lan-
guage processing, especially with unsupervised
learning problems (Chen and Ji, 2010). The Chinese
Whispers algorithm itself (Biemann, 2006) has been
previously applied to several tasks including word
sense induction (Klapaftis and M., 2010) and unsu-
pervised part-of-speech tagging (Christodoulopou-
los et al., 2010). The same algorithm is also de-
scribed in Abney (2007, pp. 146-147) under the
name “clustering by propagation”. The term makes
explicit the algorithm’s connection to label propa-
gation, a general framework2 for semi-supervised
learning (Zhu et al., 2003) with applications to
machine translation (Alexandrescu and Kirchhoff,
2009), information extraction (Talukdar and Pereira,
2010) and structured part-of-speech tagging (Sub-
ramanya et al., 2010). The basic idea behind la-
bel propagation is to represent labeled and unlabeled
instances as vertices in an undirected graph with
edges whose weights express similarity (and possi-
bly dissimilarity) between the instances. Label in-
formation is then propagated between the vertices
in such a way that similar instances tend to be as-
signed the same label. Analogously, Chinese Whis-
pers works by propagating cluster membership in-
formation along the edges of a graph, even though
the graph does not contain any human-labeled in-
stance vertices.
</bodyText>
<sectionHeader confidence="0.985772" genericHeader="method">
3 Problem Setting
</sectionHeader>
<bodyText confidence="0.99981444">
We adopt the standard architecture of supervised se-
mantic role labeling systems where argument identi-
fication and argument classification are treated sep-
arately. Our role labeler is fully unsupervised with
respect to both tasks — it does not rely on any role
annotated data or semantic resources. However, our
system does not learn from raw text. In common
with most semantic role labeling research, we as-
sume that the input is syntactically analyzed in the
form of dependency trees.
We view argument identification as a syntactic
processing step that can be largely undertaken deter-
ministically through structural analysis of the depen-
dency tree. We therefore use a small set of rules to
detect arguments with high precision and recall (see
Section 4). Argument classification is more chal-
lenging and must take into account syntactic as well
as lexical-semantic information. Both types of in-
formation are incorporated into our model through
a similarity function that assigns similarity scores
to pairs of argument instances. Following previous
work (Lang and Lapata, 2010; Grenager and Man-
ning, 2006), our system outputs verb-specific roles
by grouping argument instances into clusters and la-
beling each argument instance with an identifier cor-
</bodyText>
<footnote confidence="0.988034666666667">
2For example, Haffari and Sarkar (2007) use label propa-
gation to analyze other semi-supervised algorithms such as the
Yarowsky (1995) algorithm.
</footnote>
<page confidence="0.992566">
1322
</page>
<bodyText confidence="0.999742666666667">
responding to the cluster it has been assigned to.
Such identifiers are similar to PropBank-style core
labels (e.g., A0, A1).
</bodyText>
<sectionHeader confidence="0.981953" genericHeader="method">
4 Argument Identification
</sectionHeader>
<bodyText confidence="0.9999702">
Supervised semantic role labelers often employ a
classifier in order to decide for each node in the
parse tree whether or not it represents a semantic
argument. Nodes classified as arguments are then
assigned a semantic role. In the unsupervised set-
ting, we slightly reformulate argument identification
as the task of discarding as many non-semantic ar-
guments as possible. This means that the argument
identification component does not make a final posi-
tive decision for any of the argument candidates; in-
stead, a final decision is only made in the subsequent
argument classification stage.
We discard or select argument candidates us-
ing the set of rules developed in Lang and Lap-
ata (2011). These are mainly based on the parts
of speech and syntactic relations encountered when
traversing a dependency tree from the predicate
node to the argument node. For each candidate,
rules are considered in a prespecified order and the
first matching rule is applied. When evaluated on
its own, the argument identification component ob-
tained 88.1% precision (percentage of semantic ar-
guments out of those identified) and 87.9% recall
(percentage of identified arguments out of all gold
arguments).
</bodyText>
<sectionHeader confidence="0.981343" genericHeader="method">
5 Argument Classification
</sectionHeader>
<bodyText confidence="0.999795785714286">
After identifying likely arguments for each verb,
the next step is to infer a label for each argument
instance. Since we aim to induce verb-specific
roles (see Section 3), we construct an undirected,
weighted graph for each verb. Vertices corre-
spond to verb argument instances and edge weights
quantify the similarities between them. This
argument-instance graph is then partitioned into
clusters of vertices representing semantic roles and
each argument instance is assigned a label that indi-
cates the cluster it belongs to. In what follows we
first describe how the graph is constructed and then
provide the details of our graph partitioning algo-
rithm.
</bodyText>
<figureCaption confidence="0.921344375">
Figure 1: Simplified example of an argument-instance
graph. All pairs of vertices with non-zero similarity are
connected through edges that are weighted with a simi-
larity score φ(vi,vj). Upon updating the label for a vertex
all neighboring vertices propagate their label to the vertex
being updated. The score for each label is determined by
summing together the weighted votes for that label and
the label with the maximal score is chosen.
</figureCaption>
<subsectionHeader confidence="0.98838">
5.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999988692307692">
For each verb we construct an undirected, weighted
graph G = (V,E,φ) with vertices V, edges E, and
edge weight function φ as follows. Each argu-
ment instance in the corpus that belongs to the
verb is added as a vertex. Then, for each possi-
ble pair of vertices (vi,vj) we compute a weight
φ(vi,vj) ∈ R according to the function φ. If the
weight is non-zero, an undirected edge e = (vi,vj)
with weight φ(vi,vj) is added to the graph. The func-
tion φ quantifies the similarity or dissimilarity be-
tween instances; positive values indicate that roles
are likely to be the same, negative values indicate
that roles are likely to differ, and zero values indicate
that there is no evidence for either case. Our simi-
larity function is symmetric, i.e., φ(vi,vj) = φ(vj,vi)
and permits negative values (see Section 5.4 for a
detailed description).
Figure 1 shows an example of a graph for a verb
with five argument instances (vertices A–E). Edges
are drawn between pairs of vertices with non-zero
similarity values. For instance, vertex D is con-
nected to vertex A with weight 0.2, to vertex E
with 1, and vertex C with −1. Since edges are drawn
between all pairs of vertices with non-zero simi-
larity, the resulting graphs tend to be densely con-
nected, which for large datasets may be prohibitively
</bodyText>
<figure confidence="0.9871128">
0.3
A C
1
E D
B
</figure>
<page confidence="0.936023">
1323
</page>
<bodyText confidence="0.990220333333333">
inefficient. A solution would be to sample a subset
from all possible pairs, but we did not make use of
any kind of edge pruning in our experiments.
</bodyText>
<subsectionHeader confidence="0.999211">
5.2 Graph Partitioning
</subsectionHeader>
<bodyText confidence="0.999982947368421">
Graph partitioning is realized with a variant of Chi-
nese Whispers (Biemann, 2006) whose details are
given below. In addition, we discuss how our algo-
rithm relates to other graph-based models in order to
help provide a better theoretical understanding.
We assume each vertex vi is assigned a label
li ∈ {1...L} indicating the cluster it belongs to. Ini-
tially, each vertex belongs to its own cluster, i.e., we
let the number of clusters L = |V |and set li ← i.
Given this initial vertex labeling, the algorithm pro-
ceeds by iteratively updating the label for each ver-
tex. The update is based on the labels of neighbor-
ing vertices and reflects their similarity to the vertex
being updated. Intuitively, each neighboring vertex
votes for the cluster it is currently assigned to, where
the strength of the vote is determined by the similar-
ity (i.e., edge weight) to the vertex being updated.
The label li of vertex vi is thus updated according to
the following equation:
</bodyText>
<equation confidence="0.99862">
li ← arg max ∑ φ(vi,vj) (2)
l∈{1 ...L} vj∈Ni(l)
</equation>
<bodyText confidence="0.999924884615385">
where Ni(l) = {vj|(vi,vj) ∈ E ∧ l = lj} denotes the
set of vi’s neighbors with label l. In other words,
for each label we compute a score by summing to-
gether the weights of edges to neighboring vertices
with that label and select the label with the maximal
score. Note that negative edges decrease the score
for a particular label, thus demoting the label.
Consider again Figure 1. Assume we wish to up-
date vertex A. In addition, assume that B and E are
currently assigned the same label (i.e., they belong
to the same cluster) whereas C and D are each in
different clusters. The score for cluster {B,E} is
0.4 + 0.8 = 1.2, the score for cluster {C} is 0.3 and
the score for cluster {D} is 0.2. We would thus as-
sign A to cluster {B,E} as it has the highest score.
The algorithm is run for several iterations. At
each iteration it passes over all vertices, and the up-
date order of the vertices is chosen randomly. As
the updates proceed, labels can disappear from the
graph, whereby the number of clusters decreases.
Empirically, we observe that for sufficiently many
iterations the algorithm converges to a fixed labeling
or oscillates between labelings that differ only in a
few vertices. The result of the algorithm is a hard
partitioning of the given graph, where the number of
clusters is determined automatically.
</bodyText>
<subsectionHeader confidence="0.992382">
5.3 Propagation Prioritization
</subsectionHeader>
<bodyText confidence="0.999983333333333">
We make one important modification to the basic al-
gorithm described so far based on the intuition that
higher scores for a label indicate more reliable prop-
agations. More precisely, when updating vertex vi to
label l we define the confidence of the update as the
average similarity to neighbors with label l:
</bodyText>
<equation confidence="0.9975705">
|Ni(l) |∑ φ(vi,vj) (3)
vj∈Ni(l)
</equation>
<bodyText confidence="0.999954090909091">
We can then prioritize high-confidence updates by
setting a threshold θ and allowing only updates with
confidence greater or equal to θ. The threshold is
initially set to 1 (i.e., the maximal possible confi-
dence) and then lowered by some small constant Δ
after each iteration until it reaches a minimum θmin,
at which point the algorithm terminates. This im-
proves the resulting clustering, since it promotes
reliable updates in earlier phases of the algorithm
which in turn has a positive effect on successive up-
dates.
</bodyText>
<subsectionHeader confidence="0.980535">
5.4 Argument-Instance Similarity
</subsectionHeader>
<bodyText confidence="0.999993736842105">
As described earlier, the edge weights in our graph
are similarity scores, with positive values indicating
similarity and negative values indicating dissimilar-
ity. Determining the similarity function φ without
access to labeled training data poses a major diffi-
culty which we resolve by relying on prior linguis-
tic knowledge. Specifically, we measure the sim-
ilarity of argument instances based on three sim-
ple and intuitive criteria: (1) whether the instances
are lexically similar; (2) whether the instances oc-
cur in the same syntactic position; and (3) whether
the instances occur in the same frame (i.e., are argu-
ments in the same clause). The same criteria were
used in (Lang and Lapata, 2011) and shown effec-
tive in quantifying role-semantic similarity between
clusters of argument instances. Lexical and syntac-
tic similarity are scored through functions lex(vi,vj)
and syn(vi,vj) with range [−1,1], whereas the third
criterion enters the scoring function directly:
</bodyText>
<equation confidence="0.9895144">
1
conf(li ← l) =
1324
φ(vi,vj)= −∞ if vi and vj are in same frame (4)
αlex(vi,vj) + (1− α)syn(vi,vj) otherwise.
</equation>
<bodyText confidence="0.999989315789474">
The first case in the function expresses a com-
mon linguistic assumption, i.e., that two argument
instances vi and vj occurring in the same frame can-
not have the same semantic role. The function im-
plements this constraint by returning −∞.3 The syn-
tactic similarity function s(vi,vj) indicates whether
two argument instances occur in a similar syntactic
position. We define syntactic positions through four
cues: the relation of the argument head word to its
governor, verb voice (active/passive), the linear po-
sition of the argument relative to the verb (left/right)
and the preposition used for realizing the argument
(if any). The score is S4 where S is the number of cues
which agree, i.e., have the same value. The syntac-
tic score is set to zero when the governor relation
of the arguments is not the same. Lexical similar-
ity l(vi,vj) is measured in terms of the cosine of the
angle between vectors hi and hj representing the ar-
gument head words:
</bodyText>
<equation confidence="0.962571666666667">
hi·hj
lex(vi,vj) = cos(hi,hj) = (5)
IIhi1111hj11
</equation>
<bodyText confidence="0.999894444444445">
We obtain hi and hj from a simple semantic space
model (Turney and Pantel, 2010) which requires no
supervision (Section 6 describes the details of the
model used in our experiments).
Our similarity function weights the contribution
of syntax vs. semantics equally, i.e., α is set to 0.5.
This reflects the linguistic intuition that lexical and
syntactic information are roughly of equal impor-
tance.
</bodyText>
<subsectionHeader confidence="0.98017">
5.5 Relation to Other Models
</subsectionHeader>
<bodyText confidence="0.999925166666667">
This section briefly points out some connections to
related models. The averaging procedure used for
updating the graph vertices (Equation 2) appears in
some form in most label propagation algorithms (see
Talukdar (2010) for details). Label propagation al-
gorithms are commonly interpreted as random walks
</bodyText>
<footnote confidence="0.96091325">
3Formally, φ has range ran(φ) = [−1,1] U {−∞} and for
x E ran(φ) we define x+(−∞) = −∞. This means that the over-
all score computed for a label (Equation 2) is −∞ if one of the
summands is −∞.
</footnote>
<figureCaption confidence="0.990629">
Figure 2: The update rule (Equation 2) can be under-
stood as choosing a minimal edge-cut, thereby greedily
maximizing intra-cluster similarity and minimizing inter-
cluster similarity. Assuming equal weight for all edges
above, label 3 is chosen for the vertex being updated such
that the sum of weights of edges crossing the cut is mini-
mal.
</figureCaption>
<bodyText confidence="0.999250384615385">
on graphs. In our case such an interpretation is
not directly possible due to the presence of negative
edge weights. This could be changed by transform-
ing the edge weights onto a non-negative scale, but
we find the current setup more expedient for model-
ing dissimilarity.
Our model could be also transformed into a prob-
abilistic graphical model that specifies a distribution
over vertex labels. In the transformed model each
vertex corresponds to a random variable over labels
and edges are associated with binary potential func-
tions over vertex-pairs. Let 1(vi = vj) denote an in-
dicator function which takes value 1 iff. li = lj and
value 0, otherwise. Then pairwise potentials can be
defined in terms of the original edge weights4 as
ψ(vi,vj) = exp(1(vi = vj)φ(vi,vj)). A Gibbs sam-
pler used to sample from the distribution of the
resulting pairwise Markov random field (Bishop,
2006; Wainwright and Jordan, 2008) would employ
almost the same update procedure as in Equation 2,
the difference being that labels would be sampled
according to their probabilities, rather than chosen
deterministically based on scores.
A third way of understanding the update rule
is as a heuristic for maximizing intra-cluster sim-
ilarity and minimizing inter-cluster similarity. By
</bodyText>
<footnote confidence="0.9401885">
4Including weights with value zero and thus connecting all
vertex pairs.
</footnote>
<figure confidence="0.974512333333333">
? 2
3 3
1
</figure>
<page confidence="0.984008">
1325
</page>
<bodyText confidence="0.999795529411765">
assigning the label with maximal score to vi, we
greedily maximize the sum of intra-cluster edge
weights while minimizing the sum of inter-cluster
edge weights, i.e., the weight of the edge-cut. This
is illustrated in Figure 2. Cut-based methods are
a common method in graph clustering (Schaeffer,
2007) and are also used for inference in pairwise
Markov random fields like the one described in the
previous paragraph (Boykov et al., 2001).
Note that while it would be possible to transform
our model into a model with a formal probabilistic
interpretation (either as a graph random walk or as a
probabilistic graphical model) this would not change
the non-empirical nature of the similarity function,
which is unavoidable in the unsupervised setting and
is also common in the semi-supervised methods dis-
cussed in Section 2.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999917515151515">
In this section we describe how we assessed the
performance of our model. We discuss the dataset
on which our experiments were carried out, explain
how our system’s output was evaluated and present
the methods used for comparison with our approach.
Data We compared the output of our model
against the PropBank gold standard annotations con-
tained in the CoNLL 2008 shared task dataset (Sur-
deanu et al., 2008). The latter was taken from the
Wall Street Journal portion of the Penn Treebank
and converted into a dependency format (Surdeanu
et al., 2008). In addition to gold standard depen-
dency parses, the dataset also contains automatic
parses obtained from the MaltParser (Nivre et al.,
2007). The dataset provides annotations for ver-
bal and nominal predicate-argument constructions,
but we only considered the former, following previ-
ous work on semantic role labeling (M`arquez et al.,
2008). All the experiments described in this paper
use the CoNLL 2008 training dataset.
Evaluation Metrics For each verb, we determine
the extent to which argument instances in the clus-
ters share the same gold standard role (purity) and
the extent to which a particular gold standard role is
assigned to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, Gj the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can be
then written as:
</bodyText>
<equation confidence="0.958791666666667">
PU = 1
max|Gj ∩ Ci|(6)
N j
</equation>
<bodyText confidence="0.9987522">
Collocation is defined as follows. For each gold
role, we determine the cluster with the largest num-
ber of instances for that role (the role’s primary clus-
ter) and then compute the percentage of instances
that belong to the primary cluster for each gold role:
</bodyText>
<equation confidence="0.992991666666667">
1
CO = ∑max|Gj ∩Ci |(7)
Nj
</equation>
<bodyText confidence="0.999701714285714">
Per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the
micro-average obtained by weighting the scores for
individual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
</bodyText>
<equation confidence="0.985820333333333">
2·CO·PU
(8)
CO + PU
</equation>
<bodyText confidence="0.999438882352941">
Model Parameters Recall that our algorithm pri-
oritizes updates with confidence higher than a
threshold θ. Initially, θ is set to 1 and its value
decreases at each iteration by a small constant Δ
which we set to 0.0025. The algorithm terminates
when a minimum confidence θmin is reached. While
choosing a value for Δ is straightforward — it sim-
ply has to be a small fraction of the maximal pos-
sible confidence — specifying θmin on the basis of
objective prior knowledge is less so. And although
a human judge could determine the optimal termina-
tion point based on several criteria such as clustering
quality or the number of clusters, we used a develop-
ment set instead for the sake of reproducibility and
comparability. Specifically, we optimized θmin on
the CoNLL test set and obtained best results with
θmin = 1. This value was used for all our experi-
</bodyText>
<page confidence="0.864317">
3
</page>
<bodyText confidence="0.999916">
ments and was also kept fixed for all verbs. Impor-
tantly, the development set was not used for any kind
of supervised training.
</bodyText>
<equation confidence="0.849071">
F1 =
</equation>
<page confidence="0.961246">
1326
</page>
<table confidence="0.9823245">
Syntactic CO Function Latent Logistic F1 Split -Merge F1 Graph Partitioning F1
PU F1 PU CO PU CO PU CO
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2 82.5 68.8 75.0
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9 84.0 73.5 78.4
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3 87.4 65.9 75.2
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1 88.6 70.7 78.6
</table>
<tableCaption confidence="0.9906245">
Table 1: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baseline
that assigns arguments to clusters based on their syntactic function.
</tableCaption>
<figure confidence="0.94101">
0 10 20 30 40 50 60 70
Average number of clusters per verb
</figure>
<figureCaption confidence="0.964590333333333">
Figure 3: Purity (vertical axis) against average number
of clusters per verb (horizontal axis) on the auto/auto
dataset.
</figureCaption>
<bodyText confidence="0.999753954545454">
Recall that one of the components in our simi-
larity function is lexical similarity which we mea-
sure using a vector-based model (see Section 5.4).
We created such a model from the Google N-Grams
corpus (Brants and Franz, 2006) using a context
window of two words on both sides of the target
word and co-occurrence frequencies as vector com-
ponents (no weighting was applied). The large size
of this corpus allows us to use bigram frequencies,
rather than frequencies of individual words and to
distinguish between left and right bigrams. We used
randomized algorithms (Ravichandran et al., 2005)
to build the semantic space efficiently.
Comparison Models We compared our graph par-
titioning algorithm against three competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as a point of com-
parison by other unsupervised semantic role induc-
tion systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
</bodyText>
<figureCaption confidence="0.9895725">
Figure 4: F1 (vertical axis) against number of iterations
(horizontal axis) on the auto/auto dataset.
</figureCaption>
<bodyText confidence="0.999945">
Our implementation allocates up to N = 21 clusters5
for each verb, one for each of the 20 most frequent
syntactic functions and a default cluster for all other
functions. We also compared our approach to Lang
and Lapata (2010) using the same model settings
(with 10 latent variables) and feature set proposed
in that paper. Finally, our third comparison model
is Lang and Lapata’s (2011) split-merge clustering
algorithm. Again we used the same parameters and
number of clusters (on average 10 per verb). Our
graph partitioning method uses identical cues for as-
sessing role-semantic similarity as the method de-
scribed in Lang and Lapata (2011).
</bodyText>
<sectionHeader confidence="0.999727" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999744375">
Our results are summarized in Table 1. We report
cluster purity (PU), collocation (CO) and their har-
monic mean (F1) for the baseline (Syntactic Func-
tion), our two previous models (the Latent Logistic
classifier and Split-Merge) and the graph partition-
ing algorithm on four datasets. These result from the
combination of automatic parses with automatically
identified arguments (auto/auto), gold parses with
</bodyText>
<footnote confidence="0.99178">
5This is the number of gold standard roles.
</footnote>
<figure confidence="0.9961019375">
0 100 200 300 400 500
Number of iterations
Syntactic Function
Graph Partitioning
80
60
40
20
F1 score (%)
0
100
90
80
70
Cluster purity (%)
60
</figure>
<page confidence="0.971391">
1327
</page>
<table confidence="0.9992764">
Syntactic Function
PU 91.4 68.6 45.1 59.7 62.4 61.9 63.5 75.9 76.7 69.6 63.1 53.7
CO 91.3 71.9 56.0 68.4 72.7 76.8 65.6 79.7 76.0 63.8 73.4 58.9
F1 91.4 70.2 49.9 63.7 67.1 68.6 64.5 77.7 76.3 66.6 67.9 56.2
Graph Partitioning
PU 95.6 83.5 72.3 75.4 83.3 84.4 74.8 84.8 89.5 83.0 73.2 66.3
CO 89.1 62.7 42.1 64.2 56.2 66.3 57.2 73.2 64.1 54.3 66.0 57.7
F1 92.2 71.6 53.2 69.4 67.1 74.3 64.8 78.5 74.7 65.7 69.4 61.7
Verb say make go increase know tell consider acquire meet send open break
Freq 15238 4250 2109 1392 983 911 753 704 574 506 482 246
</table>
<tableCaption confidence="0.9814765">
Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and the
syntactic function baseline; the scores were taken from a single run.
</tableCaption>
<bodyText confidence="0.999733642857143">
automatic arguments (gold/auto), automatic parses
with gold arguments (auto/gold) and gold parses
with gold arguments (gold/gold). Table 1 reports
averages across multiple runs. This was necessary
in order to ensure that the results of our randomized
graph partitioning algorithm are stable.6 The argu-
ments for the auto/auto and gold/auto datasets were
identified using the rules described in Lang and Lap-
ata (2011) (see Section 4). Bold-face is used to high-
light the best performing system under each measure
(PU, CO, or F1) on each dataset.
Compared to the Syntactic Function baseline,
the Graph Partitioning algorithm has higher F1 on
the auto/auto and auto/gold datasets but lags be-
hind by 0.5 points on the gold/auto dataset and
by 0.9 points on the gold/gold dataset. It attains
highest purity on all datasets except for gold/gold,
where it is 0.1 points below Split-Merge. When con-
sidering F1 in conjunction with purity and colloca-
tion, we observe that Graph Partitioning can attain
higher purity than the comparison models by trading
off collocation. If we were to hand label the clusters
output by our system, purity would correspond to the
quality of the resulting labeling, while collocation
would determine the labeling effort. The relation-
ship is illustrated more explicitly in Figure 3, which
plots purity against the average number of clusters
per verb on the auto/auto dataset. As the algorithm
</bodyText>
<footnote confidence="0.660612333333333">
6For example, on the auto/auto dataset and over 10 runs,
the standard deviation in F1 was 0.11 points in collocation 0.16
points and in purity 0.08 points. The worst F1 was 0.20 points
below the average, the worst collocation was 0.32 points be-
low the average and the worst purity was 0.17 points below the
average.
</footnote>
<bodyText confidence="0.999905606060606">
proceeds the number of clusters is reduced which
results in a decrease of purity. The latter decreases
more rapidly once the number of 20 clusters per verb
is reached. This is accompanied by a decreasing
tradeoff ratio between collocation and purity: at this
stage decreasing purity by one point increases collo-
cation by roughly one point, whereas in earlier itera-
tions a decrease of purity by one point goes together
with several points increase in collocation. This is
most likely due to the fact that the number of gold
standard classes is around 20.
Figure 4 shows the complete learning curve of our
graph partitioning method on the auto/auto dataset
(F1 is plotted against the number of iterations).
The algorithm naturally terminates at iteration 266
(when 0min = 1/3), but we have also plotted itera-
tions beyond that point. Since lower values of 0 per-
mit unreliable propagations, F1 eventually falls be-
low the baseline (see Section 5.2). The importance
of our propagation prioritization mechanism is fur-
ther underlined by the fact that when it is not em-
ployed (i.e., when using the vanilla Chinese Whis-
pers algorithm without any modifications), it per-
forms substantially worse than the comparison mod-
els. On the auto/auto dataset, F1 converges to 59.1
(purity is 55.5 and collocation 63.2) within 10 itera-
tions.
Finally, Table 2 shows how performance varies
across verbs. We report results for the Syntac-
tic Function baseline and Graph Partitioning on the
auto/auto dataset for 12 verbs. These were selected
so as to exhibit varied occurrence frequencies and
alternation patterns. As can be seen, the macro-
</bodyText>
<page confidence="0.805851">
1328
</page>
<bodyText confidence="0.990953085714286">
scopic result — increase in F1 and purity — also and the 4th International Joint Conference on Natural
holds across verbs. Language Processing of the Asian Federation of Natu-
8 Conclusions ral Language Processing, pages 28–36, Singapore.
In this paper we described an unsupervised method S. Abney. 2007. Semisupervised Learning for Computa-
for semantic role induction, in which argument- tional Linguistics. Chapman &amp; Hall/CRC.
instance graphs are partitioned into clusters repre- A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
senting semantic roles. The approach is conceptu- learning for statistical machine translation. In Pro-
ally and algorithmically simple and novel in its for- ceedings of Human Language Technologies: The 2009
malization of role induction as a graph partitioning Annual Conference of the North American Chapter of
problem. We believe this constitutes an interesting the Association for Computational Linguistics, pages
alternative for two reasons. Firstly, eliciting and 119–127, Boulder, Colorado.
encoding problem-specific knowledge in the form C. Biemann. 2006. Chinese Whispers: an efficient
of instance-wise similarity judgments can be easier graph clustering algorithm and its application to nat-
than encoding it into model structure e.g., by mak- ural language processing problems. In Proceedings
ing statistical independence assumptions or assump- of TextGraphs: the First Workshop on Graph Based
tions about latent structure. Secondly, the approach Methods for Natural Language Processing, pages 73–
is general and amenable to other graph partitioning 80, New York City.
algorithms and relates to well-known graph-based C. Bishop. 2006. Pattern Recognition and Machine
semi-supervised learning methods. Learning. Springer.
The similarity function in this paper is by neces- Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast Ap-
sity rudimentary, since it cannot be estimated from proximate Energy Minimization via Graph Cuts. IEEE
data. Nevertheless, the resulting system attains com- Transactions on Pattern Analysis and Machine Intelli-
petitive F1 and notably higher purity than the com- gence, 23(11):1222–1239.
parison models. Arguably, performance could be T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
improved by developing a better similarity function. Linguistic Data Consortium, Philadelphia.
Therefore, in the future we intend to investigate how Z. Chen and H. Ji. 2010. Graph-based clustering for
our system performs in a weakly supervised setting, computational linguistics: A survey. In Proceedings of
where the similarity function is estimated from a TextGraphs-5 - 2010 Workshop on Graph-based Meth-
small amount of labeled instances, since this would ods for Natural Language Processing, pages 1–9, Up-
allow us to incorporate richer syntactic features and psala, Sweden.
result in more precise similarity scores. C. Christodoulopoulos, S. Goldwater, and M. Steedman.
Acknowledgments We are grateful to Charles 2010. Two decades of unsupervised POS induction:
Sutton for his valuable feedback on this work. The How far have we come? In Proceedings of the 2010
authors acknowledge the support of EPSRC (grant Conference on Empirical Methods in Natural Lan-
GR/T04540/01). guage Processing, pages 575–584, Cambridge, MA.
</bodyText>
<table confidence="0.948528210526316">
References D. Dowty. 1991. Thematic Proto Roles and Argument
O. Abend and A. Rappoport. 2010. Fully unsupervised Selection. Language, 67(3):547–619.
core-adjunct argument classification. In Proceedings H. F¨urstenau and M. Lapata. 2009. Graph Aligment
of the 48th Annual Meeting of the Association for for Semi-Supervised Semantic Role Labeling. In Pro-
Computational Linguistics, pages 226–236, Uppsala, ceedings of the Conference on Empirical Methods in
Sweden. Natural Language Processing, pages 11–20, Singa-
O. Abend, R. Reichart, and A. Rappoport. 2009. Un- pore.
supervised Argument Identification for Semantic Role D. Gildea and D. Jurafsky. 2002. Automatic Label-
Labeling. In Proceedings of the 47th Annual Meet- ing of Semantic Roles. Computational Linguistics,
ing of the Association for Computational Linguistics 28(3):245–288.
1329 A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192–199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1–8, Sydney, Australia.
</table>
<reference confidence="0.999205669811321">
G. Haffari and A. Sarkar. 2007. Analysis of Semi-
Supervised Learning with the Yarowsky Algorithm. In
Proceedings of the 23rd Conference on Uncertainty in
Artificial Intelligence, Vancouver, BC.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691–696. AAAI Press / The MIT Press.
I. Klapaftis and Suresh M. 2010. Word sense induction
&amp; disambiguation using hierarchical random graphs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 745–
755, Cambridge, MA.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939–
947, Los Angeles, California.
J. Lang and M. Lapata. 2011. Unsupervised Semantic
Role Induction via Split-Merge Clustering. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, Portland, Oregon.
To appear in.
L. M`arquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145–
159.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95–135.
S. Pad´o and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307–340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71–106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289–310.
D. Ravichandran, P. Pantel, and E. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Function for High Speed Noun Clustering.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, page 622629,
Ann Arbor, Michigan.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice, version 1.3. Technical report, In-
ternational Computer Science Institute, Berkeley, CA,
USA.
S. Schaeffer. 2007. Graph clustering. Computer Science
Review, 1(1):27–64.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12–21,
Prague, Czech Republic.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 167–176, Cambridge, MA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8–15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. M`arquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159–177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95–102, Barcelona, Spain.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1473–1481, Uppsala, Sweden.
P. Talukdar. 2010. Graph-Based Weakly Supervised
Methods for Information Extraction &amp; Integration.
Ph.D. thesis, CIS Department, University of Pennsyl-
vania.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.
M. Wainwright and M. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Foundations and Trends in Machine Learning, 1(1-
2):1–305.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
</reference>
<page confidence="0.759316">
1330
</page>
<reference confidence="0.998258615384615">
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13–16, Boulder, Colorado.
D. Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189–196, Cam-
bridge, MA.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. In Proceedings of the 20th Interna-
tional Conference on Machine Learning, Washington,
DC.
</reference>
<page confidence="0.992808">
1331
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.599048">
<title confidence="0.990158">Unsupervised Semantic Role Induction with Graph Partitioning</title>
<author confidence="0.820475">Lang</author>
<affiliation confidence="0.980034">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.737005">10 Crichton Street, Edinburgh EH8 9AB,</address>
<abstract confidence="0.999128833333333">this paper we present a method for unsuperrole induction which we formalize as a graph partitioning problem. Argument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity. Graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on the cluster assignments of neighboring vertices. Our method is algorithmically and conceptually simple, especially with respect to how problem-specific knowledge is incorporated into the model. Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of F1 whilst attaining significantly higher cluster purity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Haffari</author>
<author>A Sarkar</author>
</authors>
<title>Analysis of SemiSupervised Learning with the Yarowsky Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence,</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="12426" citStr="Haffari and Sarkar (2007)" startWordPosition="1915" endWordPosition="1918"> use a small set of rules to detect arguments with high precision and recall (see Section 4). Argument classification is more challenging and must take into account syntactic as well as lexical-semantic information. Both types of information are incorporated into our model through a similarity function that assigns similarity scores to pairs of argument instances. Following previous work (Lang and Lapata, 2010; Grenager and Manning, 2006), our system outputs verb-specific roles by grouping argument instances into clusters and labeling each argument instance with an identifier cor2For example, Haffari and Sarkar (2007) use label propagation to analyze other semi-supervised algorithms such as the Yarowsky (1995) algorithm. 1322 responding to the cluster it has been assigned to. Such identifiers are similar to PropBank-style core labels (e.g., A0, A1). 4 Argument Identification Supervised semantic role labelers often employ a classifier in order to decide for each node in the parse tree whether or not it represents a semantic argument. Nodes classified as arguments are then assigned a semantic role. In the unsupervised setting, we slightly reformulate argument identification as the task of discarding as many </context>
</contexts>
<marker>Haffari, Sarkar, 2007</marker>
<rawString>G. Haffari and A. Sarkar. 2007. Analysis of SemiSupervised Learning with the Yarowsky Algorithm. In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>ClassBased Construction of a Verb Lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>691--696</pages>
<publisher>AAAI Press / The MIT Press.</publisher>
<contexts>
<context position="6973" citStr="Kipper et al., 2000" startWordPosition="1076" endWordPosition="1079">rojection, Gordon and Swanson (2007) propose to increase the coverage of PropBank to unseen verbs by finding syntactically similar (labeled) verbs and using their annotations as surrogate training data. Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system. Their algorithm induces role labels following a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method starts with a dataset containing no role annotations at all, but crucially relies on VerbNet (Kipper et al., 2000) for identifying the arguments of predicates and making initial role assignments. VerbNet is a manually constructed lexicon of verb classes each of which is explicitly associated with argument realization and semantic role specifications. Subsequent work has focused on unsupervised methods for argument identification and classification. Abend et al. (2009) recognize the arguments of predicates by relying solely on part of speech annotations whereas Abend and Rappoport (2010) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. Grenager and Manning</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H. T. Dang, and M. Palmer. 2000. ClassBased Construction of a Verb Lexicon. In Proceedings of the 17th AAAI Conference on Artificial Intelligence, pages 691–696. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Klapaftis</author>
<author>M Suresh</author>
</authors>
<title>Word sense induction &amp; disambiguation using hierarchical random graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>745--755</pages>
<location>Cambridge, MA.</location>
<marker>Klapaftis, Suresh, 2010</marker>
<rawString>I. Klapaftis and Suresh M. 2010. Word sense induction &amp; disambiguation using hierarchical random graphs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745– 755, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lang</author>
<author>M Lapata</author>
</authors>
<title>Unsupervised Induction of Semantic Roles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>939--947</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="7929" citStr="Lang and Lapata (2010)" startWordPosition="1220" endWordPosition="1223">ion. Abend et al. (2009) recognize the arguments of predicates by relying solely on part of speech annotations whereas Abend and Rappoport (2010) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. Grenager and Manning (2006) address the role induction problem and propose a directed graphical model which relates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) formulate role induction as the process of de1321 tecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Lang and Lapata (2011) propose a clustering algorithm which first splits the argument instances of a verb into fine-grained clusters b</context>
<context position="12214" citStr="Lang and Lapata, 2010" startWordPosition="1883" endWordPosition="1886">n the form of dependency trees. We view argument identification as a syntactic processing step that can be largely undertaken deterministically through structural analysis of the dependency tree. We therefore use a small set of rules to detect arguments with high precision and recall (see Section 4). Argument classification is more challenging and must take into account syntactic as well as lexical-semantic information. Both types of information are incorporated into our model through a similarity function that assigns similarity scores to pairs of argument instances. Following previous work (Lang and Lapata, 2010; Grenager and Manning, 2006), our system outputs verb-specific roles by grouping argument instances into clusters and labeling each argument instance with an identifier cor2For example, Haffari and Sarkar (2007) use label propagation to analyze other semi-supervised algorithms such as the Yarowsky (1995) algorithm. 1322 responding to the cluster it has been assigned to. Such identifiers are similar to PropBank-style core labels (e.g., A0, A1). 4 Argument Identification Supervised semantic role labelers often employ a classifier in order to decide for each node in the parse tree whether or not</context>
<context position="30352" citStr="Lang and Lapata, 2010" startWordPosition="4921" endWordPosition="4924">am frequencies, rather than frequencies of individual words and to distinguish between left and right bigrams. We used randomized algorithms (Ravichandran et al., 2005) to build the semantic space efficiently. Comparison Models We compared our graph partitioning algorithm against three competitive approaches. The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser. This baseline has been previously used as a point of comparison by other unsupervised semantic role induction systems (Grenager and Manning, 2006; Lang and Lapata, 2010) and shown difficult to outperform. Figure 4: F1 (vertical axis) against number of iterations (horizontal axis) on the auto/auto dataset. Our implementation allocates up to N = 21 clusters5 for each verb, one for each of the 20 most frequent syntactic functions and a default cluster for all other functions. We also compared our approach to Lang and Lapata (2010) using the same model settings (with 10 latent variables) and feature set proposed in that paper. Finally, our third comparison model is Lang and Lapata’s (2011) split-merge clustering algorithm. Again we used the same parameters and nu</context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>J. Lang and M. Lapata. 2010. Unsupervised Induction of Semantic Roles. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 939– 947, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lang</author>
<author>M Lapata</author>
</authors>
<title>Unsupervised Semantic Role Induction via Split-Merge Clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Portland, Oregon.</location>
<note>To appear in.</note>
<contexts>
<context position="8417" citStr="Lang and Lapata (2011)" startWordPosition="1296" endWordPosition="1299">nts and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) formulate role induction as the process of de1321 tecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Lang and Lapata (2011) propose a clustering algorithm which first splits the argument instances of a verb into fine-grained clusters based on syntactic cues and then executes a series of merge steps (mainly) based on lexical cues. The split phase creates a large number of small clusters with high purity but low collocation, i.e., while the instances in a particular cluster typically belong to the same role the instances for a particular role are commonly scattered amongst many clusters. The subsequent merge phase conflates clusters with the same role in order to increase collocation. Like Grenager and Manning (2006</context>
<context position="13379" citStr="Lang and Lapata (2011)" startWordPosition="2066" endWordPosition="2070"> to decide for each node in the parse tree whether or not it represents a semantic argument. Nodes classified as arguments are then assigned a semantic role. In the unsupervised setting, we slightly reformulate argument identification as the task of discarding as many non-semantic arguments as possible. This means that the argument identification component does not make a final positive decision for any of the argument candidates; instead, a final decision is only made in the subsequent argument classification stage. We discard or select argument candidates using the set of rules developed in Lang and Lapata (2011). These are mainly based on the parts of speech and syntactic relations encountered when traversing a dependency tree from the predicate node to the argument node. For each candidate, rules are considered in a prespecified order and the first matching rule is applied. When evaluated on its own, the argument identification component obtained 88.1% precision (percentage of semantic arguments out of those identified) and 87.9% recall (percentage of identified arguments out of all gold arguments). 5 Argument Classification After identifying likely arguments for each verb, the next step is to infer</context>
<context position="20454" citStr="Lang and Lapata, 2011" startWordPosition="3266" endWordPosition="3269">with positive values indicating similarity and negative values indicating dissimilarity. Determining the similarity function φ without access to labeled training data poses a major difficulty which we resolve by relying on prior linguistic knowledge. Specifically, we measure the similarity of argument instances based on three simple and intuitive criteria: (1) whether the instances are lexically similar; (2) whether the instances occur in the same syntactic position; and (3) whether the instances occur in the same frame (i.e., are arguments in the same clause). The same criteria were used in (Lang and Lapata, 2011) and shown effective in quantifying role-semantic similarity between clusters of argument instances. Lexical and syntactic similarity are scored through functions lex(vi,vj) and syn(vi,vj) with range [−1,1], whereas the third criterion enters the scoring function directly: 1 conf(li ← l) = 1324 φ(vi,vj)= −∞ if vi and vj are in same frame (4) αlex(vi,vj) + (1− α)syn(vi,vj) otherwise. The first case in the function expresses a common linguistic assumption, i.e., that two argument instances vi and vj occurring in the same frame cannot have the same semantic role. The function implements this cons</context>
<context position="31133" citStr="Lang and Lapata (2011)" startWordPosition="5047" endWordPosition="5050">locates up to N = 21 clusters5 for each verb, one for each of the 20 most frequent syntactic functions and a default cluster for all other functions. We also compared our approach to Lang and Lapata (2010) using the same model settings (with 10 latent variables) and feature set proposed in that paper. Finally, our third comparison model is Lang and Lapata’s (2011) split-merge clustering algorithm. Again we used the same parameters and number of clusters (on average 10 per verb). Our graph partitioning method uses identical cues for assessing role-semantic similarity as the method described in Lang and Lapata (2011). 7 Results Our results are summarized in Table 1. We report cluster purity (PU), collocation (CO) and their harmonic mean (F1) for the baseline (Syntactic Function), our two previous models (the Latent Logistic classifier and Split-Merge) and the graph partitioning algorithm on four datasets. These result from the combination of automatic parses with automatically identified arguments (auto/auto), gold parses with 5This is the number of gold standard roles. 0 100 200 300 400 500 Number of iterations Syntactic Function Graph Partitioning 80 60 40 20 F1 score (%) 0 100 90 80 70 Cluster purity (</context>
<context position="32897" citStr="Lang and Lapata (2011)" startWordPosition="5346" endWordPosition="5350"> 574 506 482 246 Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and the syntactic function baseline; the scores were taken from a single run. automatic arguments (gold/auto), automatic parses with gold arguments (auto/gold) and gold parses with gold arguments (gold/gold). Table 1 reports averages across multiple runs. This was necessary in order to ensure that the results of our randomized graph partitioning algorithm are stable.6 The arguments for the auto/auto and gold/auto datasets were identified using the rules described in Lang and Lapata (2011) (see Section 4). Bold-face is used to highlight the best performing system under each measure (PU, CO, or F1) on each dataset. Compared to the Syntactic Function baseline, the Graph Partitioning algorithm has higher F1 on the auto/auto and auto/gold datasets but lags behind by 0.5 points on the gold/auto dataset and by 0.9 points on the gold/gold dataset. It attains highest purity on all datasets except for gold/gold, where it is 0.1 points below Split-Merge. When considering F1 in conjunction with purity and collocation, we observe that Graph Partitioning can attain higher purity than the co</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>J. Lang and M. Lapata. 2011. Unsupervised Semantic Role Induction via Split-Merge Clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, Portland, Oregon. To appear in.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>K Litkowski</author>
<author>S Stevenson</author>
</authors>
<title>Semantic Role Labeling: an Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>159</pages>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>L. M`arquez, X. Carreras, K. Litkowski, and S. Stevenson. 2008. Semantic Role Labeling: an Introduction to the Special Issue. Computational Linguistics, 34(2):145– 159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Melli</author>
<author>Y Wang</author>
<author>Y Liu</author>
<author>M M Kashani</author>
<author>Z Shi</author>
<author>B Gu</author>
<author>A Sarkar</author>
<author>F Popowich</author>
</authors>
<title>Description of SQUASH, the SFU Question Answering Summary Handler for the DUC-2005 Summarization Task.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing Document Understanding Workshop,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3745" citStr="Melli et al., 2005" startWordPosition="572" endWordPosition="575">ow, se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1320 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (Pradhan et al., 2008). Unfortunately, the reliance on role-annotated data which is expensive and time-consuming to produce for every language </context>
</contexts>
<marker>Melli, Wang, Liu, Kashani, Shi, Gu, Sarkar, Popowich, 2005</marker>
<rawString>G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi, B. Gu, A. Sarkar, and F. Popowich. 2005. Description of SQUASH, the SFU Question Answering Summary Handler for the DUC-2005 Summarization Task. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing Document Understanding Workshop, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryigit A Chanev</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A Language-independent System for Datadriven Dependency Parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Nivre, Hall, Nilsson, Chanev, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev, S. K¨ubler, S. Marinov, and E. Marsi. 2007. MaltParser: A Language-independent System for Datadriven Dependency Parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Cross-lingual Annotation Projection of Semantic Roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>36--307</pages>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>S. Pad´o and M. Lapata. 2009. Cross-lingual Annotation Projection of Semantic Roles. Journal of Artificial Intelligence Research, 36:307–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2129" citStr="Palmer et al., 2005" startWordPosition="326" endWordPosition="329">actic configurations. In the example sentences below, window occupies different syntactic positions — it is the object of broke in sentences (1a,b), and the subject in (1c) — while bearing the same semantic role, i.e., the physical object affected by the breaking event. Analogously, ball is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. [Jim]A0 broke the [window]A1 with a [ball]A2. b. The [ball]A2 broke the [window]A1. c. The [window]A1 broke [last night]TMP. The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broadcoverage human-annotated corpus of semantic roles and their syntactic realizations. Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles such as location or time whose interpretation is common across predicates (e.g., last night in sentence (1c)). The availability of PropBank and related resources (e.g., FrameNet; Ruppenhofer et al. (2006)) has sparked the development of great many semantic </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J Martin</author>
</authors>
<title>Towards Robust Semantic Role Labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="4224" citStr="Pradhan et al., 2008" startWordPosition="649" endWordPosition="652">t al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (Pradhan et al., 2008). Unfortunately, the reliance on role-annotated data which is expensive and time-consuming to produce for every language and domain, presents a major bottleneck to the widespread application of semantic role labeling. In this paper we argue that unsupervised methods offer a promising yet challenging alternative. If successful, such methods could lead to significant savings in terms of annotation effort and ultimately yield more portable semantic role labelers that require overall less engineering effort. Our approach formalizes semantic role induction as a graph partitioning problem. Given a v</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>S. Pradhan, W. Ward, and J. Martin. 2008. Towards Robust Semantic Role Labeling. Computational Linguistics, 34(2):289–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>P Pantel</author>
<author>E Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering.</title>
<date>2005</date>
<contexts>
<context position="29898" citStr="Ravichandran et al., 2005" startWordPosition="4852" endWordPosition="4855"> axis) on the auto/auto dataset. Recall that one of the components in our similarity function is lexical similarity which we measure using a vector-based model (see Section 5.4). We created such a model from the Google N-Grams corpus (Brants and Franz, 2006) using a context window of two words on both sides of the target word and co-occurrence frequencies as vector components (no weighting was applied). The large size of this corpus allows us to use bigram frequencies, rather than frequencies of individual words and to distinguish between left and right bigrams. We used randomized algorithms (Ravichandran et al., 2005) to build the semantic space efficiently. Comparison Models We compared our graph partitioning algorithm against three competitive approaches. The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser. This baseline has been previously used as a point of comparison by other unsupervised semantic role induction systems (Grenager and Manning, 2006; Lang and Lapata, 2010) and shown difficult to outperform. Figure 4: F1 (vertical axis) against number of iterations (horizontal axis) on the auto/auto dataset. Our impl</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>D. Ravichandran, P. Pantel, and E. Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>622629</pages>
<location>Ann Arbor, Michigan.</location>
<marker></marker>
<rawString>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, page 622629, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>M Ellsworth</author>
<author>M Petruck</author>
<author>C Johnson</author>
<author>J Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice, version 1.3.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="2676" citStr="Ruppenhofer et al. (2006)" startWordPosition="414" endWordPosition="417">les in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broadcoverage human-annotated corpus of semantic roles and their syntactic realizations. Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles such as location or time whose interpretation is common across predicates (e.g., last night in sentence (1c)). The availability of PropBank and related resources (e.g., FrameNet; Ruppenhofer et al. (2006)) has sparked the development of great many semantic role labeling systems most of which conceptualize the task as a supervised learning problem and rely on role-annotated data for model training. Most of these systems implement a two-stage architecture consisting of argument identification (determining the arguments of the verbal predicate) and argument classification (labeling these arguments with semantic roles). Despite being relatively shallow, se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1320 Pr</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk. 2006. FrameNet II: Extended Theory and Practice, version 1.3. Technical report, International Computer Science Institute, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schaeffer</author>
</authors>
<title>Graph clustering.</title>
<date>2007</date>
<journal>Computer Science Review,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="9626" citStr="Schaeffer, 2007" startWordPosition="1490" endWordPosition="1491">g (2006) and Lang and Lapata (2010; 2011), this paper describes an unsupervised method for semantic role induction, i.e., one that does not require any role annotated data or additional semantic resources for training. Contrary to these previous approaches, we conceptualize role induction in a novel way, as a graph partitioning problem. Our method is simple, computationally efficient, and does not rely on hidden variables. Moreover, the graph-based representation for verbs and their arguments affords greater modeling flexibility. A wide range of methods exist for finding partitions in graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. However, we leave this to future work. Graph-based methods are popular in natural language processing, especially with unsupervised learning problems (Chen and Ji, 2010). The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the name “clu</context>
<context position="24758" citStr="Schaeffer, 2007" startWordPosition="3976" endWordPosition="3977">to their probabilities, rather than chosen deterministically based on scores. A third way of understanding the update rule is as a heuristic for maximizing intra-cluster similarity and minimizing inter-cluster similarity. By 4Including weights with value zero and thus connecting all vertex pairs. ? 2 3 3 1 1325 assigning the label with maximal score to vi, we greedily maximize the sum of intra-cluster edge weights while minimizing the sum of inter-cluster edge weights, i.e., the weight of the edge-cut. This is illustrated in Figure 2. Cut-based methods are a common method in graph clustering (Schaeffer, 2007) and are also used for inference in pairwise Markov random fields like the one described in the previous paragraph (Boykov et al., 2001). Note that while it would be possible to transform our model into a model with a formal probabilistic interpretation (either as a graph random walk or as a probabilistic graphical model) this would not change the non-empirical nature of the similarity function, which is unavoidable in the unsupervised setting and is also common in the semi-supervised methods discussed in Section 2. 6 Experimental Setup In this section we describe how we assessed the performan</context>
</contexts>
<marker>Schaeffer, 2007</marker>
<rawString>S. Schaeffer. 2007. Graph clustering. Computer Science Review, 1(1):27–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using Semantic Roles to Improve Question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the Conference on Computational Natural Language Learning,</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3662" citStr="Shen and Lapata, 2007" startWordPosition="559" endWordPosition="562">ication (labeling these arguments with semantic roles). Despite being relatively shallow, se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1320 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (Pradhan et al., 2008). Unfortunately, the reliance on role-</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using Semantic Roles to Improve Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the Conference on Computational Natural Language Learning, pages 12–21, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Subramanya</author>
<author>S Petrov</author>
<author>F Pereira</author>
</authors>
<title>Efficient graph-based semi-supervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>167--176</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="10585" citStr="Subramanya et al., 2010" startWordPosition="1624" endWordPosition="1628">06) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the name “clustering by propagation”. The term makes explicit the algorithm’s connection to label propagation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances. Label information is then propagated between the vertices in such a way that similar instances tend to be assigned the same label. Analogously, Chinese Whispers works by propagating cluster membership information along the edges of a graph, even though the graph does not contain any human-labeled instance vertices. 3 Problem Setting We adopt the standard architecture of superv</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>A. Subramanya, S. Petrov, and F. Pereira. 2010. Efficient graph-based semi-supervised learning of structured tagging models. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167–176, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using Predicate-Argument Structures for Information Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3615" citStr="Surdeanu et al., 2003" startWordPosition="551" endWordPosition="554">s of the verbal predicate) and argument classification (labeling these arguments with semantic roles). Despite being relatively shallow, se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1320 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (Pradhan et a</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using Predicate-Argument Structures for Information Extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8–15, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th CoNLL,</booktitle>
<pages>159--177</pages>
<location>Manchester, England.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, and L. M`arquez. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th CoNLL, pages 159–177, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Swier</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised Semantic Role Labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>95--102</pages>
<location>Barcelona,</location>
<contexts>
<context position="6582" citStr="Swier and Stevenson (2004)" startWordPosition="1016" endWordPosition="1019">he literature. The majority of semi-supervised models have been developed within a framework known as annotation projection. The idea is to combine labeled and unlabeled data by projecting annotations from a labeled source sentence onto an unlabeled target sentence within the same language (F¨urstenau and Lapata, 2009) or across different languages (Pad´o and Lapata, 2009). Outwith annotation projection, Gordon and Swanson (2007) propose to increase the coverage of PropBank to unseen verbs by finding syntactically similar (labeled) verbs and using their annotations as surrogate training data. Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system. Their algorithm induces role labels following a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method starts with a dataset containing no role annotations at all, but crucially relies on VerbNet (Kipper et al., 2000) for identifying the arguments of predicates and making initial role assignments. VerbNet is a manually constructed lexicon of verb classes each of which is explicitly associated with argument realization and </context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>R. Swier and S. Stevenson. 2004. Unsupervised Semantic Role Labelling. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 95–102, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Talukdar</author>
<author>F Pereira</author>
</authors>
<title>Experiments in graphbased semi-supervised learning methods for classinstance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1473--1481</pages>
<location>Uppsala,</location>
<contexts>
<context position="10521" citStr="Talukdar and Pereira, 2010" startWordPosition="1616" endWordPosition="1619">n and Ji, 2010). The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the name “clustering by propagation”. The term makes explicit the algorithm’s connection to label propagation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances. Label information is then propagated between the vertices in such a way that similar instances tend to be assigned the same label. Analogously, Chinese Whispers works by propagating cluster membership information along the edges of a graph, even though the graph does not contain any human-labeled instance vertices</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>P. Talukdar and F. Pereira. 2010. Experiments in graphbased semi-supervised learning methods for classinstance acquisition. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1473–1481, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Talukdar</author>
</authors>
<title>Graph-Based Weakly Supervised Methods for Information Extraction &amp; Integration.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>CIS Department, University of Pennsylvania.</institution>
<contexts>
<context position="22487" citStr="Talukdar (2010)" startWordPosition="3598" endWordPosition="3599"> semantic space model (Turney and Pantel, 2010) which requires no supervision (Section 6 describes the details of the model used in our experiments). Our similarity function weights the contribution of syntax vs. semantics equally, i.e., α is set to 0.5. This reflects the linguistic intuition that lexical and syntactic information are roughly of equal importance. 5.5 Relation to Other Models This section briefly points out some connections to related models. The averaging procedure used for updating the graph vertices (Equation 2) appears in some form in most label propagation algorithms (see Talukdar (2010) for details). Label propagation algorithms are commonly interpreted as random walks 3Formally, φ has range ran(φ) = [−1,1] U {−∞} and for x E ran(φ) we define x+(−∞) = −∞. This means that the overall score computed for a label (Equation 2) is −∞ if one of the summands is −∞. Figure 2: The update rule (Equation 2) can be understood as choosing a minimal edge-cut, thereby greedily maximizing intra-cluster similarity and minimizing intercluster similarity. Assuming equal weight for all edges above, label 3 is chosen for the vertex being updated such that the sum of weights of edges crossing the </context>
</contexts>
<marker>Talukdar, 2010</marker>
<rawString>P. Talukdar. 2010. Graph-Based Weakly Supervised Methods for Information Extraction &amp; Integration. Ph.D. thesis, CIS Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<pages>188</pages>
<contexts>
<context position="21919" citStr="Turney and Pantel, 2010" startWordPosition="3509" endWordPosition="3512">s governor, verb voice (active/passive), the linear position of the argument relative to the verb (left/right) and the preposition used for realizing the argument (if any). The score is S4 where S is the number of cues which agree, i.e., have the same value. The syntactic score is set to zero when the governor relation of the arguments is not the same. Lexical similarity l(vi,vj) is measured in terms of the cosine of the angle between vectors hi and hj representing the argument head words: hi·hj lex(vi,vj) = cos(hi,hj) = (5) IIhi1111hj11 We obtain hi and hj from a simple semantic space model (Turney and Pantel, 2010) which requires no supervision (Section 6 describes the details of the model used in our experiments). Our similarity function weights the contribution of syntax vs. semantics equally, i.e., α is set to 0.5. This reflects the linguistic intuition that lexical and syntactic information are roughly of equal importance. 5.5 Relation to Other Models This section briefly points out some connections to related models. The averaging procedure used for updating the graph vertices (Equation 2) appears in some form in most label propagation algorithms (see Talukdar (2010) for details). Label propagation</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>M Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="24017" citStr="Wainwright and Jordan, 2008" startWordPosition="3857" endWordPosition="3860">d be also transformed into a probabilistic graphical model that specifies a distribution over vertex labels. In the transformed model each vertex corresponds to a random variable over labels and edges are associated with binary potential functions over vertex-pairs. Let 1(vi = vj) denote an indicator function which takes value 1 iff. li = lj and value 0, otherwise. Then pairwise potentials can be defined in terms of the original edge weights4 as ψ(vi,vj) = exp(1(vi = vj)φ(vi,vj)). A Gibbs sampler used to sample from the distribution of the resulting pairwise Markov random field (Bishop, 2006; Wainwright and Jordan, 2008) would employ almost the same update procedure as in Equation 2, the difference being that labels would be sampled according to their probabilities, rather than chosen deterministically based on scores. A third way of understanding the update rule is as a heuristic for maximizing intra-cluster similarity and minimizing inter-cluster similarity. By 4Including weights with value zero and thus connecting all vertex pairs. ? 2 3 3 1 1325 assigning the label with maximal score to vi, we greedily maximize the sum of intra-cluster edge weights while minimizing the sum of inter-cluster edge weights, i</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>M. Wainwright and M. Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>P Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model.</title>
<date>2009</date>
<booktitle>In Proceedings of North</booktitle>
<contexts>
<context position="3706" citStr="Wu and Fung, 2009" startWordPosition="566" endWordPosition="569">roles). Despite being relatively shallow, se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1320 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (Pradhan et al., 2008). Unfortunately, the reliance on role-annotated data which is expensive and time-c</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>D. Wu and P. Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In Proceedings of North</rawString>
</citation>
<citation valid="false">
<booktitle>American Annual Meeting of the Association for Computational Linguistics HLT 2009: Short Papers,</booktitle>
<pages>13--16</pages>
<location>Boulder, Colorado.</location>
<marker></marker>
<rawString>American Annual Meeting of the Association for Computational Linguistics HLT 2009: Short Papers, pages 13–16, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="12520" citStr="Yarowsky (1995)" startWordPosition="1931" endWordPosition="1932">lassification is more challenging and must take into account syntactic as well as lexical-semantic information. Both types of information are incorporated into our model through a similarity function that assigns similarity scores to pairs of argument instances. Following previous work (Lang and Lapata, 2010; Grenager and Manning, 2006), our system outputs verb-specific roles by grouping argument instances into clusters and labeling each argument instance with an identifier cor2For example, Haffari and Sarkar (2007) use label propagation to analyze other semi-supervised algorithms such as the Yarowsky (1995) algorithm. 1322 responding to the cluster it has been assigned to. Such identifiers are similar to PropBank-style core labels (e.g., A0, A1). 4 Argument Identification Supervised semantic role labelers often employ a classifier in order to decide for each node in the parse tree whether or not it represents a semantic argument. Nodes classified as arguments are then assigned a semantic role. In the unsupervised setting, we slightly reformulate argument identification as the task of discarding as many non-semantic arguments as possible. This means that the argument identification component does</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>SemiSupervised Learning Using Gaussian Fields and Harmonic Functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 20th International Conference on Machine Learning,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="10392" citStr="Zhu et al., 2003" startWordPosition="1601" endWordPosition="1604">rk. Graph-based methods are popular in natural language processing, especially with unsupervised learning problems (Chen and Ji, 2010). The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the name “clustering by propagation”. The term makes explicit the algorithm’s connection to label propagation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances. Label information is then propagated between the vertices in such a way that similar instances tend to be assigned the same label. Analogously, Chinese Whispers works by propagating clus</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. SemiSupervised Learning Using Gaussian Fields and Harmonic Functions. In Proceedings of the 20th International Conference on Machine Learning, Washington, DC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>