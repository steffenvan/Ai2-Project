<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010385">
<title confidence="0.993136">
Top-Down K-Best A* Parsing
</title>
<author confidence="0.997779">
Adam Pauls and Dan Klein
</author>
<affiliation confidence="0.9978155">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.990739">
{adpauls,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961">
We propose a top-down algorithm for ex-
tracting k-best lists from a parser. Our
algorithm, TKA* is a variant of the k-
best A* (KA*) algorithm of Pauls and
Klein (2009). In contrast to KA*, which
performs an inside and outside pass be-
fore performing k-best extraction bottom
up, TKA* performs only the inside pass
before extracting k-best lists top down.
TKA* maintains the same optimality and
efficiency guarantees of KA*, but is sim-
pler to both specify and implement.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985624">
Many situations call for a parser to return a k-
best list of parses instead of a single best hypothe-
sis.1 Currently, there are two efficient approaches
known in the literature. The k-best algorithm of
Jim´enez and Marzal (2000) and Huang and Chi-
ang (2005), referred to hereafter as LAZY, oper-
ates by first performing an exhaustive Viterbi in-
side pass and then lazily extracting k-best lists in
top-down manner. The k-best A* algorithm of
Pauls and Klein (2009), hereafter KA*, computes
Viterbi inside and outside scores before extracting
k-best lists bottom up.
Because these additional passes are only partial,
KA* can be significantly faster than LAZY, espe-
cially when a heuristic is used (Pauls and Klein,
2009). In this paper, we propose TKA*, a top-
down variant of KA* that, like LAZY, performs
only an inside pass before extracting k-best lists
top-down, but maintains the same optimality and
efficiency guarantees as KA*. This algorithm can
be seen as a generalization of the lattice k-best al-
gorithm of Soong and Huang (1991) to parsing.
Because TKA* eliminates the outside pass from
KA*, TKA* is simpler both in implementation and
specification.
</bodyText>
<footnote confidence="0.80226">
1See Huang and Chiang (2005) for a review.
</footnote>
<author confidence="0.703319">
Chris Quirk
</author>
<affiliation confidence="0.635778">
Microsoft Research
</affiliation>
<address confidence="0.947331">
Redmond, WA, 98052
</address>
<email confidence="0.990034">
chrisq@microsoft.com
</email>
<sectionHeader confidence="0.999667" genericHeader="introduction">
2 Review
</sectionHeader>
<bodyText confidence="0.9989224">
Because our algorithm is very similar to KA*,
which is in turn an extension of the (1-best) A*
parsing algorithm of Klein and Manning (2003),
we first introduce notation and review those two
algorithms before presenting our new algorithm.
</bodyText>
<subsectionHeader confidence="0.740773">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.999797454545455">
Assume we have a PCFG2 !g and an input sen-
tence s0 ... sn_1 of length n. The grammar !g has
a set of symbols denoted by capital letters, includ-
ing a distinguished goal (root) symbol G. With-
out loss of generality, we assume Chomsky nor-
mal form: each non-terminal rule r in !g has the
form r = A —* B C with weight wr. Edges
are labeled spans e = (A, i, j). Inside deriva-
tions of an edge (A, i, j) are trees with root non-
terminal A, spanning si ... sj_1. The weight (neg-
ative log-probability) of the best (minimum) inside
derivation for an edge e is called the Viterbi in-
side score Q(e), and the weight of the best deriva-
tion of G —* s0 ... si_1 A sj ... sn_1 is called
the Viterbi outside score a(e). The goal of a k-
best parsing algorithm is to compute the k best
(minimum weight) inside derivations of the edge
(G, 0, n).
We formulate the algorithms in this paper
in terms of prioritized weighted deduction rules
(Shieber et al., 1995; Nederhof, 2003). A prior-
itized weighted deduction rule has the form
</bodyText>
<equation confidence="0.9911035">
�(���������)
φ1 : w1, ... , φn : wn φ0 : g(w1, ... ,wn
</equation>
<bodyText confidence="0.974264333333333">
where 01, ... , 0n are the antecedent items of the
deduction rule and 00 is the conclusion item. A
deduction rule states that, given the antecedents
01, ... , 0n with weights w1, ... , wn, the conclu-
sion 00 can be formed with weight g(w1, ... , wn)
and priority Aw1, ... , wn).
</bodyText>
<footnote confidence="0.982648">
2While we present the algorithm specialized to parsing
with a PCFG, this algorithm generalizes to a wide range of
</footnote>
<page confidence="0.864762">
200
</page>
<note confidence="0.8443095">
Proceedings of the ACL 2010 Conference Short Papers, pages 200–204,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.995356666666667">
Figure 1: Representations of the different types of items
used in parsing. (a) An inside edge item I(VP, 2, 5). (b)
An outside edge item O(VP, 2, 5). (c) An inside deriva-
tion item: D(TVP, 2, 5). (d) An outside derivation item:
Q(TGVP,1, 2, {(NP, 2, n)}. The edges in boldface are fron-
tier edges.
</figureCaption>
<bodyText confidence="0.999960882352941">
These deduction rules are “executed” within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item 0 from the
agenda, put it into the chart with its current weight,
and apply deduction rules to form any items which
can be built by combining 0 with items already
in the chart. When the resulting items are either
new or have a weight smaller than an item’s best
score so far, they are put on the agenda with pri-
ority given by p(·). Because all antecedents must
be constructed before a deduction rule is executed,
we sometimes refer to particular conclusion item
as “waiting” on another item before it can be built.
</bodyText>
<subsectionHeader confidence="0.993313">
2.2 A*
</subsectionHeader>
<bodyText confidence="0.996069625">
A* parsing (Klein and Manning, 2003) is an al-
gorithm for computing the 1-best parse of a sen-
tence. A* operates on items called inside edge
items I(A, i, j), which represent the many pos-
sible inside derivations of an edge (A, i, j). In-
side edge items are constructed according to the
IN deduction rule of Table 1. This deduction rule
constructs inside edge items in a bottom-up fash-
ion, combining items representing smaller edges
I(B, i, k) and I(C, k, j) with a grammar rule r =
A —* B C to form a larger item I(A, i, j). The
weight of a newly constructed item is given by the
sum of the weights of the antecedent items and
the grammar rule r, and its priority is given by
hypergraph search problems as shown in Klein and Manning
(2001).
</bodyText>
<figure confidence="0.856368">
G G
s0 S1 S2 S3 S4 S5 s0 S1 S2 S3 S4 S5
(a) (b)
</figure>
<figureCaption confidence="0.995939">
Figure 2: (a) An outside derivation item before expansion at
the edge (VP, 1, 4). (b) A possible expansion of the item in
(a) using the rule VPS VP NN. Frontier edges are marked in
boldface.
</figureCaption>
<bodyText confidence="0.999163666666667">
its weight plus a heuristic h(A, i, j). For consis-
tent and admissible heuristics h(·), this deduction
rule guarantees that when an inside edge item is
removed from the agenda, its current weight is its
true Viterbi inside score.
The heuristic h controls the speed of the algo-
rithm. It can be shown that an edge e satisfying
Q(e) + h(A, i, j) &gt; Q(G, 0, n) will never be re-
moved from the agenda, allowing some edges to
be safely pruned during parsing. The more closely
h(e) approximates the Viterbi outside cost a(e),
the more items are pruned.
</bodyText>
<subsectionHeader confidence="0.984754">
2.3 KA*
</subsectionHeader>
<bodyText confidence="0.99978616">
The use of inside edge items in A* exploits the op-
timal substructure property of derivations – since
a best derivation of a larger edge is always com-
posed of best derivations of smaller edges, it is
only necessary to compute the best way of build-
ing a particular inside edge item. When finding
k-best lists, this is no longer possible, since we are
interested in suboptimal derivations.
Thus, KA*, the k-best extension of A*, must
search not in the space of inside edge items,
but rather in the space of inside derivation items
D(TA, i, j), which represent specific derivations
of the edge (A, i, j) using tree TA. However, the
number of inside derivation items is exponential
in the length of the input sentence, and even with
a very accurate heuristic, running A* directly in
this space is not feasible.
Fortunately, Pauls and Klein (2009) show that
with a perfect heuristic, that is, h(e) = a(e) be,
A* search on inside derivation items will only
remove items from the agenda that participate
in the true k-best lists (up to ties). In order
to compute this perfect heuristic, KA* makes
use of outside edge items O(A, i, j) which rep-
resent the many possible derivations of G —*
</bodyText>
<figure confidence="0.995321666666667">
(c)
(d) G
VP
(a) (b)
VP
s2 s3 s4 s0 ...s2 s5 ... sn-1
NP
VP
G
VP
VBZ NP
DT NN
s2 s3 s4
SO s1 s2 sn-1
NN
VP NP
NP
NP
VP
NN
NP
VP
VP
VP
NP
NN
VP NN
</figure>
<page confidence="0.90641">
201
</page>
<equation confidence="0.639255333333333">
w1+w2+wr+h(A,i,j)
IN∗†: I(B, i, l) :w1 I(C, l, j) : w2 → I(A, i, j) : w1 + w2 + wr
IN-D†: O(A, i, j) : w1 D(T B, i, l) : w2 D(TC, l, j) : w3 −−−−−−−−−−→
w2+w3+wr+w1 D(TA, i, j) : w2 + w3 + wr
OUT-L†: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3 −−−−−−−−−−→
w1+w3+wr+w2 O(B, i, l) : w1 + w3 + wr
OUT-R†: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3 −−−−−−−−−−→
w1+w2+wr+w3 O(C, l, j) : w1 + w2 + wr
w1+wr+w2+w3+β(F) OUT-D∗: Q(T A G , i, j, F) : w1 I(B, i, l) : w2 I(C, l, j) : w3 → Q(T B G , i, l, FC) : w1 + wr
</equation>
<tableCaption confidence="0.6984128">
Table 1: The deduction rules used in this paper. Here, r is the rule A → B C. A superscript * indicates that the rule is used
in TKA∗, and a superscript † indicates that the rule is used in KA∗. In IN-D, the tree TA is rooted at (A, i, j) and has children
TB and TC. In OUT-D, the tree TBG is the tree TAG extended at (A, i, j) with rule r, FC is the list F with (C, l, j) prepended,
and β(F) is Ee∈F β(e). Whenever the left child I(B, i, l) of an application of OUT-D represents a terminal, the next edge is
removed from F and is used as the new point of expansion.
</tableCaption>
<note confidence="0.251016">
s1 ... si A sj+1 ... sr,, (see Figure 1(b)).
</note>
<bodyText confidence="0.999877727272728">
Outside items are built using the OUT-L and
OUT-R deduction rules shown in Table 1. OUT-
L and OUT-R combine, in a top-down fashion, an
outside edge over a larger span and inside edge
over a smaller span to form a new outside edge
over a smaller span. Because these rules make ref-
erence to inside edge items I(A, i, j), these items
must also be built using the IN deduction rules
from 1-best A*. Outside edge items must thus wait
until the necessary inside edge items have been
built. The outside pass is initialized with the item
O(G, 0, n) when the inside edge item I(G, 0, n) is
popped from the agenda.
Once we have started populating outside scores
using the outside deductions, we can initiate a
search on inside derivation items.3 These items
are built bottom-up using the IN-D deduction rule.
The crucial element of this rule is that derivation
items for a particular edge wait until the exact out-
side score of that edge has been computed. The al-
gorithm terminates when k derivation items rooted
at (G, 0, n) have been popped from the agenda.
</bodyText>
<sectionHeader confidence="0.9977" genericHeader="method">
3 TKA*
</sectionHeader>
<bodyText confidence="0.999217461538462">
KA* efficiently explores the space of inside
derivation items because it waits for the exact
Viterbi outside cost before building each deriva-
tion item. However, these outside costs and asso-
ciated deduction items are only auxiliary quanti-
ties used to guide the exploration of inside deriva-
tions: they allow KA* to prioritize currently con-
structed inside derivation items (i.e., constructed
derivations of the goal) by their optimal comple-
tion costs. Outside costs are thus only necessary
because we construct partial derivations bottom-
up; if we constructed partial derivations in a top-
down fashion, all we would need to compute opti-
</bodyText>
<footnote confidence="0.899295">
3We stress that the order of computation is entirely speci-
fied by the deduction rules – we only speak about e.g. “initi-
ating a search” as an appeal to intuition.
</footnote>
<bodyText confidence="0.999202195121951">
mal completion costs are Viterbi inside scores, and
we could forget the outside pass.
TKA* does exactly that. Inside edge items are
constructed in the same way as KA*, but once the
inside edge item I(G, 0, n) has been discovered,
TKA* begins building partial derivations from the
goal outwards. We replace the inside derivation
items of KA* with outside derivation items, which
represent trees rooted at the goal and expanding
downwards. These items bottom out in a list of
edges called the frontier edges. See Figure 1(d)
for a graphical representation. When a frontier
edge represents a single word in the input, i.e. is
of the form (si, i, i + 1), we say that edge is com-
plete. An outside derivation can be expanded by
applying a rule to one of its incomplete frontier
edges; see Figure 2. In the same way that inside
derivation items wait on exact outside scores be-
fore being built, outside derivation items wait on
the inside edge items of all frontier edges before
they can be constructed.
Although building derivations top-down obvi-
ates the need for a 1-best outside pass, it raises a
new issue. When building derivations bottom-up,
the only way to expand a particular partial inside
derivation is to combine it with another partial in-
side derivation to build a bigger tree. In contrast,
an outside derivation item can be expanded any-
where along its frontier. Naively building deriva-
tions top-down would lead to a prohibitively large
number of expansion choices.
We solve this issue by always expanding the
left-most incomplete frontier edge of an outside
derivation item. We show the deduction rule
OUT-D which performs this deduction in Fig-
ure 1(d). We denote an outside derivation item as
Q(TA , i, j, .F), where T��is a tree rooted at the
goal with left-most incomplete edge (A, i, j), and
.F is the list of incomplete frontier edges exclud-
ing (A, i, j), ordered from left to right. Whenever
the application of this rule “completes” the left-
</bodyText>
<page confidence="0.995233">
202
</page>
<bodyText confidence="0.9997226">
most edge, the next edge is removed from F and
is used as the new point of expansion. Once all
frontier edges are complete, the item represents a
correctly scored derivation of the goal, explored in
a pre-order traversal.
</bodyText>
<subsectionHeader confidence="0.999534">
3.1 Correctness
</subsectionHeader>
<bodyText confidence="0.999885739130435">
It should be clear that expanding the left-most in-
complete frontier edge first eventually explores the
same set of derivations as expanding all frontier
edges simultaneously. The only worry in fixing
this canonical order is that we will somehow ex-
plore the Q items in an incorrect order, possibly
building some complete derivation Q&apos;&apos; C before a
more optimal complete derivation QC. However,
note that all items Q along the left-most construc-
tion of QC have priority equal to or better than any
less optimal complete derivation Q&apos;&apos; C. Therefore,
when Q&apos;&apos; C is enqueued, it will have lower priority
than all Q; Q&apos;&apos; C will therefore not be dequeued un-
til all Q – and hence QC – have been built.
Furthermore, it can be shown that the top-down
expansion strategy maintains the same efficiency
and optimality guarantees as KA* for all item
types: for consistent heuristics h, the first k en-
tirely complete outside derivation items are the
true k-best derivations (modulo ties), and that only
derivation items which participate in those k-best
derivations will be removed from the queue (up to
ties).
</bodyText>
<subsectionHeader confidence="0.999827">
3.2 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999981894736842">
Building derivations bottom-up is convenient from
an indexing point of view: since larger derivations
are built from smaller ones, it is not necessary to
construct the larger derivation from scratch. In-
stead, one can simply construct a new tree whose
children point to the old trees, saving both mem-
ory and CPU time.
In order keep the same efficiency when build-
ing trees top-down, a slightly different data struc-
ture is necessary. We represent top-down deriva-
tions as a lazy list of expansions. The top node
TG is an empty list, and whenever we expand an
outside derivation item Q(TA, i, j, F) with a rule
r = A —* B C and split point l, the resulting
derivation TB is a new list item with (r, l) as the
head data, and TA as its tail. The tree can be re-
constructed later by recursively reconstructing the
parent, and adding the edges (B, i,l) and (C, l, j)
as children of (A, i, j).
</bodyText>
<subsectionHeader confidence="0.994961">
3.3 Advantages
</subsectionHeader>
<bodyText confidence="0.999992315789474">
Although our algorithm eliminates the 1-best out-
side pass of KA*, in practice, even for k = 104,
the 1-best inside pass remains the overwhelming
bottleneck (Pauls and Klein, 2009), and our modi-
fications leave that pass unchanged.
However, we argue that our implementation is
simpler to specify and implement. In terms of de-
duction rules, our algorithm eliminates the 2 out-
side deduction rules and replaces the IN-D rule
with the OUT-D rule, bringing the total number
of rules from four to two.
The ease of specification translates directly into
ease of implementation. In particular, if high-
quality heuristics are not available, it is often more
efficient to implement the 1-best inside pass as
an exhaustive dynamic program, as in Huang and
Chiang (2005). In this case, one would only need
to implement a single, agenda-based k-best extrac-
tion phase, instead of the 2 needed for KA*.
</bodyText>
<subsectionHeader confidence="0.897704">
3.4 Performance
</subsectionHeader>
<bodyText confidence="0.999986625">
The contribution of this paper is theoretical, not
empirical. We have argued that TKA* is simpler
than TKA*, but we do not expect it to do any more
or less work than KA*, modulo grammar specific
optimizations. Therefore, we simply verify, like
KA*, that the additional work of extracting k-best
lists with TKA* is negligible compared to the time
spent building 1-best inside edges.
We examined the time spent building 100-best
lists for the same experimental setup as Pauls and
Klein (2009).4 On 100 sentences, our implemen-
tation of TKA* constructed 3.46 billion items, of
which about 2% were outside derivation items.
Our implementation of KA* constructed 3.41 bil-
lion edges, of which about 0.1% were outside edge
items or inside derivation items. In other words,
the cost of k-best extraction is dwarfed by the
the 1-best inside edge computation in both cases.
The reason for the slight performance advantage
of KA* is that our implementation of KA* uses
lazy optimizations discussed in Pauls and Klein
(2009), and while such optimizations could easily
be incorporated in TKA*, we have not yet done so
in our implementation.
</bodyText>
<footnote confidence="0.882772">
4This setup used 3- and 6-round state-split grammars from
Petrov et al. (2006), the former used to compute a heuristic
for the latter, tested on sentences of length up to 25.
</footnote>
<page confidence="0.998987">
203
</page>
<sectionHeader confidence="0.999684" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999994">
We have presented TKA*, a simplification to the
KA* algorithm. Our algorithm collapses the 1-
best outside and bottom-up derivation passes of
KA* into a single, top-down pass without sacri-
ficing efficiency or optimality. This reduces the
number of non base-case deduction rules, making
TKA* easier both to specify and implement.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999452666666667">
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
</bodyText>
<sectionHeader confidence="0.999662" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999608473684211">
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53–64.
V´ıctor M. Jim´enez and Andr´es Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183–192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of the Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 123–134.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119–126.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Computationl Linguis-
tics, 29(1):135–143.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Proccedings of the Association for Computational
Linguistics (ACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3–36.
Frank K. Soong and Eng-Fong Huang. 1991. A tree-
trellis based fast search for finding the n best sen-
tence hypotheses in continuous speech recognition.
In Proceedings of the Workshop on Speech and Nat-
ural Language.
</reference>
<page confidence="0.998879">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728318">
<title confidence="0.999285">K-Best Parsing</title>
<author confidence="0.998754">Pauls Klein</author>
<affiliation confidence="0.999929">Computer Science Division University of California at Berkeley</affiliation>
<abstract confidence="0.978229307692308">We propose a top-down algorithm for exlists from a parser. Our is a variant of the algorithm of Pauls and (2009). In contrast to which performs an inside and outside pass beperforming extraction bottom performs only the inside pass extracting lists top down. maintains the same optimality and guarantees of but is simpler to both specify and implement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>53--64</pages>
<contexts>
<context position="892" citStr="Huang and Chiang (2005)" startWordPosition="141" endWordPosition="145">e kbest A* (KA*) algorithm of Pauls and Klein (2009). In contrast to KA*, which performs an inside and outside pass before performing k-best extraction bottom up, TKA* performs only the inside pass before extracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement. 1 Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as LAZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A* algorithm of Pauls and Klein (2009), hereafter KA*, computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA* can be significantly faster than LAZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA*, a topdown variant of KA* that, like LAZY, performs only an inside pass before extracting k-best lists top-d</context>
<context position="15711" citStr="Huang and Chiang (2005)" startWordPosition="2853" endWordPosition="2856">overwhelming bottleneck (Pauls and Klein, 2009), and our modifications leave that pass unchanged. However, we argue that our implementation is simpler to specify and implement. In terms of deduction rules, our algorithm eliminates the 2 outside deduction rules and replaces the IN-D rule with the OUT-D rule, bringing the total number of rules from four to two. The ease of specification translates directly into ease of implementation. In particular, if highquality heuristics are not available, it is often more efficient to implement the 1-best inside pass as an exhaustive dynamic program, as in Huang and Chiang (2005). In this case, one would only need to implement a single, agenda-based k-best extraction phase, instead of the 2 needed for KA*. 3.4 Performance The contribution of this paper is theoretical, not empirical. We have argued that TKA* is simpler than TKA*, but we do not expect it to do any more or less work than KA*, modulo grammar specific optimizations. Therefore, we simply verify, like KA*, that the additional work of extracting k-best lists with TKA* is negligible compared to the time spent building 1-best inside edges. We examined the time spent building 100-best lists for the same experime</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the International Workshop on Parsing Technologies (IWPT), pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´ıctor M Jim´enez</author>
<author>Andr´es Marzal</author>
</authors>
<title>Computation of the n best parse trees for weighted and stochastic context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition,</booktitle>
<pages>183--192</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<marker>Jim´enez, Marzal, 2000</marker>
<rawString>V´ıctor M. Jim´enez and Andr´es Marzal. 2000. Computation of the n best parse trees for weighted and stochastic context-free grammars. In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition, pages 183–192, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>123--134</pages>
<contexts>
<context position="5643" citStr="Klein and Manning (2001)" startWordPosition="997" endWordPosition="1000">n items called inside edge items I(A, i, j), which represent the many possible inside derivations of an edge (A, i, j). Inside edge items are constructed according to the IN deduction rule of Table 1. This deduction rule constructs inside edge items in a bottom-up fashion, combining items representing smaller edges I(B, i, k) and I(C, k, j) with a grammar rule r = A —* B C to form a larger item I(A, i, j). The weight of a newly constructed item is given by the sum of the weights of the antecedent items and the grammar rule r, and its priority is given by hypergraph search problems as shown in Klein and Manning (2001). G G s0 S1 S2 S3 S4 S5 s0 S1 S2 S3 S4 S5 (a) (b) Figure 2: (a) An outside derivation item before expansion at the edge (VP, 1, 4). (b) A possible expansion of the item in (a) using the rule VPS VP NN. Frontier edges are marked in boldface. its weight plus a heuristic h(A, i, j). For consistent and admissible heuristics h(·), this deduction rule guarantees that when an inside edge item is removed from the agenda, its current weight is its true Viterbi inside score. The heuristic h controls the speed of the algorithm. It can be shown that an edge e satisfying Q(e) + h(A, i, j) &gt; Q(G, 0, n) will</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of the International Workshop on Parsing Technologies (IWPT), pages 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>119--126</pages>
<contexts>
<context position="2054" citStr="Klein and Manning (2003)" startWordPosition="330" endWordPosition="333">forms only an inside pass before extracting k-best lists top-down, but maintains the same optimality and efficiency guarantees as KA*. This algorithm can be seen as a generalization of the lattice k-best algorithm of Soong and Huang (1991) to parsing. Because TKA* eliminates the outside pass from KA*, TKA* is simpler both in implementation and specification. 1See Huang and Chiang (2005) for a review. Chris Quirk Microsoft Research Redmond, WA, 98052 chrisq@microsoft.com 2 Review Because our algorithm is very similar to KA*, which is in turn an extension of the (1-best) A* parsing algorithm of Klein and Manning (2003), we first introduce notation and review those two algorithms before presenting our new algorithm. 2.1 Notation Assume we have a PCFG2 !g and an input sentence s0 ... sn_1 of length n. The grammar !g has a set of symbols denoted by capital letters, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form: each non-terminal rule r in !g has the form r = A —* B C with weight wr. Edges are labeled spans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si ... sj_1. The weight (negative log-probabilit</context>
<context position="4943" citStr="Klein and Manning, 2003" startWordPosition="862" endWordPosition="865">al operation of the algorithm is to pop the highest priority item 0 from the agenda, put it into the chart with its current weight, and apply deduction rules to form any items which can be built by combining 0 with items already in the chart. When the resulting items are either new or have a weight smaller than an item’s best score so far, they are put on the agenda with priority given by p(·). Because all antecedents must be constructed before a deduction rule is executed, we sometimes refer to particular conclusion item as “waiting” on another item before it can be built. 2.2 A* A* parsing (Klein and Manning, 2003) is an algorithm for computing the 1-best parse of a sentence. A* operates on items called inside edge items I(A, i, j), which represent the many possible inside derivations of an edge (A, i, j). Inside edge items are constructed according to the IN deduction rule of Table 1. This deduction rule constructs inside edge items in a bottom-up fashion, combining items representing smaller edges I(B, i, k) and I(C, k, j) with a grammar rule r = A —* B C to form a larger item I(A, i, j). The weight of a newly constructed item is given by the sum of the weights of the antecedent items and the grammar </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL), pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computationl Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3117" citStr="Nederhof, 2003" startWordPosition="535" endWordPosition="536">pans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si ... sj_1. The weight (negative log-probability) of the best (minimum) inside derivation for an edge e is called the Viterbi inside score Q(e), and the weight of the best derivation of G —* s0 ... si_1 A sj ... sn_1 is called the Viterbi outside score a(e). The goal of a kbest parsing algorithm is to compute the k best (minimum weight) inside derivations of the edge (G, 0, n). We formulate the algorithms in this paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form �(���������) φ1 : w1, ... , φn : wn φ0 : g(w1, ... ,wn where 01, ... , 0n are the antecedent items of the deduction rule and 00 is the conclusion item. A deduction rule states that, given the antecedents 01, ... , 0n with weights w1, ... , wn, the conclusion 00 can be formed with weight g(w1, ... , wn) and priority Aw1, ... , wn). 2While we present the algorithm specialized to parsing with a PCFG, this algorithm generalizes to a wide range of 200 Proceedings of the ACL 2010 Conference Short Papers, pages 200–204, Uppsala, Sweden, 11-16 July </context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computationl Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>K-best A* parsing.</title>
<date>2009</date>
<booktitle>In Proccedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1097" citStr="Pauls and Klein (2009)" startWordPosition="176" endWordPosition="179">xtracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement. 1 Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as LAZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A* algorithm of Pauls and Klein (2009), hereafter KA*, computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA* can be significantly faster than LAZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA*, a topdown variant of KA* that, like LAZY, performs only an inside pass before extracting k-best lists top-down, but maintains the same optimality and efficiency guarantees as KA*. This algorithm can be seen as a generalization of the lattice k-best algorithm of Soong and Huang (1991) to parsing. Because TKA* el</context>
<context position="7276" citStr="Pauls and Klein (2009)" startWordPosition="1297" endWordPosition="1300">mpute the best way of building a particular inside edge item. When finding k-best lists, this is no longer possible, since we are interested in suboptimal derivations. Thus, KA*, the k-best extension of A*, must search not in the space of inside edge items, but rather in the space of inside derivation items D(TA, i, j), which represent specific derivations of the edge (A, i, j) using tree TA. However, the number of inside derivation items is exponential in the length of the input sentence, and even with a very accurate heuristic, running A* directly in this space is not feasible. Fortunately, Pauls and Klein (2009) show that with a perfect heuristic, that is, h(e) = a(e) be, A* search on inside derivation items will only remove items from the agenda that participate in the true k-best lists (up to ties). In order to compute this perfect heuristic, KA* makes use of outside edge items O(A, i, j) which represent the many possible derivations of G —* (c) (d) G VP (a) (b) VP s2 s3 s4 s0 ...s2 s5 ... sn-1 NP VP G VP VBZ NP DT NN s2 s3 s4 SO s1 s2 sn-1 NN VP NP NP NP VP NN NP VP VP VP NP NN VP NN 201 w1+w2+wr+h(A,i,j) IN∗†: I(B, i, l) :w1 I(C, l, j) : w2 → I(A, i, j) : w1 + w2 + wr IN-D†: O(A, i, j) : w1 D(T B</context>
<context position="15135" citStr="Pauls and Klein, 2009" startWordPosition="2758" endWordPosition="2761">rivations as a lazy list of expansions. The top node TG is an empty list, and whenever we expand an outside derivation item Q(TA, i, j, F) with a rule r = A —* B C and split point l, the resulting derivation TB is a new list item with (r, l) as the head data, and TA as its tail. The tree can be reconstructed later by recursively reconstructing the parent, and adding the edges (B, i,l) and (C, l, j) as children of (A, i, j). 3.3 Advantages Although our algorithm eliminates the 1-best outside pass of KA*, in practice, even for k = 104, the 1-best inside pass remains the overwhelming bottleneck (Pauls and Klein, 2009), and our modifications leave that pass unchanged. However, we argue that our implementation is simpler to specify and implement. In terms of deduction rules, our algorithm eliminates the 2 outside deduction rules and replaces the IN-D rule with the OUT-D rule, bringing the total number of rules from four to two. The ease of specification translates directly into ease of implementation. In particular, if highquality heuristics are not available, it is often more efficient to implement the 1-best inside pass as an exhaustive dynamic program, as in Huang and Chiang (2005). In this case, one woul</context>
<context position="16868" citStr="Pauls and Klein (2009)" startWordPosition="3045" endWordPosition="3048">ined the time spent building 100-best lists for the same experimental setup as Pauls and Klein (2009).4 On 100 sentences, our implementation of TKA* constructed 3.46 billion items, of which about 2% were outside derivation items. Our implementation of KA* constructed 3.41 billion edges, of which about 0.1% were outside edge items or inside derivation items. In other words, the cost of k-best extraction is dwarfed by the the 1-best inside edge computation in both cases. The reason for the slight performance advantage of KA* is that our implementation of KA* uses lazy optimizations discussed in Pauls and Klein (2009), and while such optimizations could easily be incorporated in TKA*, we have not yet done so in our implementation. 4This setup used 3- and 6-round state-split grammars from Petrov et al. (2006), the former used to compute a heuristic for the latter, tested on sentences of length up to 25. 203 4 Conclusion We have presented TKA*, a simplification to the KA* algorithm. Our algorithm collapses the 1- best outside and bottom-up derivation passes of KA* into a single, top-down pass without sacrificing efficiency or optimality. This reduces the number of non base-case deduction rules, making TKA* e</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. K-best A* parsing. In Proccedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proccedings of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proccedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="3100" citStr="Shieber et al., 1995" startWordPosition="531" endWordPosition="534">r. Edges are labeled spans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si ... sj_1. The weight (negative log-probability) of the best (minimum) inside derivation for an edge e is called the Viterbi inside score Q(e), and the weight of the best derivation of G —* s0 ... si_1 A sj ... sn_1 is called the Viterbi outside score a(e). The goal of a kbest parsing algorithm is to compute the k best (minimum weight) inside derivations of the edge (G, 0, n). We formulate the algorithms in this paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form �(���������) φ1 : w1, ... , φn : wn φ0 : g(w1, ... ,wn where 01, ... , 0n are the antecedent items of the deduction rule and 00 is the conclusion item. A deduction rule states that, given the antecedents 01, ... , 0n with weights w1, ... , wn, the conclusion 00 can be formed with weight g(w1, ... , wn) and priority Aw1, ... , wn). 2While we present the algorithm specialized to parsing with a PCFG, this algorithm generalizes to a wide range of 200 Proceedings of the ACL 2010 Conference Short Papers, pages 200–204, Uppsala, Sw</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank K Soong</author>
<author>Eng-Fong Huang</author>
</authors>
<title>A treetrellis based fast search for finding the n best sentence hypotheses in continuous speech recognition.</title>
<date>1991</date>
<booktitle>In Proceedings of the Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="1669" citStr="Soong and Huang (1991)" startWordPosition="269" endWordPosition="272"> The k-best A* algorithm of Pauls and Klein (2009), hereafter KA*, computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA* can be significantly faster than LAZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA*, a topdown variant of KA* that, like LAZY, performs only an inside pass before extracting k-best lists top-down, but maintains the same optimality and efficiency guarantees as KA*. This algorithm can be seen as a generalization of the lattice k-best algorithm of Soong and Huang (1991) to parsing. Because TKA* eliminates the outside pass from KA*, TKA* is simpler both in implementation and specification. 1See Huang and Chiang (2005) for a review. Chris Quirk Microsoft Research Redmond, WA, 98052 chrisq@microsoft.com 2 Review Because our algorithm is very similar to KA*, which is in turn an extension of the (1-best) A* parsing algorithm of Klein and Manning (2003), we first introduce notation and review those two algorithms before presenting our new algorithm. 2.1 Notation Assume we have a PCFG2 !g and an input sentence s0 ... sn_1 of length n. The grammar !g has a set of sy</context>
</contexts>
<marker>Soong, Huang, 1991</marker>
<rawString>Frank K. Soong and Eng-Fong Huang. 1991. A treetrellis based fast search for finding the n best sentence hypotheses in continuous speech recognition. In Proceedings of the Workshop on Speech and Natural Language.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>