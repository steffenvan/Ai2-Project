<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.987559">
Using Conceptual Class Attributes to Characterize Social Media Users
</title>
<author confidence="0.998694">
Shane Bergsma and Benjamin Van Durme
</author>
<affiliation confidence="0.795870333333333">
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
</affiliation>
<sectionHeader confidence="0.960444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899947368421">
We describe a novel approach for automat-
ically predicting the hidden demographic
properties of social media users. Building
on prior work in common-sense knowl-
edge acquisition from third-person text,
we first learn the distinguishing attributes
of certain classes of people. For exam-
ple, we learn that people in the Female
class tend to have maiden names and en-
gagement rings. We then show that this
knowledge can be used in the analysis of
first-person communication; knowledge of
distinguishing attributes allows us to both
classify users and to bootstrap new train-
ing examples. Our novel approach enables
substantial improvements on the widely-
studied task of user gender prediction, ob-
taining a 20% relative error reduction over
the current state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997959163265306">
There has been growing interest in characteriz-
ing social media users based on the content they
generate; that is, automatically labeling users with
demographic categories such as age and gender
(Burger and Henderson, 2006; Schler et al., 2006;
Rao et al., 2010; Mukherjee and Liu, 2010; Pen-
nacchiotti and Popescu, 2011; Burger et al., 2011;
Van Durme, 2012). Automatic user character-
ization has applications in targeted advertising
and personalization, and could also lead to finer-
grained assessment of public opinion (O’Connor
et al., 2010) and health (Paul and Dredze, 2011).
Consider the following tweet and suppose we
wish to predict the user’s gender:
Dirac was one of my boyhood heroes.
I’m glad I met him once. RT Paul Dirac
image by artist Eric Handy: http:...
State-of-the-art approaches cast this problem as a
classification task and train classifiers using super-
vised learning (Section 2). The features of the
classifier are indicators of specific words in the
user-generated text. While a human would as-
sume that someone with boyhood heroes is male,
a standard classifier has no way of exploiting such
knowledge unless the phrase occurs in training
data. We present an algorithm that improves user
characterization by collecting and exploiting such
common-sense knowledge.
Our work is inspired by algorithms that pro-
cesses large text corpora in order to discover the
attributes of semantic classes, e.g. (Berland and
Charniak, 1999; Schubert, 2002; Almuhareb and
Poesio, 2004; Tokunaga et al., 2005; Girju et al.,
2006; Pas¸ca and Van Durme, 2008; Alfonseca et
al., 2010). We learn the distinguishing attributes
of different demographic groups (Section 3), and
then automatically assign users to these groups
whenever they refer to a distinguishing attribute in
their writings (Section 4). Our approach obviates
the need for expensive annotation efforts, and al-
lows us to rapidly bootstrap training data for new
classification tasks.
We validate our approach by advancing the
state-of-the-art on the most well-studied user clas-
sification task: predicting user gender (Section 5).
Our bootstrapped system, trained purely from
automatically-annotated Twitter data, significantly
reduces error over a state-of-the-art system trained
on thousands of gold-standard training examples.
</bodyText>
<sectionHeader confidence="0.977736" genericHeader="method">
2 Supervised User Characterization
</sectionHeader>
<bodyText confidence="0.999905142857143">
The current state-of-the-art in user characteriza-
tion is to use supervised classifiers trained on an-
notated data. For each instance to be classified, the
output is a decision about a distinct demographic
property, such as Male/Female or Over/Under-18.
A variety of classification algorithms have been
employed, including SVMs (Rao et al., 2010), de-
</bodyText>
<page confidence="0.950561">
710
</page>
<note confidence="0.92918">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710–720,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.96230576">
cision trees (Pennacchiotti and Popescu, 2011), lo-
gistic regression (Van Durme, 2012), and the Win-
now algorithm (Burger et al., 2011).
Content Features: BoW Prior classifiers use a
set of features encoding the presence of specific
words in the user-generated text. We call these
features BoW features as they encode the stan-
dard Bag-of-Words representation which has been
highly effective in text categorization and informa-
tion retrieval (Sebastiani, 2002).
User-Profile Features: Usr Some researchers
have explored features for user-profile meta-
information in addition to user content. This may
include the user’s communication behavior and
network of contacts (Rao et al., 2010), their full
name (Burger et al., 2011) and whether they pro-
vide a profile picture (Pennacchiotti and Popescu,
2011). We focus on the case where we only
have access to the user’s screen-name (a.k.a. user-
name). Using a combination of content and user-
name features “represents a use case common to
many different social media sites, such as chat
rooms and news article comment streams” (Burger
et al., 2011). We refer to features derived from a
username as Usr features in our experiments.
</bodyText>
<sectionHeader confidence="0.98907" genericHeader="method">
3 Learning Class Attributes
</sectionHeader>
<bodyText confidence="0.971589491803279">
We aim to improve the automated classification
of users into various demographic categories by
learning and applying the distinguishing attributes
of those categories, e.g. that males have boyhood
heroes. Our approach builds on lexical-semantic
research on the topic of class-attribute extraction.
In this research, the objective is to discover vari-
ous attributes or parts of classes of entities. For
example, Berland and Charniak (1999) learn that
the class car has parts such as headlight, wind-
shield, dashboard, etc. Berland and Charniak ex-
tract these attributes by mining a corpus for fillers
of patterns such as ‘car’s X’ or ‘X of a car’. Note
their patterns explicitly include the class itself
(car). Another approach is to use patterns that are
based on instances (i.e. hyponyms or sub-classes)
of the class. For example, Pas¸ca and Van Durme
(2007) learn the attributes of the class car via pat-
terns involving instances of cars, e.g. Chevrolet
Corvette’s X and X of a Honda Civic. For these ap-
proaches, lists of instances are typically collected
from publicly-available resources such as Word-
Net or Wikipedia (Pas¸ca and Van Durme, 2007;
Van Durme et al., 2008), acquired automatically
from corpora (Pas¸ca and Van Durme, 2008; Al-
fonseca et al., 2010), or simply specified by hand
(Schubert, 2002).
Creation of Instance Lists We use an instance-
based approach; our instances are derived from
collections of common nouns that are associated
with roles and occupations of people. For the
gender task that we study in our experiments, we
acquire class instances by filtering the dataset of
nouns and their genders created by Bergsma and
Lin (2006). This dataset indicates how often a
noun is referenced by a male, female, neutral or
plural pronoun. We extract prevalent common
nouns for males and females by selecting only
those nouns that (a) occur more than 200 times
in the dataset, (b) mostly occur with male or fe-
male pronouns, and (c) occur as lower-case more
often than upper-case in a web-scale N-gram cor-
pus (Lin et al., 2010). We then classify a noun as
Male (resp. Female) if the noun is indicated to
occur with male (resp. female) pronouns at least
85% of the time. Since the gender data is noisy,
we also quickly pruned by hand any instances that
were malformed or obviously incorrectly assigned
by our automatic process. This results in 652 in-
stances in total. Table 1 provides some examples.
Male: bouncer, altar boy, army officer, dictator,
assailant, cameraman, drifter, chauffeur, bad guy
Female: young lady, lesbian, ballerina, waitress,
granny, chairwoman, heiress, soprano, socialite
Table 1: Example instances used for extraction of
class attributes for the gender classification task
Attribute Extraction We next collect and rank
attributes for each class. We first look for fillers of
attribute-patterns involving each of the instances.
Let I represent an instance of one of our classes.
We find fillers of the single high-precision pattern:
</bodyText>
<equation confidence="0.934753">
[{word=.*}* {tag=N.*}]
� Y �
attribute
</equation>
<bodyText confidence="0.999461">
(E.g. dictator ’s [former mistress]). The expres-
sion “tag=NN” means that I must be tagged as
a noun. The expression in square brackets is the
filler, i.e. the extracted attribute, A. The notation
“{word=.*}* tag=N.*” means that A can be any
sequence of tokens ending in a noun. We use an
</bodyText>
<equation confidence="0.833171833333333">
{word=I,tag=NN}
� Y �
instance
{word=’s}
� Y �
’s
</equation>
<page confidence="0.971689">
711
</page>
<bodyText confidence="0.9995824375">
equivalent pattern when I is multi-token. The out-
put of this process is a set of (I,A) pairs.
In attribute extraction, typically one must
choose between the precise results of rich pat-
terns (involving punctuation and parts-of-speech)
applied to small corpora (Berland and Charniak,
1999) and the high-coverage results of superficial
patterns applied to web-scale data, e.g. via the
Google API (Almuhareb and Poesio, 2004). We
obtain the best of both worlds by matching our
precise pattern against a version of the Google N-
gram Corpus that includes the part-of-speech tag
distributions for every N-gram (Lin et al., 2010).
We found that applying this pattern to web-scale
data is effective in extracting useful attributes. We
acquired around 20,000 attributes in total.
</bodyText>
<subsectionHeader confidence="0.586268">
Finding Distinguishing Attributes Unlike
</subsectionHeader>
<bodyText confidence="0.99985585">
prior work, we aim to find distinguishing proper-
ties of each class; that is, the kinds of properties
that uniquely distinguish a particular category.
Prior work has mostly focused on finding “rel-
evant” attributes (Alfonseca et al., 2010) or
“correct” parts (Berland and Charniak, 1999). A
leg is a relevant and correct part of both a male and
a female (and many other living and inanimate
objects), but it does not help us distinguish males
from females in social media. We therefore rank
our attributes for each class by their strength of
association with instances of that specific class.1
To calculate the association, we first disregard
the count of each (I,A) pair and consider each
unique pair to be a single probabilistic event.
We then convert the (I,A) pairs to corresponding
(C,A) pairs by replacing I with the corresponding
class, C. We then calculate the pointwise mutual
information (Church and Hanks, 1990) between
each C and A over the set of events:
</bodyText>
<equation confidence="0.998669">
PMI(C, A) = log p(C, A) (1)
p(C)p(A)
</equation>
<bodyText confidence="0.974164140625">
If the PMI&gt;0, the observed probability of a class
and attribute co-occurring is greater than the prob-
ability of co-occurrence that we would expect if C
and A were independently distributed. For each
class, we rank the attributes by their PMI scores.
1Reisinger and Pas¸ca (2009) considered the related prob-
lem of finding the most appropriate class for each attribute;
they take an existing ontology of concepts (WordNet) as a
class hierarchy and use a Bayesian approach to decide “the
correct level of abstraction for each attribute.”
Filtering Attributes We experimented with two
different methods to select a final set of distin-
guishing attributes for each class: (1) we used
a threshold to select the top-ranked attributes for
each class, and (2) we manually filtered the at-
tributes. For the gender classification task, we
manually filtered the entire set of attributes to se-
lect around 1000 attributes that were judged to be
discriminative (two thirds of which are female).
This filtering took one annotator only a few hours
to complete. Because this process was so trivial,
we did not invest in developing annotation guide-
lines or measuring inter-annotator agreement. We
make these filter attributes available online as an
attachment to this article, available through the
ACL Anthology.
Ultimately, we discovered that manual filter-
ing was necessary to avoid certain pathological
cases in our Twitter data. For example, our PMI
scoring finds homepage to be strongly associated
with males. In our gold-standard gender data
(Section 5), however, every user has a home-
page [by dataset construction]; we might there-
fore incorrectly classify every user as Male. We
agree with Richardson et al. (1998) that “auto-
matic procedures ... provide the only credible
prospect for acquiring world knowledge on the
scale needed to support common-sense reasoning”
but “hand vetting” might be needed to ensure “ac-
curacy and consistency in production level sys-
tems.” Since our approach requires manual in-
volvement in the filtering of the attribute list, one
might argue that one should simply manually enu-
merate the most relevant attributes directly. How-
ever, the manual generation of conceptual features
by a single researcher results in substantial vari-
ability both across and within participants (McRae
et al., 2005). Psychologists therefore generate
such lists by pooling the responses across many
participants: future work may compare our “auto-
matically generate, manually prune” approach to
soliciting attributes via crowdsourcing.2
Table 2 gives examples of our extracted at-
2One can also view the work of manually filtering at-
tributes as a kind of “feature labeling.” There is evidence
from Zaidan et al. (2007) that a few hours of feature labeling
can be more productive than annotating new training exam-
ples. In fact, since Zaidan et al. (2007) label features at the
token level (e.g., in our case one would highlight “handbag”
in a given tweet), while we label features at the type level
(e.g., deciding whether to mark the word “handbag” as fem-
inine in general), our process is likely even more efficient.
Future work may also wish to consider this connection to so-
called ”annotator rationales” more deeply.
</bodyText>
<page confidence="0.984701">
712
</page>
<bodyText confidence="0.96599384">
Male: wife, widow, wives, ex-girlfriend, erec-
tion, testicles, wet dream, bride, buddies, ex-
wife, first-wife, penis, death sentence, manhood
Female: vagina, womb, maiden name, dresses,
clitoris, wedding dress, uterus, shawl, necklace,
ex-husband, ex-boyfriend, dowry, nightgown
Table 2: Example attributes for gender classes, in
descending order of class-association score
tributes. Our approach captures many multi-token
attributes; these are often distinguishing even
though the head noun is ambiguous (e.g. name
is ambiguous, maiden name is not). Our attributes
also go beyond the traditional meronyms that were
the target of earlier work. As we discuss further
in Related Work (Section 7), previous researchers
have worried about a proper definition of parts or
attributes and relied on human judgments for eval-
uation (Berland and Charniak, 1999; Girju et al.,
2006; Van Durme et al., 2008). For us, whether
a property such as dowry should be considered
an “attribute” of the class Female is immaterial;
we echo Almuhareb and Poesio (2004) who (on a
different task) noted that “while the notion of ‘at-
tribute’ is not completely clear... our results sug-
gest that trying to identify attributes is beneficial.”
</bodyText>
<sectionHeader confidence="0.978176" genericHeader="method">
4 Applying Class Attributes
</sectionHeader>
<bodyText confidence="0.9999769375">
To classify users using the extracted attributes, we
look for cases where users refer to such attributes
in their first-person writings. We performed a pre-
liminary analysis of a two-week sample of tweets
from the TREC Tweets2011 Corpus.3 We found
that users most often reveal their attributes in the
possessive construction, “my X” where X is an at-
tribute, quality or event that they possess (in a lin-
guistic sense). For example, we found over 1000
tweets with the phrase “my wife.” In contrast, “I
have a wife” occurs only 5 times.4
We therefore assign a user to a demographic
category as follows: We first part-of-speech tag
our data using CRFTagger (Phan, 2006) and then
look for “my X” patterns where X is a sequence
of tokens terminating in a noun, analogous to our
</bodyText>
<footnote confidence="0.591000714285714">
3http://trec.nist.gov/data/tweets/ This corpus was de-
veloped for the TREC Microblog track (Soboroff et al., 2012).
4Note that “I am a man” occurs only 20 times. Users
also reveal their names in “my name is X” patterns in several
hundred tweets, but this is small compared to cases of self-
distinguishing attributes. Exploiting these alternative pat-
terns could nevertheless be a possible future direction.
</footnote>
<bodyText confidence="0.977345875">
attribute-extraction pattern (Section 3).5 When a
user uses such a “my X” construction, we match
the filler X against our attribute lists for each
class. If the filler is on a list, we call it a self-
distinguishing attribute of a user. We then apply
our knowledge of the self-distinguishing attribute
and its corresponding class in one of the following
three ways:
</bodyText>
<listItem confidence="0.987914307692308">
(1) ARules: Using Attribute-Based Rules to
Override a Classifier When human-annotated
data is available for training and testing a su-
pervised classifier, we refer to it as gold stan-
dard data. Our first technique provides a sim-
ple way to use our identified self-distinguishing
attributes in conjunction with a classifier trained
on gold-standard data. If the user has any self-
distinguishing attributes, we assign the user to the
corresponding class; otherwise, we trust the output
of the classifier.
(2) Bootstrapped: Automatic Labeling of Train-
ing Examples Even without gold standard train-
</listItem>
<bodyText confidence="0.810721533333333">
ing data, we can use our self-distinguishing at-
tributes to automatically bootstrap annotations.
We collect a large pool of unlabeled users and their
tweets, and we apply the ARules described above
to label those users that have self-distinguishing
attributes. Once an example is auto-annotated,
we delete the self-distinguishing attributes from
the user’s content. This prevents the subsequent
learning algorithm from trivially learning the rules
with which we auto-annotated the data. Next, the
auto-annotated examples are used as training data
for a supervised system.6 Finally, when applying
the Bootstrapped classifiers, we can still apply the
ARules as a post-process (although in practice this
made little difference in our final results).
</bodyText>
<listItem confidence="0.9567755">
(3) BootStacked: Gold Standard and Boot-
strapped Combination Although we show that
</listItem>
<bodyText confidence="0.9675446">
an accurate classifier can be trained using auto-
annotated Bootstrapped data alone, we also test
whether we can combine this data with any gold-
standard training examples to achieve even better
performance. We use the following simple but
</bodyText>
<footnote confidence="0.781208555555556">
5While we used an “off the shelf” POS tagger in this
work, we note that taggers optimized specifically for social
media are now available and would likely have resulted in
higher tagging accuracy (e.g. Owoputi et al. (2013)).
6Note that while our target gender task presents mutually-
exclusive output classes, we can still train classifiers for other
categories without clear opposites (e.g. for labeling users
as Parents or Doctors) by using the 1-class classification
paradigm (Koppel and Schler, 2004).
</footnote>
<page confidence="0.99702">
713
</page>
<bodyText confidence="0.9999262">
effective method for combining data from these
two sources, inspired by prior techniques used in
the domain adaptation literature (Daum´e III and
Marcu, 2006). We first use the trained Boot-
strapped system to make predictions on the entire
set of gold standard data (gold train, development,
and test sets). We then use these predictions as
features in a classifier trained on the gold standard
data. We refer to this system as the BootStacked
system in our evaluation.
</bodyText>
<sectionHeader confidence="0.978159" genericHeader="method">
5 Twitter Gender Prediction
</sectionHeader>
<bodyText confidence="0.993488261363637">
To test the use of self-distinguishing attributes
in user classification, we apply our methods to
the task of gender classification on Twitter. This
is an important and intensely-studied task within
academia and industry. Furthermore, for this task
it is possible to semi-automatically acquire large
amounts of ground truth (Burger et al., 2011).
We can therefore benchmark our approach against
state-of-the-art supervised systems trained with
plentiful gold-standard data, giving us an idea of
how well our Bootstrapped system might compare
to theoretically top-performing systems on other
tasks, domains, and social media platforms where
such gold-standard training data is not available.
Gold Data Our data is derived from the corpus
created by Burger et al. (2011). Burger et al. ob-
served that many Twitter users link their Twitter
profile to homepages on popular blogging web-
sites. Since “many of these [sites] have well-
structured profile pages [where users] must se-
lect gender and other attributes from dropdown
menus,” they were able to link these attributes to
the Twitter users. Using this process, they created
a large multi-lingual corpus of Twitter users and
genders.
We filter non-English tweets from this corpus
using the LID system of Bergsma et al. (2012)
and also tweets containing URLs (since many of
these are spam) and re-tweets. We then filter users
with &lt;40 tweets and randomly divide the remain-
ing users into 2282 training, 1140 development,
and 1141 test examples.
Classifier Set-up We train logistic-regression
classifiers on this gold standard data via the LI-
BLINEAR package (Fan et al., 2008). We optimize
the classifier’s regularization parameter on devel-
opment data and report final results on the held-
out test examples. We also report the results of
our new attribute-based strategies (Section 4) on
the test data. We report accuracy: the percentage
of examples labeled correctly.
Our classifiers use both BoW and Usr features
(Section 2). To increase the generality of our
BoW features, we preprocess the text by lower-
casing and converting all digits to special ‘#’ sym-
bols. We then create real-valued features that
encode the log-count of each word in the input.
While Burger et al. (2011) found “no apprecia-
ble difference in performance” when using either
binary presence/absence features or encoding the
frequency of the word, we found real-valued fea-
tures worked better in development experiments.
For the Usr features, we add special beginning and
ending characters to the username, and then create
features for all character n-grams of length two-
to-four in the modified username string. We in-
clude n-gram features with the original capitaliza-
tion pattern and separate features with the n-grams
lower-cased.
Unlabeled Data For Bootstrapped training, we
also use a pool of unlabeled Twitter data. This
pool comprises the union of 2.2 billion tweets
from 05/2009 to 10/2010 (O’Connor et al., 2010),
1.9 billion tweets collected from 07/2011 to
11/2012, and 80 million tweets collected from the
followers of 10-thousand location and language-
specific Twitter feeds. We filter this corpus as
above, except we do not put any restrictions on the
number of tweets needed per user. We also filter
any users that overlap with our gold standard data.
Bootstrapping Analysis We apply our Boot-
strapped auto-annotation strategy to this unlabeled
data, yielding 789,285 auto-annotated examples
of users and their tweets. The decisions of our
bootstrapping process reflect the true gender dis-
tribution; the auto-annotated data is 60.5% Fe-
male, remarkably close to the 60.9% proportion
in our gold standard test set. Figure 1 shows that
a wide range of self-distinguishing attributes are
used in the auto-annotation process. This is impor-
tant because if only a few attributes are used (e.g.
wife/husband or penis/vagina), we might system-
atically miss a segment of users (e.g. young people
that don’t have husbands or wives, or people that
don’t frequently talk about their genitalia). Thus a
wide range of common-sense knowledge is useful
for bootstrapping, which is one reason why auto-
matic approaches are needed to acquire it.
</bodyText>
<page confidence="0.994824">
714
</page>
<figureCaption confidence="0.992374">
Figure 1: Frequency with which attributes are used to auto-annotate examples in the bootstrapping ap-
proach. The plot identifies some attributes and their corresponding class (labeled via gender symbol).
</figureCaption>
<figure confidence="0.985569166666667">
boyfriend ♀ Note: showing only first 10% of attributes used
200000
wife ♂
husband ♀
engagement ring ♀
hubby ♀
due date ♀
purse ♀
beard ♂
tux ♂
natural hair ♀
bra ♀ future wife ♂
jewelry ♀
bride ♂
0
150000
100000
50000
</figure>
<table confidence="0.951393571428572">
Majority-class baseline 60.9
Supervised on 100 examples 72.0
Supervised on 2282 examples 84.0
Supervised on 100 examples + ARules 74.7
Supervised on 2282 examples + ARules 84.7
Bootstrapped 86.0
BootStacked 87.2
</table>
<tableCaption confidence="0.6763495">
Table 3: Classification accuracy (%) on gold stan-
dard test data for user gender prediction on Twitter
</tableCaption>
<sectionHeader confidence="0.99985" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999588533333334">
Our main classification results are presented in Ta-
ble 3. The majority-class baseline for this task
is to always choose Female; this achieves an ac-
curacy of 60.9%. A standard classifier trained
on 100 gold-standard training examples improves
over this baseline, to 72.0%, while one with 2282
training examples achieves 84.0%. This latter re-
sult represents the current state-of-the-art: a clas-
sifier trained on thousands of gold standard exam-
ples, making use of both Usr and BoW features.
Our performance compares favourably to Burger
et al. (2011), who achieved 81.4% using the same
features, but on a very different subset of the data
(also including tweets in other languages).7
Applying the ARules as a post-process signifi-
cantly improves performance in both cases (Mc-
Nemar’s, p&lt;0.05). It is also possible to use the
ARules as a stand-alone system rather than as a
post-process, however the coverage is low: we find
a distinguishing attribute in 18.3% of the 695 Fe-
male instances in the test data, and make the cor-
7Note that it is possible to achieve even higher perfor-
mance on gender classification in social media if you have
further information about a user, such as their full first and
last name (Burger et al., 2011; Bergsma et al., 2013).
rect decision in 96.9% of these cases. We find a
distinguishing attribute in 11.4% of the 446 Male
instances, with 86.3% correct decisions.
The Bootstrapped system substantially im-
proves over the state-of-the-art, achieving 86% ac-
curacy and doing so without using any gold stan-
dard training data. This is important because hav-
ing thousands of gold standard annotations for ev-
ery possible user characterization task, in every
domain and social media platform, is not realis-
tic. Combining the bootstrapped classifier with
the gold standard annotations in the BootStacked
model results in further gains in performance.8
These results provide strong validation for both
the inherent utility of class-attributes knowledge in
user characterization and the effectiveness of our
specific strategies for exploiting such knowledge.
Figure 2 shows the learning curve of the Boot-
strapped classifier. Performance rises consistently
across all the auto-annotated training data; this
is encouraging because there is theoretically no
reason not to vastly increase the amount of auto-
annotated data by collecting an even larger col-
lection of tweets. Finally, note that most of the
gains of the Bootstrapped system appear to derive
from the tweet content itself, i.e. the BoW fea-
tures. However, the Usr features are also helpful
at most training sizes.
We provide some of the top-ranked features of
the Bootstrapped system in Table 4. We see that
a variety of other common-sense knowledge is
learned by the system (e.g., the association be-
tween males and urinals, boxers, fatherhood, etc.),
as well as stylistic clues (e.g. Female users using
betcha and xox in their writing). The username
</bodyText>
<footnote confidence="0.8421545">
8We observed no further gains in accuracy when applying
the ARules as a post-process on top of these systems.
</footnote>
<page confidence="0.99648">
715
</page>
<figure confidence="0.9859675">
100 1000 10000 100000 1e+06
Number of auto-annotated training pts.
</figure>
<figureCaption confidence="0.962161">
Figure 2: Learning curve for Bootstrapped
logistic-regression classifier, with automatically-
labeled data, for different feature classes.
</figureCaption>
<bodyText confidence="0.8457925">
features capture reasonable associations between
gender classes and particular names (such as mike,
tony, omar, etc.) and also between gender classes
and common nouns (such as guy, dad, sir, etc.).
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.997064917808219">
User Characterization The field of sociolin-
guistics has long been concerned with how various
morphological, phonological and stylistic aspects
of language can vary with a person’s age, gender,
social class, etc. (Fischer, 1968; Labov, 1972).
This early work therefore had an emphasis on ana-
lyzing the form of language, as opposed to its con-
tent. This emphasis continued into early machine
learning approaches, which predicted author prop-
erties based on the usage of function words, parts-
of-speech, punctuation (Koppel et al., 2002) and
spelling/grammatical errors (Koppel et al., 2005).
Recently, researchers have focused less on the
sociolinguistic implications and more on the tasks
themselves, naturally leading to classifiers with
feature representations capturing content in ad-
dition to style (Schler et al., 2006; Garera and
Yarowsky, 2009; Mukherjee and Liu, 2010). Our
work represents a logical next step for content-
based classification, a step partly suggested by
Schler et al. (2006) who noted that “those who
are interested in automatically profiling bloggers
for commercial purposes would be well served by
considering additional features - which we delib-
erately ignore in this study - such as author self-
identification.”
Male BoW features: wife, wifey, sucked, shave,
boner, boxers, missus, installed, manly, in-laws,
brah, urinal, kickoff, golf, comics, ubuntu, homo,
nhl, jedi, fatherhood, nigga, movember, algebra
Male Usr features: boy, mike, ben, guy, mr, dad,
jr, kid, tony, dog, lord, sir, omar, dude, man, big
Female BoW features: hubby, hubs, jewelry,
sewing, mascara, fabulous, bf, softball, betcha,
motherhood, perky, cozy, zumba, xox, cuddled,
belieber, bridesmaid, anorexic, jammies, pad
Female Usr features: mrs, mom, jen, lady, wife,
mary, joy, mama, pink, kim, diva, elle, woma, ms
Table 4: Examples of highly-weighted BoW (con-
tent) and Usr (username) features (in descending
order of weight) in the Bootstrapped system for
predicting user gender in Twitter.
Many recent papers have analyzed the lan-
guage of social media users, along dimensions
such as ethnicity (Eisenstein et al., 2011; Rao et
al., 2011; Pennacchiotti and Popescu, 2011; Fink
et al., 2012) time zone (Kiciman, 2010), polit-
ical orientation (Rao et al., 2010; Pennacchiotti
and Popescu, 2011) and gender (Rao et al., 2010;
Burger et al., 2011; Van Durme, 2012).
Class-Attribute Extraction The idea of using
simple patterns to extract useful semantic relations
goes back to Hearst (1992) who focused on hy-
ponyms. Hearst reports that she “tried applying
this technique to meronymy (i.e., the part/whole
relation), but without great success.” Berland and
Charniak (1999) did have success using Hearst-
style patterns for part-whole detection, which they
attribute to their “very large corpus and the use of
more refined statistical measures for ranking the
output.” Girju et al. (2006) devised a supervised
classification scheme for part/whole relation dis-
covery that integrates the evidence from multiple
patterns. These efforts focused exclusively on the
meronymy relation as used in WordNet (Miller et
al., 1990). Indeed, Berland and Charniak (1999)
attempted to filter out attributes that were regarded
as qualities (like driveability) rather than parts
(like steering wheels) by removing words end-
ing with the suffixes -ness, -ing, and -ity. In our
work, such qualities are not filtered and are ulti-
mately valuable in classification; for example, the
attributes peak fertility and loveliness are highly
</bodyText>
<figure confidence="0.969214454545455">
90
85
80
75
70
65
60
BoW+Usr
BoW
Usr
Accuracy
</figure>
<page confidence="0.993328">
716
</page>
<bodyText confidence="0.998111340909091">
associated with females.
As subsequent research became more focused
on applications, looser definitions of class at-
tributes were adopted. Almuhareb and Poesio
(2004) automatically mined class attributes that in-
clude parts, qualities, and those with an “agen-
tive” or “telic” role with the class. Their ex-
tended set of attributes was shown to enable an
improved representation of nouns for the purpose
of clustering these nouns into semantic concepts.
Tokunaga et al. (2005) define attributes as prop-
erties that can serve as focus words in questions
about a target class; e.g. director is an attribute
of a movie since one might ask, “Who is the di-
rector of this movie?” Another line of research
has been motivated by the observation that much
of Internet search consists of people looking for
values of various class attributes (Bellare et al.,
2007; Pas¸ca and Van Durme, 2007; Pas¸ca and Van
Durme, 2008; Alfonseca et al., 2010). By knowing
the attributes of different classes, search engines
can better recognize that queries such as “altitude
guadalajara” or “population guadalajara” are seek-
ing values for a particular city’s “altitude” and
“population” attributes (Pas¸ca and Van Durme,
2007). Finally, note that Van Durme et al. (2008)
compared instance-based and class-based patterns
for broad-definition attribute extraction, and found
both to be effective.
Of course, text-mining with custom-designed
patterns is not the only way to extract class-
attribute information. Experts can manually spec-
ify the attributes of entities, as in the WordNet
project (Miller et al., 1990). Others have auto-
matically extracted attribute relations from dictio-
nary definitions (Richardson et al., 1998), struc-
tured online sources such as Wikipedia infoboxes,
(Wu and Weld, 2007) and large-scale collections
of high-quality tabular web data (Cafarella et al.,
2008). Attribute extraction has also been viewed
as a sub-component or special case of the infor-
mation obtained by general-purpose knowledge
extractors (Schubert, 2002; Pantel and Pennac-
chiotti, 2006).
</bodyText>
<subsectionHeader confidence="0.73796">
NLP Applications of Common-Sense Knowl-
</subsectionHeader>
<bodyText confidence="0.999958722222222">
edge The kind of information derived from
class-attribute extraction is sometimes referred to
as a type of common-sense knowledge. The need
for computer programs to represent common-
sense knowledge has been recognized since the
work of McCarthy (1959). Lenat et al. (1990)
defines common sense as “human consensus re-
ality knowledge: the facts and concepts that you
and I know and which we each assume the other
knows.”
While we are the first to exploit common-
sense knowledge in user characterization, com-
mon sense has been applied to a range of other
problems in natural language processing. In many
ways WordNet can be regarded as a collection of
common-sense relationships. WordNet has been
applied in a myriad of NLP applications, includ-
ing in seminal works on semantic-role labeling
(Gildea and Jurafsky, 2002), coreference resolu-
tion (Soon et al., 2001) and spelling correction
(Budanitsky and Hirst, 2006). Also, many ap-
proaches to the task of sentiment analysis “be-
gin with a large lexicon of words marked with
their prior polarity” (Wilson et al., 2009). Like
our class-attribute associations, the common-sense
knowledge that the word cool is positive while
unethical is negative can be learned from asso-
ciations in web-scale data (Turney, 2002). We
might also view information about synonyms or
conceptually-similar words as a kind of common-
sense knowledge. In this perspective, our work
is related to recent work that has extracted
distributionally-similar words from web-scale data
and applied this knowledge in tasks such as
named-entity recognition (Lin and Wu, 2009) and
dependency parsing (T¨ackstr¨om et al., 2012).
</bodyText>
<sectionHeader confidence="0.997374" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999884421052632">
We have proposed, developed and successfully
evaluated a novel approach to user characteriza-
tion based on exploiting knowledge of user class
attributes. The knowledge is obtained using a new
algorithm that discovers distinguishing attributes
of particular classes. Our approach to discovering
distinguishing attributes represents a significant
new direction for research in class-attribute extrac-
tion, and provides a valuable bridge between the
fields of user characterization and lexical knowl-
edge extraction.
We presented three effective techniques for
leveraging this knowledge within the framework
of supervised user characterization: rule-based
post-processing, a learning-by-bootstrapping ap-
proach, and a stacking approach that integrates the
predictions of the bootstrapped system into a sys-
tem trained on annotated gold-standard training
data. All techniques lead to significant improve-
</bodyText>
<page confidence="0.986849">
717
</page>
<bodyText confidence="0.9998383">
ments over state-of-the-art supervised systems on
the task of Twitter gender classification.
While our technique has advanced the state-of-
the-art on this important task, our approach may
prove even more useful on other tasks where train-
ing on thousands of gold-standard examples is not
even an option. Currently we are exploring the
prediction of finer-grained user roles, such as stu-
dent, waitress, parent, and so forth, based on ex-
tensions to the process laid out here.
</bodyText>
<sectionHeader confidence="0.997944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995440241758242">
Enrique Alfonseca, Marius Pas¸ca, and Enrique
Robledo-Arnuncio. 2010. Acquisition of instance
attributes via labeled and related instances. In Proc.
SIGIR, pages 58–65.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An
evaluation. In Proc. EMNLP, pages 158–165.
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2007. Lightly-Supervised
Attribute Extraction. In NIPS Workshop on Machine
Learning for Web Search.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proc. Coling-
ACL, pages 33–40.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65–74.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly improving user classification via
communication-based name and location clustering
on twitter. In Proc. NAACL.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proc. ACL, pages
57–64.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
John D. Burger and John C. Henderson. 2006. An
exploration of observable features related to blogger
age. In Proc. AAAI Spring Symposium: Computa-
tional Approaches to Analyzing Weblogs, pages 15–
20.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proc. EMNLP, pages 1301–1309.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,
Eugene Wu, and Yang Zhang. 2008. WebTables:
exploring the power of tables on the web. Proc.
PVLDB, 1(1):538–549.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1).
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365–1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. J. Mach.
Learn. Res., 9:1871–1874.
Clayton Fink, Jonathon Kopecky, Nathan Bos, and
Max Thomas. 2012. Mapping the Twitterverse in
the developing world: An analysis of social media
use in Nigeria. In Proc. International Conference on
Social Computing, Behavioral Modeling, and Pre-
diction, pages 164–171.
John L. Fischer. 1968. Social influences on the choice
of a linguistic variant. Word, 14:47–56.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proc. ACL-IJCNLP, pages 710–718.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245–288.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. Coling,
pages 539–545.
Emre Kiciman. 2010. Language differences and meta-
data features on Twitter. In Proc. SIGIR 2010 Web
N-gram Workshop, pages 47–51.
Moshe Koppel and Jonathan Schler. 2004. Authorship
verification as a one-class classification problem. In
Proc. ICML, pages 489–495.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401–412.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author’s native language by
mining a text for errors. In Proc. KDD, pages 624–
628.
</reference>
<page confidence="0.987828">
718
</page>
<reference confidence="0.999874152380952">
William Labov. 1972. Sociolinguistic Patterns. Uni-
versity of Pennsylvania Press.
Douglas B. Lenat, R. V. Guha, Karen Pittman, Dex-
ter Pratt, and Mary Shepherd. 1990. CYC: toward
programs with common sense. Commun. ACM,
33(8):30–49.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030–1038.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In Proc. LREC, pages 2221–
2227.
John McCarthy. 1959. Programs with common sense.
In Proc. Teddington Conference on the Mechaniza-
tion of Thought Processes, pages 75–91. London:
Her Majesty’s Stationery Office.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547–
559.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: an on-line lexical
database. International Journal of Lexicography,
3(4).
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proc. EMNLP,
pages 207–217.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proc. ICWSM, pages
122–129.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. Coling-
ACL, pages 113–120.
Marius Pas¸ca and Benjamin Van Durme. 2007. What
you seek is what you get: extraction of class at-
tributes from query logs. In Proc. IJCAI, pages
2832–2837.
Marius Pas¸ca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. ACL-08: HLT, pages 19–27.
Michael Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proc. ICWSM, pages 265–272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to Twitter user classi-
fication. In Proc. ICWSM, pages 281–288.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Work-
shop on Search and Mining User-Generated Con-
tents, pages 37–44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hi-
erarchical bayesian models for latent attribute detec-
tion in social media. In Proc. ICWSM, pages 598–
601.
Joseph Reisinger and Marius Pas¸ca. 2009. Latent
variable models of concept-attribute attachment. In
Proc. ACL-IJCNLP, pages 620–628.
Stephen D. Richardson, William B. Dolan, and Lucy
Vanderwende. 1998. MindNet: Acquiring and
structuring semantic information from text. In Proc.
ACL-Coling, pages 1098–1102.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199–205.
Lenhart Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. HLT, pages 84–87.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34:1–47.
Ian Soboroff, Dean McCullough, Jimmy Lin, Craig
Macdonald, Iadh Ounis, and Richard McCreadie.
2012. Evaluating real-time search over tweets. In
Proc. ICWSM.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4).
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. NAACL-
HLT, pages 477–487.
Kosuke Tokunaga, Jun’ichi Kazama, and Kentaro Tori-
sawa. 2005. Automatic discovery of attribute words
from web documents. In Proc. IJCNLP, pages 106–
118.
</reference>
<page confidence="0.983225">
719
</page>
<reference confidence="0.9988767">
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proc. ACL, pages 417–424.
Benjamin Van Durme, Ting Qian, and Lenhart Schu-
bert. 2008. Class-driven attribute extraction. In
Proc. Coling, pages 921–928.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proc. EMNLP-CoNLL,
pages 48–58.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics., 35(3):399–433.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In Proc. CIKM, pages 41–
50.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using “annotator rationales” to improve ma-
chine learning for text categorization. In Proc.
NAACL-HLT.
</reference>
<page confidence="0.996807">
720
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726303">
<title confidence="0.999851">Using Conceptual Class Attributes to Characterize Social Media Users</title>
<author confidence="0.999998">Shane Bergsma</author>
<author confidence="0.999998">Benjamin Van</author>
<affiliation confidence="0.8668995">Department of Computer Science and Human Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.99945">Baltimore, MD 21218, USA</address>
<abstract confidence="0.9993804">We describe a novel approach for automatically predicting the hidden demographic properties of social media users. Building on prior work in common-sense knowledge acquisition from third-person text, we first learn the distinguishing attributes of certain classes of people. For examwe learn that people in the tend to have names en- We then show that this knowledge can be used in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Marius Pas¸ca</author>
<author>Enrique Robledo-Arnuncio</author>
</authors>
<title>Acquisition of instance attributes via labeled and related instances.</title>
<date>2010</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>58--65</pages>
<marker>Alfonseca, Pas¸ca, Robledo-Arnuncio, 2010</marker>
<rawString>Enrique Alfonseca, Marius Pas¸ca, and Enrique Robledo-Arnuncio. 2010. Acquisition of instance attributes via labeled and related instances. In Proc. SIGIR, pages 58–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdulrahman Almuhareb</author>
<author>Massimo Poesio</author>
</authors>
<title>Attribute-based and value-based clustering: An evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>158--165</pages>
<contexts>
<context position="2511" citStr="Almuhareb and Poesio, 2004" startWordPosition="384" endWordPosition="387"> using supervised learning (Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classification task: predicting user gender (Section</context>
<context position="8857" citStr="Almuhareb and Poesio, 2004" startWordPosition="1382" endWordPosition="1385">s the filler, i.e. the extracted attribute, A. The notation “{word=.*}* tag=N.*” means that A can be any sequence of tokens ending in a noun. We use an {word=I,tag=NN} � Y � instance {word=’s} � Y � ’s 711 equivalent pattern when I is multi-token. The output of this process is a set of (I,A) pairs. In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004). We obtain the best of both worlds by matching our precise pattern against a version of the Google Ngram Corpus that includes the part-of-speech tag distributions for every N-gram (Lin et al., 2010). We found that applying this pattern to web-scale data is effective in extracting useful attributes. We acquired around 20,000 attributes in total. Finding Distinguishing Attributes Unlike prior work, we aim to find distinguishing properties of each class; that is, the kinds of properties that uniquely distinguish a particular category. Prior work has mostly focused on finding “relevant” attribute</context>
<context position="14500" citStr="Almuhareb and Poesio (2004)" startWordPosition="2282" endWordPosition="2285">token attributes; these are often distinguishing even though the head noun is ambiguous (e.g. name is ambiguous, maiden name is not). Our attributes also go beyond the traditional meronyms that were the target of earlier work. As we discuss further in Related Work (Section 7), previous researchers have worried about a proper definition of parts or attributes and relied on human judgments for evaluation (Berland and Charniak, 1999; Girju et al., 2006; Van Durme et al., 2008). For us, whether a property such as dowry should be considered an “attribute” of the class Female is immaterial; we echo Almuhareb and Poesio (2004) who (on a different task) noted that “while the notion of ‘attribute’ is not completely clear... our results suggest that trying to identify attributes is beneficial.” 4 Applying Class Attributes To classify users using the extracted attributes, we look for cases where users refer to such attributes in their first-person writings. We performed a preliminary analysis of a two-week sample of tweets from the TREC Tweets2011 Corpus.3 We found that users most often reveal their attributes in the possessive construction, “my X” where X is an attribute, quality or event that they possess (in a lingu</context>
<context position="30958" citStr="Almuhareb and Poesio (2004)" startWordPosition="4880" endWordPosition="4883">iller et al., 1990). Indeed, Berland and Charniak (1999) attempted to filter out attributes that were regarded as qualities (like driveability) rather than parts (like steering wheels) by removing words ending with the suffixes -ness, -ing, and -ity. In our work, such qualities are not filtered and are ultimately valuable in classification; for example, the attributes peak fertility and loveliness are highly 90 85 80 75 70 65 60 BoW+Usr BoW Usr Accuracy 716 associated with females. As subsequent research became more focused on applications, looser definitions of class attributes were adopted. Almuhareb and Poesio (2004) automatically mined class attributes that include parts, qualities, and those with an “agentive” or “telic” role with the class. Their extended set of attributes was shown to enable an improved representation of nouns for the purpose of clustering these nouns into semantic concepts. Tokunaga et al. (2005) define attributes as properties that can serve as focus words in questions about a target class; e.g. director is an attribute of a movie since one might ask, “Who is the director of this movie?” Another line of research has been motivated by the observation that much of Internet search cons</context>
</contexts>
<marker>Almuhareb, Poesio, 2004</marker>
<rawString>Abdulrahman Almuhareb and Massimo Poesio. 2004. Attribute-based and value-based clustering: An evaluation. In Proc. EMNLP, pages 158–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Partha P Talukdar</author>
<author>Giridhar Kumaran</author>
<author>Fernando Pereira</author>
<author>Mark Liberman</author>
<author>Andrew McCallum</author>
<author>Mark Dredze</author>
</authors>
<title>Lightly-Supervised Attribute Extraction.</title>
<date>2007</date>
<booktitle>In NIPS Workshop on Machine Learning</booktitle>
<institution>for Web Search.</institution>
<contexts>
<context position="31641" citStr="Bellare et al., 2007" startWordPosition="4996" endWordPosition="4999">ties, and those with an “agentive” or “telic” role with the class. Their extended set of attributes was shown to enable an improved representation of nouns for the purpose of clustering these nouns into semantic concepts. Tokunaga et al. (2005) define attributes as properties that can serve as focus words in questions about a target class; e.g. director is an attribute of a movie since one might ask, “Who is the director of this movie?” Another line of research has been motivated by the observation that much of Internet search consists of people looking for values of various class attributes (Bellare et al., 2007; Pas¸ca and Van Durme, 2007; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). By knowing the attributes of different classes, search engines can better recognize that queries such as “altitude guadalajara” or “population guadalajara” are seeking values for a particular city’s “altitude” and “population” attributes (Pas¸ca and Van Durme, 2007). Finally, note that Van Durme et al. (2008) compared instance-based and class-based patterns for broad-definition attribute extraction, and found both to be effective. Of course, text-mining with custom-designed patterns is not the only way to extrac</context>
</contexts>
<marker>Bellare, Talukdar, Kumaran, Pereira, Liberman, McCallum, Dredze, 2007</marker>
<rawString>Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran, Fernando Pereira, Mark Liberman, Andrew McCallum, and Mark Dredze. 2007. Lightly-Supervised Attribute Extraction. In NIPS Workshop on Machine Learning for Web Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In Proc. ColingACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="6736" citStr="Bergsma and Lin (2006)" startWordPosition="1035" endWordPosition="1038"> are typically collected from publicly-available resources such as WordNet or Wikipedia (Pas¸ca and Van Durme, 2007; Van Durme et al., 2008), acquired automatically from corpora (Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010), or simply specified by hand (Schubert, 2002). Creation of Instance Lists We use an instancebased approach; our instances are derived from collections of common nouns that are associated with roles and occupations of people. For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by Bergsma and Lin (2006). This dataset indicates how often a noun is referenced by a male, female, neutral or plural pronoun. We extract prevalent common nouns for males and females by selecting only those nouns that (a) occur more than 200 times in the dataset, (b) mostly occur with male or female pronouns, and (c) occur as lower-case more often than upper-case in a web-scale N-gram corpus (Lin et al., 2010). We then classify a noun as Male (resp. Female) if the noun is indicated to occur with male (resp. female) pronouns at least 85% of the time. Since the gender data is noisy, we also quickly pruned by hand any in</context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In Proc. ColingACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clayton Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific Twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>65--74</pages>
<contexts>
<context position="20167" citStr="Bergsma et al. (2012)" startWordPosition="3181" endWordPosition="3184">standard training data is not available. Gold Data Our data is derived from the corpus created by Burger et al. (2011). Burger et al. observed that many Twitter users link their Twitter profile to homepages on popular blogging websites. Since “many of these [sites] have wellstructured profile pages [where users] must select gender and other attributes from dropdown menus,” they were able to link these attributes to the Twitter users. Using this process, they created a large multi-lingual corpus of Twitter users and genders. We filter non-English tweets from this corpus using the LID system of Bergsma et al. (2012) and also tweets containing URLs (since many of these are spam) and re-tweets. We then filter users with &lt;40 tweets and randomly divide the remaining users into 2282 training, 1140 development, and 1141 test examples. Classifier Set-up We train logistic-regression classifiers on this gold standard data via the LIBLINEAR package (Fan et al., 2008). We optimize the classifier’s regularization parameter on development data and report final results on the heldout test examples. We also report the results of our new attribute-based strategies (Section 4) on the test data. We report accuracy: the pe</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identification for creating language-specific Twitter collections. In Proceedings of the Second Workshop on Language in Social Media, pages 65–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Broadly improving user classification via communication-based name and location clustering on twitter.</title>
<date>2013</date>
<booktitle>In Proc. NAACL.</booktitle>
<marker>Bergsma, Dredze, Van Durme, Wilson, Yarowsky, 2013</marker>
<rawString>Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, and David Yarowsky. 2013. Broadly improving user classification via communication-based name and location clustering on twitter. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="2467" citStr="Berland and Charniak, 1999" startWordPosition="378" endWordPosition="381"> a classification task and train classifiers using supervised learning (Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classifi</context>
<context position="5522" citStr="Berland and Charniak (1999)" startWordPosition="832" endWordPosition="835">tes, such as chat rooms and news article comment streams” (Burger et al., 2011). We refer to features derived from a username as Usr features in our experiments. 3 Learning Class Attributes We aim to improve the automated classification of users into various demographic categories by learning and applying the distinguishing attributes of those categories, e.g. that males have boyhood heroes. Our approach builds on lexical-semantic research on the topic of class-attribute extraction. In this research, the objective is to discover various attributes or parts of classes of entities. For example, Berland and Charniak (1999) learn that the class car has parts such as headlight, windshield, dashboard, etc. Berland and Charniak extract these attributes by mining a corpus for fillers of patterns such as ‘car’s X’ or ‘X of a car’. Note their patterns explicitly include the class itself (car). Another approach is to use patterns that are based on instances (i.e. hyponyms or sub-classes) of the class. For example, Pas¸ca and Van Durme (2007) learn the attributes of the class car via patterns involving instances of cars, e.g. Chevrolet Corvette’s X and X of a Honda Civic. For these approaches, lists of instances are typ</context>
<context position="8723" citStr="Berland and Charniak, 1999" startWordPosition="1362" endWordPosition="1365">.g. dictator ’s [former mistress]). The expression “tag=NN” means that I must be tagged as a noun. The expression in square brackets is the filler, i.e. the extracted attribute, A. The notation “{word=.*}* tag=N.*” means that A can be any sequence of tokens ending in a noun. We use an {word=I,tag=NN} � Y � instance {word=’s} � Y � ’s 711 equivalent pattern when I is multi-token. The output of this process is a set of (I,A) pairs. In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004). We obtain the best of both worlds by matching our precise pattern against a version of the Google Ngram Corpus that includes the part-of-speech tag distributions for every N-gram (Lin et al., 2010). We found that applying this pattern to web-scale data is effective in extracting useful attributes. We acquired around 20,000 attributes in total. Finding Distinguishing Attributes Unlike prior work, we aim to find distinguishing properties of each class; that is, </context>
<context position="14306" citStr="Berland and Charniak, 1999" startWordPosition="2248" endWordPosition="2251">, necklace, ex-husband, ex-boyfriend, dowry, nightgown Table 2: Example attributes for gender classes, in descending order of class-association score tributes. Our approach captures many multi-token attributes; these are often distinguishing even though the head noun is ambiguous (e.g. name is ambiguous, maiden name is not). Our attributes also go beyond the traditional meronyms that were the target of earlier work. As we discuss further in Related Work (Section 7), previous researchers have worried about a proper definition of parts or attributes and relied on human judgments for evaluation (Berland and Charniak, 1999; Girju et al., 2006; Van Durme et al., 2008). For us, whether a property such as dowry should be considered an “attribute” of the class Female is immaterial; we echo Almuhareb and Poesio (2004) who (on a different task) noted that “while the notion of ‘attribute’ is not completely clear... our results suggest that trying to identify attributes is beneficial.” 4 Applying Class Attributes To classify users using the extracted attributes, we look for cases where users refer to such attributes in their first-person writings. We performed a preliminary analysis of a two-week sample of tweets from </context>
<context position="29907" citStr="Berland and Charniak (1999)" startWordPosition="4718" endWordPosition="4721"> social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output.” Girju et al. (2006) devised a supervised classification scheme for part/whole relation discovery that integrates the evidence from multiple patterns. These efforts focused exclusively on the meronymy relation as used in WordNet (Miller et al., 1990). Indeed, Berland and Charniak (1999) attempted to filter out attributes that were regarded as qualities (like driveability) rather than parts (like steering</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proc. ACL, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="33793" citStr="Budanitsky and Hirst, 2006" startWordPosition="5323" endWordPosition="5326">mon sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this k</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
</authors>
<title>An exploration of observable features related to blogger age.</title>
<date>2006</date>
<booktitle>In Proc. AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</booktitle>
<pages>15--20</pages>
<contexts>
<context position="1245" citStr="Burger and Henderson, 2006" startWordPosition="184" endWordPosition="187">ngs. We then show that this knowledge can be used in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a c</context>
</contexts>
<marker>Burger, Henderson, 2006</marker>
<rawString>John D. Burger and John C. Henderson. 2006. An exploration of observable features related to blogger age. In Proc. AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, pages 15– 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on Twitter.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1301--1309</pages>
<contexts>
<context position="1363" citStr="Burger et al., 2011" startWordPosition="205" endWordPosition="208">attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a classification task and train classifiers using supervised learning (Section 2). The features of the classifier are ind</context>
<context position="4022" citStr="Burger et al., 2011" startWordPosition="601" endWordPosition="604">n is to use supervised classifiers trained on annotated data. For each instance to be classified, the output is a decision about a distinct demographic property, such as Male/Female or Over/Under-18. A variety of classification algorithms have been employed, including SVMs (Rao et al., 2010), de710 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710–720, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cision trees (Pennacchiotti and Popescu, 2011), logistic regression (Van Durme, 2012), and the Winnow algorithm (Burger et al., 2011). Content Features: BoW Prior classifiers use a set of features encoding the presence of specific words in the user-generated text. We call these features BoW features as they encode the standard Bag-of-Words representation which has been highly effective in text categorization and information retrieval (Sebastiani, 2002). User-Profile Features: Usr Some researchers have explored features for user-profile metainformation in addition to user content. This may include the user’s communication behavior and network of contacts (Rao et al., 2010), their full name (Burger et al., 2011) and whether t</context>
<context position="19241" citStr="Burger et al., 2011" startWordPosition="3036" endWordPosition="3039">ns on the entire set of gold standard data (gold train, development, and test sets). We then use these predictions as features in a classifier trained on the gold standard data. We refer to this system as the BootStacked system in our evaluation. 5 Twitter Gender Prediction To test the use of self-distinguishing attributes in user classification, we apply our methods to the task of gender classification on Twitter. This is an important and intensely-studied task within academia and industry. Furthermore, for this task it is possible to semi-automatically acquire large amounts of ground truth (Burger et al., 2011). We can therefore benchmark our approach against state-of-the-art supervised systems trained with plentiful gold-standard data, giving us an idea of how well our Bootstrapped system might compare to theoretically top-performing systems on other tasks, domains, and social media platforms where such gold-standard training data is not available. Gold Data Our data is derived from the corpus created by Burger et al. (2011). Burger et al. observed that many Twitter users link their Twitter profile to homepages on popular blogging websites. Since “many of these [sites] have wellstructured profile p</context>
<context position="21117" citStr="Burger et al. (2011)" startWordPosition="3336" endWordPosition="3339">(Fan et al., 2008). We optimize the classifier’s regularization parameter on development data and report final results on the heldout test examples. We also report the results of our new attribute-based strategies (Section 4) on the test data. We report accuracy: the percentage of examples labeled correctly. Our classifiers use both BoW and Usr features (Section 2). To increase the generality of our BoW features, we preprocess the text by lowercasing and converting all digits to special ‘#’ symbols. We then create real-valued features that encode the log-count of each word in the input. While Burger et al. (2011) found “no appreciable difference in performance” when using either binary presence/absence features or encoding the frequency of the word, we found real-valued features worked better in development experiments. For the Usr features, we add special beginning and ending characters to the username, and then create features for all character n-grams of length twoto-four in the modified username string. We include n-gram features with the original capitalization pattern and separate features with the n-grams lower-cased. Unlabeled Data For Bootstrapped training, we also use a pool of unlabeled Twi</context>
<context position="24340" citStr="Burger et al. (2011)" startWordPosition="3851" endWordPosition="3854"> gold standard test data for user gender prediction on Twitter 6 Results Our main classification results are presented in Table 3. The majority-class baseline for this task is to always choose Female; this achieves an accuracy of 60.9%. A standard classifier trained on 100 gold-standard training examples improves over this baseline, to 72.0%, while one with 2282 training examples achieves 84.0%. This latter result represents the current state-of-the-art: a classifier trained on thousands of gold standard examples, making use of both Usr and BoW features. Our performance compares favourably to Burger et al. (2011), who achieved 81.4% using the same features, but on a very different subset of the data (also including tweets in other languages).7 Applying the ARules as a post-process significantly improves performance in both cases (McNemar’s, p&lt;0.05). It is also possible to use the ARules as a stand-alone system rather than as a post-process, however the coverage is low: we find a distinguishing attribute in 18.3% of the 695 Female instances in the test data, and make the cor7Note that it is possible to achieve even higher performance on gender classification in social media if you have further informat</context>
<context position="29582" citStr="Burger et al., 2011" startWordPosition="4670" endWordPosition="4673">e Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output.” Girju et al. (2006) devised a supervised classification scheme for part/whole rel</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In Proc. EMNLP, pages 1301–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Cafarella</author>
<author>Alon Y Halevy</author>
<author>Daisy Zhe Wang</author>
<author>Eugene Wu</author>
<author>Yang Zhang</author>
</authors>
<title>WebTables: exploring the power of tables on the web.</title>
<date>2008</date>
<booktitle>Proc. PVLDB,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="32648" citStr="Cafarella et al., 2008" startWordPosition="5144" endWordPosition="5147">l. (2008) compared instance-based and class-based patterns for broad-definition attribute extraction, and found both to be effective. Of course, text-mining with custom-designed patterns is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you a</context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: exploring the power of tables on the web. Proc. PVLDB, 1(1):538–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="10166" citStr="Church and Hanks, 1990" startWordPosition="1594" endWordPosition="1597"> relevant and correct part of both a male and a female (and many other living and inanimate objects), but it does not help us distinguish males from females in social media. We therefore rank our attributes for each class by their strength of association with instances of that specific class.1 To calculate the association, we first disregard the count of each (I,A) pair and consider each unique pair to be a single probabilistic event. We then convert the (I,A) pairs to corresponding (C,A) pairs by replacing I with the corresponding class, C. We then calculate the pointwise mutual information (Church and Hanks, 1990) between each C and A over the set of events: PMI(C, A) = log p(C, A) (1) p(C)p(A) If the PMI&gt;0, the observed probability of a class and attribute co-occurring is greater than the probability of co-occurrence that we would expect if C and A were independently distributed. For each class, we rank the attributes by their PMI scores. 1Reisinger and Pas¸ca (2009) considered the related problem of finding the most appropriate class for each attribute; they take an existing ontology of concepts (WordNet) as a class hierarchy and use a Bayesian approach to decide “the correct level of abstraction for</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1365--1374</pages>
<contexts>
<context position="29360" citStr="Eisenstein et al., 2011" startWordPosition="4633" endWordPosition="4636"> dog, lord, sir, omar, dude, man, big Female BoW features: hubby, hubs, jewelry, sewing, mascara, fabulous, bf, softball, betcha, motherhood, perky, cozy, zumba, xox, cuddled, belieber, bridesmaid, anorexic, jammies, pad Female Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proc. ACL, pages 1365–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<contexts>
<context position="20515" citStr="Fan et al., 2008" startWordPosition="3237" endWordPosition="3240">s from dropdown menus,” they were able to link these attributes to the Twitter users. Using this process, they created a large multi-lingual corpus of Twitter users and genders. We filter non-English tweets from this corpus using the LID system of Bergsma et al. (2012) and also tweets containing URLs (since many of these are spam) and re-tweets. We then filter users with &lt;40 tweets and randomly divide the remaining users into 2282 training, 1140 development, and 1141 test examples. Classifier Set-up We train logistic-regression classifiers on this gold standard data via the LIBLINEAR package (Fan et al., 2008). We optimize the classifier’s regularization parameter on development data and report final results on the heldout test examples. We also report the results of our new attribute-based strategies (Section 4) on the test data. We report accuracy: the percentage of examples labeled correctly. Our classifiers use both BoW and Usr features (Section 2). To increase the generality of our BoW features, we preprocess the text by lowercasing and converting all digits to special ‘#’ symbols. We then create real-valued features that encode the log-count of each word in the input. While Burger et al. (201</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clayton Fink</author>
<author>Jonathon Kopecky</author>
<author>Nathan Bos</author>
<author>Max Thomas</author>
</authors>
<title>Mapping the Twitterverse in the developing world: An analysis of social media use in Nigeria.</title>
<date>2012</date>
<booktitle>In Proc. International Conference on Social Computing, Behavioral Modeling, and Prediction,</booktitle>
<pages>164--171</pages>
<contexts>
<context position="29431" citStr="Fink et al., 2012" startWordPosition="4645" endWordPosition="4648">lry, sewing, mascara, fabulous, bf, softball, betcha, motherhood, perky, cozy, zumba, xox, cuddled, belieber, bridesmaid, anorexic, jammies, pad Female Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and </context>
</contexts>
<marker>Fink, Kopecky, Bos, Thomas, 2012</marker>
<rawString>Clayton Fink, Jonathon Kopecky, Nathan Bos, and Max Thomas. 2012. Mapping the Twitterverse in the developing world: An analysis of social media use in Nigeria. In Proc. International Conference on Social Computing, Behavioral Modeling, and Prediction, pages 164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John L Fischer</author>
</authors>
<title>Social influences on the choice of a linguistic variant.</title>
<date>1968</date>
<journal>Word,</journal>
<pages>14--47</pages>
<contexts>
<context position="27470" citStr="Fischer, 1968" startWordPosition="4348" endWordPosition="4349">mber of auto-annotated training pts. Figure 2: Learning curve for Bootstrapped logistic-regression classifier, with automaticallylabeled data, for different feature classes. features capture reasonable associations between gender classes and particular names (such as mike, tony, omar, etc.) and also between gender classes and common nouns (such as guy, dad, sir, etc.). 7 Related Work User Characterization The field of sociolinguistics has long been concerned with how various morphological, phonological and stylistic aspects of language can vary with a person’s age, gender, social class, etc. (Fischer, 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera</context>
</contexts>
<marker>Fischer, 1968</marker>
<rawString>John L. Fischer. 1968. Social influences on the choice of a linguistic variant. Word, 14:47–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="28089" citStr="Garera and Yarowsky, 2009" startWordPosition="4439" endWordPosition="4442">, 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in this study - such as author selfidentification.” Male BoW features: wife, wifey, sucked, shave, boner, boxers, missus, installed, manly, in-laws, brah, urinal, kickoff, golf, comics, ubuntu, homo, nhl, jedi, fatherhood, nigga, movember, algebra Male Usr featur</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proc. ACL-IJCNLP, pages 710–718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--245</pages>
<contexts>
<context position="33696" citStr="Gildea and Jurafsky, 2002" startWordPosition="5309" endWordPosition="5312">knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to rec</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2554" citStr="Girju et al., 2006" startWordPosition="392" endWordPosition="395">s of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classification task: predicting user gender (Section 5). Our bootstrapped system, trained purel</context>
<context position="14326" citStr="Girju et al., 2006" startWordPosition="2252" endWordPosition="2255">oyfriend, dowry, nightgown Table 2: Example attributes for gender classes, in descending order of class-association score tributes. Our approach captures many multi-token attributes; these are often distinguishing even though the head noun is ambiguous (e.g. name is ambiguous, maiden name is not). Our attributes also go beyond the traditional meronyms that were the target of earlier work. As we discuss further in Related Work (Section 7), previous researchers have worried about a proper definition of parts or attributes and relied on human judgments for evaluation (Berland and Charniak, 1999; Girju et al., 2006; Van Durme et al., 2008). For us, whether a property such as dowry should be considered an “attribute” of the class Female is immaterial; we echo Almuhareb and Poesio (2004) who (on a different task) noted that “while the notion of ‘attribute’ is not completely clear... our results suggest that trying to identify attributes is beneficial.” 4 Applying Class Attributes To classify users using the extracted attributes, we look for cases where users refer to such attributes in their first-person writings. We performed a preliminary analysis of a two-week sample of tweets from the TREC Tweets2011 </context>
<context position="30120" citStr="Girju et al. (2006)" startWordPosition="4752" endWordPosition="4755">ennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output.” Girju et al. (2006) devised a supervised classification scheme for part/whole relation discovery that integrates the evidence from multiple patterns. These efforts focused exclusively on the meronymy relation as used in WordNet (Miller et al., 1990). Indeed, Berland and Charniak (1999) attempted to filter out attributes that were regarded as qualities (like driveability) rather than parts (like steering wheels) by removing words ending with the suffixes -ness, -ing, and -ity. In our work, such qualities are not filtered and are ultimately valuable in classification; for example, the attributes peak fertility and</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. Coling,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="29726" citStr="Hearst (1992)" startWordPosition="4693" endWordPosition="4694"> (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output.” Girju et al. (2006) devised a supervised classification scheme for part/whole relation discovery that integrates the evidence from multiple patterns. These efforts focused exclusively on the meronymy relation as used in WordN</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. Coling, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emre Kiciman</author>
</authors>
<title>Language differences and metadata features on Twitter.</title>
<date>2010</date>
<booktitle>In Proc. SIGIR 2010 Web N-gram Workshop,</booktitle>
<pages>47--51</pages>
<contexts>
<context position="29457" citStr="Kiciman, 2010" startWordPosition="4651" endWordPosition="4652">, bf, softball, betcha, motherhood, perky, cozy, zumba, xox, cuddled, belieber, bridesmaid, anorexic, jammies, pad Female Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined st</context>
</contexts>
<marker>Kiciman, 2010</marker>
<rawString>Emre Kiciman. 2010. Language differences and metadata features on Twitter. In Proc. SIGIR 2010 Web N-gram Workshop, pages 47–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>Authorship verification as a one-class classification problem.</title>
<date>2004</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>489--495</pages>
<contexts>
<context position="18393" citStr="Koppel and Schler, 2004" startWordPosition="2902" endWordPosition="2905">ne this data with any goldstandard training examples to achieve even better performance. We use the following simple but 5While we used an “off the shelf” POS tagger in this work, we note that taggers optimized specifically for social media are now available and would likely have resulted in higher tagging accuracy (e.g. Owoputi et al. (2013)). 6Note that while our target gender task presents mutuallyexclusive output classes, we can still train classifiers for other categories without clear opposites (e.g. for labeling users as Parents or Doctors) by using the 1-class classification paradigm (Koppel and Schler, 2004). 713 effective method for combining data from these two sources, inspired by prior techniques used in the domain adaptation literature (Daum´e III and Marcu, 2006). We first use the trained Bootstrapped system to make predictions on the entire set of gold standard data (gold train, development, and test sets). We then use these predictions as features in a classifier trained on the gold standard data. We refer to this system as the BootStacked system in our evaluation. 5 Twitter Gender Prediction To test the use of self-distinguishing attributes in user classification, we apply our methods to</context>
</contexts>
<marker>Koppel, Schler, 2004</marker>
<rawString>Moshe Koppel and Jonathan Schler. 2004. Authorship verification as a one-class classification problem. In Proc. ICML, pages 489–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
</authors>
<title>Shlomo Argamon, and Anat Rachel Shimoni.</title>
<date>2002</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--4</pages>
<marker>Koppel, 2002</marker>
<rawString>Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2002. Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 17(4):401–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>In Proc. KDD,</booktitle>
<pages>624--628</pages>
<contexts>
<context position="27829" citStr="Koppel et al., 2005" startWordPosition="4402" endWordPosition="4405">y, dad, sir, etc.). 7 Related Work User Characterization The field of sociolinguistics has long been concerned with how various morphological, phonological and stylistic aspects of language can vary with a person’s age, gender, social class, etc. (Fischer, 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Determining an author’s native language by mining a text for errors. In Proc. KDD, pages 624– 628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>Sociolinguistic Patterns.</title>
<date>1972</date>
<publisher>University of Pennsylvania Press.</publisher>
<contexts>
<context position="27484" citStr="Labov, 1972" startWordPosition="4350" endWordPosition="4351">notated training pts. Figure 2: Learning curve for Bootstrapped logistic-regression classifier, with automaticallylabeled data, for different feature classes. features capture reasonable associations between gender classes and particular names (such as mike, tony, omar, etc.) and also between gender classes and common nouns (such as guy, dad, sir, etc.). 7 Related Work User Characterization The field of sociolinguistics has long been concerned with how various morphological, phonological and stylistic aspects of language can vary with a person’s age, gender, social class, etc. (Fischer, 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky,</context>
</contexts>
<marker>Labov, 1972</marker>
<rawString>William Labov. 1972. Sociolinguistic Patterns. University of Pennsylvania Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
<author>R V Guha</author>
<author>Karen Pittman</author>
<author>Dexter Pratt</author>
<author>Mary Shepherd</author>
</authors>
<title>CYC: toward programs with common sense.</title>
<date>1990</date>
<journal>Commun. ACM,</journal>
<volume>33</volume>
<issue>8</issue>
<contexts>
<context position="33154" citStr="Lenat et al. (1990)" startWordPosition="5219" endWordPosition="5222">oboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling </context>
</contexts>
<marker>Lenat, Guha, Pittman, Pratt, Shepherd, 1990</marker>
<rawString>Douglas B. Lenat, R. V. Guha, Karen Pittman, Dexter Pratt, and Mary Shepherd. 1990. CYC: toward programs with common sense. Commun. ACM, 33(8):30–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP,</booktitle>
<pages>1030--1038</pages>
<contexts>
<context position="34462" citStr="Lin and Wu, 2009" startWordPosition="5427" endWordPosition="5430">lysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). 8 Conclusion We have proposed, developed and successfully evaluated a novel approach to user characterization based on exploiting knowledge of user class attributes. The knowledge is obtained using a new algorithm that discovers distinguishing attributes of particular classes. Our approach to discovering distinguishing attributes represents a significant new direction for research in class-attribute extraction, and provides a valuable bridge between the fields of user characterization and lexical knowledge extraction. We presented three effec</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proc. ACL-IJCNLP, pages 1030–1038.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale N-grams. In</title>
<date>2010</date>
<booktitle>Proc. LREC,</booktitle>
<pages>2221--2227</pages>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale.</institution>
<contexts>
<context position="7124" citStr="Lin et al., 2010" startWordPosition="1104" endWordPosition="1107">ouns that are associated with roles and occupations of people. For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by Bergsma and Lin (2006). This dataset indicates how often a noun is referenced by a male, female, neutral or plural pronoun. We extract prevalent common nouns for males and females by selecting only those nouns that (a) occur more than 200 times in the dataset, (b) mostly occur with male or female pronouns, and (c) occur as lower-case more often than upper-case in a web-scale N-gram corpus (Lin et al., 2010). We then classify a noun as Male (resp. Female) if the noun is indicated to occur with male (resp. female) pronouns at least 85% of the time. Since the gender data is noisy, we also quickly pruned by hand any instances that were malformed or obviously incorrectly assigned by our automatic process. This results in 652 instances in total. Table 1 provides some examples. Male: bouncer, altar boy, army officer, dictator, assailant, cameraman, drifter, chauffeur, bad guy Female: young lady, lesbian, ballerina, waitress, granny, chairwoman, heiress, soprano, socialite Table 1: Example instances use</context>
<context position="9056" citStr="Lin et al., 2010" startWordPosition="1416" endWordPosition="1419">ivalent pattern when I is multi-token. The output of this process is a set of (I,A) pairs. In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004). We obtain the best of both worlds by matching our precise pattern against a version of the Google Ngram Corpus that includes the part-of-speech tag distributions for every N-gram (Lin et al., 2010). We found that applying this pattern to web-scale data is effective in extracting useful attributes. We acquired around 20,000 attributes in total. Finding Distinguishing Attributes Unlike prior work, we aim to find distinguishing properties of each class; that is, the kinds of properties that uniquely distinguish a particular category. Prior work has mostly focused on finding “relevant” attributes (Alfonseca et al., 2010) or “correct” parts (Berland and Charniak, 1999). A leg is a relevant and correct part of both a male and a female (and many other living and inanimate objects), but it does</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale N-grams. In Proc. LREC, pages 2221– 2227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
</authors>
<title>Programs with common sense.</title>
<date>1959</date>
<booktitle>In Proc. Teddington Conference on the Mechanization of Thought Processes,</booktitle>
<pages>75--91</pages>
<institution>Her Majesty’s Stationery Office.</institution>
<location>London:</location>
<contexts>
<context position="33133" citStr="McCarthy (1959)" startWordPosition="5217" endWordPosition="5218"> as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al.</context>
</contexts>
<marker>McCarthy, 1959</marker>
<rawString>John McCarthy. 1959. Programs with common sense. In Proc. Teddington Conference on the Mechanization of Thought Processes, pages 75–91. London: Her Majesty’s Stationery Office.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>559</pages>
<contexts>
<context position="12554" citStr="McRae et al., 2005" startWordPosition="1978" endWordPosition="1981"> et al. (1998) that “automatic procedures ... provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning” but “hand vetting” might be needed to ensure “accuracy and consistency in production level systems.” Since our approach requires manual involvement in the filtering of the attribute list, one might argue that one should simply manually enumerate the most relevant attributes directly. However, the manual generation of conceptual features by a single researcher results in substantial variability both across and within participants (McRae et al., 2005). Psychologists therefore generate such lists by pooling the responses across many participants: future work may compare our “automatically generate, manually prune” approach to soliciting attributes via crowdsourcing.2 Table 2 gives examples of our extracted at2One can also view the work of manually filtering attributes as a kind of “feature labeling.” There is evidence from Zaidan et al. (2007) that a few hours of feature labeling can be more productive than annotating new training examples. In fact, since Zaidan et al. (2007) label features at the token level (e.g., in our case one would hi</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S. Cree, Mark S. Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547– 559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: an on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="30350" citStr="Miller et al., 1990" startWordPosition="4785" endWordPosition="4788"> focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output.” Girju et al. (2006) devised a supervised classification scheme for part/whole relation discovery that integrates the evidence from multiple patterns. These efforts focused exclusively on the meronymy relation as used in WordNet (Miller et al., 1990). Indeed, Berland and Charniak (1999) attempted to filter out attributes that were regarded as qualities (like driveability) rather than parts (like steering wheels) by removing words ending with the suffixes -ness, -ing, and -ity. In our work, such qualities are not filtered and are ultimately valuable in classification; for example, the attributes peak fertility and loveliness are highly 90 85 80 75 70 65 60 BoW+Usr BoW Usr Accuracy 716 associated with females. As subsequent research became more focused on applications, looser definitions of class attributes were adopted. Almuhareb and Poesi</context>
<context position="32375" citStr="Miller et al., 1990" startWordPosition="5105" endWordPosition="5108">fferent classes, search engines can better recognize that queries such as “altitude guadalajara” or “population guadalajara” are seeking values for a particular city’s “altitude” and “population” attributes (Pas¸ca and Van Durme, 2007). Finally, note that Van Durme et al. (2008) compared instance-based and class-based patterns for broad-definition attribute extraction, and found both to be effective. Of course, text-mining with custom-designed patterns is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred t</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: an on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Improving gender classification of blog authors.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>207--217</pages>
<contexts>
<context position="1309" citStr="Mukherjee and Liu, 2010" startWordPosition="196" endWordPosition="199">f first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a classification task and train classifiers using supervised learni</context>
<context position="28115" citStr="Mukherjee and Liu, 2010" startWordPosition="4443" endWordPosition="4446">early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in this study - such as author selfidentification.” Male BoW features: wife, wifey, sucked, shave, boner, boxers, missus, installed, manly, in-laws, brah, urinal, kickoff, golf, comics, ubuntu, homo, nhl, jedi, fatherhood, nigga, movember, algebra Male Usr features: boy, mike, ben, guy, m</context>
</contexts>
<marker>Mukherjee, Liu, 2010</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2010. Improving gender classification of blog authors. In Proc. EMNLP, pages 207–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proc. ICWSM,</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proc. ICWSM, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proc. ColingACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="32843" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="5171" endWordPosition="5175">is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems i</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: leveraging generic patterns for automatically harvesting semantic relations. In Proc. ColingACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>What you seek is what you get: extraction of class attributes from query logs.</title>
<date>2007</date>
<booktitle>In Proc. IJCAI,</booktitle>
<pages>2832--2837</pages>
<marker>Pas¸ca, Van Durme, 2007</marker>
<rawString>Marius Pas¸ca and Benjamin Van Durme. 2007. What you seek is what you get: extraction of class attributes from query logs. In Proc. IJCAI, pages 2832–2837.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT,</booktitle>
<pages>pages</pages>
<marker>Pas¸ca, Van Durme, 2008</marker>
<rawString>Marius Pas¸ca and Benjamin Van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proc. ACL-08: HLT, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Mark Dredze</author>
</authors>
<title>You are what you tweet: Analyzing Twitter for public health.</title>
<date>2011</date>
<booktitle>In Proc. ICWSM,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="1600" citStr="Paul and Dredze, 2011" startWordPosition="240" endWordPosition="243"> the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a classification task and train classifiers using supervised learning (Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We presen</context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>Michael Paul and Mark Dredze. 2011. You are what you tweet: Analyzing Twitter for public health. In Proc. ICWSM, pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>A machine learning approach to Twitter user classification.</title>
<date>2011</date>
<booktitle>In Proc. ICWSM,</booktitle>
<pages>281--288</pages>
<contexts>
<context position="1342" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="200" endWordPosition="204">ion; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a classification task and train classifiers using supervised learning (Section 2). The features of t</context>
<context position="3935" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="586" endWordPosition="589">g examples. 2 Supervised User Characterization The current state-of-the-art in user characterization is to use supervised classifiers trained on annotated data. For each instance to be classified, the output is a decision about a distinct demographic property, such as Male/Female or Over/Under-18. A variety of classification algorithms have been employed, including SVMs (Rao et al., 2010), de710 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710–720, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cision trees (Pennacchiotti and Popescu, 2011), logistic regression (Van Durme, 2012), and the Winnow algorithm (Burger et al., 2011). Content Features: BoW Prior classifiers use a set of features encoding the presence of specific words in the user-generated text. We call these features BoW features as they encode the standard Bag-of-Words representation which has been highly effective in text categorization and information retrieval (Sebastiani, 2002). User-Profile Features: Usr Some researchers have explored features for user-profile metainformation in addition to user content. This may include the user’s communication behavior and netw</context>
<context position="29411" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="4641" endWordPosition="4644">e BoW features: hubby, hubs, jewelry, sewing, mascara, fabulous, bf, softball, betcha, motherhood, perky, cozy, zumba, xox, cuddled, belieber, bridesmaid, anorexic, jammies, pad Female Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “ve</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011. A machine learning approach to Twitter user classification. In Proc. ICWSM, pages 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<date>2006</date>
<journal>CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</journal>
<contexts>
<context position="15361" citStr="Phan, 2006" startWordPosition="2429" endWordPosition="2430">look for cases where users refer to such attributes in their first-person writings. We performed a preliminary analysis of a two-week sample of tweets from the TREC Tweets2011 Corpus.3 We found that users most often reveal their attributes in the possessive construction, “my X” where X is an attribute, quality or event that they possess (in a linguistic sense). For example, we found over 1000 tweets with the phrase “my wife.” In contrast, “I have a wife” occurs only 5 times.4 We therefore assign a user to a demographic category as follows: We first part-of-speech tag our data using CRFTagger (Phan, 2006) and then look for “my X” patterns where X is a sequence of tokens terminating in a noun, analogous to our 3http://trec.nist.gov/data/tweets/ This corpus was developed for the TREC Microblog track (Soboroff et al., 2012). 4Note that “I am a man” occurs only 20 times. Users also reveal their names in “my name is X” patterns in several hundred tweets, but this is small compared to cases of selfdistinguishing attributes. Exploiting these alternative patterns could nevertheless be a possible future direction. attribute-extraction pattern (Section 3).5 When a user uses such a “my X” construction, w</context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in Twitter.</title>
<date>2010</date>
<booktitle>In Proc. International Workshop on Search and Mining User-Generated Contents,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="1284" citStr="Rao et al., 2010" startWordPosition="192" endWordPosition="195"> in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a classification task and train classifier</context>
<context position="3694" citStr="Rao et al., 2010" startWordPosition="554" endWordPosition="557">k: predicting user gender (Section 5). Our bootstrapped system, trained purely from automatically-annotated Twitter data, significantly reduces error over a state-of-the-art system trained on thousands of gold-standard training examples. 2 Supervised User Characterization The current state-of-the-art in user characterization is to use supervised classifiers trained on annotated data. For each instance to be classified, the output is a decision about a distinct demographic property, such as Male/Female or Over/Under-18. A variety of classification algorithms have been employed, including SVMs (Rao et al., 2010), de710 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710–720, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cision trees (Pennacchiotti and Popescu, 2011), logistic regression (Van Durme, 2012), and the Winnow algorithm (Burger et al., 2011). Content Features: BoW Prior classifiers use a set of features encoding the presence of specific words in the user-generated text. We call these features BoW features as they encode the standard Bag-of-Words representation which has been highly effective in text categori</context>
<context position="29498" citStr="Rao et al., 2010" startWordPosition="4656" endWordPosition="4659">ky, cozy, zumba, xox, cuddled, belieber, bridesmaid, anorexic, jammies, pad Female Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in Twitter. In Proc. International Workshop on Search and Mining User-Generated Contents, pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Michael Paul</author>
<author>Clay Fink</author>
<author>David Yarowsky</author>
<author>Timothy Oates</author>
<author>Glen Coppersmith</author>
</authors>
<title>Hierarchical bayesian models for latent attribute detection in social media.</title>
<date>2011</date>
<booktitle>In Proc. ICWSM,</booktitle>
<pages>598--601</pages>
<contexts>
<context position="29378" citStr="Rao et al., 2011" startWordPosition="4637" endWordPosition="4640">de, man, big Female BoW features: hubby, hubs, jewelry, sewing, mascara, fabulous, bf, softball, betcha, motherhood, perky, cozy, zumba, xox, cuddled, belieber, bridesmaid, anorexic, jammies, pad Female Usr features: mrs, mom, jen, lady, wife, mary, joy, mama, pink, kim, diva, elle, woma, ms Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, </context>
</contexts>
<marker>Rao, Paul, Fink, Yarowsky, Oates, Coppersmith, 2011</marker>
<rawString>Delip Rao, Michael Paul, Clay Fink, David Yarowsky, Timothy Oates, and Glen Coppersmith. 2011. Hierarchical bayesian models for latent attribute detection in social media. In Proc. ICWSM, pages 598– 601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Latent variable models of concept-attribute attachment.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP,</booktitle>
<pages>620--628</pages>
<marker>Reisinger, Pas¸ca, 2009</marker>
<rawString>Joseph Reisinger and Marius Pas¸ca. 2009. Latent variable models of concept-attribute attachment. In Proc. ACL-IJCNLP, pages 620–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Richardson</author>
<author>William B Dolan</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MindNet: Acquiring and structuring semantic information from text.</title>
<date>1998</date>
<booktitle>In Proc. ACL-Coling,</booktitle>
<pages>1098--1102</pages>
<contexts>
<context position="11949" citStr="Richardson et al. (1998)" startWordPosition="1884" endWordPosition="1887">l, we did not invest in developing annotation guidelines or measuring inter-annotator agreement. We make these filter attributes available online as an attachment to this article, available through the ACL Anthology. Ultimately, we discovered that manual filtering was necessary to avoid certain pathological cases in our Twitter data. For example, our PMI scoring finds homepage to be strongly associated with males. In our gold-standard gender data (Section 5), however, every user has a homepage [by dataset construction]; we might therefore incorrectly classify every user as Male. We agree with Richardson et al. (1998) that “automatic procedures ... provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning” but “hand vetting” might be needed to ensure “accuracy and consistency in production level systems.” Since our approach requires manual involvement in the filtering of the attribute list, one might argue that one should simply manually enumerate the most relevant attributes directly. However, the manual generation of conceptual features by a single researcher results in substantial variability both across and within participants (McRae et al., </context>
<context position="32486" citStr="Richardson et al., 1998" startWordPosition="5120" endWordPosition="5123">tion guadalajara” are seeking values for a particular city’s “altitude” and “population” attributes (Pas¸ca and Van Durme, 2007). Finally, note that Van Durme et al. (2008) compared instance-based and class-based patterns for broad-definition attribute extraction, and found both to be effective. Of course, text-mining with custom-designed patterns is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has be</context>
</contexts>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Stephen D. Richardson, William B. Dolan, and Lucy Vanderwende. 1998. MindNet: Acquiring and structuring semantic information from text. In Proc. ACL-Coling, pages 1098–1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Schler</author>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>James W Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In Proc. AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</booktitle>
<pages>199--205</pages>
<contexts>
<context position="1266" citStr="Schler et al., 2006" startWordPosition="188" endWordPosition="191">knowledge can be used in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 1 Introduction There has been growing interest in characterizing social media users based on the content they generate; that is, automatically labeling users with demographic categories such as age and gender (Burger and Henderson, 2006; Schler et al., 2006; Rao et al., 2010; Mukherjee and Liu, 2010; Pennacchiotti and Popescu, 2011; Burger et al., 2011; Van Durme, 2012). Automatic user characterization has applications in targeted advertising and personalization, and could also lead to finergrained assessment of public opinion (O’Connor et al., 2010) and health (Paul and Dredze, 2011). Consider the following tweet and suppose we wish to predict the user’s gender: Dirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... State-of-the-art approaches cast this problem as a classification task an</context>
<context position="28062" citStr="Schler et al., 2006" startWordPosition="4435" endWordPosition="4438"> class, etc. (Fischer, 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in this study - such as author selfidentification.” Male BoW features: wife, wifey, sucked, shave, boner, boxers, missus, installed, manly, in-laws, brah, urinal, kickoff, golf, comics, ubuntu, homo, nhl, jedi, fatherhood, nigga, movemb</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. 2006. Effects of age and gender on blogging. In Proc. AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, pages 199–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart Schubert</author>
</authors>
<title>Can we derive general world knowledge from texts?</title>
<date>2002</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>84--87</pages>
<contexts>
<context position="2483" citStr="Schubert, 2002" startWordPosition="382" endWordPosition="383">rain classifiers using supervised learning (Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classification task: pre</context>
<context position="6390" citStr="Schubert, 2002" startWordPosition="980" endWordPosition="981">ss itself (car). Another approach is to use patterns that are based on instances (i.e. hyponyms or sub-classes) of the class. For example, Pas¸ca and Van Durme (2007) learn the attributes of the class car via patterns involving instances of cars, e.g. Chevrolet Corvette’s X and X of a Honda Civic. For these approaches, lists of instances are typically collected from publicly-available resources such as WordNet or Wikipedia (Pas¸ca and Van Durme, 2007; Van Durme et al., 2008), acquired automatically from corpora (Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010), or simply specified by hand (Schubert, 2002). Creation of Instance Lists We use an instancebased approach; our instances are derived from collections of common nouns that are associated with roles and occupations of people. For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by Bergsma and Lin (2006). This dataset indicates how often a noun is referenced by a male, female, neutral or plural pronoun. We extract prevalent common nouns for males and females by selecting only those nouns that (a) occur more than 200 times in the dataset, (b) mostly occu</context>
<context position="32810" citStr="Schubert, 2002" startWordPosition="5169" endWordPosition="5170">signed patterns is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been appli</context>
</contexts>
<marker>Schubert, 2002</marker>
<rawString>Lenhart Schubert. 2002. Can we derive general world knowledge from texts? In Proc. HLT, pages 84–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Comput. Surv.,</journal>
<pages>34--1</pages>
<contexts>
<context position="4345" citStr="Sebastiani, 2002" startWordPosition="651" endWordPosition="652">nual Meeting of the Association for Computational Linguistics, pages 710–720, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cision trees (Pennacchiotti and Popescu, 2011), logistic regression (Van Durme, 2012), and the Winnow algorithm (Burger et al., 2011). Content Features: BoW Prior classifiers use a set of features encoding the presence of specific words in the user-generated text. We call these features BoW features as they encode the standard Bag-of-Words representation which has been highly effective in text categorization and information retrieval (Sebastiani, 2002). User-Profile Features: Usr Some researchers have explored features for user-profile metainformation in addition to user content. This may include the user’s communication behavior and network of contacts (Rao et al., 2010), their full name (Burger et al., 2011) and whether they provide a profile picture (Pennacchiotti and Popescu, 2011). We focus on the case where we only have access to the user’s screen-name (a.k.a. username). Using a combination of content and username features “represents a use case common to many different social media sites, such as chat rooms and news article comment s</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Comput. Surv., 34:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
<author>Dean McCullough</author>
<author>Jimmy Lin</author>
<author>Craig Macdonald</author>
<author>Iadh Ounis</author>
<author>Richard McCreadie</author>
</authors>
<title>Evaluating real-time search over tweets.</title>
<date>2012</date>
<booktitle>In Proc. ICWSM.</booktitle>
<contexts>
<context position="15581" citStr="Soboroff et al., 2012" startWordPosition="2463" endWordPosition="2466"> often reveal their attributes in the possessive construction, “my X” where X is an attribute, quality or event that they possess (in a linguistic sense). For example, we found over 1000 tweets with the phrase “my wife.” In contrast, “I have a wife” occurs only 5 times.4 We therefore assign a user to a demographic category as follows: We first part-of-speech tag our data using CRFTagger (Phan, 2006) and then look for “my X” patterns where X is a sequence of tokens terminating in a noun, analogous to our 3http://trec.nist.gov/data/tweets/ This corpus was developed for the TREC Microblog track (Soboroff et al., 2012). 4Note that “I am a man” occurs only 20 times. Users also reveal their names in “my name is X” patterns in several hundred tweets, but this is small compared to cases of selfdistinguishing attributes. Exploiting these alternative patterns could nevertheless be a possible future direction. attribute-extraction pattern (Section 3).5 When a user uses such a “my X” construction, we match the filler X against our attribute lists for each class. If the filler is on a list, we call it a selfdistinguishing attribute of a user. We then apply our knowledge of the self-distinguishing attribute and its c</context>
</contexts>
<marker>Soboroff, McCullough, Lin, Macdonald, Ounis, McCreadie, 2012</marker>
<rawString>Ian Soboroff, Dean McCullough, Jimmy Lin, Craig Macdonald, Iadh Ounis, and Richard McCreadie. 2012. Evaluating real-time search over tweets. In Proc. ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="33740" citStr="Soon et al., 2001" startWordPosition="5316" endWordPosition="5319">rthy (1959). Lenat et al. (1990) defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proc. NAACLHLT,</booktitle>
<pages>477--487</pages>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proc. NAACLHLT, pages 477–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kosuke Tokunaga</author>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Automatic discovery of attribute words from web documents.</title>
<date>2005</date>
<booktitle>In Proc. IJCNLP,</booktitle>
<pages>106--118</pages>
<contexts>
<context position="2534" citStr="Tokunaga et al., 2005" startWordPosition="388" endWordPosition="391">Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classification task: predicting user gender (Section 5). Our bootstrapped s</context>
<context position="31265" citStr="Tokunaga et al. (2005)" startWordPosition="4930" endWordPosition="4933">ly valuable in classification; for example, the attributes peak fertility and loveliness are highly 90 85 80 75 70 65 60 BoW+Usr BoW Usr Accuracy 716 associated with females. As subsequent research became more focused on applications, looser definitions of class attributes were adopted. Almuhareb and Poesio (2004) automatically mined class attributes that include parts, qualities, and those with an “agentive” or “telic” role with the class. Their extended set of attributes was shown to enable an improved representation of nouns for the purpose of clustering these nouns into semantic concepts. Tokunaga et al. (2005) define attributes as properties that can serve as focus words in questions about a target class; e.g. director is an attribute of a movie since one might ask, “Who is the director of this movie?” Another line of research has been motivated by the observation that much of Internet search consists of people looking for values of various class attributes (Bellare et al., 2007; Pas¸ca and Van Durme, 2007; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). By knowing the attributes of different classes, search engines can better recognize that queries such as “altitude guadalajara” or “populatio</context>
</contexts>
<marker>Tokunaga, Kazama, Torisawa, 2005</marker>
<rawString>Kosuke Tokunaga, Jun’ichi Kazama, and Kentaro Torisawa. 2005. Automatic discovery of attribute words from web documents. In Proc. IJCNLP, pages 106– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="34135" citStr="Turney, 2002" startWordPosition="5380" endWordPosition="5381">tion of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). 8 Conclusion We have proposed, developed and successfully evaluated a novel approach to user characterization based on exploiting knowledge of user class attributes. The knowledge is obtained using a new algorithm that di</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proc. ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ting Qian</author>
<author>Lenhart Schubert</author>
</authors>
<title>Class-driven attribute extraction.</title>
<date>2008</date>
<booktitle>In Proc. Coling,</booktitle>
<pages>921--928</pages>
<marker>Van Durme, Qian, Schubert, 2008</marker>
<rawString>Benjamin Van Durme, Ting Qian, and Lenhart Schubert. 2008. Class-driven attribute extraction. In Proc. Coling, pages 921–928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Streaming analysis of discourse participants.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<pages>48--58</pages>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012. Streaming analysis of discourse participants. In Proc. EMNLP-CoNLL, pages 48–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics.,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="33943" citStr="Wilson et al., 2009" startWordPosition="5350" endWordPosition="5353"> to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). 8 Conclusion We have proposed</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics., 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying Wikipedia. In</title>
<date>2007</date>
<booktitle>Proc. CIKM,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="32562" citStr="Wu and Weld, 2007" startWordPosition="5132" endWordPosition="5135">ation” attributes (Pas¸ca and Van Durme, 2007). Finally, note that Van Durme et al. (2008) compared instance-based and class-based patterns for broad-definition attribute extraction, and found both to be effective. Of course, text-mining with custom-designed patterns is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). NLP Applications of Common-Sense Knowledge The kind of information derived from class-attribute extraction is sometimes referred to as a type of common-sense knowledge. The need for computer programs to represent commonsense knowledge has been recognized since the work of McCarthy (1959). Lenat et al. (1990) defines</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying Wikipedia. In Proc. CIKM, pages 41– 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using “annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In Proc. NAACL-HLT.</booktitle>
<contexts>
<context position="12953" citStr="Zaidan et al. (2007)" startWordPosition="2039" endWordPosition="2042">lly enumerate the most relevant attributes directly. However, the manual generation of conceptual features by a single researcher results in substantial variability both across and within participants (McRae et al., 2005). Psychologists therefore generate such lists by pooling the responses across many participants: future work may compare our “automatically generate, manually prune” approach to soliciting attributes via crowdsourcing.2 Table 2 gives examples of our extracted at2One can also view the work of manually filtering attributes as a kind of “feature labeling.” There is evidence from Zaidan et al. (2007) that a few hours of feature labeling can be more productive than annotating new training examples. In fact, since Zaidan et al. (2007) label features at the token level (e.g., in our case one would highlight “handbag” in a given tweet), while we label features at the type level (e.g., deciding whether to mark the word “handbag” as feminine in general), our process is likely even more efficient. Future work may also wish to consider this connection to socalled ”annotator rationales” more deeply. 712 Male: wife, widow, wives, ex-girlfriend, erection, testicles, wet dream, bride, buddies, exwife</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In Proc. NAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>