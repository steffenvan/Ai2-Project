<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.164976">
<title confidence="0.995857">
Model-Portability Experiments for Textual Temporal Analysis
</title>
<author confidence="0.998508">
Oleksandr Kolomiyets, Steven Bethard and Marie-Francine Moens
</author>
<affiliation confidence="0.912251">
Department of Computer Science
Katholieke Universiteit Leuven
</affiliation>
<address confidence="0.941059">
Celestijnenlaan 200A, Heverlee, 3001, Belgium
</address>
<email confidence="0.993296">
{oleksandr.kolomiyets, steven.bethard, sien.moens}@cs.kuleuven.be
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953619047619">
We explore a semi-supervised approach for
improving the portability of time expression
recognition to non-newswire domains: we
generate additional training examples by
substituting temporal expression words with
potential synonyms. We explore using
synonyms both from WordNet and from the
Latent Words Language Model (LWLM),
which predicts synonyms in context using
an unsupervised approach. We evaluate a
state-of-the-art time expression recognition
system trained both with and without the
additional training examples using data from
TempEval 2010, Reuters and Wikipedia.
We find that the LWLM provides substan-
tial improvements on the Reuters corpus,
and smaller improvements on the Wikipedia
corpus. We find that WordNet alone never
improves performance, though intersecting
the examples from the LWLM and WordNet
provides more stable results for Wikipedia.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963285714286">
The recognition of time expressions such as April
2011, mid-September and early next week is a cru-
cial first step for applications like question answer-
ing that must be able to handle temporally
anchored queries. This need has inspired a variety
of shared tasks for identifying time expressions,
including the Message Understanding Conference
named entity task (Grishman and Sundheim,
1996), the Automatic Content Extraction time
normalization task (http://fofoca.mitre.org/tern.html)
and the TempEval 2010 time expression task
(Verhagen et al., 2010). Many researchers com-
peted in these tasks, applying both rule-based and
machine-learning approaches (Mani and Wilson,
2000; Negri and Marseglia, 2004; Hacioglu et al.,
2005; Ahn et al., 2007; Poveda et al., 2007;
Strštgen and Gertz 2010; Llorens et al., 2010), and
achieving F1 measures as high as 0.86 for recog-
nizing temporal expressions.
Yet in most of these recent evaluations, models
are both trained and evaluated on text from the
same domain, typically newswire. Thus we know
little about how well time expression recognition
systems generalize to other sorts of text. We there-
fore take a state-of-the-art time recognizer and eva-
luate it both on TempEval 2010 and on two new
test sets drawn from Reuters and Wikipedia.
At the same time, we are interested in helping
the model recognize more types of time expres-
sions than are available explicitly in the newswire
training data. We therefore introduce a semi-
supervised approach for expanding the training
data, where we take words from temporal expres-
sions in the data, substitute these words with likely
synonyms, and add the generated examples to the
training set. We select synonyms both via Word-
Net, and via predictions from the Latent Words
Language Model (LWLM) (Deschacht and Moens,
2009). We then evaluate the semi-supervised mod-
el on the TempEval, Reuters and Wikipedia test
sets and observe how well the model has expanded
its temporal vocabulary.
</bodyText>
<page confidence="0.969058">
271
</page>
<note confidence="0.7960965">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999604" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999956310344827">
Semi-supervised approaches have been applied to a
wide variety of natural language processing tasks,
including word sense disambiguation (Yarowsky,
1995), named entity recognition (Collins and
Singer, 1999), and document classification (Sur-
deanu et al., 2006).
The most relevant research to our work here is
that of (Poveda et al., 2009), which investigated a
semi-supervised approach to time expression rec-
ognition. They begin by selecting 100 time expres-
sions as seeds, selecting only expressions that are
almost always annotated as times in the training
half of the Automatic Content Extraction corpus.
Then they begin an iterative process where they
search an unlabeled corpus for patterns given their
seeds (with patterns consisting of surrounding to-
kens, parts-of-speech, syntactic chunks etc.) and
then search for new seeds given their patterns. The
patterns resulting from this iterative process
achieve F1 scores of up to 0.604 on the test half of
the Automatic Content Extraction corpus.
Our approach is quite different from that of (Po-
veda et al., 2009) – we use our training corpus for
learning a supervised model rather than for se-
lecting high precision seeds, we generate addi-
tional training examples using synonyms rather
than bootstrapping based on patterns, and we
evaluate on Reuters and Wikipedia data that differ
from the domain on which our model was trained.
</bodyText>
<sectionHeader confidence="0.986768" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999517571428571">
The proposed method implements a supervised
machine learning approach that classifies each
chunk-phrase candidate top-down starting at the
parse tree root provided by the OpenNLP parser.
Time expressions are identified as phrasal chunks
with spans derived from the parse as described in
(Kolomiyets and Moens, 2010).
</bodyText>
<subsectionHeader confidence="0.998851">
3.1 Basic TempEval Model
</subsectionHeader>
<bodyText confidence="0.9992085">
We implemented a logistic regression model with
the following features for each phrase-candidate:
</bodyText>
<listItem confidence="0.999355307692308">
• The head word of the phrase
• The part-of-speech tag of the head word
• All tokens and part-of-speech tags in the
phrase as a bag of words
• The word-shape representation of the head
word and the entire phrase, e.g. Xxxxx 99
for the expression April 30
• The condensed word-shape representation for
the head word and the entire phrase, e.g.
X(x) (9) for the expression April 30
• The concatenated string of the syntactic types
of the children of the phrase in the parse tree
• The depth in the parse tree
</listItem>
<subsectionHeader confidence="0.999738">
3.2 Lexical Resources for Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999974481481481">
Sparsity of annotated corpora is the biggest chal-
lenge for any supervised machine learning tech-
nique and especially for porting the trained models
onto other domains. To overcome this problem we
hypothesize that knowledge of semantically similar
words, like temporal triggers, could be found by
associating words that do not occur in the training
set to similar words that do occur in the training
set. Furthermore, we would like to learn these
similarities automatically to be independent of
knowledge sources that might not be available for
all languages or domains. The first option is to use
the Latent Words Language Model (LWLM)
(Deschacht and Moens, 2009) – a language model
that learns from an unlabeled corpus how to pro-
vide a weighted set of synonyms for words in con-
text. The LWLM model is trained on the Reuters
news article corpus of 80 million words.
WordNet (Miller, 1995) is another resource for
synonyms widely used in research and applications
of natural language processing. Synonyms from
WordNet seem to be very useful for bootstrapping
as they provide replacement words to a specific
word in a particular sense. For each synset in
WordNet there is a collection of other “sister” syn-
sets, called coordinate terms, which are topologi-
cally located under the same hypernym.
</bodyText>
<subsectionHeader confidence="0.999957">
3.3 Bootstrapping Strategies
</subsectionHeader>
<bodyText confidence="0.999945666666667">
Having a list of synonyms for each token in the
sentence, we can replace one of the original tokens
by its synonym while still mostly preserving the
sentence semantics. We choose to replace just the
headword, under the assumption that since tempo-
ral trigger words usually occur at the headword
position, adding alternative synonyms for the
headword should allow our model to learn tempo-
ral triggers that did not appear in the training data.
</bodyText>
<page confidence="0.992532">
272
</page>
<table confidence="0.999388764705882">
Basic Bootstrapped Models
TempEval
Model
LWLM WordNet 1st WordNet LWLM+
Sense Pseudo-Lesk WordNet
TempEval 2010 # Syn 0 1 1 1 2
P 0.916 0.865 0.881 0.894 0.857
R 0.770 0.807 0.773 0.781 0.830
F1 0.834 0.835 0.824 0.833 0.829
Reuters # Syn 0 5 7 6 4
P 0.896 0.841 0.820 0.839 0.860
R 0.679 0.812 0.721 0.717 0.742
F1 0.773 0.826 0.767 0.773 0.796
Wikipedia # Syn 0 3 1 6 5
P 0.959 0.924 0.922 0.909 0.913
R 0.770 0.830 0.781 0.820 0.844
F1 0.859 0.874 0.858 0.862 0.877
</table>
<tableCaption confidence="0.9807256">
Table 1: Precision, recall and F1 scores for all models on the source (TempEval 2010) and target (Reuters
and Wikipedia) domains. Bootstrapped models were asked to generate between one and ten additional train-
ing examples per instance. The maximum P, R, F1 and the number of synonyms at which this maximum
was achieved are given in the P, R, F1 and # Syn rows. F1 scores more than 0.010 above the Basic Tem-
pEval Model are marked in bold.
</tableCaption>
<bodyText confidence="0.999523">
We designed the following bootstrapping strate-
gies for generating new temporal expressions:
</bodyText>
<listItem confidence="0.983399733333333">
• LWLM: the phrasal head is replaced by one of
the LWLM synonyms.
• WordNet 1st Sense: Synonyms and coordinate
terms for the most common sense of the
phrasal head are selected and used for generat-
ing new examples of time expressions.
• WordNet Pseudo-Lesk: The synset for the
phrasal head is selected as having the largest
intersection between the synset’s words and
the LWLM synonyms. Then, synonyms and
coordinate terms are used for generating new
examples of time expressions.
• LWLM+WordNet: The intersection of the
LWLM synonyms and the WordNet synset
found by pseudo-Lesk are used.
</listItem>
<bodyText confidence="0.999934333333333">
In this way for every annotated time expression we
generate n new examples (n∈[1,10]) and use them
for training bootstrapped classification models.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999841043478261">
The tested model is trained on the official Tem-
pEval 2010 training data with 53450 tokens and
2117 annotated TIMEX3 tokens. For testing the
portability of the model to other domains we anno-
tated two small target domain document collec-
tions with TIMEX3 tags. The first corpus is 12
Reuters news articles from the Reuters corpus
(Lewis et al., 2004), containing 2960 total tokens
and 240 annotated TIMEX3 tokens (inter-
annotator agreement 0.909 F1-score). The second
corpus is the Wikipedia article for Barak Obama
(http://en.wikipedia.org/wiki/Obama), containing
7029 total tokens and 512 annotated TIMEX3 to-
kens (inter-annotator agreement 0.901 F1-score).
The basic TempEval model is evaluated on the
source domain (TempEval 2010 evaluation set –
9599 tokens in total and 269 TIMEX3 annotated
tokens) and target domain data (Reuters and
Wikipedia) using the TempEval 2010 evaluation
metrics. Since porting the model onto other do-
mains usually causes a performance drop, our ex-
periments are focused on improving the results by
employing different bootstrapping strategies1.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999944375">
The recognition performance of the model is re-
ported in Table 1 (column “Basic TempEval Mod-
el”) for the source and the target domains. The
basic TempEval model itself achieves F1-score of
0.834 on the official TempEval 2010 evaluation
corpus and has a potential rank 8 among 15 par-
ticipated systems. The top seven TempEval-2 sys-
tems achieved F1-score between 0.83 and 0.86.
</bodyText>
<footnote confidence="0.9572965">
1 The annotated datasets are available at
http://www.cs.kuleuven.be/groups/liir/software.php
</footnote>
<page confidence="0.997827">
273
</page>
<figureCaption confidence="0.998253333333333">
Figure 1: F1 score of the LWLM bootstrapping strat-
egy, generating from zero to ten additional training
examples per instance.
</figureCaption>
<bodyText confidence="0.999951242424242">
However, this model does not port well to the
Reuters corpus (0.773 vs. 0.834 F1-score). For the
Wikipedia-based corpus, the basic TempEval mod-
el actually performs a little better than on the
source domain (0.859 vs. 0.834 F1-score).
Four bootstrapping strategies were proposed and
evaluated. Table 1 shows the maximum F1 score
achieved by each of these strategies, along with the
number of generated synonyms (between one and
ten) at which this maximum was achieved. None of
the bootstrapped models outperformed the basic
TempEval model on the TempEval 2010 evalua-
tion data, and the WordNet 1st Sense strategy and
the WordNet Pseudo-Lesk strategy never outper-
formed the basic TempEval model on any corpus.
However, for the Reuters and Wikipedia cor-
pora, the LWLM and LWLM+WordNet bootstrap-
ping strategies outperformed the basic TempEval
model. The LWLM strategy gives a large boost to
model performance on the Reuters corpus from
0.773 up to 0.826 (a 23.3% error reduction) when
using the first 5 synonyms. This puts performance
on Reuters near performance on the TempEval
domain from which the model was trained (0.834).
This suggests that the (Reuters-trained) LWLM is
finding exactly the right kinds of synonyms: those
that were not originally present in the TempEval
data but are present in the Reuters test data. On the
Wikipedia corpus, the LWLM bootstrapping strat-
egy results in a moderate boost, from 0.859 up to
0.874 (a 10.6% error reduction) when using the
first three synonyms. Figure 1 shows that using
more synonyms with this strategy drops perform-
</bodyText>
<figureCaption confidence="0.992206">
Figure 2: F1 score of the LWLM+WordNet bootstrap-
ping strategy, generating from zero to ten additional
training examples per instance.
</figureCaption>
<bodyText confidence="0.999929166666667">
ance on the Wikipedia corpus back down to the
level of the basic TempEval model.
The LWLM+WordNet strategy gives a moderate
boost on the Reuters corpus from 0.773 up to 0.796
(a 10.1% error reduction) when four synonyms are
used. Figure 2 shows that using six or more syno-
nyms drops this performance back to just above the
basic TempEval model. On the Wikipedia corpus,
the LWLM+WordNet strategy results in a moder-
ate boost, from 0.859 up to 0.877 (a 12.8% error
reduction), with five synonyms. Using additional
synonyms results in a small decline in perform-
ance, though even with ten synonyms, the per-
formance is better than the basic TempEval model.
In general, the LWLM strategy gives the best
performance, while the LWLM+WordNet strategy
is less sensitive to the exact number of synonyms
used when expanding the training data.
</bodyText>
<sectionHeader confidence="0.968539" genericHeader="method">
6 TempEval Error Analysis
</sectionHeader>
<bodyText confidence="0.999845833333333">
We were curious why synonym-based boot-
strapping did not improve performance on the
source-domain TempEval 2010 data. An error
analysis suggested that some time expressions
might have been left unannotated by the human
annotators. Two of the authors re-annotated the
TempEval evaluation data, finding inter-annotator
agreement of 0.912 F1-score with each other, but
only 0.868 and 0.887 F1-score with the TempEval
annotators, primarily due to unannotated time ex-
pressions such as 23-year, a few days and third-
quarter.
</bodyText>
<page confidence="0.997512">
274
</page>
<figureCaption confidence="0.991945">
Figure 3: F1 score of the LWLM bootstrapping strat-
egy, comparing performance on the original TempEval
data to the re-annotated version.
</figureCaption>
<bodyText confidence="0.999941583333333">
Using this re-annotated TempEval 2010 data2,
we re-evaluated the proposed bootstrapping tech-
niques. Figure 3 and Figure 4 compare perform-
ance on the original TempEval data to performance
on the re-annotated version. We now see the same
trends for the TempEval data as were observed for
the Reuters and Wikipedia corpora: using a small
number of synonyms from the LWLM to generate
new training examples leads to performance gains.
The LWLM bootstrapping model using the first
synonym achieves 0.861 F1 score, a 22.8% error
reduction over the baseline of 0.820 F1 score.
</bodyText>
<sectionHeader confidence="0.990214" genericHeader="discussions">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.9997828">
We have presented model-portability experiments
on time expression recognition with a number of
bootstrapping strategies. These bootstrapping strat-
egies generate additional training examples by
substituting temporal expression words with poten-
tial synonyms from two sources: WordNet and the
Latent Word Language Model (LWLM).
Bootstrapping with LWLM synonyms provides
a large boost for Reuters data and TempEval data
and a decent boost for Wikipedia data when the top
few synonyms are used. Additional synonyms do
not help, probably because they are too newswire-
specific: both the contexts from the TempEval
training data and the synonyms from the Reuters-
trained LWLM come from newswire text, so the
</bodyText>
<footnote confidence="0.8771495">
2 Available at
http://www.cs.kuleuven.be/groups/liir/software.php
</footnote>
<figureCaption confidence="0.989947666666667">
Figure 4: F1 score of the LWLM+WordNet bootstrap-
ping strategy, comparing performance on the original
TempEval data to the re-annotated version.
</figureCaption>
<bodyText confidence="0.999644944444445">
lower synonyms are probably more domain-
specific.
Intersecting the synonyms generated by the
LWLM and by WordNet moderates the LWLM,
making the bootstrapping strategy less sensitive to
the exact number of synonyms used. However,
while the intersected model performs as well as the
LWLM model on Wikipedia, the gains over the
non-bootstrapped model on Reuters and TempEval
data are smaller.
Overall, our results show that when porting time
expression recognition models to other domains, a
performance drop can be avoided by synonym-
based bootstrapping. Future work will focus on
using synonym-based expansion in the contexts
(not just the time expressions headwords), and on
incorporating contextual information and syntactic
transformations.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9957705">
This work has been funded by the Flemish gov-
ernment as a part of the project AMASS++ (Ad-
vanced Multimedia Alignment and Structured
Summarization) (Grant: IWT-SBO-060051).
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990699333333333">
David Ahn, Joris van Rantwijk, and Maarten de Rijke.
2007. A Cascaded Machine Learning Approach to
Interpreting Temporal Expressions. In Proceedings
of the Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT 2007).
</reference>
<page confidence="0.992399">
275
</page>
<reference confidence="0.968681337662337">
Michael Collins and Yoram Singer. 1999. Unsupervised
Models for Named Entity Classification. In Proceed-
ings of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pp. 100–110, College Park, MD.
ACL.
Koen Deschacht and Marie-Francine Moens. 2009. Us-
ing the Latent Words Language Model for Semi-
Supervised Semantic Role Labeling. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference-6: A Brief History. In
Proceedings of the 16th Conference on Computa-
tional Linguistics, pp. 466–471.
Kadri Hacioglu, Ying Chen, and Benjamin Douglas
2005. Automatic Time Expression Labeling for Eng-
lish and Chinese Text. In Gelbukh, A. (ed.) CICLing
2005. LNCS, vol. 3406, pp. 548–559. Springer, Hei-
delberg.
Oleksandr Kolomiyets, Marie-Francine Moens. 2010.
KUL: Recognition and Normalization of Temporal
Expressions. In Proceedings of SemEval-2 5th Work-
shop on Semantic Evaluation. pp. 325-328. Uppsala,
Sweden. ACL.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A New Benchmark Collection for
Text Categorization Research. Machine Learning Re-
search. 5: 361-397
Inderjeet Mani, and George Wilson. 2000. Robust Tem-
poral Processing of News. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pp. 69-76, Morristown, NJ. ACL.
George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):
39-41.
Matteo Negri, and Luca Marseglia. 2004. Recognition
and Normalization of Time Expressions: ITC-irst at
TERN 2004. Technical Report, ITC-irst, Trento.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. TIPSem (English and Spanish): Evaluating
CRFs and Semantic Roles in TempEval 2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pp. 284–291, Uppsala, Sweden.
ACL.
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2007.
A Comparison of Statistical and Rule-Induction
Learners for Automatic Tagging of Time Expressions
in English. In Proceedings of the International Sym-
posium on Temporal Representation and Reasoning,
pp. 141-149.
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2009.
An Analysis of Bootstrapping for the Recognition of
Temporal Expressions. In Proceedings of the NAACL
HLT 2009 Workshop on Semi-Supervised Learning
for Natural Language Processing, pp. 49-57,
Stroudsburg, PA, USA. ACL.
Jannik Strštgen and Michael Gertz. 2010. HeidelTime:
High Quality Rule-Based Extraction and Normaliza-
tion of Temporal Expressions. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pp. 321–324, Uppsala, Sweden. ACL.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A Hybrid Approach for the Acquisition of Informa-
tion Extraction Patterns. In Proceedings of the EACL
2006 Workshop on Adaptive Text Extraction and
Mining (ATEM 2006). ACL.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval 2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pp. 57–62, Upp-
sala, Sweden. ACL.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pp. 189–
196, Cambridge, MA. ACL.
</reference>
<page confidence="0.998466">
276
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774646">
<title confidence="0.99969">Model-Portability Experiments for Textual Temporal Analysis</title>
<author confidence="0.929701">Oleksandr Kolomiyets</author>
<author confidence="0.929701">Steven Bethard</author>
<author confidence="0.929701">Marie-Francine Moens</author>
<affiliation confidence="0.987425">Department of Computer Katholieke Universiteit</affiliation>
<address confidence="0.97856">Celestijnenlaan 200A, Heverlee, 3001,</address>
<email confidence="0.963562">oleksandr.kolomiyets@cs.kuleuven.be</email>
<email confidence="0.963562">steven.bethard@cs.kuleuven.be</email>
<email confidence="0.963562">sien.moens@cs.kuleuven.be</email>
<abstract confidence="0.994865454545454">We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Ahn</author>
<author>Joris van Rantwijk</author>
<author>Maarten de Rijke</author>
</authors>
<title>A Cascaded Machine Learning Approach to Interpreting Temporal Expressions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT</booktitle>
<marker>Ahn, van Rantwijk, de Rijke, 2007</marker>
<rawString>David Ahn, Joris van Rantwijk, and Maarten de Rijke. 2007. A Cascaded Machine Learning Approach to Interpreting Temporal Expressions. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<publisher>ACL.</publisher>
<location>College Park, MD.</location>
<contexts>
<context position="3573" citStr="Collins and Singer, 1999" startWordPosition="513" endWordPosition="516">l (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding tokens, parts-of-speech, syntactic chunks etc.) and t</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 100–110, College Park, MD. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Using the Latent Words Language Model for SemiSupervised Semantic Role Labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2984" citStr="Deschacht and Moens, 2009" startWordPosition="432" endWordPosition="435">zer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in the newswire training data. We therefore introduce a semisupervised approach for expanding the training data, where we take words from temporal expressions in the data, substitute these words with likely synonyms, and add the generated examples to the training set. We select synonyms both via WordNet, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and docum</context>
<context position="6406" citStr="Deschacht and Moens, 2009" startWordPosition="973" endWordPosition="976"> biggest challenge for any supervised machine learning technique and especially for porting the trained models onto other domains. To overcome this problem we hypothesize that knowledge of semantically similar words, like temporal triggers, could be found by associating words that do not occur in the training set to similar words that do occur in the training set. Furthermore, we would like to learn these similarities automatically to be independent of knowledge sources that might not be available for all languages or domains. The first option is to use the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009) – a language model that learns from an unlabeled corpus how to provide a weighted set of synonyms for words in context. The LWLM model is trained on the Reuters news article corpus of 80 million words. WordNet (Miller, 1995) is another resource for synonyms widely used in research and applications of natural language processing. Synonyms from WordNet seem to be very useful for bootstrapping as they provide replacement words to a specific word in a particular sense. For each synset in WordNet there is a collection of other “sister” synsets, called coordinate terms, which are topologically loca</context>
</contexts>
<marker>Deschacht, Moens, 2009</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2009. Using the Latent Words Language Model for SemiSupervised Semantic Role Labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message Understanding Conference-6: A Brief History.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics,</booktitle>
<pages>466--471</pages>
<contexts>
<context position="1571" citStr="Grishman and Sundheim, 1996" startWordPosition="208" endWordPosition="211">Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia. 1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the s</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In Proceedings of the 16th Conference on Computational Linguistics, pp. 466–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Ying Chen</author>
<author>Benjamin Douglas</author>
</authors>
<title>Automatic Time Expression Labeling for English and Chinese Text.</title>
<date>2005</date>
<booktitle>CICLing 2005. LNCS,</booktitle>
<volume>3406</volume>
<pages>548--559</pages>
<editor>In Gelbukh, A. (ed.)</editor>
<publisher>Springer,</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="1905" citStr="Hacioglu et al., 2005" startWordPosition="252" endWordPosition="255"> first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping</context>
</contexts>
<marker>Hacioglu, Chen, Douglas, 2005</marker>
<rawString>Kadri Hacioglu, Ying Chen, and Benjamin Douglas 2005. Automatic Time Expression Labeling for English and Chinese Text. In Gelbukh, A. (ed.) CICLing 2005. LNCS, vol. 3406, pp. 548–559. Springer, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Marie-Francine Moens</author>
</authors>
<title>KUL: Recognition and Normalization of Temporal Expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2 5th Workshop on Semantic Evaluation.</booktitle>
<pages>325--328</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="5072" citStr="Kolomiyets and Moens, 2010" startWordPosition="750" endWordPosition="753">training corpus for learning a supervised model rather than for selecting high precision seeds, we generate additional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia data that differ from the domain on which our model was trained. 3 Method The proposed method implements a supervised machine learning approach that classifies each chunk-phrase candidate top-down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010). 3.1 Basic TempEval Model We implemented a logistic regression model with the following features for each phrase-candidate: • The head word of the phrase • The part-of-speech tag of the head word • All tokens and part-of-speech tags in the phrase as a bag of words • The word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30 • The condensed word-shape representation for the head word and the entire phrase, e.g. X(x) (9) for the expression April 30 • The concatenated string of the syntactic types of the children of the phrase in the parse tre</context>
</contexts>
<marker>Kolomiyets, Moens, 2010</marker>
<rawString>Oleksandr Kolomiyets, Marie-Francine Moens. 2010. KUL: Recognition and Normalization of Temporal Expressions. In Proceedings of SemEval-2 5th Workshop on Semantic Evaluation. pp. 325-328. Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: A New Benchmark Collection for Text Categorization Research.</title>
<date>2004</date>
<journal>Machine Learning Research.</journal>
<volume>5</volume>
<pages>361--397</pages>
<contexts>
<context position="9612" citStr="Lewis et al., 2004" startWordPosition="1522" endWordPosition="1525">M+WordNet: The intersection of the LWLM synonyms and the WordNet synset found by pseudo-Lesk are used. In this way for every annotated time expression we generate n new examples (n∈[1,10]) and use them for training bootstrapped classification models. 4 Experimental Setup The tested model is trained on the official TempEval 2010 training data with 53450 tokens and 2117 annotated TIMEX3 tokens. For testing the portability of the model to other domains we annotated two small target domain document collections with TIMEX3 tags. The first corpus is 12 Reuters news articles from the Reuters corpus (Lewis et al., 2004), containing 2960 total tokens and 240 annotated TIMEX3 tokens (interannotator agreement 0.909 F1-score). The second corpus is the Wikipedia article for Barak Obama (http://en.wikipedia.org/wiki/Obama), containing 7029 total tokens and 512 annotated TIMEX3 tokens (inter-annotator agreement 0.901 F1-score). The basic TempEval model is evaluated on the source domain (TempEval 2010 evaluation set – 9599 tokens in total and 269 TIMEX3 annotated tokens) and target domain data (Reuters and Wikipedia) using the TempEval 2010 evaluation metrics. Since porting the model onto other domains usually cause</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. Machine Learning Research. 5: 361-397</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>George Wilson</author>
</authors>
<title>Robust Temporal Processing of News.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>69--76</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ.</location>
<contexts>
<context position="1855" citStr="Mani and Wilson, 2000" startWordPosition="244" endWordPosition="247">11, mid-September and early next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikiped</context>
</contexts>
<marker>Mani, Wilson, 2000</marker>
<rawString>Inderjeet Mani, and George Wilson. 2000. Robust Temporal Processing of News. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pp. 69-76, Morristown, NJ. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="6631" citStr="Miller, 1995" startWordPosition="1017" endWordPosition="1018">s, could be found by associating words that do not occur in the training set to similar words that do occur in the training set. Furthermore, we would like to learn these similarities automatically to be independent of knowledge sources that might not be available for all languages or domains. The first option is to use the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009) – a language model that learns from an unlabeled corpus how to provide a weighted set of synonyms for words in context. The LWLM model is trained on the Reuters news article corpus of 80 million words. WordNet (Miller, 1995) is another resource for synonyms widely used in research and applications of natural language processing. Synonyms from WordNet seem to be very useful for bootstrapping as they provide replacement words to a specific word in a particular sense. For each synset in WordNet there is a collection of other “sister” synsets, called coordinate terms, which are topologically located under the same hypernym. 3.3 Bootstrapping Strategies Having a list of synonyms for each token in the sentence, we can replace one of the original tokens by its synonym while still mostly preserving the sentence semantics</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Luca Marseglia</author>
</authors>
<title>Recognition and Normalization of Time Expressions: ITC-irst at TERN</title>
<date>2004</date>
<tech>Technical Report, ITC-irst,</tech>
<location>Trento.</location>
<contexts>
<context position="1882" citStr="Negri and Marseglia, 2004" startWordPosition="248" endWordPosition="251">arly next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we ar</context>
</contexts>
<marker>Negri, Marseglia, 2004</marker>
<rawString>Matteo Negri, and Luca Marseglia. 2004. Recognition and Normalization of Time Expressions: ITC-irst at TERN 2004. Technical Report, ITC-irst, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Llorens</author>
<author>Estela Saquete</author>
<author>Borja Navarro</author>
</authors>
<title>TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval 2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>284--291</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="1992" citStr="Llorens et al., 2010" startWordPosition="268" endWordPosition="271">ally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in th</context>
</contexts>
<marker>Llorens, Saquete, Navarro, 2010</marker>
<rawString>Hector Llorens, Estela Saquete, and Borja Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval 2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 284–291, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Poveda</author>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>A Comparison of Statistical and Rule-Induction Learners for Automatic Tagging of Time Expressions in English.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Symposium on Temporal Representation and Reasoning,</booktitle>
<pages>141--149</pages>
<contexts>
<context position="1944" citStr="Poveda et al., 2007" startWordPosition="260" endWordPosition="263">n answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time</context>
</contexts>
<marker>Poveda, Surdeanu, Turmo, 2007</marker>
<rawString>Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2007. A Comparison of Statistical and Rule-Induction Learners for Automatic Tagging of Time Expressions in English. In Proceedings of the International Symposium on Temporal Representation and Reasoning, pp. 141-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Poveda</author>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>An Analysis of Bootstrapping for the Recognition of Temporal Expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3704" citStr="Poveda et al., 2009" startWordPosition="536" endWordPosition="539">serve how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding tokens, parts-of-speech, syntactic chunks etc.) and then search for new seeds given their patterns. The patterns resulting from this iterative process achieve F1 scores of up to 0.604 </context>
</contexts>
<marker>Poveda, Surdeanu, Turmo, 2009</marker>
<rawString>Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2009. An Analysis of Bootstrapping for the Recognition of Temporal Expressions. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pp. 49-57, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jannik Strštgen</author>
<author>Michael Gertz</author>
</authors>
<title>HeidelTime: High Quality Rule-Based Extraction and Normalization of Temporal Expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>321--324</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="1969" citStr="Strštgen and Gertz 2010" startWordPosition="264" endWordPosition="267"> be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are ava</context>
</contexts>
<marker>Strštgen, Gertz, 2010</marker>
<rawString>Jannik Strštgen and Michael Gertz. 2010. HeidelTime: High Quality Rule-Based Extraction and Normalization of Temporal Expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 321–324, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Alicia Ageno</author>
</authors>
<title>A Hybrid Approach for the Acquisition of Information Extraction Patterns.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="3626" citStr="Surdeanu et al., 2006" startWordPosition="520" endWordPosition="524">he semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding tokens, parts-of-speech, syntactic chunks etc.) and then search for new seeds given their patterns. The pa</context>
</contexts>
<marker>Surdeanu, Turmo, Ageno, 2006</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A Hybrid Approach for the Acquisition of Information Extraction Patterns. In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 13: TempEval 2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>57--62</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="1732" citStr="Verhagen et al., 2010" startWordPosition="227" endWordPosition="230"> and WordNet provides more stable results for Wikipedia. 1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strštgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a sta</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. SemEval-2010 Task 13: TempEval 2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 57–62, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>ACL.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3520" citStr="Yarowsky, 1995" startWordPosition="508" endWordPosition="509">ictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pp. 189– 196, Cambridge, MA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>