<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030269">
<title confidence="0.985939">
Non-Local Modeling with a Mixture of PCFGs
</title>
<author confidence="0.997255">
Slav Petrov Leon Barrett Dan Klein
</author>
<affiliation confidence="0.998428">
Computer Science Division, EECS Department
University of California at Berkeley
</affiliation>
<address confidence="0.795059">
Berkeley, CA 94720
</address>
<email confidence="0.988021">
{petrov, lbarrett, klein}@eecs.berkeley.edu
</email>
<sectionHeader confidence="0.997307" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999782909090909">
While most work on parsing with PCFGs
has focused on local correlations between
tree configurations, we attempt to model
non-local correlations using a finite mix-
ture of PCFGs. A mixture grammar fit
with the EM algorithm shows improve-
ment over a single PCFG, both in parsing
accuracy and in test data likelihood. We
argue that this improvement comes from
the learning of specialized grammars that
capture non-local correlations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939380952381">
The probabilistic context-free grammar (PCFG) for-
malism is the basis of most modern statistical
parsers. The symbols in a PCFG encode context-
freedom assumptions about statistical dependencies
in the derivations of sentences, and the relative con-
ditional probabilities of the grammar rules induce
scores on trees. Compared to a basic treebank
grammar (Charniak, 1996), the grammars of high-
accuracy parsers weaken independence assumptions
by splitting grammar symbols and rules with ei-
ther lexical (Charniak, 2000; Collins, 1999) or non-
lexical (Klein and Manning, 2003; Matsuzaki et al.,
2005) conditioning information. While such split-
ting, or conditioning, can cause problems for sta-
tistical estimation, it can dramatically improve the
accuracy of a parser.
However, the configurations exploited in PCFG
parsers are quite local: rules’ probabilities may de-
pend on parents or head words, but do not depend
on arbitrarily distant tree configurations. For exam-
ple, it is generally not modeled that if one quantifier
</bodyText>
<page confidence="0.985374">
14
</page>
<bodyText confidence="0.999253935483871">
phrase (QP in the Penn Treebank) appears in a sen-
tence, the likelihood of finding another QP in that
same sentence is greatly increased. This kind of ef-
fect is neither surprising nor unknown – for exam-
ple, Bock and Loebell (1990) show experimentally
that human language generation demonstrates prim-
ing effects. The mediating variables can not only in-
clude priming effects but also genre or stylistic con-
ventions, as well as many other factors which are not
adequately modeled by local phrase structure.
A reasonable way to add a latent variable to a
generative model is to use a mixture of estimators,
in this case a mixture of PCFGs (see Section 3).
The general mixture of estimators approach was first
suggested in the statistics literature by Titterington
et al. (1962) and has since been adopted in machine
learning (Ghahramani and Jordan, 1994). In a mix-
ture approach, we have a new global variable on
which all PCFG productions for a given sentence
can be conditioned. In this paper, we experiment
with a finite mixture of PCFGs. This is similar to the
latent nonterminals used in Matsuzaki et al. (2005),
but because the latent variable we use is global, our
approach is more oriented toward learning non-local
structure. We demonstrate that a mixture fit with the
EM algorithm gives improved parsing accuracy and
test data likelihood. We then investigate what is and
is not being learned by the latent mixture variable.
While mixture components are difficult to interpret,
we demonstrate that the patterns learned are better
than random splits.
</bodyText>
<sectionHeader confidence="0.994847" genericHeader="method">
2 Empirical Motivation
</sectionHeader>
<bodyText confidence="0.949606">
It is commonly accepted that the context freedom
assumptions underlying the PCFG model are too
</bodyText>
<note confidence="0.931516">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL -X),
pages 14–20, New York City, June 2006. @c 2006 Association for Computational Linguistics
</note>
<figure confidence="0.978922904761905">
VP
#CD CD
# 2.5 billion
#CD CD
# 2.25 billion
PP
NP
TO
CD
11
NP
QP
NN
%
IN
from
VBD
increased
PP
NP
to QP
</figure>
<table confidence="0.975372090909091">
Rule Score
QP → # CD CD 131.6
PRN → -LRB- ADJP -RRB 77.1
VP → VBD NP , PP PP 33.7
VP → VBD NP NP PP 28.4
PRN → -LRB- NP -RRB- 17.3
ADJP → QP 13.3
PP → IN NP ADVP 12.3
NP → NP PRN 12.3
VP → VBN PP PP PP 11.6
ADVP → NP RBR 10.1
</table>
<figureCaption confidence="0.9127575">
Figure 1: Self-triggering: QP —* # CD CD. If one British financial occurs in the sentence, the probability of
seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation
for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most
increased in a sentence containing this rule.
</figureCaption>
<bodyText confidence="0.99988375">
strong and that weakening them results in better
models of language (Johnson, 1998; Gildea, 2001;
Klein and Manning, 2003). In particular, certain
grammar productions often cooccur with other pro-
ductions, which may be either near or distant in the
parse tree. In general, there exist three types of cor-
relations: (i) local (e.g. parent-child), (ii) non-local,
and (iii) self correlations (which may be local or
non-local).
In order to quantify the strength of a correlation,
we use a likelihood ratio (LR). For two rules X —* α
and Y —* Q, we compute
</bodyText>
<equation confidence="0.998385666666667">
P(α,Q|X,Y )
LR(X —* α, Y —* Q) =
P(α|X,Y )P(Q|X,Y )
</equation>
<bodyText confidence="0.981698863636364">
This measures how much more often the rules oc-
cur together than they would in the case of indepen-
dence. For rules that are correlated, this score will
be high (» 1); if the rules are independent, it will
be around 1, and if they are anti-correlated, it will be
near 0.
Among the correlations present in the Penn Tree-
bank, the local correlations are the strongest ones;
they contribute 65% of the rule pairs with LR scores
above 90 and 85% of those with scores over 200.
Non-local and self correlations are in general com-
mon but weaker, with non-local correlations con-
tributing approximately 85% of all correlations1. By
adding a latent variable conditioning all productions,
1Quantifying the amount of non-local correlation is prob-
lematic; most pairs of cooccuring rules are non-local and will,
due to small sample effects, have LR ratios greater than 1 even
if they were truly independent in the limit.
we aim to capture some of this interdependence be-
tween rules.
Correlations at short distances have been cap-
tured effectively in previous work (Johnson, 1998;
Klein and Manning, 2003); vertical markovization
(annotating nonterminals with their ancestor sym-
bols) does this by simply producing a different dis-
tribution for each set of ancestors. This added con-
text leads to substantial improvement in parsing ac-
curacy. With local correlations already well cap-
tured, our main motivation for introducing a mix-
ture of grammars is to capture long-range rule cooc-
currences, something that to our knowledge has not
been done successfully in the past.
As an example, the rule QP —* # CD CD, rep-
resenting a quantity of British currency, cooc-
curs with itself 132 times as often as if oc-
currences were independent. These cooccur-
rences appear in cases such as seen in Figure 1.
Similarly, the rules VP —* VBD NP PP , S and
VP —* VBG NP PP PP cooccur in the Penn Tree-
bank 100 times as often as we would expect if they
were independent. They appear in sentences of a
very particular form, telling of an action and then
giving detail about it; an example can be seen in Fig-
ure 2.
</bodyText>
<sectionHeader confidence="0.907722" genericHeader="method">
3 Mixtures of PCFGs
</sectionHeader>
<bodyText confidence="0.9990548">
In a probabilistic context-free grammar (PCFG),
each rule X —* α is associated with a conditional
probability P(α|X) (Manning and Sch¨utze, 1999).
Together, these rules induce a distribution over trees
P(T). A mixture ofPCFGs enriches the basic model
</bodyText>
<page confidence="0.8608">
15
</page>
<figure confidence="0.999799184210526">
hit
a record
in 1998
,
VP
after inflation adjustment
1.7%
rising
to $13,120
NNS
or
NN
NNS
S
NP
VP
.
CC
No
NX
NX
.
DT
NX
were present
VP
PP
NP
VBG
PP
PP
NP
,
VBD
S
lawyers tape recorders
(a) (b)
S X
</figure>
<figureCaption confidence="0.9620304">
Figure 2: Tree fragments demonstrating coocurrences. (a) and (c) Repeated formulaic structure in one
grammar: rules VP —* VBD NP PP , S and VP —* VBG NP PP PP and rules VP —* VBP RB ADJP
and VP —* VBP ADVP PP. (b) Sibling effects, though not parallel structure, rules: NX —* NNS and
NX —* NN NNS. (d) A special structure for footnotes has rules ROOT —* X and X —* SYM coocurring
with high probability.
</figureCaption>
<figure confidence="0.997414103448276">
These rate indications are n’t directly comparable lending practices vary widely by location
(c) (d)
S
NP
S
VP
;
:
NP
VP
.
.
DT
PP
NN
NNS
VBP
RB
ADJP
NN
NNS
VBP
ADVP
ADJP
VBN
Projected
X
SYM
**
</figure>
<bodyText confidence="0.999874625">
by allowing for multiple grammars, Gi, which we
call individual grammars, as opposed to a single
grammar. Without loss of generality, we can as-
sume that the individual grammars share the same
set of rules. Therefore, each original rule X —* α
is now associated with a vector of probabilities,
P(αIX, i). If, in addition, the individual grammars
are assigned prior probabilities P(i), then the entire
mixture induces a joint distribution over derivations
P(T, i) = P(i)P(T li) from which we recover a dis-
tribution over trees by summing over the grammar
index i.
As a generative derivation process, we can think
of this in two ways. First, we can imagine G to be
a latent variable on which all productions are con-
ditioned. This view emphasizes that any otherwise
unmodeled variable or variables can be captured by
the latent variable G. Second, we can imagine se-
lecting an individual grammar Gi and then gener-
ating a sentence using that grammar. This view is
associated with the expectation that there are multi-
ple grammars for a language, perhaps representing
different genres or styles. Formally, of course, the
two views are the same.
</bodyText>
<subsectionHeader confidence="0.997174">
3.1 Hierarchical Estimation
</subsectionHeader>
<bodyText confidence="0.999177785714286">
So far, there is nothing in the formal mixture model
to say that rule probabilities in one component have
any relation to those in other components. However,
we have a strong intuition that many rules, such as
NP —* DT NN, will be common in all mixture com-
ponents. Moreover, we would like to pool our data
across components when appropriate to obtain more
reliable estimators.
This can be accomplished with a hierarchical es-
timator for the rule probabilities. We introduce a
shared grammar G3. Associated to each rewrite is
now a latent variable L = IS, I} which indicates
whether the used rule was derived from the shared
grammar G3 or one of the individual grammars Gi:
</bodyText>
<equation confidence="0.9943625">
P(αlX, i) =
AP(αJX, i, E=I) + (1 − A)P(αlX, i, E=S),
</equation>
<bodyText confidence="0.997694857142857">
where A - P(E = I) is the probability of
choosing the individual grammar and can also
be viewed as a mixing coefficient. Note that
P(αIX,i, E=S) = P(αIX, E=S), since the shared
grammar is the same for all individual grammars.
This kind of hierarchical estimation is analogous to
that used in hierarchical mixtures of naive-Bayes for
</bodyText>
<page confidence="0.987105">
16
</page>
<bodyText confidence="0.999489214285714">
text categorization (McCallum et al., 1998).
The hierarchical estimator is most easily de-
scribed as a generative model. First, we choose a
individual grammar GZ. Then, for each nonterminal,
we select a level from the back-off hierarchy gram-
mar: the individual grammar GZ with probability λ,
and the shared grammar G3 with probability 1 − λ.
Finally, we select a rewrite from the chosen level. To
emphasize: the derivation of a phrase-structure tree
in a hierarchically-estimated mixture of PCFGs in-
volves two kinds of hidden variables: the grammar
G used for each sentence, and the level L used at
each tree node. These hidden variables will impact
both learning and inference in this model.
</bodyText>
<subsectionHeader confidence="0.957285">
3.2 Inference: Parsing
</subsectionHeader>
<bodyText confidence="0.999773833333333">
Parsing involves inference for a given sentence S.
One would generally like to calculate the most prob-
able parse – that is, the tree T which has the high-
est probability P(TjS) a PZ P(i)P(Tji). How-
ever, this is difficult for mixture models. For a single
grammar we have:
</bodyText>
<equation confidence="0.996561666666667">
Y
P(T, i) = P(i)
X→α∈T
</equation>
<bodyText confidence="0.9996408">
This score decomposes into a product and it is sim-
ple to construct a dynamic programming algorithm
to find the optimal T (Baker, 1979). However, for a
mixture of grammars we need to sum over the indi-
vidual grammars:
</bodyText>
<equation confidence="0.9996635">
X P(T, i) = X P(i) Y P(αjX, i).
Z Z X→α∈T
</equation>
<bodyText confidence="0.9999624375">
Because of the outer sum, this expression unfor-
tunately does not decompose into a product over
scores of subparts. In particular, a tree which maxi-
mizes the sum need not be a top tree for any single
component.
As is true for many other grammar formalisms in
which there is a derivation / parse distinction, an al-
ternative to finding the most probable parse is to find
the most probable derivation (Vijay-Shankar and
Joshi, 1985; Bod, 1992; Steedman, 2000). Instead
of finding the tree T which maximizes PZ P(T, i),
we find both the tree T and component i which max-
imize P(T, i). The most probable derivation can be
found by simply doing standard PCFG parsing once
for each component, then comparing the resulting
trees’ likelihoods.
</bodyText>
<subsectionHeader confidence="0.99779">
3.3 Learning: Training
</subsectionHeader>
<bodyText confidence="0.999619">
Training a mixture of PCFGs from a treebank is an
incomplete data problem. We need to decide which
individual grammar gave rise to a given observed
tree. Moreover, we need to select a generation path
(individual grammar or shared grammar) for each
rule in the tree. To learn estimate parameters, we
can use a standard Expectation-Maximization (EM)
approach.
In the E-step, we compute the posterior distribu-
tions of the latent variables, which are in this case
both the component G of each sentence and the hier-
archy level L of each rewrite. Note that, unlike dur-
ing parsing, there is no uncertainty over the actual
rules used, so the E-step does not require summing
over possible trees. Specifically, for the variable G
we have
</bodyText>
<equation confidence="0.996719142857143">
P(T, i)
P(ijT) =
Pj P(T, j).
For the hierarchy level L we can write
P(ℓ = IjX —* α, i, T) =
λP(αjX, ℓ=I)
λP(αjX, i, ℓ=I) + (1 − λ)P(αjX, ℓ= S),
</equation>
<bodyText confidence="0.9998294">
where we slightly abuse notation since the rule
X —* α can occur multiple times in a tree T.
In the M-step, we find the maximum-likelihood
model parameters given these posterior assign-
ments; i.e., we find the best grammars given the way
the training data’s rules are distributed between in-
dividual and shared grammars. This is done exactly
as in the standard single-grammar model using rela-
tive expected frequencies. The updates are shown in
Figure 3.3, where T = {T1, T2,... } is the training
set.
We initialize the algorithm by setting the assign-
ments from sentences to grammars to be uniform
between all the individual grammars, with a small
random perturbation to break symmetry.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999516">
We ran our experiments on the Wall Street Jour-
nal (WSJ) portion of the Penn Treebank using the
standard setup: We trained on sections 2 to 21,
and we used section 22 as a validation set for tun-
ing model hyperparameters. Results are reported
</bodyText>
<equation confidence="0.858333">
P(αjX, i).
</equation>
<page confidence="0.97835">
17
</page>
<figureCaption confidence="0.742727">
Eα′ ETkET EX,α′ETk P(i|Tk)P(ℓ = I|Tk, i, X → α′)
Figure 3: Parameter updates. The shared grammar’s parameters are re-estimated in the same manner.
</figureCaption>
<equation confidence="0.997508">
P(i) ETkET P(i|Tk) = F-Tk∈ kP(i|Tk)
Ei ETkET P(i|Tk)
P(l
ETkET EX,αETk P(ℓ = I |X → α)
= I) ←
ETkET |Tk|
P(α|X, i, ℓ = I) ←
ETkET EX,αETk P(i|Tk)P(ℓ = I|Tk, i, X → α)
</equation>
<bodyText confidence="0.999946846153846">
on all sentences of 40 words or less from section
23. We use a markovized grammar which was an-
notated with parent and sibling information as a
baseline (see Section 4.2). Unsmoothed maximum-
likelihood estimates were used for rule probabili-
ties as in Charniak (1996). For the tagging proba-
bilities, we used maximum-likelihood estimates for
P(tag|word). Add-one smoothing was applied to
unknown and rare (seen ten times or less during
training) words before inverting those estimates to
give P(word|tag). Parsing was done with a sim-
ple Java implementation of an agenda-based chart
parser.
</bodyText>
<subsectionHeader confidence="0.999786">
4.1 Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.9992165">
The EM algorithm is guaranteed to continuously in-
crease the likelihood on the training set until conver-
gence to a local maximum. However, the likelihood
on unseen data will start decreasing after a number
of iterations, due to overfitting. This is demonstrated
in Figure 4. We use the likelihood on the validation
set to stop training before overfitting occurs.
In order to evaluate the performance of our model,
we trained mixture grammars with various numbers
of components. For each configuration, we used EM
to obtain twelve estimates, each time with a different
random initialization. We show the F1-score for the
model with highest log-likelihood on the validation
set in Figure 4. The results show that a mixture of
grammars outperforms a standard, single grammar
PCFG parser.2
</bodyText>
<subsectionHeader confidence="0.998427">
4.2 Capturing Rule Correlations
</subsectionHeader>
<bodyText confidence="0.997641">
As described in Section 2, we hope that the mix-
ture model will capture long-range correlations in
</bodyText>
<footnote confidence="0.891038">
2This effect is statistically significant.
</footnote>
<bodyText confidence="0.9995901">
the data. Since local correlations can be captured
by adding parent annotation, we combine our mix-
ture model with a grammar in which node probabil-
ities depend on the parent (the last vertical ancestor)
and the closest sibling (the last horizontal ancestor).
Klein and Manning (2003) refer to this grammar as
a markovized grammar of vertical order = 2 and hor-
izontal order = 1. Because many local correlations
are captured by the markovized grammar, there is a
greater hope that observed improvements stem from
non-local correlations.
In fact, we find that the mixture does capture
non-local correlations. We measure the degree to
which a grammar captures correlations by calculat-
ing the total squared error between LR scores of the
grammar and corpus, weighted by the probability
of seeing nonterminals. This is 39422 for a sin-
gle PCFG, but drops to 37125 for a mixture with
five individual grammars, indicating that the mix-
ture model better captures the correlations present
in the corpus. As a concrete example, in the Penn
Treebank, we often see the rules FRAG → ADJP
and PRN → , SBAR , cooccurring; their LR is 134.
When we learn a single markovized PCFG from the
treebank, that grammar gives a likelihood ratio of
only 61. However, when we train with a hierarchi-
cal model composed of a shared grammar and four
individual grammars, we find that the grammar like-
lihood ratio for these rules goes up to 126, which is
very similar to that of the empirical ratio.
</bodyText>
<subsectionHeader confidence="0.97759">
4.3 Genre
</subsectionHeader>
<bodyText confidence="0.9999395">
The mixture of grammars model can equivalently be
viewed as capturing either non-local correlations or
variations in grammar. The latter view suggests that
the model might benefit when the syntactic structure
</bodyText>
<page confidence="0.99043">
18
</page>
<figure confidence="0.998536375">
Training data
Validation data
Testing data
Log Likelihood
F1
79.8
79.6
79.4
79.2
80
79
Mixture model
Baseline: 1 grammar
0 10 20 30 40 50 60 1 2 3 4 5 6 7 8 9
Iteration Number of Component Grammars
(a) (b)
</figure>
<figureCaption confidence="0.85495725">
Figure 4: (a) Log likelihood of training, validation, and test data during training (transformed to fit on the
same plot). Note that when overfitting occurs the likelihood on the validation and test data starts decreasing
(after 13 iterations). (b) The accuracy of the mixture of grammars model with A = 0.4 versus the number of
grammars. Note the improvement over a 1-grammar PCFG model.
</figureCaption>
<bodyText confidence="0.99993565">
varies significantly, as between different genres. We
tested this with the Brown corpus, of which we used
8 different genres (f, g, k, l, m, n, p, and r). We fol-
low Gildea (2001) in using the ninth and tenth sen-
tences of every block of ten as validation and test
data, respectively, because a contiguous test section
might not be representative due to the genre varia-
tion.
To test the effects of genre variation, we evalu-
ated various training schemes on the Brown corpus.
The single grammar baseline for this corpus gives
F1 = 79.75, with log likelihood (LL) on the testing
data=-242561. The first test, then, was to estimate
each individual grammar from only one genre. We
did this by assigning sentences to individual gram-
mars by genre, without using any EM training. This
increases the data likelihood, though it reduces the
F1 score (F1 = 79.48, LL=-242332). The increase
in likelihood indicates that there are genre-specific
features that our model can represent. (The lack of
F1 improvement may be attributed to the increased
difficulty of estimating rule probabilities after divid-
ing the already scant data available in the Brown cor-
pus. This small quantity of data makes overfitting
almost certain.)
However, local minima and lack of data cause dif-
ficulty in learning genre-specific features. If we start
with sentences assigned by genre as before, but then
train with EM, both F1 and test data log likelihood
drop (F1 = 79.37, LL=-242100). When we use
EM with a random initialization, so that sentences
are not assigned directly to grammars, the scores go
down even further (F1 = 79.16, LL=-242459). This
indicates that the model can capture variation be-
tween genres, but that maximum training data likeli-
hood does not necessarily give maximum accuracy.
Presumably, with more genre-specific data avail-
able, learning would generalize better. So, genre-
specific grammar variation is real, but it is difficult
to capture via EM.
</bodyText>
<subsectionHeader confidence="0.99784">
4.4 Smoothing Effects
</subsectionHeader>
<bodyText confidence="0.999974">
While the mixture of grammars captures rule cor-
relations, it may also enhance performance via
smoothing effects. Splitting the data randomly could
produce a smoothed shared grammar, G3, that is
a kind of held-out estimate which could be supe-
rior to the unsmoothed ML estimates for the single-
component grammar.
We tested the degree of generalization by eval-
uating the shared grammar alone and also a mix-
ture of the shared grammar with the known sin-
gle grammar. Those shared grammars were ex-
tracted after training the mixture model with four in-
dividual grammars. We found that both the shared
grammar alone (F1=79.13, LL=-333278) and the
shared grammar mixed with the single grammar
(F1=79.36, LL=-331546) perform worse than a sin-
</bodyText>
<page confidence="0.996869">
19
</page>
<bodyText confidence="0.94365">
gle PCFG (Fi=79.37, LL=-327658). This indicates
that smoothing is not the primary learning effect
contributing to increased Fl.
</bodyText>
<sectionHeader confidence="0.99959" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999904555555556">
We examined the sorts of rule correlations that may
be found in natural language corpora, discovering
non-local correlations not captured by traditional
models. We found that using a model capable of
representing these non-local features gives improve-
ment in parsing accuracy and data likelihood. This
improvement is modest, however, primarily because
local correlations are so much stronger than non-
local ones.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99966208">
J. Baker. 1979. Trainable grammars for speech recog-
nition. Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America, pages
547–550.
K. Bock and H. Loebell. 1990. Framing sentences. Cog-
nition, 35:1–39.
R. Bod. 1992. A computational model of language per-
formance: Data oriented parsing. International Con-
ference on Computational Linguistics (COLING).
E. Charniak. 1996. Tree-bank grammars. In Proc. of
the 13th National Conference on Artificial Intelligence
(AAAI), pages 1031–1036.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In Proc. of the Conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL), pages 132–139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
Z. Ghahramani and M. I. Jordan. 1994. Supervised
learning from incomplete data via an EM approach. In
Advances in Neural Information Processing Systems
(NIPS), pages 120–127.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. Conference on Empirical Methods in Natural
Language Processing (EMNLP).
M. Johnson. 1998. Pcfg models of linguistic tree repre-
sentations. Computational Linguistics, 24:613–632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. Proc. of the 41st Meeting of the Association
for Computational Linguistics (ACL), pages 423–430.
C. Manning and H. Sch¨utze. 1999. Foundations of Sta-
tistical Natural Language Processing. The MIT Press,
Cambridge, Massachusetts.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of the 43rd
Meeting ofthe Association for Computational Linguis-
tics (ACL), pages 75–82.
A. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.
1998. Improving text classification by shrinkage in a
hierarchy of classes. In Int. Conf. on Machine Learn-
ing (ICML), pages 359–367.
M. Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Massachusetts.
D. Titterington, A. Smith, and U. Makov. 1962. Statisti-
cal Analysis ofFinite Mixture Distributions. Wiley.
K. Vijay-Shankar and A. Joshi. 1985. Some computa-
tional properties of tree adjoining grammars. Proc. of
the 23th Meeting of the Association for Computational
Linguistics (ACL), pages 82–93.
</reference>
<page confidence="0.994816">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949771">
<title confidence="0.999069">Non-Local Modeling with a Mixture of PCFGs</title>
<author confidence="0.999878">Slav Petrov Leon Barrett Dan Klein</author>
<affiliation confidence="0.9976465">Computer Science Division, EECS University of California at</affiliation>
<address confidence="0.982153">Berkeley, CA</address>
<email confidence="0.976124">lbarrett,</email>
<abstract confidence="0.9995595">While most work on parsing with PCFGs has focused on local correlations between tree configurations, we attempt to model non-local correlations using a finite mixture of PCFGs. A mixture grammar fit with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood. We argue that this improvement comes from the learning of specialized grammars that capture non-local correlations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baker</author>
</authors>
<title>Trainable grammars for speech recognition. Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</title>
<date>1979</date>
<pages>547--550</pages>
<contexts>
<context position="11347" citStr="Baker, 1979" startWordPosition="1965" endWordPosition="1966">bles: the grammar G used for each sentence, and the level L used at each tree node. These hidden variables will impact both learning and inference in this model. 3.2 Inference: Parsing Parsing involves inference for a given sentence S. One would generally like to calculate the most probable parse – that is, the tree T which has the highest probability P(TjS) a PZ P(i)P(Tji). However, this is difficult for mixture models. For a single grammar we have: Y P(T, i) = P(i) X→α∈T This score decomposes into a product and it is simple to construct a dynamic programming algorithm to find the optimal T (Baker, 1979). However, for a mixture of grammars we need to sum over the individual grammars: X P(T, i) = X P(i) Y P(αjX, i). Z Z X→α∈T Because of the outer sum, this expression unfortunately does not decompose into a product over scores of subparts. In particular, a tree which maximizes the sum need not be a top tree for any single component. As is true for many other grammar formalisms in which there is a derivation / parse distinction, an alternative to finding the most probable parse is to find the most probable derivation (Vijay-Shankar and Joshi, 1985; Bod, 1992; Steedman, 2000). Instead of finding </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. Baker. 1979. Trainable grammars for speech recognition. Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bock</author>
<author>H Loebell</author>
</authors>
<title>Framing sentences.</title>
<date>1990</date>
<journal>Cognition,</journal>
<pages>35--1</pages>
<contexts>
<context position="1919" citStr="Bock and Loebell (1990)" startWordPosition="292" endWordPosition="295">e such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier 14 phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of effect is neither surprising nor unknown – for example, Bock and Loebell (1990) show experimentally that human language generation demonstrates priming effects. The mediating variables can not only include priming effects but also genre or stylistic conventions, as well as many other factors which are not adequately modeled by local phrase structure. A reasonable way to add a latent variable to a generative model is to use a mixture of estimators, in this case a mixture of PCFGs (see Section 3). The general mixture of estimators approach was first suggested in the statistics literature by Titterington et al. (1962) and has since been adopted in machine learning (Ghahrama</context>
</contexts>
<marker>Bock, Loebell, 1990</marker>
<rawString>K. Bock and H. Loebell. 1990. Framing sentences. Cognition, 35:1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A computational model of language performance:</title>
<date>1992</date>
<booktitle>Data oriented parsing. International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="11909" citStr="Bod, 1992" startWordPosition="2069" endWordPosition="2070">g algorithm to find the optimal T (Baker, 1979). However, for a mixture of grammars we need to sum over the individual grammars: X P(T, i) = X P(i) Y P(αjX, i). Z Z X→α∈T Because of the outer sum, this expression unfortunately does not decompose into a product over scores of subparts. In particular, a tree which maximizes the sum need not be a top tree for any single component. As is true for many other grammar formalisms in which there is a derivation / parse distinction, an alternative to finding the most probable parse is to find the most probable derivation (Vijay-Shankar and Joshi, 1985; Bod, 1992; Steedman, 2000). Instead of finding the tree T which maximizes PZ P(T, i), we find both the tree T and component i which maximize P(T, i). The most probable derivation can be found by simply doing standard PCFG parsing once for each component, then comparing the resulting trees’ likelihoods. 3.3 Learning: Training Training a mixture of PCFGs from a treebank is an incomplete data problem. We need to decide which individual grammar gave rise to a given observed tree. Moreover, we need to select a generation path (individual grammar or shared grammar) for each rule in the tree. To learn estimat</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>R. Bod. 1992. A computational model of language performance: Data oriented parsing. International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proc. of the 13th National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1031--1036</pages>
<contexts>
<context position="1040" citStr="Charniak, 1996" startWordPosition="152" endWordPosition="153">fit with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood. We argue that this improvement comes from the learning of specialized grammars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it i</context>
<context position="14626" citStr="Charniak (1996)" startWordPosition="2551" endWordPosition="2552">. Results are reported P(αjX, i). 17 Eα′ ETkET EX,α′ETk P(i|Tk)P(ℓ = I|Tk, i, X → α′) Figure 3: Parameter updates. The shared grammar’s parameters are re-estimated in the same manner. P(i) ETkET P(i|Tk) = F-Tk∈ kP(i|Tk) Ei ETkET P(i|Tk) P(l ETkET EX,αETk P(ℓ = I |X → α) = I) ← ETkET |Tk| P(α|X, i, ℓ = I) ← ETkET EX,αETk P(i|Tk)P(ℓ = I|Tk, i, X → α) on all sentences of 40 words or less from section 23. We use a markovized grammar which was annotated with parent and sibling information as a baseline (see Section 4.2). Unsmoothed maximumlikelihood estimates were used for rule probabilities as in Charniak (1996). For the tagging probabilities, we used maximum-likelihood estimates for P(tag|word). Add-one smoothing was applied to unknown and rare (seen ten times or less during training) words before inverting those estimates to give P(word|tag). Parsing was done with a simple Java implementation of an agenda-based chart parser. 4.1 Parsing Accuracy The EM algorithm is guaranteed to continuously increase the likelihood on the training set until convergence to a local maximum. However, the likelihood on unseen data will start decreasing after a number of iterations, due to overfitting. This is demonstra</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-bank grammars. In Proc. of the 13th National Conference on Artificial Intelligence (AAAI), pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum–entropy–inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of the Conference of the North American chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1185" citStr="Charniak, 2000" startWordPosition="173" endWordPosition="174">nt comes from the learning of specialized grammars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier 14 phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum–entropy–inspired parser. In Proc. of the Conference of the North American chapter of the Association for Computational Linguistics (NAACL), pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="1201" citStr="Collins, 1999" startWordPosition="175" endWordPosition="176">e learning of specialized grammars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier 14 phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sente</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Ghahramani</author>
<author>M I Jordan</author>
</authors>
<title>Supervised learning from incomplete data via an EM approach.</title>
<date>1994</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>120--127</pages>
<contexts>
<context position="2539" citStr="Ghahramani and Jordan, 1994" startWordPosition="393" endWordPosition="396">l (1990) show experimentally that human language generation demonstrates priming effects. The mediating variables can not only include priming effects but also genre or stylistic conventions, as well as many other factors which are not adequately modeled by local phrase structure. A reasonable way to add a latent variable to a generative model is to use a mixture of estimators, in this case a mixture of PCFGs (see Section 3). The general mixture of estimators approach was first suggested in the statistics literature by Titterington et al. (1962) and has since been adopted in machine learning (Ghahramani and Jordan, 1994). In a mixture approach, we have a new global variable on which all PCFG productions for a given sentence can be conditioned. In this paper, we experiment with a finite mixture of PCFGs. This is similar to the latent nonterminals used in Matsuzaki et al. (2005), but because the latent variable we use is global, our approach is more oriented toward learning non-local structure. We demonstrate that a mixture fit with the EM algorithm gives improved parsing accuracy and test data likelihood. We then investigate what is and is not being learned by the latent mixture variable. While mixture compone</context>
</contexts>
<marker>Ghahramani, Jordan, 1994</marker>
<rawString>Z. Ghahramani and M. I. Jordan. 1994. Supervised learning from incomplete data via an EM approach. In Advances in Neural Information Processing Systems (NIPS), pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4338" citStr="Gildea, 2001" startWordPosition="723" endWordPosition="724">7 VP → VBD NP NP PP 28.4 PRN → -LRB- NP -RRB- 17.3 ADJP → QP 13.3 PP → IN NP ADVP 12.3 NP → NP PRN 12.3 VP → VBN PP PP PP 11.6 ADVP → NP RBR 10.1 Figure 1: Self-triggering: QP —* # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X —* α and Y —* Q, we compute P(α,Q|X,Y ) LR(X —* α, Y —* Q) = P(α|X,Y )P(Q|X,Y ) This measures how much more often the rules occur together than they would in the case of ind</context>
<context position="18386" citStr="Gildea (2001)" startWordPosition="3181" endWordPosition="3182">teration Number of Component Grammars (a) (b) Figure 4: (a) Log likelihood of training, validation, and test data during training (transformed to fit on the same plot). Note that when overfitting occurs the likelihood on the validation and test data starts decreasing (after 13 iterations). (b) The accuracy of the mixture of grammars model with A = 0.4 versus the number of grammars. Note the improvement over a 1-grammar PCFG model. varies significantly, as between different genres. We tested this with the Brown corpus, of which we used 8 different genres (f, g, k, l, m, n, p, and r). We follow Gildea (2001) in using the ninth and tenth sentences of every block of ten as validation and test data, respectively, because a contiguous test section might not be representative due to the genre variation. To test the effects of genre variation, we evaluated various training schemes on the Brown corpus. The single grammar baseline for this corpus gives F1 = 79.75, with log likelihood (LL) on the testing data=-242561. The first test, then, was to estimate each individual grammar from only one genre. We did this by assigning sentences to individual grammars by genre, without using any EM training. This inc</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Corpus variation and parser performance. Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Pcfg models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--613</pages>
<contexts>
<context position="4324" citStr="Johnson, 1998" startWordPosition="721" endWordPosition="722"> NP , PP PP 33.7 VP → VBD NP NP PP 28.4 PRN → -LRB- NP -RRB- 17.3 ADJP → QP 13.3 PP → IN NP ADVP 12.3 NP → NP PRN 12.3 VP → VBN PP PP PP 11.6 ADVP → NP RBR 10.1 Figure 1: Self-triggering: QP —* # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X —* α and Y —* Q, we compute P(α,Q|X,Y ) LR(X —* α, Y —* Q) = P(α|X,Y )P(Q|X,Y ) This measures how much more often the rules occur together than they would in t</context>
<context position="5905" citStr="Johnson, 1998" startWordPosition="993" endWordPosition="994"> with scores over 200. Non-local and self correlations are in general common but weaker, with non-local correlations contributing approximately 85% of all correlations1. By adding a latent variable conditioning all productions, 1Quantifying the amount of non-local correlation is problematic; most pairs of cooccuring rules are non-local and will, due to small sample effects, have LR ratios greater than 1 even if they were truly independent in the limit. we aim to capture some of this interdependence between rules. Correlations at short distances have been captured effectively in previous work (Johnson, 1998; Klein and Manning, 2003); vertical markovization (annotating nonterminals with their ancestor symbols) does this by simply producing a different distribution for each set of ancestors. This added context leads to substantial improvement in parsing accuracy. With local correlations already well captured, our main motivation for introducing a mixture of grammars is to capture long-range rule cooccurrences, something that to our knowledge has not been done successfully in the past. As an example, the rule QP —* # CD CD, representing a quantity of British currency, cooccurs with itself 132 times</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24:613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proc. of the 41st Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1240" citStr="Klein and Manning, 2003" startWordPosition="180" endWordPosition="183">mars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier 14 phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of </context>
<context position="4364" citStr="Klein and Manning, 2003" startWordPosition="725" endWordPosition="728">NP PP 28.4 PRN → -LRB- NP -RRB- 17.3 ADJP → QP 13.3 PP → IN NP ADVP 12.3 NP → NP PRN 12.3 VP → VBN PP PP PP 11.6 ADVP → NP RBR 10.1 Figure 1: Self-triggering: QP —* # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X —* α and Y —* Q, we compute P(α,Q|X,Y ) LR(X —* α, Y —* Q) = P(α|X,Y )P(Q|X,Y ) This measures how much more often the rules occur together than they would in the case of independence. For rules that </context>
<context position="5931" citStr="Klein and Manning, 2003" startWordPosition="995" endWordPosition="998">er 200. Non-local and self correlations are in general common but weaker, with non-local correlations contributing approximately 85% of all correlations1. By adding a latent variable conditioning all productions, 1Quantifying the amount of non-local correlation is problematic; most pairs of cooccuring rules are non-local and will, due to small sample effects, have LR ratios greater than 1 even if they were truly independent in the limit. we aim to capture some of this interdependence between rules. Correlations at short distances have been captured effectively in previous work (Johnson, 1998; Klein and Manning, 2003); vertical markovization (annotating nonterminals with their ancestor symbols) does this by simply producing a different distribution for each set of ancestors. This added context leads to substantial improvement in parsing accuracy. With local correlations already well captured, our main motivation for introducing a mixture of grammars is to capture long-range rule cooccurrences, something that to our knowledge has not been done successfully in the past. As an example, the rule QP —* # CD CD, representing a quantity of British currency, cooccurs with itself 132 times as often as if occurrence</context>
<context position="16209" citStr="Klein and Manning (2003)" startWordPosition="2798" endWordPosition="2801">he model with highest log-likelihood on the validation set in Figure 4. The results show that a mixture of grammars outperforms a standard, single grammar PCFG parser.2 4.2 Capturing Rule Correlations As described in Section 2, we hope that the mixture model will capture long-range correlations in 2This effect is statistically significant. the data. Since local correlations can be captured by adding parent annotation, we combine our mixture model with a grammar in which node probabilities depend on the parent (the last vertical ancestor) and the closest sibling (the last horizontal ancestor). Klein and Manning (2003) refer to this grammar as a markovized grammar of vertical order = 2 and horizontal order = 1. Because many local correlations are captured by the markovized grammar, there is a greater hope that observed improvements stem from non-local correlations. In fact, we find that the mixture does capture non-local correlations. We measure the degree to which a grammar captures correlations by calculating the total squared error between LR scores of the grammar and corpus, weighted by the probability of seeing nonterminals. This is 39422 for a single PCFG, but drops to 37125 for a mixture with five in</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. Proc. of the 41st Meeting of the Association for Computational Linguistics (ACL), pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Meeting ofthe Association for Computational Linguistics (ACL),</booktitle>
<pages>75--82</pages>
<contexts>
<context position="1265" citStr="Matsuzaki et al., 2005" startWordPosition="184" endWordPosition="187">al correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier 14 phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of effect is neither surpris</context>
<context position="2800" citStr="Matsuzaki et al. (2005)" startWordPosition="440" endWordPosition="443">rase structure. A reasonable way to add a latent variable to a generative model is to use a mixture of estimators, in this case a mixture of PCFGs (see Section 3). The general mixture of estimators approach was first suggested in the statistics literature by Titterington et al. (1962) and has since been adopted in machine learning (Ghahramani and Jordan, 1994). In a mixture approach, we have a new global variable on which all PCFG productions for a given sentence can be conditioned. In this paper, we experiment with a finite mixture of PCFGs. This is similar to the latent nonterminals used in Matsuzaki et al. (2005), but because the latent variable we use is global, our approach is more oriented toward learning non-local structure. We demonstrate that a mixture fit with the EM algorithm gives improved parsing accuracy and test data likelihood. We then investigate what is and is not being learned by the latent mixture variable. While mixture components are difficult to interpret, we demonstrate that the patterns learned are better than random splits. 2 Empirical Motivation It is commonly accepted that the context freedom assumptions underlying the PCFG model are too Proceedings of the 10th Conference on C</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of the 43rd Meeting ofthe Association for Computational Linguistics (ACL), pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>R Rosenfeld</author>
<author>T Mitchell</author>
<author>A Ng</author>
</authors>
<title>Improving text classification by shrinkage in a hierarchy of classes.</title>
<date>1998</date>
<booktitle>In Int. Conf. on Machine Learning (ICML),</booktitle>
<pages>359--367</pages>
<contexts>
<context position="10248" citStr="McCallum et al., 1998" startWordPosition="1773" endWordPosition="1776">grammar G3. Associated to each rewrite is now a latent variable L = IS, I} which indicates whether the used rule was derived from the shared grammar G3 or one of the individual grammars Gi: P(αlX, i) = AP(αJX, i, E=I) + (1 − A)P(αlX, i, E=S), where A - P(E = I) is the probability of choosing the individual grammar and can also be viewed as a mixing coefficient. Note that P(αIX,i, E=S) = P(αIX, E=S), since the shared grammar is the same for all individual grammars. This kind of hierarchical estimation is analogous to that used in hierarchical mixtures of naive-Bayes for 16 text categorization (McCallum et al., 1998). The hierarchical estimator is most easily described as a generative model. First, we choose a individual grammar GZ. Then, for each nonterminal, we select a level from the back-off hierarchy grammar: the individual grammar GZ with probability λ, and the shared grammar G3 with probability 1 − λ. Finally, we select a rewrite from the chosen level. To emphasize: the derivation of a phrase-structure tree in a hierarchically-estimated mixture of PCFGs involves two kinds of hidden variables: the grammar G used for each sentence, and the level L used at each tree node. These hidden variables will i</context>
</contexts>
<marker>McCallum, Rosenfeld, Mitchell, Ng, 1998</marker>
<rawString>A. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng. 1998. Improving text classification by shrinkage in a hierarchy of classes. In Int. Conf. on Machine Learning (ICML), pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="11926" citStr="Steedman, 2000" startWordPosition="2071" endWordPosition="2072"> to find the optimal T (Baker, 1979). However, for a mixture of grammars we need to sum over the individual grammars: X P(T, i) = X P(i) Y P(αjX, i). Z Z X→α∈T Because of the outer sum, this expression unfortunately does not decompose into a product over scores of subparts. In particular, a tree which maximizes the sum need not be a top tree for any single component. As is true for many other grammar formalisms in which there is a derivation / parse distinction, an alternative to finding the most probable parse is to find the most probable derivation (Vijay-Shankar and Joshi, 1985; Bod, 1992; Steedman, 2000). Instead of finding the tree T which maximizes PZ P(T, i), we find both the tree T and component i which maximize P(T, i). The most probable derivation can be found by simply doing standard PCFG parsing once for each component, then comparing the resulting trees’ likelihoods. 3.3 Learning: Training Training a mixture of PCFGs from a treebank is an incomplete data problem. We need to decide which individual grammar gave rise to a given observed tree. Moreover, we need to select a generation path (individual grammar or shared grammar) for each rule in the tree. To learn estimate parameters, we </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Titterington</author>
<author>A Smith</author>
<author>U Makov</author>
</authors>
<title>Statistical Analysis ofFinite Mixture Distributions.</title>
<date>1962</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="2462" citStr="Titterington et al. (1962)" startWordPosition="381" endWordPosition="384"> of effect is neither surprising nor unknown – for example, Bock and Loebell (1990) show experimentally that human language generation demonstrates priming effects. The mediating variables can not only include priming effects but also genre or stylistic conventions, as well as many other factors which are not adequately modeled by local phrase structure. A reasonable way to add a latent variable to a generative model is to use a mixture of estimators, in this case a mixture of PCFGs (see Section 3). The general mixture of estimators approach was first suggested in the statistics literature by Titterington et al. (1962) and has since been adopted in machine learning (Ghahramani and Jordan, 1994). In a mixture approach, we have a new global variable on which all PCFG productions for a given sentence can be conditioned. In this paper, we experiment with a finite mixture of PCFGs. This is similar to the latent nonterminals used in Matsuzaki et al. (2005), but because the latent variable we use is global, our approach is more oriented toward learning non-local structure. We demonstrate that a mixture fit with the EM algorithm gives improved parsing accuracy and test data likelihood. We then investigate what is a</context>
</contexts>
<marker>Titterington, Smith, Makov, 1962</marker>
<rawString>D. Titterington, A. Smith, and U. Makov. 1962. Statistical Analysis ofFinite Mixture Distributions. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shankar</author>
<author>A Joshi</author>
</authors>
<title>Some computational properties of tree adjoining grammars.</title>
<date>1985</date>
<booktitle>Proc. of the 23th Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>82--93</pages>
<contexts>
<context position="11898" citStr="Vijay-Shankar and Joshi, 1985" startWordPosition="2065" endWordPosition="2068"> construct a dynamic programming algorithm to find the optimal T (Baker, 1979). However, for a mixture of grammars we need to sum over the individual grammars: X P(T, i) = X P(i) Y P(αjX, i). Z Z X→α∈T Because of the outer sum, this expression unfortunately does not decompose into a product over scores of subparts. In particular, a tree which maximizes the sum need not be a top tree for any single component. As is true for many other grammar formalisms in which there is a derivation / parse distinction, an alternative to finding the most probable parse is to find the most probable derivation (Vijay-Shankar and Joshi, 1985; Bod, 1992; Steedman, 2000). Instead of finding the tree T which maximizes PZ P(T, i), we find both the tree T and component i which maximize P(T, i). The most probable derivation can be found by simply doing standard PCFG parsing once for each component, then comparing the resulting trees’ likelihoods. 3.3 Learning: Training Training a mixture of PCFGs from a treebank is an incomplete data problem. We need to decide which individual grammar gave rise to a given observed tree. Moreover, we need to select a generation path (individual grammar or shared grammar) for each rule in the tree. To le</context>
</contexts>
<marker>Vijay-Shankar, Joshi, 1985</marker>
<rawString>K. Vijay-Shankar and A. Joshi. 1985. Some computational properties of tree adjoining grammars. Proc. of the 23th Meeting of the Association for Computational Linguistics (ACL), pages 82–93.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>