<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000587">
<title confidence="0.994801">
Adaptive Quality Estimation for Machine Translation
</title>
<author confidence="0.604843">
Marco Turchi(1) Antonios Anastasopoulos(3)
Jos´e G. C. de Souza(1 2) Matteo Negri(1)
</author>
<affiliation confidence="0.544589">
(1) FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2) University of Trento, Italy
(3) National Technical University of Athens, Greece
</affiliation>
<email confidence="0.990355">
{turchi,desouza,negri}@fbk.eu
anastasopoulos.ant@gmail.com
</email>
<sectionHeader confidence="0.993807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992363636364">
The automatic estimation of machine
translation (MT) output quality is a hard
task in which the selection of the appro-
priate algorithm and the most predictive
features over reasonably sized training sets
plays a crucial role. When moving from
controlled lab evaluations to real-life sce-
narios the task becomes even harder. For
current MT quality estimation (QE) sys-
tems, additional complexity comes from
the difficulty to model user and domain
changes. Indeed, the instability of the sys-
tems with respect to data coming from dif-
ferent distributions calls for adaptive so-
lutions that react to new operating con-
ditions. To tackle this issue we propose
an online framework for adaptive QE that
targets reactivity and robustness to user
and domain changes. Contrastive exper-
iments in different testing conditions in-
volving user and domain changes demon-
strate the effectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999993594594595">
After two decades of steady progress, research
in statistical machine translation (SMT) started to
cross its path with translation industry with tan-
gible mutual benefit. On one side, SMT research
brings to the industry improved output quality and
a number of appealing solutions useful to increase
translators’ productivity. On the other side, the
market needs suggest concrete problems to solve,
providing real-life scenarios to develop and eval-
uate new ideas with rapid turnaround. The evolu-
tion of computer-assisted translation (CAT) envi-
ronments is an evidence of this trend, shown by
the increasing interest towards the integration of
suggestions obtained from MT engines with those
derived from translation memories (TMs).
The possibility to speed up the translation pro-
cess and reduce its costs by post-editing good-
quality MT output raises interesting research chal-
lenges. Among others, these include deciding
what to present as a suggestion, and how to do it
in the most effective way.
In recent years, these issues motivated research
on automatic QE, which addresses the problem
of estimating the quality of a translated sentence
given the source and without access to reference
translations (Blatz et al., 2003; Specia et al., 2009;
Mehdad et al., 2012). Despite the substantial
progress done so far in the field and in success-
ful evaluation campaigns (Callison-Burch et al.,
2012; Bojar et al., 2013), focusing on concrete
market needs makes possible to further define the
scope of research on QE. For instance, moving
from controlled lab testing scenarios to real work-
ing environments poses additional constraints in
terms of adaptability of the QE models to the vari-
able conditions of a translation job. Such variabil-
ity is due to two main reasons:
</bodyText>
<listItem confidence="0.976308647058823">
1. The notion of MT output quality is highly
subjective (Koponen, 2012; Turchi et al.,
2013; Turchi and Negri, 2014). Since the
quality standards of individual users may
vary considerably (e.g. according to their
knowledge of the source and target lan-
guages), the estimates of a static QE model
trained with data collected from a group of
post-editors might not fit with the actual
judgements of a new user;
2. Each translation job has its own specifici-
ties (domain, complexity of the source text,
average target quality). Since data from a
new job may differ from those used to train
the QE model, its estimates on the new in-
stances might result to be biased or uninfor-
mative.
</listItem>
<bodyText confidence="0.788346">
The ability of a system to self-adapt to the be-
</bodyText>
<page confidence="0.950422">
710
</page>
<note confidence="0.8305185">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 710–720,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99975446875">
haviour of specific users and domain changes is
a facet of the QE problem that so far has been
disregarded. To cope with these issues and deal
with the erratic conditions of real-world trans-
lation workflows, we propose an adaptive ap-
proach to QE that is sensitive and robust to dif-
ferences between training and test data. Along this
direction, our main contribution is a framework in
which QE models can be trained and can continu-
ously evolve over time accounting for knowledge
acquired from post editors’ work.
Our approach is based on the online learning
paradigm and exploits a key difference between
such framework and the batch learning methods
currently used. On one side, the QE models ob-
tained with batch methods are learned exclusively
from a predefined set of training examples under
the assumption that they have similar characteris-
tics with respect to the test data. This makes them
suitable for controlled evaluation scenarios where
such condition holds. On the other side, online
learning techniques are designed to learn in a step-
wise manner (either from scratch, or by refining an
existing model) from new, unseen test instances
by taking advantage of external feedback. This
makes them suitable for real-life scenarios where
the new instances to be labelled can considerably
differ from the data used to train the QE model.
To develop our approach, different online algo-
rithms have been embedded in the backbone of
a QE system. This required the adaptation of its
standard batch learning workflow to:
</bodyText>
<listItem confidence="0.979316666666667">
1. Perform online feature extraction from a
source–target pair (i.e. one instance at a time
instead of processing an entire training set);
2. Emit a prediction for the input instance;
3. Gather user feedback for the instance (i.e.
calculating a “true label” based on the
amount of user post-editions);
4. Send the true label back to the model to up-
date its predictions for future instances.
</listItem>
<bodyText confidence="0.999752133333333">
Focusing on the adaptability to user and domain
changes, we report the results of comparative ex-
periments with two online algorithms and the stan-
dard batch approach. The evaluation is carried out
by measuring the global error of each algorithm
on test sets featuring different degrees of similar-
ity with the data used for training. Our results
show that the sensitivity of online QE models to
different distributions of training and test instances
makes them more suitable than batch methods for
integration in a CAT framework.
Our adaptive QE infrastructure has been re-
leased as open source. Its C++ implementation is
available at http://hlt.fbk.eu/technologies/
aqet.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999858878787879">
QE is generally cast as a supervised machine
learning task, where a model trained from a col-
lection of (source, target, label) instances is used
to predict labels1 for new, unseen test items (Spe-
cia et al., 2010).
In the last couple of years, research in the field
received a strong boost by the shared tasks orga-
nized within the WMT workshop on SMT,2 which
is also the framework of our first experiment in
§5. Current approaches to the tasks proposed at
WMT have mainly focused on three main direc-
tions, namely: i) feature engineering, as in (Hard-
meier et al., 2012; de Souza et al., 2013a; de Souza
et al., 2013b; Rubino et al., 2013b), ii) model
learning with a variety of classification and regres-
sion algorithms, as in (Bicici, 2013; Beck et al.,
2013; Soricut et al., 2012), and iii) feature selec-
tion as a way to overcome sparsity and overfitting
issues, as in (Soricut et al., 2012).
Being optimized to perform well on specific
WMT sub-tasks and datasets, current systems re-
flect variations along these directions but leave im-
portant aspects of the QE problem still partially
investigated or totally unexplored.3 Among these,
the necessity to model the diversity of human qual-
ity judgements and correction strategies (Kopo-
nen, 2012; Koponen et al., 2012) calls for solu-
tions that: i) account for annotator-specific be-
haviour, thus being capable of learning from inher-
ently noisy datasets produced by multiple annota-
tors, and ii) self-adapt to changes in data distribu-
tion, learning from user feedback on new, unseen
test items.
</bodyText>
<footnote confidence="0.939536888888889">
1Possible label types include post-editing effort scores
(e.g. 1-5 Likert scores indicating the estimated percentage
of MT output that has to be corrected), HTER values (Snover
et al., 2006), and post-editing time (e.g. seconds per word).
2http://www.statmt.org/wmt13/
3For a comprehensive overview of the QE approaches
proposed so far we refer the reader to the WMT12 and
WMT13 QE shared task reports (Callison-Burch et al., 2012;
Bojar et al., 2013).
</footnote>
<page confidence="0.997676">
711
</page>
<bodyText confidence="0.999961333333333">
These interconnected issues are particularly rel-
evant in the CAT framework, where translation
jobs from different domains are routed to pro-
fessional translators with different idiolect, back-
ground and quality standards.
The first aspect, modelling annotators’ individ-
ual behaviour and interdependences, has been ad-
dressed by Cohn and Specia (2013), who explored
multi-task Gaussian Processes as a way to jointly
learn from the output of multiple annotations. This
technique is suitable to cope with the unbalanced
distribution of training instances and yields better
models when heterogeneous training datasets are
available.
The second problem, the adaptability of QE
models, has not been explored yet. A common
trait of all current approaches, in fact, is the re-
liance on batch learning techniques, which assume
a “static” nature of the world where new unseen
instances that will be encountered will be similar
to the training data.4 However, similarly to trans-
lation memories that incrementally store translated
segments and evolve over time incorporating users
style and terminology, all components of a CAT
tool (the MT engine and the mechanisms to assign
quality scores to the suggested translations) should
take advantage of translators feedback.
On the MT system side, research on adaptive
approaches tailored to interactive SMT and CAT
scenarios explored the online learning protocol
(Littlestone, 1988) to improve various aspects of
the decoding process (Cesa-Bianchi et al., 2008;
Ortiz-Martinez et al., 2010; Mart´ınez-G´omez et
al., 2011; Martinez-G´omez et al., 2012; Mathur
et al., 2013; Bertoldi et al., 2013).
As regards QE models, our work represents the
first investigation on incremental adaptation by ex-
ploiting users feedback to provide targeted (sys-
tem, user, or project specific) quality judgements.
</bodyText>
<sectionHeader confidence="0.99581" genericHeader="method">
3 Online QE for CAT environments
</sectionHeader>
<bodyText confidence="0.99966675">
When operating with advanced CAT tools, transla-
tors are presented with suggestions (either match-
ing fragments from a translation memory or auto-
matic translations produced by an MT system) for
each sentence of a source document. Before being
approved and published, translation suggestions
may require different amounts of post-editing op-
erations depending on their quality.
</bodyText>
<footnote confidence="0.995274">
4This assumption holds in the WMT evaluation scenario,
but it is not necessarily valid in real operating conditions.
</footnote>
<bodyText confidence="0.995539739130435">
Each post-edition brings a wealth of dynamic
knowledge about the whole translation process
and the involved actors. For instance, adaptive QE
components could exploit information about the
distance between automatically assigned scores
and the quality standards of individual translators
(inferred from the amount of their corrections) to
“profile” their behaviour.
The online learning paradigm fits well with this
research objective. In the online framework, dif-
ferently from the batch mode, the learning al-
gorithm sequentially processes an unknown se-
quence of instances X = x1, x2, ..., xn, returning
a prediction p(xi) as output at each step. Differ-
ences between p(xi) and the true label ˆp(xi) ob-
tained as feedback are used by the learner to refine
the next prediction p(xi+1).
In our experiments on adaptive QE we aim to
predict the quality of the suggested translations
in terms of HTER, which measures the minimum
edit distance between the MT output and its man-
ually post-edited version in the [0,1] interval.5 In
this scenario:
</bodyText>
<listItem confidence="0.993640833333333">
• The set of instances X is represented by
(source, target) pairs;
• The prediction p(xi) is the automatically es-
timated HTER score;
• The true label ˆp(xi) is the actual HTER score
calculated over the target and its post-edition.
</listItem>
<bodyText confidence="0.9925552">
At each step of the process, the goal of the learner
is to exploit user post-editions to reduce the differ-
ence between the predicted HTER values and the
true labels for the following (source, target) pairs.
As depicted in Figure 1, this is done as follows:
1. At step i, an unlabelled (source, target) pair
xi is sent to a feature extraction component.
To this aim, we used an adapted version
(Shah et al., 2014) of the open-source QuEst6
tool (Specia et al., 2013). The tool, which im-
plements a large number of features proposed
by participants in the WMT QE shared tasks,
has been modified to process one sentence at
a time as requested for integration in a CAT
environment;
</bodyText>
<footnote confidence="0.9713844">
5Edit distance is calculated as the number of edits (word
insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indi-
cate better translations.
6http://www.quest.dcs.shef.ac.uk/
</footnote>
<page confidence="0.989834">
712
</page>
<figureCaption confidence="0.932261">
Figure 1: Online QE workflow. &lt;src&gt;, &lt;trg&gt; and &lt;pe&gt; respectively stand for the source sentence, the
target translation and the post-edited target.
</figureCaption>
<bodyText confidence="0.990514068965517">
2. The extracted features are sent to an on-
line regressor, which returns a QE prediction
score p(xi) in the [0,1] interval (set to 0 at the
first round of the iteration);
3. Based on the post-edition done by the user,
the true HTER label ˆp(xi) is calculated by
means of the TERCpp7 open source tool;
4. The true label is sent back to the online al-
gorithm for a stepwise model improvement.
The updated model is then ready to process
the following instance xi+1.
This new paradigm for QE makes it possible
to: i) let the QE system learn from one point at
a time without complete re-training from scratch,
ii) customize the predictions of an existing QE
model with respect to a specific situation (post-
editor or domain), or even iii) build a QE model
from scratch when training data is not available.
For the sake of clarity it is worth observing that,
at least in principle, a model built in a batch fash-
ion could also be adapted to new test data. For in-
stance, this could be done by running periodic re-
training routines once a certain amount of new la-
belled instances has been collected (de facto mim-
icking an online process). Such periodic updates,
however, would not represent a viable solution in
the CAT framework where post-editors’ work can-
not be slowed by time-consuming procedures to
re-train core system components from scratch.
</bodyText>
<page confidence="0.486331">
7goo.gl/nkh2rE
</page>
<sectionHeader confidence="0.997537" genericHeader="method">
4 Evaluation framework
</sectionHeader>
<bodyText confidence="0.999993428571429">
To measure the adaptation capability of different
QE models, we experiment with a range of condi-
tions defined by variable degrees of similarity be-
tween training and test data.
The degree of similarity depends on several fac-
tors: the MT engine used, the domain of the docu-
ments to be translated, and the post-editing style of
individual translators. In our experiments, the de-
gree of similarity is measured in terms of AHTER,
which is computed as the absolute value of the dif-
ference between the average HTER of the training
and test sets. Large values indicate a low simi-
larity between training and test data and a more
challenging scenario for the learning algorithms.
</bodyText>
<subsectionHeader confidence="0.992417">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9867255">
In the range of possible evaluation scenarios, our
experiments cover:
</bodyText>
<listItem confidence="0.934023166666667">
• One artificial setting (§5) obtained from the
WMT12 QE shared task data, in which train-
ing/test instances are arranged to reflect ho-
mogeneous distributions of the HTER labels.
• Two settings obtained from data collected
with a CAT tool in real working condi-
</listItem>
<bodyText confidence="0.681493">
tions, in which different facets of the adap-
tive QE problem interact with each other.
In the first (user change, §6.1), train-
ing and test data from the same domain are
obtained from different users. In the sec-
</bodyText>
<page confidence="0.992903">
713
</page>
<bodyText confidence="0.999910304347826">
ond (user+domain change, §6.2), train-
ing and test data are obtained from different
users and domains.
For each setting, we compare an adaptive and
an empty model against a system trained in batch
mode. The adaptive model is built on top of an
existing model created from the training data and
exploits the new test instances to refine its predic-
tions in a stepwise manner. The empty model only
learns from the test set, simulating the worst con-
dition where training data is not available. The
batch model is built by learning only from the
training data and is evaluated on the test set with-
out exploiting information from the test instances.
Each model is also compared against a common
baseline for regression tasks, which is particularly
relevant in settings featuring different data distri-
butions between training and test sets. This base-
line (p henceforth) is calculated by labelling each
instance of the test set with the mean HTER score
of the training set. Previous works (Rubino et al.,
2013a) demonstrated that its results can be partic-
ularly hard to beat.
</bodyText>
<subsectionHeader confidence="0.999583">
4.2 Performance indicator and feature set
</subsectionHeader>
<bodyText confidence="0.99999435">
To measure the adaptability of our model to a
given test set we compute the Mean Absolute Er-
ror (MAE), a metric for regression problems also
used in the WMT QE shared tasks. The MAE is
the average of the absolute errors ez = |fz − yz|,
where fz is the prediction of the model and yz is
the true value for the ith instance.
As our focus is on the algorithmic aspect, in all
experiments we use the same feature set, which
consists of the seventeen features proposed in
(Specia et al., 2009). This feature set, fully de-
scribed in (Callison-Burch et al., 2012), takes into
account the complexity of the source sentence
(e.g. number of tokens, number of translations per
source word) and the fluency of the target trans-
lation (e.g. language model probabilities). The
results of previous WMT QE shared tasks have
shown that these baseline features are particularly
competitive in the regression task (with only few
systems able to beat them at WMT12).
</bodyText>
<subsectionHeader confidence="0.992544">
4.3 Online algorithms
</subsectionHeader>
<bodyText confidence="0.9863755">
In our experiments we evaluate two online algo-
rithms, OnlineSVR (Parrella, 2007)8 and Passive-
</bodyText>
<footnote confidence="0.9363005">
8http://www2.imperial.ac.uk/˜gmontana/
onlinesvr.htm
</footnote>
<bodyText confidence="0.999098902439024">
Aggressive Perceptron (Crammer et al., 2006),9 by
comparing their performance with a batch learning
strategy based on the Scikit-learn implementation
of Support Vector Regression (SVR).10
The choice of the OnlineSVR and Passive-
Aggressive (OSVR and PA henceforth) is moti-
vated by different considerations. From a perfor-
mance point of view, as an adaptation of E-SVR
which proved to be one of the top performing algo-
rithms in the regression QE tasks at WMT, OSVR
seems to be the best candidate. For this reason,
we use the online adaptation of E-SVR proposed
by (Ma et al., 2003). The goal of OnlineSVR is to
find a way to add each new sample to one of three
sets (support, empty, error) maintaining the con-
sistency of a set of conditions known as Karush-
Kuhn Tucker (KKT) conditions. For each new
point, OSVR starts a cycle where the samples are
moved across the three sets until the KKT condi-
tions are verified and the new point is assigned to
one of the sets. If the point is identified as a sup-
port vector, the parameters of the model are up-
dated. This allows OSVR to benefit from the pre-
diction capability of E-SVR in an online setting.
From a practical point of view, providing the
best trade off between accuracy and computational
time (He and Wang, 2012), PA represents a good
solution to meet the demand of efficiency posed
by the CAT framework. For each instance i, after
emitting a prediction and receiving the true label,
PA computes the E-insensitive hinge loss function.
If its value is larger than the tolerance parameter
(E), the weights of the model are updated as much
as the aggressiveness parameter C allows. In con-
trast with OSVR, which keeps track of the most
important points seen in the past (support vectors),
the update of the weights is done without consid-
ering the previously processed i-1 instances. Al-
though it makes PA faster than OSVR, this is a
riskier strategy because it may lead the algorithm
to change the model to adapt to outlier points.
</bodyText>
<sectionHeader confidence="0.989832" genericHeader="method">
5 Experiments with WMT12 data
</sectionHeader>
<bodyText confidence="0.999140833333333">
The motivations for experiments with training and
test data featuring homogeneous label distribu-
tions are twofold. First, since in this artificial sce-
nario adaptation capabilities are not required for
the QE component, batch methods operate in the
ideal conditions (as training and test are indepen-
</bodyText>
<footnote confidence="0.999764">
9https://code.google.com/p/sofia-ml/
10http://scikit-learn.org/
</footnote>
<page confidence="0.969203">
714
</page>
<table confidence="0.999822571428571">
WMT Dataset
Train Test Δ µ Batch Adaptive Empty
HTER
MAE MAE MAE Alg. MAE Alg.
200 754 0.39 13.7 13.2 13.2* OSVR 13.5* OSVR
600 754 1.32 13.8 12.7 12.9* OSVR 13.5* OSVR
1500 754 1.22 13.8 12.7 12.8* OSVR 13.5* OSVR
</table>
<tableCaption confidence="0.997903">
Table 1: MAE of the best performing batch, adaptive and empty models on WMT12 data. Training sets
</tableCaption>
<bodyText confidence="0.981013533333334">
of different size and the test set have been arranged to reflect homogeneous label distributions.
dent and identically distributed). This makes pos-
sible to obtain from batch models the best possible
performance to compare with. Second, this sce-
nario provides the fairest conditions for such com-
parison because, in principle, online algorithms
are not favoured by the possibility to learn from
the diversity of the test instances.
For our controlled experiments we use the
WMT12 English-Spanish corpus, which consists
of 2,254 source-target pairs (1,832 for training,
422 for test). The HTER labels for our regression
task are calculated from the post-edited version
and the target sentences provided in the dataset.
To avoid biases in the label distribution, the
WMT12 training and test data have been merged,
shuffled, and eventually separated to generate
three training sets of different size (200, 600, and
1500 instances), and one test set with 754 in-
stances. For each algorithm, the training sets are
used for learning the QE models, optimizing pa-
rameters (i.e. C, c, the kernel and its parame-
ters for SVR and OSVR; tolerance and aggressive-
ness for PA) through grid search in 10-fold cross-
validation.
Evaluation is carried out by measuring the per-
formance of the batch (learning only from the
training set), the adaptive (learning from the train-
ing set and adapting to the test set), and the empty
(learning from scratch from the test set) models in
terms of global MAE scores on the test set.
Table 1 reports the results achieved by the
best performing algorithm for each type of model
(batch, adaptive, empty). As can be seen, close
MAE values show a similar behaviour for the three
types of models.11 With the same amount of train-
ing data, the performance of the batch and the
adaptive models (in this case always obtained with
OSVR) is almost identical. This demonstrates
that, as expected, the online algorithms do not take
11Results marked with the “*” symbol are NOT statisti-
cally significant compared to the corresponding batch model.
The others are always statistically significant at p≤0.005, cal-
culated with approximate randomization (Yeh, 2000).
advantage of test data with a label distribution sim-
ilar to the training set. All the models outper-
form the baseline, even if the minimal differences
confirm the competitiveness of such a simple ap-
proach.
Overall, these results bring some interesting in-
dications about the behaviour of the different on-
line algorithms. First, the good results achieved
by the empty models (less than one MAE point
separates them from the best ones built on the
largest training set) suggest their high potential
when training data are not available. Second,
our results show that OSVR is always the best
performing algorithm for the adaptive and empty
models. This suggests a lower capability of PA to
learn from instances similar to the training data.
</bodyText>
<sectionHeader confidence="0.988641" genericHeader="method">
6 Experiments with CAT data
</sectionHeader>
<bodyText confidence="0.999944909090909">
To experiment with adaptive QE in more realis-
tic conditions we used a CAT tool12 to collect
two datasets of (source, target, post edited tar-
get) English-Italian tuples.The source sentences in
the datasets come from two documents from dif-
ferent domains, respectively legal (L) and infor-
mation technology (IT). The L document, which
was extracted from a European Parliament resolu-
tion published on the EUR-Lex platform,13 con-
tains 164 sentences. The IT document, which was
taken from a software user manual, contains 280
sentences. The source sentences were translated
with two SMT systems built by training the Moses
toolkit (Koehn et al., 2007) on parallel data from
the two domains (about 2M sentences for IT and
1.5M for L). Post-editions were collected from
eight professional translators (four for each docu-
ment) operating with the CAT tool in real working
conditions.
According to the way they are created, the two
datasets allow us to evaluate the adaptability of
different QE models with respect to user changes
</bodyText>
<footnote confidence="0.9996695">
12MateCat – http://www.matecat.com/
13http://eur-lex.europa.eu/
</footnote>
<page confidence="0.99304">
715
</page>
<table confidence="0.999341411764706">
user change
Legal Domain
Train Test Δ P Batch Adaptive Empty
HTER
MAE MAE MAE Alg. MAE Alg.
rad cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
cons rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
sim1 sim2 3.3 14.7 12.2 12.6* OSVR 12.9* OSVR
sim2 sim1 3.2 13.4 13.3 13.9* OSVR 15.2* OSVR
IT Domain
Train Test Δ P Batch Adaptive Empty
HTER
MAE MAE MAE Alg MAE Alg
cons rad 12.8 19.2 19.8 17.5* OSVR 16.6 OSVR
rad cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
sim2 sim1 3.3 14.7 14.4 15* OSVR 15.5* OSVR
sim1 sim2 1.1 15 13.9 14.4* OSVR 16.1* OSVR
</table>
<tableCaption confidence="0.875868">
Table 2: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users in the same domain.
</tableCaption>
<bodyText confidence="0.99671425">
within the same domain (§6.1), as well as user and
domain changes at the same time (§6.2).
For each document D (L or IT), these two sce-
narios are obtained by dividing D into two parts
of equal size (80 instances for L and 140 for IT).
The result is one training set and one test set for
each post-editor within the same domain. For the
user change experiments, training and test sets
are selected from different post-editors within the
same domain. For the user+domain change
experiments, training and test sets are selected
from different post-editors in different domains.
On each combination of training and test sets,
the batch, adaptive, and empty models are trained
and evaluated in terms of global MAE scores on
the test set.
</bodyText>
<subsectionHeader confidence="0.99975">
6.1 Dealing with user changes
</subsectionHeader>
<bodyText confidence="0.999978326923077">
Among the possible combinations of training and
test data from different post-editors in the same
domain, Table 2 refers to two opposite scenarios.
For each domain, these respectively involve the
most dissimilar and the most similar post-editors
according to the ΔHTER. Also in this case, for
each model (batch, adaptive and empty) we only
report the MAE of the best performing algorithm.
The first scenario defines a challenging situation
where two post-editors (rad and cons) are charac-
terized by opposite behaviour. As evidenced by
the high ΔHTER values, one of them (rad) is the
most “radical” post-editor (performing more cor-
rections) while the other (cons) is the most “con-
servative” one. As shown in Table 2, global MAE
scores for the online algorithms (both adaptive and
empty) indicate their good adaptation capabilities.
This is evident from the significant improvements
both over the baseline (µ) and the batch models.
Interestingly, the best results are always achieved
by the empty models (with MAE reductions up to
10 points when tested on rad in the L domain,
and 3.2 points when tested on rad in the IT do-
main). These results (MAE reductions are always
statistically significant) suggest that, when deal-
ing with datasets with very different label distri-
butions, the evident limitations of batch methods
are more easily overcome by learning from scratch
from the feedback of a new post-editor. This also
holds when the amount of test points to learn from
is limited, as in the L domain where the test set
contains only 80 instances. From the application-
oriented perspective that motivates our work, con-
sidering the high costs of acquiring large and rep-
resentative QE training data, this is an important
finding.
The second scenario defines a less challeng-
ing situation where the two post-editors (sim1 and
sim2) are characterized by the most similar be-
haviour (small ΔHTER). This scenario is closer to
the situation described in Section §5. Also in this
case MAE results for the adaptive and empty mod-
els are slightly worse, but not significantly, than
those of the batch models and the baseline. How-
ever, considering the very small amount of “unin-
formative” instances to learn from (especially for
the empty models), these lower results are not sur-
prising.
A closer look at the behaviour of the online al-
gorithms in the two domains leads to other obser-
vations. First, OSVR always outperforms PA for
the empty models and when post-editors have sim-
</bodyText>
<page confidence="0.995262">
716
</page>
<table confidence="0.9995771875">
user+domain change
Train Test Δ µ Batch Adaptive Empty
HTER
MAE MAE MAE Alg MAE Alg
L cons IT rad 24.5 26.4 27 18.2 OSVR 16.6 OSVR
IT rad L cons 24.0 24.9 25.4 19.7 OSVR 12.5 OSVR
L rad L cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
L cons L rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
IT cons L cons 13.5 17.3 17.5 15.7 OSVR 12.5 OSVR
IT cons IT rad 12.8 19.2 19.8 17.5 OSVR 16.6 OSVR
L cons IT cons 12.7 17.6 17.6 15.1 OSVR 15.5 OSVR
IT rad IT cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
IT cons L rad 8.3 12.3 13 10.7 OSVR 11.3 OSVR
L rad IT rad 6.8 17 16.9 16.2 OSVR 16.6 OSVR
L rad IT cons 5.0 15.4 16.2 14.7 OSVR 15.5 OSVR
IT rad L rad 2.2 10.6 10.8 10.5 OSVR 11.3 OSVR
</table>
<tableCaption confidence="0.963995">
Table 3: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users and domains.
</tableCaption>
<bodyText confidence="0.985513285714286">
ilar behaviour, which are situations where the al-
gorithm does not have to quickly adapt or react to
sudden changes.
Second, PA seems to perform better for the
adaptive models when the post-editors have sig-
nificantly different behaviour and a quick adapta-
tion to the incoming points is required. This can
be motivated by the fact that PA relies on a simpler
and less robust learning strategy that does not keep
track of all the information coming from the previ-
ously processed instances, and can easily modify
its weights taking into consideration the last seen
point (see Section §3). For OSVR the addition of
new points to the support set may have a limited
effect on the whole model, in particular if the num-
ber of points in the set is large. This also results
in a different processing time for the two algo-
rithms.14 For instance, in the empty configurations
on IT data, OSVR devotes 6.0 ms per instance to
update the model, while PA devotes 4.8 ms, which
comes at the cost of lower performance.
</bodyText>
<subsectionHeader confidence="0.999983">
6.2 Dealing with user and domain changes
</subsectionHeader>
<bodyText confidence="0.999909222222222">
In the last round of experiments we evaluate the
reactivity of different online models to simultane-
ous user and domain changes. To this aim, our
QE models are created using a training set coming
from one domain (L or IT), and then used to pre-
dict the HTER labels for the test instances coming
from the other domain (e.g. training on L, testing
on IT).
Among the possible combinations of training
</bodyText>
<footnote confidence="0.98314675">
14Their complexity depends on the number of features (f)
and the number of previously seen instances (n). While for
PA it is linear in f, i.e. O(f), for OSVR it is quadratic in n, i.e.
O(n2*f).
</footnote>
<bodyText confidence="0.999947757575757">
and test data, Table 3 refers to scenarios involv-
ing the most conservative and radical post-editors
in each domain (previously identified with cons
and rad)15. In the table, results are ordered ac-
cording to the AHTER computed between the se-
lected post-editor in the training domain (e.g. L
cons) and the selected post-editor in the test do-
main (e.g. IT rad). For the sake of comparison,
we also report (grey rows) the results of the ex-
periments within the same domain presented in
§6.1. For each type of model (batch, adaptive and
empty) we only show the MAE obtained by the
best performing algorithm.
Intuitively, dealing with simultaneous user and
domain changes represents a more challenging
problem compared to the previous setting where
only post-editors changes were considered. Such
intuition is confirmed by the results of the adaptive
models that outperform both the baseline (p) and
the batch models even for low AHTER values. Al-
though in these cases the distance between train-
ing and test data is comparable to the experiments
with similar post-editors working in the same do-
main (sim1 and sim2), here the predictive power
of the batch models seems in fact to be lower. The
same holds also for the empty models except in
two cases where the AHTER is the smallest (2.2
and 5.0). This is a strong evidence of the fact that,
in case of domain changes, online models can still
learn from new test instances even if they have a
label distribution similar to the training set.
When the distance between training and test in-
creases, our results confirm our previous findings
</bodyText>
<footnote confidence="0.633897333333333">
15For brevity, we omit the results for the other post-editors
which, however, show similar trends with respect to the pre-
vious experiments.
</footnote>
<page confidence="0.992915">
717
</page>
<bodyText confidence="0.999991098039216">
about the potential of the empty models. The ob-
served MAE reductions range in fact from 10.4
to 12.9 points for the two combinations with the
highest ΔHTER.
From the algorithmic point of view, our results
indicate that OSVR achieves the best performance
for all the combinations involving user and domain
changes. This contrasts with the results of most of
the combinations involving only user changes with
post-editors characterized by opposite behaviour
(grey rows in Table 3). However, it has to be re-
marked that in the case of heterogeneous datasets
the difference between the two algorithms is al-
ways very high. In our experiments, when PA out-
performs OSVR, its MAE results are significantly
lower and vice-versa (respectively up to 1.5 and
1.7 MAE points). This suggests that, although PA
is potentially capable of achieving higher results
and better adapt to the new test points, its instabil-
ity makes it less reliable for practical use.
As a final analysis of our results, we investi-
gated how the performance of the different types
of models (batch, adaptive, empty) relates to the
distance between training and test sets. To this
aim, we computed the Pearson correlation be-
tween the ΔHTER (column 3 in Table 3) and the
MAE of each model (columns 5, 6 and 8), which
respectively resulted in 0.9 for the batch, 0.63 for
the adaptive and -0.07 for the empty model. These
values confirm that batch models are heavily af-
fected by the dissimilarity between training and
test data: large differences in the label distribution
imply higher MAE results and vice-versa. This
is in line with our previous findings about batch
models that, learning only from the training set,
cannot leverage possible dissimilarities of the test
set. The lower correlation observed for the adap-
tive models also confirms our intuitions: adapting
to the new test points, these models are in fact
more robust to differences with the training data.
As expected, the results of the empty models are
completely uncorrelated with the ΔHTER since
they only use the test set.
This analysis confirms that, even when dealing
with different domains, the similarity between the
training and test data is one of the main factors that
should drive the choice of the QE model. When
this distance is minimal, batch models can be a
reasonable option, but when the gap between train-
ing and test data increases, adaptive or empty mod-
els are a preferable choice to achieve good results.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99999095">
In the CAT scenario, each translation job can be
seen as a complex situation where the user (his
personal style and background), the source doc-
ument (the language and the domain) and the un-
derlying technology (the translation memory and
the MT engine that generate translation sugges-
tions) contribute to make the task unique. So far,
the adaptability to such specificities (a major chal-
lenge for CAT technology) has been mainly sup-
ported by the evolution of translation memories,
which incrementally store translated segments in-
corporating the user style. The wide adoption of
translation memories demonstrates the importance
of capitalizing on such information to increase
translators productivity.
While this lesson recently motivated research
on adaptive MT decoders that learn from user cor-
rections, nothing has been done to develop adap-
tive QE components. In the first attempt to ad-
dress this problem, we proposed the application
of the online learning protocol to leverage users
feedback and to tailor QE predictions to their qual-
ity standards. Besides highlighting the limitations
of current batch methods to adapt to user and
domain changes, we performed an application-
oriented analysis of different online algorithms fo-
cusing on specific aspects relevant to the CAT sce-
nario. Our results show that the wealth of dynamic
knowledge brought by user corrections can be ex-
ploited to refine in a stepwise fashion the qual-
ity judgements in different testing conditions (user
changes as well as simultaneous user and domain
changes).
As an additional contribution, to spark further
research on this facet of the QE problem, our adap-
tive QE infrastructure (integrating all the compo-
nents and the algorithms described in this paper)
has been released as open source. Its C++ im-
plementation is available at http://hlt.fbk.eu/
technologies/aqet.
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997163">
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992679333333333">
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite: When less is more for
translation quality estimation. In Proceedings of the
</reference>
<page confidence="0.986799">
718
</page>
<reference confidence="0.999103281818182">
8th Workshop on Statistical Machine Translation,
Sofia, Bulgaria, August.
Nicola Bertoldi, Mauro Cettolo, and Federico Mar-
cello. 2013. Cache-based Online Adaptation
for Machine Translation Enhanced Computer As-
sisted Translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 1147–1162, Nice,
France.
Ergun Bicici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the 8th Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of the 8th Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1–44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation (WMT’12),
pages 10–51, Montr´eal, Canada.
Nicol`o Cesa-Bianchi, Gabriel Reverberi, and Sandor
Szedmak. 2008. Online Learning Algorithms for
Computer-Assisted Translation. Deliverable D4.2,
SMART: Statistical Multilingual Analysis for Re-
trieval and Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-2013, pages 32–42, Sofia, Bulgaria.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms. J. Mach. Learn.
Res., 7:551–585, December.
Jos´e G.C. de Souza, Christian Buck, Marco Turchi, and
Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 quality estimation shared task. In Pro-
ceedings of the 8th Workshop on Statistical Machine
Translation, Sofia, Bulgaria, August.
Jos´e G.C. de Souza, Miquel Espl`a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics - Short Papers, pages 771–776,
Sofia, Bulgaria.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Tree Kernels for Machine Transla-
tion Quality Estimation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT’12), pages 109–113, Montr´eal, Canada.
Zhengyan He and Houfeng Wang. 2012. A Com-
parison and Improvement of Online Learning Al-
gorithms for Sequence Labeling. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1147–
1162, Mumbai, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing Time as a Mea-
sure of Cognitive Effort. In Proceedings of the
AMTA 2012 Workshop on Post-editing Technology
and Practice (WPTP 2012), San Diego, California.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Op-
erations. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 181–190,
Montr´eal, Canada.
Nick Littlestone. 1988. Learning Quickly when Irrel-
evant Attributes Abound: A New Linear-Threshold
Algorithm. In Machine Learning, pages 285–318.
Junshui Ma, James Theiler, and Simon Perkins. 2003.
Accurate Online Support Vector Regression. Neural
Computation, 15:2683–2703.
Pascual Martinez-G´omez, Germ´an Sanchis-Trilles, and
Francisco Casacuberta. 2011. Online Learning via
Dynamic Reranking for Computer Assisted Transla-
tion. In Proceedings of the 12th international con-
ference on Computational linguistics and intelligent
text processing - Volume Part II, CICLing’11.
Pascual Martinez-G´omez, Germ´an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193–
3203, September.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online Learning Approaches in Com-
puter Assisted Translation. In Proceedings of the
8th Workshop on Statistical Machine Translation,
Sofia, Bulgaria.
</reference>
<page confidence="0.983357">
719
</page>
<reference confidence="0.999768465753425">
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Match without a Referee: Evaluating MT
Adequacy without Reference Translations. In Pro-
ceedings of the 7th Workshop on Statistical Machine
Translation, pages 171–180, Montr´eal, Canada.
Daniel Ortiz-Martfnez, Ismael Garcfa-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
546–554, Stroudsburg, PA, USA.
Francesco Parrella. 2007. Online support vector re-
gression. Master’s Thesis, Department of Informa-
tion Science, University of Genoa, Italy.
Raphael Rubino, Jos´e G.C. de Souza, Jennifer Fos-
ter, and Lucia Specia. 2013a. Topic Models for
Translation Quality Estimation for Gisting Purposes.
In Proceedings of the Machine Translation Summit
XIV, Nice, France.
Raphael Rubino, Antonio Toral, S Cort´es Va´ıllo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013b. The CNGL-DCU-Prompsit translation sys-
tems for WMT13. In Proceedings of the 8th Work-
shop on Statistical Machine Translation, pages 211–
216, Sofia, Bulgaria.
Kashif Shah, Marco Turchi, and Lucia Specia. 2014.
An Efficient and User-friendly Tool for Machine
Translation Quality Estimation. In Proceedings of
the 9th International Conference on Language Re-
sources and Evaluation, Reykjavik, Iceland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223–231, Cambridge,
Massachusetts, USA.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the 7th Workshop on Statistical Machine Translation
(WMT’12), pages 145–151, Montr´eal, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Con-
ference of the European Association for Machine
Translation (EAMT’09), pages 28–35, Barcelona,
Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39–50.
Lucia Specia, Kashif Shah, Jos´e G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, ACL-
2013, pages 79–84, Sofia, Bulgaria.
Marco Turchi and Matteo Negri. 2014. Automatic An-
notation of Machine Translation Datasets with Bi-
nary Quality Judgements. In Proceedings of the 9th
International Conference on Language Resources
and Evaluation, Reykjavik, Iceland.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the Subjectivity of Human
Judgements in MT Quality Estimation. In Proceed-
ings of the 8th Workshop on Statistical Machine
Translation, pages 240–251, Sofia, Bulgaria.
Alexander Yeh. 2000. More Accurate Tests for the
Statistical Significance of Result Differences. In
Proceedings of the 18th conference on Computa-
tional linguistics (COLING 2000) - Volume 2, pages
947–953, Saarbrucken, Germany.
</reference>
<page confidence="0.996852">
720
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907002">
<title confidence="0.9976615">Adaptive Quality Estimation for Machine Translation Antonios</title>
<author confidence="0.991025">Matteo</author>
<affiliation confidence="0.976389333333333">(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, (2)University of Trento, (3)National Technical University of Athens,</affiliation>
<email confidence="0.999633">anastasopoulos.ant@gmail.com</email>
<abstract confidence="0.999204130434783">The automatic estimation of machine translation (MT) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role. When moving from controlled lab evaluations to real-life scenarios the task becomes even harder. For current MT quality estimation (QE) systems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Beck</author>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>SHEF-Lite: When less is more for translation quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7305" citStr="Beck et al., 2013" startWordPosition="1178" endWordPosition="1181"> used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of </context>
</contexts>
<marker>Beck, Shah, Cohn, Specia, 2013</marker>
<rawString>Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite: When less is more for translation quality estimation. In Proceedings of the 8th Workshop on Statistical Machine Translation, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
<author>Federico Marcello</author>
</authors>
<title>Cache-based Online Adaptation for Machine Translation Enhanced Computer Assisted Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the XIV Machine Translation Summit,</booktitle>
<pages>1147--1162</pages>
<location>Nice, France.</location>
<contexts>
<context position="10167" citStr="Bertoldi et al., 2013" startWordPosition="1617" endWordPosition="1620"> translated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Martinez et al., 2010; Mart´ınez-G´omez et al., 2011; Martinez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Online QE for CAT environments When operating with advanced CAT tools, translators are presented with suggestions (either matching fragments from a translation memory or automatic translations produced by an MT system) for each sentence of a source document. Before being approved and published, translation suggestions may require different amounts of post-editing operations depending on their qua</context>
</contexts>
<marker>Bertoldi, Cettolo, Marcello, 2013</marker>
<rawString>Nicola Bertoldi, Mauro Cettolo, and Federico Marcello. 2013. Cache-based Online Adaptation for Machine Translation Enhanced Computer Assisted Translation. In Proceedings of the XIV Machine Translation Summit, pages 1147–1162, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bicici</author>
</authors>
<title>Feature decay algorithms for fast deployment of accurate statistical machine translation systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7286" citStr="Bicici, 2013" startWordPosition="1176" endWordPosition="1177">) instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thu</context>
</contexts>
<marker>Bicici, 2013</marker>
<rawString>Ergun Bicici. 2013. Feature decay algorithms for fast deployment of accurate statistical machine translation systems. In Proceedings of the 8th Workshop on Statistical Machine Translation, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation. Summer workshop final report,</title>
<date>2003</date>
<publisher>JHU/CLSP.</publisher>
<contexts>
<context position="2488" citStr="Blatz et al., 2003" startWordPosition="373" endWordPosition="376">g interest towards the integration of suggestions obtained from MT engines with those derived from translation memories (TMs). The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (K</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Summer workshop final report, JHU/CLSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the 8th Workshop on Statistical Machine Translation, WMT-2013,</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2679" citStr="Bojar et al., 2013" startWordPosition="405" endWordPosition="408">its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target l</context>
<context position="8536" citStr="Bojar et al., 2013" startWordPosition="1372" endWordPosition="1375">m inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. 1Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2http://www.statmt.org/wmt13/ 3For a comprehensive overview of the QE approaches proposed so far we refer the reader to the WMT12 and WMT13 QE shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013). 711 These interconnected issues are particularly relevant in the CAT framework, where translation jobs from different domains are routed to professional translators with different idiolect, background and quality standards. The first aspect, modelling annotators’ individual behaviour and interdependences, has been addressed by Cohn and Specia (2013), who explored multi-task Gaussian Processes as a way to jointly learn from the output of multiple annotations. This technique is suitable to cope with the unbalanced distribution of training instances and yields better models when heterogeneous t</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the 8th Workshop on Statistical Machine Translation, WMT-2013, pages 1–44, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation (WMT’12),</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2658" citStr="Callison-Burch et al., 2012" startWordPosition="401" endWordPosition="404">anslation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of th</context>
<context position="8515" citStr="Callison-Burch et al., 2012" startWordPosition="1368" endWordPosition="1371">being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. 1Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2http://www.statmt.org/wmt13/ 3For a comprehensive overview of the QE approaches proposed so far we refer the reader to the WMT12 and WMT13 QE shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013). 711 These interconnected issues are particularly relevant in the CAT framework, where translation jobs from different domains are routed to professional translators with different idiolect, background and quality standards. The first aspect, modelling annotators’ individual behaviour and interdependences, has been addressed by Cohn and Specia (2013), who explored multi-task Gaussian Processes as a way to jointly learn from the output of multiple annotations. This technique is suitable to cope with the unbalanced distribution of training instances and yields better models</context>
<context position="17500" citStr="Callison-Burch et al., 2012" startWordPosition="2849" endWordPosition="2852">be particularly hard to beat. 4.2 Performance indicator and feature set To measure the adaptability of our model to a given test set we compute the Mean Absolute Error (MAE), a metric for regression problems also used in the WMT QE shared tasks. The MAE is the average of the absolute errors ez = |fz − yz|, where fz is the prediction of the model and yz is the true value for the ith instance. As our focus is on the algorithmic aspect, in all experiments we use the same feature set, which consists of the seventeen features proposed in (Specia et al., 2009). This feature set, fully described in (Callison-Burch et al., 2012), takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The results of previous WMT QE shared tasks have shown that these baseline features are particularly competitive in the regression task (with only few systems able to beat them at WMT12). 4.3 Online algorithms In our experiments we evaluate two online algorithms, OnlineSVR (Parrella, 2007)8 and Passive8http://www2.imperial.ac.uk/˜gmontana/ onlinesvr.htm Aggressive Perceptron (Crammer et al., 200</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation (WMT’12), pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Gabriel Reverberi</author>
<author>Sandor Szedmak</author>
</authors>
<title>Online Learning Algorithms for Computer-Assisted Translation. Deliverable D4.2, SMART: Statistical Multilingual Analysis for Retrieval and Translation.</title>
<date>2008</date>
<contexts>
<context position="10032" citStr="Cesa-Bianchi et al., 2008" startWordPosition="1597" endWordPosition="1600">tances that will be encountered will be similar to the training data.4 However, similarly to translation memories that incrementally store translated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Martinez et al., 2010; Mart´ınez-G´omez et al., 2011; Martinez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Online QE for CAT environments When operating with advanced CAT tools, translators are presented with suggestions (either matching fragments from a translation memory or automatic translations produced by an MT system) for each sentence of a source document. Befor</context>
</contexts>
<marker>Cesa-Bianchi, Reverberi, Szedmak, 2008</marker>
<rawString>Nicol`o Cesa-Bianchi, Gabriel Reverberi, and Sandor Szedmak. 2008. Online Learning Algorithms for Computer-Assisted Translation. Deliverable D4.2, SMART: Statistical Multilingual Analysis for Retrieval and Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL-2013,</booktitle>
<pages>32--42</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8889" citStr="Cohn and Specia (2013)" startWordPosition="1423" endWordPosition="1426">., 2006), and post-editing time (e.g. seconds per word). 2http://www.statmt.org/wmt13/ 3For a comprehensive overview of the QE approaches proposed so far we refer the reader to the WMT12 and WMT13 QE shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013). 711 These interconnected issues are particularly relevant in the CAT framework, where translation jobs from different domains are routed to professional translators with different idiolect, background and quality standards. The first aspect, modelling annotators’ individual behaviour and interdependences, has been addressed by Cohn and Specia (2013), who explored multi-task Gaussian Processes as a way to jointly learn from the output of multiple annotations. This technique is suitable to cope with the unbalanced distribution of training instances and yields better models when heterogeneous training datasets are available. The second problem, the adaptability of QE models, has not been explored yet. A common trait of all current approaches, in fact, is the reliance on batch learning techniques, which assume a “static” nature of the world where new unseen instances that will be encountered will be similar to the training data.4 However, si</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL-2013, pages 32–42, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>7--551</pages>
<contexts>
<context position="18102" citStr="Crammer et al., 2006" startWordPosition="2936" endWordPosition="2939">Burch et al., 2012), takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The results of previous WMT QE shared tasks have shown that these baseline features are particularly competitive in the regression task (with only few systems able to beat them at WMT12). 4.3 Online algorithms In our experiments we evaluate two online algorithms, OnlineSVR (Parrella, 2007)8 and Passive8http://www2.imperial.ac.uk/˜gmontana/ onlinesvr.htm Aggressive Perceptron (Crammer et al., 2006),9 by comparing their performance with a batch learning strategy based on the Scikit-learn implementation of Support Vector Regression (SVR).10 The choice of the OnlineSVR and PassiveAggressive (OSVR and PA henceforth) is motivated by different considerations. From a performance point of view, as an adaptation of E-SVR which proved to be one of the top performing algorithms in the regression QE tasks at WMT, OSVR seems to be the best candidate. For this reason, we use the online adaptation of E-SVR proposed by (Ma et al., 2003). The goal of OnlineSVR is to find a way to add each new sample to </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. J. Mach. Learn. Res., 7:551–585, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Christian Buck</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>FBK-UEdin participation to the WMT13 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<marker>de Souza, Buck, Turchi, Negri, 2013</marker>
<rawString>Jos´e G.C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013a. FBK-UEdin participation to the WMT13 quality estimation shared task. In Proceedings of the 8th Workshop on Statistical Machine Translation, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Miquel Espl`a-Gomis</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics - Short Papers,</booktitle>
<pages>771--776</pages>
<location>Sofia, Bulgaria.</location>
<marker>de Souza, Espl`a-Gomis, Turchi, Negri, 2013</marker>
<rawString>Jos´e G.C. de Souza, Miquel Espl`a-Gomis, Marco Turchi, and Matteo Negri. 2013b. Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics - Short Papers, pages 771–776, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Tree Kernels for Machine Translation Quality Estimation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT’12),</booktitle>
<pages>109--113</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7115" citStr="Hardmeier et al., 2012" startWordPosition="1143" endWordPosition="1147">at http://hlt.fbk.eu/technologies/ aqet. 2 Related work QE is generally cast as a supervised machine learning task, where a model trained from a collection of (source, target, label) instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diver</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Tree Kernels for Machine Translation Quality Estimation. In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT’12), pages 109–113, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengyan He</author>
<author>Houfeng Wang</author>
</authors>
<title>A Comparison and Improvement of Online Learning Algorithms for Sequence Labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1147--1162</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="19317" citStr="He and Wang, 2012" startWordPosition="3153" endWordPosition="3156">to one of three sets (support, empty, error) maintaining the consistency of a set of conditions known as KarushKuhn Tucker (KKT) conditions. For each new point, OSVR starts a cycle where the samples are moved across the three sets until the KKT conditions are verified and the new point is assigned to one of the sets. If the point is identified as a support vector, the parameters of the model are updated. This allows OSVR to benefit from the prediction capability of E-SVR in an online setting. From a practical point of view, providing the best trade off between accuracy and computational time (He and Wang, 2012), PA represents a good solution to meet the demand of efficiency posed by the CAT framework. For each instance i, after emitting a prediction and receiving the true label, PA computes the E-insensitive hinge loss function. If its value is larger than the tolerance parameter (E), the weights of the model are updated as much as the aggressiveness parameter C allows. In contrast with OSVR, which keeps track of the most important points seen in the past (support vectors), the update of the weights is done without considering the previously processed i-1 instances. Although it makes PA faster than </context>
</contexts>
<marker>He, Wang, 2012</marker>
<rawString>Zhengyan He and Houfeng Wang. 2012. A Comparison and Improvement of Online Learning Algorithms for Sequence Labeling. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1147– 1162, Mumbai, India.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="24312" citStr="Koehn et al., 2007" startWordPosition="3971" endWordPosition="3974">ve QE in more realistic conditions we used a CAT tool12 to collect two datasets of (source, target, post edited target) English-Italian tuples.The source sentences in the datasets come from two documents from different domains, respectively legal (L) and information technology (IT). The L document, which was extracted from a European Parliament resolution published on the EUR-Lex platform,13 contains 164 sentences. The IT document, which was taken from a software user manual, contains 280 sentences. The source sentences were translated with two SMT systems built by training the Moses toolkit (Koehn et al., 2007) on parallel data from the two domains (about 2M sentences for IT and 1.5M for L). Post-editions were collected from eight professional translators (four for each document) operating with the CAT tool in real working conditions. According to the way they are created, the two datasets allow us to evaluate the adaptability of different QE models with respect to user changes 12MateCat – http://www.matecat.com/ 13http://eur-lex.europa.eu/ 715 user change Legal Domain Train Test Δ P Batch Adaptive Empty HTER MAE MAE MAE Alg. MAE Alg. rad cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR cons rad 19.4 21.2 21.3</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarit Koponen</author>
<author>Wilker Aziz</author>
<author>Luciana Ramos</author>
<author>Lucia Specia</author>
</authors>
<title>Post-editing Time as a Measure of Cognitive Effort.</title>
<date>2012</date>
<booktitle>In Proceedings of the AMTA 2012 Workshop on Post-editing Technology and Practice (WPTP 2012),</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="7811" citStr="Koponen et al., 2012" startWordPosition="1259" endWordPosition="1262">) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. 1Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2http://www.statmt.org/wmt13/ 3For a comprehensive overview of the QE approaches propos</context>
</contexts>
<marker>Koponen, Aziz, Ramos, Specia, 2012</marker>
<rawString>Maarit Koponen, Wilker Aziz, Luciana Ramos, and Lucia Specia. 2012. Post-editing Time as a Measure of Cognitive Effort. In Proceedings of the AMTA 2012 Workshop on Post-editing Technology and Practice (WPTP 2012), San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarit Koponen</author>
</authors>
<title>Comparing Human Perceptions of Post-editing Effort with Post-editing Operations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>181--190</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="3100" citStr="Koponen, 2012" startWordPosition="477" endWordPosition="478">3; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; 2. Each translation job has its own specificities (domain, complexity of the source text, average target quality). Since data from a new job may differ from those used to train the QE model, its estimates on the new instances might result to be biased or uninfor</context>
<context position="7788" citStr="Koponen, 2012" startWordPosition="1256" endWordPosition="1258">al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. 1Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2http://www.statmt.org/wmt13/ 3For a comprehensive overview of t</context>
</contexts>
<marker>Koponen, 2012</marker>
<rawString>Maarit Koponen. 2012. Comparing Human Perceptions of Post-editing Effort with Post-editing Operations. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 181–190, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Littlestone</author>
</authors>
<title>Learning Quickly when Irrelevant Attributes Abound: A New Linear-Threshold Algorithm.</title>
<date>1988</date>
<booktitle>In Machine Learning,</booktitle>
<pages>285--318</pages>
<contexts>
<context position="9954" citStr="Littlestone, 1988" startWordPosition="1587" endWordPosition="1588">iques, which assume a “static” nature of the world where new unseen instances that will be encountered will be similar to the training data.4 However, similarly to translation memories that incrementally store translated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Martinez et al., 2010; Mart´ınez-G´omez et al., 2011; Martinez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Online QE for CAT environments When operating with advanced CAT tools, translators are presented with suggestions (either matching fragments from a translation memory or automatic transl</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>Nick Littlestone. 1988. Learning Quickly when Irrelevant Attributes Abound: A New Linear-Threshold Algorithm. In Machine Learning, pages 285–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junshui Ma</author>
<author>James Theiler</author>
<author>Simon Perkins</author>
</authors>
<title>Accurate Online Support Vector Regression.</title>
<date>2003</date>
<journal>Neural Computation,</journal>
<pages>15--2683</pages>
<contexts>
<context position="18635" citStr="Ma et al., 2003" startWordPosition="3026" endWordPosition="3029">mperial.ac.uk/˜gmontana/ onlinesvr.htm Aggressive Perceptron (Crammer et al., 2006),9 by comparing their performance with a batch learning strategy based on the Scikit-learn implementation of Support Vector Regression (SVR).10 The choice of the OnlineSVR and PassiveAggressive (OSVR and PA henceforth) is motivated by different considerations. From a performance point of view, as an adaptation of E-SVR which proved to be one of the top performing algorithms in the regression QE tasks at WMT, OSVR seems to be the best candidate. For this reason, we use the online adaptation of E-SVR proposed by (Ma et al., 2003). The goal of OnlineSVR is to find a way to add each new sample to one of three sets (support, empty, error) maintaining the consistency of a set of conditions known as KarushKuhn Tucker (KKT) conditions. For each new point, OSVR starts a cycle where the samples are moved across the three sets until the KKT conditions are verified and the new point is assigned to one of the sets. If the point is identified as a support vector, the parameters of the model are updated. This allows OSVR to benefit from the prediction capability of E-SVR in an online setting. From a practical point of view, provid</context>
</contexts>
<marker>Ma, Theiler, Perkins, 2003</marker>
<rawString>Junshui Ma, James Theiler, and Simon Perkins. 2003. Accurate Online Support Vector Regression. Neural Computation, 15:2683–2703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascual Martinez-G´omez</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online Learning via Dynamic Reranking for Computer Assisted Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th international conference on Computational linguistics and intelligent text processing - Volume Part II, CICLing’11.</booktitle>
<marker>Martinez-G´omez, Sanchis-Trilles, Casacuberta, 2011</marker>
<rawString>Pascual Martinez-G´omez, Germ´an Sanchis-Trilles, and Francisco Casacuberta. 2011. Online Learning via Dynamic Reranking for Computer Assisted Translation. In Proceedings of the 12th international conference on Computational linguistics and intelligent text processing - Volume Part II, CICLing’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascual Martinez-G´omez</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online adaptation strategies for statistical machine translation in postediting scenarios.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>9</issue>
<pages>3203</pages>
<marker>Martinez-G´omez, Sanchis-Trilles, Casacuberta, 2012</marker>
<rawString>Pascual Martinez-G´omez, Germ´an Sanchis-Trilles, and Francisco Casacuberta. 2012. Online adaptation strategies for statistical machine translation in postediting scenarios. Pattern Recognition, 45(9):3193– 3203, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Mathur</author>
<author>Mauro Cettolo</author>
<author>Marcello Federico</author>
</authors>
<title>Online Learning Approaches in Computer Assisted Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="10143" citStr="Mathur et al., 2013" startWordPosition="1613" endWordPosition="1616">t incrementally store translated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Martinez et al., 2010; Mart´ınez-G´omez et al., 2011; Martinez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Online QE for CAT environments When operating with advanced CAT tools, translators are presented with suggestions (either matching fragments from a translation memory or automatic translations produced by an MT system) for each sentence of a source document. Before being approved and published, translation suggestions may require different amounts of post-editing operation</context>
</contexts>
<marker>Mathur, Cettolo, Federico, 2013</marker>
<rawString>Prashant Mathur, Mauro Cettolo, and Marcello Federico. 2013. Online Learning Approaches in Computer Assisted Translation. In Proceedings of the 8th Workshop on Statistical Machine Translation, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Match without a Referee: Evaluating MT Adequacy without Reference Translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Statistical Machine Translation,</booktitle>
<pages>171--180</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2531" citStr="Mehdad et al., 2012" startWordPosition="381" endWordPosition="384">gestions obtained from MT engines with those derived from translation memories (TMs). The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi a</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy without Reference Translations. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 171–180, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ortiz-Martfnez</author>
<author>Ismael Garcfa-Varea</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online learning for interactive statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>546--554</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Ortiz-Martfnez, Garcfa-Varea, Casacuberta, 2010</marker>
<rawString>Daniel Ortiz-Martfnez, Ismael Garcfa-Varea, and Francisco Casacuberta. 2010. Online learning for interactive statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 546–554, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Parrella</author>
</authors>
<title>Online support vector regression.</title>
<date>2007</date>
<tech>Master’s Thesis,</tech>
<institution>Department of Information Science, University of Genoa, Italy.</institution>
<contexts>
<context position="17992" citStr="Parrella, 2007" startWordPosition="2928" endWordPosition="2929">he seventeen features proposed in (Specia et al., 2009). This feature set, fully described in (Callison-Burch et al., 2012), takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The results of previous WMT QE shared tasks have shown that these baseline features are particularly competitive in the regression task (with only few systems able to beat them at WMT12). 4.3 Online algorithms In our experiments we evaluate two online algorithms, OnlineSVR (Parrella, 2007)8 and Passive8http://www2.imperial.ac.uk/˜gmontana/ onlinesvr.htm Aggressive Perceptron (Crammer et al., 2006),9 by comparing their performance with a batch learning strategy based on the Scikit-learn implementation of Support Vector Regression (SVR).10 The choice of the OnlineSVR and PassiveAggressive (OSVR and PA henceforth) is motivated by different considerations. From a performance point of view, as an adaptation of E-SVR which proved to be one of the top performing algorithms in the regression QE tasks at WMT, OSVR seems to be the best candidate. For this reason, we use the online adapta</context>
</contexts>
<marker>Parrella, 2007</marker>
<rawString>Francesco Parrella. 2007. Online support vector regression. Master’s Thesis, Department of Information Science, University of Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Jos´e G C de Souza</author>
<author>Jennifer Foster</author>
<author>Lucia Specia</author>
</authors>
<title>Topic Models for Translation Quality Estimation for Gisting Purposes.</title>
<date>2013</date>
<booktitle>In Proceedings of the Machine Translation Summit XIV,</booktitle>
<location>Nice, France.</location>
<marker>Rubino, de Souza, Foster, Specia, 2013</marker>
<rawString>Raphael Rubino, Jos´e G.C. de Souza, Jennifer Foster, and Lucia Specia. 2013a. Topic Models for Translation Quality Estimation for Gisting Purposes. In Proceedings of the Machine Translation Summit XIV, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Antonio Toral</author>
<author>S Cort´es Va´ıllo</author>
<author>Jun Xie</author>
<author>Xiaofeng Wu</author>
<author>Stephen Doherty</author>
<author>Qun Liu</author>
</authors>
<title>The CNGL-DCU-Prompsit translation systems for WMT13.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<pages>211--216</pages>
<location>Sofia, Bulgaria.</location>
<marker>Rubino, Toral, Va´ıllo, Xie, Wu, Doherty, Liu, 2013</marker>
<rawString>Raphael Rubino, Antonio Toral, S Cort´es Va´ıllo, Jun Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu. 2013b. The CNGL-DCU-Prompsit translation systems for WMT13. In Proceedings of the 8th Workshop on Statistical Machine Translation, pages 211– 216, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Marco Turchi</author>
<author>Lucia Specia</author>
</authors>
<title>An Efficient and User-friendly Tool for Machine Translation Quality Estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation,</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="12569" citStr="Shah et al., 2014" startWordPosition="2004" endWordPosition="2007">is represented by (source, target) pairs; • The prediction p(xi) is the automatically estimated HTER score; • The true label ˆp(xi) is the actual HTER score calculated over the target and its post-edition. At each step of the process, the goal of the learner is to exploit user post-editions to reduce the difference between the predicted HTER values and the true labels for the following (source, target) pairs. As depicted in Figure 1, this is done as follows: 1. At step i, an unlabelled (source, target) pair xi is sent to a feature extraction component. To this aim, we used an adapted version (Shah et al., 2014) of the open-source QuEst6 tool (Specia et al., 2013). The tool, which implements a large number of features proposed by participants in the WMT QE shared tasks, has been modified to process one sentence at a time as requested for integration in a CAT environment; 5Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference. Lower HTER values indicate better translations. 6http://www.quest.dcs.shef.ac.uk/ 712 Figure 1: Online QE workflow. &lt;src&gt;, &lt;trg&gt; and &lt;pe&gt; respectively stand for the source senten</context>
</contexts>
<marker>Shah, Turchi, Specia, 2014</marker>
<rawString>Kashif Shah, Marco Turchi, and Lucia Specia. 2014. An Efficient and User-friendly Tool for Machine Translation Quality Estimation. In Proceedings of the 9th International Conference on Language Resources and Evaluation, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="8275" citStr="Snover et al., 2006" startWordPosition="1332" endWordPosition="1335">y unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. 1Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2http://www.statmt.org/wmt13/ 3For a comprehensive overview of the QE approaches proposed so far we refer the reader to the WMT12 and WMT13 QE shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013). 711 These interconnected issues are particularly relevant in the CAT framework, where translation jobs from different domains are routed to professional translators with different idiolect, background and quality standards. The first aspect, modelling annotators’ individual behaviour and interdependences, has been addressed by Cohn and</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Nguyen Bach</author>
<author>Ziyuan Wang</author>
</authors>
<title>The SDL Language Weaver Systems in the WMT12 Quality Estimation Shared Task.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Statistical Machine Translation (WMT’12),</booktitle>
<pages>145--151</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7328" citStr="Soricut et al., 2012" startWordPosition="1182" endWordPosition="1185">bels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherentl</context>
</contexts>
<marker>Soricut, Bach, Wang, 2012</marker>
<rawString>Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver Systems in the WMT12 Quality Estimation Shared Task. In Proceedings of the 7th Workshop on Statistical Machine Translation (WMT’12), pages 145–151, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09),</booktitle>
<pages>28--35</pages>
<location>Barcelona,</location>
<contexts>
<context position="2509" citStr="Specia et al., 2009" startWordPosition="377" endWordPosition="380">he integration of suggestions obtained from MT engines with those derived from translation memories (TMs). The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi </context>
<context position="17432" citStr="Specia et al., 2009" startWordPosition="2838" endWordPosition="2841">ks (Rubino et al., 2013a) demonstrated that its results can be particularly hard to beat. 4.2 Performance indicator and feature set To measure the adaptability of our model to a given test set we compute the Mean Absolute Error (MAE), a metric for regression problems also used in the WMT QE shared tasks. The MAE is the average of the absolute errors ez = |fz − yz|, where fz is the prediction of the model and yz is the true value for the ith instance. As our focus is on the algorithmic aspect, in all experiments we use the same feature set, which consists of the seventeen features proposed in (Specia et al., 2009). This feature set, fully described in (Callison-Burch et al., 2012), takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The results of previous WMT QE shared tasks have shown that these baseline features are particularly competitive in the regression task (with only few systems able to beat them at WMT12). 4.3 Online algorithms In our experiments we evaluate two online algorithms, OnlineSVR (Parrella, 2007)8 and Passive8http://www2.imperial.ac.uk</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09), pages 28–35, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Dhwaj Raj</author>
<author>Marco Turchi</author>
</authors>
<date>2010</date>
<booktitle>Machine Translation Evaluation versus Quality Estimation. Machine translation,</booktitle>
<pages>24--1</pages>
<contexts>
<context position="6761" citStr="Specia et al., 2010" startWordPosition="1079" endWordPosition="1083">degrees of similarity with the data used for training. Our results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework. Our adaptive QE infrastructure has been released as open source. Its C++ implementation is available at http://hlt.fbk.eu/technologies/ aqet. 2 Related work QE is generally cast as a supervised machine learning task, where a model trained from a collection of (source, target, label) instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a</context>
</contexts>
<marker>Specia, Raj, Turchi, 2010</marker>
<rawString>Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine Translation Evaluation versus Quality Estimation. Machine translation, 24(1):39–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jos´e G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - A Translation Quality Estimation Framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL2013,</booktitle>
<pages>79--84</pages>
<location>Sofia, Bulgaria.</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jos´e G.C. de Souza, and Trevor Cohn. 2013. QuEst - A Translation Quality Estimation Framework. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL2013, pages 79–84, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>Automatic Annotation of Machine Translation Datasets with Binary Quality Judgements.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation,</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="3146" citStr="Turchi and Negri, 2014" startWordPosition="483" endWordPosition="486">., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; 2. Each translation job has its own specificities (domain, complexity of the source text, average target quality). Since data from a new job may differ from those used to train the QE model, its estimates on the new instances might result to be biased or uninformative. The ability of a system to self-adapt </context>
</contexts>
<marker>Turchi, Negri, 2014</marker>
<rawString>Marco Turchi and Matteo Negri. 2014. Automatic Annotation of Machine Translation Datasets with Binary Quality Judgements. In Proceedings of the 9th International Conference on Language Resources and Evaluation, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Coping with the Subjectivity of Human Judgements in MT Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<pages>240--251</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3121" citStr="Turchi et al., 2013" startWordPosition="479" endWordPosition="482">., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; 2. Each translation job has its own specificities (domain, complexity of the source text, average target quality). Since data from a new job may differ from those used to train the QE model, its estimates on the new instances might result to be biased or uninformative. The ability o</context>
</contexts>
<marker>Turchi, Negri, Federico, 2013</marker>
<rawString>Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the Subjectivity of Human Judgements in MT Quality Estimation. In Proceedings of the 8th Workshop on Statistical Machine Translation, pages 240–251, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More Accurate Tests for the Statistical Significance of Result Differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics (COLING</booktitle>
<volume>2</volume>
<pages>947--953</pages>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="22902" citStr="Yeh, 2000" startWordPosition="3743" endWordPosition="3744">forming algorithm for each type of model (batch, adaptive, empty). As can be seen, close MAE values show a similar behaviour for the three types of models.11 With the same amount of training data, the performance of the batch and the adaptive models (in this case always obtained with OSVR) is almost identical. This demonstrates that, as expected, the online algorithms do not take 11Results marked with the “*” symbol are NOT statistically significant compared to the corresponding batch model. The others are always statistically significant at p≤0.005, calculated with approximate randomization (Yeh, 2000). advantage of test data with a label distribution similar to the training set. All the models outperform the baseline, even if the minimal differences confirm the competitiveness of such a simple approach. Overall, these results bring some interesting indications about the behaviour of the different online algorithms. First, the good results achieved by the empty models (less than one MAE point separates them from the best ones built on the largest training set) suggest their high potential when training data are not available. Second, our results show that OSVR is always the best performing </context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More Accurate Tests for the Statistical Significance of Result Differences. In Proceedings of the 18th conference on Computational linguistics (COLING 2000) - Volume 2, pages 947–953, Saarbrucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>