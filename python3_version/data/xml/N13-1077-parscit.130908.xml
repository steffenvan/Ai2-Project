<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000705">
<title confidence="0.994209">
Zipfian corruptions for robust POS tagging
</title>
<author confidence="0.994825">
Anders Søgaard
</author>
<affiliation confidence="0.997843">
Center for Language Technology
University of Copenhagen
</affiliation>
<email confidence="0.995394">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.983422" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.781605076923077">
Inspired by robust generalization and adver-
sarial learning we describe a novel approach
to learning structured perceptrons for part-of-
speech (POS) tagging that is less sensitive to
domain shifts. The objective of our method is
to minimize average loss under random distri-
bution shifts. We restrict the possible target
distributions to mixtures of the source distri-
bution and random Zipfian distributions. Our
algorithm is used for POS tagging and eval-
uated on the English Web Treebank and the
Danish Dependency Treebank with an average
4.4% error reduction in tagging accuracy.
</figureCaption>
<sectionHeader confidence="0.997383" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973769230769">
Supervised learning approaches have advanced the
state of the art on a variety of tasks in natural lan-
guage processing, often resulting in systems ap-
proaching the level of inter-annotator agreement on
in-domain data, e.g. in POS tagging, where Shen
et al. (2007) report a tagging accuracy of 97.3%.
However, performance of state-of-the-art supervised
systems is known to drop considerably on out-of-
domain data. State-of-the-art POS taggers trained
on the Penn Treebank (Marcus et al., 1993) mapped
to Google’s universal tag set (Petrov et al., 2011)
achieve tagging accuracies in the range of 89–91%
on Web 2.0 data (Petrov and McDonald, 2012) .
To bridge this gap we may consider using semi-
supervised or transfer learning methods to adjust to
new target domains (Blitzer et al., 2006; Daume III,
2007), pooling unlabeled data from those domains.
However, in many applications this is not possible.
If we want to provide an online service or design a
piece of software with many potential users covering
a wide range of use cases, we do not know the target
domain in advance. This is the usual problem of ro-
bust learning, but in this paper we describe a novel
learning algorithm that goes beyond robust learning
by making various assumptions about the difference
between the source domain and the (unknown) target
domain. Under these assumptions we can minimize
average loss under (all possible or a representative
sample of) domain shifts. We evaluate our approach
on two recently introduced cross-domain POS tag-
ging datasets.
Our approach is inspired by work in robust gen-
eralization (Ben-Tal and Nemirovski, 1998; Trafalis
and Gilbert, 2007) and adversarial learning (Glober-
son and Roweis, 2006; Dekel and Shamir, 2008;
Søgaard and Johannsen, 2012). Our approach also
bears similarities to feature bagging (Sutton et al.,
2006). Sutton et al. (2006) noted that in learning of
linear models useful features are often swamped by
correlating, but more indicative features. If the more
indicative features are absent in the target domain
due to out-of-vocabulary (OOV) effects, we are left
with the swamped features which were not updated
properly. This is, indirectly, the problem solved in
adversarial learning with corrupted data points. Ad-
versarial learning can also be seen as a way of av-
eraging exponentially many models (Hinton et al.,
2012).
Adversarial learning techniques have been devel-
oped for security-related learning tasks, e.g. where
systems need to be robust to failing sensors. We also
show how we can do better than straight-forward ap-
</bodyText>
<page confidence="0.965581">
668
</page>
<subsectionHeader confidence="0.292532">
Proceedings of NAACL-HLT 2013, pages 668–672,
</subsectionHeader>
<bodyText confidence="0.948370142857143">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
plication of adversarial learning techniques by mak-
ing a second assumption about our data, namely that
domains are mixtures of Zipfian distributions over
our features. Similar assumptions have been made
before in computational linguistics, e.g. by Goldberg
and Elhadad (2008).
</bodyText>
<sectionHeader confidence="0.810185" genericHeader="introduction">
2 Approach overview
</sectionHeader>
<bodyText confidence="0.998918205128205">
In this paper we consider the structured perceptron
(Collins, 2002) – with POS tagging as our practical
application. The structured perceptron is prone to
feature swamping (Sutton et al., 2006), and we want
to prevent that using a technique inspired by adver-
sarial learning (Globerson and Roweis, 2006; Dekel
and Shamir, 2008). The modification presented here
to the structured perceptron only affects a single line
of code in a publicly available implementation (see
below), but the consequences are significant.
Online adversarial learning (Søgaard and Jo-
hannsen, 2012), briefly, works by sampling random
corruptions of our data, or random feature deletions,
in the learning phase. A discriminative learner see-
ing corrupted data points with missing features will
not update part of the model and will thus try to
find a decision boundary classifying the training data
correctly relying on the remaining features. This de-
cision boundary may be very different from the deci-
sion boundary found otherwise by the discriminative
learner. If we sample enough corruptions, the model
learned from the corrupted data will converge on the
model minimizing average loss over all corruptions
(Dekel and Shamir, 2008).
Example Consider the plot in Figure 1. The solid
line with no stars (2d-fit) is the SVM fit in two
dimensions, while the dashed line is what that fit
amounts to if the feature x is missing in the tar-
get. The solid line with stars (1d-fit) is our fit if we
could predict the missing feature, training an SVM
only with the y feature. The 1d-fit decision bound-
ary only misclassifies a single data point compared
to the original fit which misclassifies more than 15
negatives with the x feature missing.
The plot thus shows that the best fit in m dimen-
sions is often not the best in &lt; m dimensions. Con-
sequently, if we think there is a risk that features will
be missing in the target, finding the best fit in m di-
mensions is not necessarily the best we can do. Of
</bodyText>
<figureCaption confidence="0.996789">
Figure 1: The best fit in m dimensions is often not the
best in &lt; m dimensions.
</figureCaption>
<bodyText confidence="0.999930285714286">
course we do not know what features will be miss-
ing in advance. The intuition in adversarial learning
is that we may obtain more robust decision bound-
aries by minimizing loss over a set of possible fea-
ture deletions. We extend this idea below, modeling
not only OOV effects, but a broader class of distri-
butional shifts.
</bodyText>
<sectionHeader confidence="0.970891" genericHeader="method">
3 Structured perceptron
</sectionHeader>
<bodyText confidence="0.994216681818182">
The structured perceptron (Collins, 2002) models
sequences as Markov chains of unobserved variables
(POS), each emitting an observed variable (a word
form). The structured perceptron is similar to the av-
eraged perceptron (Freund and Schapire, 1999), ex-
cept data points are sequences of vectors rather than
just vectors. Consequently, the structured percep-
tron does not predict a class label but a sequence of
labels (using Viterbi decoding). In learning we up-
date the features at the positions where the predicted
labels are different from the true labels. We do this
by adding weight to features present in the correct
solution and subtracting weight from features only
present in the predicted solution. The generic aver-
aged perceptron learning algorithm is presented in
Figure 2. A publicly available and easy-to-modify
Python reimplementation of the structured percep-
tron can be found in the LXMLS toolkit.1 We use
the LXMLS toolkit as our baseline with the default
feature model, but use the PTB tagset rather than the
Google tagset (Petrov et al., 2011) used by default
in the LXMLS toolkit.
</bodyText>
<footnote confidence="0.604043">
1https://github.com/gracaninja/lxmls-toolkit
</footnote>
<figure confidence="0.989962071428572">
4
2
0
−2
−4
−6
−8
−10
−12
0 2 4 6 8 10 12
−14
2d-fit
2d-fit with missing feature
1d-fit
</figure>
<page confidence="0.708096">
669
</page>
<listItem confidence="0.70156275">
1: X = {hyi,xii}Ni_1
2: w° = 0,v = 0,i = 0
3: fork ∈ K do
4: for n ∈ N do
5: if sign(w · x) =6 yn then
6: wi+1 ← update(wi)
7: i ← i + 1
8: end if
9: v ← v + wi
10: end for
11: end for
12: return w = v/(N × K)
</listItem>
<figureCaption confidence="0.996015">
Figure 2: Generic averaged perceptron
</figureCaption>
<sectionHeader confidence="0.90791" genericHeader="method">
4 Minimizing loss under OOV effects
</sectionHeader>
<bodyText confidence="0.99971496969697">
We will think of domain shifts as data point corrup-
tions. Søgaard and Johannsen (2012) model domain
shifts using binary vectors of length m where m is
the size of of our feature representation. Each vector
then represents an expected OOV effect by encoding
what features are (predicted to be) missing in the tar-
get data, i.e. the ith feature will be missing if the ith
element of the binary vector is 0. However, since
we are minimizing average loss under OOV effects
it makes sense to restrict the class of vectors to en-
code OOV effects that we are likely to observe. This
could, for example, involve fixing an expected rate
of missing features or bounding it by some interval,
or it could involve distinguishing between features
that are likely to be missing in the target and fea-
tures that are not. Here is what we do in this paper:
Rather than thinking of domain shifts as some-
thing that deletes features, we propose to see do-
main shifts as something making certain features
less likely to occur in our data. We will in other
words simulate soft OOV effects, rather than hard
OOV effects. One way to think of this is as an im-
portance weighting of our features. This section pro-
vides some intuition for using inverse Zipfian distri-
butions as weight functions.
Say we are interested in making a model BD1
learned from a known distribution D1 robust against
the distributional differences between D1 and an un-
known distribution D2. These two distributions are
somehow related to a distribution D0 (the underlying
language distribution from which the domain distri-
butions are sampled).
It is common to assume that linguistic distribu-
</bodyText>
<listItem confidence="0.69439475">
1: X = {hyi,xii}Ni_1
2: w° = 0,v = 0,i = 0
3: fork ∈ K do
4: for n ∈ N do
</listItem>
<figure confidence="0.729460333333333">
5: � ← random.zipf(3, M)
6: if sign(w · x ◦ �) =6 yn then
7: wi+1 ← update(wi)
8: i ← i + 1
9: end if
10: v ← v + wi
11: end for
12: end for
13: return w = v/(N × K)
</figure>
<figureCaption confidence="0.999676">
Figure 3: Z3SP
</figureCaption>
<bodyText confidence="0.994486515151515">
tions follow power laws (Zipf, 1935; Goldberg and
Elhadad, 2008). We will assume that D1 = D0 ×Z1
where Z1 is some Zipfian distribution. Say D0 ∼ Z0
is the master Zipfian distribution of language L0. If
we assume that (otherwise independent) domains L1
and L2 follow products of Zipfians Z0 × Z1 and
Z0 × Z2, we derive the following:
Say w = BZ0xZ1 is the model learned from the
source data. The ideal model is w&apos; = BZ0xZ2, but
both Zipfians Z1 and Z2 are unknown. Since Z2
is unknown (and in many applications, we want to
model several Zi), the overall best model we can
hope for is w&apos; = BZ0. Z0 is also unknown, but we
can observe a finite sample Z0 × Z1. Since the den-
sity of Z1 is directly related to the weights in w, a
crude estimate of BZ0 would be w&apos; ∼ w Z11. Since
we cannot observe Z1, we instead try to minimize
average loss under all hypotheses about Z1.
In practice, we implement the idea of reweight-
ing by random inverse Zipfian distributitons (instead
of binary vectors) in the following way: Passing
through the data in averaged perceptron learning
(Figure 2), we consider one data point at a time. In
order to minize loss in all possible domains, we need
to consider all possible inverse Zipfian reweightings.
This would be possible if we provided a convex
formulation of the minimization problem along the
lines of Dekel and Shamir (2008), but instead we
randomly sample from a Zipfian and factor its in-
verse into our dataset. The parameter of the Zipfians
is set (to 3) on development data (the EWT-email de-
velopment data). The modified learning algorithm,
Z3SP, is presented in Figure 3.
</bodyText>
<page confidence="0.997807">
670
</page>
<sectionHeader confidence="0.965607" genericHeader="method">
5 POS tagging
</sectionHeader>
<bodyText confidence="0.999938266666667">
POS tagging is the problem of assigning syntactic
categories or POS to tokenized word forms in run-
ning text. Most approaches to POS tagging use su-
pervised learning to learn sequence labeling models
from annotated ressources. The major ressource for
English is the Wall Street Journal (WSJ) sections of
the English Treebank (Marcus et al., 1993). POS
taggers are usually trained on Sect. 0–18 and eval-
uated on Sect. 22–24. In this paper we are not in-
terested in in-domain performance on WSJ data, but
rather in developing a robust POS tagger that is less
sensitive to domain shifts than current state-of-the-
art POS taggers and use the splits from a recent pars-
ing shared task rather than the standard POS tagging
ones.
</bodyText>
<sectionHeader confidence="0.999306" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999944">
We train our tagger on Sections 2–21 of the WSJ
sections of the English Treebank, in the Ontotes
4.0 release. This was also the training data used
in the experiments in the Parsing the Web (PTW)
shared task at NAACL 2012.2 In the shared task
they used the coarse-grained Google tagset (Petrov
et al., 2011). We believe this tagset is too coarse-
grained for most purposes (Manning, 2011) and do
experiments with the original PTB tagset instead.
Our evaluation data comes from the English Web
Treebank (EWT),3 which was also used in the PTW
shared task. The EWT contains development and
evaluation data for five domains: answers (from Ya-
hoo!), emails (from the Enron corpus), BBC news-
groups, Amazon reviews, and weblogs. In order not
to optimize on in-domain data, we tune on the Email
development data and evaluate on the remaining do-
mains (the test sections).
The Web 2.0 data used for evaluation contains a
lot of non-canonical language use. An example is
the sentence you r retarded. from the Email section.
The POS tagger finds no support for r as a verb in the
training data, but needs to infer this from the context.
We also include experiments on the Danish De-
pendency Treebank (DDT) (Buch-Kromann, 2003),
which comes with meta-data enabling us to single
out four domains: newspaper, law, literature and
</bodyText>
<footnote confidence="0.996868">
2https://sites.google.com/site/sancl2012/home/shared-task
3LDC Catalog No.: LDC2012T13.
</footnote>
<table confidence="0.999660125">
SP BSP Z3SP
EWT-answers 85.22 85.45 85.59
EWT-newsgroups 86.82 86.94 87.42
EWT-reviews 84.92 85.14 85.67
EWT-weblogs 87.00 87.06 87.39
DDT-law 92.38 92.80 93.35
DDT-lit 93.61 93.80 93.85
DDT-mag 94.71 94.44 94.68
</table>
<tableCaption confidence="0.9959775">
Table 1: Results. BSP samples binary vectors with prob-
abilities 10 : 0.1,1 : 0.91
</tableCaption>
<bodyText confidence="0.9939135">
magazines. We train our tagger on the newspaper
data and evaluate on the remaining three sections.
</bodyText>
<sectionHeader confidence="0.641478" genericHeader="evaluation">
6.1 Results
</sectionHeader>
<bodyText confidence="0.999982166666667">
The results are presented in Table 1. We first note
that improvements over the structured perceptron
are statistically significant with p &lt; 0.01 across all
domains, except DDT-mag. We also note that us-
ing inverse Zipfian reweightings is better than using
binary vectors in almost all cases. We believe that
these are strong results given that we are assuming
no knowledge of the target domain, and our mod-
ification of the learning algorithm does not affect
computational efficiency at training or test time. The
average error reduction of Z3SP over the structured
perceptron (SP) is 8%. Since using inverse Zipfian
reweightings seems more motivated for node poten-
tials than for edge potentials, we also tried using
BSP for edge potentials and Z3SP for node poten-
tials. This mixed model acchieved 93.70, 93.91 and
94.35 on the DDT data, which on average is slightly
better than Z3SP.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999727">
Inspired by robust generalization and adversarial
learning we introduced a novel approach to learning
structured perceptrons for sequential labeling, which
is less sensitive to OOV effects. We evaluated our
approach on POS tagging data from the EWT and
the DDT with an average 4.4% error reduction over
the structured perceptron.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.686525">
Anders Søgaard is funded by the ERC Starting Grant
LOWLANDS No. 313695.
</reference>
<page confidence="0.999065">
671
</page>
<sectionHeader confidence="0.990199" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843696428571">
Aharon Ben-Tal and Arkadi Nemirovski. 1998. Robust
convex optimization. Mathematics of Operations Re-
search, 23(4).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Matthias Buch-Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG Treebank Tool. In
TLT.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
Ofer Dekel and Ohad Shamir. 2008. Learning to classify
with missing and corrupted features. In ICML.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37:277–296.
Amir Globerson and Sam Roweis. 2006. Nightmare
at test time: robust learning by feature deletion. In
ICML.
Yoav Goldberg and Michael Elhadad. 2008. splitSVM:
fast, space-efficient, non-heuristic, polynomial kernel
computation for NLP applications. In ACL.
Geoffrey Hinton, N. Srivastava, A. Krizhevsky,
I. Sutskever, and R. Salakhutdinov. 2012. Im-
proving neural networks by preventing co-adaptation
of feature detectors. http://arxiv.org/abs/1207.0580.
Chris Manning. 2011. Part-of-speech tagging from
97%˜to 100%: Is it time for some linguistics? In CI-
CLing.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313–330.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011. A universal part-of-speech tagset. CoRR
abs/1104.2086.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In ACL.
Anders Søgaard and Anders Johannsen. 2012. Robust
learning in random subspaces: equipping NLP against
OOV effects. In COLING.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2006. Reducing weight undertraining in struc-
tured discriminative learning. In NAACL.
T Trafalis and R Gilbert. 2007. Robust support vector
machines for classification and computational issues.
Optimization Methods and Software, 22:187–198.
George Zipf. 1935. The psycho-biology of language.
Houghton Mifflin.
</reference>
<page confidence="0.998261">
672
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811445">
<title confidence="0.983905">Zipfian corruptions for robust POS tagging</title>
<author confidence="0.891137">Anders</author>
<affiliation confidence="0.9977065">Center for Language University of</affiliation>
<email confidence="0.939941">soegaard@hum.ku.dk</email>
<abstract confidence="0.997702285714286">Inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech (POS) tagging that is less sensitive to domain shifts. The objective of our method is to minimize average loss under random distribution shifts. We restrict the possible target distributions to mixtures of the source distribution and random Zipfian distributions. Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Anders Søgaard is funded by the ERC Starting Grant LOWLANDS</title>
<tech>No. 313695.</tech>
<marker></marker>
<rawString>Anders Søgaard is funded by the ERC Starting Grant LOWLANDS No. 313695.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aharon Ben-Tal</author>
<author>Arkadi Nemirovski</author>
</authors>
<title>Robust convex optimization.</title>
<date>1998</date>
<journal>Mathematics of Operations Research,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2350" citStr="Ben-Tal and Nemirovski, 1998" startWordPosition="368" endWordPosition="371">l users covering a wide range of use cases, we do not know the target domain in advance. This is the usual problem of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts. We evaluate our approach on two recently introduced cross-domain POS tagging datasets. Our approach is inspired by work in robust generalization (Ben-Tal and Nemirovski, 1998; Trafalis and Gilbert, 2007) and adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008; Søgaard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning wit</context>
</contexts>
<marker>Ben-Tal, Nemirovski, 1998</marker>
<rawString>Aharon Ben-Tal and Arkadi Nemirovski. 1998. Robust convex optimization. Mathematics of Operations Research, 23(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1519" citStr="Blitzer et al., 2006" startWordPosition="232" endWordPosition="235">evel of inter-annotator agreement on in-domain data, e.g. in POS tagging, where Shen et al. (2007) report a tagging accuracy of 97.3%. However, performance of state-of-the-art supervised systems is known to drop considerably on out-ofdomain data. State-of-the-art POS taggers trained on the Penn Treebank (Marcus et al., 1993) mapped to Google’s universal tag set (Petrov et al., 2011) achieve tagging accuracies in the range of 89–91% on Web 2.0 data (Petrov and McDonald, 2012) . To bridge this gap we may consider using semisupervised or transfer learning methods to adjust to new target domains (Blitzer et al., 2006; Daume III, 2007), pooling unlabeled data from those domains. However, in many applications this is not possible. If we want to provide an online service or design a piece of software with many potential users covering a wide range of use cases, we do not know the target domain in advance. This is the usual problem of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (al</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Buch-Kromann</author>
</authors>
<title>The Danish Dependency Treebank and the DTAG Treebank Tool.</title>
<date>2003</date>
<booktitle>In TLT.</booktitle>
<contexts>
<context position="13085" citStr="Buch-Kromann, 2003" startWordPosition="2251" endWordPosition="2252">tion data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs. In order not to optimize on in-domain data, we tune on the Email development data and evaluate on the remaining domains (the test sections). The Web 2.0 data used for evaluation contains a lot of non-canonical language use. An example is the sentence you r retarded. from the Email section. The POS tagger finds no support for r as a verb in the training data, but needs to infer this from the context. We also include experiments on the Danish Dependency Treebank (DDT) (Buch-Kromann, 2003), which comes with meta-data enabling us to single out four domains: newspaper, law, literature and 2https://sites.google.com/site/sancl2012/home/shared-task 3LDC Catalog No.: LDC2012T13. SP BSP Z3SP EWT-answers 85.22 85.45 85.59 EWT-newsgroups 86.82 86.94 87.42 EWT-reviews 84.92 85.14 85.67 EWT-weblogs 87.00 87.06 87.39 DDT-law 92.38 92.80 93.35 DDT-lit 93.61 93.80 93.85 DDT-mag 94.71 94.44 94.68 Table 1: Results. BSP samples binary vectors with probabilities 10 : 0.1,1 : 0.91 magazines. We train our tagger on the newspaper data and evaluate on the remaining three sections. 6.1 Results The re</context>
</contexts>
<marker>Buch-Kromann, 2003</marker>
<rawString>Matthias Buch-Kromann. 2003. The Danish Dependency Treebank and the DTAG Treebank Tool. In TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3785" citStr="Collins, 2002" startWordPosition="587" endWordPosition="588">, e.g. where systems need to be robust to failing sensors. We also show how we can do better than straight-forward ap668 Proceedings of NAACL-HLT 2013, pages 668–672, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 2 Approach overview In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the</context>
<context position="6152" citStr="Collins, 2002" startWordPosition="993" endWordPosition="994">e think there is a risk that features will be missing in the target, finding the best fit in m dimensions is not necessarily the best we can do. Of Figure 1: The best fit in m dimensions is often not the best in &lt; m dimensions. course we do not know what features will be missing in advance. The intuition in adversarial learning is that we may obtain more robust decision boundaries by minimizing loss over a set of possible feature deletions. We extend this idea below, modeling not only OOV effects, but a broader class of distributional shifts. 3 Structured perceptron The structured perceptron (Collins, 2002) models sequences as Markov chains of unobserved variables (POS), each emitting an observed variable (a word form). The structured perceptron is similar to the averaged perceptron (Freund and Schapire, 1999), except data points are sequences of vectors rather than just vectors. Consequently, the structured perceptron does not predict a class label but a sequence of labels (using Viterbi decoding). In learning we update the features at the positions where the predicted labels are different from the true labels. We do this by adding weight to features present in the correct solution and subtract</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Ohad Shamir</author>
</authors>
<title>Learning to classify with missing and corrupted features.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="2456" citStr="Dekel and Shamir, 2008" startWordPosition="384" endWordPosition="387">em of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts. We evaluate our approach on two recently introduced cross-domain POS tagging datasets. Our approach is inspired by work in robust generalization (Ben-Tal and Nemirovski, 1998; Trafalis and Gilbert, 2007) and adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008; Søgaard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning with corrupted data points. Adversarial learning can also be seen as a way of averaging exponentially many mo</context>
<context position="4044" citStr="Dekel and Shamir, 2008" startWordPosition="627" endWordPosition="630">plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 2 Approach overview In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the learning phase. A discriminative learner seeing corrupted data points with missing features will not update part of the model and will thus try to find a decision boundary classifying the training data correctly relying on the remaining features. This decisi</context>
<context position="10874" citStr="Dekel and Shamir (2008)" startWordPosition="1868" endWordPosition="1871">timate of BZ0 would be w&apos; ∼ w Z11. Since we cannot observe Z1, we instead try to minimize average loss under all hypotheses about Z1. In practice, we implement the idea of reweighting by random inverse Zipfian distributitons (instead of binary vectors) in the following way: Passing through the data in averaged perceptron learning (Figure 2), we consider one data point at a time. In order to minize loss in all possible domains, we need to consider all possible inverse Zipfian reweightings. This would be possible if we provided a convex formulation of the minimization problem along the lines of Dekel and Shamir (2008), but instead we randomly sample from a Zipfian and factor its inverse into our dataset. The parameter of the Zipfians is set (to 3) on development data (the EWT-email development data). The modified learning algorithm, Z3SP, is presented in Figure 3. 670 5 POS tagging POS tagging is the problem of assigning syntactic categories or POS to tokenized word forms in running text. Most approaches to POS tagging use supervised learning to learn sequence labeling models from annotated ressources. The major ressource for English is the Wall Street Journal (WSJ) sections of the English Treebank (Marcus</context>
</contexts>
<marker>Dekel, Shamir, 2008</marker>
<rawString>Ofer Dekel and Ohad Shamir. 2008. Learning to classify with missing and corrupted features. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="6359" citStr="Freund and Schapire, 1999" startWordPosition="1022" endWordPosition="1025"> not the best in &lt; m dimensions. course we do not know what features will be missing in advance. The intuition in adversarial learning is that we may obtain more robust decision boundaries by minimizing loss over a set of possible feature deletions. We extend this idea below, modeling not only OOV effects, but a broader class of distributional shifts. 3 Structured perceptron The structured perceptron (Collins, 2002) models sequences as Markov chains of unobserved variables (POS), each emitting an observed variable (a word form). The structured perceptron is similar to the averaged perceptron (Freund and Schapire, 1999), except data points are sequences of vectors rather than just vectors. Consequently, the structured perceptron does not predict a class label but a sequence of labels (using Viterbi decoding). In learning we update the features at the positions where the predicted labels are different from the true labels. We do this by adding weight to features present in the correct solution and subtracting weight from features only present in the predicted solution. The generic averaged perceptron learning algorithm is presented in Figure 2. A publicly available and easy-to-modify Python reimplementation o</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
<author>Sam Roweis</author>
</authors>
<title>Nightmare at test time: robust learning by feature deletion.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="2432" citStr="Globerson and Roweis, 2006" startWordPosition="379" endWordPosition="383">nce. This is the usual problem of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts. We evaluate our approach on two recently introduced cross-domain POS tagging datasets. Our approach is inspired by work in robust generalization (Ben-Tal and Nemirovski, 1998; Trafalis and Gilbert, 2007) and adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008; Søgaard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning with corrupted data points. Adversarial learning can also be seen as a way of averagi</context>
<context position="4019" citStr="Globerson and Roweis, 2006" startWordPosition="623" endWordPosition="626">r Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 2 Approach overview In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the learning phase. A discriminative learner seeing corrupted data points with missing features will not update part of the model and will thus try to find a decision boundary classifying the training data correctly relying on the remain</context>
</contexts>
<marker>Globerson, Roweis, 2006</marker>
<rawString>Amir Globerson and Sam Roweis. 2006. Nightmare at test time: robust learning by feature deletion. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>splitSVM: fast, space-efficient, non-heuristic, polynomial kernel computation for NLP applications.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3696" citStr="Goldberg and Elhadad (2008)" startWordPosition="572" endWordPosition="575">et al., 2012). Adversarial learning techniques have been developed for security-related learning tasks, e.g. where systems need to be robust to failing sensors. We also show how we can do better than straight-forward ap668 Proceedings of NAACL-HLT 2013, pages 668–672, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 2 Approach overview In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), brief</context>
<context position="9582" citStr="Goldberg and Elhadad, 2008" startWordPosition="1628" endWordPosition="1631">wn distribution D1 robust against the distributional differences between D1 and an unknown distribution D2. These two distributions are somehow related to a distribution D0 (the underlying language distribution from which the domain distributions are sampled). It is common to assume that linguistic distribu1: X = {hyi,xii}Ni_1 2: w° = 0,v = 0,i = 0 3: fork ∈ K do 4: for n ∈ N do 5: � ← random.zipf(3, M) 6: if sign(w · x ◦ �) =6 yn then 7: wi+1 ← update(wi) 8: i ← i + 1 9: end if 10: v ← v + wi 11: end for 12: end for 13: return w = v/(N × K) Figure 3: Z3SP tions follow power laws (Zipf, 1935; Goldberg and Elhadad, 2008). We will assume that D1 = D0 ×Z1 where Z1 is some Zipfian distribution. Say D0 ∼ Z0 is the master Zipfian distribution of language L0. If we assume that (otherwise independent) domains L1 and L2 follow products of Zipfians Z0 × Z1 and Z0 × Z2, we derive the following: Say w = BZ0xZ1 is the model learned from the source data. The ideal model is w&apos; = BZ0xZ2, but both Zipfians Z1 and Z2 are unknown. Since Z2 is unknown (and in many applications, we want to model several Zi), the overall best model we can hope for is w&apos; = BZ0. Z0 is also unknown, but we can observe a finite sample Z0 × Z1. Since </context>
</contexts>
<marker>Goldberg, Elhadad, 2008</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2008. splitSVM: fast, space-efficient, non-heuristic, polynomial kernel computation for NLP applications. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>N Srivastava</author>
<author>A Krizhevsky</author>
<author>I Sutskever</author>
<author>R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors.</title>
<date>2012</date>
<note>http://arxiv.org/abs/1207.0580.</note>
<contexts>
<context position="3082" citStr="Hinton et al., 2012" startWordPosition="483" endWordPosition="486">ard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning with corrupted data points. Adversarial learning can also be seen as a way of averaging exponentially many models (Hinton et al., 2012). Adversarial learning techniques have been developed for security-related learning tasks, e.g. where systems need to be robust to failing sensors. We also show how we can do better than straight-forward ap668 Proceedings of NAACL-HLT 2013, pages 668–672, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and </context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. http://arxiv.org/abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
</authors>
<title>Part-of-speech tagging from 97%˜to 100%: Is it time for some linguistics? In CICLing.</title>
<date>2011</date>
<contexts>
<context position="12261" citStr="Manning, 2011" startWordPosition="2110" endWordPosition="2111">her in developing a robust POS tagger that is less sensitive to domain shifts than current state-of-theart POS taggers and use the splits from a recent parsing shared task rather than the standard POS tagging ones. 6 Experiments We train our tagger on Sections 2–21 of the WSJ sections of the English Treebank, in the Ontotes 4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NAACL 2012.2 In the shared task they used the coarse-grained Google tagset (Petrov et al., 2011). We believe this tagset is too coarsegrained for most purposes (Manning, 2011) and do experiments with the original PTB tagset instead. Our evaluation data comes from the English Web Treebank (EWT),3 which was also used in the PTW shared task. The EWT contains development and evaluation data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs. In order not to optimize on in-domain data, we tune on the Email development data and evaluate on the remaining domains (the test sections). The Web 2.0 data used for evaluation contains a lot of non-canonical language use. An example is the sentence you r retarded. </context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Chris Manning. 2011. Part-of-speech tagging from 97%˜to 100%: Is it time for some linguistics? In CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1225" citStr="Marcus et al., 1993" startWordPosition="181" endWordPosition="184"> English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy. 1 Introduction Supervised learning approaches have advanced the state of the art on a variety of tasks in natural language processing, often resulting in systems approaching the level of inter-annotator agreement on in-domain data, e.g. in POS tagging, where Shen et al. (2007) report a tagging accuracy of 97.3%. However, performance of state-of-the-art supervised systems is known to drop considerably on out-ofdomain data. State-of-the-art POS taggers trained on the Penn Treebank (Marcus et al., 1993) mapped to Google’s universal tag set (Petrov et al., 2011) achieve tagging accuracies in the range of 89–91% on Web 2.0 data (Petrov and McDonald, 2012) . To bridge this gap we may consider using semisupervised or transfer learning methods to adjust to new target domains (Blitzer et al., 2006; Daume III, 2007), pooling unlabeled data from those domains. However, in many applications this is not possible. If we want to provide an online service or design a piece of software with many potential users covering a wide range of use cases, we do not know the target domain in advance. This is the us</context>
<context position="11488" citStr="Marcus et al., 1993" startWordPosition="1971" endWordPosition="1974">(2008), but instead we randomly sample from a Zipfian and factor its inverse into our dataset. The parameter of the Zipfians is set (to 3) on development data (the EWT-email development data). The modified learning algorithm, Z3SP, is presented in Figure 3. 670 5 POS tagging POS tagging is the problem of assigning syntactic categories or POS to tokenized word forms in running text. Most approaches to POS tagging use supervised learning to learn sequence labeling models from annotated ressources. The major ressource for English is the Wall Street Journal (WSJ) sections of the English Treebank (Marcus et al., 1993). POS taggers are usually trained on Sect. 0–18 and evaluated on Sect. 22–24. In this paper we are not interested in in-domain performance on WSJ data, but rather in developing a robust POS tagger that is less sensitive to domain shifts than current state-of-theart POS taggers and use the splits from a recent parsing shared task rather than the standard POS tagging ones. 6 Experiments We train our tagger on Sections 2–21 of the WSJ sections of the English Treebank, in the Ontotes 4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NA</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="1378" citStr="Petrov and McDonald, 2012" startWordPosition="207" endWordPosition="210">ng approaches have advanced the state of the art on a variety of tasks in natural language processing, often resulting in systems approaching the level of inter-annotator agreement on in-domain data, e.g. in POS tagging, where Shen et al. (2007) report a tagging accuracy of 97.3%. However, performance of state-of-the-art supervised systems is known to drop considerably on out-ofdomain data. State-of-the-art POS taggers trained on the Penn Treebank (Marcus et al., 1993) mapped to Google’s universal tag set (Petrov et al., 2011) achieve tagging accuracies in the range of 89–91% on Web 2.0 data (Petrov and McDonald, 2012) . To bridge this gap we may consider using semisupervised or transfer learning methods to adjust to new target domains (Blitzer et al., 2006; Daume III, 2007), pooling unlabeled data from those domains. However, in many applications this is not possible. If we want to provide an online service or design a piece of software with many potential users covering a wide range of use cases, we do not know the target domain in advance. This is the usual problem of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions a</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<note>CoRR abs/1104.2086.</note>
<contexts>
<context position="1284" citStr="Petrov et al., 2011" startWordPosition="191" endWordPosition="194">th an average 4.4% error reduction in tagging accuracy. 1 Introduction Supervised learning approaches have advanced the state of the art on a variety of tasks in natural language processing, often resulting in systems approaching the level of inter-annotator agreement on in-domain data, e.g. in POS tagging, where Shen et al. (2007) report a tagging accuracy of 97.3%. However, performance of state-of-the-art supervised systems is known to drop considerably on out-ofdomain data. State-of-the-art POS taggers trained on the Penn Treebank (Marcus et al., 1993) mapped to Google’s universal tag set (Petrov et al., 2011) achieve tagging accuracies in the range of 89–91% on Web 2.0 data (Petrov and McDonald, 2012) . To bridge this gap we may consider using semisupervised or transfer learning methods to adjust to new target domains (Blitzer et al., 2006; Daume III, 2007), pooling unlabeled data from those domains. However, in many applications this is not possible. If we want to provide an online service or design a piece of software with many potential users covering a wide range of use cases, we do not know the target domain in advance. This is the usual problem of robust learning, but in this paper we descri</context>
<context position="7170" citStr="Petrov et al., 2011" startWordPosition="1155" endWordPosition="1158">). In learning we update the features at the positions where the predicted labels are different from the true labels. We do this by adding weight to features present in the correct solution and subtracting weight from features only present in the predicted solution. The generic averaged perceptron learning algorithm is presented in Figure 2. A publicly available and easy-to-modify Python reimplementation of the structured perceptron can be found in the LXMLS toolkit.1 We use the LXMLS toolkit as our baseline with the default feature model, but use the PTB tagset rather than the Google tagset (Petrov et al., 2011) used by default in the LXMLS toolkit. 1https://github.com/gracaninja/lxmls-toolkit 4 2 0 −2 −4 −6 −8 −10 −12 0 2 4 6 8 10 12 −14 2d-fit 2d-fit with missing feature 1d-fit 669 1: X = {hyi,xii}Ni_1 2: w° = 0,v = 0,i = 0 3: fork ∈ K do 4: for n ∈ N do 5: if sign(w · x) =6 yn then 6: wi+1 ← update(wi) 7: i ← i + 1 8: end if 9: v ← v + wi 10: end for 11: end for 12: return w = v/(N × K) Figure 2: Generic averaged perceptron 4 Minimizing loss under OOV effects We will think of domain shifts as data point corruptions. Søgaard and Johannsen (2012) model domain shifts using binary vectors of length m </context>
<context position="12182" citStr="Petrov et al., 2011" startWordPosition="2095" endWordPosition="2098">24. In this paper we are not interested in in-domain performance on WSJ data, but rather in developing a robust POS tagger that is less sensitive to domain shifts than current state-of-theart POS taggers and use the splits from a recent parsing shared task rather than the standard POS tagging ones. 6 Experiments We train our tagger on Sections 2–21 of the WSJ sections of the English Treebank, in the Ontotes 4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NAACL 2012.2 In the shared task they used the coarse-grained Google tagset (Petrov et al., 2011). We believe this tagset is too coarsegrained for most purposes (Manning, 2011) and do experiments with the original PTB tagset instead. Our evaluation data comes from the English Web Treebank (EWT),3 which was also used in the PTW shared task. The EWT contains development and evaluation data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs. In order not to optimize on in-domain data, we tune on the Email development data and evaluate on the remaining domains (the test sections). The Web 2.0 data used for evaluation contains a</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. CoRR abs/1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="997" citStr="Shen et al. (2007)" startWordPosition="148" endWordPosition="151">e average loss under random distribution shifts. We restrict the possible target distributions to mixtures of the source distribution and random Zipfian distributions. Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy. 1 Introduction Supervised learning approaches have advanced the state of the art on a variety of tasks in natural language processing, often resulting in systems approaching the level of inter-annotator agreement on in-domain data, e.g. in POS tagging, where Shen et al. (2007) report a tagging accuracy of 97.3%. However, performance of state-of-the-art supervised systems is known to drop considerably on out-ofdomain data. State-of-the-art POS taggers trained on the Penn Treebank (Marcus et al., 1993) mapped to Google’s universal tag set (Petrov et al., 2011) achieve tagging accuracies in the range of 89–91% on Web 2.0 data (Petrov and McDonald, 2012) . To bridge this gap we may consider using semisupervised or transfer learning methods to adjust to new target domains (Blitzer et al., 2006; Daume III, 2007), pooling unlabeled data from those domains. However, in man</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind Joshi. 2007. Guided learning for bidirectional sequence classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Anders Johannsen</author>
</authors>
<title>Robust learning in random subspaces: equipping NLP against OOV effects.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2486" citStr="Søgaard and Johannsen, 2012" startWordPosition="388" endWordPosition="391">ut in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts. We evaluate our approach on two recently introduced cross-domain POS tagging datasets. Our approach is inspired by work in robust generalization (Ben-Tal and Nemirovski, 1998; Trafalis and Gilbert, 2007) and adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008; Søgaard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning with corrupted data points. Adversarial learning can also be seen as a way of averaging exponentially many models (Hinton et al., 2012). Ad</context>
<context position="4289" citStr="Søgaard and Johannsen, 2012" startWordPosition="661" endWordPosition="665">e.g. by Goldberg and Elhadad (2008). 2 Approach overview In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the learning phase. A discriminative learner seeing corrupted data points with missing features will not update part of the model and will thus try to find a decision boundary classifying the training data correctly relying on the remaining features. This decision boundary may be very different from the decision boundary found otherwise by the discriminative learner. If we sample enough corruptions, the model learned from the corrupted data will converge on the model minimizing average loss over all co</context>
<context position="7716" citStr="Søgaard and Johannsen (2012)" startWordPosition="1276" endWordPosition="1279"> model, but use the PTB tagset rather than the Google tagset (Petrov et al., 2011) used by default in the LXMLS toolkit. 1https://github.com/gracaninja/lxmls-toolkit 4 2 0 −2 −4 −6 −8 −10 −12 0 2 4 6 8 10 12 −14 2d-fit 2d-fit with missing feature 1d-fit 669 1: X = {hyi,xii}Ni_1 2: w° = 0,v = 0,i = 0 3: fork ∈ K do 4: for n ∈ N do 5: if sign(w · x) =6 yn then 6: wi+1 ← update(wi) 7: i ← i + 1 8: end if 9: v ← v + wi 10: end for 11: end for 12: return w = v/(N × K) Figure 2: Generic averaged perceptron 4 Minimizing loss under OOV effects We will think of domain shifts as data point corruptions. Søgaard and Johannsen (2012) model domain shifts using binary vectors of length m where m is the size of of our feature representation. Each vector then represents an expected OOV effect by encoding what features are (predicted to be) missing in the target data, i.e. the ith feature will be missing if the ith element of the binary vector is 0. However, since we are minimizing average loss under OOV effects it makes sense to restrict the class of vectors to encode OOV effects that we are likely to observe. This could, for example, involve fixing an expected rate of missing features or bounding it by some interval, or it c</context>
</contexts>
<marker>Søgaard, Johannsen, 2012</marker>
<rawString>Anders Søgaard and Anders Johannsen. 2012. Robust learning in random subspaces: equipping NLP against OOV effects. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Michael Sindelar</author>
<author>Andrew McCallum</author>
</authors>
<title>Reducing weight undertraining in structured discriminative learning.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2565" citStr="Sutton et al., 2006" startWordPosition="400" endWordPosition="403">g by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts. We evaluate our approach on two recently introduced cross-domain POS tagging datasets. Our approach is inspired by work in robust generalization (Ben-Tal and Nemirovski, 1998; Trafalis and Gilbert, 2007) and adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008; Søgaard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning with corrupted data points. Adversarial learning can also be seen as a way of averaging exponentially many models (Hinton et al., 2012). Adversarial learning techniques have been developed for security-related learning</context>
<context position="3911" citStr="Sutton et al., 2006" startWordPosition="605" endWordPosition="608"> Proceedings of NAACL-HLT 2013, pages 668–672, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics plication of adversarial learning techniques by making a second assumption about our data, namely that domains are mixtures of Zipfian distributions over our features. Similar assumptions have been made before in computational linguistics, e.g. by Goldberg and Elhadad (2008). 2 Approach overview In this paper we consider the structured perceptron (Collins, 2002) – with POS tagging as our practical application. The structured perceptron is prone to feature swamping (Sutton et al., 2006), and we want to prevent that using a technique inspired by adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008). The modification presented here to the structured perceptron only affects a single line of code in a publicly available implementation (see below), but the consequences are significant. Online adversarial learning (Søgaard and Johannsen, 2012), briefly, works by sampling random corruptions of our data, or random feature deletions, in the learning phase. A discriminative learner seeing corrupted data points with missing features will not update part of the model</context>
</contexts>
<marker>Sutton, Sindelar, McCallum, 2006</marker>
<rawString>Charles Sutton, Michael Sindelar, and Andrew McCallum. 2006. Reducing weight undertraining in structured discriminative learning. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Trafalis</author>
<author>R Gilbert</author>
</authors>
<title>Robust support vector machines for classification and computational issues. Optimization Methods and Software,</title>
<date>2007</date>
<pages>22--187</pages>
<contexts>
<context position="2379" citStr="Trafalis and Gilbert, 2007" startWordPosition="372" endWordPosition="375">of use cases, we do not know the target domain in advance. This is the usual problem of robust learning, but in this paper we describe a novel learning algorithm that goes beyond robust learning by making various assumptions about the difference between the source domain and the (unknown) target domain. Under these assumptions we can minimize average loss under (all possible or a representative sample of) domain shifts. We evaluate our approach on two recently introduced cross-domain POS tagging datasets. Our approach is inspired by work in robust generalization (Ben-Tal and Nemirovski, 1998; Trafalis and Gilbert, 2007) and adversarial learning (Globerson and Roweis, 2006; Dekel and Shamir, 2008; Søgaard and Johannsen, 2012). Our approach also bears similarities to feature bagging (Sutton et al., 2006). Sutton et al. (2006) noted that in learning of linear models useful features are often swamped by correlating, but more indicative features. If the more indicative features are absent in the target domain due to out-of-vocabulary (OOV) effects, we are left with the swamped features which were not updated properly. This is, indirectly, the problem solved in adversarial learning with corrupted data points. Adve</context>
</contexts>
<marker>Trafalis, Gilbert, 2007</marker>
<rawString>T Trafalis and R Gilbert. 2007. Robust support vector machines for classification and computational issues. Optimization Methods and Software, 22:187–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Zipf</author>
</authors>
<title>The psycho-biology of language.</title>
<date>1935</date>
<publisher>Houghton Mifflin.</publisher>
<contexts>
<context position="9553" citStr="Zipf, 1935" startWordPosition="1626" endWordPosition="1627">d from a known distribution D1 robust against the distributional differences between D1 and an unknown distribution D2. These two distributions are somehow related to a distribution D0 (the underlying language distribution from which the domain distributions are sampled). It is common to assume that linguistic distribu1: X = {hyi,xii}Ni_1 2: w° = 0,v = 0,i = 0 3: fork ∈ K do 4: for n ∈ N do 5: � ← random.zipf(3, M) 6: if sign(w · x ◦ �) =6 yn then 7: wi+1 ← update(wi) 8: i ← i + 1 9: end if 10: v ← v + wi 11: end for 12: end for 13: return w = v/(N × K) Figure 3: Z3SP tions follow power laws (Zipf, 1935; Goldberg and Elhadad, 2008). We will assume that D1 = D0 ×Z1 where Z1 is some Zipfian distribution. Say D0 ∼ Z0 is the master Zipfian distribution of language L0. If we assume that (otherwise independent) domains L1 and L2 follow products of Zipfians Z0 × Z1 and Z0 × Z2, we derive the following: Say w = BZ0xZ1 is the model learned from the source data. The ideal model is w&apos; = BZ0xZ2, but both Zipfians Z1 and Z2 are unknown. Since Z2 is unknown (and in many applications, we want to model several Zi), the overall best model we can hope for is w&apos; = BZ0. Z0 is also unknown, but we can observe a </context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>George Zipf. 1935. The psycho-biology of language. Houghton Mifflin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>