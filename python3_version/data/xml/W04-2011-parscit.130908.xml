<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001433">
<title confidence="0.909488">
Knowledge Extraction Using Dynamical Updating of Representation
</title>
<sectionHeader confidence="0.637867" genericHeader="abstract">
ALDO DRAGONI
</sectionHeader>
<bodyText confidence="0.783204">
D.E.I.T., Università Politecnica delle Marche
Via Brecce Bianche
Ancona, Italy, 60131
dragon@inform.unian.it
GUIDO TASCINI
D.E.I.T., Università Politecnica delle Marche
Via Brecce Bianche
Ancona, Italy, 60131
tascini@inform.unian.it
</bodyText>
<sectionHeader confidence="0.489971" genericHeader="keywords">
LUIGI LELLA
</sectionHeader>
<bodyText confidence="0.552126">
D.E.I.T., Università Politecnica delle Marche
Via Brecce Bianche
Ancona, Italy, 60131
l.lella@inform.unian.it
</bodyText>
<sectionHeader confidence="0.969857" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999818923076923">
We present a system that extracts knowledge
from the textual content of documents.
The acquired knowledge is represented through
an associative network, that is dynamically
updated by the integration of a contextualized
structure representing the content of the new
analysed document.
Grounded on the basis of “long term working
memory” theory by W. Kintsch and K.A. Ericsson,
our system makes use of a scale free graph model
to update the final knowledge representation.
This knowledge acquisition system has been
validated by first experimental results.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993575090909091">
From an historical perspective, four types of
knowledge representation schemas are worth to be
considered (W.Kintsch, 1998).
“Feature systems” (J.J. Katz, J.A. Fodor, 1963)
have been developed in philosophy and linguistics
and became very popular especially in psychology.
This representation aimed at finding a limited set
of basic semantic characteristics that, combined by
means of particular composition rules, could
express complex concepts. It was a very simple
representation system but conceptual relations
were not considered. Furthermore the defined
features did not change with the context and the
goals that had to be achieved.
“Associative networks” consider also semantic
relations between concepts. Knowledge is
represented by a network of concepts bounded by
more or less strong associations. This formalism is
bolstered by a lot of experimental data, for
example by word priming experiments (D.E.
Meyer, R.W. Schvaneveldt, 1971). But networks
WILLIAM GIORDANO
D.E.I.T., Università Politecnica delle Marche
Via Brecce Bianche
Ancona, Italy, 60131
whose links are not labelled are not very
expressive.
“Semantic networks” (A.M. Collins, M.R.
Quillian, 1969) are an evolution of associative
networks. Concepts continue to be symbolized by
nodes, but these are linked by labeled arcs (IS-A,
PART-OF etc.). In this way well ordered concept
hierarchies can be defined and the hereditariness of
properties is allowed.
“Schemas”, “frames” and “scripts” are structures
for coordinating concepts that belong to the same
event or superstructure. Classical examples of
these formalisms are the “room frame” of Minsky
(M. Minsky, 1975) and the restaurant script of
Schank and Abelson (R.C. Schank, R.P. Abelson,
1977).
The problem with these representation forms is
that they are static. In fact human mind generates
contextualized structures, that are adapted to the
particular context of use.
“Networks of propositions” (or “knowledge
nets”, W.Kintsch, 1998) are an alternative
formalism that combines and extends the
advantages of the representation forms that have
been introduced so far.
The predicate-argument schema can be
considered as the fundamental linguistic unit
especially in the representation of textual content.
Atomic propositions consist of a relational term
(the predicate) and one or more arguments.
Networks of propositions link these atomic
propositions through weighted and not labeled
arcs. According to this formalism the meaning of a
node is given by its position in the net.
From a psychologic point of view only the nodes
that are active (i.e. that are maintained in the
working memory) contribute to specify the sense
of a node. Hence the meaning of a concept is not
permanent and fixed but is built every time in the
working memory by the activation of a certain
subset of propositions in the neighbour of the node
that represents the concept. The context of use
(objectives, accumulated experiences, emotional
and situational state etc.) determines which nodes
have to be activated.
For the definition of retrieval modalities
Ericsson and Kintsch has introduced the concept of
long term working memory (LTWM) (W.Kintsch,
V.L. Patel, K.A.Ericsson, 1999). They noticed that
some cognitive tasks, as textual comprehension,
cannot be explained only using the concept of
working memory. Given the strict limits of
capacity of the short term memory (STM) and of
the working memory (WM), tasks that require an
enormous employment of resources cannot be
carried out.
The theory of long term working memory
specifies under which conditions the capacity of
WM can be extended. The LTWM is involved only
in the execution of well known tasks and actions,
that belong to a particular cognitive domain that
has been well experienced. In these cases the
working memory can be subdivided in a short term
part (STWM) that has a limited capacity and a
LTWM that is a part of the long term memory
represented by the network of propositions. The
content of STWM automatically generates the
LTWM. In particular objects present in the STWM
are linked to other objects in the LTM by fixed and
stable memory structures (retrieval cues).
2 Implementation of the Kintsch-Ericsson
model
The approach of the network of propositions
yielded two project problems. The creation of the
LTWM and the activation of LTM nodes, i.e. the
creation of the retrieval cues.
Kintsch has developed two methods for the
definition of the LTWM.
The first, defined with Van Dijk (T.A. van Dijk,
W. Kintsch, 1983), is a manual technique that
starts from the propositions present in the text
(micropropositions) and using some organizing
rules arrives to the definition of macropropositions
and macrostructures and even to the definition of
LTWM.
The second is based on the latent semantic analysis
(LSA) (T.K. Landauer, P.W. Foltz, D. Laham,
1998). This technique can infer, from the matrix of
co-occurrence rates of the words, a semantic space
that reflects the semantic relations between words
and phrases. This space has typically 300-400
dimensions and allows to represent words, phrases
and entire texts in a vectorial form. In this way the
semantic relation between two vectors can be
estimated by their cosine (a measure that according
to Kintsch can be interpreted as a correlation
coefficient).
This latter solution to the problem of the
definition of LTWM puts a great and inevitable
technical problem. How many objects must be
retrieved from the semantic space for every word
present in the text ? In some cases, when the
textbase, i.e. the representation obtained directly
from the text, is sufficiently expressed, the
retrieval of knowledge from the LTM is not
necessary. In other cases a correct comprehension
of the text (or the relative situation model) requires
the retrieval of knowledge from the LTM.
After the creation of the LTWM the integration
process begins i.e. the activation of the nodes
correspondent to the meaning of the phrase.
Kintsch uses a diffusion of activation pocedure that
is a simplified version of the one developed by
McClelland and Rumelhart (J.L. McClelland, D.E.
Rumelhart, 1986). Firstly an activation vector is
defined whose elements are indexed over the
nodes of LTWM. Any element’s value is “1” or
“0” depending on the presence or the absence of
the corresponding node in the analyzed phrase (i.e.
in the STWM). This vector is multiplied by the
matrix of the correlation rates (the weights of the
links of the LTWM) and the resulting vector is
normalized. This becomes the new activation
vector that must be multiplied again by the matrix
of the correlation rates. This procedure goes on
until the activation vector becomes stable. After
the integration process, the irrelevant nodes are
deactivated and only those that represent the
situation model remain activated.
</bodyText>
<subsectionHeader confidence="0.987872">
2.1 An alternative representation of the
Kintsch-Ericsson model
</subsectionHeader>
<bodyText confidence="0.994012209302326">
The adoption of a network of propositions for the
knowledge representation presents certainly great
advantages in comparison with the classic
formalisms. While semantic networks, frames and
scripts organize knowledge in a more ordered and
logical way, the networks of propositions are
definitely more disorganized and chaotic, but
present the not negligible advantage that are
capable to vary dynamically not only in time, on
the basis of the past experiences, but also on the
basis of the perceived context.
But the technique worked out by Kintsch and
Ericsson for the definition of LTWM presents
some limits. Retrieving knowledge from the
semantic space is only the first. Another problem is
the evolution of the LTWM. The position occupied
by a word in the LTWM is determined by the
experience, i.e. its past use and this should be a
lifetime experience. But this kind of knowledge
cannot be reached practically and Kintsch resorts
to the use of a dictionary for the definition of the
semantic space that represents the LTWM.
Furthermore the construction-integration process
does not always assure the semantic
disambiguation of the analysed phrase (W.Kintsch,
1998).
The use of an external dictionary, as WordNet,
(G. A. Miller, 1993) and of particular
disambiguation procedures can overcome the last
two limits.
Instead the first problem can be fully solved only
by dropping the intermediate representation of the
semantic space and by developing new methods
for the direct formation of networks of concepts
and propositions.
Let us describe now the system for the automatic
acquisition of the knowledge that we developed on
the basis of the LTWM model of Kintsch-Ericsson.
The lack of adequate textual parsers able to
convert the paragraphs of a text in the
correspondent atomic propositions has driven us to
develop, at least in this initial phase of our project,
simple dynamic models of associative networks.
</bodyText>
<figureCaption confidence="0.940125">
Figure 1: A possibile architecture of a system for
the dynamical acquisition of knowledge from a
repository of documents.
</figureCaption>
<bodyText confidence="0.999803521739131">
The part of the document that is analysed (the
content of the buffer) must be codified on the basis
of the context before being elaborated by the
working memory block. The context represents the
theme, the subject of the processed text and for its
correct characterization not only the information
present in the document must be considered, but
also the one that can be retrieved from the structure
representing the knowledge accumulated during
the analysis of the previous documents presented
to the system (Long Term Memory).
For the implementation of the working memory
block, self organizing networks with suitable
procedures for the labeling of their nodes could be
used, but this solution requires a lot of
computational time, especially for the analysis of
entire repositories of documents.
So we considered alternative models based on
the theory of scale free graphs (R.Albert,
A.L.Barabasi, 2001) for the implementation of an
associative network.
The graph theory dealed with regular graphs
untill the 50s. Subsequently random graphs were
introduced (P.Erdos, A.Renyi, 1959). They were
the first simple forms of complex graphs that had
ever been studied.
Their model started with a network made by N
isolated nodes. Successively each pair of nodes
could be connected with a probability p, leading to
a graph having approximately pN(N-1)/2 links.
But this model was still far from real networks
present in nature and artificial systems. So
scientists defined other models characterized by an
higher complexity level.
The actual models have three main features.
First their “small world” structure. That means
there is a relatively short path between any two
nodes (D.J.Watts, S.H.Strogatz, 1998).
Second their inherent tendency to cluster that is
quantified by a coefficient that was introduced by
Watts and Strogatz. Given a node i of ki degree i.e.
having ki edges which connect it to ki other nodes,
if those make a cluster, they can establish ki(ki-1)/2
edges at best. The ratio between the actual number
of edges and the maximum number gives the
cluster coefficient of node i. The clustering
coefficient of the whole network is the average of
the all individual clustering coefficients. In a
random graph the clustering coefficient is C = p. In
real networks the clustering coefficient is much
larger than p.
Actual graph models are also characterized by a
particular degree distribution. While in a random
graph the majority of the nodes have
approximately the same degree close to the
average degree, the degree distribution P(k) of a
real network has a power-law tail P(k)~k-�. For this
reason these networks are called “scale free”
(R.Albert, A.L.Barabasi, 2000).
Recently it has been found that human
knowledge seems to be structured as a scale free
graph (M.Steyvers, J.Tenenbaum, 2001).
Representing words and concepts with nodes,
some of these (hubs) establish much more links
compared with the other ones.
In table 2 are reported the average shortest path
length, the clustering coefficient and the power law
exponent of two different types of semantic
networks.
</bodyText>
<table confidence="0.9991605">
Average Clustering Power
path coefficient law
length exponent
WordNet 10.56 0.0265 3.11
Roget 5.60 0.875 3.19
Thesaurus
</table>
<tableCaption confidence="0.981398">
Table 1: General characteristics of some
semantic networks.
</tableCaption>
<bodyText confidence="0.998017652631579">
This particular conformation seems to optimize
the communication between nodes. Thanks to the
presence of the hubs, every pair of nodes can be
connected by a low number of links in comparison
with a random network with the same dimensions.
The definition and the eventual updating of a scale
free network does not require a lot of time and the
execution of particular processes, as the diffusion
of the activation signal, is very fast.
The textual analysis is performed through the
following steps.
The new text is analysed paragraph by
paragraph. The buffer contains not only the words
of the paragraph analysed, but also words retrieved
from the long term memory using the diffusion of
the activation procedure (the activation signal
starts from the nodes in the LTM that represents
the words in the paragraph). Theoretically, the
buffer should contain also the words activated
during the analysis of the previous paragraph, but
this aspect has not been considered for its
computational complexity. The buffer, the working
memory and the activated part of the LTM block
can be compared (but they are not the same
structure) to the LTWM defined by Kintsch and
Ericsson.
During the acquisition of the content of the
paragraph a stoplist of words that must not be
considered (as articles, pronouns etc.) is used.
For any word in the text, the paragraphs where it
has appeared (or where it has been inserted after
the retrieval procedure) are stored. When the entire
text has been parsed and the data of all the N not
filtered words have been memorized, the formation
of the network of concepts in the working memory
begins. The model adopted is similar to the one
defined by Bianconi and Barabasi (G.Bianconi,
A.Barabasi, 2001). The process starts with a net
consisting of N disconnected nodes.
At every step t=1..N each node (associated to
one of the N words) establishes a link with other M
units (M=5). If j is the selected unit, the probability
that this node establishes a link with the unit i is:
where ki is the degree of the unit i 1, i.e. the
number of links established by it, while Ui is the
fitness value associated to the node, and it can be
computed as the ratio between the number of
paragraphs that contain both i and j and the number
of paragraphs that contain either i or j.
LTM is an associative network that is updated
with the content of the WM. Whenever a link of
the WM corresponds to a link present in the LTM,
the weight of this one is increased by “1”.
Example :
The WM links “Hemingway” to “writer”.
In the LTM “Hemingway” is linked to “writer”
with weight “7” and to “story” with weight “4”.
In the updated LTM “Hemingway” is linked to
“writer” with weight “8” and to “story” with
weight “4” (unchanged).
To perform the diffusion of the activation signal
all the weights must be normalized. In this case
“Hemingway” must be linked to “writer” with
weight 8/(8+4) and to “story” with weight 4/(8+4).
Since the scale free network that represents the
content of the WM is used to update the content of
LTM, this associative networks should take the
form of a scale free graph. Unfortunately the
modalities of evolution of the LTM does not allow
the definition of a simple equivalent mathematic
model, that is necessary to make useful previsions
about its evolution.
In the scale free graph models proposed by
literature at each temporal step M new nodes are
added to the graph, with M defined beforehand.
These M nodes generally establish M links with M
old units of the network. In the system that we
have developed, after the analysis of a new
document the links related to an unknown number
of nodes of the LTM network are updated on the
basis of the content of the WM. This number
depends on the analysed document because it is the
number of the words that have not been filtered by
the stoplist.
Another important difference with other scale
free models presented in literature (S.N.
Dorogovtsev, J.F.F. Mendes, 2001) is the
particular fitness function that is used. This
function does not depend on a single node but on
the considered pair of nodes. If this value is
choosen as proportional to the weights of the LTM
associative network, the fitness value of a word is
not constant but depends on the other word that
could be linked to it. For example the noun
“house” should present for the link with “door” a
</bodyText>
<equation confidence="0.976986">
1 Each node is connected to itself by a loop.
U ; k;
U k
1 1 ...
+ + U N kN
=
P;
</equation>
<bodyText confidence="0.990833">
fitness value greater than the ones presented for the
links with “person” and “industry”.
</bodyText>
<sectionHeader confidence="0.523171" genericHeader="method">
3 Evaluation of the WM block
</sectionHeader>
<bodyText confidence="0.9964914">
To test the validity of the scale free graph model
adopted for the WM, we gave 100 files of the
Reuters Corpus2 as input to the system disabling
the retrieval of information from the LTM.
Two versions of the model have been tested, one
with bidirectional links and the other with directed
links (in this case we considered ki = ki(IN) + ki(OUT)).
In fig. 2 (http://www.deit.univpm.it/~dragoni
/downloads/scale_free.jpg) an example of a
network with bidirectional links is represented.
Please notice that the economic bias of the
articles justifies the presence of hubs as “interest
rate”, “economy”, etc., while other frequent words
as “child”, “restaurant”, etc. establish less link with
the others.
</bodyText>
<figureCaption confidence="0.996001666666667">
Figure 2: A network with bidirectional links
obtained with the analysis of 100 files of the
Reuters Corpus.
</figureCaption>
<bodyText confidence="0.6607105">
Fig.3 reports the average path length between
each pair of nodes, the clustering coefficient and
the degrees distribution of the nodes of the
obtained networks.
</bodyText>
<footnote confidence="0.971905">
2 Reuters Corpus, Volume 1, English language, 1996-
08-20 to 1997-08-19, http://about.reuters.com/
researchandstandards/corpus.
</footnote>
<figureCaption confidence="0.99895">
Figure 3: Comparison of average path lengths of
different types of networks.
</figureCaption>
<bodyText confidence="0.894878833333333">
The tendency of the average path length is clear.
The trend related to the random graphs, having the
same dimensions of the considered scale free
graphs, has an higher slope. This result confirms
the one obtained by Bianconi and Barabasi
reported in fig.4.
</bodyText>
<figureCaption confidence="0.993187153846154">
Figure 4: Comparison of average path lengths of
different types of networks (Bianconi-Barabasi
model).
Fig.5 shows that the clustering coefficient of the
scale free graph model has an higher order of
magnitude in comparison with the one computed
for the random networks. Even this result is
confirmed by the one obtained by Bianconi and
Barabasi (fig.6).
Figure 5: Comparison of clustering coefficients
of different types of networks.
Fig. 8 highlights the trend by redrawing the
graphic using the logarithmic coordinates.
</figureCaption>
<figure confidence="0.9992238">
-0,5
-1
LOG[P(k)]
-1,5
-2
-2,5
0
0,8 1 1,2 1,4 1,6 1,8
-3
LOG(k)
</figure>
<figureCaption confidence="0.9825498125">
Figure 8: Previous graphic in logaritmic
coordinates.
The degree distribution decays as P(k) ˜ k-G with
G = 3.2657.
The degree distribution of a graph with directed
links is reported below.
Figure 6: Comparison of clustering coefficients
of different types of networks (Bianconi-Barabasi
model).
Fig. 7 reports the degrees distribution of the
graph with bidirectional links.
Figure 9: Degree distribution of a graph with
M=5 and directed links.
Fig. 10 redraws the previous graphic using the
logarithmic coordinates. The power law trend has a
coefficient G = 2.3897.
</figureCaption>
<figure confidence="0.999500761904762">
0 20 40 60
k
0,5
0,4
0,3
P(k)
0,2
0,1
0
0 10 20 30 40
k
0,4
0,35
0,3
0,25
P(k)
0,2
0,15
0,1
0,05
0
</figure>
<figureCaption confidence="0.99047225">
Figure 7: Degree distribution of a graph with
M=5 and bidirectional links.
Figure 10: Degree distribution of a graph with
M=5 and directed links.
</figureCaption>
<sectionHeader confidence="0.402145" genericHeader="method">
4 Evaluation of the LTM block
</sectionHeader>
<bodyText confidence="0.999432888888889">
In order to evaluate the learning capabilities of
the system, we applied it on a medical article. The
sections of the paper have been presented
separately as independent texts regarding the same
topic. This choice has been imposed by the
necessity to enable also the retrieval of information
from LTM.
As expected, the resulting LTM network was a
typical scale-free graph (tab. 2).
</bodyText>
<table confidence="0.999809714285714">
M Average Average Clustering
path length degree coefficient
1 2.559 5.95 0.32290
2 2.499 6.50 0.33758
3 2.267 8.30 0.45428
4 2.255 9.50 0.43099
5 2.232 9.85 0.43151
</table>
<tableCaption confidence="0.999497">
Table 2: LTM with 40 nodes
</tableCaption>
<bodyText confidence="0.9998504">
The analysis has been repeated 30 times
examining the coherence rate of each resulting
LTM representation.
The coherence measure is based on a kind of
transitivity assumption, i.e. if two concepts have
similar relationships with other concepts, then the
two concepts should be similar.
The coherence rate is obtained by correlating the
LTM ratings given for each item in a pair with all
of the other concepts3. Its value can be correctly
computed only producing symmetric versions of
the LTM data.
The average coherence rate was 0.45, indicating
that the system has conceptualized the terms
according to a precise inner schema.
</bodyText>
<footnote confidence="0.990572333333333">
3 All the operations described in this section are
performed by the software PCKNOT 4.3, a product of
Interlink Inc.
</footnote>
<bodyText confidence="0.9999306875">
To evaluate the correctness of this schema we
are going to compare the obtained LTM
representations with experimental data obtained
from a group of human subjects. The subjects will
be asked to read the same medical article examined
by the system, assigning a rate of similarity to each
pair of words that has been considered by the
system. A Pathfinder analysis (R.W. Schvaneveldt,
F.T. Durso, D.W. Dearholt, 1985.) will be
performed on the relatedness matrices provided by
human subjects and the LTM matrices in order to
extract the so called “latent semantic”, i.e. other
implicit relations between words. The obtained
matrices will be compared using a similarity rate
determined by the correspondence of links in the
two types of networks.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="method">
5 Future work
</sectionHeader>
<bodyText confidence="0.998362605263158">
Some important considerations can be made on
the overall structure of the system.
The absence of an external feedback does not
guarantee the correspondence between the LTM
and the form of representation that must be
modelled ( the knowledge of an organization, the
knowledge of a working group, the knowledge of a
single user ). A possible external feedback could
be based on the evaluation of the performances of
the system in the execution of particular tasks as
the retrieval or the filtering of documents. For
example the acceptance or the rejection of the
documents selected by the system could be
reflected in the updating modality of the LTM. In
the first case the content of the WM could be used
to strenghten the links in the LTM or to create new
ones (as explained previously), in the second case
the content of the WM could be used to weaken or
delete the links in the LTM.
During the formation of the network in the WM
the information about the weights of the links in
LTM is not considered explicitly. Even if the
weights can condition the retrieval of the
information from the LTM, they could also modify
the value of the fitness function used for the
computation of the probability of the creation of
new links in the WM.
Furthermore, the association of an age to the
links of the LTM could guarantee more plasticity
to its structure. Also the ages could be used in the
computation of the fitness values, for example in
accordance with the modalities suggested by
Dorogovtsev (S.N. Dorogovtsev, J.F.F. Mendes,
2000).
We think that our knowledge acquisition system
can be effectively used for the semantic
disambiguation, that is the first phase of the
analysis in the most recent systems for the
</bodyText>
<figure confidence="0.9983497">
0
0,5 1 1,5 2
-0,5
-1
-1,5
-2
-2,5
-3
LOG(k)
LOG[P(k)]
</figure>
<bodyText confidence="0.995772357142857">
extraction of ontologies from texts (R. Navigli, P.
Velardi, A. Gangemi, 2003).
As a further development, we are thinking of
extracting from our representation form a simple
taxonomy of concepts using techniques for the
extraction of subsumption and equivalence
relations. These techniques are based on the
elaboration of the correlations between concepts
expressed as fuzzy relations. A taxonomical
representation can be considered as an important
step towards the creation of an ontological
representation. In this way our system could be
used to model the user knowledge representing it
in an ontological form.
</bodyText>
<sectionHeader confidence="0.999763" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99997728125">
A new system for the automatic acquisition of
the knowledge has been presented. It is based on
the concept of long term working memory
developed by Kintsch and Ericsson.
The system updates an associative network
(LTM) whose structure varies dynamically in time
on the basis of the textual content of the analyzed
documents. During the analysis of each new
document the LTM can be queried by the simple
procedure of the diffusion of the activation signal
developed by Kintsch and Ericsson. In this way the
context of the document can be easily and exactly
identified.
To reduce the computational time we have
implemented the WM block with a scale free graph
model. The obtained network is used to update the
content of the LTM.
Some analyses have been performed over the
WM model developed. The results have confirmed
that the network evolves as a scale free graph.
Also the LTM graphs seems to keep the scale
free features, and their coherence rate indicates that
the system conceptualizes the terms according to a
precise inner schema.
Now we are considering alternative models for
the WM that use much more information present in
the LTM and that guarantee more plasticity to its
structure. We are also going to compare the LTM
graphs with the knowledge structures obtained by
the Pathfinder analysis computed over the
associations provided by a group of human
subjects.
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.99711425">
The authors are grateful to Prof. Ignazio Licata
(Istituto di Cibernetica Non-Lineare per lo Studio
dei Sistemi Complessi, Marsala(TP) - Italy) for
helpful discussions, comments and criticisms.
</bodyText>
<sectionHeader confidence="0.995269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932314285714">
R.Albert, A. Barabasi. 2000. Topology of evolving
networks: Local events and universality. Phys.
Rev. Lett. 85, p.5234.
R. Albert, A. Barabasi. 2001. Statistical Mechanics
of Complex Networks. Rev. Mod. Phys., no.74,
pp.47-97.
G. Bianconi, A. Barabasi. 2001. Bose-Einstein
Condensation in Complex Networks. Phys. Rev.
Lett., vol. 86, no. 24.
A.M. Collins, M.R. Quillian. 1969. Retrieval from
semantic memory. Journal of Verbal Learning
and Verbal Behaviour, 8, pp.240-247.
S.N. Dorogovtsev, J.F.F. Mendes. 2000. Evolution
of reference networks with aging, arXiv: cond-
mat/0001419.
S.N. Dorogovtsev, J.F.F. Mendes. 2001. Evolution
of networks. arXiv: cond-mat/0106144,
submitted to Adv. Phys.
P.Erdos, Renyi A.. 1959. On Random Graphs.
Publ. Math. Debrecen 6, p. 290.
J.J. Katz, J.A. Fodor. 1963. The structure of
semantic theory. Language, 39, pp.170-210.
W. Kintsch. 1998. Comprehension. A Paradigm
for Cognition. Cambridge University Press.
W. Kintsch. 1998. The Representation of
Knowledge in Minds and Machines. International
Journal of Psychology, 33(6), pp.411-420.
W. Kintsch, V.L. Patel, K.A.Ericsson. 1999. The
role of long-term working memory in text
comprehension. Psychologia, 42, pp.186-198.
T.K. Landauer, P.W. Foltz, D. Laham. 1998. An
Introduction to Latent Semantic Analysis.
Discourse Processes, 25, pp.259-284.
J.L. McClelland, D.E. Rumelhart. 1986. Parallel
distributed processing. Cambridge, MA: MIT
Press.
D.E.Meyer, R.W. Schvaneveldt. 1971. Facilitation
in recognizing pairs of words: Evidence of a
dependence between retrieval operations.
Journal of Experimental Psychology, 90, pp.227-
234.
G. A. Miller. 1993. Five papers on WordNet.
Cognitive Science Laboratory Report 43.
M. Minsky. 1975. A framework for representing
knowledge. In P.H. Winston (Ed.), The
psychology of computer vision. New York:
McGraw-Hill.
R. Navigli, P. Velardi, A. Gangemi. 2003.
Ontology Learning and Its Application to
Automated Terminology Translation. IEEE
Intelligent Systems, January/February 2003, pp.
22-31.
R.C. Schank, R.P. Abelson. 1977. Scripts, plans,
goals, and understanding. Hillsdale, NJ:
Erlbaum.
R.W. Schvaneveldt, F.T. Durso, D.W. Dearholt.
1985. Pathfinder: Scaling with network
structures. Memorandum in Computer and
Cognitive Science, MCCS-85-9, Computing
Research Laboratory. Las Cruces: New Mexico
State University.
M. Steyvers, J. Tenenbaum. 2001. The Large-Scale
structure of Semantic Networks. Working draft
submitted to Cognitive Science.
T.A. van Dijk, W. Kintsch. 1983. Strategies of
discourse comprehension. New York: Academic
Press.
D.J. Watts, S.H. Strogatz. 1998. Collective
dynamics of ‘small-world’ networks. Nature, vol.
393, pp. 440-442.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.144420">
<title confidence="0.999542">Knowledge Extraction Using Dynamical Updating of Representation</title>
<author confidence="0.967319">ALDO DRAGONI</author>
<affiliation confidence="0.7071265">D.E.I.T., Università Politecnica delle Via Brecce</affiliation>
<address confidence="0.95605">Ancona, Italy,</address>
<email confidence="0.99866">dragon@inform.unian.it</email>
<author confidence="0.902877">GUIDO TASCINI</author>
<affiliation confidence="0.629819">D.E.I.T., Università Politecnica delle Via Brecce</affiliation>
<address confidence="0.912748">Ancona, Italy,</address>
<email confidence="0.998899">tascini@inform.unian.it</email>
<author confidence="0.946856">LUIGI LELLA</author>
<affiliation confidence="0.892287">D.E.I.T., Università Politecnica delle Marche Via Brecce Bianche</affiliation>
<address confidence="0.99932">Ancona, Italy, 60131</address>
<email confidence="0.997514">l.lella@inform.unian.it</email>
<abstract confidence="0.999777166666667">We present a system that extracts knowledge from the textual content of documents. The acquired knowledge is represented through an associative network, that is dynamically updated by the integration of a contextualized structure representing the content of the new analysed document. Grounded on the basis of “long term working memory” theory by W. Kintsch and K.A. Ericsson, our system makes use of a scale free graph model to update the final knowledge representation.</abstract>
<note confidence="0.832332">This knowledge acquisition system has been validated by first experimental results.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Barabasi R Albert</author>
</authors>
<title>Topology of evolving networks: Local events and universality.</title>
<date>2000</date>
<journal>Phys. Rev. Lett.</journal>
<volume>85</volume>
<pages>5234</pages>
<marker>Albert, 2000</marker>
<rawString>R.Albert, A. Barabasi. 2000. Topology of evolving networks: Local events and universality. Phys. Rev. Lett. 85, p.5234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Albert</author>
<author>A Barabasi</author>
</authors>
<date>2001</date>
<journal>Statistical Mechanics of Complex Networks. Rev. Mod. Phys.,</journal>
<volume>74</volume>
<pages>47--97</pages>
<marker>Albert, Barabasi, 2001</marker>
<rawString>R. Albert, A. Barabasi. 2001. Statistical Mechanics of Complex Networks. Rev. Mod. Phys., no.74, pp.47-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bianconi</author>
<author>A Barabasi</author>
</authors>
<date>2001</date>
<journal>Bose-Einstein Condensation in Complex Networks. Phys. Rev. Lett.,</journal>
<volume>86</volume>
<pages>24</pages>
<marker>Bianconi, Barabasi, 2001</marker>
<rawString>G. Bianconi, A. Barabasi. 2001. Bose-Einstein Condensation in Complex Networks. Phys. Rev. Lett., vol. 86, no. 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>M R Quillian</author>
</authors>
<title>Retrieval from semantic memory.</title>
<date>1969</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<volume>8</volume>
<pages>240--247</pages>
<marker>Collins, Quillian, 1969</marker>
<rawString>A.M. Collins, M.R. Quillian. 1969. Retrieval from semantic memory. Journal of Verbal Learning and Verbal Behaviour, 8, pp.240-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Dorogovtsev</author>
<author>J F F Mendes</author>
</authors>
<title>Evolution of reference networks with aging, arXiv:</title>
<date>2000</date>
<pages>0001419</pages>
<marker>Dorogovtsev, Mendes, 2000</marker>
<rawString>S.N. Dorogovtsev, J.F.F. Mendes. 2000. Evolution of reference networks with aging, arXiv: condmat/0001419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Dorogovtsev</author>
<author>J F F Mendes</author>
</authors>
<title>Evolution of networks.</title>
<date>2001</date>
<note>arXiv: cond-mat/0106144, submitted to Adv. Phys.</note>
<marker>Dorogovtsev, Mendes, 2001</marker>
<rawString>S.N. Dorogovtsev, J.F.F. Mendes. 2001. Evolution of networks. arXiv: cond-mat/0106144, submitted to Adv. Phys.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renyi A P Erdos</author>
</authors>
<title>On Random Graphs.</title>
<date>1959</date>
<journal>Publ. Math. Debrecen</journal>
<volume>6</volume>
<pages>290</pages>
<marker>Erdos, 1959</marker>
<rawString>P.Erdos, Renyi A.. 1959. On Random Graphs. Publ. Math. Debrecen 6, p. 290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Katz</author>
<author>J A Fodor</author>
</authors>
<title>The structure of semantic theory.</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<pages>170--210</pages>
<marker>Katz, Fodor, 1963</marker>
<rawString>J.J. Katz, J.A. Fodor. 1963. The structure of semantic theory. Language, 39, pp.170-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<title>Comprehension. A Paradigm for Cognition.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1135" citStr="Kintsch, 1998" startWordPosition="151" endWordPosition="152">s. The acquired knowledge is represented through an associative network, that is dynamically updated by the integration of a contextualized structure representing the content of the new analysed document. Grounded on the basis of “long term working memory” theory by W. Kintsch and K.A. Ericsson, our system makes use of a scale free graph model to update the final knowledge representation. This knowledge acquisition system has been validated by first experimental results. 1 Introduction From an historical perspective, four types of knowledge representation schemas are worth to be considered (W.Kintsch, 1998). “Feature systems” (J.J. Katz, J.A. Fodor, 1963) have been developed in philosophy and linguistics and became very popular especially in psychology. This representation aimed at finding a limited set of basic semantic characteristics that, combined by means of particular composition rules, could express complex concepts. It was a very simple representation system but conceptual relations were not considered. Furthermore the defined features did not change with the context and the goals that had to be achieved. “Associative networks” consider also semantic relations between concepts. Knowledge</context>
<context position="2976" citStr="Kintsch, 1998" startWordPosition="421" endWordPosition="422">t hierarchies can be defined and the hereditariness of properties is allowed. “Schemas”, “frames” and “scripts” are structures for coordinating concepts that belong to the same event or superstructure. Classical examples of these formalisms are the “room frame” of Minsky (M. Minsky, 1975) and the restaurant script of Schank and Abelson (R.C. Schank, R.P. Abelson, 1977). The problem with these representation forms is that they are static. In fact human mind generates contextualized structures, that are adapted to the particular context of use. “Networks of propositions” (or “knowledge nets”, W.Kintsch, 1998) are an alternative formalism that combines and extends the advantages of the representation forms that have been introduced so far. The predicate-argument schema can be considered as the fundamental linguistic unit especially in the representation of textual content. Atomic propositions consist of a relational term (the predicate) and one or more arguments. Networks of propositions link these atomic propositions through weighted and not labeled arcs. According to this formalism the meaning of a node is given by its position in the net. From a psychologic point of view only the nodes that are </context>
<context position="9050" citStr="Kintsch, 1998" startWordPosition="1394" endWordPosition="1395">icsson for the definition of LTWM presents some limits. Retrieving knowledge from the semantic space is only the first. Another problem is the evolution of the LTWM. The position occupied by a word in the LTWM is determined by the experience, i.e. its past use and this should be a lifetime experience. But this kind of knowledge cannot be reached practically and Kintsch resorts to the use of a dictionary for the definition of the semantic space that represents the LTWM. Furthermore the construction-integration process does not always assure the semantic disambiguation of the analysed phrase (W.Kintsch, 1998). The use of an external dictionary, as WordNet, (G. A. Miller, 1993) and of particular disambiguation procedures can overcome the last two limits. Instead the first problem can be fully solved only by dropping the intermediate representation of the semantic space and by developing new methods for the direct formation of networks of concepts and propositions. Let us describe now the system for the automatic acquisition of the knowledge that we developed on the basis of the LTWM model of Kintsch-Ericsson. The lack of adequate textual parsers able to convert the paragraphs of a text in the corre</context>
</contexts>
<marker>Kintsch, 1998</marker>
<rawString>W. Kintsch. 1998. Comprehension. A Paradigm for Cognition. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<title>The Representation of Knowledge in Minds and Machines.</title>
<date>1998</date>
<journal>International Journal of Psychology,</journal>
<volume>33</volume>
<issue>6</issue>
<pages>411--420</pages>
<contexts>
<context position="1135" citStr="Kintsch, 1998" startWordPosition="151" endWordPosition="152">s. The acquired knowledge is represented through an associative network, that is dynamically updated by the integration of a contextualized structure representing the content of the new analysed document. Grounded on the basis of “long term working memory” theory by W. Kintsch and K.A. Ericsson, our system makes use of a scale free graph model to update the final knowledge representation. This knowledge acquisition system has been validated by first experimental results. 1 Introduction From an historical perspective, four types of knowledge representation schemas are worth to be considered (W.Kintsch, 1998). “Feature systems” (J.J. Katz, J.A. Fodor, 1963) have been developed in philosophy and linguistics and became very popular especially in psychology. This representation aimed at finding a limited set of basic semantic characteristics that, combined by means of particular composition rules, could express complex concepts. It was a very simple representation system but conceptual relations were not considered. Furthermore the defined features did not change with the context and the goals that had to be achieved. “Associative networks” consider also semantic relations between concepts. Knowledge</context>
<context position="2976" citStr="Kintsch, 1998" startWordPosition="421" endWordPosition="422">t hierarchies can be defined and the hereditariness of properties is allowed. “Schemas”, “frames” and “scripts” are structures for coordinating concepts that belong to the same event or superstructure. Classical examples of these formalisms are the “room frame” of Minsky (M. Minsky, 1975) and the restaurant script of Schank and Abelson (R.C. Schank, R.P. Abelson, 1977). The problem with these representation forms is that they are static. In fact human mind generates contextualized structures, that are adapted to the particular context of use. “Networks of propositions” (or “knowledge nets”, W.Kintsch, 1998) are an alternative formalism that combines and extends the advantages of the representation forms that have been introduced so far. The predicate-argument schema can be considered as the fundamental linguistic unit especially in the representation of textual content. Atomic propositions consist of a relational term (the predicate) and one or more arguments. Networks of propositions link these atomic propositions through weighted and not labeled arcs. According to this formalism the meaning of a node is given by its position in the net. From a psychologic point of view only the nodes that are </context>
<context position="9050" citStr="Kintsch, 1998" startWordPosition="1394" endWordPosition="1395">icsson for the definition of LTWM presents some limits. Retrieving knowledge from the semantic space is only the first. Another problem is the evolution of the LTWM. The position occupied by a word in the LTWM is determined by the experience, i.e. its past use and this should be a lifetime experience. But this kind of knowledge cannot be reached practically and Kintsch resorts to the use of a dictionary for the definition of the semantic space that represents the LTWM. Furthermore the construction-integration process does not always assure the semantic disambiguation of the analysed phrase (W.Kintsch, 1998). The use of an external dictionary, as WordNet, (G. A. Miller, 1993) and of particular disambiguation procedures can overcome the last two limits. Instead the first problem can be fully solved only by dropping the intermediate representation of the semantic space and by developing new methods for the direct formation of networks of concepts and propositions. Let us describe now the system for the automatic acquisition of the knowledge that we developed on the basis of the LTWM model of Kintsch-Ericsson. The lack of adequate textual parsers able to convert the paragraphs of a text in the corre</context>
</contexts>
<marker>Kintsch, 1998</marker>
<rawString>W. Kintsch. 1998. The Representation of Knowledge in Minds and Machines. International Journal of Psychology, 33(6), pp.411-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
<author>V L Patel</author>
<author>K A Ericsson</author>
</authors>
<title>The role of long-term working memory in text comprehension.</title>
<date>1999</date>
<journal>Psychologia,</journal>
<volume>42</volume>
<pages>186--198</pages>
<marker>Kintsch, Patel, Ericsson, 1999</marker>
<rawString>W. Kintsch, V.L. Patel, K.A.Ericsson. 1999. The role of long-term working memory in text comprehension. Psychologia, 42, pp.186-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T.K. Landauer, P.W. Foltz, D. Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, 25, pp.259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L McClelland</author>
<author>D E Rumelhart</author>
</authors>
<title>Parallel distributed processing.</title>
<date>1986</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>McClelland, Rumelhart, 1986</marker>
<rawString>J.L. McClelland, D.E. Rumelhart. 1986. Parallel distributed processing. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Schvaneveldt D E Meyer</author>
</authors>
<title>Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations.</title>
<date>1971</date>
<journal>Journal of Experimental Psychology,</journal>
<volume>90</volume>
<pages>227--234</pages>
<marker>Meyer, 1971</marker>
<rawString>D.E.Meyer, R.W. Schvaneveldt. 1971. Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations. Journal of Experimental Psychology, 90, pp.227-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1993</date>
<journal>Cognitive Science Laboratory Report</journal>
<volume>43</volume>
<contexts>
<context position="9119" citStr="Miller, 1993" startWordPosition="1406" endWordPosition="1407">wledge from the semantic space is only the first. Another problem is the evolution of the LTWM. The position occupied by a word in the LTWM is determined by the experience, i.e. its past use and this should be a lifetime experience. But this kind of knowledge cannot be reached practically and Kintsch resorts to the use of a dictionary for the definition of the semantic space that represents the LTWM. Furthermore the construction-integration process does not always assure the semantic disambiguation of the analysed phrase (W.Kintsch, 1998). The use of an external dictionary, as WordNet, (G. A. Miller, 1993) and of particular disambiguation procedures can overcome the last two limits. Instead the first problem can be fully solved only by dropping the intermediate representation of the semantic space and by developing new methods for the direct formation of networks of concepts and propositions. Let us describe now the system for the automatic acquisition of the knowledge that we developed on the basis of the LTWM model of Kintsch-Ericsson. The lack of adequate textual parsers able to convert the paragraphs of a text in the correspondent atomic propositions has driven us to develop, at least in th</context>
</contexts>
<marker>Miller, 1993</marker>
<rawString>G. A. Miller. 1993. Five papers on WordNet. Cognitive Science Laboratory Report 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
</authors>
<title>A framework for representing knowledge. In P.H. Winston (Ed.), The psychology of computer vision.</title>
<date>1975</date>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="2651" citStr="Minsky, 1975" startWordPosition="373" endWordPosition="374"> Brecce Bianche Ancona, Italy, 60131 whose links are not labelled are not very expressive. “Semantic networks” (A.M. Collins, M.R. Quillian, 1969) are an evolution of associative networks. Concepts continue to be symbolized by nodes, but these are linked by labeled arcs (IS-A, PART-OF etc.). In this way well ordered concept hierarchies can be defined and the hereditariness of properties is allowed. “Schemas”, “frames” and “scripts” are structures for coordinating concepts that belong to the same event or superstructure. Classical examples of these formalisms are the “room frame” of Minsky (M. Minsky, 1975) and the restaurant script of Schank and Abelson (R.C. Schank, R.P. Abelson, 1977). The problem with these representation forms is that they are static. In fact human mind generates contextualized structures, that are adapted to the particular context of use. “Networks of propositions” (or “knowledge nets”, W.Kintsch, 1998) are an alternative formalism that combines and extends the advantages of the representation forms that have been introduced so far. The predicate-argument schema can be considered as the fundamental linguistic unit especially in the representation of textual content. Atomic</context>
</contexts>
<marker>Minsky, 1975</marker>
<rawString>M. Minsky. 1975. A framework for representing knowledge. In P.H. Winston (Ed.), The psychology of computer vision. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
<author>A Gangemi</author>
</authors>
<title>Ontology Learning and Its Application to Automated Terminology Translation.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems, January/February</journal>
<pages>22--31</pages>
<marker>Navigli, Velardi, Gangemi, 2003</marker>
<rawString>R. Navigli, P. Velardi, A. Gangemi. 2003. Ontology Learning and Its Application to Automated Terminology Translation. IEEE Intelligent Systems, January/February 2003, pp. 22-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
<author>R P Abelson</author>
</authors>
<title>Scripts, plans, goals, and understanding.</title>
<date>1977</date>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>R.C. Schank, R.P. Abelson. 1977. Scripts, plans, goals, and understanding. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Schvaneveldt</author>
<author>F T Durso</author>
<author>D W Dearholt</author>
</authors>
<title>Pathfinder: Scaling with network structures.</title>
<date>1985</date>
<booktitle>Memorandum in Computer and Cognitive Science,</booktitle>
<pages>85--9</pages>
<institution>Computing Research Laboratory. Las Cruces: New Mexico State University.</institution>
<marker>Schvaneveldt, Durso, Dearholt, 1985</marker>
<rawString>R.W. Schvaneveldt, F.T. Durso, D.W. Dearholt. 1985. Pathfinder: Scaling with network structures. Memorandum in Computer and Cognitive Science, MCCS-85-9, Computing Research Laboratory. Las Cruces: New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>J Tenenbaum</author>
</authors>
<title>The Large-Scale structure of Semantic Networks. Working draft submitted to Cognitive Science.</title>
<date>2001</date>
<marker>Steyvers, Tenenbaum, 2001</marker>
<rawString>M. Steyvers, J. Tenenbaum. 2001. The Large-Scale structure of Semantic Networks. Working draft submitted to Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A van Dijk</author>
<author>W Kintsch</author>
</authors>
<title>Strategies of discourse comprehension.</title>
<date>1983</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>van Dijk, Kintsch, 1983</marker>
<rawString>T.A. van Dijk, W. Kintsch. 1983. Strategies of discourse comprehension. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Watts</author>
<author>S H Strogatz</author>
</authors>
<title>Collective dynamics of ‘small-world’ networks.</title>
<date>1998</date>
<journal>Nature,</journal>
<volume>393</volume>
<pages>440--442</pages>
<marker>Watts, Strogatz, 1998</marker>
<rawString>D.J. Watts, S.H. Strogatz. 1998. Collective dynamics of ‘small-world’ networks. Nature, vol. 393, pp. 440-442.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>