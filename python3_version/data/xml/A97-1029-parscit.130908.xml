<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.7941045">
Nymble:
a High-Performance Learning Name-finder
</title>
<author confidence="0.988731">
Scott Miller
</author>
<affiliation confidence="0.933235">
BBN Corporation
</affiliation>
<address confidence="0.965563">
70 Fawcett Street
Cambridge, MA 02138
</address>
<email confidence="0.996927">
szmiller@bbn.com
</email>
<author confidence="0.972687">
Daniel M. Bikel
</author>
<affiliation confidence="0.918876">
BBN Corporation
</affiliation>
<address confidence="0.980611">
70 Fawcett Street
Cambridge, MA 02138
</address>
<email confidence="0.998474">
dbikel@bbn.com
</email>
<author confidence="0.990191">
Richard Schwartz
</author>
<affiliation confidence="0.934444">
BBN Corporation
</affiliation>
<address confidence="0.9655765">
70 Fawcett Street
Cambridge, MA 02138
</address>
<email confidence="0.996751">
schwartz@bbn.com
</email>
<author confidence="0.987167">
Ralph Weischedel
</author>
<affiliation confidence="0.932109">
BBN Corporation
</affiliation>
<address confidence="0.9655945">
70 Fawcett Street
Cambridge, MA 02138
</address>
<email confidence="0.997447">
weisched@bbn.com
</email>
<sectionHeader confidence="0.995621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990666666667">
This paper presents a statistical, learned
approach to finding names and other non-
recursive entities in text (as per the MUC-6
definition of the NE task), using a variant of
the standard hidden Markov model. We
present our justification for the problem and
our approach, a detailed discussion of the
model itself and finally the successful results
of this new approach.
</bodyText>
<sectionHeader confidence="0.992136" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999982620689655">
In the past decade, the speech recognition commu-
nity has had huge successes in applying hidden
Markov models, or HMM&apos;s to their problems. More
recently, the natural language processing community
has effectively employed these models for part-of-
speech tagging, as in the seminal (Church, 1988) and
other, more recent efforts (Weischedel et al., 1993).
We would now propose that HMM&apos;s have success-
fully been applied to the problem of name-finding.
We have built a named-entity (NE) recognition
system using a slightly-modified version of an
HMM; we call our system &amp;quot;Nymble&amp;quot;. To our
knowledge, Nymble out-performs the best published
results of any other learning name-finder. Further-
more, it performs at or above the 90% accuracy level,
often considered &amp;quot;near-human performance&amp;quot;.
The system arose from the NE task as specified in
the last Message Understanding Conference (MUC),
where organization names, person names, location
names, times, dates, percentages and money amounts
were to be delimited in text using SGML-markup.
We will describe the various models employed, the
methods for training these models and the method for
&amp;quot;decoding&amp;quot; on test data (the term &amp;quot;decoding&amp;quot; borrowed
from the speech recognition community, since one
goal of traversing an HMM is to recover the hidden
state sequence). To date, we have successfully trained
and used the model on both English and Spanish, the
latter for MET, the multi-lingual entity task.
</bodyText>
<sectionHeader confidence="0.994252" genericHeader="introduction">
2. Background
</sectionHeader>
<subsectionHeader confidence="0.9986725">
2.1 Name-finding as an Information-
theoretic Problem
</subsectionHeader>
<bodyText confidence="0.999086846153846">
The basic premise of the approach is to consider
the raw text encountered when decoding as though it
had passed through a noisy channel, where it had been
originally marked with named entities.&apos; The job of
the generative model is to model the original process
which generated the name-class–annotated words,
before they went through the noisy channel.
More formally, we must find the most likely
sequence of name-classes (NC) given a sequence of
words (W):
Pr(NC I W) (2.1)
In order to treat this as a generative model (where it
generates the original, name-class–annotated words),
</bodyText>
<equation confidence="0.87335375">
we use Bayes&apos; Rule:
Pr(W, NC)
Pr(NC I W) =
Pr(W)
</equation>
<bodyText confidence="0.823267">
and since the a priori probability of the word
sequence—the denominator—is constant for any
given sentence, we can maxi-mize Equation 2.2 by
maximizing the numerator alone.
I See (Cover and Thomas, 1991), ch. 2, for an excellent overview
of the principles of information theory.
(2.2)
</bodyText>
<page confidence="0.846631">
194
</page>
<figure confidence="0.995045333333333">
•
START-OF-SENTENCE
ORGANIZATION
END-OF-SENTENCE
•
,
NOT-A-NAME
&apos;--------&amp;quot;-PERSON
,
(five other riame-classes)
,
i
w
,
Figure 3.1 Pictorial representation of conceptual model.
•
•
,
</figure>
<subsectionHeader confidence="0.970087333333333">
2.2 Previous
Approaches to Name-
finding
</subsectionHeader>
<bodyText confidence="0.991327285714286">
Previous approaches have
typically used manually
constructed finite state
patterns (Weischodel, 1995,
Appelt et al., 1995). For
every new language and every
new class of new information
to spot, one has to write a
new set of rules to cover the
new language and to cover the
new class of information. A
finite-state pattern rule
attempts to match against a
sequence of tokens (words), in
much the same way as a
general regular expression
matcher.
In addition to these finite-
state pattern approaches, a
variant of Brill rules has been applied to the problem,
as outlined in (Aberdeen et al., 1995).
</bodyText>
<subsectionHeader confidence="0.9984115">
2.3 Interest in Problem and Potential
Applications
</subsectionHeader>
<bodyText confidence="0.995994421052631">
The atomic elements of information extraction—
indeed, of language as a whole—could be considered
the who, where, when and how much in a sentence.
A name-finder performs what is known as surface- or
lightweight-parsing, delimiting sequences of tokens
that answer these important questions. It can be used
as the first step in a chain of processors: a next level
of processing could relate two or more named entities,
or perhaps even give semantics to that relationship
using a verb. In this way, further processing could
discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body
of text.
Furthermore, name-finding can be useful in its
own right: an Internet query system might use name-
finding to construct more appropriately-formed
queries: &amp;quot;When was Bill Gates born?&amp;quot; could yield the
query &amp;quot;Bill Gates&amp;quot;+born. Also, name-finding
can be directly employed for link analysis and other
information retrieval problems.
</bodyText>
<sectionHeader confidence="0.831342" genericHeader="method">
3. Model
</sectionHeader>
<bodyText confidence="0.999996666666667">
We will present the model twice, first in a
conceptual and informal overview, then in a more-
detailed, formal description of it as a type of HMM.
The model bears resemblance to Scott Miller&apos;s novel
work in the Air Traffic Information System (ATIS)
task, as documented in (Miller et al., 1994).
</bodyText>
<subsectionHeader confidence="0.998199">
3.1 Conceptual Model
</subsectionHeader>
<bodyText confidence="0.993072583333333">
Figure 3.1 is a pictorial overview of our model.
Informally, we have an ergodic HMM with only
eight internal states (the name classes, including the
NOT-A-NAME class), with two special states, the
START- and END-OF-SENTENCE states. Within each
of the name-class states, we use a statistical bigram
language model, with the usual one-word-per-state
emission. This means that the number of states in
each of the name-class states is equal to the
vocabulary size, I VI .
The generation of words and name-classes proceeds
in three steps:
</bodyText>
<listItem confidence="0.8923405">
1. Select a name-class NC, conditioning on the
previous name-class and the previous word.
2. Generate the first word inside that name-class,
conditioning on the current and previous name-
classes.
3. Generate all subsequent words inside the current
name-class, where each subsequent word is
conditioned on its immediate predecessor.
</listItem>
<bodyText confidence="0.989572">
These three steps are repeated until the entire observed
word sequence is generated. Using the Viterbi
algorithm, we efficiently search the entire space of all
possible name-class assignments, maximizing the
numerator of Equation 2.2, Pr(W, NC).
Informally, the construction of the model in this
manner indicates that we view each type of &amp;quot;name&amp;quot; to
be its own language, with separate bigram
probabilities for generating its words. While the
number of word-states within each name-class is equal
</bodyText>
<page confidence="0.994905">
195
</page>
<bodyText confidence="0.9997125">
to I VI , this &amp;quot;interior&amp;quot; bigram language model is
ergodic, i.e., there is a probability associated with
every one of the 1V12transitions. As a parameter-
ized, trained model, if such a transition were never
observed, the model &amp;quot;backs off&apos; to a less-powerful
model, as described below, in §3.3.3 on p. 4.
</bodyText>
<subsectionHeader confidence="0.99987">
3.2 Words and Word-Features
</subsectionHeader>
<bodyText confidence="0.999946583333333">
Throughout most of the model, we consider words
to be ordered pairs (or two-element vectors),
composed of word and word-feature, denoted (w, f).
The word feature is a simple, deterministic computa-
tion performed on each word as it is added to or
feature computation is an extremely small part of the
implementation, at roughly ten lines of code. Also,
most of the word features are used to distinguish
types of numbers, which are language-independent.2
The rationale for having such features is clear: in
Roman languages, capitalization gives good evidence
of names.3
</bodyText>
<subsectionHeader confidence="0.99496">
3.3 Formal Model
</subsectionHeader>
<bodyText confidence="0.999996">
This section describes the model formally,
discussing the transition probabilities to the word-
states, which &amp;quot;generate&amp;quot; the words of each name-class.
</bodyText>
<subsectionHeader confidence="0.874022">
3.3.1 Top Level Model
</subsectionHeader>
<bodyText confidence="0.97537">
As with most trained, probabilistic models, we
</bodyText>
<table confidence="0.922189266666667">
Word Feature Example Text Intuition
twoDigitNum 90 Two-digit year
f ourDigi tNum 1990 Four digit year
containsDigitAndAlpha A8956-67 Product code
containsDigitAndDash 09-96 Date
containsDigi tAndS lash 11/9/89 Date
containsDigitAndComma 23,000.00 Monetary amount
containsDigitAndPeriod 1.00 Monetary amount, percentage
otherNum 456789 Other number
allCaps BBN Organization
capPeriod M. Person name initial
f irstWord first word of sentence No useful capitalization information
initCap Sally Capitalized word
lowerCase can Uncapitalized word
other , Punctuation marks, all other words
</table>
<tableCaption confidence="0.938366">
Table 3.1 Word features, examples and intuition behind them
</tableCaption>
<bodyText confidence="0.972462025641026">
looked up in the vocabulary. It produces one of the
fourteen values in Table 3.1.
These values are computed in the order listed, so
that in the case of non-disjoint feature-classes, such as
containsDigitAndAlpha and
containsDigitAndDash, the former will take
precedence. The first eight features arise from the
need to distinguish and annotate monetary amounts,
percentages, times and dates. The rest of the features
distinguish types of capitalization and all other words
(such as punctuation marks, which are separate
tokens). In particular, the f irstWord feature arises
from the fact that if a word is capitalized and is the
first word of the sentence, we have no good
information as to why it is capitalized (but note that
allCaps and capPeriod are computed before
f irstWord, and therefore take precedence).
The word feature is the one part of this model
which is language-dependent. Fortunately, the word
have a most accurate, most powerful model, which
will &amp;quot;back off&apos; to a less-powerful model when there is
insufficient training, and ultimately back-off to
unigram probabilities.
In order to generate the first word, we must make
a transition from one name-class to another, as well
as calculate the likelihood of that word. Our intuition
was that a word preceding the start of a name-class
(such as &amp;quot;Mr.&amp;quot;, &amp;quot;President&amp;quot; or other titles preceding
the PERSON name-class) and the word following a
name-class would be strong indicators of the
subsequent and preceding name-classes, respectively.
2 Non-english languages tend to use the comma and period in the
reverse way in which English does, i.e., the comma is a decimal
point and the period separates groups of three digits in large
numbers. However, the re-ordering of the precedence of the two
relevant word-features had little effect when decoding Spanish,
so they were left as is.
3 Although Spanish has many lower-case words in organization
names. See §4.1 on p. 6 for more details.
</bodyText>
<page confidence="0.995519">
196
</page>
<bodyText confidence="0.9776086875">
Accordingly, the probabilitiy for generating the first
word of a name-class is factored into two parts:
Pr(NC I NC_,, w_1) Pr((w,f)firs, I NC, NC_,).
(3.1)
The top level model for generating all but the first
word in a name-class is
Pr((w, NC). (3.2)
There is also a magical &amp;quot;+end+&amp;quot; word, so that the
probability may be computed for any current word to
be the final word of its name-class, i.e.,
Pr((+end+, o the r) I(w, f)find&apos; NC). (3.3)
As one might imagine, it would be useless to have
the first factor in Equation 3.1 be conditioned off of
the +end+ word, so the probability is conditioned on
the previous real word of the previous name-class,
i.e., we compute
</bodyText>
<equation confidence="0.902599">
Pr(NC I NC_,, w_1) {w _, = +end + if
</equation>
<bodyText confidence="0.9475525">
W-1 = last observed word otherwise
NC , = START - OF - SENTENCE
(3.4)
Note that the above probability is not conditioned on
the word-feature of w_1, the intuition of which is
that in the cases where the previous word would help
the model predict the next name-class, the world
feature—capitalization in particular—is not impor-
tant: &amp;quot;Mr.&amp;quot; is a good indicator of the next word
beginning the PERSON name-class, regardless of
capitalization, especially since it is almost never seen
as &amp;quot;mr.&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.993386">
3.3.2 Calculation of Probabilities
</subsectionHeader>
<bodyText confidence="0.999453">
The calculation of the above probabilities is
straightforward, using events/sample-size:
</bodyText>
<equation confidence="0.999456444444444">
Pr(NC NC_,, w_,) = c(NC, NC ,,w)
c(NC_,,w_,) (3.5)
c((w, f)11„„ NC, NC_,)
Pr((w,f) (3.6)
firs, NC, NC_,)=
c(NC,NC_,)
c((w, f),(w, f)_,, NC) (3.7)
Pr((w,f) (w,A, NC) =
I i
</equation>
<bodyText confidence="0.99944">
where c() represents the number of times the events
occurred in the training data (the count).
</bodyText>
<subsectionHeader confidence="0.946105">
3.3.3 Back-off Models and Smoothing
</subsectionHeader>
<bodyText confidence="0.969524137254902">
Ideally, we would have sufficient training (or at
least one observation of!) every event whose
conditional probability we wish to calculate. Also,
ideally, we would have sufficient samples of that
upon which each conditional probability is
conditioned, e.g., for Pr(NC I NC 1, w_,), we would
like to have seen sufficient numbers of NC_,, w1.
Unfortunately, there is rarely enough training data to
compute accurate probabilities when &amp;quot;decoding&amp;quot; on
new data.
3. 3. 3.1 Unknown Words
The vocabulary of the system is built as it trains.
Necessarily, then, the system knows about all words
for which it stores bigram counts in order to compute
the probabilities in Equations 3.1 – 3.3. The
question arises how the system should deal with
unknown words, since there are three ways in which
they can appear in a bigram: as the current word, as
the previous word or as both. A good answer is to
train a separate, unknown word–model off of held-out
data, to gather statistics of unknown words occurring
in the midst of known words.
Typically, one holds out 10-20% of one&apos;s
training for smoothing or unknown word–training.
In order to overcome the limitations of a small
amount of training data—particularly in Spanish—we
hold out 50% of our data to train the unknown word–
model (the vocabulary is built up on the first 50%),
save these counts in training data file, then hold out
the other 50% and concatentate these bigram counts
with the first unknown word–training file. This way,
we can gather likelihoods of an unknown word
appearing in the bigram using all available training
data. This approach is perfectly valid, as we art
trying to estimate that which we have not
legitimately seen in training. When decoding, if
either word of the bigram is unknown, the model used
to estimate the probabilities of Equations 3.1-3 is the
unknown word model, otherwise it is the model from
the normal training. The unknown word–model can
be viewed as a first level of back-off, therefore, since
it is used as a backup model when an unknown word
is encountered, and is necessarily not as accurate as
the bigram model formed from the actual training.
3. 3. 3.2 Further Back-off Models and
Smoothing
Whether a bigram contains an unknown word or
not, it is possible that either model may not have
seen this bigram, in which case the model backs off
to a less-powerful, less-descriptive model. Table 3.2
shows a graphic illustration of the back-off scheme:
</bodyText>
<page confidence="0.950835">
197
</page>
<figure confidence="0.677546294117647">
Name-class Bi ams
Pr(NC I NC„, w,)
Pr(NC I NC„)
Pr(NC)
1
number of name - classes
First-word Bi rams
Pr((w,f)fi„, I NC, NC_
Pr((w,f) I (+begin+, other), NC)
Pr((w, PI NC)
Pr(w I NC). Pr(f I NC)
Non—first-word Bi rams
Pr((w,f) I (w,f)„, NC
Pr((w,f) I NC)
Pr(w I NC). Pr(f I NC)
I VI number of word features
I VI number of word features
</figure>
<tableCaption confidence="0.727731">
Table 3.2 Back-off strategy
</tableCaption>
<bodyText confidence="0.999824333333333">
The weight for each back-off model is computed on-
the-fly, using the following formula:
If computing Pr(XIY), assign weight of A to
the direct computation (using one of the
formulae of §3.3.2) and a weight of (1 — A) to
the back-off model, where
</bodyText>
<equation confidence="0.9508405">
old c(Y) 1
A = (1
c(y) 1 + unique outcomes of Y
c(Y)
</equation>
<bodyText confidence="0.988773821428572">
(3.8)
where &amp;quot;old c(Y)&amp;quot; is the sample size of the model from
which we are backing off. This is a rather simple
method of smoothing, which tends to work well
when there are only three or four levels of back-off.4
This method also overcomes the problem when a
back-off model has roughly the same amount of
training as the current model, via the first factor of
Equation 3.8, which essentially ignores the back-off
model and puts all the weight on the primary model,
in such an equi-trained situation.
As an example—disregarding the first factor—if
we saw the bigram &amp;quot;come hither&amp;quot; once in training and
we saw &amp;quot;come here&amp;quot; three times, and nowhere else did
we see the word &amp;quot;come&amp;quot; in the NOT-A-NAME class,
when computing
Pr(&amp;quot;hither&amp;quot; I &amp;quot;come&amp;quot;, NOT-A-NAME),
we would back off to the unigram probability
Pr(&amp;quot;hither&amp;quot; I NOT-A-NAME)
with a weight of , since the number of unique
outcomes for the word-state for &amp;quot;come&amp;quot; would be two,
and the total number of times &amp;quot;come&amp;quot; had been the
preceding word in a bigram would be four (a
4 Any more levels of back-off might require a more sophisticated
smoothing technique, such as deleted interpolation. No matter
what smoothing technique is used, one must remember that
smoothing is the art of estimating the probability of that which is
unknown (i.e., not seen in training).
</bodyText>
<equation confidence="0.744926">
1/(1 + 4) =4 weight for the bigram probability, a
1 —4 4 I weight for the back-off model).
</equation>
<subsectionHeader confidence="0.99558">
3.4 Comparison with a traditional HMM
</subsectionHeader>
<bodyText confidence="0.99998965">
Unlike a traditional HMM, the probability of
generating a particular word is 1 for each word-state
inside each of the name-class states. An alternative—
and more traditional—model would have a small
number of states within each name-class, each
having, perhaps, some semantic signficance, e.g.,
three states in the PERSON name-class, representing a
first, middle and last name, where each of these three
states would have some probability associated with
emitting any word from the vocabulary. We chose to
use a bigram language model because, while less
semantically appealing, such n-gram language models
work remarkably well in practice. Also, as a first
research attempt, an n-gram model captures the most
general significance of the words in each name-class,
without presupposing any specifics of the structure of
names, a la the PERSON name-class example, above.
More important, either approach is mathematically
valid, as long as all transitions out of a given state
sum to one.
</bodyText>
<subsectionHeader confidence="0.989319">
3.5 Decoding
</subsectionHeader>
<bodyText confidence="0.999967818181818">
All of this modeling would be for naught were it
not for the existence of an efficient algorithm for
finding the optimal state sequence, thereby &amp;quot;decoding&amp;quot;
the original sequence of name-classes. The number of
possible state sequences for N states in an ergodic
model for a sentence of m words is Alm, but, using
dynamic programming and an appropriate merging of
multiple theories when they converge on a particular
state—the Viterbi decoding algorithm—a sentence can
be &amp;quot;decoded&amp;quot; in time linear to the number of tokens in
the sentence, 0(m) (Viterbi, 1967). Since we are
</bodyText>
<page confidence="0.997369">
198
</page>
<bodyText confidence="0.999532333333333">
interested in recovering the name-class state sequence,
we pursue eight theories at every given step of the
algorithm.
</bodyText>
<sectionHeader confidence="0.960693" genericHeader="method">
4. Implementation and Results
</sectionHeader>
<subsectionHeader confidence="0.999908">
4.1 Development History
</subsectionHeader>
<bodyText confidence="0.999968147058824">
Initially, the word-feature was not in the model;
instead the system relied on a third-level back-off part-
of-speech tag, which in turn was computed by our
stochastic part-of-speech tagger. The tags were taken
at face value: there were not k-best tags; the system
treated the part-of-speech tagger as a &amp;quot;black box&amp;quot;.
Although the part-of-speech tagger used capitalization
to help it determine proper-noun tags, this feature was
only implicit in the model, and then only after two
levels of back-off! Also, the capitalization of a word
was submerged in the muddiness of part-of-speech
tags, which can &amp;quot;smear&amp;quot; the capitalization probability
mass over several tags. Because it seemed that
capitalization would be a good name-predicting
feature, and that it should appear earlier in the model,
we eliminated the reliance on part-of-speech
altogether, and opted for the more direct, word-feature
model described above, in §3. Originally, we had a
very small number of features, indicating whether the
word was a number, the first word of a sentence, all
uppercase, inital-capitalized or lower-case. We then
expanded the feature set to its current state in order to
capture more subtleties related mostly to numbers;
due to increased performance (although not entirely
dramatic) on every test, we kept the enlarged feature
set.
Contrary to our expectations (which were based on
our experience with English), Spanish contained
many examples of lower-case words in organization
and location names. For example, departamento
(&amp;quot;Department&amp;quot;) could often start an organization
name, and adjectival place-names, such as coreana
(&amp;quot;Korean&amp;quot;) could appear in locations and by
convention are not capitalized.
</bodyText>
<subsectionHeader confidence="0.997592">
4.2 Current Implementation
</subsectionHeader>
<bodyText confidence="0.999688578947368">
The entire system is implemented in C++, atop a
&amp;quot;home-brewed&amp;quot;, general-purpose class library,
providing a rapid code-compile-train-test cycle. In
fact, many NLP systems suffer from a lack of
software and computer-science engineering effort: run-
time efficiency is key to performing numerous
experiments, which, in turn, is key to improving
performance. A system may have excellent
performance on a given task, but if it takes long to
compile and/or run on test data, the rate of
improvement of that system will be miniscule
compared to that which can run very efficiently. On a
Sparc20 or SGI Indy with an appropritae amount of
RAM, Nymble can compile in 10 minutes, train in 5
minutes and run at 6MB/hr. There were days in
which we had as much as a 15% reduction in error
rate, to borrow the performance measure used by the
speech community, where error rate = 100% — F-
measure. (See §4.3 for the definition of F-measure.)
</bodyText>
<subsectionHeader confidence="0.999574">
4.3 Results of evaluation
</subsectionHeader>
<bodyText confidence="0.9999742">
In this section we report the results of evaluating
the final version of the learning software. We report
the results for English and for Spanish and then the
results of a set of experiments to determine the
impact of the training set size on the algorithm&apos;s
performance in both English and Spanish.
For each language, we have a held-out
development test set and a held-out, blind test set.
We only report results on the blind test set for each
respective language.
</bodyText>
<subsectionHeader confidence="0.926007">
4.3.1 F-measure
</subsectionHeader>
<bodyText confidence="0.9878438">
The scoring program measures both precision =I
recall, terms borrowed from the information-retrieval
community, where
number of correct responses and
number responses
</bodyText>
<equation confidence="0.859714">
R = (4.1)
</equation>
<bodyText confidence="0.954834363636364">
number of correct responses
number correct in key
Put informally, recall measures the number of &amp;quot;hits&amp;quot;
vs. the number of possible correct answers as
specified in the key file, whereas precision measures
how many answers were correct ones compared to the
number of answers delivered. These two measures of
performance combine to form one measure of
performance, the F-measure, which is computed by
the weighted harmonic mean of precision and recall:
(/32 +1)RP
</bodyText>
<subsectionHeader confidence="0.730354">
F (s2R)+ p
</subsectionHeader>
<bodyText confidence="0.999809666666667">
where if represents the relative weight of recall to
precision (and typically has the value 1). To our
knowledge, our learned name-finding system has
achieved a higher F-measure than any other learned
system when compared to state-of-the-art manual
(rule-based) systems on similar data.
</bodyText>
<subsectionHeader confidence="0.815021">
4.3.2 English and Spanish Results
</subsectionHeader>
<bodyText confidence="0.929557875">
Our test set of English data for reporting results is
that of the MUC-6 test set, a collection of 30 WSJ
documents (we used a different test set during
development). Our Spanish test set is that used for
MET, comprised of articles from the news agency
AFP. Table 4.1 illustrates Nymble&apos;s performance as
compared to the best reported scores for each category.
(4.2)
</bodyText>
<page confidence="0.991856">
199
</page>
<table confidence="0.9990716">
Case Language Best Reported Nymble
Score
Mixed English 96 93
Upper English 89 91
Mixed Spanish 93 90
</table>
<tableCaption confidence="0.931952">
Table 4.1 F-measure Scores
</tableCaption>
<sectionHeader confidence="0.699012" genericHeader="method">
4.3.3 The Amount of Training Data
Required
</sectionHeader>
<bodyText confidence="0.999983586206897">
With any learning technique one of the important
questions is how much training data is required to get
acceptable performance. More generally how does
performance vary as the training set size is increased
or decreased? We ran a sequence of experiments in
English and in Spanish to try to answer this question
for the final model that was implemented.
For English, there were 450,000 words of training
data. By that we mean that the text of the document
itself (including headlines but not including SGML
tags) was 450,000 words long. Given this maximum
size of training available to us, we successfully
divided the training material in half until we were
using only one eighth of the original training set size
or a training set of 50,000 words for the smallest
experiment. To give a sense of the size of 450,000
words, that is roughly half the length of one edition
of the Wall Street Journal.
The results are shown in a histogram in Figure
4.1 below. The positive outcome of the experiment
is that half as much training data would have given
almost equivalent performance. Had we used only
one quarter of the data or approximately 100,000
words, performance would have degraded slightly,
only about 1-2 percent. Reducing the training set
size to 50,000 words would have had a more
significant decrease in the performance of the system;
however, the performance is still impressive even
with such a small training set.
</bodyText>
<figureCaption confidence="0.7891532">
Figure 4.1: Impact of Various Training
Set Sizes on Performance in English. The
learning algorithm petforms remarkable well, nearly
comparable to handcrafted systems with as little as
100,000 words of training data.
</figureCaption>
<bodyText confidence="0.999839888888889">
On the other hand, the result also shows that
merely annotating more data will not yield dramatic
improvement in the performance. With increased
training data it would be possible to use even more
detailed models that require more data and could
achieve significantly improved overall system
performance with those more detailed models.
For Spanish we had only 223,000 words of
training data. We also measured the performance of
the system with half the training data or slightly
more than 100,000 words of text. Figure 4.2 shows
the results. There is almost no change in
performance by using as little as 100,000 words of
training data.
Therefore the results in both languages were
comparable. As little as 100,000 words of training
data produces performance nearly comparable to
handcrafted systems.
</bodyText>
<figure confidence="0.9954543125">
100
90
80
70
60
50
40
30
20
10
0
o =
o
o
C
*E
&apos;ars
0 c
to • —
C■1 Eti
200
100
90
80
70
60
50
40
30
20
10
o
</figure>
<figureCaption confidence="0.983969">
Figure 4.2: Impact of Training Set Size on
Performance in Spanish
</figureCaption>
<sectionHeader confidence="0.995486" genericHeader="method">
5. Further Work
</sectionHeader>
<bodyText confidence="0.99091">
While our initial results have been quite favorable,
there is still much that can be done potentially to
improve performance and completely close the gap
between learned and rule-based name-finding systems.
We would like to incorporate the following into the
current model:
</bodyText>
<listItem confidence="0.957823714285714">
• lists of organizations, person names and
locations
• an aliasing algorithm, which dynamically updates
the model (where e.g. IBM is an alias of
International Business Machines)
• longer-distance information, to find names not
captured by our bigram model
</listItem>
<sectionHeader confidence="0.970753" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.99925615">
We have shown that using a fairly simple
probabilistic model, finding names and other
numerical entities as specified by the MUC tasks can
be performed with &amp;quot;near-human performance&amp;quot;, often
likened to an F of 90 or above. We have also shown
that such a system can be trained efficiently and that,
given appropriately and consistently marked answer
keys, it can be trained on languages foreign to the
trainer of the system; for example, we do not speak
Spanish, but trained Nymble on answer keys marked
by native speakers. None of the formalisms or
techniques presented in this paper is new; rather, the
approach to this task—the model itself—is wherein
lies the novelty. Given the incredibly difficult nature
of many NLP tasks, this example of a learned,
stochastic approach to name-finding lends credence to
the argument that the NLP community ought to push
these approaches, to find the limit of phenomena that
may be captured by probabilistic, finite-state
methods.
</bodyText>
<sectionHeader confidence="0.783356" genericHeader="acknowledgments">
7. References
</sectionHeader>
<reference confidence="0.999916580645162">
Aberdeen, J., Burger, J., Day, D., Hirschman, L.,
Robinson, P. and Vilain, M. (1995) In
Proceedings of the Sixth Message
Understanding Conference (MUC-6)Morgan
Kaufmann Publishers, Inc., Columbia,
Maryland, pp. 141-155.
Appelt, D. E., Jerry R. Hobbs, Bear, J., Israel, D.,
Kameyama, M., Kehler, A., Martin, D.,
Myers, K. and Tyson, M. (1995) In
Proceedings of the Sixth Message
Understanding Conference (MUC-6)Morgan
Kaufmann Publishers, Inc., Columbia,
Maryland, pp. 237-248.
Church, K. (1988) In Second Conference on Applied
Natural Language Processing, Austin, Texas.
Cover, T. and Thomas, J. A. (1991) Elements of
Information Theory, John Wiley &amp; Sons, Inc.,
New York.
Miller, S., Bobrow, R., Schwartz, R. and Ingria, R.
(1994) In Human Language Technology
Workshop, Morgan Kaufmann Publishers,
Plainsboro, New Jersey, pp. 278-282.
Viterbi, A. J. (1967) IEEE Transactions on
Information Theory, 1T-13(2), 260-269.
Weischedel, R. (1995) In Proceedings of the Sixth
Message Understanding Conference (MUC-
6)Morgan Kaufmann Publishers, Inc.,
Columbia, Maryland, pp. 55-69.
Weischedel, R., Meteer, M., Schwartz, R.,
Ramshaw, L. and Palmucci, J. (1993)
Computational Linguistics, 19(2), 359-382.
</reference>
<sectionHeader confidence="0.529791" genericHeader="references">
8. Acknowledgements
</sectionHeader>
<bodyText confidence="0.999552">
The work reported here was supported in part by
the Defense Advanced Research Projects Agency; a
technical agent for part of the work was Fort
Huachucha under contract number DABT63-94-C-
0062. The views and conclusions contained in this
document are those of the authors and should not be
interpreted as necessarily representing the official
policies, either expressed or implied, of the Defense
Advanced Research Projects Agency or the United
States Government.
We would also like to give special acknowledge-
ment to Stuart Shieber, McKay Professor of
Computer Science at Harvard University, who
endorsed and helped foster the completion of this, the
first phase of Nymble&apos;s development.
</bodyText>
<page confidence="0.997597">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940813">
<title confidence="0.988267">Nymble: a High-Performance Learning Name-finder</title>
<author confidence="0.999857">Scott Miller</author>
<affiliation confidence="0.997637">Corporation</affiliation>
<address confidence="0.9996655">70 Fawcett Street Cambridge, MA 02138</address>
<email confidence="0.999806">szmiller@bbn.com</email>
<author confidence="0.999987">Daniel M Bikel</author>
<affiliation confidence="0.992651">Corporation</affiliation>
<address confidence="0.9995495">70 Fawcett Street Cambridge, MA 02138</address>
<email confidence="0.999825">dbikel@bbn.com</email>
<author confidence="0.999982">Richard Schwartz</author>
<affiliation confidence="0.998389">Corporation</affiliation>
<address confidence="0.999677">70 Fawcett Street Cambridge, MA 02138</address>
<email confidence="0.999676">schwartz@bbn.com</email>
<author confidence="0.999806">Ralph Weischedel</author>
<affiliation confidence="0.99224">Corporation</affiliation>
<address confidence="0.9995445">70 Fawcett Street Cambridge, MA 02138</address>
<email confidence="0.9999">weisched@bbn.com</email>
<abstract confidence="0.9984791">This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>J Burger</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6)Morgan</booktitle>
<pages>141--155</pages>
<publisher>Kaufmann Publishers, Inc.,</publisher>
<location>Columbia, Maryland,</location>
<contexts>
<context position="4025" citStr="Aberdeen et al., 1995" startWordPosition="627" endWordPosition="630"> Approaches to Namefinding Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995). For every new language and every new class of new information to spot, one has to write a new set of rules to cover the new language and to cover the new class of information. A finite-state pattern rule attempts to match against a sequence of tokens (words), in much the same way as a general regular expression matcher. In addition to these finitestate pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995). 2.3 Interest in Problem and Potential Applications The atomic elements of information extraction— indeed, of language as a whole—could be considered the who, where, when and how much in a sentence. A name-finder performs what is known as surface- or lightweight-parsing, delimiting sequences of tokens that answer these important questions. It can be used as the first step in a chain of processors: a next level of processing could relate two or more named entities, or perhaps even give semantics to that relationship using a verb. In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;h</context>
</contexts>
<marker>Aberdeen, Burger, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>Aberdeen, J., Burger, J., Day, D., Hirschman, L., Robinson, P. and Vilain, M. (1995) In Proceedings of the Sixth Message Understanding Conference (MUC-6)Morgan Kaufmann Publishers, Inc., Columbia, Maryland, pp. 141-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Appelt</author>
<author>Jerry R Hobbs</author>
<author>J Bear</author>
<author>D Israel</author>
<author>M Kameyama</author>
<author>A Kehler</author>
<author>D Martin</author>
<author>K Myers</author>
<author>M Tyson</author>
</authors>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6)Morgan</booktitle>
<pages>237--248</pages>
<publisher>Kaufmann Publishers, Inc.,</publisher>
<location>Columbia, Maryland,</location>
<contexts>
<context position="3552" citStr="Appelt et al., 1995" startWordPosition="542" endWordPosition="545">e a priori probability of the word sequence—the denominator—is constant for any given sentence, we can maxi-mize Equation 2.2 by maximizing the numerator alone. I See (Cover and Thomas, 1991), ch. 2, for an excellent overview of the principles of information theory. (2.2) 194 • START-OF-SENTENCE ORGANIZATION END-OF-SENTENCE • , NOT-A-NAME &apos;--------&amp;quot;-PERSON , (five other riame-classes) , i w , Figure 3.1 Pictorial representation of conceptual model. • • , 2.2 Previous Approaches to Namefinding Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995). For every new language and every new class of new information to spot, one has to write a new set of rules to cover the new language and to cover the new class of information. A finite-state pattern rule attempts to match against a sequence of tokens (words), in much the same way as a general regular expression matcher. In addition to these finitestate pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995). 2.3 Interest in Problem and Potential Applications The atomic elements of information extraction— indeed, of language as a wh</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Kameyama, Kehler, Martin, Myers, Tyson, 1995</marker>
<rawString>Appelt, D. E., Jerry R. Hobbs, Bear, J., Israel, D., Kameyama, M., Kehler, A., Martin, D., Myers, K. and Tyson, M. (1995) In Proceedings of the Sixth Message Understanding Conference (MUC-6)Morgan Kaufmann Publishers, Inc., Columbia, Maryland, pp. 237-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<date>1988</date>
<booktitle>In Second Conference on Applied Natural Language Processing,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="1080" citStr="Church, 1988" startWordPosition="157" endWordPosition="158">mes and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. 1. Introduction In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM&apos;s to their problems. More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993). We would now propose that HMM&apos;s have successfully been applied to the problem of name-finding. We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &amp;quot;Nymble&amp;quot;. To our knowledge, Nymble out-performs the best published results of any other learning name-finder. Furthermore, it performs at or above the 90% accuracy level, often considered &amp;quot;near-human performance&amp;quot;. The system arose from the NE task as specified in the last Message Understanding Conference (MUC), where organizati</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988) In Second Conference on Applied Natural Language Processing, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory,</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="3123" citStr="Cover and Thomas, 1991" startWordPosition="480" endWordPosition="483">enerative model is to model the original process which generated the name-class–annotated words, before they went through the noisy channel. More formally, we must find the most likely sequence of name-classes (NC) given a sequence of words (W): Pr(NC I W) (2.1) In order to treat this as a generative model (where it generates the original, name-class–annotated words), we use Bayes&apos; Rule: Pr(W, NC) Pr(NC I W) = Pr(W) and since the a priori probability of the word sequence—the denominator—is constant for any given sentence, we can maxi-mize Equation 2.2 by maximizing the numerator alone. I See (Cover and Thomas, 1991), ch. 2, for an excellent overview of the principles of information theory. (2.2) 194 • START-OF-SENTENCE ORGANIZATION END-OF-SENTENCE • , NOT-A-NAME &apos;--------&amp;quot;-PERSON , (five other riame-classes) , i w , Figure 3.1 Pictorial representation of conceptual model. • • , 2.2 Previous Approaches to Namefinding Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995). For every new language and every new class of new information to spot, one has to write a new set of rules to cover the new language and to cover the new class of inform</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. and Thomas, J. A. (1991) Elements of Information Theory, John Wiley &amp; Sons, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>R Bobrow</author>
<author>R Schwartz</author>
<author>R Ingria</author>
</authors>
<date>1994</date>
<booktitle>In Human Language Technology Workshop,</booktitle>
<pages>278--282</pages>
<publisher>Morgan Kaufmann Publishers, Plainsboro,</publisher>
<location>New Jersey,</location>
<contexts>
<context position="5286" citStr="Miller et al., 1994" startWordPosition="831" endWordPosition="834">rmore, name-finding can be useful in its own right: an Internet query system might use namefinding to construct more appropriately-formed queries: &amp;quot;When was Bill Gates born?&amp;quot; could yield the query &amp;quot;Bill Gates&amp;quot;+born. Also, name-finding can be directly employed for link analysis and other information retrieval problems. 3. Model We will present the model twice, first in a conceptual and informal overview, then in a moredetailed, formal description of it as a type of HMM. The model bears resemblance to Scott Miller&apos;s novel work in the Air Traffic Information System (ATIS) task, as documented in (Miller et al., 1994). 3.1 Conceptual Model Figure 3.1 is a pictorial overview of our model. Informally, we have an ergodic HMM with only eight internal states (the name classes, including the NOT-A-NAME class), with two special states, the START- and END-OF-SENTENCE states. Within each of the name-class states, we use a statistical bigram language model, with the usual one-word-per-state emission. This means that the number of states in each of the name-class states is equal to the vocabulary size, I VI . The generation of words and name-classes proceeds in three steps: 1. Select a name-class NC, conditioning on </context>
</contexts>
<marker>Miller, Bobrow, Schwartz, Ingria, 1994</marker>
<rawString>Miller, S., Bobrow, R., Schwartz, R. and Ingria, R. (1994) In Human Language Technology Workshop, Morgan Kaufmann Publishers, Plainsboro, New Jersey, pp. 278-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>260--269</pages>
<contexts>
<context position="18106" citStr="Viterbi, 1967" startWordPosition="2940" endWordPosition="2941">transitions out of a given state sum to one. 3.5 Decoding All of this modeling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence, thereby &amp;quot;decoding&amp;quot; the original sequence of name-classes. The number of possible state sequences for N states in an ergodic model for a sentence of m words is Alm, but, using dynamic programming and an appropriate merging of multiple theories when they converge on a particular state—the Viterbi decoding algorithm—a sentence can be &amp;quot;decoded&amp;quot; in time linear to the number of tokens in the sentence, 0(m) (Viterbi, 1967). Since we are 198 interested in recovering the name-class state sequence, we pursue eight theories at every given step of the algorithm. 4. Implementation and Results 4.1 Development History Initially, the word-feature was not in the model; instead the system relied on a third-level back-off partof-speech tag, which in turn was computed by our stochastic part-of-speech tagger. The tags were taken at face value: there were not k-best tags; the system treated the part-of-speech tagger as a &amp;quot;black box&amp;quot;. Although the part-of-speech tagger used capitalization to help it determine proper-noun tags,</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, A. J. (1967) IEEE Transactions on Information Theory, 1T-13(2), 260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
</authors>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC6)Morgan</booktitle>
<pages>55--69</pages>
<publisher>Kaufmann Publishers, Inc.,</publisher>
<location>Columbia, Maryland,</location>
<marker>Weischedel, 1995</marker>
<rawString>Weischedel, R. (1995) In Proceedings of the Sixth Message Understanding Conference (MUC6)Morgan Kaufmann Publishers, Inc., Columbia, Maryland, pp. 55-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>L Ramshaw</author>
<author>J Palmucci</author>
</authors>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>359--382</pages>
<contexts>
<context position="1137" citStr="Weischedel et al., 1993" startWordPosition="164" endWordPosition="167">s per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. 1. Introduction In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM&apos;s to their problems. More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993). We would now propose that HMM&apos;s have successfully been applied to the problem of name-finding. We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &amp;quot;Nymble&amp;quot;. To our knowledge, Nymble out-performs the best published results of any other learning name-finder. Furthermore, it performs at or above the 90% accuracy level, often considered &amp;quot;near-human performance&amp;quot;. The system arose from the NE task as specified in the last Message Understanding Conference (MUC), where organization names, person names, location names, times, dates, per</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. and Palmucci, J. (1993) Computational Linguistics, 19(2), 359-382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>