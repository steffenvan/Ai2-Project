<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002408">
<title confidence="0.998261">
Analysis of Morph-Based Speech Recognition and the Modeling of
Out-of-Vocabulary Words Across Languages
</title>
<author confidence="0.9791015">
Mathias Creutz*, Teemu Hirsimiki*, Mikko Kurimo*, Antti Puurula*, Janne Pylkk¨onen*,
Vesa Siivola*, Matti Varjokallio*, Ebru Arısoyt, Murat Sarac¸lart, and Andreas Stolcket
</author>
<affiliation confidence="0.815513666666667">
*Helsinki University of Technology, &lt;firstname&gt;.&lt;lastname&gt;@tkk.fi,
t Boˇgazic¸i University, arisoyeb@boun.edu.tr, murat.saraclar@boun.edu.tr,
t SRI International/ International Computer Science Institute, stolcke@speech.sri.com
</affiliation>
<sectionHeader confidence="0.991825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960611111111">
We analyze subword-based language
models (LMs) in large-vocabulary
continuous speech recognition across
four “morphologically rich” languages:
Finnish, Estonian, Turkish, and Egyptian
Colloquial Arabic. By estimating n-gram
LMs over sequences of morphs instead
of words, better vocabulary coverage
and reduced data sparsity is obtained.
Standard word LMs suffer from high
out-of-vocabulary (OOV) rates, whereas
the morph LMs can recognize previously
unseen word forms by concatenating
morphs. We show that the morph LMs
generally outperform the word LMs and
that they perform fairly well on OOVs
without compromising the accuracy
obtained for in-vocabulary words.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950519230769">
As automatic speech recognition systems are being
developed for an increasing number of languages,
there is growing interest in language modeling ap-
proaches that are suitable for so-called “morpholog-
ically rich” languages. In these languages, the num-
ber of possible word forms is very large because
of many productive morphological processes; words
are formed through extensive use of, e.g., inflection,
derivation and compounding (such as the English
words ‘rooms’, ‘roomy’, ‘bedroom’, which all stem
from the noun ‘room’).
For some languages, language modeling based on
surface forms of words has proven successful, or at
least satisfactory. The most studied language, En-
glish, is not characterized by a multitude of word
forms. Thus, the recognition vocabulary can sim-
ply consist of a list of words observed in the training
text, and n-gram language models (LMs) are esti-
mated over word sequences. The applicability of the
word-based approach to morphologically richer lan-
guages has been questioned. In highly compounding
languages, such as the Germanic languages German,
Dutch and Swedish, decomposition of compound
words can be carried out to reduce the vocabulary
size. Highly inflecting languages are found, e.g.,
among the Slavic, Romance, Turkic, and Semitic
language families. LMs incorporating morphologi-
cal knowledge about these languages can be applied.
A further challenging category comprises languages
that are both highly inflecting and compounding,
such as the Finno-Ugric languages Finnish and Es-
tonian.
Morphology modeling aims to reduce the out-
of-vocabulary (OOV) rate as well as data sparsity,
thereby producing more effective language mod-
els. However, obtaining considerable improvements
in speech recognition accuracy seems hard, as is
demonstrated by the fairly meager improvements
(1–4 % relative) over standard word-based models
accomplished by, e.g., Berton et al. (1996), Ordel-
man et al. (2003), Kirchhoff et al. (2006), Whit-
taker and Woodland (2000), Kwon and Park (2003),
and Shafran and Hall (2006) for Dutch, Arabic, En-
glish, Korean, and Czech, or even the worse perfor-
mance reported by Larson et al. (2000) for German
and Byrne et al. (2001) for Czech. Nevertheless,
clear improvements over a word baseline have been
achieved for Serbo-Croatian (Geutner et al., 1998),
Finnish, Estonian (Kurimo et al., 2006b) and Turk-
ish (Kurimo et al., 2006a).
In this paper, subword language models in the
recognition of speech of four languages are ana-
</bodyText>
<page confidence="0.97392">
380
</page>
<note confidence="0.7977065">
Proceedings of NAACL HLT 2007, pages 380–387,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.989541">
lyzed: Finnish, Estonian, Turkish, and the dialect
of Arabic spoken in Egypt, Egyptian Colloquial
Arabic (ECA). All these languages are considered
“morphologically rich”, but the benefits of using
subword-based LMs differ across languages. We at-
tempt to discover explanations for these differences.
In particular, the focus is on the analysis of OOVs:
A perceived strength of subword models, when con-
trasted with word models, is that subword models
can generalize to previously unseen word forms by
recognizing them as sequences of shorter familiar
word fragments.
</bodyText>
<sectionHeader confidence="0.998909" genericHeader="introduction">
2 Morfessor
</sectionHeader>
<bodyText confidence="0.99859953125">
Morfessor is an unsupervised, data-driven, method
for the segmentation of words into morpheme-like
units. The general idea is to discover as com-
pact a description of the input text corpus as possi-
ble. Substrings occurring frequently enough in sev-
eral different word forms are proposed as morphs,
and the words in the corpus are then represented
as a concatenation of morphs, e.g., ‘hand, hand+s,
left+hand+ed, hand+ful’. Through maximum a pos-
teriori optimization (MAP), an optimal balance is
sought between the compactness of the inventory of
morphs, i.e., the morph lexicon, versus the compact-
ness of the representation of the corpus.
Among others, de Marcken (1996), Brent (1999),
Goldsmith (2001), Creutz and Lagus (2002), and
Creutz (2006) have shown that models based on
the above approach produce segmentations that re-
semble linguistic morpheme segmentations, when
formulated mathematically in a probabilistic frame-
work or equivalently using the Minimum Descrip-
tion Length (MDL) principle (Rissanen, 1989).
Similarly, Goldwater et al. (2006) use a hierarchical
Dirichlet model in combination with morph bigram
probabilities.
The Morfessor model has been developed over
the years, and different model versions exist. The
model used in the speech recognition experiments of
the current paper is the original, so-called Morfes-
sor Baseline algorithm, which is publicly available
for download.1. The mathematics of the Morfessor
Baseline model is briefly outlined in the following;
consult Creutz (2006) for details.
</bodyText>
<footnote confidence="0.982139">
1http://www.cis.hut.fi/projects/morpho/
</footnote>
<subsectionHeader confidence="0.97063">
2.1 MAP Optimization Criterion
</subsectionHeader>
<bodyText confidence="0.999988">
In slightly simplified form, the optimization crite-
rion utilized in the model corresponds to the maxi-
mization of the following posterior probability:
</bodyText>
<equation confidence="0.9814375">
P(lexicon  |corpus) a
P(lexicon) · P(corpus  |lexicon) _
H P(α) · � P(µ). (1)
letters α morphs µ
</equation>
<bodyText confidence="0.99993975">
The lexicon consists of all distinct morphs spelled
out; this forms a long string of letters α, in which
each morph is separated from the next morph using
a morph boundary character. The probability of the
lexicon is the product of the probability of each let-
ter in this string. Analogously, the corpus is repre-
sented as a sequence of morphs, which corresponds
to a particular segmentation of the words in the cor-
pus. The probability of this segmentation equals the
product of the probability of each morph token µ.
Letter and morph probabilities are maximum likeli-
hood estimates (empirical Bayes).
</bodyText>
<subsectionHeader confidence="0.995804">
2.2 From Morphs to n-Grams
</subsectionHeader>
<bodyText confidence="0.999987538461538">
As a result of the probabilistic (or MDL) approach,
the morph inventory discovered by the Morfessor
Baseline algorithm is larger the more training data
there is. In some speech recognition experiments,
however, it has been desirable to restrict the size of
the morph inventory. This has been achieved by set-
ting a frequency threshold on the words on which
Morfessor is trained, such that the rarest words will
not affect the learning process. Nonetheless, the
rarest words can be split into morphs in accordance
with the model learned, by using the Viterbi algo-
rithm to select the most likely segmentation. The
process is depicted in Figure 1.
</bodyText>
<subsectionHeader confidence="0.995501">
2.3 Grapheme-to-Phoneme Mapping
</subsectionHeader>
<bodyText confidence="0.999925555555556">
The mapping between graphemes (letters) and
phonemes is straightforward in the languages stud-
ied in the current paper. More or less, there is
a one-to-one correspondence between letters and
phonemes. That is, the spelling of a word indicates
the pronunciation of the word, and when splitting the
word into parts, the pronunciation of the parts in iso-
lation does not differ much from the pronunciation
of the parts in context. However, a few exceptions
</bodyText>
<page confidence="0.998788">
381
</page>
<figureCaption confidence="0.825126333333333">
Figure 1: How to train a segmentation model using
the Morfessor Baseline algorithm, and how to fur-
ther train an n-gram model based on morphs.
have been treated more rigorously in the Arabic ex-
periments: e.g., in some contexts the same (spelled)
morph can have multiple possible pronunciations.
</figureCaption>
<sectionHeader confidence="0.99713" genericHeader="method">
3 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999595">
The goal of the conducted experiments is to com-
pare n-gram language models based on morphs to
standard word n-gram models in automatic speech
recognition across languages.
</bodyText>
<subsectionHeader confidence="0.99994">
3.1 Data Sets and Recognition Systems
</subsectionHeader>
<bodyText confidence="0.99981015">
The results from eight different tests have been an-
alyzed. Some central properties of the test config-
urations are shown in Table 1. The Finnish, Esto-
nian, and Turkish test configurations are slight vari-
ations of experiments reported earlier in Hirsim¨aki
et al. (2006) (Fin1: ‘News task’, Fin2: ‘Book task’),
Kurimo et al. (2006a) (Fin3, Tur1), and Kurimo et
al. (2006b) (Fin4, Est, Tur2).
Three different recognition platforms have been
used, all of which are state-of-the-art large vocab-
ulary continuous speech recognition (LVCSR) sys-
tems. The Finnish and Estonian experiments have
been run on the HUT speech recognition system de-
veloped at Helsinki University of Technology.
The Turkish tests were performed using the
AT&amp;T decoder (Mohri and Riley, 2002); the acous-
tic features were produced using the HTK front end
(Young et al., 2002). The experiments on Egyptian
Colloquial Arabic (ECA) were carried out using the
SRI DecipherTM speech recognition system.
</bodyText>
<subsectionHeader confidence="0.898008">
3.1.1 Speech Data and Acoustic Models
</subsectionHeader>
<bodyText confidence="0.999992964285714">
The type and amount of speech data vary from
one language to another. The Finnish data con-
sists of news broadcasts read by one single female
speaker (Fin1), as well as an audio book read by an-
other female speaker (Fin2, Fin3, Fin4). The Finnish
acoustic models are speaker dependent (SD). Mono-
phones (mon) were used in the earlier experiments
(Fin1, Fin2), but these were later replaced by cross-
context triphones (tri).
The Estonian speech data has been collected from
a large number of speakers and consists of sen-
tences from newspapers as well as names and dig-
its read aloud. The acoustic models are speaker-
independent triphones (SI tri) adapted online using
Cepstral Mean Subtraction and Constrained Maxi-
mum Likelihood Linear Regression. Also the Turk-
ish acoustic training data contains speech from hun-
dreds of speakers. The test set is composed of news-
paper text read by one female speaker. Speaker-
independent triphones are used as acoustic models.
The Finnish, Estonian, and Turkish data sets con-
tain planned speech, i.e., written text read aloud.
By contrast, the Arabic data consists of transcribed
spontaneous telephone conversations,2 which are
characterized by disfluencies and by the presence
of “non-speech”, such as laugh and cough sounds.
There are multiple speakers in the Arabic data, and
online speaker adaptation has been performed.
</bodyText>
<subsectionHeader confidence="0.925224">
3.1.2 Text Data and Language Models
</subsectionHeader>
<bodyText confidence="0.999993235294118">
The n-gram language models are trained using
the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2,
Tur1, Tur2, ECA) or similar software developed
at HUT (Siivola and Pellom, 2005) (Fin3, Fin4,
Est). All models utilize the Modified Interpolated
Kneser-Ney smoothing technique (Chen and Good-
man, 1999). The Arabic LM is trained on the
same corpus that is used for acoustic training. This
data set is regrettably small (160 000 words), but it
matches the test set well in style, as it consists of
transcribed spontaneous speech. The LM training
corpora used for the other languages contain fairly
large amounts of mainly news and book texts and
conceivably match the style of the test data well.
In the morph-based models, words are split into
morphs using Morfessor, and statistics are collected
for morph n-grams. As the desired output of the
</bodyText>
<footnote confidence="0.819342">
2LDC CallHome corpus of Egyptian Colloquial Ara-
bic: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC97S45
</footnote>
<figure confidence="0.995385352941176">
Morph
inventory
+ probs
Text with words
segmented into
morphs
Train
n−grams
Extract
words
Morfessor
Frequency
cut−off
LM
Viterbi
segm.
Text corpus
</figure>
<page confidence="0.990671">
382
</page>
<tableCaption confidence="0.999521">
Table 1: Test configurations
</tableCaption>
<table confidence="0.994852166666667">
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
Recognizer HUT HUT HUT HUT HUT AT&amp;T AT&amp;T SRI
Speech data
Type of speech read read read read read read read spont.
Training set [kwords] 20 49 49 49 790 230 110 160
Speakers in training set 1 1 1 1 1300 550 250 310
Test set [kwords] 4.3 1.9 1.9 1.9 3.7 7.0 7.0 16
Speakers in test set 1 1 1 1 50 1 1 57
Text data
LM training set [Mwords] 36 36 32 150 53 17 27 0.16
Models
Acoustic models SD mon SD mon SD tri SD tri SI tri SI tri SI tri SI tri
Morph lexicon [kmorphs] 66 66 120 25 37 52 34 6.1
Word lexicon [kwords] 410 410 410 – 60 120 50 18
Out-of-vocabulary words
OOV LM training set [%] 5.0 5.0 5.9 – 14 5.3 9.6 0.61
OOV test set [%] 5.0 7.2 7.3 – 19 5.5 12 9.9
New words in test set [%] 2.7 3.0 3.1 1.5 3.4 1.6 1.5 9.8
</table>
<bodyText confidence="0.999793212121212">
speech recognizer is a sequence of words rather than
morphs, the LM explicitly models word breaks as
special symbols occurring in the morph sequence.
For comparison, word n-gram models have been
tested. The vocabulary cannot typically include ev-
ery word form occurring in the training set (because
of the large number of different words), so the most
frequent words are given priority; the actual lexicon
sizes used in each experiment are shown in Table 1.
Any word not contained in the lexicon is replaced by
a special out-of-vocabulary symbol.
As words and morphs are units of different length,
their optimal performance may occur at different or-
ders of the n-gram. The best order of the n-gram
has been optimized on development test sets in the
following cases: Fin1, Fin2, Tur1, ECA (4-grams
for both morphs and words) and Tur2 (5-grams for
morphs, 3-grams for words). The models have ad-
ditionally been pruned using entropy-based pruning
(Tur1, Tur2, ECA) (Stolcke, 1998). In the other
experiments (Fin3, Fin4, Est), no fixed maximum
value of n was selected. n-Gram growing was per-
formed (Siivola and Pellom, 2005), such that those
n-grams that maximize the training set likelihood
are gradually added to the model. The unrestricted
growth of the model is counterbalanced by an MDL-
type complexity term. The highest order of n-grams
accepted was 7 for Finnish and 8 for Estonian.
Note that the optimization procedure is neutral
with respect to morphs vs. words. Roughly the
same number of parameters are allowed in the result-
ing LMs, but typically the morph n-gram LMs are
smaller than the corresponding word n-gram LMs.
</bodyText>
<sectionHeader confidence="0.665554" genericHeader="method">
3.1.3 Out-of-Vocabulary Words
</sectionHeader>
<bodyText confidence="0.999840592592593">
Table 1 further shows statistics on out-of-
vocabulary rates in the data sets. This is relevant
for the assessment of the word models, as the OOV
rates define the limits of these models.
The OOV rate for the LM training set corresponds
to the proportion of words replaced by the OOV
symbol in the LM training data, i.e., words that were
not included in the recognition vocabulary. The high
OOV rates for Estonian (14 %) and Tur2 (9.6 %) in-
dicate that the word lexicons have poor coverage of
these sets. By contrast, the ECA word lexicon cov-
ers virtually the entire training set vocabulary.
Correspondingly, the test set OOV rate is the pro-
portion of words that occur in the data sets used
for running the speech recognition tests, but that are
missing from the recognition lexicons. This value
is thus the minimum error that can be obtained by
the word models, or put differently, the recognizer
is guaranteed to get at least this proportion of words
wrong. Again, the values are very high for Estonian
(19 %) and Tur2 (12 %), but also for Arabic (9.9 %)
because of the insufficient amount of training data.
Finally, the figures labeled “new words in test set”
denote the proportion of words in the test set that do
not occur in the LM training set. Thus, these values
indicate the minimum error achievable by any word
model trained on the training sets available.
</bodyText>
<page confidence="0.998198">
383
</page>
<figureCaption confidence="0.9845165">
Figure 2: Word accuracies for the different speech
recognition test configurations.
</figureCaption>
<subsectionHeader confidence="0.933926">
3.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999315727272727">
The morph-based and word-based results of the con-
ducted speech recognition experiments are shown in
Figure 2 (for Fin4, no comparable word experiment
has been carried out). The evaluation measure used
is word accuracy (WAC): the number of correctly
recognized words minus the number of incorrectly
inserted words divided by the number of words in
the reference transcript. (Another frequently used
measure is the word error rate, WER, which relates
to word accuracy as WER = 100 % – WAC.)
Figure 2 shows that the morph models perform
better than the word models, with the exception
of the Arabic experiment (ECA), where the word
model outperforms the morph model. The statisti-
cal significance of these differences is confirmed by
one-tailed paired Wilcoxon signed-rank tests at the
significance level of 0.05.
Overall, the best performance is observed for the
Finnish data sets, which is explained by the speaker-
dependent acoustic models and clean noise condi-
tions. The Arabic setup suffers from the insufficient
amount of LM training data.
</bodyText>
<sectionHeader confidence="0.789365" genericHeader="method">
3.2.1 In-Vocabulary Words
</sectionHeader>
<bodyText confidence="0.9995676875">
For a further investigation of the outcome of the
experiments, the test sets have been partitioned into
regions based on the types of words they contain.
The recognition output is aligned with the refer-
ence transcript, and the regions aligned with in-
vocabulary (IV) reference words (words contained
in the vocabulary of the word model) are put in
one partition and the remaining words (OOVs) are
put in another partition. Word accuracies are then
computed separately for the two partitions. Inserted
words, i.e., words that are not aligned with any word
in the reference, are put in the IV partition, unless
they are adjacent to an OOV region, in which case
they are put in the OOV partition.
Figure 3a shows word accuracies for the in-
vocabulary words. Without exception, the accuracy
for the IVs is higher than that of the entire test set vo-
cabulary. One could imagine that the word models
would do better than the morph models on the IVs,
since the word models are totally focused on these
words, whereas the morph models reserve modeling
capacity for a much larger set of words. The word
accuracies in Fig. 3a also partly seem to support this
view. However, Wilcoxon signed-rank tests (level
0.05) show that the superiority of the word model is
statistically significant only for Arabic and for Fin3.
With few exceptions, it is thus possible to draw
the conclusion that morph models are capable of
modeling a much larger set of words than word
models without, however, compromising the perfor-
mance on the limited vocabulary covered by the
word models in a statistically significant way.
</bodyText>
<sectionHeader confidence="0.492175" genericHeader="method">
3.2.2 Out-of-Vocabulary Words
</sectionHeader>
<bodyText confidence="0.999985105263158">
Since the word model and morph model perform
equally well on the subset of words that are included
in the lexicon of the word model, the overall supe-
riority of the morph model needs to come from its
successful coping with out-of-vocabulary words.
In Figure 3b, word accuracies have been plot-
ted for the out-of-vocabulary words contained in the
test set. It is clear that the recognition accuracy for
the OOVs is much lower than the overall accuracy.
Also, negative accuracy values are observed. This
happens when the number of insertions exceeds the
number of correctly recognized units.
In Figure 3b, if speaker-dependent and speaker-
independent setups are considered separately (and
Arabic is left out), there is a tendency for the morph
models to recognize the OOVs more accurately, the
higher the OOV rate is. One could say that a morph
model has a double advantage over a correspond-
ing word model: the larger the proportion of OOVs
</bodyText>
<figure confidence="0.995252630952381">
Word accuracy [%]
100
40
90
80
70
60
50
30
20
10
0
76.6
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
71.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
Morphs
Words
384
79.979.781.9 92.594.6 Morphs
Words
77.9 73.374.771.872.671.771.9 45.648.1
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
100
90
80
70
60
50
40
30
20
10
0
Word accuracy for in−vocabulary words [%]
92.8
82.2
68.667.466.7
61.2
51.9
40.141.8
29.2
13.2
−10.1−14.6
−19.4
−47.7
−62.6
Morphs
Words
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
15.1
89.8
76.6 79.1
71.9
67.8
66.7
55.1
43.2
38.0
−21.8
−74.8
−76.1
100
80
60
40
20
0
Word accuracy for OOVs [%]
−20
−40
−60
−80
−100
(a) (b)
</figure>
<figureCaption confidence="0.96105025">
Figure 3: Word accuracies computed separately for those words in the test sets that are (a) included in and
(b) excluded from the vocabularies of the word vocabulary; cf. figures listed on the row “OOV test set” in
Table 1. Together these two partitions make up the entire test set vocabulary. For comparison, the results for
the entire sets are shown using gray-shaded bars (also displayed in Figure 2).
</figureCaption>
<bodyText confidence="0.999991714285714">
in the word model is, the larger the proportion of
words that the morph model can recognize but the
word model cannot, a priori. In addition, the larger
the proportion of OOVs, the more frequent and more
“easily modelable” words are left out of the word
model, and the more successfully these words are
indeed learned by the morph model.
</bodyText>
<subsectionHeader confidence="0.936152">
3.2.3 New Words in the Test Set
</subsectionHeader>
<bodyText confidence="0.999957777777778">
All words present in the training data (some of
which are OOVs in the word models) “leave some
trace” in the morph models, in the n-gram statistics
that are collected for morph sequences. How, then,
about new words that occur only in the test set, but
not in the training set? In order to recognize such
words correctly, the model must combine morphs in
ways it has not observed before.
Figure 4 demonstrates that the new unseen words
are very challenging. Now, also the morph mod-
els mostly obtain negative word accuracies, which
means that the number of insertions adjacent to new
words exceeds the number of correctly recognized
new words. The best results are obtained in clean
acoustic conditions (Fin2, Fin3, Fin4) with only few
foreign names, which are difficult to get right using
typical Finnish phoneme-to-grapheme mappings (as
the negative accuracy of Fin1 suggests).
</bodyText>
<subsectionHeader confidence="0.999947">
3.3 Vocabulary Growth and Arabic
</subsectionHeader>
<bodyText confidence="0.999968615384615">
Figure 5 shows the development of the size of
the vocabulary (unique word forms) for growing
amounts of text in different corpora. The corpora
used for Finnish, Estonian, and Turkish (planned
speech/text), as well as Arabic (spontaneous speech)
are the LM training sets used in the experiments.
Additional sources have been provided for Arabic
and English: Arabic text (planned) from the FBIS
corpus of Modern Standard Arabic (a collection
of transcribed radio newscasts from various radio
stations in the Arabic speaking world), as well as
text from the New York Times magazine (English
planned) and spontaneous transcribed English tele-
phone conversations from the Fisher corpus.
The figure illustrates two points: (1) The faster
the vocabulary growth is, the larger the potential ad-
vantage of morph models is in comparison to stan-
dard word models, because of OOV and data spar-
sity problems. The obtained speech recognition re-
sults seem to support this hypothesis; the applied
morph LMs are clearly beneficial for Finnish and
Estonian, mostly beneficial for Turkish, and slightly
detrimental for ECA. (2) A more slowly growing
vocabulary is used in spontaneous speech than in
planned speech (or written text). Moreover, the
Arabic ‘spontaneous’ curve is located fairly close
</bodyText>
<page confidence="0.995805">
385
</page>
<figure confidence="0.999214519230769">
100
80
60
40
20
0
Word accuracy for unseen words [%]
−20
−40
−60
−80
92.8
89.8
50
82.2
76.6 79.1
71.9
68.667.466.7
61.2
67.8
45
66.7
51.9
Unique words [1000 words]
40
40.141.8
31.0
35
20.7
17.9
30
25
−6.1
−5.9
−8.7
−10.1−14.6
−24.6
−24.5
20
−32.3
15
−64.6
10
Morphs
Words
−75.9
−81.0
5
−93.0
−100
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
00 20 40 60 80 100 120 140 160 180
</figure>
<figureCaption confidence="0.995731">
Figure 4: Word accuracies computed for the words
</figureCaption>
<bodyText confidence="0.9678274">
in the test sets that do not occur at all in the train-
ing sets; cf. figures listed on the row “new words
in test set” in Table 1. For comparison, the gray-
shaded bars show the corresponding results for the
entire test sets (also displayed in Figure 2).
</bodyText>
<figure confidence="0.557975">
Corpus size [1000 words]
</figure>
<figureCaption confidence="0.9037772">
Figure 5: Vocabulary growth curves for the differ-
ent corpora of spontaneous and planned speech (or
written text). For growing amounts of text (word
tokens) the number of unique different word forms
(word types) occurring in the corpus are plotted.
</figureCaption>
<bodyText confidence="0.999797484848485">
to the English ‘planned’ curve and much below
the Finnish, Estonian, and Turkish curves. Thus,
even though Arabic is considered a “morphologi-
cally rich” language, this is not manifested through
a considerable vocabulary growth (and high OOV
rate) in the Egyptian Colloquial Arabic data used in
the current speech recognition experiments. Conse-
quently, it may not be that surprising that the morph
model did not work particularly well for Arabic.
Arabic words consist of a stem surrounded by pre-
fixes and suffixes, which are fairly successfully seg-
mented out by Morfessor. However, Arabic also
has templatic morphology, i.e., the stem is formed
through the insertion of a vowel pattern into a “con-
sonantal skeleton”.
Additional experiments have been performed us-
ing the ECA data and Factored Language Models
(FLMs) (Kirchhoff et al., 2006). The FLM is a
powerful model that makes use of several sources
of information, in particular a morphological lexi-
con of ECA. The FLM incorporates mechanisms for
handling templatic morphology, but despite its so-
phistication, it barely outperforms the standard word
model: The word accuracy of the FLM is 42.3 % and
that of the word model is 41.8 %. The speech recog-
nition implementation of both the FLM and the word
model is based on whole words (although subword
units are used for assigning probabilities to word
forms in the FLM). This contrasts these models with
the morph model, which splits words into subword
units also in the speech recognition implementation.
It seems that the splitting is a source of errors in this
experimental setup with very little data available.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999615764705883">
Alternative morph-based and word-based ap-
proaches exist. We have tried some, but none of
them has outperformed the described morph models
for Finnish, Estonian, and Turkish, or the word and
FLM models for Egyptian Arabic (in a statistically
significant way). The tested models comprise
more linguistically accurate morph segmentations
obtained using later Morfessor versions (Categories-
ML and Categories-MAP) (Creutz, 2006), as well
as analyses obtained from morphological parsers.
Hybrids, i.e., word models augmented with
phonemes or other subword units have been pro-
posed (Bazzi and Glass, 2000; Galescu, 2003;
Bisani and Ney, 2005). In our experiments, such
models have outperformed the standard word mod-
els, but not the morph models.
Simply growing the word vocabulary to cover the
</bodyText>
<page confidence="0.996366">
386
</page>
<bodyText confidence="0.999959294117647">
entire vocabulary of large training corpora could be
one (fairly “brute-force”) approach, but this is hardly
feasible for languages such as Finnish. The en-
tire Finnish LM training data of 150 million words
(used in Fin4) contains more than 4 million unique
word forms, a value ten times the size of the rather
large word lexicon currently used. And even if a 4-
million-word lexicon were to be used, the OOV rate
of the test set would still be relatively high: 1.5 %.
Judging by the Arabic experiments, there seems
to be some potential in Factored Language Models.
The FLMs might work well also for the other lan-
guages, and in fact, to do justice to the more ad-
vanced morph models from later versions of Mor-
fessor, FLMs or some other refined techniques may
be necessary as a complement to the currently used
standard n-grams.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999936142857143">
We are most grateful to Katrin Kirchhoff and Dimitra Vergyri
for their valuable help on issues related to Arabic, and to the EU
AMI training program for funding part of this work. The work
was also partly funded by DARPA under contract No. HR0011-
06-C-0023 (approved for public release, distribution is unlim-
ited). The views herein are those of the authors and do not nec-
essarily reflect the views of the funding agencies.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923043478261">
I. Bazzi and J. R. Glass. 2000. Modeling out-of-vocabulary
words for robust speech recognition. In Proc. ICSLP, Bei-
jing, China.
A. Berton, P. Fetter, and P. Regel-Brietzmann. 1996. Com-
pound words in large-vocabulary German speech recognition
systems. In Proc. ICSLP, pp. 1165–1168, Philadelphia, PA,
USA.
M. Bisani and H. Ney. 2005. Open vocabulary speech recog-
nition with flat hybrid models. In Proc. Interspeech, Lisbon,
Portugal.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34:71–105.
W. Byrne, J. Hajiˇc, P. Ircing, F. Jelinek, S. Khudanpur, P. Kr-
bec, and J. Psutka. 2001. On large vocabulary continuous
speech recognition of highly inflectional language — Czech.
In Proc. Eurospeech, pp. 487–489, Aalborg, Denmark.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Computer
Speech and Language, 13:359–394.
M. Creutz and K. Lagus. 2002. Unsupervised discovery of
morphemes. In Proc. ACL SIGPHON, pp. 21–30, Philadel-
phia, PA, USA.
M. Creutz. 2006. Induction of the Morphology of Natural
Language: Unsupervised Morpheme Segmentation with Ap-
plication to Automatic Speech Recognition. Ph.D. thesis,
Helsinki University of Technology. http://lib.tkk.fi/
Diss/2006/isbn9512282119/.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, MIT.
L. Galescu. 2003. Recognition of out-of-vocabulary words
with sub-lexical language models. In Proc. Eurospeech, pp.
249–252, Geneva, Switzerland.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adaptive vocabu-
laries for transcribing multilingual broadcast news. In Proc.
ICASSP, pp. 925–928, Seattle, WA, USA.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguistics,
27(2):153–198.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. Coling/ACL, pp. 673–680, Sydney, Australia.
T. Hirsim¨aki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, and
J. Pylkk¨onen. 2006. Unlimited vocabulary speech recogni-
tion with morph language models applied to Finnish. Com-
puter Speech and Language, 20(4):515–541.
K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-
cke. 2006. Morphology-based language modeling for Ara-
bic speech recognition. Computer Speech and Language,
20(4):589–608.
M. Kurimo, M. Creutz, M. Varjokallio, E. Arısoy, and
M. Sarac¸lar. 2006a. Unsupervised segmentation of words
into morphemes – Morpho Challenge 2005, Application to
automatic speech recognition. In Proc. Interspeech, Pitts-
burgh, PA, USA.
M. Kurimo, A. Puurula, E. Arısoy, V. Siivola, T. Hirsim¨aki,
J. Pylkk¨onen, T. Alum¨ae, and M. Sarac¸lar. 2006b. Un-
limited vocabulary speech recognition for agglutinative lan-
guages. In Proc. NAACL-HLT, New York, USA.
O.-W. Kwon and J. Park. 2003. Korean large vocabulary con-
tinuous speech recognition with morpheme-based recogni-
tion units. Speech Communication, 39(3–4):287–300.
M. Larson, D. Willett, J. Koehler, and G. Rigoll. 2000. Com-
pound splitting and lexical unit recombination for improved
performance of a speech recognition system for German par-
liamentary speeches. In Proc. ICSLP.
M. Mohri and M. D. Riley. 2002. DCD library, Speech
recognition decoder library. AT&amp;T Labs Research. http:
//www.research.att.com/sw/tools/dcd/.
R. Ordelman, A. van Hessen, and F. de Jong. 2003. Compound
decomposition in Dutch large vocabulary speech recogni-
tion. In Proc. Eurospeech, pp. 225–228, Geneva, Switzer-
land.
J. Rissanen. 1989. Stochastic complexity in statistical inquiry.
World Scientific Series in Computer Science, 15:79–93.
I. Shafran and K. Hall. 2006. Corrective models for speech
recognition of inflected languages. In Proc. EMNLP, Syd-
ney, Australia.
V. Siivola and B. Pellom. 2005. Growing an n-gram model. In
Proc. Interspeech, pp. 1309–1312, Lisbon, Portugal.
A. Stolcke. 1998. Entropy-based pruning of backoff language
models. In Proc. DARPA BNTU Workshop, pp. 270–274,
Lansdowne, VA, USA.
A. Stolcke. 2002. SRILM – an extensible language modeling
toolkit. In Proc. ICSLP, pp. 901–904. http://www.speech.
sri.com/projects/srilm/.
E. W. D. Whittaker and P. C. Woodland. 2000. Particle-based
language modelling. In Proc. ICSLP, pp. 170–173, Beijing,
China.
S. Young, D. Ollason, V. Valtchev, and P. Woodland. 2002.
The HTK book (for version 3.2 ofHTK). University of Cam-
bridge.
</reference>
<page confidence="0.998348">
387
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720308">
<title confidence="0.997941">Analysis of Morph-Based Speech Recognition and the Modeling Out-of-Vocabulary Words Across Languages</title>
<author confidence="0.906473">Teemu Mikko Antti Janne Matti Ebru Murat</author>
<affiliation confidence="0.989904">University of Technology, University, International/ International Computer Science Institute,</affiliation>
<abstract confidence="0.99429747368421">We analyze subword-based language models (LMs) in large-vocabulary continuous speech recognition across four “morphologically rich” languages: Finnish, Estonian, Turkish, and Egyptian Arabic. By estimating over sequences of of words, better vocabulary coverage and reduced data sparsity is obtained. Standard word LMs suffer from high out-of-vocabulary (OOV) rates, whereas the morph LMs can recognize previously unseen word forms by concatenating morphs. We show that the morph LMs generally outperform the word LMs and that they perform fairly well on OOVs without compromising the accuracy obtained for in-vocabulary words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Bazzi</author>
<author>J R Glass</author>
</authors>
<title>Modeling out-of-vocabulary words for robust speech recognition.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="26306" citStr="Bazzi and Glass, 2000" startWordPosition="4272" endWordPosition="4275">available. 4 Discussion Alternative morph-based and word-based approaches exist. We have tried some, but none of them has outperformed the described morph models for Finnish, Estonian, and Turkish, or the word and FLM models for Egyptian Arabic (in a statistically significant way). The tested models comprise more linguistically accurate morph segmentations obtained using later Morfessor versions (CategoriesML and Categories-MAP) (Creutz, 2006), as well as analyses obtained from morphological parsers. Hybrids, i.e., word models augmented with phonemes or other subword units have been proposed (Bazzi and Glass, 2000; Galescu, 2003; Bisani and Ney, 2005). In our experiments, such models have outperformed the standard word models, but not the morph models. Simply growing the word vocabulary to cover the 386 entire vocabulary of large training corpora could be one (fairly “brute-force”) approach, but this is hardly feasible for languages such as Finnish. The entire Finnish LM training data of 150 million words (used in Fin4) contains more than 4 million unique word forms, a value ten times the size of the rather large word lexicon currently used. And even if a 4- million-word lexicon were to be used, the OO</context>
</contexts>
<marker>Bazzi, Glass, 2000</marker>
<rawString>I. Bazzi and J. R. Glass. 2000. Modeling out-of-vocabulary words for robust speech recognition. In Proc. ICSLP, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berton</author>
<author>P Fetter</author>
<author>P Regel-Brietzmann</author>
</authors>
<title>Compound words in large-vocabulary German speech recognition systems.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>1165--1168</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="3092" citStr="Berton et al. (1996)" startWordPosition="425" endWordPosition="428">milies. LMs incorporating morphological knowledge about these languages can be applied. A further challenging category comprises languages that are both highly inflecting and compounding, such as the Finno-Ugric languages Finnish and Estonian. Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models. However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL H</context>
</contexts>
<marker>Berton, Fetter, Regel-Brietzmann, 1996</marker>
<rawString>A. Berton, P. Fetter, and P. Regel-Brietzmann. 1996. Compound words in large-vocabulary German speech recognition systems. In Proc. ICSLP, pp. 1165–1168, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Open vocabulary speech recognition with flat hybrid models.</title>
<date>2005</date>
<booktitle>In Proc. Interspeech,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="26344" citStr="Bisani and Ney, 2005" startWordPosition="4278" endWordPosition="4281">rph-based and word-based approaches exist. We have tried some, but none of them has outperformed the described morph models for Finnish, Estonian, and Turkish, or the word and FLM models for Egyptian Arabic (in a statistically significant way). The tested models comprise more linguistically accurate morph segmentations obtained using later Morfessor versions (CategoriesML and Categories-MAP) (Creutz, 2006), as well as analyses obtained from morphological parsers. Hybrids, i.e., word models augmented with phonemes or other subword units have been proposed (Bazzi and Glass, 2000; Galescu, 2003; Bisani and Ney, 2005). In our experiments, such models have outperformed the standard word models, but not the morph models. Simply growing the word vocabulary to cover the 386 entire vocabulary of large training corpora could be one (fairly “brute-force”) approach, but this is hardly feasible for languages such as Finnish. The entire Finnish LM training data of 150 million words (used in Fin4) contains more than 4 million unique word forms, a value ten times the size of the rather large word lexicon currently used. And even if a 4- million-word lexicon were to be used, the OOV rate of the test set would still be </context>
</contexts>
<marker>Bisani, Ney, 2005</marker>
<rawString>M. Bisani and H. Ney. 2005. Open vocabulary speech recognition with flat hybrid models. In Proc. Interspeech, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="5050" citStr="Brent (1999)" startWordPosition="735" endWordPosition="736">n of words into morpheme-like units. The general idea is to discover as compact a description of the input text corpus as possible. Substrings occurring frequently enough in several different word forms are proposed as morphs, and the words in the corpus are then represented as a concatenation of morphs, e.g., ‘hand, hand+s, left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Byrne</author>
<author>J Hajiˇc</author>
<author>P Ircing</author>
<author>F Jelinek</author>
<author>S Khudanpur</author>
<author>P Krbec</author>
<author>J Psutka</author>
</authors>
<title>On large vocabulary continuous speech recognition of highly inflectional language — Czech.</title>
<date>2001</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>487--489</pages>
<location>Aalborg, Denmark.</location>
<marker>Byrne, Hajiˇc, Ircing, Jelinek, Khudanpur, Krbec, Psutka, 2001</marker>
<rawString>W. Byrne, J. Hajiˇc, P. Ircing, F. Jelinek, S. Khudanpur, P. Krbec, and J. Psutka. 2001. On large vocabulary continuous speech recognition of highly inflectional language — Czech. In Proc. Eurospeech, pp. 487–489, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language,</title>
<date>1999</date>
<pages>13--359</pages>
<contexts>
<context position="11171" citStr="Chen and Goodman, 1999" startWordPosition="1698" endWordPosition="1702">contrast, the Arabic data consists of transcribed spontaneous telephone conversations,2 which are characterized by disfluencies and by the presence of “non-speech”, such as laugh and cough sounds. There are multiple speakers in the Arabic data, and online speaker adaptation has been performed. 3.1.2 Text Data and Language Models The n-gram language models are trained using the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2, Tur1, Tur2, ECA) or similar software developed at HUT (Siivola and Pellom, 2005) (Fin3, Fin4, Est). All models utilize the Modified Interpolated Kneser-Ney smoothing technique (Chen and Goodman, 1999). The Arabic LM is trained on the same corpus that is used for acoustic training. This data set is regrettably small (160 000 words), but it matches the test set well in style, as it consists of transcribed spontaneous speech. The LM training corpora used for the other languages contain fairly large amounts of mainly news and book texts and conceivably match the style of the test data well. In the morph-based models, words are split into morphs using Morfessor, and statistics are collected for morph n-grams. As the desired output of the 2LDC CallHome corpus of Egyptian Colloquial Arabic: http:</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>S. F. Chen and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>K Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proc. ACL SIGPHON,</booktitle>
<pages>21--30</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5093" citStr="Creutz and Lagus (2002)" startWordPosition="739" endWordPosition="742">its. The general idea is to discover as compact a description of the input text corpus as possible. Substrings occurring frequently enough in several different word forms are proposed as morphs, and the words in the corpus are then represented as a concatenation of morphs, e.g., ‘hand, hand+s, left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is the original, so-called M</context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>M. Creutz and K. Lagus. 2002. Unsupervised discovery of morphemes. In Proc. ACL SIGPHON, pp. 21–30, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
</authors>
<title>Induction of the Morphology of Natural Language: Unsupervised Morpheme Segmentation with Application to Automatic Speech Recognition.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Helsinki University of Technology.</institution>
<note>http://lib.tkk.fi/ Diss/2006/isbn9512282119/.</note>
<contexts>
<context position="5112" citStr="Creutz (2006)" startWordPosition="744" endWordPosition="745">iscover as compact a description of the input text corpus as possible. Substrings occurring frequently enough in several different word forms are proposed as morphs, and the words in the corpus are then represented as a concatenation of morphs, e.g., ‘hand, hand+s, left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is the original, so-called Morfessor Baseline a</context>
<context position="26132" citStr="Creutz, 2006" startWordPosition="4247" endWordPosition="4248"> into subword units also in the speech recognition implementation. It seems that the splitting is a source of errors in this experimental setup with very little data available. 4 Discussion Alternative morph-based and word-based approaches exist. We have tried some, but none of them has outperformed the described morph models for Finnish, Estonian, and Turkish, or the word and FLM models for Egyptian Arabic (in a statistically significant way). The tested models comprise more linguistically accurate morph segmentations obtained using later Morfessor versions (CategoriesML and Categories-MAP) (Creutz, 2006), as well as analyses obtained from morphological parsers. Hybrids, i.e., word models augmented with phonemes or other subword units have been proposed (Bazzi and Glass, 2000; Galescu, 2003; Bisani and Ney, 2005). In our experiments, such models have outperformed the standard word models, but not the morph models. Simply growing the word vocabulary to cover the 386 entire vocabulary of large training corpora could be one (fairly “brute-force”) approach, but this is hardly feasible for languages such as Finnish. The entire Finnish LM training data of 150 million words (used in Fin4) contains mo</context>
</contexts>
<marker>Creutz, 2006</marker>
<rawString>M. Creutz. 2006. Induction of the Morphology of Natural Language: Unsupervised Morpheme Segmentation with Application to Automatic Speech Recognition. Ph.D. thesis, Helsinki University of Technology. http://lib.tkk.fi/ Diss/2006/isbn9512282119/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<marker>de Marcken, 1996</marker>
<rawString>C. G. de Marcken. 1996. Unsupervised Language Acquisition. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Galescu</author>
</authors>
<title>Recognition of out-of-vocabulary words with sub-lexical language models.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>249--252</pages>
<location>Geneva,</location>
<contexts>
<context position="26321" citStr="Galescu, 2003" startWordPosition="4276" endWordPosition="4277"> Alternative morph-based and word-based approaches exist. We have tried some, but none of them has outperformed the described morph models for Finnish, Estonian, and Turkish, or the word and FLM models for Egyptian Arabic (in a statistically significant way). The tested models comprise more linguistically accurate morph segmentations obtained using later Morfessor versions (CategoriesML and Categories-MAP) (Creutz, 2006), as well as analyses obtained from morphological parsers. Hybrids, i.e., word models augmented with phonemes or other subword units have been proposed (Bazzi and Glass, 2000; Galescu, 2003; Bisani and Ney, 2005). In our experiments, such models have outperformed the standard word models, but not the morph models. Simply growing the word vocabulary to cover the 386 entire vocabulary of large training corpora could be one (fairly “brute-force”) approach, but this is hardly feasible for languages such as Finnish. The entire Finnish LM training data of 150 million words (used in Fin4) contains more than 4 million unique word forms, a value ten times the size of the rather large word lexicon currently used. And even if a 4- million-word lexicon were to be used, the OOV rate of the t</context>
</contexts>
<marker>Galescu, 2003</marker>
<rawString>L. Galescu. 2003. Recognition of out-of-vocabulary words with sub-lexical language models. In Proc. Eurospeech, pp. 249–252, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Geutner</author>
<author>M Finke</author>
<author>P Scheytt</author>
</authors>
<title>Adaptive vocabularies for transcribing multilingual broadcast news.</title>
<date>1998</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>925--928</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="3494" citStr="Geutner et al., 1998" startWordPosition="493" endWordPosition="496">ning considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL HLT 2007, pages 380–387, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics lyzed: Finnish, Estonian, Turkish, and the dialect of Arabic spoken in Egypt, Egyptian Colloquial Arabic (ECA). All these languages are considered “morphologically rich”, but the benefits of using subword-based LMs differ across languages. We attempt to discover explanations for these differences. In </context>
</contexts>
<marker>Geutner, Finke, Scheytt, 1998</marker>
<rawString>P. Geutner, M. Finke, and P. Scheytt. 1998. Adaptive vocabularies for transcribing multilingual broadcast news. In Proc. ICASSP, pp. 925–928, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="5068" citStr="Goldsmith (2001)" startWordPosition="737" endWordPosition="738">o morpheme-like units. The general idea is to discover as compact a description of the input text corpus as possible. Substrings occurring frequently enough in several different word forms are proposed as morphs, and the words in the corpus are then represented as a concatenation of morphs, e.g., ‘hand, hand+s, left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is </context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proc. Coling/ACL,</booktitle>
<pages>673--680</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5416" citStr="Goldwater et al. (2006)" startWordPosition="784" endWordPosition="787">ough maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is the original, so-called Morfessor Baseline algorithm, which is publicly available for download.1. The mathematics of the Morfessor Baseline model is briefly outlined in the following; consult Creutz (2006) for details. 1http://www.cis.hut.fi/projects/morpho/ 2.1 MAP Optimization Criterion In slightly simplified form, the optimization criterion ut</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proc. Coling/ACL, pp. 673–680, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirsim¨aki</author>
<author>M Creutz</author>
<author>V Siivola</author>
<author>M Kurimo</author>
<author>S Virpioja</author>
<author>J Pylkk¨onen</author>
</authors>
<title>Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language,</title>
<date>2006</date>
<marker>Hirsim¨aki, Creutz, Siivola, Kurimo, Virpioja, Pylkk¨onen, 2006</marker>
<rawString>T. Hirsim¨aki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, and J. Pylkk¨onen. 2006. Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language, 20(4):515–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>D Vergyri</author>
<author>J Bilmes</author>
<author>K Duh</author>
<author>A Stolcke</author>
</authors>
<title>Morphology-based language modeling for Arabic speech recognition.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="3141" citStr="Kirchhoff et al. (2006)" startWordPosition="434" endWordPosition="437">dge about these languages can be applied. A further challenging category comprises languages that are both highly inflecting and compounding, such as the Finno-Ugric languages Finnish and Estonian. Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models. However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL HLT 2007, pages 380–387, Rochester, NY, April 2007</context>
<context position="24917" citStr="Kirchhoff et al., 2006" startWordPosition="4052" endWordPosition="4055">lary growth (and high OOV rate) in the Egyptian Colloquial Arabic data used in the current speech recognition experiments. Consequently, it may not be that surprising that the morph model did not work particularly well for Arabic. Arabic words consist of a stem surrounded by prefixes and suffixes, which are fairly successfully segmented out by Morfessor. However, Arabic also has templatic morphology, i.e., the stem is formed through the insertion of a vowel pattern into a “consonantal skeleton”. Additional experiments have been performed using the ECA data and Factored Language Models (FLMs) (Kirchhoff et al., 2006). The FLM is a powerful model that makes use of several sources of information, in particular a morphological lexicon of ECA. The FLM incorporates mechanisms for handling templatic morphology, but despite its sophistication, it barely outperforms the standard word model: The word accuracy of the FLM is 42.3 % and that of the word model is 41.8 %. The speech recognition implementation of both the FLM and the word model is based on whole words (although subword units are used for assigning probabilities to word forms in the FLM). This contrasts these models with the morph model, which splits wor</context>
</contexts>
<marker>Kirchhoff, Vergyri, Bilmes, Duh, Stolcke, 2006</marker>
<rawString>K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stolcke. 2006. Morphology-based language modeling for Arabic speech recognition. Computer Speech and Language, 20(4):589–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kurimo</author>
<author>M Creutz</author>
<author>M Varjokallio</author>
<author>E Arısoy</author>
<author>M Sarac¸lar</author>
</authors>
<title>Unsupervised segmentation of words into morphemes – Morpho Challenge</title>
<date>2006</date>
<booktitle>In Proc. Interspeech,</booktitle>
<location>Pittsburgh, PA, USA.</location>
<marker>Kurimo, Creutz, Varjokallio, Arısoy, Sarac¸lar, 2006</marker>
<rawString>M. Kurimo, M. Creutz, M. Varjokallio, E. Arısoy, and M. Sarac¸lar. 2006a. Unsupervised segmentation of words into morphemes – Morpho Challenge 2005, Application to automatic speech recognition. In Proc. Interspeech, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kurimo</author>
<author>A Puurula</author>
<author>E Arısoy</author>
<author>V Siivola</author>
<author>T Hirsim¨aki</author>
<author>J Pylkk¨onen</author>
<author>T Alum¨ae</author>
<author>M Sarac¸lar</author>
</authors>
<title>Unlimited vocabulary speech recognition for agglutinative languages. In</title>
<date>2006</date>
<booktitle>Proc. NAACL-HLT,</booktitle>
<location>New York, USA.</location>
<marker>Kurimo, Puurula, Arısoy, Siivola, Hirsim¨aki, Pylkk¨onen, Alum¨ae, Sarac¸lar, 2006</marker>
<rawString>M. Kurimo, A. Puurula, E. Arısoy, V. Siivola, T. Hirsim¨aki, J. Pylkk¨onen, T. Alum¨ae, and M. Sarac¸lar. 2006b. Unlimited vocabulary speech recognition for agglutinative languages. In Proc. NAACL-HLT, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O-W Kwon</author>
<author>J Park</author>
</authors>
<title>Korean large vocabulary continuous speech recognition with morpheme-based recognition units.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>39--3</pages>
<contexts>
<context position="3194" citStr="Kwon and Park (2003)" startWordPosition="443" endWordPosition="446">lenging category comprises languages that are both highly inflecting and compounding, such as the Finno-Ugric languages Finnish and Estonian. Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models. However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL HLT 2007, pages 380–387, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ly</context>
</contexts>
<marker>Kwon, Park, 2003</marker>
<rawString>O.-W. Kwon and J. Park. 2003. Korean large vocabulary continuous speech recognition with morpheme-based recognition units. Speech Communication, 39(3–4):287–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Larson</author>
<author>D Willett</author>
<author>J Koehler</author>
<author>G Rigoll</author>
</authors>
<title>Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="3333" citStr="Larson et al. (2000)" startWordPosition="468" endWordPosition="471">n. Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models. However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL HLT 2007, pages 380–387, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics lyzed: Finnish, Estonian, Turkish, and the dialect of Arabic spoken in Egypt, Egyptian Colloquial Arabic (ECA). All these languages are consi</context>
</contexts>
<marker>Larson, Willett, Koehler, Rigoll, 2000</marker>
<rawString>M. Larson, D. Willett, J. Koehler, and G. Rigoll. 2000. Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>M D Riley</author>
</authors>
<date>2002</date>
<booktitle>DCD library, Speech recognition decoder library. AT&amp;T Labs Research. http:</booktitle>
<pages>//www.research.att.com/sw/tools/dcd/.</pages>
<contexts>
<context position="9249" citStr="Mohri and Riley, 2002" startWordPosition="1393" endWordPosition="1396"> Finnish, Estonian, and Turkish test configurations are slight variations of experiments reported earlier in Hirsim¨aki et al. (2006) (Fin1: ‘News task’, Fin2: ‘Book task’), Kurimo et al. (2006a) (Fin3, Tur1), and Kurimo et al. (2006b) (Fin4, Est, Tur2). Three different recognition platforms have been used, all of which are state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems. The Finnish and Estonian experiments have been run on the HUT speech recognition system developed at Helsinki University of Technology. The Turkish tests were performed using the AT&amp;T decoder (Mohri and Riley, 2002); the acoustic features were produced using the HTK front end (Young et al., 2002). The experiments on Egyptian Colloquial Arabic (ECA) were carried out using the SRI DecipherTM speech recognition system. 3.1.1 Speech Data and Acoustic Models The type and amount of speech data vary from one language to another. The Finnish data consists of news broadcasts read by one single female speaker (Fin1), as well as an audio book read by another female speaker (Fin2, Fin3, Fin4). The Finnish acoustic models are speaker dependent (SD). Monophones (mon) were used in the earlier experiments (Fin1, Fin2), </context>
</contexts>
<marker>Mohri, Riley, 2002</marker>
<rawString>M. Mohri and M. D. Riley. 2002. DCD library, Speech recognition decoder library. AT&amp;T Labs Research. http: //www.research.att.com/sw/tools/dcd/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ordelman</author>
<author>A van Hessen</author>
<author>F de Jong</author>
</authors>
<title>Compound decomposition in Dutch large vocabulary speech recognition.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>225--228</pages>
<location>Geneva,</location>
<marker>Ordelman, van Hessen, de Jong, 2003</marker>
<rawString>R. Ordelman, A. van Hessen, and F. de Jong. 2003. Compound decomposition in Dutch large vocabulary speech recognition. In Proc. Eurospeech, pp. 225–228, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic complexity in statistical inquiry.</title>
<date>1989</date>
<booktitle>World Scientific Series in Computer Science,</booktitle>
<pages>15--79</pages>
<contexts>
<context position="5380" citStr="Rissanen, 1989" startWordPosition="781" endWordPosition="782">left+hand+ed, hand+ful’. Through maximum a posteriori optimization (MAP), an optimal balance is sought between the compactness of the inventory of morphs, i.e., the morph lexicon, versus the compactness of the representation of the corpus. Among others, de Marcken (1996), Brent (1999), Goldsmith (2001), Creutz and Lagus (2002), and Creutz (2006) have shown that models based on the above approach produce segmentations that resemble linguistic morpheme segmentations, when formulated mathematically in a probabilistic framework or equivalently using the Minimum Description Length (MDL) principle (Rissanen, 1989). Similarly, Goldwater et al. (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. The Morfessor model has been developed over the years, and different model versions exist. The model used in the speech recognition experiments of the current paper is the original, so-called Morfessor Baseline algorithm, which is publicly available for download.1. The mathematics of the Morfessor Baseline model is briefly outlined in the following; consult Creutz (2006) for details. 1http://www.cis.hut.fi/projects/morpho/ 2.1 MAP Optimization Criterion In slightly simplified</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>J. Rissanen. 1989. Stochastic complexity in statistical inquiry. World Scientific Series in Computer Science, 15:79–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Shafran</author>
<author>K Hall</author>
</authors>
<title>Corrective models for speech recognition of inflected languages.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3223" citStr="Shafran and Hall (2006)" startWordPosition="448" endWordPosition="451"> languages that are both highly inflecting and compounding, such as the Finno-Ugric languages Finnish and Estonian. Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models. However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL HLT 2007, pages 380–387, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics lyzed: Finnish, Estonian, Turki</context>
</contexts>
<marker>Shafran, Hall, 2006</marker>
<rawString>I. Shafran and K. Hall. 2006. Corrective models for speech recognition of inflected languages. In Proc. EMNLP, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Siivola</author>
<author>B Pellom</author>
</authors>
<title>Growing an n-gram model.</title>
<date>2005</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>1309--1312</pages>
<location>Lisbon,</location>
<contexts>
<context position="11051" citStr="Siivola and Pellom, 2005" startWordPosition="1682" endWordPosition="1685">s acoustic models. The Finnish, Estonian, and Turkish data sets contain planned speech, i.e., written text read aloud. By contrast, the Arabic data consists of transcribed spontaneous telephone conversations,2 which are characterized by disfluencies and by the presence of “non-speech”, such as laugh and cough sounds. There are multiple speakers in the Arabic data, and online speaker adaptation has been performed. 3.1.2 Text Data and Language Models The n-gram language models are trained using the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2, Tur1, Tur2, ECA) or similar software developed at HUT (Siivola and Pellom, 2005) (Fin3, Fin4, Est). All models utilize the Modified Interpolated Kneser-Ney smoothing technique (Chen and Goodman, 1999). The Arabic LM is trained on the same corpus that is used for acoustic training. This data set is regrettably small (160 000 words), but it matches the test set well in style, as it consists of transcribed spontaneous speech. The LM training corpora used for the other languages contain fairly large amounts of mainly news and book texts and conceivably match the style of the test data well. In the morph-based models, words are split into morphs using Morfessor, and statistics</context>
<context position="13895" citStr="Siivola and Pellom, 2005" startWordPosition="2185" endWordPosition="2188">n is replaced by a special out-of-vocabulary symbol. As words and morphs are units of different length, their optimal performance may occur at different orders of the n-gram. The best order of the n-gram has been optimized on development test sets in the following cases: Fin1, Fin2, Tur1, ECA (4-grams for both morphs and words) and Tur2 (5-grams for morphs, 3-grams for words). The models have additionally been pruned using entropy-based pruning (Tur1, Tur2, ECA) (Stolcke, 1998). In the other experiments (Fin3, Fin4, Est), no fixed maximum value of n was selected. n-Gram growing was performed (Siivola and Pellom, 2005), such that those n-grams that maximize the training set likelihood are gradually added to the model. The unrestricted growth of the model is counterbalanced by an MDLtype complexity term. The highest order of n-grams accepted was 7 for Finnish and 8 for Estonian. Note that the optimization procedure is neutral with respect to morphs vs. words. Roughly the same number of parameters are allowed in the resulting LMs, but typically the morph n-gram LMs are smaller than the corresponding word n-gram LMs. 3.1.3 Out-of-Vocabulary Words Table 1 further shows statistics on out-ofvocabulary rates in th</context>
</contexts>
<marker>Siivola, Pellom, 2005</marker>
<rawString>V. Siivola and B. Pellom. 2005. Growing an n-gram model. In Proc. Interspeech, pp. 1309–1312, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA BNTU Workshop,</booktitle>
<pages>270--274</pages>
<location>Lansdowne, VA, USA.</location>
<contexts>
<context position="13752" citStr="Stolcke, 1998" startWordPosition="2163" endWordPosition="2164">ords are given priority; the actual lexicon sizes used in each experiment are shown in Table 1. Any word not contained in the lexicon is replaced by a special out-of-vocabulary symbol. As words and morphs are units of different length, their optimal performance may occur at different orders of the n-gram. The best order of the n-gram has been optimized on development test sets in the following cases: Fin1, Fin2, Tur1, ECA (4-grams for both morphs and words) and Tur2 (5-grams for morphs, 3-grams for words). The models have additionally been pruned using entropy-based pruning (Tur1, Tur2, ECA) (Stolcke, 1998). In the other experiments (Fin3, Fin4, Est), no fixed maximum value of n was selected. n-Gram growing was performed (Siivola and Pellom, 2005), such that those n-grams that maximize the training set likelihood are gradually added to the model. The unrestricted growth of the model is counterbalanced by an MDLtype complexity term. The highest order of n-grams accepted was 7 for Finnish and 8 for Estonian. Note that the optimization procedure is neutral with respect to morphs vs. words. Roughly the same number of parameters are allowed in the resulting LMs, but typically the morph n-gram LMs are</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>A. Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA BNTU Workshop, pp. 270–274, Lansdowne, VA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>901--904</pages>
<note>http://www.speech. sri.com/projects/srilm/.</note>
<contexts>
<context position="10957" citStr="Stolcke, 2002" startWordPosition="1669" endWordPosition="1670"> newspaper text read by one female speaker. Speakerindependent triphones are used as acoustic models. The Finnish, Estonian, and Turkish data sets contain planned speech, i.e., written text read aloud. By contrast, the Arabic data consists of transcribed spontaneous telephone conversations,2 which are characterized by disfluencies and by the presence of “non-speech”, such as laugh and cough sounds. There are multiple speakers in the Arabic data, and online speaker adaptation has been performed. 3.1.2 Text Data and Language Models The n-gram language models are trained using the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2, Tur1, Tur2, ECA) or similar software developed at HUT (Siivola and Pellom, 2005) (Fin3, Fin4, Est). All models utilize the Modified Interpolated Kneser-Ney smoothing technique (Chen and Goodman, 1999). The Arabic LM is trained on the same corpus that is used for acoustic training. This data set is regrettably small (160 000 words), but it matches the test set well in style, as it consists of transcribed spontaneous speech. The LM training corpora used for the other languages contain fairly large amounts of mainly news and book texts and conceivably match the style of the test dat</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. ICSLP, pp. 901–904. http://www.speech. sri.com/projects/srilm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>P C Woodland</author>
</authors>
<title>Particle-based language modelling.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>170--173</pages>
<location>Beijing,</location>
<contexts>
<context position="3172" citStr="Whittaker and Woodland (2000)" startWordPosition="438" endWordPosition="442"> can be applied. A further challenging category comprises languages that are both highly inflecting and compounding, such as the Finno-Ugric languages Finnish and Estonian. Morphology modeling aims to reduce the outof-vocabulary (OOV) rate as well as data sparsity, thereby producing more effective language models. However, obtaining considerable improvements in speech recognition accuracy seems hard, as is demonstrated by the fairly meager improvements (1–4 % relative) over standard word-based models accomplished by, e.g., Berton et al. (1996), Ordelman et al. (2003), Kirchhoff et al. (2006), Whittaker and Woodland (2000), Kwon and Park (2003), and Shafran and Hall (2006) for Dutch, Arabic, English, Korean, and Czech, or even the worse performance reported by Larson et al. (2000) for German and Byrne et al. (2001) for Czech. Nevertheless, clear improvements over a word baseline have been achieved for Serbo-Croatian (Geutner et al., 1998), Finnish, Estonian (Kurimo et al., 2006b) and Turkish (Kurimo et al., 2006a). In this paper, subword language models in the recognition of speech of four languages are ana380 Proceedings of NAACL HLT 2007, pages 380–387, Rochester, NY, April 2007. c�2007 Association for Comput</context>
</contexts>
<marker>Whittaker, Woodland, 2000</marker>
<rawString>E. W. D. Whittaker and P. C. Woodland. 2000. Particle-based language modelling. In Proc. ICSLP, pp. 170–173, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>D Ollason</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<title>The HTK book (for version 3.2 ofHTK).</title>
<date>2002</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="9331" citStr="Young et al., 2002" startWordPosition="1408" endWordPosition="1411">nts reported earlier in Hirsim¨aki et al. (2006) (Fin1: ‘News task’, Fin2: ‘Book task’), Kurimo et al. (2006a) (Fin3, Tur1), and Kurimo et al. (2006b) (Fin4, Est, Tur2). Three different recognition platforms have been used, all of which are state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems. The Finnish and Estonian experiments have been run on the HUT speech recognition system developed at Helsinki University of Technology. The Turkish tests were performed using the AT&amp;T decoder (Mohri and Riley, 2002); the acoustic features were produced using the HTK front end (Young et al., 2002). The experiments on Egyptian Colloquial Arabic (ECA) were carried out using the SRI DecipherTM speech recognition system. 3.1.1 Speech Data and Acoustic Models The type and amount of speech data vary from one language to another. The Finnish data consists of news broadcasts read by one single female speaker (Fin1), as well as an audio book read by another female speaker (Fin2, Fin3, Fin4). The Finnish acoustic models are speaker dependent (SD). Monophones (mon) were used in the earlier experiments (Fin1, Fin2), but these were later replaced by crosscontext triphones (tri). The Estonian speech</context>
</contexts>
<marker>Young, Ollason, Valtchev, Woodland, 2002</marker>
<rawString>S. Young, D. Ollason, V. Valtchev, and P. Woodland. 2002. The HTK book (for version 3.2 ofHTK). University of Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>