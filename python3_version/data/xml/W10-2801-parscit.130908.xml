<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010284">
<title confidence="0.966636">
Capturing Nonlinear Structure in Word Spaces through Dimensionality
Reduction
</title>
<author confidence="0.998343">
David Jurgens
</author>
<affiliation confidence="0.999504">
University of California, Los Angeles,
</affiliation>
<address confidence="0.807804">
4732 Boelter Hall
Los Angeles, CA 90095
</address>
<email confidence="0.999456">
jurgens@cs.ucla.edu
</email>
<author confidence="0.995354">
Keith Stevens
</author>
<affiliation confidence="0.99911">
University of California, Los Angeles,
</affiliation>
<address confidence="0.807782">
4732 Boelter Hall
Los Angeles, CA 90095
</address>
<email confidence="0.999548">
kstevens@cs.ucla.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999762076923077">
Dimensionality reduction has been shown
to improve processing and information ex-
traction from high dimensional data. Word
space algorithms typically employ lin-
ear reduction techniques that assume the
space is Euclidean. We investigate the ef-
fects of extracting nonlinear structure in
the word space using Locality Preserv-
ing Projections, a reduction algorithm that
performs manifold learning. We apply
this reduction to two common word space
models and show improved performance
over the original models on benchmarks.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998316875">
Vector space models of semantics frequently em-
ploy some form of dimensionality reduction for
improvement in representations or computational
overhead. Many of the dimensionality reduc-
tion algorithms assume that the unreduced word
space is linear. However, word similarities have
been shown to exhibit many non-metric proper-
ties: asymmetry, e.g North Korea is more sim-
ilar to Red China than Red China is to North
Korea, and non-transitivity, e.g. Cuba is similar
the former USSR, Jamaica is similar to Cuba,
but Jamaica is not similar to the USSR (Tversky,
1977). We hypothesize that a non-linear word
space model might more accurately preserve these
non-metric relationships.
To test our hypothesis, we capture the non-
linear structure with dimensionality reduction by
using Locality Preserving Projection (LPP) (He
and Niyogi, 2003), an efficient, linear approxi-
mation of Eigenmaps (Belkin and Niyogi, 2002).
With this reduction, the word space vectors are as-
sumed to exist on a nonlinear manifold that LPP
learns in order to project the vectors into a Eu-
clidean space. We measure the effects of us-
ing LPP on two basic word space models: the
Vector Space Model and a Word Co-occurrence
model. We begin with a brief overview of these
word spaces and common dimensionality reduc-
tion techniques. We then formally introduce LPP.
Following, we use two experiments to demonstrate
LPP’s capacity to accurately dimensionally reduce
word spaces.
</bodyText>
<sectionHeader confidence="0.86892" genericHeader="method">
2 Word Spaces and Reductions
</sectionHeader>
<bodyText confidence="0.999983625">
We consider two common word space models
that have been used with dimensionality reduc-
tion. The first is the Vector Space Model (VSM)
(Salton et al., 1975). Words are represented as
vectors where each dimension corresponds to a
document in the corpus and the dimension’s value
is the number of times the word occurred in the
document. We label the second model the Word
Co-occurrence (WC) model: each dimension cor-
respond to a unique word, with the dimension’s
value indicating the number of times that dimen-
sion’s word co-occurred.
Dimensionality reduction has been applied to
both models for three kinds of benefits: to im-
prove computational efficiency, to capture higher
order relationships between words, and to reduce
noise by smoothing or eliminating noisy features.
We consider three of the most popular reduction
techniques and the general word space models to
which they have been applied: linear projections,
feature elimination and random approximations.
The most frequently applied linear projection
technique is the Singular Value Decomposition
(SVD). The SVD factors a matrix A, which rep-
resents a word space, into three matrices UEV ⊤
such that E is a diagonal matrix containing the
singular values of A, ordered descending based on
their effect on the variance in the values of A. The
original matrix can be approximated by using only
the top k singular values, setting all others to 0.
The approximation matrix, A = UkEkVk⊤, is the
least squares best-fit rank-k approximation of A.
</bodyText>
<page confidence="0.798716">
1
</page>
<note confidence="0.6269725">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1–6,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99986508">
The SVD has been used with great success on
both models. Latent Semantic Analysis (LSA)
(Landauer et al., 1998) extends the (VSM) by de-
composing the space using the SVD and mak-
ing the word space the left singular vectors, Uk.
WC models have also utilized the SVD to improve
performance (Sch¨utze, 1992; Bullinaria and Levy,
2007; Baroni and Lenci, 2008).
Feature elimination reduces the dimensional-
ity by removing those with low information con-
tent. This approach has been successfully applied
to WC models such as HAL (Lund and Burgess,
1996) by dropping those with low entropy. This
technique effectively removes the feature dimen-
sions of high frequency words, which provide lit-
tle discriminatory content.
Randomized projections have also been suc-
cessfully applied to VSM models, e.g. (Kanerva
et al., 2000) and WC models, e.g. (Sahlgren et al.,
2008). This reduction statistically approximates
the original space in a much lower dimensional
space. The projection does not take into account
the structure of data, which provides only a com-
putational benefit from fewer dimensions, unlike
the previous two reductions.
</bodyText>
<sectionHeader confidence="0.969122" genericHeader="method">
3 Locality Preserving Projection
</sectionHeader>
<bodyText confidence="0.927618166666667">
For a set of vectors, x1, x2,. . . , xn E Rm, LPP
preserves the distance in the k-dimensional space,
where k « m, by solving the following minimiza-
tion problem,
�min (w⊤xi − w⊤xj)2Sij (1)
w
ij
where w is a transformation vector that projects x
into the lower dimensional space, and S is a ma-
trix that represents the local structure of the origi-
nal space. Minimizing this equation is equivalent
to finding the transformation vector that best pre-
serves the local distances in the original space ac-
cording to S. LPP assumes that the data points xi
exist on a manifold. This is in contrast to the SVD,
which assumes that the space is Euclidean and per-
forms a global, rather than local, minimization. In
treating the space as a manifold, LPP is able to dis-
cover some of the nonlinear structure of the data
from its local structure.
To solve the minimization problem in Equation
1, LPP uses a linear approximation of the Lapla-
cian Eigenmaps procedure (Belkin and Niyogi,
2002) as follows:
</bodyText>
<listItem confidence="0.9729438">
1. Let X be a matrix where xi is the ith row vec-
tor. Construct an adjacency matrix, S, which
represents the local structure of the original
vector space, by making an edge between
points xi and xj if xj is locally proximate to
xi. Two variations are available for determin-
ing proximity: either the k-nearest neighbors,
or all the data points with similarity &gt; ǫ.
2. Weight the edges in S proportional to the
closeness of the data points. Four main op-
tions are available: a Gaussian kernel, a poly-
nomial kernel, cosine similarity, or binary.
3. Construct the diagonal matrix D where entry
Dii = Ej Sij. Let L = D − S. Then solve
the generalized eigenvector problem:
</listItem>
<equation confidence="0.975036">
XLX⊤w = λXDX⊤w. (2)
</equation>
<bodyText confidence="0.996006">
He and Niyogi (2003) show that solving this
problem is equivalent to solving Equation 1.
</bodyText>
<listItem confidence="0.83358">
4. Let Wk = [w1, ... , wk] denote the matrix of
transformation vectors, sorted in descending
order according to their eigenvalues λ. The
original space is projected into k dimensions
by Wk⊤ X —* Xk.
</listItem>
<bodyText confidence="0.998885357142857">
For many applications of LPP, such as doc-
ument clustering (He et al., 2004), the original
data matrix X is transformed by first perform-
ing Principle Component Analysis and discarding
the smallest principle components, which requires
computing the full SVD. However, for large data
sets such as those frequently used in word space
algorithms, performing the full SVD is computa-
tionally infeasible.
To overcome this limitation, Cai et al. (2007a)
show how Spectral Regression may be used as
an alternative for solving the same minimization
equation through an iterative process. The princi-
ple idea is that Equation 2 may be recast as
</bodyText>
<equation confidence="0.846168">
Sy = λDy (3)
</equation>
<bodyText confidence="0.999984333333333">
where y = X⊤w, which ensures y will be an
eigenvector with the same eigenvalue for the prob-
lem in Equation 2. Finding the transformation
matrix Wk, used in step 4, is done in two steps.
First, Equation 3 is solved to produce eigenvectors
[y0, ... , yk], sorted in decreasing order according
to their eigenvalues λ. Second, the set of trans-
formation vectors composing Wk, [w1, . . . , wk],
is found by a least-squares regression:
</bodyText>
<equation confidence="0.867256666666667">
n
wj = argmin (w⊤xi − yji)2 + α||w||2 (4)
W i=1
</equation>
<page confidence="0.840638">
2
</page>
<bodyText confidence="0.9999324">
where yz denotes the value of the jth dimension
of yz. The α parameter penalizes solutions pro-
portionally to their magnitude, which Cai et al.
(2007b) note ensures the stability of w as an ap-
proximate eigenproblem solution.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999965666666667">
Two experiments measures the effects of nonlin-
ear dimensionality reduction for word spaces. For
both, we apply LPP to two basic word space mod-
els, the VSM and WC. In the first experiment,
we measure the word spaces’ abilities to model
semantic relations, as determined by priming ex-
periments. In the second experiment, we evaluate
the representation capabilities of the LPP-reduced
models on standard word space benchmarks.
</bodyText>
<subsectionHeader confidence="0.981238">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999976965517241">
For the VSM-based word space, we consider three
different weighting schemes: no weighting, TF-
IDF and the log-entropy (LE) used in (Landauer
et al., 1998). For the WC-based word space, we
use a 5 word sliding window. Due to the large pa-
rameter space for LPP models, we performed only
a limited configuration search. An initial analysis
using the 20 nearest neighbors and cosine simi-
larity did not show significant performance differ-
ences when the number of dimensions was varied
between 50 and 1000. We therefore selected 300
dimensions for all tests. Further work is needed to
identify the impact of different parameters. Stop
words were removed only for the WC+LPP model.
We compare the LPP-based spaces to three mod-
els: VSM, HAL, and LSA.
Two corpora are used to train the models in both
experiments. The first corpus, TASA, is a collec-
tion of 44,486 essays that are representative of the
reading a student might see upon entering college,
introduced by (Landauer et al., 1998). The cor-
pus consists of 98,420 unique words; no filtering
is done when processing this corpus. The second
corpus, WIKI, is a 387,082 article subset of a De-
cember 2009 Wikipedia snapshot consisting of all
the articles with more than 1,000 tokens. The cor-
pus is filtered to retain the top 100,000 most fre-
quent tokens in addition to all the tokens used in
each experiment’s data set.
</bodyText>
<subsectionHeader confidence="0.859859">
4.2 Experiment 1
</subsectionHeader>
<bodyText confidence="0.99993125">
Semantic priming measures word association
based on human responses to a provided cue.
Priming studies have been used to evaluate word
spaces by equating vector similarity with an in-
creased priming response. We use data from two
types of priming experiments to measure whether
LPP models better correlate with human perfor-
mance than non-LPP word spaces.
Normed Priming Nelson et al. (1998) collected
free association responses to 5,019 prime words.
An average of 149 participants responded to each
prime with the first word that came to mind.
Based on this dataset, we introduce a new
benchmark that correlates word space similarity
with the associative strength of semantic priming
pairs. We use three measures for modeling prime-
target strength, which were inspired by Steyvers
et al. (2004). Let Wab be the percentage of partici-
pants who responded to prime a with target b. The
three measures of associative strength are
</bodyText>
<equation confidence="0.9867115">
S1ab = Wab
S2ab = Wab + Wba
3 2 2
Sab _ — 5ab + �c SacSc2b
</equation>
<bodyText confidence="0.999949413793104">
These measure three different levels of semantic
relatedness between words a and b. S1ab measures
the relationship from a to b, which is frequently
asymmetric due to ordering, e.g. “orange” pro-
duces “juice” more frequently than “juice” pro-
duces “orange.” S2 ab measures the symmetric asso-
ciation between a and b; Steyvers et al. (2004) note
that this may better model the associative strength
by including weaker associates that may have been
a suitable second response. S3ab further increases
the association by including the indirect associa-
tions between a and b from all cued primes.
For each measure, we rank a prime’s targets
according to their strength and then compute the
Spearman rank correlation with the prime-target
similarities in the word space. The rank compari-
son measures how well word space similarity cor-
responds to the priming association. We report the
average rank correlation of associational strengths
over all primes.
Priming Effect The priming study by Hodgson
(1991), which evaluated how different semantic
relationships affected the strength of priming, pro-
vides the data for our second priming test. Six re-
lationships were examined in the study: antonymy,
synonymy, conceptual association (sleep and bed),
categorical coordinates (mist and rain), phrasal as-
sociates (pony and express), and super- and sub-
ordinates. Each relationship contained an average
</bodyText>
<page confidence="0.998315">
3
</page>
<table confidence="0.9998985">
Algorithm Antonymy Conceptual Coordinates
Rb U E R U E R U E
VSM+LPP+LE 0.103 0.018 0.085 0.197 0.050 0.147 0.071 0.027 0.044
VSM+LPP+TF-IDF 0.348 0.321 0.027 0.408 0.414 -0.005 0.323 0.294 0.029
VSM+LPP 0.247 0.122 0.124 0.312 0.120 0.193 0.230 0.111 0.119
VSM+LPPa 0.298 0.070 0.228 0.284 0.033 0.252 0.321 0.037 0.284
WC+LPP 0.255 0.071 0.185 0.413 0.110 0.303 0.431 0.134 0.298
HAL 0.813 0.716 0.096 0.845 0.814 0.031 0.861 0.809 0.052
HALa 0.915 0.879 0.037 0.867 0.846 0.021 0.913 0.861 0.052
LSA 0.235 0.023 0.213 0.392 0.028 0.364 0.199 0.014 0.185
LSAa 0.287 0.061 0.226 0.362 0.041 0.321 0.316 0.037 0.278
VSM 0.051 0.011 0.040 0.111 0.012 0.099 0.032 0.008 0.024
Phrasal Ordinates Synonymy
Algorithm R U E R U E R U E
VSM+LPP+LE 0.147 0.039 0.108 0.225 0.032 0.193 0.081 0.027 0.053
VSM+LPP+TF-IDF 0.438 0.425 0.013 0.277 0.290 -0.013 0.344 0.328 0.017
VSM+LPP 0.234 0.107 0.127 0.273 0.115 0.158 0.237 0.157 0.080
VSM+LPPa 0.202 0.031 0.171 0.270 0.032 0.238 0.299 0.069 0.230
WC+LPP 0.274 0.087 0.186 0.324 0.076 0.248 0.345 0.111 0.233
HAL 0.805 0.776 0.029 0.825 0.789 0.036 0.757 0.681 0.076
HALa 0.866 0.856 0.010 0.881 0.857 0.024 0.898 0.879 0.019
LSA 0.280 0.021 0.258 0.258 0.018 0.240 0.197 0.019 0.178
LSAa 0.269 0.030 0.238 0.326 0.032 0.294 0.327 0.052 0.275
VSM 0.104 0.013 0.091 0.061 0.008 0.053 0.052 0.009 0.043
</table>
<tableCaption confidence="0.967841666666667">
a Processed using the WIKI corpus
b R are related primes, U are unrelated primes, E is the priming effect
Table 1: Experiment 1 priming results for the six relation categories from Hodgson (1991)
</tableCaption>
<table confidence="0.99997975">
Word Choice Word Association
Algorithm Corpus TOEFL ESL RDWP F. et al. R.&amp;G. Deese
VSM+LPP+le TASA 24.000 50.000 45.313 0.296 0.092 0.034
VSM+LPP+tf-idf TASA 22.667 25.000 37.209 0.023 0.086 0.001
VSM+LPP TASA 41.333 54.167 39.063 0.219 0.136 0.045
VSM+LPP Wiki 33.898 48.780 43.434 0.530 0.503 0.108
WC+LPP TASA 46.032 40.000 45.783 0.423 0.414 0.126
HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318
HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042
LSA TASA 56.000 50.000 55.814 0.516 0.651 0.349
LSA Wiki 60.759 54.167 59.200 0.614 0.681 0.206
VSM TASA 61.333 52.083 84.884 0.396 0.496 0.200
</table>
<tableCaption confidence="0.999778">
Table 2: Results from Experiment 2 on six word space benchmarks
</tableCaption>
<bodyText confidence="0.9997926">
of 23 word pairs. Hodgson’s results showed that
priming effects were exhibited by the prime-target
pairs in all six categories.
We use the same methodology as Pad´o and La-
pata (2007) for this data set; the prime-target (Re-
lated Primes) cosine similarity is compared with
the average cosine similarity between the prime
and all other targets (Unrelated Primes) within the
semantic category. The priming effect is the dif-
ference between the two similarity values.
</bodyText>
<subsectionHeader confidence="0.936794">
4.3 Experiment 2
</subsectionHeader>
<bodyText confidence="0.9990733125">
We use six standard word space benchmarks to
test our hypothesis that LPP can accurately capture
general semantic knowledge and association based
relations. The benchmarks come in two forms:
word association and word choice tests.
Word choice tests provide a target word and a
list of options, one of which has the desired rela-
tion to the target. To answer these questions, we
select the option with the highest cosine similar-
ity with the target. Three word choice synonymy
benchmarks are used: the Test of English as a For-
eign Language (TOEFL) test set from (Landauer
et al., 1998), the English as a Second Language
(ESL) test set from (Turney, 2001), and the Cana-
dian Reader’s Digest Word Power (RDWP) from
(Jarmasz and Szpakowicz, 2003).
</bodyText>
<page confidence="0.995412">
4
</page>
<table confidence="0.999893727272727">
Algorithm Corpus S1 S2 S3
VSM+LPP+LE TASA 0.457 0.413 0.255
VSM+LPP+TF-IDF TASA 0.464 0.390 0.207
VSM+LPP TASA 0.457 0.427 0.275
VSM+LPP Wiki 0.472 0.440 0.333
WC+LPP TASA 0.469 0.437 0.315
HAL TASA 0.485 0.434 0.310
HAL Wiki 0.462 0.406 0.266
LSA TASA 0.494 0.481 0.414
LSA Wiki 0.489 0.472 0.398
VSM TASA 0.484 0.460 0.407
</table>
<tableCaption confidence="0.999941">
Table 3: Experiment 1 results for normed priming.
</tableCaption>
<bodyText confidence="0.999714571428572">
Word association tests measure the semantic re-
latedness of two words by comparing their simi-
larity in the word space with human judgements.
These tests are more precise than word choice tests
because they take into account the specific value
of the word similarity. Three word association
benchmarks are used: the word similarity data set
of Rubenstein and Goodenough (1965), the word-
relatedness data set of Finkelstein et al. (2002),
and the antonymy data set of Deese (1964), which
measures the degree to which high similarity cap-
tures the antonymy relationship. The Finkelstein
et al. test is notable in that the human judges were
free to score based on any word relationship.
</bodyText>
<sectionHeader confidence="0.998647" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999986909090909">
The LPP-based models show mixed performance
in comparison to existing models on normed prim-
ing tasks, shown in Table 3. Adding LPP to
the VSM decreased performance; however, when
WIKI was used instead of TASA, the VSM+LPP
model increased .15 on all correlations, whereas
LSA’s performance decreased. This suggests that
LPP needs more data than LSA to properly model
the word space manifold. WC+LPP performs
comparably to HAL, which indicates that LPP
is effective in retaining the original WC space’s
structure in significantly fewer dimensions.
For the categorical priming tests shown in Ta-
ble 1, LPP-based models show competitive results.
VSM+LPP with the WIKI corpus performs much
better than other VSM+LPP configurations. Un-
like in the previous priming experiment, adding
LPP to the base models resulted in a significant
performance improvement. We also note that both
HAL models and the VSM+LPP+TF-IDF model
have high similarity ratings for unrelated primes.
We posit that these models’ feature weighting re-
sults in poor differentiation between words in the
same semantic category, which causes their de-
creased performance.
For experiment 2, LPP-based spaces showed
mixed results on word choice benchmarks, while
showing notable improvement on the more pre-
cise word association benchmarks. Table 2 lists
the results. Notably, LPP-based spaces performed
well on the ESL synonym benchmark but poorly
on the TOEFL synonym benchmark, even when
the larger WIKI corpus was used. This suggests
that LPP was not effective in retaining the re-
lationship between certain classes of synonyms.
Given that performance did not improve with the
WIKI corpus, further analysis is needed to iden-
tify whether a different representation of the local
structure would improve results or if the poor per-
formance is due to another factor. While LSA and
VSM model performed best on all benchmarks,
LPP-based spaces performed competitively on the
word association tests. In all but two tests, the
WC+LPP model outperformed HAL.
The results from both experiments indicate that
LPP is capable of accurately representing distri-
butional information in a much lower dimensional
space. However, in many cases, applications using
the SVD-reduced representations performed bet-
ter. In addition, application of standard weight-
ing schemes worsened LPP-models’ performance,
which suggests that the local neighborhood is ad-
versely distorted. Nevertheless, we view these re-
sults as a promising starting point for further eval-
uation of nonlinear dimensionality reduction.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999960647058824">
We have shown that LPP is an effective dimen-
sionality reduction technique for word space algo-
rithms. In several benchmarks, LPP provided a
significant benefit to the base models and in a few
cases outperformed the SVD. However, it does not
perform consistently better than existing models.
Future work will focus on four themes: identifying
optimal LPP parameter configurations; improving
LPP with weighting; measuring LPP’s capacity to
capture higher order co-occurrence relationships,
as was shown for the SVD (Lemaire et al., 2006);
and investigating whether more computationally
expensive nonlinear reduction algorithms such as
ISOMAP (Tenenbaum et al., 2000) are better for
word space algorithms. We plan to release imple-
mentations of the LPP-based models as a part of
the S-Space Package (Jurgens and Stevens, 2010).
</bodyText>
<page confidence="0.991177">
5
</page>
<sectionHeader confidence="0.989934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999435843137255">
Marco Baroni and Alessandro Lenci. 2008. Con-
cepts and properties in word spaces. From context to
meaning: Distributional models of the lexicon in lin-
guistics and cognitive science (Special issue of the
Italian Journal ofLinguistics), 1(20):55–88.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps and Spectral Techniques for Embedding
and Clustering. In Advances in Neural Information
Processing Systems, number 14.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: a computational study. Behav-
ior Research Methods, 39:510–526.
Deng Cai, Xiaofei He, and Jiawei Han. 2007a. Spec-
tral regression for efficient regularized subspace
learning. In IEEE International Conference on
Computer Vision (ICCV’07).
Deng Cai, Xiaofei He, Wei Vivian Zhang, , and Jiawei
Han. 2007b. Regularized Locality Preserving In-
dexing via Spectral Regression. In Proceedings of
the 2007 ACM International Conference on Infor-
mation and Knowledge Management (CIKM’07).
James Deese. 1964. The associative structure of
some common english adjectives. Journal of Verbal
Learning and Verbal Behavior, 3(5):347–357.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Woflman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions ofInformation
Systems, 20(1):116–131.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Advances in Neural Information
Processing Systems 16 (NIPS).
Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying
Ma. 2004. Locality preserving indexing for doc-
ument representation. In SIGIR ’04: Proceedings
of the 27th annual international ACMSIGIR confer-
ence on Research and development in information
retrieval, pages 96–103.
James M. Hodgson. 1991. Informational constraints
on pre-lexical priming. Language and Cognitive
Processes, 6:169–205.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
thesaurus and semantic similarity. In Conference on
Recent Advances in Natural Language Processing,
pages 212–219.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In L. R. Gleitman and A. K. Josh,
editors, Proceedings of the 22nd Annual Conference
of the Cognitive Science Society, page 1036.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, (25):259–284.
Benoit Lemaire, , and Guy Henhi´ere. 2006. Effects
of High-Order Co-occurrences on Word Semantic
Similarities. Current Psychology Letters, 1(18).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occrrence. Behavoir Research Methods, Instru-
ments &amp; Computers, 28(2):203–208.
Douglas L. Nelson, Cathy L. McEvoy, and
Thomas A. Schreiber. 1998. The Uni-
versity of South Florida word association,
rhyme, and word fragment norms. http:
//www.usf.edu/FreeAssociation/.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161–199.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8:627–633.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode or-
der in word space. In Proceedings of the 30th
Annual Meeting of the Cognitive Science Society
(CogSci’08).
Gerard Salton, A. Wong, and C. S. Yang. 1975. A
vector space model for automatic indexing. Com-
munications of the ACM, 18(11):613–620.
Hinrich Sch¨utze. 1992. Dimensions of meaning.
In Proceedings of Supercomputing ’92, pages 787–
796.
Mark Steyvers, Richard M. Shiffrin, and Douglas L.
Nelson, 2004. Word association spaces for predict-
ing semantic similarity effects in episodic memory.
American Psychological Assocation.
Joshua B. Tenenbaum, Vin de Silva, and John C.
Langford. 2000. A global geometric framework
for nonlinear dimensionality reduction. Science,
290(5500):2319–2323.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491–502.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84:327–352.
</reference>
<page confidence="0.998786">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671680">
<title confidence="0.999627">Capturing Nonlinear Structure in Word Spaces through Dimensionality Reduction</title>
<author confidence="0.986208">David</author>
<affiliation confidence="0.998795">University of California, Los</affiliation>
<address confidence="0.987771">4732 Boelter Los Angeles, CA</address>
<email confidence="0.999292">jurgens@cs.ucla.edu</email>
<author confidence="0.722742">Keith</author>
<affiliation confidence="0.994891">University of California, Los</affiliation>
<address confidence="0.9855745">4732 Boelter Los Angeles, CA</address>
<email confidence="0.99912">kstevens@cs.ucla.edu</email>
<abstract confidence="0.999832142857143">Dimensionality reduction has been shown to improve processing and information extraction from high dimensional data. Word space algorithms typically employ linear reduction techniques that assume the space is Euclidean. We investigate the effects of extracting nonlinear structure in the word space using Locality Preserving Projections, a reduction algorithm that performs manifold learning. We apply this reduction to two common word space models and show improved performance over the original models on benchmarks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Concepts and properties in word spaces. From context to meaning: Distributional models of the lexicon in linguistics and cognitive science</title>
<date>2008</date>
<journal>(Special issue of the Italian Journal ofLinguistics),</journal>
<volume>1</volume>
<issue>20</issue>
<contexts>
<context position="4355" citStr="Baroni and Lenci, 2008" startWordPosition="677" endWordPosition="680">on matrix, A = UkEkVk⊤, is the least squares best-fit rank-k approximation of A. 1 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1–6, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics The SVD has been used with great success on both models. Latent Semantic Analysis (LSA) (Landauer et al., 1998) extends the (VSM) by decomposing the space using the SVD and making the word space the left singular vectors, Uk. WC models have also utilized the SVD to improve performance (Sch¨utze, 1992; Bullinaria and Levy, 2007; Baroni and Lenci, 2008). Feature elimination reduces the dimensionality by removing those with low information content. This approach has been successfully applied to WC models such as HAL (Lund and Burgess, 1996) by dropping those with low entropy. This technique effectively removes the feature dimensions of high frequency words, which provide little discriminatory content. Randomized projections have also been successfully applied to VSM models, e.g. (Kanerva et al., 2000) and WC models, e.g. (Sahlgren et al., 2008). This reduction statistically approximates the original space in a much lower dimensional space. Th</context>
</contexts>
<marker>Baroni, Lenci, 2008</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2008. Concepts and properties in word spaces. From context to meaning: Distributional models of the lexicon in linguistics and cognitive science (Special issue of the Italian Journal ofLinguistics), 1(20):55–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems, number 14.</booktitle>
<contexts>
<context position="1755" citStr="Belkin and Niyogi, 2002" startWordPosition="252" endWordPosition="255"> shown to exhibit many non-metric properties: asymmetry, e.g North Korea is more similar to Red China than Red China is to North Korea, and non-transitivity, e.g. Cuba is similar the former USSR, Jamaica is similar to Cuba, but Jamaica is not similar to the USSR (Tversky, 1977). We hypothesize that a non-linear word space model might more accurately preserve these non-metric relationships. To test our hypothesis, we capture the nonlinear structure with dimensionality reduction by using Locality Preserving Projection (LPP) (He and Niyogi, 2003), an efficient, linear approximation of Eigenmaps (Belkin and Niyogi, 2002). With this reduction, the word space vectors are assumed to exist on a nonlinear manifold that LPP learns in order to project the vectors into a Euclidean space. We measure the effects of using LPP on two basic word space models: the Vector Space Model and a Word Co-occurrence model. We begin with a brief overview of these word spaces and common dimensionality reduction techniques. We then formally introduce LPP. Following, we use two experiments to demonstrate LPP’s capacity to accurately dimensionally reduce word spaces. 2 Word Spaces and Reductions We consider two common word space models </context>
<context position="6124" citStr="Belkin and Niyogi, 2002" startWordPosition="974" endWordPosition="977">l space. Minimizing this equation is equivalent to finding the transformation vector that best preserves the local distances in the original space according to S. LPP assumes that the data points xi exist on a manifold. This is in contrast to the SVD, which assumes that the space is Euclidean and performs a global, rather than local, minimization. In treating the space as a manifold, LPP is able to discover some of the nonlinear structure of the data from its local structure. To solve the minimization problem in Equation 1, LPP uses a linear approximation of the Laplacian Eigenmaps procedure (Belkin and Niyogi, 2002) as follows: 1. Let X be a matrix where xi is the ith row vector. Construct an adjacency matrix, S, which represents the local structure of the original vector space, by making an edge between points xi and xj if xj is locally proximate to xi. Two variations are available for determining proximity: either the k-nearest neighbors, or all the data points with similarity &gt; ǫ. 2. Weight the edges in S proportional to the closeness of the data points. Four main options are available: a Gaussian kernel, a polynomial kernel, cosine similarity, or binary. 3. Construct the diagonal matrix D where entry</context>
</contexts>
<marker>Belkin, Niyogi, 2002</marker>
<rawString>Mikhail Belkin and Partha Niyogi. 2002. Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering. In Advances in Neural Information Processing Systems, number 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: a computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="4330" citStr="Bullinaria and Levy, 2007" startWordPosition="673" endWordPosition="676">thers to 0. The approximation matrix, A = UkEkVk⊤, is the least squares best-fit rank-k approximation of A. 1 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1–6, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics The SVD has been used with great success on both models. Latent Semantic Analysis (LSA) (Landauer et al., 1998) extends the (VSM) by decomposing the space using the SVD and making the word space the left singular vectors, Uk. WC models have also utilized the SVD to improve performance (Sch¨utze, 1992; Bullinaria and Levy, 2007; Baroni and Lenci, 2008). Feature elimination reduces the dimensionality by removing those with low information content. This approach has been successfully applied to WC models such as HAL (Lund and Burgess, 1996) by dropping those with low entropy. This technique effectively removes the feature dimensions of high frequency words, which provide little discriminatory content. Randomized projections have also been successfully applied to VSM models, e.g. (Kanerva et al., 2000) and WC models, e.g. (Sahlgren et al., 2008). This reduction statistically approximates the original space in a much lo</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: a computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Xiaofei He</author>
<author>Jiawei Han</author>
</authors>
<title>Spectral regression for efficient regularized subspace learning.</title>
<date>2007</date>
<booktitle>In IEEE International Conference on Computer Vision (ICCV’07).</booktitle>
<contexts>
<context position="7552" citStr="Cai et al. (2007" startWordPosition="1222" endWordPosition="1225">] denote the matrix of transformation vectors, sorted in descending order according to their eigenvalues λ. The original space is projected into k dimensions by Wk⊤ X —* Xk. For many applications of LPP, such as document clustering (He et al., 2004), the original data matrix X is transformed by first performing Principle Component Analysis and discarding the smallest principle components, which requires computing the full SVD. However, for large data sets such as those frequently used in word space algorithms, performing the full SVD is computationally infeasible. To overcome this limitation, Cai et al. (2007a) show how Spectral Regression may be used as an alternative for solving the same minimization equation through an iterative process. The principle idea is that Equation 2 may be recast as Sy = λDy (3) where y = X⊤w, which ensures y will be an eigenvector with the same eigenvalue for the problem in Equation 2. Finding the transformation matrix Wk, used in step 4, is done in two steps. First, Equation 3 is solved to produce eigenvectors [y0, ... , yk], sorted in decreasing order according to their eigenvalues λ. Second, the set of transformation vectors composing Wk, [w1, . . . , wk], is found</context>
</contexts>
<marker>Cai, He, Han, 2007</marker>
<rawString>Deng Cai, Xiaofei He, and Jiawei Han. 2007a. Spectral regression for efficient regularized subspace learning. In IEEE International Conference on Computer Vision (ICCV’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Xiaofei He</author>
<author>Wei Vivian Zhang</author>
</authors>
<title>Regularized Locality Preserving Indexing via Spectral Regression.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 ACM International Conference on Information and Knowledge Management (CIKM’07).</booktitle>
<contexts>
<context position="7552" citStr="Cai et al. (2007" startWordPosition="1222" endWordPosition="1225">] denote the matrix of transformation vectors, sorted in descending order according to their eigenvalues λ. The original space is projected into k dimensions by Wk⊤ X —* Xk. For many applications of LPP, such as document clustering (He et al., 2004), the original data matrix X is transformed by first performing Principle Component Analysis and discarding the smallest principle components, which requires computing the full SVD. However, for large data sets such as those frequently used in word space algorithms, performing the full SVD is computationally infeasible. To overcome this limitation, Cai et al. (2007a) show how Spectral Regression may be used as an alternative for solving the same minimization equation through an iterative process. The principle idea is that Equation 2 may be recast as Sy = λDy (3) where y = X⊤w, which ensures y will be an eigenvector with the same eigenvalue for the problem in Equation 2. Finding the transformation matrix Wk, used in step 4, is done in two steps. First, Equation 3 is solved to produce eigenvectors [y0, ... , yk], sorted in decreasing order according to their eigenvalues λ. Second, the set of transformation vectors composing Wk, [w1, . . . , wk], is found</context>
</contexts>
<marker>Cai, He, Zhang, 2007</marker>
<rawString>Deng Cai, Xiaofei He, Wei Vivian Zhang, , and Jiawei Han. 2007b. Regularized Locality Preserving Indexing via Spectral Regression. In Proceedings of the 2007 ACM International Conference on Information and Knowledge Management (CIKM’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Deese</author>
</authors>
<title>The associative structure of some common english adjectives.</title>
<date>1964</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>3</volume>
<issue>5</issue>
<contexts>
<context position="16913" citStr="Deese (1964)" startWordPosition="2788" endWordPosition="2789">.266 LSA TASA 0.494 0.481 0.414 LSA Wiki 0.489 0.472 0.398 VSM TASA 0.484 0.460 0.407 Table 3: Experiment 1 results for normed priming. Word association tests measure the semantic relatedness of two words by comparing their similarity in the word space with human judgements. These tests are more precise than word choice tests because they take into account the specific value of the word similarity. Three word association benchmarks are used: the word similarity data set of Rubenstein and Goodenough (1965), the wordrelatedness data set of Finkelstein et al. (2002), and the antonymy data set of Deese (1964), which measures the degree to which high similarity captures the antonymy relationship. The Finkelstein et al. test is notable in that the human judges were free to score based on any word relationship. 5 Results and Discussion The LPP-based models show mixed performance in comparison to existing models on normed priming tasks, shown in Table 3. Adding LPP to the VSM decreased performance; however, when WIKI was used instead of TASA, the VSM+LPP model increased .15 on all correlations, whereas LSA’s performance decreased. This suggests that LPP needs more data than LSA to properly model the w</context>
</contexts>
<marker>Deese, 1964</marker>
<rawString>James Deese. 1964. The associative structure of some common english adjectives. Journal of Verbal Learning and Verbal Behavior, 3(5):347–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Woflman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions ofInformation Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="16870" citStr="Finkelstein et al. (2002)" startWordPosition="2778" endWordPosition="2781"> 0.315 HAL TASA 0.485 0.434 0.310 HAL Wiki 0.462 0.406 0.266 LSA TASA 0.494 0.481 0.414 LSA Wiki 0.489 0.472 0.398 VSM TASA 0.484 0.460 0.407 Table 3: Experiment 1 results for normed priming. Word association tests measure the semantic relatedness of two words by comparing their similarity in the word space with human judgements. These tests are more precise than word choice tests because they take into account the specific value of the word similarity. Three word association benchmarks are used: the word similarity data set of Rubenstein and Goodenough (1965), the wordrelatedness data set of Finkelstein et al. (2002), and the antonymy data set of Deese (1964), which measures the degree to which high similarity captures the antonymy relationship. The Finkelstein et al. test is notable in that the human judges were free to score based on any word relationship. 5 Results and Discussion The LPP-based models show mixed performance in comparison to existing models on normed priming tasks, shown in Table 3. Adding LPP to the VSM decreased performance; however, when WIKI was used instead of TASA, the VSM+LPP model increased .15 on all correlations, whereas LSA’s performance decreased. This suggests that LPP needs</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Woflman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Woflman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions ofInformation Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei He</author>
<author>Partha Niyogi</author>
</authors>
<title>Locality preserving projections.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 16 (NIPS).</booktitle>
<contexts>
<context position="1680" citStr="He and Niyogi, 2003" startWordPosition="241" endWordPosition="244">he unreduced word space is linear. However, word similarities have been shown to exhibit many non-metric properties: asymmetry, e.g North Korea is more similar to Red China than Red China is to North Korea, and non-transitivity, e.g. Cuba is similar the former USSR, Jamaica is similar to Cuba, but Jamaica is not similar to the USSR (Tversky, 1977). We hypothesize that a non-linear word space model might more accurately preserve these non-metric relationships. To test our hypothesis, we capture the nonlinear structure with dimensionality reduction by using Locality Preserving Projection (LPP) (He and Niyogi, 2003), an efficient, linear approximation of Eigenmaps (Belkin and Niyogi, 2002). With this reduction, the word space vectors are assumed to exist on a nonlinear manifold that LPP learns in order to project the vectors into a Euclidean space. We measure the effects of using LPP on two basic word space models: the Vector Space Model and a Word Co-occurrence model. We begin with a brief overview of these word spaces and common dimensionality reduction techniques. We then formally introduce LPP. Following, we use two experiments to demonstrate LPP’s capacity to accurately dimensionally reduce word spa</context>
<context position="6842" citStr="He and Niyogi (2003)" startWordPosition="1105" endWordPosition="1108">, which represents the local structure of the original vector space, by making an edge between points xi and xj if xj is locally proximate to xi. Two variations are available for determining proximity: either the k-nearest neighbors, or all the data points with similarity &gt; ǫ. 2. Weight the edges in S proportional to the closeness of the data points. Four main options are available: a Gaussian kernel, a polynomial kernel, cosine similarity, or binary. 3. Construct the diagonal matrix D where entry Dii = Ej Sij. Let L = D − S. Then solve the generalized eigenvector problem: XLX⊤w = λXDX⊤w. (2) He and Niyogi (2003) show that solving this problem is equivalent to solving Equation 1. 4. Let Wk = [w1, ... , wk] denote the matrix of transformation vectors, sorted in descending order according to their eigenvalues λ. The original space is projected into k dimensions by Wk⊤ X —* Xk. For many applications of LPP, such as document clustering (He et al., 2004), the original data matrix X is transformed by first performing Principle Component Analysis and discarding the smallest principle components, which requires computing the full SVD. However, for large data sets such as those frequently used in word space al</context>
</contexts>
<marker>He, Niyogi, 2003</marker>
<rawString>Xiaofei He and Partha Niyogi. 2003. Locality preserving projections. In Advances in Neural Information Processing Systems 16 (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei He</author>
<author>Deng Cai</author>
<author>Haifeng Liu</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Locality preserving indexing for document representation.</title>
<date>2004</date>
<booktitle>In SIGIR ’04: Proceedings of the 27th annual international ACMSIGIR conference on Research and development in information retrieval,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="7185" citStr="He et al., 2004" startWordPosition="1166" endWordPosition="1169">ints. Four main options are available: a Gaussian kernel, a polynomial kernel, cosine similarity, or binary. 3. Construct the diagonal matrix D where entry Dii = Ej Sij. Let L = D − S. Then solve the generalized eigenvector problem: XLX⊤w = λXDX⊤w. (2) He and Niyogi (2003) show that solving this problem is equivalent to solving Equation 1. 4. Let Wk = [w1, ... , wk] denote the matrix of transformation vectors, sorted in descending order according to their eigenvalues λ. The original space is projected into k dimensions by Wk⊤ X —* Xk. For many applications of LPP, such as document clustering (He et al., 2004), the original data matrix X is transformed by first performing Principle Component Analysis and discarding the smallest principle components, which requires computing the full SVD. However, for large data sets such as those frequently used in word space algorithms, performing the full SVD is computationally infeasible. To overcome this limitation, Cai et al. (2007a) show how Spectral Regression may be used as an alternative for solving the same minimization equation through an iterative process. The principle idea is that Equation 2 may be recast as Sy = λDy (3) where y = X⊤w, which ensures y</context>
</contexts>
<marker>He, Cai, Liu, Ma, 2004</marker>
<rawString>Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying Ma. 2004. Locality preserving indexing for document representation. In SIGIR ’04: Proceedings of the 27th annual international ACMSIGIR conference on Research and development in information retrieval, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James M Hodgson</author>
</authors>
<title>Informational constraints on pre-lexical priming.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--169</pages>
<contexts>
<context position="12260" citStr="Hodgson (1991)" startWordPosition="2022" endWordPosition="2023">tive strength by including weaker associates that may have been a suitable second response. S3ab further increases the association by including the indirect associations between a and b from all cued primes. For each measure, we rank a prime’s targets according to their strength and then compute the Spearman rank correlation with the prime-target similarities in the word space. The rank comparison measures how well word space similarity corresponds to the priming association. We report the average rank correlation of associational strengths over all primes. Priming Effect The priming study by Hodgson (1991), which evaluated how different semantic relationships affected the strength of priming, provides the data for our second priming test. Six relationships were examined in the study: antonymy, synonymy, conceptual association (sleep and bed), categorical coordinates (mist and rain), phrasal associates (pony and express), and super- and subordinates. Each relationship contained an average 3 Algorithm Antonymy Conceptual Coordinates Rb U E R U E R U E VSM+LPP+LE 0.103 0.018 0.085 0.197 0.050 0.147 0.071 0.027 0.044 VSM+LPP+TF-IDF 0.348 0.321 0.027 0.408 0.414 -0.005 0.323 0.294 0.029 VSM+LPP 0.24</context>
<context position="14189" citStr="Hodgson (1991)" startWordPosition="2339" endWordPosition="2340">80 VSM+LPPa 0.202 0.031 0.171 0.270 0.032 0.238 0.299 0.069 0.230 WC+LPP 0.274 0.087 0.186 0.324 0.076 0.248 0.345 0.111 0.233 HAL 0.805 0.776 0.029 0.825 0.789 0.036 0.757 0.681 0.076 HALa 0.866 0.856 0.010 0.881 0.857 0.024 0.898 0.879 0.019 LSA 0.280 0.021 0.258 0.258 0.018 0.240 0.197 0.019 0.178 LSAa 0.269 0.030 0.238 0.326 0.032 0.294 0.327 0.052 0.275 VSM 0.104 0.013 0.091 0.061 0.008 0.053 0.052 0.009 0.043 a Processed using the WIKI corpus b R are related primes, U are unrelated primes, E is the priming effect Table 1: Experiment 1 priming results for the six relation categories from Hodgson (1991) Word Choice Word Association Algorithm Corpus TOEFL ESL RDWP F. et al. R.&amp;G. Deese VSM+LPP+le TASA 24.000 50.000 45.313 0.296 0.092 0.034 VSM+LPP+tf-idf TASA 22.667 25.000 37.209 0.023 0.086 0.001 VSM+LPP TASA 41.333 54.167 39.063 0.219 0.136 0.045 VSM+LPP Wiki 33.898 48.780 43.434 0.530 0.503 0.108 WC+LPP TASA 46.032 40.000 45.783 0.423 0.414 0.126 HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318 HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042 LSA TASA 56.000 50.000 55.814 0.516 0.651 0.349 LSA Wiki 60.759 54.167 59.200 0.614 0.681 0.206 VSM TASA 61.333 52.083 84.884 0.396 0.496 0.200 Table 2: Resu</context>
</contexts>
<marker>Hodgson, 1991</marker>
<rawString>James M. Hodgson. 1991. Informational constraints on pre-lexical priming. Language and Cognitive Processes, 6:169–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>212--219</pages>
<contexts>
<context position="16058" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="2644" endWordPosition="2647">tic knowledge and association based relations. The benchmarks come in two forms: word association and word choice tests. Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. To answer these questions, we select the option with the highest cosine similarity with the target. Three word choice synonymy benchmarks are used: the Test of English as a Foreign Language (TOEFL) test set from (Landauer et al., 1998), the English as a Second Language (ESL) test set from (Turney, 2001), and the Canadian Reader’s Digest Word Power (RDWP) from (Jarmasz and Szpakowicz, 2003). 4 Algorithm Corpus S1 S2 S3 VSM+LPP+LE TASA 0.457 0.413 0.255 VSM+LPP+TF-IDF TASA 0.464 0.390 0.207 VSM+LPP TASA 0.457 0.427 0.275 VSM+LPP Wiki 0.472 0.440 0.333 WC+LPP TASA 0.469 0.437 0.315 HAL TASA 0.485 0.434 0.310 HAL Wiki 0.462 0.406 0.266 LSA TASA 0.494 0.481 0.414 LSA Wiki 0.489 0.472 0.398 VSM TASA 0.484 0.460 0.407 Table 3: Experiment 1 results for normed priming. Word association tests measure the semantic relatedness of two words by comparing their similarity in the word space with human judgements. These tests are more precise than word choice tests because they take into accoun</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Conference on Recent Advances in Natural Language Processing, pages 212–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>The S-Space Package: An Open Source Package for Word Space Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations.</booktitle>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. The S-Space Package: An Open Source Package for Word Space Models. In Proceedings of the ACL 2010 System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristoferson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1036</pages>
<editor>In L. R. Gleitman and A. K. Josh, editors,</editor>
<contexts>
<context position="4811" citStr="Kanerva et al., 2000" startWordPosition="747" endWordPosition="750"> space the left singular vectors, Uk. WC models have also utilized the SVD to improve performance (Sch¨utze, 1992; Bullinaria and Levy, 2007; Baroni and Lenci, 2008). Feature elimination reduces the dimensionality by removing those with low information content. This approach has been successfully applied to WC models such as HAL (Lund and Burgess, 1996) by dropping those with low entropy. This technique effectively removes the feature dimensions of high frequency words, which provide little discriminatory content. Randomized projections have also been successfully applied to VSM models, e.g. (Kanerva et al., 2000) and WC models, e.g. (Sahlgren et al., 2008). This reduction statistically approximates the original space in a much lower dimensional space. The projection does not take into account the structure of data, which provides only a computational benefit from fewer dimensions, unlike the previous two reductions. 3 Locality Preserving Projection For a set of vectors, x1, x2,. . . , xn E Rm, LPP preserves the distance in the k-dimensional space, where k « m, by solving the following minimization problem, �min (w⊤xi − w⊤xj)2Sij (1) w ij where w is a transformation vector that projects x into the lowe</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In L. R. Gleitman and A. K. Josh, editors, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, page 1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="4113" citStr="Landauer et al., 1998" startWordPosition="635" endWordPosition="638">atrix containing the singular values of A, ordered descending based on their effect on the variance in the values of A. The original matrix can be approximated by using only the top k singular values, setting all others to 0. The approximation matrix, A = UkEkVk⊤, is the least squares best-fit rank-k approximation of A. 1 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1–6, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics The SVD has been used with great success on both models. Latent Semantic Analysis (LSA) (Landauer et al., 1998) extends the (VSM) by decomposing the space using the SVD and making the word space the left singular vectors, Uk. WC models have also utilized the SVD to improve performance (Sch¨utze, 1992; Bullinaria and Levy, 2007; Baroni and Lenci, 2008). Feature elimination reduces the dimensionality by removing those with low information content. This approach has been successfully applied to WC models such as HAL (Lund and Burgess, 1996) by dropping those with low entropy. This technique effectively removes the feature dimensions of high frequency words, which provide little discriminatory content. Ran</context>
<context position="9059" citStr="Landauer et al., 1998" startWordPosition="1483" endWordPosition="1486">genproblem solution. 4 Experiments Two experiments measures the effects of nonlinear dimensionality reduction for word spaces. For both, we apply LPP to two basic word space models, the VSM and WC. In the first experiment, we measure the word spaces’ abilities to model semantic relations, as determined by priming experiments. In the second experiment, we evaluate the representation capabilities of the LPP-reduced models on standard word space benchmarks. 4.1 Setup For the VSM-based word space, we consider three different weighting schemes: no weighting, TFIDF and the log-entropy (LE) used in (Landauer et al., 1998). For the WC-based word space, we use a 5 word sliding window. Due to the large parameter space for LPP models, we performed only a limited configuration search. An initial analysis using the 20 nearest neighbors and cosine similarity did not show significant performance differences when the number of dimensions was varied between 50 and 1000. We therefore selected 300 dimensions for all tests. Further work is needed to identify the impact of different parameters. Stop words were removed only for the WC+LPP model. We compare the LPP-based spaces to three models: VSM, HAL, and LSA. Two corpora </context>
<context position="15901" citStr="Landauer et al., 1998" startWordPosition="2618" endWordPosition="2621">two similarity values. 4.3 Experiment 2 We use six standard word space benchmarks to test our hypothesis that LPP can accurately capture general semantic knowledge and association based relations. The benchmarks come in two forms: word association and word choice tests. Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. To answer these questions, we select the option with the highest cosine similarity with the target. Three word choice synonymy benchmarks are used: the Test of English as a Foreign Language (TOEFL) test set from (Landauer et al., 1998), the English as a Second Language (ESL) test set from (Turney, 2001), and the Canadian Reader’s Digest Word Power (RDWP) from (Jarmasz and Szpakowicz, 2003). 4 Algorithm Corpus S1 S2 S3 VSM+LPP+LE TASA 0.457 0.413 0.255 VSM+LPP+TF-IDF TASA 0.464 0.390 0.207 VSM+LPP TASA 0.457 0.427 0.275 VSM+LPP Wiki 0.472 0.440 0.333 WC+LPP TASA 0.469 0.437 0.315 HAL TASA 0.485 0.434 0.310 HAL Wiki 0.462 0.406 0.266 LSA TASA 0.494 0.481 0.414 LSA Wiki 0.489 0.472 0.398 VSM TASA 0.484 0.460 0.407 Table 3: Experiment 1 results for normed priming. Word association tests measure the semantic relatedness of two w</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes, (25):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Henhi´ere</author>
</authors>
<date>2006</date>
<booktitle>Effects of High-Order Co-occurrences on Word Semantic Similarities. Current Psychology Letters,</booktitle>
<volume>1</volume>
<issue>18</issue>
<marker>Henhi´ere, 2006</marker>
<rawString>Benoit Lemaire, , and Guy Henhi´ere. 2006. Effects of High-Order Co-occurrences on Word Semantic Similarities. Current Psychology Letters, 1(18).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccrrence.</title>
<date>1996</date>
<journal>Behavoir Research Methods, Instruments &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="4545" citStr="Lund and Burgess, 1996" startWordPosition="707" endWordPosition="710">Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics The SVD has been used with great success on both models. Latent Semantic Analysis (LSA) (Landauer et al., 1998) extends the (VSM) by decomposing the space using the SVD and making the word space the left singular vectors, Uk. WC models have also utilized the SVD to improve performance (Sch¨utze, 1992; Bullinaria and Levy, 2007; Baroni and Lenci, 2008). Feature elimination reduces the dimensionality by removing those with low information content. This approach has been successfully applied to WC models such as HAL (Lund and Burgess, 1996) by dropping those with low entropy. This technique effectively removes the feature dimensions of high frequency words, which provide little discriminatory content. Randomized projections have also been successfully applied to VSM models, e.g. (Kanerva et al., 2000) and WC models, e.g. (Sahlgren et al., 2008). This reduction statistically approximates the original space in a much lower dimensional space. The projection does not take into account the structure of data, which provides only a computational benefit from fewer dimensions, unlike the previous two reductions. 3 Locality Preserving Pr</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccrrence. Behavoir Research Methods, Instruments &amp; Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
</authors>
<location>and</location>
<marker>Nelson, McEvoy, </marker>
<rawString>Douglas L. Nelson, Cathy L. McEvoy, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida word association, rhyme, and word fragment norms.</title>
<date>1998</date>
<note>http: //www.usf.edu/FreeAssociation/.</note>
<marker>Schreiber, 1998</marker>
<rawString>Thomas A. Schreiber. 1998. The University of South Florida word association, rhyme, and word fragment norms. http: //www.usf.edu/FreeAssociation/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-Based Construction of Semantic Space Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-Based Construction of Semantic Space Models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<pages>8--627</pages>
<contexts>
<context position="16811" citStr="Rubenstein and Goodenough (1965)" startWordPosition="2768" endWordPosition="2771">0.427 0.275 VSM+LPP Wiki 0.472 0.440 0.333 WC+LPP TASA 0.469 0.437 0.315 HAL TASA 0.485 0.434 0.310 HAL Wiki 0.462 0.406 0.266 LSA TASA 0.494 0.481 0.414 LSA Wiki 0.489 0.472 0.398 VSM TASA 0.484 0.460 0.407 Table 3: Experiment 1 results for normed priming. Word association tests measure the semantic relatedness of two words by comparing their similarity in the word space with human judgements. These tests are more precise than word choice tests because they take into account the specific value of the word similarity. Three word association benchmarks are used: the word similarity data set of Rubenstein and Goodenough (1965), the wordrelatedness data set of Finkelstein et al. (2002), and the antonymy data set of Deese (1964), which measures the degree to which high similarity captures the antonymy relationship. The Finkelstein et al. test is notable in that the human judges were free to score based on any word relationship. 5 Results and Discussion The LPP-based models show mixed performance in comparison to existing models on normed priming tasks, shown in Table 3. Adding LPP to the VSM decreased performance; however, when WIKI was used instead of TASA, the VSM+LPP model increased .15 on all correlations, wherea</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
<author>Anders Holst</author>
<author>Pentti Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08).</booktitle>
<contexts>
<context position="4855" citStr="Sahlgren et al., 2008" startWordPosition="755" endWordPosition="758">dels have also utilized the SVD to improve performance (Sch¨utze, 1992; Bullinaria and Levy, 2007; Baroni and Lenci, 2008). Feature elimination reduces the dimensionality by removing those with low information content. This approach has been successfully applied to WC models such as HAL (Lund and Burgess, 1996) by dropping those with low entropy. This technique effectively removes the feature dimensions of high frequency words, which provide little discriminatory content. Randomized projections have also been successfully applied to VSM models, e.g. (Kanerva et al., 2000) and WC models, e.g. (Sahlgren et al., 2008). This reduction statistically approximates the original space in a much lower dimensional space. The projection does not take into account the structure of data, which provides only a computational benefit from fewer dimensions, unlike the previous two reductions. 3 Locality Preserving Projection For a set of vectors, x1, x2,. . . , xn E Rm, LPP preserves the distance in the k-dimensional space, where k « m, by solving the following minimization problem, �min (w⊤xi − w⊤xj)2Sij (1) w ij where w is a transformation vector that projects x into the lower dimensional space, and S is a matrix that </context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>Magnus Sahlgren, Anders Holst, and Pentti Kanerva. 2008. Permutations as a means to encode order in word space. In Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="2469" citStr="Salton et al., 1975" startWordPosition="373" endWordPosition="376">t LPP learns in order to project the vectors into a Euclidean space. We measure the effects of using LPP on two basic word space models: the Vector Space Model and a Word Co-occurrence model. We begin with a brief overview of these word spaces and common dimensionality reduction techniques. We then formally introduce LPP. Following, we use two experiments to demonstrate LPP’s capacity to accurately dimensionally reduce word spaces. 2 Word Spaces and Reductions We consider two common word space models that have been used with dimensionality reduction. The first is the Vector Space Model (VSM) (Salton et al., 1975). Words are represented as vectors where each dimension corresponds to a document in the corpus and the dimension’s value is the number of times the word occurred in the document. We label the second model the Word Co-occurrence (WC) model: each dimension correspond to a unique word, with the dimension’s value indicating the number of times that dimension’s word co-occurred. Dimensionality reduction has been applied to both models for three kinds of benefits: to improve computational efficiency, to capture higher order relationships between words, and to reduce noise by smoothing or eliminatin</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing ’92,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of meaning. In Proceedings of Supercomputing ’92, pages 787– 796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Richard M Shiffrin</author>
<author>Douglas L Nelson</author>
</authors>
<title>Word association spaces for predicting semantic similarity effects in episodic memory.</title>
<date>2004</date>
<publisher>American Psychological Assocation.</publisher>
<contexts>
<context position="11077" citStr="Steyvers et al. (2004)" startWordPosition="1821" endWordPosition="1824">y with an increased priming response. We use data from two types of priming experiments to measure whether LPP models better correlate with human performance than non-LPP word spaces. Normed Priming Nelson et al. (1998) collected free association responses to 5,019 prime words. An average of 149 participants responded to each prime with the first word that came to mind. Based on this dataset, we introduce a new benchmark that correlates word space similarity with the associative strength of semantic priming pairs. We use three measures for modeling primetarget strength, which were inspired by Steyvers et al. (2004). Let Wab be the percentage of participants who responded to prime a with target b. The three measures of associative strength are S1ab = Wab S2ab = Wab + Wba 3 2 2 Sab _ — 5ab + �c SacSc2b These measure three different levels of semantic relatedness between words a and b. S1ab measures the relationship from a to b, which is frequently asymmetric due to ordering, e.g. “orange” produces “juice” more frequently than “juice” produces “orange.” S2 ab measures the symmetric association between a and b; Steyvers et al. (2004) note that this may better model the associative strength by including weak</context>
</contexts>
<marker>Steyvers, Shiffrin, Nelson, 2004</marker>
<rawString>Mark Steyvers, Richard M. Shiffrin, and Douglas L. Nelson, 2004. Word association spaces for predicting semantic similarity effects in episodic memory. American Psychological Assocation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua B Tenenbaum</author>
<author>Vin de Silva</author>
<author>John C Langford</author>
</authors>
<title>A global geometric framework for nonlinear dimensionality reduction.</title>
<date>2000</date>
<journal>Science,</journal>
<volume>290</volume>
<issue>5500</issue>
<marker>Tenenbaum, de Silva, Langford, 2000</marker>
<rawString>Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<contexts>
<context position="15970" citStr="Turney, 2001" startWordPosition="2632" endWordPosition="2633">rks to test our hypothesis that LPP can accurately capture general semantic knowledge and association based relations. The benchmarks come in two forms: word association and word choice tests. Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. To answer these questions, we select the option with the highest cosine similarity with the target. Three word choice synonymy benchmarks are used: the Test of English as a Foreign Language (TOEFL) test set from (Landauer et al., 1998), the English as a Second Language (ESL) test set from (Turney, 2001), and the Canadian Reader’s Digest Word Power (RDWP) from (Jarmasz and Szpakowicz, 2003). 4 Algorithm Corpus S1 S2 S3 VSM+LPP+LE TASA 0.457 0.413 0.255 VSM+LPP+TF-IDF TASA 0.464 0.390 0.207 VSM+LPP TASA 0.457 0.427 0.275 VSM+LPP Wiki 0.472 0.440 0.333 WC+LPP TASA 0.469 0.437 0.315 HAL TASA 0.485 0.434 0.310 HAL Wiki 0.462 0.406 0.266 LSA TASA 0.494 0.481 0.414 LSA Wiki 0.489 0.472 0.398 VSM TASA 0.484 0.460 0.407 Table 3: Experiment 1 results for normed priming. Word association tests measure the semantic relatedness of two words by comparing their similarity in the word space with human judge</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<pages>84--327</pages>
<contexts>
<context position="1409" citStr="Tversky, 1977" startWordPosition="205" endWordPosition="206">er the original models on benchmarks. 1 Introduction Vector space models of semantics frequently employ some form of dimensionality reduction for improvement in representations or computational overhead. Many of the dimensionality reduction algorithms assume that the unreduced word space is linear. However, word similarities have been shown to exhibit many non-metric properties: asymmetry, e.g North Korea is more similar to Red China than Red China is to North Korea, and non-transitivity, e.g. Cuba is similar the former USSR, Jamaica is similar to Cuba, but Jamaica is not similar to the USSR (Tversky, 1977). We hypothesize that a non-linear word space model might more accurately preserve these non-metric relationships. To test our hypothesis, we capture the nonlinear structure with dimensionality reduction by using Locality Preserving Projection (LPP) (He and Niyogi, 2003), an efficient, linear approximation of Eigenmaps (Belkin and Niyogi, 2002). With this reduction, the word space vectors are assumed to exist on a nonlinear manifold that LPP learns in order to project the vectors into a Euclidean space. We measure the effects of using LPP on two basic word space models: the Vector Space Model </context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Amos Tversky. 1977. Features of similarity. Psychological Review, 84:327–352.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>