<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.979012">
Syntactic and Semantic Kernels for Short Text Pair Categorization
</title>
<author confidence="0.997796">
Alessandro Moschitti
</author>
<affiliation confidence="0.9982585">
Department of Computer Science and Engineering
University of Trento
</affiliation>
<address confidence="0.952293">
Via Sommarive 14
38100 POVO (TN) - Italy
</address>
<email confidence="0.999313">
moschitti@disi.unitn.it
</email>
<sectionHeader confidence="0.996664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883">
Automatic detection of general relations
between short texts is a complex task that
cannot be carried out only relying on lan-
guage models and bag-of-words. There-
fore, learning methods to exploit syntax
and semantics are required. In this pa-
per, we present a new kernel for the repre-
sentation of shallow semantic information
along with a comprehensive study on ker-
nel methods for the exploitation of syntac-
tic/semantic structures for short text pair
categorization. Our experiments with Sup-
port Vector Machines on question/answer
classification show that our kernels can be
used to greatly improve system accuracy.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947438596492">
Previous work on Text Categorization (TC) has
shown that advanced linguistic processing for doc-
ument representation is often ineffective for this
task, e.g. (Lewis, 1992; Furnkranz et al., 1998;
Allan, 2000; Moschitti and Basili, 2004). In con-
trast, work in question answering suggests that
syntactic and semantic structures help in solving
TC (Voorhees, 2004; Hickl et al., 2006). From
these studies, it emerges that when the categoriza-
tion task is linguistically complex, syntax and se-
mantics may play a relevant role. In this perspec-
tive, the study of the automatic detection of re-
lationships between short texts is particularly in-
teresting. Typical examples of such relations are
given in (Giampiccolo et al., 2007) or those hold-
ing between question and answer, e.g. (Hovy et
al., 2002; Punyakanok et al., 2004; Lin and Katz,
2003), i.e. if a text fragment correctly responds to
a question.
In Question Answering, the latter problem is
mostly tackled by using different heuristics and
classifiers, which aim at extracting the best an-
swers (Chen et al., 2006; Collins-Thompson et
al., 2004). However, for definitional questions, a
more effective approach would be to test if a cor-
rect relationship between the answer and the query
holds. This, in turns, depends on the structure of
the two text fragments. Designing language mod-
els to capture such relation is too complex since
probabilistic models suffer from (i) computational
complexity issues, e.g. for the processing of large
bayesian networks, (ii) problems in effectively es-
timating and smoothing probabilities and (iii) high
sensitiveness to irrelevant features and processing
errors. In contrast, discriminative models such as
Support Vector Machines (SVMs) have theoreti-
cally been shown to be robust to noise and irrele-
vant features (Vapnik, 1995). Thus, partially cor-
rect linguistic structures may still provide a rel-
evant contribution since only the relevant infor-
mation would be taken into account. Moreover,
such a learning approach supports the use of kernel
methods which allow for an efficient and effective
representation of structured data.
SVMs and Kernel Methods have recently been
applied to natural language tasks with promising
results, e.g. (Collins and Duffy, 2002; Kudo and
Matsumoto, 2003; Cumby and Roth, 2003; Shen
et al., 2003; Moschitti and Bejan, 2004; Culotta
and Sorensen, 2004; Kudo et al., 2005; Toutanova
et al., 2004; Kazama and Torisawa, 2005; Zhang
et al., 2006; Moschitti et al., 2006). In particular,
in question classification, tree kernels, e.g. (Zhang
and Lee, 2003), have shown accuracy comparable
to the best models, e.g. (Li and Roth, 2005).
Moreover, (Shen and Lapata, 2007; Moschitti
et al., 2007; Surdeanu et al., 2008; Chali and Joty,
</bodyText>
<note confidence="0.9229665">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99745">
576
</page>
<bodyText confidence="0.999769769230769">
2008) have shown that shallow semantic informa-
tion in the form of Predicate Argument Structures
(PASs) (Jackendoff, 1990; Johnson and Fillmore,
2000) improves the automatic detection of cor-
rect answers to a target question. In particular,
in (Moschitti et al., 2007) kernels for the process-
ing of PASs (in PropBank1 format (Kingsbury and
Palmer, 2002)) extracted from question/answer
pairs were proposed. However, the relatively high
kernel computational complexity and the limited
improvement on bag-of-words (BOW) produced
by this approach do not make the use of such tech-
nique practical for real world applications.
In this paper, we carry out a complete study on
the use of syntactic/semantic structures for rela-
tional learning from questions and answers. We
designed sequence kernels for words and Part of
Speech Tags which capture basic lexical seman-
tics and basic syntactic information. Then, we de-
sign a novel shallow semantic kernel which is far
more efficient and also more accurate than the one
proposed in (Moschitti et al., 2007).
The extensive experiments carried out on two
different corpora of questions and answers, de-
rived from Web documents and the TREC corpus,
show that:
</bodyText>
<listItem confidence="0.8971302">
• Kernels based on PAS, POS-tag sequences and
syntactic parse trees improve the BOW approach
on both datasets. On the TREC data the improve-
ment is interestingly high, e.g. about 61%, making
its application worthwhile.
• The new kernel for processing PASs is more ef-
ficient and effective than previous models so that
it can be practically used in systems for short text
pair categorization, e.g. question/answer classifi-
cation.
</listItem>
<bodyText confidence="0.999939166666667">
In the remainder of this paper, Section 2
presents well-known kernel functions for struc-
tural information whereas Section 3 describes our
new shallow semantic kernel. Section 4 reports
on our experiments with the above models and, fi-
nally, a conclusion is drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.948603" genericHeader="introduction">
2 String and Tree Kernels
</sectionHeader>
<bodyText confidence="0.998119833333333">
Feature design, especially for modeling syntactic
and semantic structures, is one of the most dif-
ficult aspects in defining a learning system as it
requires efficient feature extraction from learning
objects. Kernel methods are an interesting rep-
resentation approach as they allow for the use of
</bodyText>
<footnote confidence="0.855633">
1www.cis.upenn.edu/˜ace
</footnote>
<bodyText confidence="0.996597333333333">
all object substructures as features. In this per-
spective, String Kernel (SK) proposed in (Shawe-
Taylor and Cristianini, 2004) and the Syntactic
Tree Kernel (STK) (Collins and Duffy, 2002) al-
low for modeling structured data in high dimen-
sional spaces.
</bodyText>
<subsectionHeader confidence="0.998969">
2.1 String Kernels
</subsectionHeader>
<bodyText confidence="0.915759586206896">
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. Gaps modify the
weight associated with the target substrings as
shown in the following.
Let E be a finite alphabet, E* = U&apos;n=0 En is the
set of all strings. Given a string s E E*, |s |denotes
the length of the strings and si its compounding
symbols, i.e s = s1..s|s|, whereas s[i : j] selects
the substring sisi+1..sj_1sj from the i-th to the
j-th character. u is a subsequence of s if there
is a sequence of indexes I = (i1, ..., i|u|), with
1 &lt; i1 &lt; ... &lt; i|u |&lt; |s|, such that u = si1..si|u|
or u = s[I] for short. d(I) is the distance between
the first and last character of the subsequence u in
s, i.e. d(I) = i|u |− i1 + 1. Finally, given s1, s2
E E*, s1s2 indicates their concatenation.
The set of all substrings of a text corpus forms a
feature space denoted by F = {u1, u2, ..} C E*.
To map a string s in R&apos; space, we can use the
following functions: φu(s) = ��I:u=s[�I] λd(l� for
some A &lt; 1. These functions count the num-
ber of occurrences of u in the string s and assign
them a weight Ad(I) proportional to their lengths.
Hence, the inner product of the feature vectors for
two strings s1 and s2 returns the sum of all com-
mon subsequences weighted according to their
frequency of occurrences and lengths, i.e.
</bodyText>
<equation confidence="0.99962">
ESK(s1, s2) = Eφu(s1)·φu(s2) = E λd(�I1)
u∈Σ* u∈Σ* z1:u=s1[I1]
E λd(�I2) = E E E λd(�I1)+d(�I2),
I2:u=t[�I2] u∈Σ* I1:u=s1[I1] I2:u=t[I2]
</equation>
<bodyText confidence="0.999944333333333">
where d(.) counts the number of characters in the
substrings as well as the gaps that were skipped in
the original string.
</bodyText>
<subsectionHeader confidence="0.999011">
2.2 Syntactic Tree Kernel (STK)
</subsectionHeader>
<bodyText confidence="0.99959975">
Tree kernels compute the number of common sub-
structures between two trees T1 and T2 without
explicitly considering the whole fragment space.
Let F = {f1, f2, ... , f|F|} be the set of tree
</bodyText>
<page confidence="0.997242">
577
</page>
<figureCaption confidence="0.999032">
Figure 1: A tree for the sentence ”Anxiety is a disease” with some of its syntactic tree fragments.
</figureCaption>
<figure confidence="0.999006890625">
VP
⇒
NP
N
a disease
VP
NP
N
a disease
VP
VBZ NP
D
NP
is D N
disease
NP
is D N
VP
NP VBZ NP
NP
D N
a disease
D N
a disease ...
NP
NNP
Anxiety
S
VBZ
is
VBZ
is
VP
VBZ
is
NP
NNP
Anxiety
NNP
Anxiety
VBZ
is
D
D
N
a disease
VP
VBZ
VP
VBZ
PAS PAS
(a) (b)
A1
Disorder
rel
characterize
A0
fear
R-A0
that
A1
anxiety
rel
causes
</figure>
<bodyText confidence="0.9754525">
fragments and χi(n) be an indicator function,
equal to 1 if the target fi is rooted at node n
and equal to 0 otherwise. A tree kernel func-
ttiio`n over T1 and T2 is deÞned as TK(T1,T2) =
</bodyText>
<equation confidence="0.788432">
�+n1∈NT1 �n2∈NT2 Δ(n1, n2), where NT1 and
NT2 are the sets of nodes in T1 and T2, respec-
tively and Δ(n1, n2) = �|F|
i=1 χi(n1)χi(n2).
</equation>
<bodyText confidence="0.891796">
Δ function counts the number of subtrees
rooted in n1 and n2 and can be evaluated as fol-
lows (Collins and Duffy, 2002):
</bodyText>
<listItem confidence="0.982797428571429">
1. if the productions at n1 and n2 are different then
A(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then A(n1, n2) = λ;
3. if the productions at n1 and n2 are the same, and
n1 and n2 are not pre-terminals then A(n1, n2) =
</listItem>
<equation confidence="0.9878315">
λ Il(n1)
j=1 (1 + A(cn1(j), cn2(j))), where l(n1) is the
</equation>
<bodyText confidence="0.999813444444445">
number of children of n1, cn(j) is the j-th child
of node n and A is a decay factor penalizing larger
structures.
Figure 1 shows some fragments of the subtree
on the left part. These satisfy the constraint that
grammatical rules cannot be broken. For exam-
ple, [VP [VBZ NP]] is a valid fragment which has
two non-terminal symbols, VBZ and NP, as leaves
whereas [VP [VBZ]] is not a valid feature.
</bodyText>
<sectionHeader confidence="0.915848" genericHeader="method">
3 Shallow Semantic Kernels
</sectionHeader>
<bodyText confidence="0.999662533333333">
The extraction of semantic representations from
text is a very complex task. For it, tradition-
ally used models are based on lexical similarity
and tends to neglect lexical dependencies. Re-
cently, work such as (Shen and Lapata, 2007; Sur-
deanu et al., 2008; Moschitti et al., 2007; Mos-
chitti and Quarteroni, 2008; Chali and Joty, 2008),
uses PAS to consider such dependencies but only
the latter three researches attempt to completely
exploit PAS with Shallow Semantic Tree Kernels
(SSTKs). Unfortunately, these kernels result com-
putational expensive for real world applications.
In the remainder of this section, we present our
new kernel for PASs and compare it with the pre-
vious SSTK.
</bodyText>
<figureCaption confidence="0.9956894">
Figure 2: Predicate Argument Structure trees associated
with the sentence: ”Panic disorder is characterized by unex-
pected and intense fear that causes anxiety.”.
Figure 3: Some of the tree substructures useful to capture
shallow semantic properties.
</figureCaption>
<subsectionHeader confidence="0.99925">
3.1 Shallow Semantic Structures
</subsectionHeader>
<bodyText confidence="0.997545551724138">
Shallow approaches to semantic processing are
making large strides in the direction of efÞciently
and effectively deriving tacit semantic informa-
tion from text. Large data resources annotated
with levels of semantic information, such as in the
FrameNet (Johnson and Fillmore, 2000) and Prop-
Bank (PB) (Kingsbury and Palmer, 2002) projects,
make it possible to design systems for the auto-
matic extraction of predicate argument structures
(PASs) (Carreras and M`arquez, 2005). PB-based
systems produce sentence annotations like:
[A1 Panic disorder] is [rel characterized] [A0 by unexpected
and intense fear] [R−A0 that] [relcauses] [A1 anxiety].
A tree representation of the above semantic in-
formation is given by the two PAS trees in Fig-
ure 2, where the argument words are replaced by
the head word to reduce data sparseness. Hence,
the semantic similarity between sentences can be
measured in terms of the number of substructures
between the two trees. The required substructures
violate the STK constraint (about breaking pro-
duction rules), i.e. since we need any set of nodes
linked by edges of the initial tree. For example,
interesting semantic fragments of Figure 2.a are
shown in Figure 3.
Unfortunately, STK applied to PAS trees cannot
generate such fragments. To overcome this prob-
lem, a Shallow Semantic Tree Kernel (SSTK) was
designed in (Moschitti et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999175">
3.2 Shallow Semantic Tree Kernel (SSTK)
</subsectionHeader>
<bodyText confidence="0.996436">
SSTK is obtained by applying two different steps:
Þrst, the PAS tree is transformed by adding a layer
</bodyText>
<figure confidence="0.996813117647059">
rel
A0
PAS
rel
characterize
PAS
rel
characterize
PAS
A1 rel A0
PAS
A1 rel
characterize
PAS
characterize
A0
fear
</figure>
<page confidence="0.985394">
578
</page>
<bodyText confidence="0.9789500625">
of SLOT nodes as many as the number of possi-
ble argument types, where each slot is assigned to
an argument following a fixed ordering (e.g. rel,
A0, A1, A2, ...). For example, if an A1 is found
in the sentence annotation it will be always posi-
tioned under the third slot. This is needed to “arti-
ficially” allow SSTK to generate structures con-
taining subsets of arguments. For example, the
tree in Figure 2.a is transformed into the first tree
of Fig. 4, where ”null” just states that there is no
corresponding argument type.
Second, to discard fragments only containing
slot nodes, in the STK algorithm, a new step 0 is
added and the step 3 is modified (see Sec. 2.2):
0. if n1 (or n2) is a pre-terminal node and its child
label is null, Δ(n1, n2) = 0;
</bodyText>
<equation confidence="0.9997025">
3. Δ(n1, n2) = Ql(n1)
j=1 (1 + Δ(cn1(j), cn2(j))) − 1.
</equation>
<bodyText confidence="0.999858666666666">
For example, Fig. 4 shows the fragments gen-
erated by SSTK. The comparison with the ideal
fragments in Fig. 3 shows that SSTK well approx-
imates the semantic features needed for the PAS
representation. The computational complexity of
SSTK is O(n2), where n is the number of the PAS
nodes (leaves excluded). Considering that the tree
including all the PB arguments contains 52 slot
nodes, the computation becomes very expensive.
To overcome this drawback, in the next section,
we propose a new kernel to efficiently process PAS
trees with no addition of slot nodes.
</bodyText>
<subsectionHeader confidence="0.999184">
3.3 Semantic Role Kernel (SRK)
</subsectionHeader>
<bodyText confidence="0.9979965">
The idea of SRK is to produce all child subse-
quences of a PAS tree, which correspond to se-
quences of predicate arguments. For this purpose,
we can use a string kernel (SK) (see Section 2.1)
for which efficient algorithms have been devel-
oped. Once a sequence of arguments is output by
SK, for each argument, we account for the poten-
tial matches of its children, i.e. the head of the
argument (or more in general the argument word
sequence).
More formally, given two sequences of argu-
ment nodes, s1 and s2, in two PAS trees and
considering the string kernel in Sec 2.1, the
SRK(s1, s2) is defined as:
</bodyText>
<equation confidence="0.993632666666667">
E H (1 + σ(s1[$I1l], s2[$I2l]))λd(�I1)+d(�I2), (1)
z1:u=s1[ Ii] l=1..|u|
j2:u=s2[�I2]
</equation>
<bodyText confidence="0.9753495625">
where u is any subsequence of argument nodes,
I1 is the index of the l-th argument node, s[I1] is
the corresponding argument node in the sequence
s and Q(s1[Iil], s2[�I2l]) is 1 if the heads of the ar-
guments are identical, otherwise is 0.
Proposition 1 SRK computes the number of all
possible tree substructures shared by the two eval-
uating PAS trees, where the considered substruc-
tures of a tree T are constituted by any set of nodes
(at least two) linked by edges of T.
Proof The PAS trees only contain three node lev-
els and, according to the proposition’s thesis, sub-
structures contain at least two nodes. The num-
ber of substructures shared by two trees, T1 and
T2, constituted by the root node (PAS) and the
subsequences of argument nodes is evaluated by
</bodyText>
<equation confidence="0.552857">
Er1:u=s1[�I1],�I2:u=s2[�I2] Ad(
</equation>
<bodyText confidence="0.9795156">
Given a node in a shared subsequence u, its child
(i.e. the head word) can be both in T1 and T2,
originating two different shared structures (with
or without such head node). The matches on the
heads (for each shared node of u) are combined
together generating different substructures. Thus
the number of substructures originating from u is
the product, lll=1..|u|(1+Q(s1[4l], s2[�I2l])). This
number multiplied by all shared subsequences
leads to Eq. 1. ✷
We can efficiently compute SRK by following a
similar approach to the string kernel evaluation in
(Shawe-Taylor and Cristianini, 2004) by defining
the following dynamic matrix:
λk−i+l−r × γp−1(s1[1 : i], s2[1 : r]),
where -yp(s1, s2) counts the number of shared sub-
structures of exactly p argument nodes between s1
and s2 and again, s[1 : i] indicates the sequence
portion from argument 1 to i. The above matrix is
then used to evaluate -yp(s1a, s2b) =
</bodyText>
<equation confidence="0.993886666666667">
f λ2(1 + σ(h(a), h(b)))Dp(|s1|, |s2|) if a = b;
SRK(s1,s2) =
�I2) (when A = 1).
�I1)+d(
Ek
i=1
Dp(k, l) =
El
r=1
Em
p=1
0 otherwise.
</equation>
<bodyText confidence="0.984460333333333">
where
and s2b indicate the concatenation of
the sequences s and t with the argument nodes, a
and b, respectively and Q(h(a), h(b)) is 1 if the
children of a and b are identical (e.g. same head).
The interesting property is that:
</bodyText>
<equation confidence="0.500354333333333">
l) =
: k],
: l]) +
l
1)
+
1, l)
1, l
1). (4)
</equation>
<bodyText confidence="0.537115">
To obtain the final kernel, we need to con-
sider all possible subsequence lengths. Let
m be the minimum between
and
</bodyText>
<equation confidence="0.652708416666667">
s1a
Dp(k,
γp−1(s1[1
s2[1
λDp(k,
−
λDp(k−
−λ2Dp(k−
−
|s1|
|s2|,
γp(s1, s2).
</equation>
<page confidence="0.975734">
579
</page>
<figureCaption confidence="0.999576">
Figure 4: Fragments of Fig. 2.a produced by the SSTK (similar to those of Fig. 3).
</figureCaption>
<figure confidence="0.997264307692307">
PAS
PAS
PAS
PAS
. . .
. . .
. . .
. . .
SLOT
null
SLOT
A1
SLOT
A1
SLOT
A0
SLOT
rel
SLOT
null
SLOT
null
SLOT
null
SLOT
A0
fear
*
SLOT
rel
characterize
*
*
PAS
. . .
SLOT
A0
fear
SLOT
A1
disorder
SLOT
null
SLOT
rel
characterize
SLOT
rel
characterize
SLOT
rel
characterize
</figure>
<bodyText confidence="0.9340025">
Regarding the processing time, if p is the max-
imum number of arguments in a predicate struc-
ture, the worst case computational complexity of
SRK is O(p3).
</bodyText>
<subsectionHeader confidence="0.981161">
3.4 SRK vs. SSTK
</subsectionHeader>
<bodyText confidence="0.999369891891892">
A comparison between SSTK and SRK suggests
the following points: first, although the computa-
tional complexity of SRK is larger than the one
of SSTK, we will show in the experiment section
that the running time (for both training and test-
ing) is much lower. The worse case is not really
informative since as shown in (Moschitti, 2006),
we can design fast algorithm with a linear average
running time (we use such algorithm for SSTK).
Second, although SRK uses trees with only
three levels, in Eq.1, the function Q (defined to
give 1 or 0 if the heads match or not) can be sub-
stituted by any kernel function. Thus, Q can re-
cursively be an SRK (and evaluate Nested PASs
(Moschitti et al., 2007)) or any other potential ker-
nel (over the arguments). The very interesting as-
pect is that the efficient algorithm that we provide
(Eqs 2, 3 and 4) can be accordingly modified to
efficiently evaluate new kernels obtained with the
Q substitution2.
Third, the interesting difference between SRK
and SSTK (in addition to efficiency) is that SSTK
requires an ordered sequence of arguments to eval-
uate the number of argument subgroups (argu-
ments are sorted before running the kernel). This
means that the natural order is lost. SRK instead is
based on subsequence kernels so it naturally takes
into account the order which is very important:
without it, syntactic/semantic properties of pred-
icates cannot be captured, e.g. passive and active
forms have the same argument order for SSTK.
Finally, SRK gives a weight to the predicate
substructures by considering their length, which
also includes gaps, e.g. the sequence (A0, A1) is
more similar to (A0, A1) than (A0, A-LOC, A1),
in turn, the latter produces a heavier match than
(A0, A-LOC, A2, A1) (please see Section 2.1).
</bodyText>
<footnote confidence="0.816668">
2For space reasons we cannot discuss it here.
</footnote>
<bodyText confidence="0.9697555">
This is another important property for modeling
shallow semantics similarity.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999946666666667">
Our experiments aim at studying the impact of our
kernels applied to syntactic/semantic structures for
the detection of relations between short texts. In
particular, we first show that our SRK is far more
efficient and effective than SSTK. Then, we study
the impact of the above kernels as well as se-
quence kernels based on words and Part of Speech
Tags and tree kernels for the classification of ques-
tion/answer text pairs.
</bodyText>
<subsectionHeader confidence="0.987779">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.981694620689655">
The task used to test our kernels is the classifi-
cation of the correctness of (q, a) pairs, where a
is an answer for the query q. The text pair ker-
nel operates by comparing the content of ques-
tions and the content of answers in a separate fash-
ion. Thus, given two pairs p1 = (q1, a1) and
p2 = (q2, a2), a kernel function is defined as
K(p1,p2) = ET KT(q1, q2) + ET KT(a1, a2),
where T varies across different kernel functions
described hereafter.
As a basic kernel machine, we used our
SVM-Light-TK toolkit, available at disi.unitn.
it/moschitti (which is based on SVM-Light
(Joachims, 1999) software). In it, we imple-
mented: the String Kernel (SK), the Syntactic Tree
Kernel (STK), the Shallow Semantic Tree Kernel
(SSTK) and the Semantic Role Kernel (SRK) de-
scribed in sections 2 and 3. Each kernel is associ-
ated with the above linguistic objects: (i) the linear
kernel is used with the bag-of-words (BOW) or the
bag-of-POS-tags (POS) features. (ii) SK is used
with word sequences (i.e. the Word Sequence Ker-
nel, WSK) and POS sequences (i.e. the POS Se-
quence Kernel, PSK). (iii) STK is used with syn-
tactic parse trees automatically derived with Char-
niak’s parser; (iv) SSTK and SRK are applied to
two different PAS trees (see Section 3.1), automat-
ically derived with our SRL system.
It is worth noting that, since answers often con-
</bodyText>
<page confidence="0.981692">
580
</page>
<figure confidence="0.993515722222222">
240
220
200
180
160
140
120
100
80
SRK (training) SRK (test)
60
SSTK (test) SSTK (training)
40
20
0
200 400 600 800 1000 1200 1400 1600 1800
Training Set Size
Time in Seconds
</figure>
<figureCaption confidence="0.999992">
Figure 5: Efficiency of SRK and SSTK
</figureCaption>
<bodyText confidence="0.9990365">
tain more than one PAS, we applied SRK or SSTK
to all pairs P1 × P2 and sum the obtained contri-
bution, where P1 and P2 are the set of PASs of the
first and second answer3. Although different ker-
nels can be used for questions and for answers, we
used (and summed together) the same kernels ex-
cept for those based on PASs, which are only used
on answers.
</bodyText>
<sectionHeader confidence="0.48893" genericHeader="evaluation">
4.1.1 Datasets
</sectionHeader>
<bodyText confidence="0.9999762">
To train and test our text QA classifiers, we
adopted the two datasets of question/answer pairs
available at disi.unitn.it/˜silviaq, contain-
ing answers to only definitional questions. The
datasets are based on the 138 TREC 2001 test
questions labeled as “description” in (Li and Roth,
2005). Each question is paired with all the top
20 answer paragraphs extracted by two basic QA
systems: one trained with the web documents and
the other trained with the AQUAINT data used in
TREC’07.
The WEB corpus (Moschitti et al., 2007) of QA
pairs contains 1,309 sentences, 416 of which are
positive4 answers whereas the TREC corpus con-
tains 2,256 sentences, 261 of which are positive.
</bodyText>
<subsubsectionHeader confidence="0.477446">
4.1.2 Measures and Parameterization
</subsubsectionHeader>
<bodyText confidence="0.9999812">
The accuracy of the classifiers is provided by the
average F1 over 5 different samples using 5-fold
cross-validation whereas each plot refers to a sin-
gle fold. We carried out some preliminary experi-
ments of the basic kernels on a validation set and
</bodyText>
<footnote confidence="0.981899875">
3More formally, let Pt and Pt be the sets of PASs ex-
tracted from text fragments t and t$; the resulting kernel will
be Kall (Pt,Pt,) _ Pp∈Pt Pp,∈ SRK (p, p$).
4For instance, given the question “What are inverte-
brates?”, the sentence “At least 99% of all animal species
are invertebrates, comprising ...” was labeled “-1” , while
“Invertebrates are animals without backbones.” was labeled
“+1”.
</footnote>
<bodyText confidence="0.998903428571429">
we noted that the F1 was maximized by using the
default cost parameters (option -c of SVM-Light),
λ = 0.04 (see Section 2). The trade-off parame-
ter varied according to different kernels on WEB
data (so it needed an ad-hoc estimation) whereas
a value of 10 was optimal for any kernel on the
TREC corpus.
</bodyText>
<subsectionHeader confidence="0.99884">
4.2 Shallow Semantic Kernel EfÞciency
</subsectionHeader>
<bodyText confidence="0.999985363636364">
Section 2 has illustrated that SRK is applied to
more compact PAS trees than SSTK, which runs
on large structures containing as many slots as
the number of possible predicate argument types.
This impacts on the memory occupancy as well
as on the kernel computation speed. To empiri-
cally verify our analytical findings (Section 3.3),
we divided the training (TREC) data in 9 bins of
increasing size (200 instances between two con-
tiguous bins) and we measured the learning and
test time5 for each bin. Figure 5 shows that in
both the classification and learning phases, SRK
is much faster than SSTK. With all training data,
SSTK employs 487.15 seconds whereas SRK only
uses 12.46 seconds, i.e. it is about 40 times faster,
making the experimentation of SVMs with large
datasets feasible. It is worth noting that to imple-
ment SSTK we used the fast version of STK and
that, although the PAS trees are smaller than syn-
tactic trees, they may still contain more than half
million of substructures (when they are formed by
seven arguments).
</bodyText>
<subsectionHeader confidence="0.9094905">
4.3 Results for Question/Answer
ClassiÞcation
</subsectionHeader>
<bodyText confidence="0.999672">
In these experiments, we tested different kernels
and some of their most promising combinations,
which are simply obtained by adding the different
kernel contributions6 (this yields the joint feature
space of the individual kernels).
Table 1 shows the average F1 ± the standard de-
viation7 over 5-folds on Web (and TREC) data of
SVMs using different kernels. We note that: (a)
BOW achieves very high accuracy, comparable to
the one produced by STK, i.e. 65.3 vs 65.1; (b)
the BOW+STK combination achieves 66.0, im-
</bodyText>
<footnote confidence="0.992132">
5Processing time in seconds of a Mac-Book Pro 2.4 Ghz.
6All adding kernels are normalized to have a sim-
ilarity score between 0 and 1, i.e. K$(X1, X2) _
K(X1,X2)
√K(X1,X1)×K(X2,X2).
7The Std. Dev. of the difference between two classifier
F1s is much lower making statistically significant almost all
our system ranking in terms of performance.
</footnote>
<page confidence="0.986996">
581
</page>
<table confidence="0.9508034">
WEB Corpus
BOW POS PSK WSK STK SSTK SRK BOW+POS BOW+STK PSK+STK WSK+STK STK+SSTK STK+SRK
65.3±2.9 56.8±0.8 62.5±2.3 65.7±6.0 65.1±3.9 52.9±1.7 50.8±1.2 63.7±1.6 66.0±2.7 65.3±2.4 66.6±3.0 (+WSK) 68.0±2.7 (+WSK) 68.2±4.3
TREC Corpus
24.2±5.0 26.5±7.9 31.6±6.8 14.0±4.2 33.1±3.8 21.8±3.7 23.6±4.7 31.9±7.1 30.3±4.1 36.4±7.0 23.7±3.9 (+PSK) 37.2±6.9 (+PSK) 39.1±7.3
</table>
<tableCaption confidence="0.976261">
Table 1: F1 ± Std. Dev. of the question/answer classifier according to several kernels on the WEB and
TREC corpora.
</tableCaption>
<bodyText confidence="0.9997188">
proving both BOW and STK; (c) WSK (65.7) im-
proves BOW and it is enhanced by WSK+STK
(66.6), demonstrating that word sequences and
STKs are very relevant for this task; and fi-
nally, WSK+STK+SSTK is slightly improved by
WSK+STK+SRK, 68.0% vs 68.2% (not signifi-
cantly) and both improve on WSK+STK.
The above findings are interesting as the syntac-
tic information provided by STK and the semantic
information brought by WSK and SRK improve
on BOW. The high accuracy of BOW is surprising
if we consider that at classification time, instances
of the training models (e.g. support vectors) are
compared with different test examples since ques-
tions cannot be shared between training and test
set8. Therefore the answer words should be dif-
ferent and useless to generalize rules for answer
classification. However, error analysis reveals that
although questions are not shared between train-
ing and test set, there are common words in the
answers due to typical Web page patterns which
indicate if a retrieved passage is an incorrect an-
swer, e.g. Learn more about X.
Although the ability to detect these patterns is
beneficial for a QA system as it improves its over-
all accuracy, it is slightly misleading for the study
that we are carrying out. Thus, we experimented
with the TREC corpus which does not contain
Web extra-linguistic texts and it is more complex
from a QA task viewpoint (it is more difficult to
find a correct answer).
Table 1 also shows the classification results on
the TREC dataset. A comparative analysis sug-
gests that: (a) the F1 of all models is much lower
than for the Web dataset; (b) BOW shows the low-
est accuracy (24.2) and also the accuracy of its
combination with STK (30.3) is lower than the
one of STK alone (33.1); (c) PSK (31.6) improves
POS (26.5) information and PSK+STK (36.4) im-
proves on PSK and STK; and (d) PAS adds further
</bodyText>
<footnote confidence="0.69445925">
8Sharing questions between test and training sets would
be an error from a machine learning viewpoint as we cannot
expect new questions to be identical to those in the training
set.
</footnote>
<bodyText confidence="0.9997964">
information as the best model is PSK+STK+SRK,
which improves BOW from 24.2 to 39.1, i.e. 61%.
Finally, it is worth noting that SRK provides a
higher improvement (39.1-36.4) than SSTK (37.2-
36.4).
</bodyText>
<subsectionHeader confidence="0.994687">
4.4 Precision/Recall Curves
</subsectionHeader>
<bodyText confidence="0.999980444444445">
To better study the benefit of the proposed linguis-
tic structures, we also plotted the Precision/Recall
curves (one fold for each corpus). Figure 6 shows
the curve of some interesting kernels applied to
the Web dataset. As expected, BOW shows the
lowest curves, although, its relevant contribution
is evident. STK improves BOW since it pro-
vides a better model generalization by exploit-
ing syntactic structures. Also, WSK can gener-
ate a more accurate model than BOW since it uses
n-grams (with gaps) and when it is summed to
STK, a very accurate model is obtained9. Finally,
WSK+STK+SRK improves all the models show-
ing the potentiality of PASs.
Such curves show that there is no superior
model. This is caused by the high contribution
of BOW, which de-emphasize all the other mod-
els’s result. In this perspective, the results on
TREC are more interesting as shown by Figure 7
since the contribution of BOW is very low making
the difference in accuracy with the other linguis-
tic models more evident. PSK+STK+SRK, which
encodes the most advanced syntactic and semantic
information, shows a very high curve which out-
performs all the others.
The analysis of the above results suggests that:
first as expected, BOW does not prove very rel-
evant to capture the relations between short texts
from examples. In the QA classification, while
BOW is useful to establish the initial ranking by
measuring the similarity between question and an-
swer, it is almost irrelevant to capture typical rules
suggesting if a description is valid or not. Indeed,
since test questions are not in the training set, their
words as well as those of candidate answers will
be different, penalizing BOW models. In these
</bodyText>
<footnote confidence="0.788916">
9Some of the kernels have been removed from the figures
so that the plots result more visible.
</footnote>
<page confidence="0.991317">
582
</page>
<figure confidence="0.992798333333333">
Precision
30 40 50 60 70 80 90 100
Recall
</figure>
<figureCaption confidence="0.995589">
Figure 6: Precision/Recall curves of some kernel
combinations on the WEB dataset.
</figureCaption>
<figure confidence="0.993005">
10 15 20 25 30 35 40 45 50 55 60
Recall
</figure>
<figureCaption confidence="0.984981">
Figure 7: Precision/Recall curves of some kernel
combinations on the TREC dataset.
</figureCaption>
<bodyText confidence="0.983352714285714">
conditions, we need to rely on syntactic structures
which at least allow for detecting well formed de-
scriptions.
Second, the results show that STK is important
to detect typical description patterns but also POS
sequences provide additional information since
they are less sparse than tree fragments. Such pat-
terns improve on the bag of POS-tags by about 6%
(see POS vs PSK). This is a relevant result consid-
ering that in standard text classification bigrams or
trigrams are usually ineffective.
Third, although PSK+STK generates a very rich
feature set, SRK significantly improves the classi-
fication F1 by about 3%, suggesting that shallow
semantics can be very useful to detect if an an-
swer is well formed and is related to a question.
Error analysis revealed that PAS can provide pat-
terns like:
- A0(X) R-A0(that) rel(result) A1(Y)
- A1(X) rel(characterize) A0(Y),
where X and Y need not necessarily be matched.
Finally, the best model, PSK+STK+SRK, im-
proves on BOW by 61%. This is strong evidence
that complex natural language tasks require ad-
vanced linguistic information that should be ex-
ploited by powerful algorithms such as SVMs
and using effective feature engineering techniques
such as kernel methods.
</bodyText>
<sectionHeader confidence="0.997707" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999980729729729">
In this paper, we have studied several types
of syntactic/semantic information: bag-of-words
(BOW), bag-of-POS tags, syntactic parse trees
and predicate argument structures (PASs), for the
design of short text pair classifiers. Our learn-
ing framework is constituted by Support Vector
Machines (SVMs) and kernel methods applied
to automatically generated syntactic and semantic
structures.
In particular, we designed (i) a new Semantic
Role Kernel (SRK) based on a very fast algorithm;
(ii) a new sequence kernel over POS tags to en-
code shallow syntactic information; (iii) many ker-
nel combinations (to our knowledge no previous
work uses so many different kernels) which allow
for the study of the role of several linguistic levels
in a well defined statistical framework.
The results on two different question/answer
classification corpora suggest that (a) SRK for pro-
cessing PASs is more efficient and effective than
previous models, (b) kernels based on PAS, POS-
tag sequences and syntactic parse trees improve on
BOW on both datasets and (c) on the TREC data
the improvement is remarkably high, e.g. about
61%.
Promising future work concerns the definition
of a kernel on the entire argument information
(e.g. by means of lexical similarity between all the
words of two arguments) and the design of a dis-
course kernel to exploit the relational information
gathered from different sentence pairs. A closer
relationship between questions and answers can be
exploited with models presented in (Moschitti and
Zanzotto, 2007; Zanzotto and Moschitti, 2006).
Also the use of PAS derived from FrameNet and
PropBank (Giuglea and Moschitti, 2006) appears
to be an interesting research line.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99737375">
I would like to thank Silvia Quarteroni for her
work on extracting linguistic structures. This
work has been partially supported by the European
Commission - LUNA project, contract n. 33549.
</bodyText>
<figure confidence="0.999049212121212">
100
40
90
80
70
60
50
30
20
10
0
STK
WSK+STK
WSK+STK+SRK
BOW
WSK
Precision
100
40
90
80
70
60
50
30
20
10
0
STK
PSK+STK
&amp;quot;PSK+STK+SRK&amp;quot;
BOW
PSK
</figure>
<page confidence="0.996616">
583
</page>
<sectionHeader confidence="0.99353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999457">
J. Allan. 2000. Natural Language Processing for Information
Retrieval. In NAACL/ANLP (tutorial notes).
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: SRL. In CoNLL-2005.
Y. Chali and S. Joty. 2008. Improving the performance of
the random walk model for answering complex questions.
In Proceedings of ACL-08: HLT, Short Papers, Columbus,
Ohio.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers
from definitional QA using language models. In ACL’06.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In ACL’02.
K. Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke.
2004. The effect of document retrieval quality on factoid
QA performance. In SIGIR’04.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree
Kernels for Relation Extraction. In ACL04, Barcelona,
Spain.
C. Cumby and D. Roth. 2003. Kernel Methods for Rela-
tional Learning. In Proceedings of ICML 2003, Washing-
ton, DC, USA.
J. Furnkranz, T. Mitchell, and E. Rilof. 1998. A case study
in using linguistic phrases for text categorization on the
www. In Working Notes of the AAAI/ICML, Workshop on
Learning for Text Categorization.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. 2007.
The third pascal recognizing textual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop, Prague.
A.-M. Giuglea and A. Moschitti. 2006. Semantic role label-
ing via framenet, verbnet and propbank. In Proceedings
of ACL 2006, Sydney, Australia.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lccs chaucer at
trec 2006. In Proceedings of TREC’06.
E. Hovy, U. Hermjakob, C.-Y. Lin, and D. Ravichandran.
2002. Using knowledge to facilitate factoid answer pin-
pointing. In Proceedings of Coling, Morristown, NJ,
USA.
R. Jackendoff. 1990. Semantic Structures. MIT Press.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods.
C. R. Johnson and C. J. Fillmore. 2000. The framenet tagset
for frame-semantic and syntactic coding of predicate-
argument structures. In ANLP-NAACL’00, pages 56–62.
J. Kazama and K. Torisawa. 2005. Speeding up Train-
ing with Tree Kernels for Node Relation Labeling. In
Proceedings of EMNLP 2005, pages 137–144, Toronto,
Canada.
P. Kingsbury and M. Palmer. 2002. From Treebank to Prop-
Bank. In LREC’02.
T. Kudo and Y. Matsumoto. 2003. Fast Methods for Kernel-
Based Text Analysis. In Erhard Hinrichs and Dan Roth,
editors, Proceedings of ACL.
T. Kudo, J. Suzuki, and H .Isozaki. 2005. Boosting-based
parse reranking with subtree features. In Proceedings of
ACL’05, US.
D. D. Lewis. 1992. An evaluation of phrasal and clustered
representations on a text categorization task. In Proceed-
ings of SIGIR-92.
X. Li and D. Roth. 2005. Learning question classifiers: the
role of semantic information. JNLE.
J. Lin and B. Katz. 2003. Question answering from the web
using knowledge annotation and knowledge mining tech-
niques. In CIKM ’03.
A. Moschitti and R. Basili. 2004. Complex linguistic fea-
tures for text classification: A comprehensive study. In
ECIR, Sunderland, UK.
A. Moschitti and C. Bejan. 2004. A semantic kernel for pred-
icate argument classification. In CoNLL-2004, Boston,
MA, USA.
A. Moschitti and S. Quarteroni. 2008. Kernels on linguistic
structures for answer extraction. In Proceedings of ACL-
08: HLT, Short Papers, Columbus, Ohio.
A. Moschitti and F. Zanzotto. 2007. Fast and effective ker-
nels for relational learning from texts. In Zoubin Ghahra-
mani, editor, Proceedings of ICML 2007.
A. Moschitti, D. Pighin, and R. Basili. 2006. Semantic role
labeling via tree kernel joint inference. In Proceedings of
CoNLL-X, New York City.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar.
2007. Exploiting syntactic and shallow semantic kernels
for question/answer classification. In ACL’07, Prague,
Czech Republic.
A. Moschitti. 2006. Making Tree Kernels Practical for Nat-
ural Language Learning. In Proceedings of EACL2006.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping depen-
dencies trees: An application to question answering. In
Proceedings of AI&amp;Math 2004.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods
for Pattern Analysis. Cambridge University Press.
D. Shen and M. Lapata. 2007. Using semantic roles to im-
prove question answering. In Proceedings of EMNLP-
CoNLL.
L. Shen, A. Sarkar, and A. k. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In EMNLP, Sapporo,
Japan.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learn-
ing to rank answers on large online QA collections. In
Proceedings of ACL-08: HLT, Columbus, Ohio.
K. Toutanova, P. Markova, and C. Manning. 2004. The Leaf
Path Projection View of Parse Trees: Exploring String
Kernels for HPSG Parse Selection. In Proceedings of
EMNLP 2004, Barcelona, Spain.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer.
E. M. Voorhees. 2004. Overview of the trec 2001 question
answering track. In Proceedings of the Thirteenth Text
REtreival Conference (TREC 2004).
F. M. Zanzotto and A. Moschitti. 2006. Automatic learning
of textual entailments with cross-pair similarities. In Pro-
ceedings of the 21st Coling and 44th ACL, Sydney, Aus-
tralia.
D. Zhang and W. Lee. 2003. Question classification using
support vector machines. In SIGIR’03, Toronto, Canada.
ACM.
M. Zhang, J. Zhang, and J. Su. 2006. Exploring Syntactic
Features for Relation Extraction using a Convolution tree
kernel. In Proceedings of NAACL, New York City, USA.
</reference>
<page confidence="0.998598">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952089">
<title confidence="0.999962">Syntactic and Semantic Kernels for Short Text Pair Categorization</title>
<author confidence="0.999971">Alessandro Moschitti</author>
<affiliation confidence="0.9998935">Department of Computer Science and Engineering University of Trento</affiliation>
<address confidence="0.9831365">Via Sommarive 14 38100 POVO (TN) - Italy</address>
<email confidence="0.999004">moschitti@disi.unitn.it</email>
<abstract confidence="0.9991238125">Automatic detection of general relations between short texts is a complex task that cannot be carried out only relying on language models and bag-of-words. Therefore, learning methods to exploit syntax and semantics are required. In this paper, we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer show that our kernels can be used to greatly improve system accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Natural Language Processing for Information Retrieval.</title>
<date>2000</date>
<booktitle>In NAACL/ANLP (tutorial notes).</booktitle>
<contexts>
<context position="1064" citStr="Allan, 2000" startWordPosition="157" endWordPosition="158">ired. In this paper, we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classification show that our kernels can be used to greatly improve system accuracy. 1 Introduction Previous work on Text Categorization (TC) has shown that advanced linguistic processing for document representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et a</context>
</contexts>
<marker>Allan, 2000</marker>
<rawString>J. Allan. 2000. Natural Language Processing for Information Retrieval. In NAACL/ANLP (tutorial notes).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: SRL.</title>
<date>2005</date>
<booktitle>In CoNLL-2005.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL-2005 shared task: SRL. In CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S Joty</author>
</authors>
<title>Improving the performance of the random walk model for answering complex questions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="10148" citStr="Chali and Joty, 2008" startWordPosition="1728" endWordPosition="1731">fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety.”. Figure 3: Some of the tree substructures useful to capture shallow semantic propertie</context>
</contexts>
<marker>Chali, Joty, 2008</marker>
<rawString>Y. Chali and S. Joty. 2008. Improving the performance of the random walk model for answering complex questions. In Proceedings of ACL-08: HLT, Short Papers, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>M Zhou</author>
<author>S Wang</author>
</authors>
<title>Reranking answers from definitional QA using language models.</title>
<date>2006</date>
<booktitle>In ACL’06.</booktitle>
<contexts>
<context position="1919" citStr="Chen et al., 2006" startWordPosition="295" endWordPosition="298">sk is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabilistic models suffer from (i) computational complexity issues, e.g. for the processing of large bayesian networks, (ii) problems in effectively estimating and smoothing probabilities and (iii) high sensitiveness to irrelevant features and processing errors. In contrast, di</context>
</contexts>
<marker>Chen, Zhou, Wang, 2006</marker>
<rawString>Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers from definitional QA using language models. In ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL’02.</booktitle>
<contexts>
<context position="3101" citStr="Collins and Duffy, 2002" startWordPosition="474" endWordPosition="477">es and processing errors. In contrast, discriminative models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, </context>
<context position="6207" citStr="Collins and Duffy, 2002" startWordPosition="966" endWordPosition="969">on our experiments with the above models and, finally, a conclusion is drawn in Section 5. 2 String and Tree Kernels Feature design, especially for modeling syntactic and semantic structures, is one of the most difficult aspects in defining a learning system as it requires efficient feature extraction from learning objects. Kernel methods are an interesting representation approach as they allow for the use of 1www.cis.upenn.edu/˜ace all object substructures as features. In this perspective, String Kernel (SK) proposed in (ShaweTaylor and Cristianini, 2004) and the Syntactic Tree Kernel (STK) (Collins and Duffy, 2002) allow for modeling structured data in high dimensional spaces. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. Gaps modify the weight associated with the target substrings as shown in the following. Let E be a finite alphabet, E* = U&apos;n=0 En is the set of all strings. Given a string s E E*, |s |denotes the length of the strings and si its compounding symbols, i.e s = s1..s|s|, whereas s[i : j] selects the substring sisi+1..sj_1sj from the i-th to the j-th </context>
<context position="9019" citStr="Collins and Duffy, 2002" startWordPosition="1519" endWordPosition="1522">NP Anxiety S VBZ is VBZ is VP VBZ is NP NNP Anxiety NNP Anxiety VBZ is D D N a disease VP VBZ VP VBZ PAS PAS (a) (b) A1 Disorder rel characterize A0 fear R-A0 that A1 anxiety rel causes fragments and χi(n) be an indicator function, equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree kernel functtiio`n over T1 and T2 is deÞned as TK(T1,T2) = �+n1∈NT1 �n2∈NT2 Δ(n1, n2), where NT1 and NT2 are the sets of nodes in T1 and T2, respectively and Δ(n1, n2) = �|F| i=1 χi(n1)χi(n2). Δ function counts the number of subtrees rooted in n1 and n2 and can be evaluated as follows (Collins and Duffy, 2002): 1. if the productions at n1 and n2 are different then A(n1, n2) = 0; 2. if the productions at n1 and n2 are the same, and n1 and n2 have only leaf children (i.e. they are pre-terminal symbols) then A(n1, n2) = λ; 3. if the productions at n1 and n2 are the same, and n1 and n2 are not pre-terminals then A(n1, n2) = λ Il(n1) j=1 (1 + A(cn1(j), cn2(j))), where l(n1) is the number of children of n1, cn(j) is the j-th child of node n and A is a decay factor penalizing larger structures. Figure 1 shows some fragments of the subtree on the left part. These satisfy the constraint that grammatical rul</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>E Terra</author>
<author>C L A Clarke</author>
</authors>
<title>The effect of document retrieval quality on factoid QA performance.</title>
<date>2004</date>
<booktitle>In SIGIR’04.</booktitle>
<contexts>
<context position="1951" citStr="Collins-Thompson et al., 2004" startWordPosition="299" endWordPosition="302">y complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabilistic models suffer from (i) computational complexity issues, e.g. for the processing of large bayesian networks, (ii) problems in effectively estimating and smoothing probabilities and (iii) high sensitiveness to irrelevant features and processing errors. In contrast, discriminative models such as Supp</context>
</contexts>
<marker>Collins-Thompson, Callan, Terra, Clarke, 2004</marker>
<rawString>K. Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke. 2004. The effect of document retrieval quality on factoid QA performance. In SIGIR’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In ACL04,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3223" citStr="Culotta and Sorensen, 2004" startWordPosition="494" endWordPosition="497">been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic infor</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In ACL04, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cumby</author>
<author>D Roth</author>
</authors>
<title>Kernel Methods for Relational Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML 2003,</booktitle>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3149" citStr="Cumby and Roth, 2003" startWordPosition="482" endWordPosition="485">ve models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for </context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>C. Cumby and D. Roth. 2003. Kernel Methods for Relational Learning. In Proceedings of ICML 2003, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Furnkranz</author>
<author>T Mitchell</author>
<author>E Rilof</author>
</authors>
<title>A case study in using linguistic phrases for text categorization on the www.</title>
<date>1998</date>
<booktitle>In Working Notes of the AAAI/ICML, Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="1051" citStr="Furnkranz et al., 1998" startWordPosition="153" endWordPosition="156">x and semantics are required. In this paper, we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classification show that our kernels can be used to greatly improve system accuracy. 1 Introduction Previous work on Text Categorization (TC) has shown that advanced linguistic processing for document representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Pu</context>
</contexts>
<marker>Furnkranz, Mitchell, Rilof, 1998</marker>
<rawString>J. Furnkranz, T. Mitchell, and E. Rilof. 1998. A case study in using linguistic phrases for text categorization on the www. In Working Notes of the AAAI/ICML, Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Dagan</author>
<author>B Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop,</booktitle>
<location>Prague.</location>
<contexts>
<context position="1577" citStr="Giampiccolo et al., 2007" startWordPosition="237" endWordPosition="240">cument representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing </context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Giuglea</author>
<author>A Moschitti</author>
</authors>
<title>Semantic role labeling via framenet, verbnet and propbank.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<location>Sydney, Australia.</location>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>A.-M. Giuglea and A. Moschitti. 2006. Semantic role labeling via framenet, verbnet and propbank. In Proceedings of ACL 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>Y Shi</author>
<author>B Rink</author>
</authors>
<title>Question answering with lccs chaucer at trec</title>
<date>2006</date>
<booktitle>In Proceedings of TREC’06.</booktitle>
<contexts>
<context position="1238" citStr="Hickl et al., 2006" startWordPosition="182" endWordPosition="185">ation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classification show that our kernels can be used to greatly improve system accuracy. 1 Introduction Previous work on Text Categorization (TC) has shown that advanced linguistic processing for document representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuris</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Shi, Rink, 2006</marker>
<rawString>A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and B. Rink. 2006. Question answering with lccs chaucer at trec 2006. In Proceedings of TREC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C-Y Lin</author>
<author>D Ravichandran</author>
</authors>
<title>Using knowledge to facilitate factoid answer pinpointing.</title>
<date>2002</date>
<booktitle>In Proceedings of Coling,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1647" citStr="Hovy et al., 2002" startWordPosition="250" endWordPosition="253">urnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabil</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, Ravichandran, 2002</marker>
<rawString>E. Hovy, U. Hermjakob, C.-Y. Lin, and D. Ravichandran. 2002. Using knowledge to facilitate factoid answer pinpointing. In Proceedings of Coling, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3899" citStr="Jackendoff, 1990" startWordPosition="603" endWordPosition="604">sawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. However, the relatively high kernel computational complexity and the limited improvement on bag-of-words (BOW) produced by this approach do not make the use of such technique practical for real world applications. In this paper, we carry out a complete study on the use of syntactic/semantic structures for relation</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>R. Jackendoff. 1990. Semantic Structures. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="20263" citStr="Joachims, 1999" startWordPosition="3484" endWordPosition="3485"> 4.1 Experimental Setup The task used to test our kernels is the classification of the correctness of (q, a) pairs, where a is an answer for the query q. The text pair kernel operates by comparing the content of questions and the content of answers in a separate fashion. Thus, given two pairs p1 = (q1, a1) and p2 = (q2, a2), a kernel function is defined as K(p1,p2) = ET KT(q1, q2) + ET KT(a1, a2), where T varies across different kernel functions described hereafter. As a basic kernel machine, we used our SVM-Light-TK toolkit, available at disi.unitn. it/moschitti (which is based on SVM-Light (Joachims, 1999) software). In it, we implemented: the String Kernel (SK), the Syntactic Tree Kernel (STK), the Shallow Semantic Tree Kernel (SSTK) and the Semantic Role Kernel (SRK) described in sections 2 and 3. Each kernel is associated with the above linguistic objects: (i) the linear kernel is used with the bag-of-words (BOW) or the bag-of-POS-tags (POS) features. (ii) SK is used with word sequences (i.e. the Word Sequence Kernel, WSK) and POS sequences (i.e. the POS Sequence Kernel, PSK). (iii) STK is used with syntactic parse trees automatically derived with Charniak’s parser; (iv) SSTK and SRK are app</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Johnson</author>
<author>C J Fillmore</author>
</authors>
<title>The framenet tagset for frame-semantic and syntactic coding of predicateargument structures.</title>
<date>2000</date>
<booktitle>In ANLP-NAACL’00,</booktitle>
<pages>56--62</pages>
<contexts>
<context position="3928" citStr="Johnson and Fillmore, 2000" startWordPosition="605" endWordPosition="608">et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. However, the relatively high kernel computational complexity and the limited improvement on bag-of-words (BOW) produced by this approach do not make the use of such technique practical for real world applications. In this paper, we carry out a complete study on the use of syntactic/semantic structures for relational learning from questions an</context>
<context position="11064" citStr="Johnson and Fillmore, 2000" startWordPosition="1864" endWordPosition="1867"> our new kernel for PASs and compare it with the previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety.”. Figure 3: Some of the tree substructures useful to capture shallow semantic properties. 3.1 Shallow Semantic Structures Shallow approaches to semantic processing are making large strides in the direction of efÞciently and effectively deriving tacit semantic information from text. Large data resources annotated with levels of semantic information, such as in the FrameNet (Johnson and Fillmore, 2000) and PropBank (PB) (Kingsbury and Palmer, 2002) projects, make it possible to design systems for the automatic extraction of predicate argument structures (PASs) (Carreras and M`arquez, 2005). PB-based systems produce sentence annotations like: [A1 Panic disorder] is [rel characterized] [A0 by unexpected and intense fear] [R−A0 that] [relcauses] [A1 anxiety]. A tree representation of the above semantic information is given by the two PAS trees in Figure 2, where the argument words are replaced by the head word to reduce data sparseness. Hence, the semantic similarity between sentences can be m</context>
</contexts>
<marker>Johnson, Fillmore, 2000</marker>
<rawString>C. R. Johnson and C. J. Fillmore. 2000. The framenet tagset for frame-semantic and syntactic coding of predicateargument structures. In ANLP-NAACL’00, pages 56–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Speeding up Training with Tree Kernels for Node Relation Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>137--144</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="3293" citStr="Kazama and Torisawa, 2005" startWordPosition="506" endWordPosition="509">. Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff</context>
</contexts>
<marker>Kazama, Torisawa, 2005</marker>
<rawString>J. Kazama and K. Torisawa. 2005. Speeding up Training with Tree Kernels for Node Relation Labeling. In Proceedings of EMNLP 2005, pages 137–144, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In LREC’02.</booktitle>
<contexts>
<context position="4130" citStr="Kingsbury and Palmer, 2002" startWordPosition="638" endWordPosition="641">oreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. However, the relatively high kernel computational complexity and the limited improvement on bag-of-words (BOW) produced by this approach do not make the use of such technique practical for real world applications. In this paper, we carry out a complete study on the use of syntactic/semantic structures for relational learning from questions and answers. We designed sequence kernels for words and Part of Speech Tags which capture basic lexical semantics and basic syntactic information. Then, we design a novel shallow semantic kernel which is </context>
<context position="11111" citStr="Kingsbury and Palmer, 2002" startWordPosition="1872" endWordPosition="1875">e previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety.”. Figure 3: Some of the tree substructures useful to capture shallow semantic properties. 3.1 Shallow Semantic Structures Shallow approaches to semantic processing are making large strides in the direction of efÞciently and effectively deriving tacit semantic information from text. Large data resources annotated with levels of semantic information, such as in the FrameNet (Johnson and Fillmore, 2000) and PropBank (PB) (Kingsbury and Palmer, 2002) projects, make it possible to design systems for the automatic extraction of predicate argument structures (PASs) (Carreras and M`arquez, 2005). PB-based systems produce sentence annotations like: [A1 Panic disorder] is [rel characterized] [A0 by unexpected and intense fear] [R−A0 that] [relcauses] [A1 anxiety]. A tree representation of the above semantic information is given by the two PAS trees in Figure 2, where the argument words are replaced by the head word to reduce data sparseness. Hence, the semantic similarity between sentences can be measured in terms of the number of substructures</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From Treebank to PropBank. In LREC’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast Methods for KernelBased Text Analysis.</title>
<date>2003</date>
<booktitle>Proceedings of ACL.</booktitle>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="3127" citStr="Kudo and Matsumoto, 2003" startWordPosition="478" endWordPosition="481"> In contrast, discriminative models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T. Kudo and Y. Matsumoto. 2003. Fast Methods for KernelBased Text Analysis. In Erhard Hinrichs and Dan Roth, editors, Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>J Suzuki</author>
<author>H Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>US.</pages>
<contexts>
<context position="3242" citStr="Kudo et al., 2005" startWordPosition="498" endWordPosition="501">oise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form </context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>T. Kudo, J. Suzuki, and H .Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings of ACL’05, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>An evaluation of phrasal and clustered representations on a text categorization task.</title>
<date>1992</date>
<booktitle>In Proceedings of SIGIR-92.</booktitle>
<contexts>
<context position="1027" citStr="Lewis, 1992" startWordPosition="151" endWordPosition="152">exploit syntax and semantics are required. In this paper, we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classification show that our kernels can be used to greatly improve system accuracy. 1 Introduction Previous work on Text Categorization (TC) has shown that advanced linguistic processing for document representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>D. D. Lewis. 1992. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of SIGIR-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers: the role of semantic information.</title>
<date>2005</date>
<publisher>JNLE.</publisher>
<contexts>
<context position="3500" citStr="Li and Roth, 2005" startWordPosition="539" endWordPosition="542">el methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 forma</context>
<context position="21879" citStr="Li and Roth, 2005" startWordPosition="3771" endWordPosition="3774">r SSTK to all pairs P1 × P2 and sum the obtained contribution, where P1 and P2 are the set of PASs of the first and second answer3. Although different kernels can be used for questions and for answers, we used (and summed together) the same kernels except for those based on PASs, which are only used on answers. 4.1.1 Datasets To train and test our text QA classifiers, we adopted the two datasets of question/answer pairs available at disi.unitn.it/˜silviaq, containing answers to only definitional questions. The datasets are based on the 138 TREC 2001 test questions labeled as “description” in (Li and Roth, 2005). Each question is paired with all the top 20 answer paragraphs extracted by two basic QA systems: one trained with the web documents and the other trained with the AQUAINT data used in TREC’07. The WEB corpus (Moschitti et al., 2007) of QA pairs contains 1,309 sentences, 416 of which are positive4 answers whereas the TREC corpus contains 2,256 sentences, 261 of which are positive. 4.1.2 Measures and Parameterization The accuracy of the classifiers is provided by the average F1 over 5 different samples using 5-fold cross-validation whereas each plot refers to a single fold. We carried out some</context>
</contexts>
<marker>Li, Roth, 2005</marker>
<rawString>X. Li and D. Roth. 2005. Learning question classifiers: the role of semantic information. JNLE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>B Katz</author>
</authors>
<title>Question answering from the web using knowledge annotation and knowledge mining techniques.</title>
<date>2003</date>
<booktitle>In CIKM ’03.</booktitle>
<contexts>
<context position="1693" citStr="Lin and Katz, 2003" startWordPosition="258" endWordPosition="261">i and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabilistic models suffer from (i) computational com</context>
</contexts>
<marker>Lin, Katz, 2003</marker>
<rawString>J. Lin and B. Katz. 2003. Question answering from the web using knowledge annotation and knowledge mining techniques. In CIKM ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>R Basili</author>
</authors>
<title>Complex linguistic features for text classification: A comprehensive study.</title>
<date>2004</date>
<booktitle>In ECIR,</booktitle>
<location>Sunderland, UK.</location>
<contexts>
<context position="1093" citStr="Moschitti and Basili, 2004" startWordPosition="159" endWordPosition="162"> paper, we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classification show that our kernels can be used to greatly improve system accuracy. 1 Introduction Previous work on Text Categorization (TC) has shown that advanced linguistic processing for document representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003)</context>
</contexts>
<marker>Moschitti, Basili, 2004</marker>
<rawString>A. Moschitti and R. Basili. 2004. Complex linguistic features for text classification: A comprehensive study. In ECIR, Sunderland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>C Bejan</author>
</authors>
<title>A semantic kernel for predicate argument classification.</title>
<date>2004</date>
<booktitle>In CoNLL-2004,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="3195" citStr="Moschitti and Bejan, 2004" startWordPosition="490" endWordPosition="493"> (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown</context>
</contexts>
<marker>Moschitti, Bejan, 2004</marker>
<rawString>A. Moschitti and C. Bejan. 2004. A semantic kernel for predicate argument classification. In CoNLL-2004, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
</authors>
<title>Kernels on linguistic structures for answer extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="10125" citStr="Moschitti and Quarteroni, 2008" startWordPosition="1723" endWordPosition="1727">structures. Figure 1 shows some fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety.”. Figure 3: Some of the tree substructures useful to capture sha</context>
</contexts>
<marker>Moschitti, Quarteroni, 2008</marker>
<rawString>A. Moschitti and S. Quarteroni. 2008. Kernels on linguistic structures for answer extraction. In Proceedings of ACL08: HLT, Short Papers, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>F Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>In Zoubin Ghahramani, editor, Proceedings of ICML</booktitle>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>A. Moschitti and F. Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Zoubin Ghahramani, editor, Proceedings of ICML 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>D Pighin</author>
<author>R Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<location>New York City.</location>
<contexts>
<context position="3338" citStr="Moschitti et al., 2006" startWordPosition="514" endWordPosition="517"> may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves </context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>A. Moschitti, D. Pighin, and R. Basili. 2006. Semantic role labeling via tree kernel joint inference. In Proceedings of CoNLL-X, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In ACL’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3558" citStr="Moschitti et al., 2007" startWordPosition="548" endWordPosition="551">representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/an</context>
<context position="4821" citStr="Moschitti et al., 2007" startWordPosition="747" endWordPosition="750">elatively high kernel computational complexity and the limited improvement on bag-of-words (BOW) produced by this approach do not make the use of such technique practical for real world applications. In this paper, we carry out a complete study on the use of syntactic/semantic structures for relational learning from questions and answers. We designed sequence kernels for words and Part of Speech Tags which capture basic lexical semantics and basic syntactic information. Then, we design a novel shallow semantic kernel which is far more efficient and also more accurate than the one proposed in (Moschitti et al., 2007). The extensive experiments carried out on two different corpora of questions and answers, derived from Web documents and the TREC corpus, show that: • Kernels based on PAS, POS-tag sequences and syntactic parse trees improve the BOW approach on both datasets. On the TREC data the improvement is interestingly high, e.g. about 61%, making its application worthwhile. • The new kernel for processing PASs is more efficient and effective than previous models so that it can be practically used in systems for short text pair categorization, e.g. question/answer classification. In the remainder of thi</context>
<context position="10093" citStr="Moschitti et al., 2007" startWordPosition="1719" endWordPosition="1722">actor penalizing larger structures. Figure 1 shows some fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety.”. Figure 3: Some of the tree sub</context>
<context position="12154" citStr="Moschitti et al., 2007" startWordPosition="2038" endWordPosition="2041">the argument words are replaced by the head word to reduce data sparseness. Hence, the semantic similarity between sentences can be measured in terms of the number of substructures between the two trees. The required substructures violate the STK constraint (about breaking production rules), i.e. since we need any set of nodes linked by edges of the initial tree. For example, interesting semantic fragments of Figure 2.a are shown in Figure 3. Unfortunately, STK applied to PAS trees cannot generate such fragments. To overcome this problem, a Shallow Semantic Tree Kernel (SSTK) was designed in (Moschitti et al., 2007). 3.2 Shallow Semantic Tree Kernel (SSTK) SSTK is obtained by applying two different steps: Þrst, the PAS tree is transformed by adding a layer rel A0 PAS rel characterize PAS rel characterize PAS A1 rel A0 PAS A1 rel characterize PAS characterize A0 fear 578 of SLOT nodes as many as the number of possible argument types, where each slot is assigned to an argument following a fixed ordering (e.g. rel, A0, A1, A2, ...). For example, if an A1 is found in the sentence annotation it will be always positioned under the third slot. This is needed to “artificially” allow SSTK to generate structures c</context>
<context position="18018" citStr="Moschitti et al., 2007" startWordPosition="3103" endWordPosition="3106">st, although the computational complexity of SRK is larger than the one of SSTK, we will show in the experiment section that the running time (for both training and testing) is much lower. The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). Second, although SRK uses trees with only three levels, in Eq.1, the function Q (defined to give 1 or 0 if the heads match or not) can be substituted by any kernel function. Thus, Q can recursively be an SRK (and evaluate Nested PASs (Moschitti et al., 2007)) or any other potential kernel (over the arguments). The very interesting aspect is that the efficient algorithm that we provide (Eqs 2, 3 and 4) can be accordingly modified to efficiently evaluate new kernels obtained with the Q substitution2. Third, the interesting difference between SRK and SSTK (in addition to efficiency) is that SSTK requires an ordered sequence of arguments to evaluate the number of argument subgroups (arguments are sorted before running the kernel). This means that the natural order is lost. SRK instead is based on subsequence kernels so it naturally takes into account</context>
<context position="22113" citStr="Moschitti et al., 2007" startWordPosition="3812" endWordPosition="3815">r) the same kernels except for those based on PASs, which are only used on answers. 4.1.1 Datasets To train and test our text QA classifiers, we adopted the two datasets of question/answer pairs available at disi.unitn.it/˜silviaq, containing answers to only definitional questions. The datasets are based on the 138 TREC 2001 test questions labeled as “description” in (Li and Roth, 2005). Each question is paired with all the top 20 answer paragraphs extracted by two basic QA systems: one trained with the web documents and the other trained with the AQUAINT data used in TREC’07. The WEB corpus (Moschitti et al., 2007) of QA pairs contains 1,309 sentences, 416 of which are positive4 answers whereas the TREC corpus contains 2,256 sentences, 261 of which are positive. 4.1.2 Measures and Parameterization The accuracy of the classifiers is provided by the average F1 over 5 different samples using 5-fold cross-validation whereas each plot refers to a single fold. We carried out some preliminary experiments of the basic kernels on a validation set and 3More formally, let Pt and Pt be the sets of PASs extracted from text fragments t and t$; the resulting kernel will be Kall (Pt,Pt,) _ Pp∈Pt Pp,∈ SRK (p, p$). 4For </context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In ACL’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Making Tree Kernels Practical for Natural Language Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL2006.</booktitle>
<contexts>
<context position="17660" citStr="Moschitti, 2006" startWordPosition="3038" endWordPosition="3039">. . . SLOT A0 fear SLOT A1 disorder SLOT null SLOT rel characterize SLOT rel characterize SLOT rel characterize Regarding the processing time, if p is the maximum number of arguments in a predicate structure, the worst case computational complexity of SRK is O(p3). 3.4 SRK vs. SSTK A comparison between SSTK and SRK suggests the following points: first, although the computational complexity of SRK is larger than the one of SSTK, we will show in the experiment section that the running time (for both training and testing) is much lower. The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). Second, although SRK uses trees with only three levels, in Eq.1, the function Q (defined to give 1 or 0 if the heads match or not) can be substituted by any kernel function. Thus, Q can recursively be an SRK (and evaluate Nested PASs (Moschitti et al., 2007)) or any other potential kernel (over the arguments). The very interesting aspect is that the efficient algorithm that we provide (Eqs 2, 3 and 4) can be accordingly modified to efficiently evaluate new kernels obtained with the Q substitutio</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Making Tree Kernels Practical for Natural Language Learning. In Proceedings of EACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Mapping dependencies trees: An application to question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of AI&amp;Math</booktitle>
<contexts>
<context position="1672" citStr="Punyakanok et al., 2004" startWordPosition="254" endWordPosition="257">98; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (Chen et al., 2006; Collins-Thompson et al., 2004). However, for definitional questions, a more effective approach would be to test if a correct relationship between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabilistic models suffer from </context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping dependencies trees: An application to question answering. In Proceedings of AI&amp;Math 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="15883" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2689" endWordPosition="2692">valuated by Er1:u=s1[�I1],�I2:u=s2[�I2] Ad( Given a node in a shared subsequence u, its child (i.e. the head word) can be both in T1 and T2, originating two different shared structures (with or without such head node). The matches on the heads (for each shared node of u) are combined together generating different substructures. Thus the number of substructures originating from u is the product, lll=1..|u|(1+Q(s1[4l], s2[�I2l])). This number multiplied by all shared subsequences leads to Eq. 1. ✷ We can efficiently compute SRK by following a similar approach to the string kernel evaluation in (Shawe-Taylor and Cristianini, 2004) by defining the following dynamic matrix: λk−i+l−r × γp−1(s1[1 : i], s2[1 : r]), where -yp(s1, s2) counts the number of shared substructures of exactly p argument nodes between s1 and s2 and again, s[1 : i] indicates the sequence portion from argument 1 to i. The above matrix is then used to evaluate -yp(s1a, s2b) = f λ2(1 + σ(h(a), h(b)))Dp(|s1|, |s2|) if a = b; SRK(s1,s2) = �I2) (when A = 1). �I1)+d( Ek i=1 Dp(k, l) = El r=1 Em p=1 0 otherwise. where and s2b indicate the concatenation of the sequences s and t with the argument nodes, a and b, respectively and Q(h(a), h(b)) is 1 if the child</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="3534" citStr="Shen and Lapata, 2007" startWordPosition="544" endWordPosition="547">fficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) ex</context>
<context position="10046" citStr="Shen and Lapata, 2007" startWordPosition="1710" endWordPosition="1713">is the j-th child of node n and A is a decay factor penalizing larger structures. Figure 1 shows some fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that c</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Sarkar</author>
<author>A k Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking. In EMNLP,</title>
<date>2003</date>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3168" citStr="Shen et al., 2003" startWordPosition="486" endWordPosition="489">ort Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Lingu</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>L. Shen, A. Sarkar, and A. k. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In EMNLP, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>M Ciaramita</author>
<author>H Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3581" citStr="Surdeanu et al., 2008" startWordPosition="552" endWordPosition="555">ured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (Moschitti et al., 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were propose</context>
<context position="10069" citStr="Surdeanu et al., 2008" startWordPosition="1714" endWordPosition="1718">de n and A is a decay factor penalizing larger structures. Figure 1 shows some fragments of the subtree on the left part. These satisfy the constraint that grammatical rules cannot be broken. For example, [VP [VBZ NP]] is a valid fragment which has two non-terminal symbols, VBZ and NP, as leaves whereas [VP [VBZ]] is not a valid feature. 3 Shallow Semantic Kernels The extraction of semantic representations from text is a very complex task. For it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies. Recently, work such as (Shen and Lapata, 2007; Surdeanu et al., 2008; Moschitti et al., 2007; Moschitti and Quarteroni, 2008; Chali and Joty, 2008), uses PAS to consider such dependencies but only the latter three researches attempt to completely exploit PAS with Shallow Semantic Tree Kernels (SSTKs). Unfortunately, these kernels result computational expensive for real world applications. In the remainder of this section, we present our new kernel for PASs and compare it with the previous SSTK. Figure 2: Predicate Argument Structure trees associated with the sentence: ”Panic disorder is characterized by unexpected and intense fear that causes anxiety.”. Figure</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of ACL-08: HLT, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>P Markova</author>
<author>C Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3266" citStr="Toutanova et al., 2004" startWordPosition="502" endWordPosition="505"> features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument St</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>K. Toutanova, P. Markova, and C. Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2668" citStr="Vapnik, 1995" startWordPosition="410" endWordPosition="411">hip between the answer and the query holds. This, in turns, depends on the structure of the two text fragments. Designing language models to capture such relation is too complex since probabilistic models suffer from (i) computational complexity issues, e.g. for the processing of large bayesian networks, (ii) problems in effectively estimating and smoothing probabilities and (iii) high sensitiveness to irrelevant features and processing errors. In contrast, discriminative models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995). Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the trec</title>
<date>2004</date>
<booktitle>In Proceedings of the Thirteenth Text REtreival Conference (TREC</booktitle>
<contexts>
<context position="1217" citStr="Voorhees, 2004" startWordPosition="180" endWordPosition="181"> for the exploitation of syntactic/semantic structures for short text pair categorization. Our experiments with Support Vector Machines on question/answer classification show that our kernels can be used to greatly improve system accuracy. 1 Introduction Previous work on Text Categorization (TC) has shown that advanced linguistic processing for document representation is often ineffective for this task, e.g. (Lewis, 1992; Furnkranz et al., 1998; Allan, 2000; Moschitti and Basili, 2004). In contrast, work in question answering suggests that syntactic and semantic structures help in solving TC (Voorhees, 2004; Hickl et al., 2006). From these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play a relevant role. In this perspective, the study of the automatic detection of relationships between short texts is particularly interesting. Typical examples of such relations are given in (Giampiccolo et al., 2007) or those holding between question and answer, e.g. (Hovy et al., 2002; Punyakanok et al., 2004; Lin and Katz, 2003), i.e. if a text fragment correctly responds to a question. In Question Answering, the latter problem is mostly tackled by u</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>E. M. Voorhees. 2004. Overview of the trec 2001 question answering track. In Proceedings of the Thirteenth Text REtreival Conference (TREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<location>Sydney, Australia.</location>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>F. M. Zanzotto and A. Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In SIGIR’03,</booktitle>
<publisher>ACM.</publisher>
<location>Toronto, Canada.</location>
<contexts>
<context position="3423" citStr="Zhang and Lee, 2003" startWordPosition="526" endWordPosition="529">taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and Fillmore, 2000) improves the automatic detection of correct answers to a target question. In particular, in (M</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>D. Zhang and W. Lee. 2003. Question classification using support vector machines. In SIGIR’03, Toronto, Canada. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Zhang</author>
<author>J Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>New York City, USA.</location>
<contexts>
<context position="3313" citStr="Zhang et al., 2006" startWordPosition="510" endWordPosition="513">inguistic structures may still provide a relevant contribution since only the relevant information would be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data. SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby and Roth, 2003; Shen et al., 2003; Moschitti and Bejan, 2004; Culotta and Sorensen, 2004; Kudo et al., 2005; Toutanova et al., 2004; Kazama and Torisawa, 2005; Zhang et al., 2006; Moschitti et al., 2006). In particular, in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005). Moreover, (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Chali and Joty, Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576–584, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 576 2008) have shown that shallow semantic information in the form of Predicate Argument Structures (PASs) (Jackendoff, 1990; Johnson and </context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>M. Zhang, J. Zhang, and J. Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL, New York City, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>