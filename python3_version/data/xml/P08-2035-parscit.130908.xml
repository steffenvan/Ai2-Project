<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018816">
<title confidence="0.987487">
Mining Wikipedia Revision Histories for Improving Sentence Compression
</title>
<author confidence="0.988629">
Elif Yamangil Rani Nelken
</author>
<affiliation confidence="0.9828915">
School of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.782537">
Cambridge, MA 02138, USA
</address>
<email confidence="0.999749">
{elif,nelken}@eecs.harvard.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884625">
A well-recognized limitation of research on
supervised sentence compression is the dearth
of available training data. We propose a new
and bountiful resource for such training data,
which we obtain by mining the revision his-
tory of Wikipedia for sentence compressions
and expansions. Using only a fraction of the
available Wikipedia data, we have collected
a training corpus of over 380,000 sentence
pairs, two orders of magnitude larger than the
standardly used Ziff-Davis corpus. Using this
newfound data, we propose a novel lexical-
ized noisy channel model for sentence com-
pression, achieving improved results in gram-
maticality and compression rate criteria with a
slight decrease in importance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994234375">
With the increasing success of machine translation
(MT) in recent years, several researchers have sug-
gested transferring similar methods for monolingual
text rewriting tasks. In particular, Knight and Marcu
(2000) (KM) applied a channel model to the task of
sentence compression – dropping words from an in-
dividual sentence while retaining its important in-
formation, and without sacriÞcing its grammatical-
ity. Compressed sentences can be useful either on
their own, e.g., for subtitles, or as part of a larger
summarization or MT system. A well-recognized
problem of this approach, however, is data spar-
sity. While bilingual parallel corpora are abundantly
available, monolingual parallel corpora, and espe-
cially collections of sentence compressions are van-
ishingly rare. Indeed, most work on sentence com-
pression has used the Ziff-Davis corpus (Knight and
Marcu, 2000), which consists of a mere 1067 sen-
tence pairs. While data sparsity is a common prob-
lem of many NLP tasks, it is much more severe for
sentence compression, leading Turner and Charniak
(2005) to question the applicability of the channel
model for this task altogether.
Our contribution in this paper is twofold. First,
we solve the data sparsity issue by showing that
abundant sentence compressions can be extracted
from Wikipedia’s revision history. Second, we use
this data to validate the channel model approach
for text compression, and improve upon it by cre-
ating a novel fully lexicalized compression model.
Our model improves grammaticality and compres-
sion rate with only a slight decrease in importance.
</bodyText>
<sectionHeader confidence="0.779852" genericHeader="method">
2 Data: Wikipedia revision histories as a
</sectionHeader>
<subsectionHeader confidence="0.857519">
source of sentence compressions
</subsectionHeader>
<bodyText confidence="0.999945642857143">
Many researchers are increasingly turning to
Wikipedia as a large-scale data source for training
NLP systems. The vast majority of this work uses
only the most recent version of the articles. In fact,
Wikipedia conveniently provides not only the lat-
est version, but the entire revision history of each
of its articles, as dramatically visualized by Vi«egas
et al. (2004). Through Wikipedia’s collaborative
editing process, articles are iteratively amended and
reÞned by multiple Web users. Users can usually
change any aspect of the document’s structure and
content, but for our purposes here, we focus only on
sentence-level edits that add or drop words.
We have downloaded the July snapshot of the
</bodyText>
<page confidence="0.980177">
137
</page>
<reference confidence="0.2237935">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 137–140,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999728129032258">
English Wikipedia, consisting of 1.4 million arti-
cles, and mined a subset of them for such compres-
sions/expansions. We make the simplifying assump-
tion that all such edits also retain the core mean-
ing of the sentence, and are therefore valid training
data for our purposes. This assumption is of course
patently na¬õve, as there are many cases in which such
revisions reverse sentence meaning, add or drop es-
sential information, are part of a flame war, etc.
Classifying these edits is an interesting task which
we relegate to future work.1
From about one-third of the snapshot, we ex-
tracted over 380,000 sentence pairs, which is 2 or-
ders of magnitude more than the Ziff-Davis corpus.2
Wikipedia currently has 2.3 million articles and is
constantly expanding. We can therefore expect an
increase of another order of magnitude. We thus can
afford to be extremely selective of the sentence pairs
we use. To handle a dataset of such size (hundreds of
GBs), we split it into smaller chunks, and distribute
all the processing.
More technically, for each article, we first extract
all revisions, and split each revision into a list of its
sentences. We run an edit-distance comparison be-
tween each such pair, treating each sentence as an
atomic “letter”. We look for all replacements of one
sentence by another and check whether one sentence
is a compression of the other.3 We then run Collins’
parser (1997), using just the sentence pairs where
parsing succeeds with a negative log likelihood be-
low 200.
</bodyText>
<sectionHeader confidence="0.997117" genericHeader="method">
3 Noisy channel model
</sectionHeader>
<bodyText confidence="0.99987125">
We follow KM in modeling the problem using a gen-
erative noisy channel model, but use the new-found
training data to lexicalize the model. Sentences start
their life in short form, s, are ranked by a source
language model, p(s), and then probabilistically ex-
panded to form the long sentence, p(l|s). During
decoding, given a long sentence, we seek the most
likely short sentence that could have generated it.
</bodyText>
<footnote confidence="0.897598285714286">
1For instance, compressions are more likely to signal op-
tional information than expansions; the lexical items added are
likely to be indicative of the type of edit, etc.
2The sentence pair corpus is available by contacting the
authors.
3We ignore word reorderings or replacements that are be-
yond word addition or deletion.
</footnote>
<bodyText confidence="0.983615">
Using Bayes’ rule, this is equivalent to seeking the
short sentence s that maximizes p(s) · p(l|s).
</bodyText>
<subsectionHeader confidence="0.992749">
3.1 Lexicalized channel model
</subsectionHeader>
<bodyText confidence="0.999937">
KM’s original model was purely syntax-based.
Daume et al. (2002) used a lexicalized PCFG to
rerank the compressions, showing that the addition
of lexical information helps eliminate improbable
compressions. Here, we propose to enhance lexical-
ization by including lexical information within the
channel model, allowing us to better model which
compressions are likely and which are not. A min-
imal example pair illustrating the utility of lexical-
ization is the following.
</bodyText>
<listItem confidence="0.999303">
(1) Hillary barely won the primaries.
(2) Hillary almost won the primaries.
</listItem>
<bodyText confidence="0.9972738">
The validity of dropping the adverbial here clearly
depends on the lexical value of the adverb. It is more
acceptable to drop the adverb in Sentence 1, since
dropping it in Sentence 2 reverses the meaning. We
learn probabilities of the form:
</bodyText>
<equation confidence="0.9206">
p( S[won]
NP[Hillary] ADVP[almost] VP[won]
</equation>
<bodyText confidence="0.999813230769231">
Our model has the power of making compression de-
cisions based on lexical dependencies between the
compressed and retained parts of the parse tree.
Note that Daume et al.’s reranking model cannot
achieve this type of distinction, since it is based on
reranking the compressed version, at which point the
adverb is no longer available.
Since we are interested not only in learning how
to compress, but also when to compress, we also in-
clude in this procedure unchanged CFG rule pairs
that are attested in the corpus. Thus, different ways
of expanding a CFG rule compete with each other as
well as the possibility of not doing any expansion.
</bodyText>
<subsectionHeader confidence="0.998138">
3.2 Smoothing
</subsectionHeader>
<bodyText confidence="0.999953">
In order to smooth our estimates we use Witten-Bell
discounting (1991) with 6 levels of back-off. This
method enables us to tune the confidence parameter
associated with an estimate inversely proportionally
with the diversity of the context of the estimate. The
different levels are illustrated in Table 1. Level 1,
</bodyText>
<equation confidence="0.568443333333333">
 |S[won]
NP[Hillary] VP[won]
)
</equation>
<page confidence="0.981009">
138
</page>
<bodyText confidence="0.9999765">
the most specific level, is fully lexicalized. Transi-
tioning to levels 2 to 4, we lose the lexical informa-
tion about the subtrees that are not dropped, the head
child bearing subtree, and the dropped subtrees, re-
spectively. At level 4, we end up with the non-
lexicalized estimates that are equivalent to KM’s
model. In subsequent back off levels, we abstract
away from the CFG rules. In particular, level 5 es-
timates the probability of dropping subtrees in the
context of a certain parent and head child, and level
6 estimates the probability of the same outcome in
the coarser context of a parent only.
</bodyText>
<subsectionHeader confidence="0.995829">
3.3 Source model
</subsectionHeader>
<bodyText confidence="0.99999175">
In addition to the lexicalized channel model, we also
use a lexicalized probabilistic syntax-based source
model, which we train from the parser’s output on
the short sentences of each pair.
</bodyText>
<subsectionHeader confidence="0.981748">
3.4 Decoding
</subsectionHeader>
<bodyText confidence="0.999990538461538">
We implemented the forest-based statistical sen-
tence generation method of Langkilde (2000). KM
tailored this method to sentence compression, com-
pactly encoding all compressions of a sentence in
a forest structure. The forest ranking algorithm
which extracts compressed parse trees, optimized
the model scores as well as an additional bigram
score. Since our model is lexicalized, the bigram
scores become less relevant, which was confirmed
by experimentation during development. Therefore
in our implementation we exclude the bigram scores
and other related aspects of the algorithm such as
pruning of bigram-suboptimal phrases.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99881725">
We evaluated our system using the same method as
KM, using the same 32 sentences taken from the
Ziff-Davis corpus. We solicited judgments of im-
portance (the value of the retained information), and
grammaticality for our compression, the KM results,
and human compressions from 8 judges, on a scale
of 1 (worst) to 5 (best). Mean and standard deviation
are shown in Table 2. Our model improves gram-
maticality and compression rate criteria with only a
slight decrease in importance. Here are some illus-
trative examples, with the deleted material shown in
brackets:
</bodyText>
<listItem confidence="0.912663571428571">
(3) The chemical etching process [used for glare
protection] is effective and will help if your
office has the fluorescent-light overkill [that
’s typical in offices].
(4) Prices range from $5,000 [for a microvax
2000] to $179,000 [for the vax 8000 or
higher series].
</listItem>
<bodyText confidence="0.999936916666667">
We suspect that the decrease in importance stems
from our indiscriminative usage of compressions
and expansions to train our system. We hypothesize
that in Wikipedia, expansions often add more useful
information, as opposed to compressions which are
more likely to drop superfluous or erroneous infor-
mation.4 Further work is required to classify sen-
tence modifications.
Since one of our model’s back-off levels simulates
KM’s model, we plan to perform an additional com-
parative evaluation of both models trained on the
same data.
</bodyText>
<sectionHeader confidence="0.986631" genericHeader="conclusions">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999977619047619">
Turner and Charniak (2005) question the viability
of a noisy channel model for the sentence compres-
sion task. Briefly put, in the typically sparse data
setting, there is no way to distinguish between the
probability of a sentence as a short sentence and its
probability as a regular sentence of English. Fur-
thermore, the channel model is likely to prefer to
leave sentences intact, since that is the most preva-
lent pattern in the training data. Thus, they argue,
the channel model is not really compressing, and it
is only by virtue of the length penalty that anything
gets shortened at all. Our hope here is that by using
a far richer source of short sentences, as well as a
huge source of compressions, we can overcome this
problem. The noisy channel model posits a virtual
competition on each word of coming either from the
source model (in which case it is retained in the com-
pression) or from the channel model (in which case
it is dropped). By having access to a large data set
for the first time, we hope to be able to learn which
parts of the sentence are more likely to come from
</bodyText>
<footnote confidence="0.9966126">
4For instance, here is an expansion seen in the data, where
the added information (italicized) is important: “In 1952 and
1953 he was stationed in Sendai, Japan during the Korean War
and was shot.” It would be undesirable to drop this added
phrase.
</footnote>
<page confidence="0.989103">
139
</page>
<table confidence="0.880819571428572">
Back-off level expanded short
1 S[won] NP[Hillary] ADVP[almost] VP[won] S[won] NP[Hillary] VP[won]
2 S[won] NP ADVP[almost] VP[won] S[won] NP VP[won]
3 S NP ADVP[almost] VP S NP VP
4 S NP ADVP VP S NP VP
5 parent = S, head-child = VP, child = ADVP parent = S, head-child = VP
6 parent = S, child = ADVP parent = S
</table>
<tableCaption confidence="0.990287">
Table 1: Back off levels
</tableCaption>
<table confidence="0.999901">
KM Our model Humans
Compression 72.91% 67.38% 53.33%
Grammaticality 4.02±1.03 4.31±0.78 4.78±0.17
Importance 3.86±1.09 3.65±1.07 3.90±0.58
</table>
<tableCaption confidence="0.999105">
Table 2: Evaluation results
</tableCaption>
<bodyText confidence="0.999938222222222">
which of the two parts of the model. Further work is
required in order to clarify this point.
Naturally, discriminative models such as McDon-
ald (2006) are also likely to improve by using the
added data. We leave the exploration of this topic
for future work.
Finally, we believe that the Wikipedia revision
history offers a wonderful resource for many addi-
tional NLP tasks, which we have begun exploring.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999719714285714">
This work was partially supported by a Google re-
search award, “Mining Wikipedia’s Revision His-
tory”. We thank Stuart Shieber for his comments on
an early draft of this paper, Kevin Knight and Daniel
Marcu for sharing the Ziff-Davis dataset with us, and
the volunteers for rating sentences. Yamangil thanks
Michael Collins for his feedback on the project idea.
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999244488888889">
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Philip R. Cohen and
Wolfgang Wahlster, editors, Proceedings of the Thirty-
Fifth Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 16–23, Somerset, New Jersey. As-
sociation for Computational Linguistics.
H. Daume, Kevin Knight, I Langkilde-Geary, Daniel
Marcu, and K Yamada. 2002. The importance of lexi-
calized syntax models for natural language generation
tasks. Proceedings of the Second International Confer-
ence on Natural Language Generation. Arden House,
NJ, July 1-3.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 703–710. AAAI Press / The MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the first conference on
North American chapter of the Association for Com-
putational Linguistics, pages 170–177, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Ryan T. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL 2006, 11st Conference of the European Chap-
ter of the Association for Computational Linguistics,
April 3-7, 2006, Trento, Italy, pages 297–304.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In ACL ’05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
290–297, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Fernanda B. Vi«egas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conßict be-
tween authors with istory flow visualizations. In Eliza-
beth Dykstra-Erickson and Manfred Tscheligi, editors,
CHI, pages 575–582. ACM.
I. Witten and T. Bell. 1991. The zero-frequency problem:
Estimating the probabilities of novel events in adaptive
text compression. IEEE Transactions on Information
Theory, 37(4).
</reference>
<page confidence="0.997511">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.995137">
<title confidence="0.999889">Mining Wikipedia Revision Histories for Improving Sentence Compression</title>
<author confidence="0.999386">Elif Yamangil Rani Nelken</author>
<affiliation confidence="0.9997225">School of Engineering and Applied Sciences Harvard University</affiliation>
<address confidence="0.999992">Cambridge, MA 02138, USA</address>
<abstract confidence="0.999779">A well-recognized limitation of research on supervised sentence compression is the dearth of available training data. We propose a new and bountiful resource for such training data, which we obtain by mining the revision history of Wikipedia for sentence compressions and expansions. Using only a fraction of the available Wikipedia data, we have collected a training corpus of over 380,000 sentence pairs, two orders of magnitude larger than the standardly used Ziff-Davis corpus. Using this newfound data, we propose a novel lexicalized noisy channel model for sentence compression, achieving improved results in grammaticality and compression rate criteria with a slight decrease in importance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>137--140</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 137–140,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>Proceedings of the ThirtyFifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<editor>In Philip R. Cohen and Wolfgang Wahlster, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the ThirtyFifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 16–23, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
<author>Kevin Knight</author>
<author>I Langkilde-Geary</author>
<author>Daniel Marcu</author>
<author>K Yamada</author>
</authors>
<title>The importance of lexicalized syntax models for natural language generation tasks.</title>
<date>2002</date>
<booktitle>Proceedings of the Second International Conference on Natural Language Generation. Arden House, NJ,</booktitle>
<marker>Daume, Knight, Langkilde-Geary, Marcu, Yamada, 2002</marker>
<rawString>H. Daume, Kevin Knight, I Langkilde-Geary, Daniel Marcu, and K Yamada. 2002. The importance of lexicalized syntax models for natural language generation tasks. Proceedings of the Second International Conference on Natural Language Generation. Arden House, NJ, July 1-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>703--710</pages>
<publisher>AAAI Press / The MIT Press.</publisher>
<contexts>
<context position="1150" citStr="Knight and Marcu (2000)" startWordPosition="161" endWordPosition="164">on of the available Wikipedia data, we have collected a training corpus of over 380,000 sentence pairs, two orders of magnitude larger than the standardly used Ziff-Davis corpus. Using this newfound data, we propose a novel lexicalized noisy channel model for sentence compression, achieving improved results in grammaticality and compression rate criteria with a slight decrease in importance. 1 Introduction With the increasing success of machine translation (MT) in recent years, several researchers have suggested transferring similar methods for monolingual text rewriting tasks. In particular, Knight and Marcu (2000) (KM) applied a channel model to the task of sentence compression – dropping words from an individual sentence while retaining its important information, and without sacriÞcing its grammaticality. Compressed sentences can be useful either on their own, e.g., for subtitles, or as part of a larger summarization or MT system. A well-recognized problem of this approach, however, is data sparsity. While bilingual parallel corpora are abundantly available, monolingual parallel corpora, and especially collections of sentence compressions are vanishingly rare. Indeed, most work on sentence compression</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 703–710. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>170--177</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 170–177, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>297--304</pages>
<location>Trento, Italy,</location>
<marker>McDonald, 2006</marker>
<rawString>Ryan T. McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, April 3-7, 2006, Trento, Italy, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>290--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1996" citStr="Turner and Charniak (2005)" startWordPosition="296" endWordPosition="299">n be useful either on their own, e.g., for subtitles, or as part of a larger summarization or MT system. A well-recognized problem of this approach, however, is data sparsity. While bilingual parallel corpora are abundantly available, monolingual parallel corpora, and especially collections of sentence compressions are vanishingly rare. Indeed, most work on sentence compression has used the Ziff-Davis corpus (Knight and Marcu, 2000), which consists of a mere 1067 sentence pairs. While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. Our contribution in this paper is twofold. First, we solve the data sparsity issue by showing that abundant sentence compressions can be extracted from Wikipedia’s revision history. Second, we use this data to validate the channel model approach for text compression, and improve upon it by creating a novel fully lexicalized compression model. Our model improves grammaticality and compression rate with only a slight decrease in importance. 2 Data: Wikipedia revision histories as a source of sentence compressions Many </context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 290–297, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernanda B Vi«egas</author>
<author>Martin Wattenberg</author>
<author>Kushal Dave</author>
</authors>
<title>Studying cooperation and conßict between authors with istory flow visualizations.</title>
<date>2004</date>
<booktitle>In Elizabeth Dykstra-Erickson and</booktitle>
<pages>575--582</pages>
<editor>Manfred Tscheligi, editors, CHI,</editor>
<publisher>ACM.</publisher>
<marker>Vi«egas, Wattenberg, Dave, 2004</marker>
<rawString>Fernanda B. Vi«egas, Martin Wattenberg, and Kushal Dave. 2004. Studying cooperation and conßict between authors with istory flow visualizations. In Elizabeth Dykstra-Erickson and Manfred Tscheligi, editors, CHI, pages 575–582. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>T Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Witten, Bell, 1991</marker>
<rawString>I. Witten and T. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>