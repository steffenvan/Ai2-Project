<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.98467">
Hybrid Simplification using Deep Semantics and Machine Translation
</title>
<author confidence="0.972072">
Shashi Narayan Claire Gardent
</author>
<affiliation confidence="0.910712">
Universit´e de Lorraine, LORIA CNRS, LORIA, UMR 7503
</affiliation>
<address confidence="0.888587">
Villers-l`es-Nancy, F-54600, France Vandoeuvre-l`es-Nancy, F-54500, France
</address>
<email confidence="0.994349">
shashi.narayan@loria.fr claire.gardent@loria.fr
</email>
<sectionHeader confidence="0.993747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869117647059">
We present a hybrid approach to sentence
simplification which combines deep se-
mantics and monolingual machine transla-
tion to derive simple sentences from com-
plex ones. The approach differs from pre-
vious work in two main ways. First, it
is semantic based in that it takes as in-
put a deep semantic representation rather
than e.g., a sentence or a parse tree. Sec-
ond, it combines a simplification model
for splitting and deletion with a monolin-
gual translation model for phrase substi-
tution and reordering. When compared
against current state of the art methods,
our model yields significantly simpler out-
put that is both grammatical and meaning
preserving.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980124590164">
Sentence simplification maps a sentence to a sim-
pler, more readable one approximating its con-
tent. Typically, a simplified sentence differs from
a complex one in that it involves simpler, more
usual and often shorter, words (e.g., use instead
of exploit); simpler syntactic constructions (e.g.,
no relative clauses or apposition); and fewer mod-
ifiers (e.g., He slept vs. He also slept). In prac-
tice, simplification is thus often modeled using
four main operations: splitting a complex sen-
tence into several simpler sentences; dropping and
reordering phrases or constituents; substituting
words/phrases with simpler ones.
As has been argued in previous work, sentence
simplification has many potential applications. It
is useful as a preprocessing step for a variety of
NLP systems such as parsers and machine trans-
lation systems (Chandrasekar et al., 1996), sum-
marisation (Knight and Marcu, 2000), sentence
fusion (Filippova and Strube, 2008) and semantic
role labelling (Vickrey and Koller, 2008). It also
has wide ranging potential societal application as a
reading aid for people with aphasis (Carroll et al.,
1999), for low literacy readers (Watanabe et al.,
2009) and for non native speakers (Siddharthan,
2002).
There has been much work recently on de-
veloping computational frameworks for sentence
simplification. Synchronous grammars have been
used in combination with linear integer program-
ming to generate and rank all possible rewrites of
an input sentence (Dras, 1999; Woodsend and La-
pata, 2011). Machine Translation systems have
been adapted to translate complex sentences into
simple ones (Zhu et al., 2010; Wubben et al., 2012;
Coster and Kauchak, 2011). And handcrafted
rules have been proposed to model the syntactic
transformations involved in simplifications (Sid-
dharthan et al., 2004; Siddharthan, 2011; Chan-
drasekar et al., 1996).
In this paper, we present a hybrid approach to
sentence simplification which departs from this
previous work in two main ways.
First, it combines a model encoding probabil-
ities for splitting and deletion with a monolin-
gual machine translation module which handles
reordering and substitution. In this way, we ex-
ploit the ability of statistical machine translation
(SMT) systems to capture phrasal/lexical substi-
tution and reordering while relying on a dedi-
cated probabilistic module to capture the splitting
and deletion operations which are less well (dele-
tion) or not at all (splitting) captured by SMT ap-
proaches.
Second, our approach is semantic based. While
previous simplification approaches starts from ei-
ther the input sentence or its parse tree, our model
takes as input a deep semantic representation
namely, the Discourse Representation Structure
(DRS, (Kamp, 1981)) assigned by Boxer (Curran
et al., 2007) to the input complex sentence. As we
</bodyText>
<page confidence="0.989247">
435
</page>
<note confidence="0.831244">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435–445,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999440769230769">
shall see in Section 4, this permits a linguistically
principled account of the splitting operation in that
semantically shared elements are taken to be the
basis for splitting a complex sentence into sev-
eral simpler ones; this facilitates completion (the
re-creation of the shared element in the split sen-
tences); and this provide a natural means to avoid
deleting obligatory arguments.
When compared against current state of the art
methods (Zhu et al., 2010; Woodsend and Lapata,
2011; Wubben et al., 2012), our model yields sig-
nificantly simpler output that is both grammatical
and meaning preserving.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99992178125">
Earlier work on sentence simplification relied
on handcrafted rules to capture syntactic sim-
plification e.g., to split coordinated and subor-
dinated sentences into several, simpler clauses
or to model active/passive transformations (Sid-
dharthan, 2002; Chandrasekar and Srinivas, 1997;
Bott et al., 2012; Canning, 2002; Siddharthan,
2011; Siddharthan, 2010). While these hand-
crafted approaches can encode precise and linguis-
tically well-informed syntactic transformation (us-
ing e.g., detailed morphological and syntactic in-
formation), they are limited in scope to purely syn-
tactic rules and do not account for lexical simpli-
fications and their interaction with the sentential
context.
Using the parallel dataset formed by Simple En-
glish Wikipedia (SWKP)1 and traditional English
Wikipedia (EWKP)2, more recent work has fo-
cused on developing machine learning approaches
to sentence simplification.
Zhu et al. (2010) constructed a parallel cor-
pus (PWKP) of 108,016/114,924 complex/simple
sentences by aligning sentences from EWKP and
SWKP and used the resulting bitext to train a sim-
plification model inspired by syntax-based ma-
chine translation (Yamada and Knight, 2001).
Their simplification model encodes the probabil-
ities for four rewriting operations on the parse
tree of an input sentences namely, substitution, re-
ordering, splitting and deletion. It is combined
with a language model to improve grammatical-
ity and the decoder translates sentences into sim-
</bodyText>
<footnote confidence="0.9898814">
1SWKP (http://simple.wikipedia.org) is a
corpus of simple texts targeting “children and adults who are
learning English Language” and whose authors are requested
to “use easy words and short sentences”.
2http://en.wikipedia.org
</footnote>
<bodyText confidence="0.999847488372093">
pler ones by greedily selecting the output sentence
with highest probability.
Using both the PWKP corpus developed by
Zhu et al. (2010) and the edit history of Simple
Wikipedia, Woodsend and Lapata (2011) learn a
quasi synchronous grammar (Smith and Eisner,
2006) describing a loose alignment between parse
trees of complex and of simple sentences. Fol-
lowing Dras (1999), they then generate all possi-
ble rewrites for a source tree and use integer lin-
ear programming to select the most appropriate
simplification. They evaluate their model on the
same dataset used by Zhu et al. (2010) namely,
an aligned corpus of 100/131 EWKP/SWKP sen-
tences and show that they achieve better BLEU
score. They also conducted a human evaluation
on 64 of the 100 test sentences and showed again
a better performance in terms of simplicity, gram-
maticality and meaning preservation.
In (Wubben et al., 2012; Coster and Kauchak,
2011), simplification is viewed as a monolingual
translation task where the complex sentence is the
source and the simpler one is the target. To ac-
count for deletions, reordering and substitution,
Coster and Kauchak (2011) trained a phrase based
machine translation system on the PWKP corpus
while modifying the word alignment output by
GIZA++ in Moses to allow for null phrasal align-
ments. In this way, they allow for phrases to be
deleted during translation. No human evaluation
is provided but the approach is shown to result in
statistically significant improvements over a tradi-
tional phrase based approach. Similarly, Wubben
et al. (2012) use Moses and the PWKP data to train
a phrase based machine translation system aug-
mented with a post-hoc reranking procedure de-
signed to rank the output based on their dissim-
ilarity from the source. A human evaluation on
20 sentences randomly selected from the test data
indicates that, in terms of fluency and adequacy,
their system is judged to outperform both Zhu et
al. (2010) and Woodsend and Lapata (2011) sys-
tems.
</bodyText>
<sectionHeader confidence="0.993356" genericHeader="method">
3 Simplification Framework
</sectionHeader>
<bodyText confidence="0.9998586">
We start by motivating our approach and explain-
ing how it relates to previous proposals w.r.t.,
the four main operations involved in simplifica-
tion namely, splitting, deletion, substitution and
reordering. We then introduce our framework.
</bodyText>
<page confidence="0.998768">
436
</page>
<bodyText confidence="0.989336375">
Sentence Splitting. Sentence splitting is ar-
guably semantic based in that in many cases, split-
ting occurs when the same semantic entity partici-
pates in two distinct eventualities. For instance, in
example (1) below, the split is on the noun bricks
which is involved in two eventualities namely,
“being resistant to cold” and “enabling the con-
struction ofpermanent buildings”.
</bodyText>
<figureCaption confidence="0.26826625">
(1) C. Being more resistant to cold, bricks enabled the con-
struction of permanent buildings.
S. Bricks were more resistant to cold. Bricks enabled
the construction of permanent buildings.
</figureCaption>
<bodyText confidence="0.999949521739131">
While splitting opportunities have a clear coun-
terpart in syntax (i.e., splitting often occurs when-
ever a relative, a subordinate or an appositive
clause occurs in the complex sentence), comple-
tion i.e., the reconstruction of the shared element
in the second simpler clause, is arguably seman-
tically governed in that the reconstructed element
corefers with its matching phrase in the first sim-
pler clause. While our semantic based approach
naturally accounts for this by copying the phrase
corresponding to the shared entity in both phrases,
syntax based approach such as Zhu et al. (2010)
and Woodsend and Lapata (2011) will often fail to
appropriately reconstruct the shared phrase and in-
troduce agreement mismatches because the align-
ment or rules they learn are based on syntax alone.
For instance, in example (2), Zhu et al. (2010)
fails to copy the shared argument “The judge” to
the second clause whereas Woodsend and Lapata
(2011) learns a synchronous rule matching (VP
and VP) to (VP. NP(It) VP) thereby failing to pro-
duce the correct subject pronoun (“he” or “she”)
for the antecedent “The judge”.
</bodyText>
<listItem confidence="0.665413555555556">
(2) C. The judge ordered that Chapman should receive
psychiatric treatment in prison and sentenced him to
twenty years to life.
S1. The judge ordered that Chapman should get psychi-
atric treatment. In prison and sentenced him to twenty
years to life. (Zhu et al., 2010)
S2. The judge ordered that Chapman should receive
psychiatric treatment in prison. It sentenced him to
twenty years to life. (Woodsend and Lapata, 2011)
Deletion. By handling deletion using a proba-
bilistic model trained on semantic representations,
we can avoid deleting obligatory arguments. Thus
in our approach, semantic subformulae which are
related to a predicate by a core thematic roles (e.g.,
agent and patient) are never considered for dele-
tion. By contrast, syntax based approaches (Zhu
et al., 2010; Woodsend and Lapata, 2011) do not
distinguish between optional and obligatory argu-
ments. For instance Zhu et al. (2010) simplifies
(3C) to (3S) thereby incorrectly deleting the oblig-
atory theme (gifts) of the complex sentence and
modifying its meaning to giving knights and war-
riors (instead of giving gifts to knights and war-
riors).
(3) C. Women would also often give knights and warriors
gifts that included thyme leaves as it was believed to
bring courage to the bearer.
</listItem>
<bodyText confidence="0.960853454545454">
S. Women also often give knights and warriors. Gifts
included thyme leaves as it was thought to bring
courage to the saint. (Zhu et al., 2010)
We also depart from Coster and Kauchak (2011)
who rely on null phrasal alignments for deletion
during phrase based machine translation. In their
approach, deletion is constrained by the training
data and the possible alignments, independent of
any linguistic knowledge.
Substitution and Reordering SMT based ap-
proaches to paraphrasing (Barzilay and Elhadad,
2003; Bannard and Callison-Burch, 2005) and to
sentence simplification (Wubben et al., 2012) have
shown that by utilising knowledge about align-
ment and translation probabilities, SMT systems
can account for the substitutions and the reorder-
ings occurring in sentence simplification. Fol-
lowing on these approaches, we therefore rely on
phrase based SMT to learn substitutions and re-
ordering. In addition, the language model we in-
tegrate in the SMT module helps ensuring better
fluency and grammaticality.
</bodyText>
<subsectionHeader confidence="0.999637">
3.1 An Example
</subsectionHeader>
<bodyText confidence="0.8261373125">
Figure 1 shows how our approach simplifies (4C)
into (4S).
(4) C. In 1964 Peter Higgs published his second paper in
Physical Review Letters describing Higgs mechanism
which predicted a new massive spin-zero boson for the
first time.
S. Peter Higgs wrote his paper explaining Higgs mech-
anism in 1964. Higgs mechanism predicted a new ele-
mentary particle.
The DRS for (4C) produced using Boxer (Cur-
ran et al., 2007) is shown at the top of the Figure
and a graph representation3 of the dependencies
between its variables is shown immediately below.
Each DRS variable labels a node in the graph and
each edge is labelled with the relation holding be-
tween the variables labelling its end vertices. The
</bodyText>
<footnote confidence="0.531372">
3The DRS to graph conversion goes through several pre-
processing steps: the relation nn is inverted making modi-
fier noun (higgs) dependent of modified noun (mechanism),
named and timex are converted to unary predicates, e.g.,
named(x,peter) is mapped to peter(x) and timex(x) =
1964 is mapped to 1964(x); and nodes are introduced for
orphan words (e.g., which).
</footnote>
<page confidence="0.994304">
437
</page>
<figure confidence="0.621833375">
[Discourse Representation Structure produced by BOXER]
∧(
∧(
∧(
;(
((
∧
X2
</figure>
<equation confidence="0.991849842857143">
second(X2)
paper(X2)
of(X2, X1)
X1
male(X1)
X3
publish(X3)
agent(X3, X0)
patient(X3, X2)
X0
named(X0, higgs, per)
named(X0, peter, per)
X4
named(X4, physical, org)
named(X4, review, org)
named(X4, letters, org)
X5
thing(X5) X7, X8
event(X3) Xs mechanism(X8)
in(X3, X4) )))));( ;( ∧
in(X3, X5) nn(X7, X8)
named(X7, higgs, org)
timex(X5) = 1964
X9, X10, X11, X12
new(X9)
massive(X9)
spin-zero(X9)
boson(X9)
predict(X10)
event(X11)
first(X12)
time(X12)
agent(X10, X8)
patient(X10, X9)
agent(X11, Xs)
patient(X11, X8)
for(X10, X12)
event(X10)
describe(X11)
R10 17 patient, X10 --+ X9
R11 23 for, X10 --+ X12
R1 5 agent, X3 --+ X0
R2 5 patient, X3 --+ X2
R3 6 of, X2 --+ X1
R4 9 in, X3 --+ X4
R5 1 in, X3 --+ X5
Rs 13 agent, X11 --+ Xs
R7 13 patient, X11 --+ X8
R8 −− nn, X8 --+ X7
R9 17 agent, X10 --+ X8
rel pos. in S predicate
node pos. in S predicate/type
X0 3, 4 higgs/per, peter/per
X1 6 male/a
X2 6, 7, 8 second/a, paper/a
X3 5 publish/v, event
physical/org
X4 10, 11, 12
review/org, letters/org
X5 2 thing/n, 1964
Xs 6, 7, 8 −−
X7 14 higgs/org
X8 14, 15 mechanism/n
18, 19, 20 new/a, spin-zero/a
21, 22 massive/a, boson/n
X10 17 predict/v, event
X11 13 describe/v, event
X12 24, 25, 26 first/a, time/n
O1 16 which/WDT
X9
</equation>
<bodyText confidence="0.910799">
In 1964 Peter Higgs published his � � � � � Higgs mechanism predicted
paper describing Higgs mechanism a new boson
PBMT+LM
Peter Higgs wrote his paper explaining Higgs mechanism predicted
</bodyText>
<equation confidence="0.738430714285714">
( )
Higgs mechanism in 1964 . a new elementary particle .
ROOT
X4
X5
Xs
O1
X3
X10
R1 R2
R11
R4
R10
X0
X9
X12
X7
X11
R5 Rs R7 R9
X2
R3
X1
X8
R8
[DRS Graph Representation]
� � � � �
ROOT
SPLIT
ROOT
R3
X1
R8
X7
R8
X7
( X3 X11 X10 O1 )
R9 R11
R1 R2
R5
Rs R7
X0
X2
R3
X1
R4
X4
X5
Xs
X8
R8
X7
X8
R8
X7
R10
X12
X9
� � � � �
ROOT
ROOT
DELETION
X3
R5
R1
R2
X11
Rs R7
X10
R9 R10
( X0 X2 X5 Xs X8 X8 X9 )
</equation>
<figureCaption confidence="0.995287">
Figure 1: Simplification of “In 1964 Peter Higgs published his second paper in Physical Review Letters
describing Higgs mechanism which predicted a new massive spin-zero boson for the first time .”
</figureCaption>
<page confidence="0.987063">
438
</page>
<bodyText confidence="0.983214125">
jectives second, massive, spin-zero and the orphan
word which.
Substitution and Reordering. Finally the trans-
lation and language model ensures that published,
describing and boson are simplified to wrote, ex-
plaining and elementary particle respectively; and
that the phrase “In 1964” is moved from the be-
ginning of the sentence to its end.
</bodyText>
<subsectionHeader confidence="0.987434">
3.2 The Simplification Model
</subsectionHeader>
<bodyText confidence="0.99936425">
Our simplification framework consists of a prob-
abilistic model for splitting and dropping which
we call DRS simplification model (DRS-SM); a
phrase based translation model for substitution
and reordering (PBMT); and a language model
learned on Simple English Wikipedia (LM) for
fluency and grammaticality. Given a complex sen-
tence c, we split the simplification process into
two steps. First, DRS-SM is applied to D, (the
DRS representation of the complex sentence c)
to produce one or more (in case of splitting) in-
termediate simplified sentence(s) s′. Second, the
simplified sentence(s) s′ is further simplified to s
using a phrase based machine translation system
(PBMT+LM). Hence, our model can be formally
defined as:
</bodyText>
<equation confidence="0.997802166666667">
sˆ = argmax p(s|c)
s
= argmax p(s′|c)p(s|s′)
s
= argmax p(s′|Dc)p(s′|s)p(s)
s
</equation>
<bodyText confidence="0.999810714285714">
where the probabilities p(s′|D,), p(s′|s) and
p(s) are given by the DRS simplification model,
the phrase based machine translation model and
the language model respectively.
To get the DRS simplification model, we com-
bine the probability of splitting with the probabil-
ity of deletion:
</bodyText>
<equation confidence="0.9880105">
p(s′|Dc) = E p(Dsplit|Dc)p(Ddel|Dsplit)
θ:str(θ(Dc))=s′
</equation>
<bodyText confidence="0.999835333333333">
where B is a sequence of simplification opera-
tions and str(B(D,)) is the sequence of words as-
sociated with a DRS resulting from simplifying D,
using B.
The probability of a splitting operation for a
given DRS D, is:
</bodyText>
<equation confidence="0.784446">
p(Dsplit|Dc) =
</equation>
<table confidence="0.527150111111111">
SPLIT(sptrue
cand), split at spcand
SPLIT(spfalse
cand), otherwise
�
�
�
H
spcand
</table>
<bodyText confidence="0.999654166666667">
two tables to the right of the picture show the pred-
icates (top table) associated with each variable and
the relation label (bottom table) associated with
each edge. Boxer also outputs the associated po-
sitions in the complex sentence for each predicate
(not shown in the DRS but in the graph tables). Or-
phan words (OW) i.e., words which have no cor-
responding material in the DRS (e.g., which at po-
sition 16), are added to the graph (node O1) thus
ensuring that the position set associated with the
graph exactly matches the positions in the input
sentence and thus deriving the input sentence.
</bodyText>
<table confidence="0.975766">
Split Candidate isSplit prob.
(agent, for, patient) - (agent, in, in, true 0.63
patient) false 0.37
</table>
<tableCaption confidence="0.999506">
Table 1: Simplification: SPLIT
</tableCaption>
<bodyText confidence="0.999925516129032">
Given the input DRS shown in Figure 1, simpli-
fication proceeds as follows.
Splitting. The splitting candidates of a DRS are
event pairs contained in that DRS. More precisely,
the splitting candidates are pairs4 of event vari-
ables associated with at least one of the core the-
matic roles (e.g., agent and patient). The features
conditioning a split are the set of thematic roles as-
sociated with each event variable. The DRS shown
in Figure 1 contains three such event variables
X3, X11 and X10 with associated thematic role
sets {agent, in, in, patient}, {agent, patient} and
{agent, for, patient} respectively. Hence, there are
3 splitting candidates (X3-X11, X3-X10 and X10-
X11) and 4 split options: no split or split at one of
the splitting candidates. Here the split with highest
probability (cf. Table 1) is chosen and the DRS is
split into two sub-DRS, one containing X3, and
the other containing X10. After splitting, dan-
gling subgraphs are attached to the root of the new
subgraph maximizing either proximity or position
overlap. Here the graph rooted in X11 is attached
to the root dominating X3 and the orphan word O1
to the root dominating X10.
Deletion. The deletion model (cf. Table 2) reg-
ulates the deletion of relations and their associated
subgraph; of adjectives and adverbs; and of orphan
words. Here, the relations in between X3 and X4
and for between X10 and X12 are deleted resulting
in the deletion of the phrases “in Physical Review
Letters” and “for thefirst time” as well as the ad-
</bodyText>
<footnote confidence="0.994070333333333">
4The splitting candidates could be sets of event variables
depending on the number of splits required. Here, we con-
sider pairs for 2 splits.
</footnote>
<page confidence="0.996522">
439
</page>
<table confidence="0.99986935">
relation candidate isDrop prob.
relation length
word range
in 0-2 true 0.22
false 0.72
in 2-5 true 0.833
false 0.167
mod. cand. isDrop prob.
mod word
new true 0.22
false 0.72
true 0.833
massive false 0.167
OW candidate isDrop prob.
orphan isBoundary
word
and true true 0.82
false 0.18
which false true 0.833
false 0.167
</table>
<tableCaption confidence="0.998405">
Table 2: Simplification: DELETION (Relations, modifiers and OW respectively)
</tableCaption>
<bodyText confidence="0.983356380952381">
root
That is, if the DRS is split on the splitting candi-
date spcand, the probability of the split is then given
by the SPLIT table (Table 1) for the isSplit value
“true” and the split candidate spcand; else it is the
product of the probability given by the SPLIT table
for the isSplit value “false” for all split candidate
considered for Dc. As mentioned above, the fea-
tures used for determining the split operation are
the role sets associated with pairs of event vari-
ables (cf. Table 1).
The deletion probability is given by three mod-
els: a model for relations determining the deletion
of prepositional phrases; a model for modifiers
(adjectives and adverbs) and a model for orphan
words (Table 2). All three deletion models use the
associated word itself as a feature. In addition, the
model for relations uses the PP length-range as a
feature while the model for orphan words relies on
boundary information i.e., whether or not, the OW
occurs at the associated sentence boundary.
</bodyText>
<equation confidence="0.95193575">
�P(Ddel|Dsplit) = �DELrel(relcand) DELmod(modcand)
relcand modcand
ri DELow(owcand)
owcand
</equation>
<subsectionHeader confidence="0.998462">
3.3 Estimating the parameters
</subsectionHeader>
<bodyText confidence="0.9994668125">
We use the EM algorithm (Dempster et al., 1977)
to estimate our split and deletion model parame-
ters. For an efficient implementation of EM algo-
rithm, we follow the work of Yamada and Knight
(2001) and Zhu et al. (2010); and build training
graphs (Figure 2) from the pair of complex and
simple sentence pairs in the training data.
Each training graph represents a complex-
simple sentence pair and consists of two types
of nodes: major nodes (M-nodes) and operation
nodes (O-nodes). An M-node contains the DRS
representation Dc of a complex sentence c and the
associated simple sentence(s) si while O-nodes
determine split and deletion operations on their
parent M-node. Only the root M-node is consid-
ered for the split operations. For example, given
</bodyText>
<equation confidence="0.542467333333333">
split
del-rel∗; del-mod∗; del-ow∗
fin
</equation>
<figureCaption confidence="0.995548">
Figure 2: An example training graph
</figureCaption>
<bodyText confidence="0.9999015">
the root M-node (Dc, (s1, s2)), multiple success-
ful split O-nodes will be created, each one further
creating two M-nodes (Dc1, s1) and (Dc2, s2). For
the training pair (c, s), the root M-node (Dc, s) is
followed by a single split O-node producing an M-
node (Dc, s) and counting all split candidates in Dc
for failed split. The M-nodes created after split op-
erations are then tried for multiple deletion opera-
tions of relations, modifiers and OW respectively.
Each deletion candidate creates a deletion O-node
marking successful or failed deletion of the can-
didate and a result M-node. The deletion process
continues on the result M-node until there is no
deletion candidate left to process. The governing
criteria for the construction of the training graph
is that, at each step, it tries to minimize the Leven-
shtein edit distance between the complex and the
simple sentences. Moreover, for the splitting op-
eration, we introduce a split only if the reference
sentence consists of several sentences (i.e., there
is a split in the training data); and only consider
splits which maximises the overlap between split
and simple reference sentences.
We initialize our probability tables Table 1 and
Table 2 with the uniform distribution, i.e., 0.5 be-
cause all our features are binary. The EM algo-
rithm iterates over training graphs counting model
features from O-nodes and updating our probabil-
ity tables. Because of the space constraints, we
do not describe our algorithm in details. We refer
the reader to (Yamada and Knight, 2001) for more
details.
</bodyText>
<page confidence="0.994816">
440
</page>
<bodyText confidence="0.99995955">
Our phrase based translation model is trained
using the Moses toolkit5 with its default command
line options on the PWKP corpus (except the sen-
tences from the test set) considering the complex
sentence as the source and the simpler one as the
target. Our trigram language model is trained us-
ing the SRILM toolkit6 on the SWKP corpus7.
Decoding. We explore the decoding graph sim-
ilar to the training graph but in a greedy approach
always picking the choice with maximal probabil-
ity. Given a complex input sentence c, a split O-
node will be selected corresponding to the deci-
sion of whether to split and where to split. Next,
deletion O-nodes are selected indicating whether
or not to drop each of the deletion candidate. The
DRS associated with the final M-node Dfin is then
mapped to a simplified sentence s′fin which is
further simplified using the phrase-based machine
translation system to produce the final simplified
sentence ssimple.
</bodyText>
<sectionHeader confidence="0.999585" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999895666666667">
We trained our simplification and translation mod-
els on the PWKP corpus. To evaluate perfor-
mance, we compare our approach with three other
state of the art systems using the test set provided
by Zhu et al. (2010) and relying both on automatic
metrics and on human judgments.
</bodyText>
<subsectionHeader confidence="0.992002">
4.1 Training and Test Data
</subsectionHeader>
<bodyText confidence="0.9998403125">
The DRS-Based simplification model is trained
on PWKP, a bi-text of complex and simple sen-
tences provided by Zhu et al. (2010). To construct
this bi-text, Zhu et al. (2010) extracted complex
and simple sentences from EWKP and SWKP re-
spectively and automatically aligned them using
TF*IDF as a similarity measure. PWKP contains
108016/114924 complex/simple sentence pairs.
We tokenize PWKP using Stanford CoreNLP
toolkit8. We then parse all complex sentences
in PWKP using Boxer9 to produce their DRSs.
Finally, our DRS-Based simplification model is
trained on 97.75% of PWKP; we drop out 2.25%
of the complex sentences in PWKP which are re-
peated in the test set or for which Boxer fails to
produce DRSs.
</bodyText>
<footnote confidence="0.9997655">
5http://www.statmt.org/moses/
6http://www.speech.sri.com/projects/srilm/
7We downloaded the snapshots of Simple Wikipedia
dated 2013-10-30 available at http://dumps.wikimedia.org/.
8http://nlp.stanford.edu/software/corenlp.shtml
9http://svn.ask.it.usyd.edu.au/trac/candc, Version 1.00
</footnote>
<bodyText confidence="0.9999705">
We evaluate our model on the test set used by
Zhu et al. (2010) namely, an aligned corpus of
100/131 EWKP/SWKP sentences. Boxer pro-
duces a DRS for 96 of the 100 input sentences.
These input are simplified using our simplifica-
tion system namely, the DRS-SM model and the
phrase-based machine translation system (Section
3.2). For the remaining four complex sentences,
Boxer fails to produce DRSs. These four sen-
tences are directly sent to the phrase-based ma-
chine translation system to produce simplified sen-
tences.
</bodyText>
<subsectionHeader confidence="0.998239">
4.2 Automatic Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99996045945946">
To assess and compare simplification systems, two
main automatic metrics have been used in previ-
ous work namely, BLEU and the Flesch-Kincaid
Grade Level Index (FKG).
The FKG index is a readability metric taking
into account the average sentence length in words
and the average word length in syllables. In its
original context (language learning), it was ap-
plied to well formed text and thus measured the
simplicity of a well formed sentence. In the con-
text of the simplification task however, the auto-
matically generated sentences are not necessarily
well formed so that the FKG index reduces to a
measure of the sentence length (in terms of words
and syllables) approximating the simplicity level
of an output sentence irrespective of the length
of the corresponding input. To assess simplifica-
tion, we instead use metrics that are directly re-
lated to the simplification task namely, the number
of splits in the overall (test and training) data and
in average per sentences; the number of generated
sentences with no edits i.e., which are identical to
the original, complex one; and the average Leven-
shtein distance between the system’s output and
both the complex and the simple reference sen-
tences.
BLEU gives a measure of how close a system’s
output is to the gold standard simple sentence. Be-
cause there are many possible ways of simplifying
a sentence, BLEU alone fails to correctly assess
the appropriateness of a simplification. Moreover
BLEU does not capture the degree to which the
system’s output differs from the complex sentence
input. We therefore use BLEU as a means to eval-
uate how close the systems output are to the refer-
ence corpus but complement it with further man-
ual metrics capturing other important factors when
</bodyText>
<page confidence="0.997401">
441
</page>
<bodyText confidence="0.99988725">
evaluating simplifications such as the fluency and
the adequacy of the output sentences and the de-
gree to which the output sentence simplifies the
input.
</bodyText>
<subsectionHeader confidence="0.985256">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999933454545454">
Number of Splits Table 3 shows the proportion
of input whose simplification involved a splitting
operation. While our system splits in proportion
similar to that observed in the training data, the
other systems either split very often (80% of the
time for Zhu and 63% of the time for Woodsend)
or not at all (Wubben). In other words, when com-
pared to the other systems, our system performs
splits in proportion closest to the reference both
in terms of total number of splits and of average
number of splits per sentence.
</bodyText>
<table confidence="0.99955975">
Data Total number % split average split /
of sentences sentence
PWKP 108,016 6.1 1.06
GOLD 100 28 1.30
Zhu 100 80 1.80
Woodsend 100 63 2.05
Wubben 100 1 1.01
Hybrid 100 10 1.10
</table>
<tableCaption confidence="0.998413">
Table 3: Proportion of Split Sentences (% split)
</tableCaption>
<bodyText confidence="0.99808224">
in the training/test data and in average per sen-
tence (average split / sentence). GOLD is the
test data with the gold standard SWKP sentences;
Zhu, Woodsend, Wubben are the best output of the
models of Zhu et al. (2010), Woodsend and Lap-
ata (2011) and Wubben et al. (2012) respectively;
Hybrid is our model.
Number of Edits Table 4 indicates the edit dis-
tance of the output sentences w.r.t. both the com-
plex and the simple reference sentences as well as
the number of input for which no simplification
occur. The right part of the table shows that our
system generate simplifications which are closest
to the reference sentence (in terms of edits) com-
pared to those output by the other systems. It
also produces the highest number of simplifica-
tions which are identical to the reference. Con-
versely our system only ranks third in terms of dis-
similarity with the input complex sentences (6.32
edits away from the input sentence) behind the
Woodsend (8.63 edits) and the Zhu (7.87 edits)
system. This is in part due to the difference in
splitting strategies noted above : the many splits
applied by these latter two systems correlate with
a high number of edits.
</bodyText>
<table confidence="0.9993915">
Edits (Complex Edits (System
System BLEU to System) to Simple)
LD No edit LD No edit
GOLD 100 12.24 3 0 100
Zhu 37.4 7.87 2 14.64 0
Woodsend 42 8.63 24 16.03 2
Wubben 41.4 3.33 6 13.57 2
Hybrid 53.6 6.32 4 11.53 3
</table>
<tableCaption confidence="0.842869">
Table 4: Automated Metrics for Simplification:
average Levenshtein distance (LD) to complex and
simple reference sentences per system; number of
input sentences for which no simplification occur
(No edit).
</tableCaption>
<bodyText confidence="0.933039333333333">
BLEU score We used Moses support tools:
multi-bleu10 to calculate BLEU scores. The
BLEU scores shown in Table 4 show that our sys-
tem produces simplifications that are closest to the
reference.
In sum, the automatic metrics indicate that our
system produces simplification that are consis-
tently closest to the reference in terms of edit dis-
tance, number of splits and BLEU score.
</bodyText>
<subsectionHeader confidence="0.995744">
4.4 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.99996344">
The human evaluation was done online using the
LG-Eval toolkit (Kow and Belz, 2012)11. The
evaluators were allocated a trial set using a Latin
Square Experimental Design (LSED) such that
each evaluator sees the same number of output
from each system and for each test set item. Dur-
ing the experiment, the evaluators were presented
with a pair of a complex and a simple sentence(s)
and asked to rate this pair w.r.t. to adequacy (Does
the simplified sentence(s) preserve the meaning
of the input?) and simplification (Does the gen-
erated sentence(s) simplify the complex input?).
They were also asked to rate the second (sim-
plified) sentence(s) of the pair w.r.t. to fluency
(Is the simplified output fluent and grammatical?).
Similar to the Wubben’s human evaluation setup,
we randomly selected 20 complex sentences from
Zhu’s test corpus and included in the evaluation
corpus: the corresponding simple (Gold) sentence
from Zhu’s test corpus, the output of our system
(Hybrid) and the output of the other three sys-
tems (Zhu, Woodsend and Wubben) which were
provided to us by the system authors. The eval-
uation data thus consisted of 100 complex/simple
pairs. We collected ratings from 27 participants.
</bodyText>
<footnote confidence="0.9987315">
10http://www.statmt.org/moses/?n=Moses.SupportTools
11http://www.nltg.brighton.ac.uk/research/lg-eval/
</footnote>
<page confidence="0.996394">
442
</page>
<bodyText confidence="0.99987325">
All were either native speakers or proficient in En-
glish, having taken part in a Master taught in En-
glish or lived in an English speaking country for
an extended period of time.
</bodyText>
<table confidence="0.99848">
Systems Simplification Fluency Adequacy
GOLD 3.57 3.93 3.66
Zhu 2.84 2.34 2.34
Woodsend 1.73 2.94 3.04
Wubben 1.81 3.65 3.84
Hybrid 3.37 3.55 3.50
</table>
<tableCaption confidence="0.632290625">
Table 5: Average Human Ratings for simplicity,
fluency and adequacy
Table 5 shows the average ratings of the human
evaluation on a slider scale from 0 to 5. Pairwise
comparisons between all models and their statisti-
cal significance were carried out using a one-way
ANOVA with post-hoc Tukey HSD tests and are
shown in Table 6.
</tableCaption>
<table confidence="0.9973328">
Systems GOLD Zhu Woodsend Wubben
Zhu 0❑0
Woodsend 0❑0 0❑0
Wubben 0■• 0❑0 ♦❑0
Hybrid ♦■♦ ♦❑0 0❑♦ 0■♦
</table>
<tableCaption confidence="0.78580975">
Table 6: 0/♦ is/not significantly different (sig.
diff.) wrt simplicity. ❑/■ is/not sig. diff. wrt
fluency. △/♦ is/not sig. diff. wrt adequacy. (sig-
nificance level: p &lt; 0.05)
</tableCaption>
<bodyText confidence="0.999929525">
With regard to simplification, our system ranks
first and is very close to the manually simpli-
fied input (the difference is not statistically signif-
icant). The low rating for Woodsend reflects the
high number of unsimplified sentences (24/100 in
the test data used for the automatic evaluation and
6/20 in the evaluation data used for human judg-
ments). Our system data is not significantly differ-
ent from the manually simplified data for simplic-
ity whereas all other systems are.
For fluency, our system rates second behind
Wubben and before Woodsend and Zhu. The
difference between our system and both Zhu
and Woodsend system is significant. In partic-
ular, Zhu’s output is judged less fluent proba-
bly because of the many incorrect splits it li-
censes. Manual examination of the data shows
that Woodsend’s system also produces incorrect
splits. For this system however, the high propor-
tion of non simplified sentences probably counter-
balances these incorrect splits, allowing for a good
fluency score overall.
Regarding adequacy, our system is against clos-
est to the reference (3.50 for our system vs.
3.66 for manual simplification). Our system, the
Wubben system and the manual simplifications
are in the same group (the differences between
these systems are not significant). The Wood-
send system comes second and the Zhu system
third (the difference between the two is signifi-
cant). Wubben’s high fluency, high adequacy but
low simplicity could be explained with their min-
imal number of edit (3.33 edits) from the source
sentence.
In sum, if we group together systems for which
there is no significant difference, our system ranks
first (together with GOLD) for simplicity; first
for fluency (together with GOLD and Wubben);
and first for adequacy (together with GOLD and
Wubben).
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999904521739131">
A key feature of our approach is that it is seman-
tically based. Typically, discourse level simplifi-
cation operations such as sentence splitting, sen-
tence reordering, cue word selection, referring ex-
pression generation and determiner choice are se-
mantically constrained. As argued by Siddharthan
(2006), correctly capturing the interactions be-
tween these phenomena is essential to ensuring
text cohesion. In the future, we would like to
investigate how our framework deals with such
discourse level simplifications i.e., simplifications
which involves manipulation of the coreference
and of the discourse structure. In the PWKP data,
the proportion of split sentences is rather low (6.1
%) and many of the split sentences are simple sen-
tence coordination splits. A more adequate but
small corpus is that used in (Siddharthan, 2006)
which consists of 95 cases of discourse simplifica-
tion. Using data from the language learning or the
children reading community, it would be interest-
ing to first construct a similar, larger scale corpus;
and to then train and test our approach on more
complex cases of sentence splitting.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999843666666667">
We are grateful to Zhemin Zhu, Kristian Wood-
send and Sander Wubben for sharing their data.
We would like to thank our annotators for partic-
ipating in our human evaluation experiments and
to anonymous reviewers for their insightful com-
ments.
</bodyText>
<page confidence="0.999104">
443
</page>
<sectionHeader confidence="0.990282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833822429907">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics (ACL), pages 597–
604. Association for Computational Linguistics.
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the 2003 conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 25–32. Association for Computa-
tional Linguistics.
Stefan Bott, Horacio Saggion, and Simon Mille. 2012.
Text simplification tools for spanish. In Proceedings
of the 8th International Conference on Language
Resources and Evaluation (LREC), pages 1665–
1671.
Yvonne Margaret Canning. 2002. Syntactic simplifica-
tion of Text. Ph.D. thesis, University of Sunderland.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), volume 99, pages 269–270. Cite-
seer.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge-Based Systems, 10(3):183–190.
Raman Chandrasekar, Christine Doran, and Banga-
lore Srinivas. 1996. Motivations and methods for
text simplification. In Proceedings of the 16th In-
ternational conference on Computational linguistics
(COLING), pages 1041–1044. Association for Com-
putational Linguistics.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1–9. Association for Computational
Linguistics.
James R Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&amp;C
and Boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) on Interactive Poster and Demonstration Ses-
sions, pages 33–36. Association for Computational
Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):1–38.
Mark Dras. 1999. Tree adjoining grammar and the
reluctant paraphrasing of text. Ph.D. thesis, Mac-
quarie University NSW 2109 Australia.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference (INLG), pages 25–32. Asso-
ciation for Computational Linguistics.
Hans Kamp. 1981. A theory of truth and semantic rep-
resentation. In J.A.G. Groenendijk, T.M.V. Janssen,
B.J. Stokhof, and M.J.B. Stokhof, editors, Formal
methods in the study of language, number pt. 1 in
Mathematical Centre tracts. Mathematisch Centrum.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence (AAAI) and
Twelfth Conference on Innovative Applications of
Artificial Intelligence (IAAI), pages 703–710. AAAI
Press.
Eric Kow and Anja Belz. 2012. LG-Eval: A Toolkit
for Creating Online Language Evaluation Experi-
ments. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC), pages 4033–4037.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), page 896. Association for Computational Lin-
guistics.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In Proceedings of the Lan-
guage Engineering Conference (LEC), pages 64–71.
IEEE Computer Society.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77–109.
Advaith Siddharthan. 2010. Complex lexico-syntactic
reformulation of sentences using typed dependency
representations. In Proceedings of the 6th Inter-
national Natural Language Generation Conference
(INLG), pages 125–133. Association for Computa-
tional Linguistics.
Advaith Siddharthan. 2011. Text simplification us-
ing typed dependencies: a comparison of the robust-
ness of different generation strategies. In Proceed-
ings ofthe 13th European Workshop on Natural Lan-
guage Generation (ENLG), pages 2–11. Association
for Computational Linguistics.
David A Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proceedings of
the HLT-NAACL Workshop on Statistical Machine
Translation, pages 23–30. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.988008">
444
</page>
<reference confidence="0.999786394736842">
David Vickrey and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL) and the
Human Language Technology Conference (HLT),
pages 344–352.
Willian Massami Watanabe, Arnaldo Candido Junior,
Vinicius Rodriguez Uzˆeda, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra Maria Aluisio. 2009. Facilita: reading as-
sistance for low-literacy readers. In Proceedings of
the 27th ACM international conference on Design of
communication, pages 29–36. ACM.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 409–420.
Association for Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL): Long Papers-Volume
1, pages 1015–1024. Association for Computational
Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics (ACL), pages 523–530. Asso-
ciation for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING), pages 1353–1361, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999107">
445
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880018">
<title confidence="0.999728">Hybrid Simplification using Deep Semantics and Machine Translation</title>
<author confidence="0.998609">Shashi Narayan Claire Gardent</author>
<affiliation confidence="0.967992">Universit´e de Lorraine, LORIA CNRS, LORIA, UMR</affiliation>
<address confidence="0.966358">Villers-l`es-Nancy, F-54600, France Vandoeuvre-l`es-Nancy, F-54500, France</address>
<email confidence="0.967209">shashi.narayan@loria.frclaire.gardent@loria.fr</email>
<abstract confidence="0.997407166666667">We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>597--604</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11973" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1852" endWordPosition="1855">warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. (Zhu et al., 2010) We also depart from Coster and Kauchak (2011) who rely on null phrasal alignments for deletion during phrase based machine translation. In their approach, deletion is constrained by the training data and the possible alignments, independent of any linguistic knowledge. Substitution and Reordering SMT based approaches to paraphrasing (Barzilay and Elhadad, 2003; Bannard and Callison-Burch, 2005) and to sentence simplification (Wubben et al., 2012) have shown that by utilising knowledge about alignment and translation probabilities, SMT systems can account for the substitutions and the reorderings occurring in sentence simplification. Following on these approaches, we therefore rely on phrase based SMT to learn substitutions and reordering. In addition, the language model we integrate in the SMT module helps ensuring better fluency and grammaticality. 3.1 An Example Figure 1 shows how our approach simplifies (4C) into (4S). (4) C. In 1964 Peter Higgs published his second paper in Phys</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 597– 604. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11938" citStr="Barzilay and Elhadad, 2003" startWordPosition="1848" endWordPosition="1851">also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. (Zhu et al., 2010) We also depart from Coster and Kauchak (2011) who rely on null phrasal alignments for deletion during phrase based machine translation. In their approach, deletion is constrained by the training data and the possible alignments, independent of any linguistic knowledge. Substitution and Reordering SMT based approaches to paraphrasing (Barzilay and Elhadad, 2003; Bannard and Callison-Burch, 2005) and to sentence simplification (Wubben et al., 2012) have shown that by utilising knowledge about alignment and translation probabilities, SMT systems can account for the substitutions and the reorderings occurring in sentence simplification. Following on these approaches, we therefore rely on phrase based SMT to learn substitutions and reordering. In addition, the language model we integrate in the SMT module helps ensuring better fluency and grammaticality. 3.1 An Example Figure 1 shows how our approach simplifies (4C) into (4S). (4) C. In 1964 Peter Higgs</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 conference on Empirical Methods in Natural Language Processing (EMNLP), pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bott</author>
<author>Horacio Saggion</author>
<author>Simon Mille</author>
</authors>
<title>Text simplification tools for spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1665--1671</pages>
<contexts>
<context position="4880" citStr="Bott et al., 2012" startWordPosition="732" endWordPosition="735">ntences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et</context>
</contexts>
<marker>Bott, Saggion, Mille, 2012</marker>
<rawString>Stefan Bott, Horacio Saggion, and Simon Mille. 2012. Text simplification tools for spanish. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), pages 1665– 1671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvonne Margaret Canning</author>
</authors>
<title>Syntactic simplification of Text.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sunderland.</institution>
<contexts>
<context position="4895" citStr="Canning, 2002" startWordPosition="736" endWordPosition="737">provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) con</context>
</contexts>
<marker>Canning, 2002</marker>
<rawString>Yvonne Margaret Canning. 2002. Syntactic simplification of Text. Ph.D. thesis, University of Sunderland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Darren Pearce</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Simplifying text for language-impaired readers.</title>
<date>1999</date>
<booktitle>In Proceedings of 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<volume>99</volume>
<pages>269--270</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2070" citStr="Carroll et al., 1999" startWordPosition="306" endWordPosition="309">o several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning, Siobhan Devlin, and John Tait. 1999. Simplifying text for language-impaired readers. In Proceedings of 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL), volume 99, pages 269–270. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification.</title>
<date>1997</date>
<journal>Knowledge-Based Systems,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="4861" citStr="Chandrasekar and Srinivas, 1997" startWordPosition="728" endWordPosition="731">he shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches to sentence sim</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Raman Chandrasekar and Bangalore Srinivas. 1997. Automatic induction of rules for text simplification. Knowledge-Based Systems, 10(3):183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Christine Doran</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International conference on Computational linguistics (COLING),</booktitle>
<pages>1041--1044</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1808" citStr="Chandrasekar et al., 1996" startWordPosition="266" endWordPosition="269"> use instead of exploit); simpler syntactic constructions (e.g., no relative clauses or apposition); and fewer modifiers (e.g., He slept vs. He also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an inpu</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>Raman Chandrasekar, Christine Doran, and Bangalore Srinivas. 1996. Motivations and methods for text simplification. In Proceedings of the 16th International conference on Computational linguistics (COLING), pages 1041–1044. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Learning to simplify sentences using wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2619" citStr="Coster and Kauchak, 2011" startWordPosition="390" endWordPosition="393"> application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitutio</context>
<context position="7177" citStr="Coster and Kauchak, 2011" startWordPosition="1082" endWordPosition="1085">nment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. In (Wubben et al., 2012; Coster and Kauchak, 2011), simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based ap</context>
<context position="11621" citStr="Coster and Kauchak (2011)" startWordPosition="1803" endWordPosition="1806">tinguish between optional and obligatory arguments. For instance Zhu et al. (2010) simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. (Zhu et al., 2010) We also depart from Coster and Kauchak (2011) who rely on null phrasal alignments for deletion during phrase based machine translation. In their approach, deletion is constrained by the training data and the possible alignments, independent of any linguistic knowledge. Substitution and Reordering SMT based approaches to paraphrasing (Barzilay and Elhadad, 2003; Bannard and Callison-Burch, 2005) and to sentence simplification (Wubben et al., 2012) have shown that by utilising knowledge about alignment and translation probabilities, SMT systems can account for the substitutions and the reorderings occurring in sentence simplification. Foll</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011. Learning to simplify sentences using wikipedia. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3720" citStr="Curran et al., 2007" startWordPosition="558" endWordPosition="561">his way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches. Second, our approach is semantic based. While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981)) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we 435 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid delet</context>
<context position="12865" citStr="Curran et al., 2007" startWordPosition="1995" endWordPosition="1999">s, we therefore rely on phrase based SMT to learn substitutions and reordering. In addition, the language model we integrate in the SMT module helps ensuring better fluency and grammaticality. 3.1 An Example Figure 1 shows how our approach simplifies (4C) into (4S). (4) C. In 1964 Peter Higgs published his second paper in Physical Review Letters describing Higgs mechanism which predicted a new massive spin-zero boson for the first time. S. Peter Higgs wrote his paper explaining Higgs mechanism in 1964. Higgs mechanism predicted a new elementary particle. The DRS for (4C) produced using Boxer (Curran et al., 2007) is shown at the top of the Figure and a graph representation3 of the dependencies between its variables is shown immediately below. Each DRS variable labels a node in the graph and each edge is labelled with the relation holding between the variables labelling its end vertices. The 3The DRS to graph conversion goes through several preprocessing steps: the relation nn is inverted making modifier noun (higgs) dependent of modified noun (mechanism), named and timex are converted to unary predicates, e.g., named(x,peter) is mapped to peter(x) and timex(x) = 1964 is mapped to 1964(x); and nodes ar</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James R Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions, pages 33–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="21345" citStr="Dempster et al., 1977" startWordPosition="3457" endWordPosition="3460">ee models: a model for relations determining the deletion of prepositional phrases; a model for modifiers (adjectives and adverbs) and a model for orphan words (Table 2). All three deletion models use the associated word itself as a feature. In addition, the model for relations uses the PP length-range as a feature while the model for orphan words relies on boundary information i.e., whether or not, the OW occurs at the associated sentence boundary. �P(Ddel|Dsplit) = �DELrel(relcand) DELmod(modcand) relcand modcand ri DELow(owcand) owcand 3.3 Estimating the parameters We use the EM algorithm (Dempster et al., 1977) to estimate our split and deletion model parameters. For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and Zhu et al. (2010); and build training graphs (Figure 2) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). An M-node contains the DRS representation Dc of a complex sentence c and the associated simple sentence(s) si while O-nodes determine split and deletion operations on </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Tree adjoining grammar and the reluctant paraphrasing of text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<pages>2109</pages>
<institution>Macquarie University NSW</institution>
<contexts>
<context position="2430" citStr="Dras, 1999" startWordPosition="362" endWordPosition="363">ion (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual mach</context>
<context position="6635" citStr="Dras (1999)" startWordPosition="993" endWordPosition="994">nces into sim1SWKP (http://simple.wikipedia.org) is a corpus of simple texts targeting “children and adults who are learning English Language” and whose authors are requested to “use easy words and short sentences”. 2http://en.wikipedia.org pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. In (Wubben et al., 2012; Coster and Kauchak, 2011), simplification is viewed as a monolingual translation ta</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Mark Dras. 1999. Tree adjoining grammar and the reluctant paraphrasing of text. Ph.D. thesis, Macquarie University NSW 2109 Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference (INLG),</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1894" citStr="Filippova and Strube, 2008" startWordPosition="278" endWordPosition="281">or apposition); and fewer modifiers (e.g., He slept vs. He also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have b</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Dependency tree based sentence compression. In Proceedings of the Fifth International Natural Language Generation Conference (INLG), pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>A theory of truth and semantic representation.</title>
<date>1981</date>
<booktitle>Formal methods in the study of language, number pt. 1 in Mathematical Centre tracts. Mathematisch Centrum.</booktitle>
<editor>In J.A.G. Groenendijk, T.M.V. Janssen, B.J. Stokhof, and M.J.B. Stokhof, editors,</editor>
<contexts>
<context position="3679" citStr="Kamp, 1981" startWordPosition="553" endWordPosition="554">eordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches. Second, our approach is semantic based. While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981)) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we 435 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and th</context>
</contexts>
<marker>Kamp, 1981</marker>
<rawString>Hans Kamp. 1981. A theory of truth and semantic representation. In J.A.G. Groenendijk, T.M.V. Janssen, B.J. Stokhof, and M.J.B. Stokhof, editors, Formal methods in the study of language, number pt. 1 in Mathematical Centre tracts. Mathematisch Centrum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI) and Twelfth Conference on Innovative Applications of Artificial Intelligence (IAAI),</booktitle>
<pages>703--710</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1848" citStr="Knight and Marcu, 2000" startWordPosition="272" endWordPosition="275"> constructions (e.g., no relative clauses or apposition); and fewer modifiers (e.g., He slept vs. He also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lap</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization-step one: Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI) and Twelfth Conference on Innovative Applications of Artificial Intelligence (IAAI), pages 703–710. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Kow</author>
<author>Anja Belz</author>
</authors>
<title>LG-Eval: A Toolkit for Creating Online Language Evaluation Experiments.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>4033--4037</pages>
<contexts>
<context position="31195" citStr="Kow and Belz, 2012" startWordPosition="5088" endWordPosition="5091">tance (LD) to complex and simple reference sentences per system; number of input sentences for which no simplification occur (No edit). BLEU score We used Moses support tools: multi-bleu10 to calculate BLEU scores. The BLEU scores shown in Table 4 show that our system produces simplifications that are closest to the reference. In sum, the automatic metrics indicate that our system produces simplification that are consistently closest to the reference in terms of edit distance, number of splits and BLEU score. 4.4 Human Evaluation The human evaluation was done online using the LG-Eval toolkit (Kow and Belz, 2012)11. The evaluators were allocated a trial set using a Latin Square Experimental Design (LSED) such that each evaluator sees the same number of output from each system and for each test set item. During the experiment, the evaluators were presented with a pair of a complex and a simple sentence(s) and asked to rate this pair w.r.t. to adequacy (Does the simplified sentence(s) preserve the meaning of the input?) and simplification (Does the generated sentence(s) simplify the complex input?). They were also asked to rate the second (simplified) sentence(s) of the pair w.r.t. to fluency (Is the si</context>
</contexts>
<marker>Kow, Belz, 2012</marker>
<rawString>Eric Kow and Anja Belz. 2012. LG-Eval: A Toolkit for Creating Online Language Evaluation Experiments. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), pages 4033–4037.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Syntactic simplification for improving content selection in multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2754" citStr="Siddharthan et al., 2004" startWordPosition="408" endWordPosition="412">on native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well</context>
</contexts>
<marker>Siddharthan, Nenkova, McKeown, 2004</marker>
<rawString>Advaith Siddharthan, Ani Nenkova, and Kathleen McKeown. 2004. Syntactic simplification for improving content selection in multi-document summarization. In Proceedings of the 20th International Conference on Computational Linguistics (COLING), page 896. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>An architecture for a text simplification system.</title>
<date>2002</date>
<booktitle>In Proceedings of the Language Engineering Conference (LEC),</booktitle>
<pages>64--71</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="2168" citStr="Siddharthan, 2002" startWordPosition="323" endWordPosition="324">ses with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan,</context>
<context position="4828" citStr="Siddharthan, 2002" startWordPosition="725" endWordPosition="727">he re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine le</context>
</contexts>
<marker>Siddharthan, 2002</marker>
<rawString>Advaith Siddharthan. 2002. An architecture for a text simplification system. In Proceedings of the Language Engineering Conference (LEC), pages 64–71. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="35444" citStr="Siddharthan (2006)" startWordPosition="5769" endWordPosition="5770">number of edit (3.33 edits) from the source sentence. In sum, if we group together systems for which there is no significant difference, our system ranks first (together with GOLD) for simplicity; first for fluency (together with GOLD and Wubben); and first for adequacy (together with GOLD and Wubben). 5 Conclusion A key feature of our approach is that it is semantically based. Typically, discourse level simplification operations such as sentence splitting, sentence reordering, cue word selection, referring expression generation and determiner choice are semantically constrained. As argued by Siddharthan (2006), correctly capturing the interactions between these phenomena is essential to ensuring text cohesion. In the future, we would like to investigate how our framework deals with such discourse level simplifications i.e., simplifications which involves manipulation of the coreference and of the discourse structure. In the PWKP data, the proportion of split sentences is rather low (6.1 %) and many of the split sentences are simple sentence coordination splits. A more adequate but small corpus is that used in (Siddharthan, 2006) which consists of 95 cases of discourse simplification. Using data fro</context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation, 4(1):77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Complex lexico-syntactic reformulation of sentences using typed dependency representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG),</booktitle>
<pages>125--133</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4934" citStr="Siddharthan, 2010" startWordPosition="740" endWordPosition="741">eleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 10</context>
</contexts>
<marker>Siddharthan, 2010</marker>
<rawString>Advaith Siddharthan. 2010. Complex lexico-syntactic reformulation of sentences using typed dependency representations. In Proceedings of the 6th International Natural Language Generation Conference (INLG), pages 125–133. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Text simplification using typed dependencies: a comparison of the robustness of different generation strategies.</title>
<date>2011</date>
<booktitle>In Proceedings ofthe 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>2--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2773" citStr="Siddharthan, 2011" startWordPosition="413" endWordPosition="414">rthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not </context>
<context position="4914" citStr="Siddharthan, 2011" startWordPosition="738" endWordPosition="739">al means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel</context>
</contexts>
<marker>Siddharthan, 2011</marker>
<rawString>Advaith Siddharthan. 2011. Text simplification using typed dependencies: a comparison of the robustness of different generation strategies. In Proceedings ofthe 13th European Workshop on Natural Language Generation (ENLG), pages 2–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6528" citStr="Smith and Eisner, 2006" startWordPosition="974" endWordPosition="977">splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim1SWKP (http://simple.wikipedia.org) is a corpus of simple texts targeting “children and adults who are learning English Language” and whose authors are requested to “use easy words and short sentences”. 2http://en.wikipedia.org pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. I</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation, pages 23–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Daphne Koller</author>
</authors>
<title>Sentence simplification for semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL) and the Human Language Technology Conference (HLT),</booktitle>
<pages>344--352</pages>
<contexts>
<context position="1949" citStr="Vickrey and Koller, 2008" startWordPosition="286" endWordPosition="289">e also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple </context>
</contexts>
<marker>Vickrey, Koller, 2008</marker>
<rawString>David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL) and the Human Language Technology Conference (HLT), pages 344–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willian Massami Watanabe</author>
<author>Arnaldo Candido Junior</author>
</authors>
<title>Vinicius Rodriguez Uzˆeda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Aluisio.</title>
<date>2009</date>
<booktitle>In Proceedings of the 27th ACM international conference on Design of communication,</booktitle>
<pages>29--36</pages>
<publisher>ACM.</publisher>
<marker>Watanabe, Junior, 2009</marker>
<rawString>Willian Massami Watanabe, Arnaldo Candido Junior, Vinicius Rodriguez Uzˆeda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Aluisio. 2009. Facilita: reading assistance for low-literacy readers. In Proceedings of the 27th ACM international conference on Design of communication, pages 29–36. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>409--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2458" citStr="Woodsend and Lapata, 2011" startWordPosition="364" endWordPosition="368">and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which</context>
<context position="4445" citStr="Woodsend and Lapata, 2011" startWordPosition="669" endWordPosition="672">for Computational Linguistics, pages 435–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformati</context>
<context position="6469" citStr="Woodsend and Lapata (2011)" startWordPosition="965" endWordPosition="968"> tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim1SWKP (http://simple.wikipedia.org) is a corpus of simple texts targeting “children and adults who are learning English Language” and whose authors are requested to “use easy words and short sentences”. 2http://en.wikipedia.org pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in term</context>
<context position="8228" citStr="Woodsend and Lapata (2011)" startWordPosition="1255" endWordPosition="1258">be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based approach. Similarly, Wubben et al. (2012) use Moses and the PWKP data to train a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source. A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both Zhu et al. (2010) and Woodsend and Lapata (2011) systems. 3 Simplification Framework We start by motivating our approach and explaining how it relates to previous proposals w.r.t., the four main operations involved in simplification namely, splitting, deletion, substitution and reordering. We then introduce our framework. 436 Sentence Splitting. Sentence splitting is arguably semantic based in that in many cases, splitting occurs when the same semantic entity participates in two distinct eventualities. For instance, in example (1) below, the split is on the noun bricks which is involved in two eventualities namely, “being resistant to cold”</context>
<context position="9692" citStr="Woodsend and Lapata (2011)" startWordPosition="1483" endWordPosition="1486">ings. While splitting opportunities have a clear counterpart in syntax (i.e., splitting often occurs whenever a relative, a subordinate or an appositive clause occurs in the complex sentence), completion i.e., the reconstruction of the shared element in the second simpler clause, is arguably semantically governed in that the reconstructed element corefers with its matching phrase in the first simpler clause. While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as Zhu et al. (2010) and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2), Zhu et al. (2010) fails to copy the shared argument “The judge” to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (“he” or “she”) for the antecedent “The judge”. (2) C. The judge ordered that Chapman should receive psychiatric treatment in prison and sentenced him to twenty </context>
<context position="10985" citStr="Woodsend and Lapata, 2011" startWordPosition="1694" endWordPosition="1697">atric treatment. In prison and sentenced him to twenty years to life. (Zhu et al., 2010) S2. The judge ordered that Chapman should receive psychiatric treatment in prison. It sentenced him to twenty years to life. (Woodsend and Lapata, 2011) Deletion. By handling deletion using a probabilistic model trained on semantic representations, we can avoid deleting obligatory arguments. Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches (Zhu et al., 2010; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance Zhu et al. (2010) simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. (Zhu et al., 2010) We also d</context>
<context position="29378" citStr="Woodsend and Lapata (2011)" startWordPosition="4774" endWordPosition="4778"> our system performs splits in proportion closest to the reference both in terms of total number of splits and of average number of splits per sentence. Data Total number % split average split / of sentences sentence PWKP 108,016 6.1 1.06 GOLD 100 28 1.30 Zhu 100 80 1.80 Woodsend 100 63 2.05 Wubben 100 1 1.01 Hybrid 100 10 1.10 Table 3: Proportion of Split Sentences (% split) in the training/test data and in average per sentence (average split / sentence). GOLD is the test data with the gold standard SWKP sentences; Zhu, Woodsend, Wubben are the best output of the models of Zhu et al. (2010), Woodsend and Lapata (2011) and Wubben et al. (2012) respectively; Hybrid is our model. Number of Edits Table 4 indicates the edit distance of the output sentences w.r.t. both the complex and the simple reference sentences as well as the number of input for which no simplification occur. The right part of the table shows that our system generate simplifications which are closest to the reference sentence (in terms of edits) compared to those output by the other systems. It also produces the highest number of simplifications which are identical to the reference. Conversely our system only ranks third in terms of dissimil</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 409–420. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers-Volume 1,</booktitle>
<pages>1015--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers-Volume 1, pages 1015–1024. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5746" citStr="Yamada and Knight, 2001" startWordPosition="858" endWordPosition="861">imited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim1SWKP (http://simple.wikipedia.org) is a corpus of simple texts targeting “children and adults who are learning English Language” and whose authors are requested to “use easy words and short sentences”. 2http://en.wikipedia.org pler ones by greedily selecting the output sentence with highest probability. Usi</context>
<context position="21494" citStr="Yamada and Knight (2001)" startWordPosition="3483" endWordPosition="3486">orphan words (Table 2). All three deletion models use the associated word itself as a feature. In addition, the model for relations uses the PP length-range as a feature while the model for orphan words relies on boundary information i.e., whether or not, the OW occurs at the associated sentence boundary. �P(Ddel|Dsplit) = �DELrel(relcand) DELmod(modcand) relcand modcand ri DELow(owcand) owcand 3.3 Estimating the parameters We use the EM algorithm (Dempster et al., 1977) to estimate our split and deletion model parameters. For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and Zhu et al. (2010); and build training graphs (Figure 2) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). An M-node contains the DRS representation Dc of a complex sentence c and the associated simple sentence(s) si while O-nodes determine split and deletion operations on their parent M-node. Only the root M-node is considered for the split operations. For example, given split del-rel∗; del-mod∗; del-ow∗ fin Figure 2: </context>
<context position="23646" citStr="Yamada and Knight, 2001" startWordPosition="3838" endWordPosition="3841">g operation, we introduce a split only if the reference sentence consists of several sentences (i.e., there is a split in the training data); and only consider splits which maximises the overlap between split and simple reference sentences. We initialize our probability tables Table 1 and Table 2 with the uniform distribution, i.e., 0.5 because all our features are binary. The EM algorithm iterates over training graphs counting model features from O-nodes and updating our probability tables. Because of the space constraints, we do not describe our algorithm in details. We refer the reader to (Yamada and Knight, 2001) for more details. 440 Our phrase based translation model is trained using the Moses toolkit5 with its default command line options on the PWKP corpus (except the sentences from the test set) considering the complex sentence as the source and the simpler one as the target. Our trigram language model is trained using the SRILM toolkit6 on the SWKP corpus7. Decoding. We explore the decoding graph similar to the training graph but in a greedy approach always picking the choice with maximal probability. Given a complex input sentence c, a split Onode will be selected corresponding to the decision </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics (ACL), pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1353--1361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2571" citStr="Zhu et al., 2010" startWordPosition="382" endWordPosition="385">lso has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT</context>
<context position="4418" citStr="Zhu et al., 2010" startWordPosition="665" endWordPosition="668">f the Association for Computational Linguistics, pages 435–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-info</context>
<context position="6400" citStr="Zhu et al. (2010)" startWordPosition="954" endWordPosition="957">the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim1SWKP (http://simple.wikipedia.org) is a corpus of simple texts targeting “children and adults who are learning English Language” and whose authors are requested to “use easy words and short sentences”. 2http://en.wikipedia.org pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of</context>
<context position="8197" citStr="Zhu et al. (2010)" startWordPosition="1250" endWordPosition="1253"> allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based approach. Similarly, Wubben et al. (2012) use Moses and the PWKP data to train a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source. A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both Zhu et al. (2010) and Woodsend and Lapata (2011) systems. 3 Simplification Framework We start by motivating our approach and explaining how it relates to previous proposals w.r.t., the four main operations involved in simplification namely, splitting, deletion, substitution and reordering. We then introduce our framework. 436 Sentence Splitting. Sentence splitting is arguably semantic based in that in many cases, splitting occurs when the same semantic entity participates in two distinct eventualities. For instance, in example (1) below, the split is on the noun bricks which is involved in two eventualities na</context>
<context position="9661" citStr="Zhu et al. (2010)" startWordPosition="1478" endWordPosition="1481">ion of permanent buildings. While splitting opportunities have a clear counterpart in syntax (i.e., splitting often occurs whenever a relative, a subordinate or an appositive clause occurs in the complex sentence), completion i.e., the reconstruction of the shared element in the second simpler clause, is arguably semantically governed in that the reconstructed element corefers with its matching phrase in the first simpler clause. While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as Zhu et al. (2010) and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2), Zhu et al. (2010) fails to copy the shared argument “The judge” to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (“he” or “she”) for the antecedent “The judge”. (2) C. The judge ordered that Chapman should receive psychiatric treatment in pris</context>
<context position="10957" citStr="Zhu et al., 2010" startWordPosition="1690" endWordPosition="1693"> should get psychiatric treatment. In prison and sentenced him to twenty years to life. (Zhu et al., 2010) S2. The judge ordered that Chapman should receive psychiatric treatment in prison. It sentenced him to twenty years to life. (Woodsend and Lapata, 2011) Deletion. By handling deletion using a probabilistic model trained on semantic representations, we can avoid deleting obligatory arguments. Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches (Zhu et al., 2010; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance Zhu et al. (2010) simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. </context>
<context position="21516" citStr="Zhu et al. (2010)" startWordPosition="3488" endWordPosition="3491">hree deletion models use the associated word itself as a feature. In addition, the model for relations uses the PP length-range as a feature while the model for orphan words relies on boundary information i.e., whether or not, the OW occurs at the associated sentence boundary. �P(Ddel|Dsplit) = �DELrel(relcand) DELmod(modcand) relcand modcand ri DELow(owcand) owcand 3.3 Estimating the parameters We use the EM algorithm (Dempster et al., 1977) to estimate our split and deletion model parameters. For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and Zhu et al. (2010); and build training graphs (Figure 2) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). An M-node contains the DRS representation Dc of a complex sentence c and the associated simple sentence(s) si while O-nodes determine split and deletion operations on their parent M-node. Only the root M-node is considered for the split operations. For example, given split del-rel∗; del-mod∗; del-ow∗ fin Figure 2: An example training gr</context>
<context position="24834" citStr="Zhu et al. (2010)" startWordPosition="4040" endWordPosition="4043">orresponding to the decision of whether to split and where to split. Next, deletion O-nodes are selected indicating whether or not to drop each of the deletion candidate. The DRS associated with the final M-node Dfin is then mapped to a simplified sentence s′fin which is further simplified using the phrase-based machine translation system to produce the final simplified sentence ssimple. 4 Experiments We trained our simplification and translation models on the PWKP corpus. To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by Zhu et al. (2010) and relying both on automatic metrics and on human judgments. 4.1 Training and Test Data The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by Zhu et al. (2010). To construct this bi-text, Zhu et al. (2010) extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit8. We then parse all complex sentences in PWKP using Boxer9 to produce their DRSs. Finally,</context>
<context position="29350" citStr="Zhu et al. (2010)" startWordPosition="4770" endWordPosition="4773"> the other systems, our system performs splits in proportion closest to the reference both in terms of total number of splits and of average number of splits per sentence. Data Total number % split average split / of sentences sentence PWKP 108,016 6.1 1.06 GOLD 100 28 1.30 Zhu 100 80 1.80 Woodsend 100 63 2.05 Wubben 100 1 1.01 Hybrid 100 10 1.10 Table 3: Proportion of Split Sentences (% split) in the training/test data and in average per sentence (average split / sentence). GOLD is the test data with the gold standard SWKP sentences; Zhu, Woodsend, Wubben are the best output of the models of Zhu et al. (2010), Woodsend and Lapata (2011) and Wubben et al. (2012) respectively; Hybrid is our model. Number of Edits Table 4 indicates the edit distance of the output sentences w.r.t. both the complex and the simple reference sentences as well as the number of input for which no simplification occur. The right part of the table shows that our system generate simplifications which are closest to the reference sentence (in terms of edits) compared to those output by the other systems. It also produces the highest number of simplifications which are identical to the reference. Conversely our system only rank</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 1353–1361, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>