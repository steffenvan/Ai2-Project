<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.992928">
Factorization of Synchronous Context-Free Grammars in Linear Time
</title>
<author confidence="0.998741">
Hao Zhang and Daniel Gildea
</author>
<affiliation confidence="0.998145">
Computer Science Department
University of Rochester
</affiliation>
<address confidence="0.336494">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.989453" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999409764705882">
Factoring a Synchronous Context-Free
Grammar into an equivalent grammar with
a smaller number of nonterminals in each
rule enables synchronous parsing algo-
rithms of lower complexity. The prob-
lem can be formalized as searching for the
tree-decomposition of a given permutation
with the minimal branching factor. In this
paper, by modifying the algorithm of Uno
and Yagiura (2000) for the closely related
problem of finding all common intervals
of two permutations, we achieve a linear
time algorithm for the permutation factor-
ization problem. We also use the algo-
rithm to analyze the maximum SCFG rule
length needed to cover hand-aligned data
from various language pairs.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884">
A number of recent syntax-based approaches to
statistical machine translation make use of Syn-
chronous Context Free Grammar (SCFG) as the un-
derlying model of translational equivalence. Wu
(1997)’s Inversion Transduction Grammar, as well
as tree-transformation models of translation such as
Yamada and Knight (2001), Galley et al. (2004), and
Chiang (2005) all fall into this category.
A crucial question for efficient computation in ap-
proaches based on SCFG is the length of the gram-
mar rules. Grammars with longer rules can represent
a larger set of reorderings between languages (Aho
</bodyText>
<page confidence="0.968942">
25
</page>
<bodyText confidence="0.999772147058824">
and Ullman, 1972), but also require greater compu-
tational complexity for word alignment algorithms
based on synchronous parsing (Satta and Peserico,
2005). Grammar rules extracted from large paral-
lel corpora by systems such as Galley et al. (2004)
can be quite large, and Wellington et al. (2006) ar-
gue that complex rules are necessary by analyzing
the coverage of gold-standard word alignments from
different language pairs by various grammars.
However, parsing complexity depends not only
on rule length, but also on the specific permutations
represented by the individual rules. It may be possi-
ble to factor an SCFG with maximum rule length
n into a simpler grammar with a maximum of k
nonterminals in any one rule, if not all n! permuta-
tions appear in the rules. Zhang et al. (2006) discuss
methods for binarizing SCFGs, ignoring the non-
binarizable grammars; in Section 2 we discuss the
generalized problem of factoring to k-ary grammars
for any k and formalize the problem as permutation
factorization in Section 3.
In Section 4, we describe an O(k · n) left-to-
right shift-reduce algorithm for analyzing permuta-
tions that can be k-arized. Its time complexity be-
comes O(n2) when k is not specified beforehand
and the minimal k is to be discovered. Instead of
linearly shifting in one number at a time, Gildea
et al. (2006) employ a balanced binary tree as the
control structure, producing an algorithm similar in
spirit to merge-sort with a reduced time complex-
ity of O(n log n). However, both algorithms rely
on reduction tests on emerging spans which involve
redundancies with the spans that have already been
tested.
</bodyText>
<note confidence="0.5923275">
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25–32,
Rochester, New York, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999105705882353">
Uno and Yagiura (2000) describe a clever algo-
rithm for the problem of finding all common inter-
vals of two permutations in time O(n + K), where
K is the number of common intervals, which can
itself be Q(n2). In Section 5, we adapt their ap-
proach to the problem of factoring SCFGs, and show
that, given this problem definition, running time can
be improved to O(n), the optimum given the time
needed to read the input permutation.
The methodology in Wellington et al. (2006) mea-
sures the complexity of word alignment using the
number of gaps that are necessary for their syn-
chronous parser which allows discontinuous spans
to succeed in parsing. In Section 6, we provide a
more direct measurement using the minimal branch-
ing factor yielded by the permutation factorization
algorithm.
</bodyText>
<sectionHeader confidence="0.935939" genericHeader="introduction">
2 Synchronous CFG and Synchronous
Parsing
</sectionHeader>
<bodyText confidence="0.998454333333333">
We begin by describing the synchronous CFG for-
malism, which is more rigorously defined by Aho
and Ullman (1972) and Satta and Peserico (2005).
We adopt the SCFG notation of Satta and Peserico
(2005). Superscript indices in the right-hand side of
grammar rules:
</bodyText>
<equation confidence="0.9952355">
X → X(1)
1 ...X(n)
n , X(π(1)) π(1) ...X(π(n))
π(n)
</equation>
<bodyText confidence="0.9999385">
indicate that the nonterminals with the same index
are linked across the two languages, and will eventu-
ally be rewritten by the same rule application. Each
Xi is a variable which can take the value of any non-
terminal in the grammar.
We say an SCFG is n-ary if and only if the max-
imum number of co-indexed nonterminals, i.e. the
longest permutation contained in the set of rules, is
of size n.
Given a synchronous CFG and a pair of input
strings, we can apply a generalized CYK-style bot-
tom up chart parser to build synchronous parse
trees over the string pair. Wu (1997) demonstrates
the case of binary SCFG parsing, where six string
boundary variables, three for each language as in
monolingual CFG parsing, interact with each other,
yielding an O(N6) dynamic programming algo-
rithm, where N is the string length, assuming the
two paired strings are comparable in length. For an
n-ary SCFG, the parsing complexity can be as high
as O(Nn+4). The reason is even if we binarize on
one side to maintain 3 indices, for many unfriendly
permutations, at most n + 1 boundary variables in
the other language are necessary.
The fact that this bound is exponential in the rule
length n suggests that it is advantageous to reduce
the length of grammar rules as much as possible.
This paper focuses on converting an SCFG to the
equivalent grammar with smallest possible maxi-
mum rule size. The algorithm processes each rule
in the input grammar independently, and determines
whether the rule can be factored into smaller SCFG
rules by analyzing the rule’s permutation 7r.
As an example, given the input rule:
</bodyText>
<equation confidence="0.933029">
[X → A(1)B(2)C(3)D(4)E(5)F(6)G(7),
X → E(5)G(7)D(4)F(6)C(3)A(1)B(2) ] (1)
we consider the associated permutation:
(5, 7, 4, 6, 3,1, 2)
</equation>
<bodyText confidence="0.9995535">
We determine that this permutation can be fac-
tored into the following permutation tree:
</bodyText>
<equation confidence="0.846521">
(2,1)
5 7 4 6
</equation>
<bodyText confidence="0.999364">
We define permutation trees formally in the next
section, but note here that nodes in the tree corre-
spond to subsets of nonterminals that form a sin-
gle continuous span in both languages, as shown by
the shaded regions in the permutation matrix above.
This tree can be converted into a set of output rules
that are generatively equivalent to the original rule:
</bodyText>
<equation confidence="0.999921333333333">
[X → X(1)1X(22), X → X(2)
2 X(1)
1 ]
[X1 → A(1)B(2), X1 → A(1)B(2) ]
[X2 → C(1)X(2)
3 , X2 → X3(2)C(1)
]
[X3 → D(1)E(2)F(3)G(4),
X3 → E(2)G(4)D(1)F(3) ]
</equation>
<bodyText confidence="0.9988578">
where X1, X2 and X3 are new nonterminals used to
represent the intermediate states in which the syn-
chronous nodes are combined. The factorized gram-
mar is only larger than the original grammar by a
constant factor.
</bodyText>
<figure confidence="0.9814602">
(2,1)
(1,2)
3
(2,4,1,3)
1 2
</figure>
<page confidence="0.991122">
26
</page>
<sectionHeader confidence="0.993017" genericHeader="method">
3 Permutation Trees
</sectionHeader>
<bodyText confidence="0.9989742">
We define the notion of permutation structure in this
section. We define a permuted sequence as a per-
mutation of n (n &gt; 1) consecutive natural numbers.
A permuted sequence is said to be k-ary parsable
if either of the following conditions holds:
</bodyText>
<listItem confidence="0.947652833333333">
1. The permuted sequence only has one number.
2. It has more than one number and can be seg-
mented into k′ (k &gt; k′ &gt; 2) permuted se-
quences each of which is k-ary parsable, and
the k′ subsequences are arranged in an order
identified by one of the k′! permutations of k′.
</listItem>
<bodyText confidence="0.9819646">
This is a recursive definition, and we call the cor-
responding recursive structure over the entire se-
quence a k-ary permutation tree.
Our goal is to find out the k-ary permutation tree
for a given permutation, where k is minimized.
</bodyText>
<sectionHeader confidence="0.905517" genericHeader="method">
4 Shift-reduce on Permutations
</sectionHeader>
<bodyText confidence="0.99826725">
In this section, we present an O(n · k) algorithm
which can be viewed as a need-to-be-optimized ver-
sion of the linear time algorithm to be presented in
the next section.
The algorithm is based on a shift-reduce parser,
which maintains a stack for subsequences that have
been discovered so far and loops over shift and re-
duce steps:
</bodyText>
<listItem confidence="0.784490357142857">
1. Shift the next number in the input permutation
onto the stack.
2. Go down the stack from the top to the bottom.
Whenever the top m subsequences satisfy the
partition property, which says the total length
of the m (k &gt; m &gt; 2) subsequences minus 1
is equal to the difference between the smallest
number and the largest number contained in the
m segments, make a reduction by gluing the
m segments into one subsequence and restart
reducing from the top of the new stack. Stop
when no reduction is possible.
3. If there are remaining numbers in the input per-
mutation, go to 1.
</listItem>
<bodyText confidence="0.982708">
When we exit from the loop, if the height of the stack
is 1, the input permutation of n has been reduced to
</bodyText>
<table confidence="0.984408615384615">
Stack Input Operation
7, 4, 6, 3, 1, 2 shift
5 7, 4, 6, 3, 1, 2 shift
5, 7 4, 6, 3, 1, 2 shift
5, 7, 4 3, 1, 2 shift
5, 7, 4, 6 3, 1, 2 reduce by (2,4,1,3)
[4...7] 3, 1, 2 shift
[4...7], 3 1, 2 reduce by (2,1)
[3...7] 1, 2 shift
[3...7], 1 2 shift
[3...7], 1, 2 reduce by (1,2)
[3...7], [1...2] reduce by (2,1)
[1...7]
</table>
<tableCaption confidence="0.770826">
Table 1: The execution trace of the shift-reduce
parser on the input permutation 5, 7, 4, 6, 3, 1, 2.
</tableCaption>
<bodyText confidence="0.99971936">
a linear sequence of 1 to n, and parsing is success-
ful. Otherwise, the input permutation of n cannot be
parsed into a k-ary permutation tree.
An example execution trace of the algorithm is
shown in Table 1.
The partition property is a sufficient and neces-
sary condition for the top m subsequences to be re-
ducible. In order to check if the property holds, we
need to compute the sum of the lengths of subse-
quences under consideration and the difference be-
tween the largest and smallest number in the cov-
ered region. We can incrementally compute both
along with each step going down the stack. If m
is bounded by k, we need O(k) operations for each
item shifted onto the stack. So, the algorithm runs in
O(n · k).
We might also wish to compute the minimum k
for which k-arization can be successful on an input
permutation of n. We can simply keep doing reduc-
tion tests for every possible top region of the stack
while going deeper in the stack to find the minimal
reduction. In the worst case, each time we go down
to the bottom of the increasingly higher stack with-
out a successful reduction. Thus, in O(n2), we can
find the minimum k-arization.
</bodyText>
<sectionHeader confidence="0.971042" genericHeader="method">
5 Linear Time Factorization
</sectionHeader>
<bodyText confidence="0.999931">
In this section, we show a linear time algorithm
which shares the left-to-right and bottom-up control
structure but uses more book-keeping operations to
reduce unnecessary reduction attempts. The reason
that our previous algorithm is asymptotically O(n2)
</bodyText>
<page confidence="0.987551">
27
</page>
<bodyText confidence="0.99998985">
is that whenever a new number is shifted in, we have
to try out every possible new span ending at the new
number. Do we need to try every possible span? Let
us start with a motivating example. The permuted
sequence (5, 7, 4, 6) in Table 1 can only be reduced
as a whole block. However, in the last algorithm,
when 4 is shifted in, we make an unsuccessful at-
tempt for the span on (7, 4), knowing we are miss-
ing 5, which will not appear when we expand the
span no matter how much further to the right. Yet
we repeat the same mistake to try on 7 when 6 is
scanned in by attempting on (7, 4, 6). Such wasteful
checks result in the quadratic behavior of the algo-
rithm. The way the following algorithm differs from
and outperforms the previous algorithm is exactly
that it crosses out impossible candidates for reduc-
tions such as 7 in the example as early as possible.
Now we state our problem mathematically. We
define a function whose value indicates the re-
ducibility of each pair of positions (x, y) (1 &lt; x &lt;
</bodyText>
<equation confidence="0.8703345">
y &lt; n): — —
f(x, y) = u(x, y) − l(x, y) − (y − x)
</equation>
<bodyText confidence="0.756826">
where
</bodyText>
<equation confidence="0.9890495">
l(x, y) = min
iE[x,y]
u(x, y) = max
iE[x,y]
</equation>
<bodyText confidence="0.999852944444445">
l records the minimum of the numbers that are
permuted to from the positions in the region [x, y].
u records the maximum. Figure 1 provides the vi-
sualization of u, l, and f for the example permuta-
tion (5, 7, 4, 6, 3, 1, 2). u and l can be visualized as
stairs. u goes up from the right end to the left. l
goes down. f is non-negative, but not monotonic
in general. We can make a reduction on (x, y) if
and only if f(x, y) = 0. This is the mathemati-
cal statement of the partition property in step 2 of
the shift-reduce algorithm. u and l can be computed
incrementally from smaller spans to larger spans to
guarantee O(1) operations for computing f on each
new span of [x, y] as long as we go bottom up. In the
new algorithm, we will reduce the size of the search
space of candidate position pairs (x, y) to be linear
in n so that the whole algorithm is O(n).
The algorithm has two main ideas:
</bodyText>
<listItem confidence="0.929769">
• We filter x’s to maintain the invariant that
f(x, y) (x &lt; y) is monotonically decreasing
with respect to x, over iterations on y (from 1
to n), so that any remaining values of x corre-
sponding to valid reductions are clustered at the
point where f tails off to zero. To put it another
way, we never have to test invalid reductions,
because the valid reductions have been sorted
together for us.
• We make greedy reductions as in the shift-
reduce algorithm.
</listItem>
<bodyText confidence="0.9919946">
In the new algorithm, we use a doubly linked list,
instead of a stack, as the data structure that stores
the candidate x’s to allow for more flexible main-
taining operations. The steps of the algorithm are as
follows:
</bodyText>
<listItem confidence="0.993729913043478">
1. Increase the left-to-right index y by one and ap-
pend it to the right end of the list.
2. Find the pivot x* in the list which is minimum
(leftmost) among x satisfying either u(x, y −
1) &lt; u(x, y) (exclusively) or l(x, y − 1) &gt;
l(x, y).
3. Remove those x’s that yield even smaller
u(x, y − 1) than u(x*, y − 1) or even larger
l(x, y − 1) than l(x*, y − 1). Those x’s must
be on the right of x* if they exist. They must
form a sub-list extending to the right end of the
original x list.
4. Denote the x which is immediately to the left
of x* as x′. Repeatedly remove all x’s such that
f(x, y) &gt; f(x′, y) where x is at the left end of
the sub-list of x’s starting from x* extending to
the right.
5. Go down the pruned list from the right end, out-
put (x, y) until f(x, y) &gt; 0. Remove x’s such
that f(x, y) = 0, sparing the smallest x which
is the leftmost among all such x’s on the list.
6. If there are remaining numbers in the input per-
mutation, go to 1.
</listItem>
<bodyText confidence="0.99958725">
The tricks lie in step 3 and step 4, where bad can-
didate x’s are filtered out. We use the following di-
agram to help readers understand the parts of x-list
that the two steps are filtering on.
</bodyText>
<equation confidence="0.60647025">
π(i)
π(i)
28
step 4
1z*} |{
x1,...,x,x ,...,xi,...,xj,...,xk
 |{z }
step 3
</equation>
<bodyText confidence="0.999989">
The steps from 2 to 4 are the operations that main-
tain the monotonic invariant which makes the reduc-
tions in step 5 as trivial as performing output. The
stack-based shift-reduce algorithm has the same top-
level structure, but lacks steps 2 to 4 so that in step 5
we have to winnow the entire list. Both algorithms
scan left to right and examine potential reduction
spans by extending the left endpoint from right to
left given a right endpoint.
</bodyText>
<subsectionHeader confidence="0.927924">
5.1 Example Execution Trace
</subsectionHeader>
<bodyText confidence="0.999991631578947">
An example of the algorithm’s execution is shown
in Figure 1. The evolution of u(x, y), l(x, y), and
f(x, y) is displayed for increasing y’s (from 2 to 7).
To identify reducible spans, we can check the plot of
f(x, y) to locate the (x, y) pairs that yield zero. The
pivots found by step 2 of the algorithm are marked
with *’s on the x-axis in the plot for u and l. The x’s
that are filtered out by step 3 or 4 are marked with
horizontal bars across. We want to point out the in-
teresting steps. When y = 3, x* = 1, x = 2 needs
to be crossed out by step 3 in the algorithm. When
y = 4, x* = 3, x = 3 itself is to be deleted by step 4
in the algorithm. x = 4 is removed at step 5 because
it is the right end in the first reduction. On the other
hand, x = 4 is also a bad starting point for future
reductions. Notice that we also remove x = 5 at
step 6, which can be a good starting point for reduc-
tions. But we exclude it from further considerations,
because we want left-most reductions.
</bodyText>
<subsectionHeader confidence="0.995541">
5.2 Correctness
</subsectionHeader>
<bodyText confidence="0.999318363636364">
Now we explain why the algorithm works. Both al-
gorithms are greedy in the sense that at each scan
point we exhaustively reduce all candidate spans to
the leftmost possible point. It can be shown that
greediness is safe for parsing permutations.
What we need to show is how the monotonic in-
variant holds and is valid. Now we sketch the proof.
We want to show for all xi remaining on the list,
f(xi, y) &gt; f(xi+1, y). When y = 1, it is trivially
true. Now we do the induction on y step by case
analysis:
</bodyText>
<equation confidence="0.340107">
Case 1: If xi &lt; xi+1 &lt; x*, then f(xi, y) −
f(xi, y − 1) = −1. The reason is if xi is on the
</equation>
<bodyText confidence="0.9930204">
left of x*, both u(xi, y) and l(xi, y) are not changed
from the y − 1-th step, so the only difference is that
y−xi has increased by one. Graphically, the f curve
extending to the left of x* shifts down a unit of 1. So,
the monotonic property still holds to the left of x*.
</bodyText>
<subsubsectionHeader confidence="0.809603">
Case 2: If x* &lt; xi &lt; xi+1, then f(xi, y) −
</subsubsectionHeader>
<bodyText confidence="0.996702483870968">
f(xi, y − 1) = c (c &gt; 0). The reason is that after
executing step 3 in the algorithm, the remaining xi’s
have either their u(xi, y) shifted up uniformly with
l(xi, y) being unchanged, or the symmetric case that
l(xi, y) is shifted down uniformly without changing
u(xi, y). In both cases, the difference between u and
l increases by at least one unit to offset the one unit
increase of y − xi. The result is that the f curve ex-
tending from x* to the right shifts up or remains the
same.
Case 3: So the half curve of f on the left of x* is
shifting down and the half right curve on the right is
shifting up, making it necessary to consider the case
that xi is on the left and xi+1 on the right. Fortu-
nately, step 4 in the algorithm deals with this case
explicitly by cutting down the head of the right half
curve to smooth the whole curve into a monotoni-
cally decreasing one.
We still need one last piece for the proof, i.e., the
validity of pruning. Is it possible we winnow off
good x’s that will become useful in later stages of
y? The answer is no. The values we remove in step
3 and 4 are similar to the points indexing into the
second and third numbers in the permuted sequence
(5, 7, 4, 6). Any span starting from these two points
will not be reducible because the element 5 is miss-
ing.1
To summarize, we remove impossible left bound-
aries and keep good ones, resulting in the mono-
tonicity of f function which in turn makes safe
greedy reductions fast.
</bodyText>
<subsectionHeader confidence="0.991792">
5.3 Implementation and Time Analysis
</subsectionHeader>
<bodyText confidence="0.99823025">
We use a doubly linked list to implement both the u
and l functions, where list element includes a span
of x values (shaded rectangles in Figure 1). Both
lists can be doubly linked with the list of x’s so that
</bodyText>
<footnote confidence="0.789776">
1Uno and Yagiura (2000) prove the validity of step 3 and
step 4 rigorously.
</footnote>
<note confidence="0.24529">
,y
</note>
<page confidence="0.994418">
29
</page>
<bodyText confidence="0.9997335">
we can access the u function and l function at O(1)
time for each x. At the same time, if we search for
x based on u or l, we can follow the stair functions,
skipping many intermediate x’s.
The total number of operations that occur at step
4 and step 5 is O(n) since these steps just involve
removing nodes on the x list, and only n nodes are
created in total over the entire algorithm. To find
x∗, we scan back from the right end of u list or l
list. Due to step 3, each u (and l) element that we
scan over is removed at this iteration. So the total
number of operations accountable to step 2 and step
3 is bounded by the maximum number of nodes ever
created on the u and l lists, which is also n.
</bodyText>
<sectionHeader confidence="0.905005" genericHeader="method">
5.4 Related Work
</sectionHeader>
<bodyText confidence="0.999980307692308">
Our algorithm is based on an algorithm for finding
all common intervals of two permutations (Uno and
Yagiura, 2000). The difference2 is in step 5, where
we remove the embedded reducible x’s and keep
only the leftmost one; their algorithm will keep all of
the reducible x’s for future considerations so that in
the example the number 3 will be able to involve in
both the reduction ([4−7], 3) and (3, [1−2]). In the
worst case, their algorithm will output a quadratic
number of reducible spans, making the whole algo-
rithm O(n2). Our algorithm is O(n) in the worst
case. We can also generate all common intervals by
transforming the permutation tree output by our al-
gorithm.
However, we are not the first to specialize the Uno
and Yagiura algorithm to produce tree structures for
permutations. Bui-Xuan et al. (2005) reached a lin-
ear time algorithm in the definition framework of
PQ trees. PQ trees represent families of permuta-
tions that can be created by composing operations
of scrambling subsequences according to any per-
mutation (P nodes) and concatenating subsequences
in order (Q nodes). Our definition of permutation
tree can be thought of as a more specific version of a
PQ tree, where the nodes are all labeled with a spe-
cific permutation which is not decomposable.
</bodyText>
<footnote confidence="0.8065485">
2The original Uno and Yagiura algorithm also has the minor
difference that the scan point goes from right to left.
</footnote>
<sectionHeader confidence="0.995257" genericHeader="method">
6 Experiments on Analyzing Word
Alignments
</sectionHeader>
<bodyText confidence="0.999991204545455">
We apply the factorization algorithm to analyzing
word alignments in this section. Wellington et al.
(2006) indicate the necessity of introducing discon-
tinuous spans for synchronous parsing to match up
with human-annotated word alignment data. The
number of discontinuous spans reflects the struc-
tural complexity of the synchronous rules that are
involved in building the synchronous trees for the
given alignments. However, the more direct and de-
tailed analysis would be on the branching factors of
the synchronous trees for the aligned data.
Since human-aligned data has many-to-one word
links, it is necessary to modify the alignments into
one-to-one. Wellington et al. (2006) treat many-to-
one word links disjunctively in their synchronous
parser. We also commit to one of the many-one links
by extracting a maximum match (Cormen et al.,
1990) from the bipartite graph of the alignment. In
other words, we abstract away the alternative links
in the given alignment while capturing the backbone
using the maximum number of word links.
We use the same alignment data for the five
language pairs Chinese/English, Romanian/English,
Hindi/English, Spanish/English, and French/English
(Wellington et al., 2006). In Table 2, we report the
number of sentences that are k-ary parsable but not
k − 1-ary parsable for increasing k’s. Our analysis
reveals that the permutations that are accountable for
non-ITG alignments include higher order permuta-
tions such as (3, 1, 5, 2, 4), albeit sparsely seen.
We also look at the number of terminals the non-
binary synchronous nodes can cover. We are in-
terested in doing so, because this can tell us how
general these unfriendly rules are. Wellington et al.
(2006) did a similar analysis on the English-English
bitext. They found out the majority of non-ITG
parsable cases are not local in the sense that phrases
of length up to 10 are not helpful in covering the
gaps. We analyzed the translation data for the five
language pairs instead. Our result differs. The right-
most column in Table 2 shows that only a tiny per-
cent of the non-ITG cases are significant in the sense
that we can not deal with them through phrases or
tree-flattening within windows of size 10.
</bodyText>
<page confidence="0.970544">
30
</page>
<figure confidence="0.995515158163266">
y = 2:
y = 5:
u,l
f
u,l
f
6
6
7
7
5
5
6
6
5
4
5
4
4
3
4
3
3
2
3
2
2
1
2
1
1
0
1
0
x
x
x
x
((1
4�)
5)
1
*
2
3
4
5
6
1
2
3
4
5
6
7
�2
3
6
1
2
3
4
5
6
7
7
7
y = 3:
y = 6:
u,l
f
u,l
f
6
6
7
7
5
5
6
6
5
5
4
4
4
3
4
3
3
2
2
3
2
1
2
1
1
1
0
0
x
x
x
x
((1
4�)
5)
1
*
3 4
5
6
2
3 4
5
6
7
3
6
1
3 4
5
6
7
�2
7
1
�2
7
2
y = 4:
y = 7:
u,l
f
u,l
f
6
6
7
7
6
5
6
5
5
5
4
4
4
3
4
3
2
2
3
3
2
1
2
1
1
1
0
0
x
x
x
x
(1
4)
7))
(((1
4�)
5)
(6
3
*
5
6
7
1
2
3 4
5
6
7
3
1
2
3 4
5
6
7
�2
�2
</figure>
<figureCaption confidence="0.688136">
Figure 1: Evolution of u(x, y), l(x, y), and f(x, y) as y goes from 2 to 7 for the permutation
(5, 7, 4, 6, 3, 1, 2). We use * under the x-axis to indicate the x∗’s that are pivots in the algorithm. Use-
less x’s are crossed out. x’s that contribute to reductions are marked with either ( on its left or ) on its right.
For the f function, we use solid boxes to plot the values of remaining x’s on the list but also show the other
f values for completeness.
</figureCaption>
<page confidence="0.999538">
31
</page>
<table confidence="0.999164428571429">
Branching Factor
1 2 4 5 6 7 10 &gt; 4 (and covering &gt; 10 words)
Chinese/English 451 30 4 5 1 7(1.4%)
Romanian/English 195 4 0
Hindi/English 3 85 1 1 0
Spanish/English 195 4 1(0.5%)
French/English 425 9 9 3 1 6(1.3%)
</table>
<tableCaption confidence="0.995747">
Table 2: Distribution of branching factors for synchronous trees on various language pairs .
</tableCaption>
<sectionHeader confidence="0.998307" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999963777777778">
We present a linear time algorithm for factorizing
any n-ary SCFG rule into a set of k-ary rules where
k is minimized. The algorithm speeds up an easy-
to-understand shift-reduce algorithm, by avoiding
unnecessary reduction attempts while maintaining
the left-to-right bottom-up control structure. Em-
pirically, we provide a complexity analysis of word
alignments based on the concept of minimal branch-
ing factor.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999416072727273">
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura’s algo-
rithm. In The 16th Annual International Symposium
on Algorithms and Computation (ISAAC’05), pages
146–155.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263–270,
Ann Arbor, Michigan.
Thomas H. Cormen, Charles E. Leiserson, and Ronald L.
Rivest. 1990. Introduction to algorithms. MIT Press,
Cambridge, MA.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of the Human Language Technology Confer-
ence/North American Chapter of the Association for
Computational Linguistics (HLT/NAACL).
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006. Fac-
toring synchronous grammars by sorting. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06) Poster Session, Sydney.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803–810, Vancouver, Canada,
October.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast algo-
rithms to enumerate all common intervals of two per-
mutations. Algorithmica, 26(2):290–309.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference/North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
</reference>
<page confidence="0.999262">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964791">
<title confidence="0.999799">Factorization of Synchronous Context-Free Grammars in Linear Time</title>
<author confidence="0.984308">Zhang</author>
<affiliation confidence="0.9998095">Computer Science University of</affiliation>
<address confidence="0.999064">Rochester, NY 14627</address>
<abstract confidence="0.998822555555555">Factoring a Synchronous Context-Free Grammar into an equivalent grammar with a smaller number of nonterminals in each rule enables synchronous parsing algorithms of lower complexity. The problem can be formalized as searching for the tree-decomposition of a given permutation with the minimal branching factor. In this paper, by modifying the algorithm of Uno and Yagiura (2000) for the closely related problem of finding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Albert V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling,</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="4207" citStr="Aho and Ullman (1972)" startWordPosition="674" endWordPosition="677">definition, running time can be improved to O(n), the optimum given the time needed to read the input permutation. The methodology in Wellington et al. (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. In Section 6, we provide a more direct measurement using the minimal branching factor yielded by the permutation factorization algorithm. 2 Synchronous CFG and Synchronous Parsing We begin by describing the synchronous CFG formalism, which is more rigorously defined by Aho and Ullman (1972) and Satta and Peserico (2005). We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: X → X(1) 1 ...X(n) n , X(π(1)) π(1) ...X(π(n)) π(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. We say an SCFG is n-ary if and only if the maximum number of co-indexed nonterminals, i.e. the longest permutation contained in the set of rules, is of size n. Given</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Albert V. Aho and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binh Minh Bui-Xuan</author>
<author>Michel Habib</author>
<author>Christophe Paul</author>
</authors>
<title>Revisiting T. Uno and M. Yagiura’s algorithm.</title>
<date>2005</date>
<booktitle>In The 16th Annual International Symposium on Algorithms and Computation (ISAAC’05),</booktitle>
<pages>146--155</pages>
<contexts>
<context position="20256" citStr="Bui-Xuan et al. (2005)" startWordPosition="3746" endWordPosition="3749">d keep only the leftmost one; their algorithm will keep all of the reducible x’s for future considerations so that in the example the number 3 will be able to involve in both the reduction ([4−7], 3) and (3, [1−2]). In the worst case, their algorithm will output a quadratic number of reducible spans, making the whole algorithm O(n2). Our algorithm is O(n) in the worst case. We can also generate all common intervals by transforming the permutation tree output by our algorithm. However, we are not the first to specialize the Uno and Yagiura algorithm to produce tree structures for permutations. Bui-Xuan et al. (2005) reached a linear time algorithm in the definition framework of PQ trees. PQ trees represent families of permutations that can be created by composing operations of scrambling subsequences according to any permutation (P nodes) and concatenating subsequences in order (Q nodes). Our definition of permutation tree can be thought of as a more specific version of a PQ tree, where the nodes are all labeled with a specific permutation which is not decomposable. 2The original Uno and Yagiura algorithm also has the minor difference that the scan point goes from right to left. 6 Experiments on Analyzin</context>
</contexts>
<marker>Bui-Xuan, Habib, Paul, 2005</marker>
<rawString>Binh Minh Bui-Xuan, Michel Habib, and Christophe Paul. 2005. Revisiting T. Uno and M. Yagiura’s algorithm. In The 16th Annual International Symposium on Algorithms and Computation (ISAAC’05), pages 146–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1215" citStr="Chiang (2005)" startWordPosition="180" endWordPosition="181">ommon intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho 25 and Ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (Satta and Peserico, 2005). Grammar rules extracted from large parallel corpora by systems such as Galley et al. (2004) can be quite large, and Wellington et al. (2006) argue that complex rules are necessary by analyzing the coverage of g</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05), pages 263–270, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to algorithms.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21720" citStr="Cormen et al., 1990" startWordPosition="3980" endWordPosition="3983">d alignment data. The number of discontinuous spans reflects the structural complexity of the synchronous rules that are involved in building the synchronous trees for the given alignments. However, the more direct and detailed analysis would be on the branching factors of the synchronous trees for the aligned data. Since human-aligned data has many-to-one word links, it is necessary to modify the alignments into one-to-one. Wellington et al. (2006) treat many-toone word links disjunctively in their synchronous parser. We also commit to one of the many-one links by extracting a maximum match (Cormen et al., 1990) from the bipartite graph of the alignment. In other words, we abstract away the alternative links in the given alignment while capturing the backbone using the maximum number of word links. We use the same alignment data for the five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English (Wellington et al., 2006). In Table 2, we report the number of sentences that are k-ary parsable but not k − 1-ary parsable for increasing k’s. Our analysis reveals that the permutations that are accountable for non-ITG alignments include higher order permutations</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to algorithms. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="1196" citStr="Galley et al. (2004)" startWordPosition="175" endWordPosition="178">d problem of finding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho 25 and Ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (Satta and Peserico, 2005). Grammar rules extracted from large parallel corpora by systems such as Galley et al. (2004) can be quite large, and Wellington et al. (2006) argue that complex rules are necessary by analyzin</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Giorgio Satta</author>
<author>Hao Zhang</author>
</authors>
<title>Factoring synchronous grammars by sorting.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06) Poster Session,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="2775" citStr="Gildea et al. (2006)" startWordPosition="440" endWordPosition="443">ny one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, producing an algorithm similar in spirit to merge-sort with a reduced time complexity of O(n log n). However, both algorithms rely on reduction tests on emerging spans which involve redundancies with the spans that have already been tested. Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25–32, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals of t</context>
</contexts>
<marker>Gildea, Satta, Zhang, 2006</marker>
<rawString>Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006. Factoring synchronous grammars by sorting. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06) Poster Session, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
<author>Enoch Peserico</author>
</authors>
<title>Some computational complexity results for synchronous contextfree grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<pages>803--810</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="1603" citStr="Satta and Peserico, 2005" startWordPosition="240" endWordPosition="243">ammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho 25 and Ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (Satta and Peserico, 2005). Grammar rules extracted from large parallel corpora by systems such as Galley et al. (2004) can be quite large, and Wellington et al. (2006) argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars. However, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules. It may be possible to factor an SCFG with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear i</context>
<context position="4237" citStr="Satta and Peserico (2005)" startWordPosition="679" endWordPosition="682">an be improved to O(n), the optimum given the time needed to read the input permutation. The methodology in Wellington et al. (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. In Section 6, we provide a more direct measurement using the minimal branching factor yielded by the permutation factorization algorithm. 2 Synchronous CFG and Synchronous Parsing We begin by describing the synchronous CFG formalism, which is more rigorously defined by Aho and Ullman (1972) and Satta and Peserico (2005). We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: X → X(1) 1 ...X(n) n , X(π(1)) π(1) ...X(π(n)) π(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. We say an SCFG is n-ary if and only if the maximum number of co-indexed nonterminals, i.e. the longest permutation contained in the set of rules, is of size n. Given a synchronous CFG and a pair </context>
</contexts>
<marker>Satta, Peserico, 2005</marker>
<rawString>Giorgio Satta and Enoch Peserico. 2005. Some computational complexity results for synchronous contextfree grammars. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 803–810, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeaki Uno</author>
<author>Mutsunori Yagiura</author>
</authors>
<title>Fast algorithms to enumerate all common intervals of two permutations.</title>
<date>2000</date>
<journal>Algorithmica,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="3294" citStr="Uno and Yagiura (2000)" startWordPosition="519" endWordPosition="522">nimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, producing an algorithm similar in spirit to merge-sort with a reduced time complexity of O(n log n). However, both algorithms rely on reduction tests on emerging spans which involve redundancies with the spans that have already been tested. Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25–32, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals of two permutations in time O(n + K), where K is the number of common intervals, which can itself be Q(n2). In Section 5, we adapt their approach to the problem of factoring SCFGs, and show that, given this problem definition, running time can be improved to O(n), the optimum given the time needed to read the input permutation. The methodology in Wellington et al. (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans t</context>
<context position="18667" citStr="Uno and Yagiura (2000)" startWordPosition="3447" endWordPosition="3450">e points indexing into the second and third numbers in the permuted sequence (5, 7, 4, 6). Any span starting from these two points will not be reducible because the element 5 is missing.1 To summarize, we remove impossible left boundaries and keep good ones, resulting in the monotonicity of f function which in turn makes safe greedy reductions fast. 5.3 Implementation and Time Analysis We use a doubly linked list to implement both the u and l functions, where list element includes a span of x values (shaded rectangles in Figure 1). Both lists can be doubly linked with the list of x’s so that 1Uno and Yagiura (2000) prove the validity of step 3 and step 4 rigorously. ,y 29 we can access the u function and l function at O(1) time for each x. At the same time, if we search for x based on u or l, we can follow the stair functions, skipping many intermediate x’s. The total number of operations that occur at step 4 and step 5 is O(n) since these steps just involve removing nodes on the x list, and only n nodes are created in total over the entire algorithm. To find x∗, we scan back from the right end of u list or l list. Due to step 3, each u (and l) element that we scan over is removed at this iteration. So </context>
</contexts>
<marker>Uno, Yagiura, 2000</marker>
<rawString>Takeaki Uno and Mutsunori Yagiura. 2000. Fast algorithms to enumerate all common intervals of two permutations. Algorithmica, 26(2):290–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06).</booktitle>
<contexts>
<context position="1745" citStr="Wellington et al. (2006)" startWordPosition="265" endWordPosition="268">odels of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho 25 and Ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (Satta and Peserico, 2005). Grammar rules extracted from large parallel corpora by systems such as Galley et al. (2004) can be quite large, and Wellington et al. (2006) argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars. However, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules. It may be possible to factor an SCFG with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the gener</context>
<context position="3744" citStr="Wellington et al. (2006)" startWordPosition="600" endWordPosition="603">shop on Syntax and Structure in Statistical Translation, pages 25–32, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals of two permutations in time O(n + K), where K is the number of common intervals, which can itself be Q(n2). In Section 5, we adapt their approach to the problem of factoring SCFGs, and show that, given this problem definition, running time can be improved to O(n), the optimum given the time needed to read the input permutation. The methodology in Wellington et al. (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. In Section 6, we provide a more direct measurement using the minimal branching factor yielded by the permutation factorization algorithm. 2 Synchronous CFG and Synchronous Parsing We begin by describing the synchronous CFG formalism, which is more rigorously defined by Aho and Ullman (1972) and Satta and Peserico (2005). We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of gr</context>
<context position="20981" citStr="Wellington et al. (2006)" startWordPosition="3865" endWordPosition="3868"> of permutations that can be created by composing operations of scrambling subsequences according to any permutation (P nodes) and concatenating subsequences in order (Q nodes). Our definition of permutation tree can be thought of as a more specific version of a PQ tree, where the nodes are all labeled with a specific permutation which is not decomposable. 2The original Uno and Yagiura algorithm also has the minor difference that the scan point goes from right to left. 6 Experiments on Analyzing Word Alignments We apply the factorization algorithm to analyzing word alignments in this section. Wellington et al. (2006) indicate the necessity of introducing discontinuous spans for synchronous parsing to match up with human-annotated word alignment data. The number of discontinuous spans reflects the structural complexity of the synchronous rules that are involved in building the synchronous trees for the given alignments. However, the more direct and detailed analysis would be on the branching factors of the synchronous trees for the aligned data. Since human-aligned data has many-to-one word links, it is necessary to modify the alignments into one-to-one. Wellington et al. (2006) treat many-toone word links</context>
<context position="22571" citStr="Wellington et al. (2006)" startWordPosition="4118" endWordPosition="4121">he five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English (Wellington et al., 2006). In Table 2, we report the number of sentences that are k-ary parsable but not k − 1-ary parsable for increasing k’s. Our analysis reveals that the permutations that are accountable for non-ITG alignments include higher order permutations such as (3, 1, 5, 2, 4), albeit sparsely seen. We also look at the number of terminals the nonbinary synchronous nodes can cover. We are interested in doing so, because this can tell us how general these unfriendly rules are. Wellington et al. (2006) did a similar analysis on the English-English bitext. They found out the majority of non-ITG parsable cases are not local in the sense that phrases of length up to 10 are not helpful in covering the gaps. We analyzed the translation data for the five language pairs instead. Our result differs. The rightmost column in Table 2 shows that only a tiny percent of the non-ITG cases are significant in the sense that we can not deal with them through phrases or tree-flattening within windows of size 10. 30 y = 2: y = 5: u,l f u,l f 6 6 7 7 5 5 6 6 5 4 5 4 4 3 4 3 3 2 3 2 2 1 2 1 1 0 1 0 x x x x ((1 4</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1054" citStr="Wu (1997)" startWordPosition="157" endWordPosition="158">ation with the minimal branching factor. In this paper, by modifying the algorithm of Uno and Yagiura (2000) for the closely related problem of finding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho 25 and Ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (Satta and Peserico, 2005). Grammar rules extracted from large parallel corpo</context>
<context position="4979" citStr="Wu (1997)" startWordPosition="816" endWordPosition="817">.X(n) n , X(π(1)) π(1) ...X(π(n)) π(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. We say an SCFG is n-ary if and only if the maximum number of co-indexed nonterminals, i.e. the longest permutation contained in the set of rules, is of size n. Given a synchronous CFG and a pair of input strings, we can apply a generalized CYK-style bottom up chart parser to build synchronous parse trees over the string pair. Wu (1997) demonstrates the case of binary SCFG parsing, where six string boundary variables, three for each language as in monolingual CFG parsing, interact with each other, yielding an O(N6) dynamic programming algorithm, where N is the string length, assuming the two paired strings are comparable in length. For an n-ary SCFG, the parsing complexity can be as high as O(Nn+4). The reason is even if we binarize on one side to maintain 3 indices, for many unfriendly permutations, at most n + 1 boundary variables in the other language are necessary. The fact that this bound is exponential in the rule leng</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01),</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1174" citStr="Yamada and Knight (2001)" startWordPosition="171" endWordPosition="174">00) for the closely related problem of finding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho 25 and Ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (Satta and Peserico, 2005). Grammar rules extracted from large parallel corpora by systems such as Galley et al. (2004) can be quite large, and Wellington et al. (2006) argue that complex rules are</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01), Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="2235" citStr="Zhang et al. (2006)" startWordPosition="349" endWordPosition="352">es extracted from large parallel corpora by systems such as Galley et al. (2004) can be quite large, and Wellington et al. (2006) argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars. However, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules. It may be possible to factor an SCFG with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, pro</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>