<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005518">
<author confidence="0.476942">
Subjectivity and Sentiment Analysis of Modern Standard Arabic
</author>
<affiliation confidence="0.865179">
Columbia University, NYC, USA,
</affiliation>
<email confidence="0.963196">
mdiab@ccls.columbia.edu
</email>
<author confidence="0.987079">
Muhammad Abdul-Mageed
</author>
<affiliation confidence="0.97365">
Department of Linguistics &amp;
School of Library &amp; Info. Science,
Indiana University,
Bloomington, USA,
</affiliation>
<author confidence="0.574837">
Mona T. Diab
</author>
<affiliation confidence="0.531542">
Center for Computational
</affiliation>
<author confidence="0.684835">
Learning Systems,
Mohammed Korayem
</author>
<affiliation confidence="0.905351">
School of Informatics
and Computing,
Indiana University,
Bloomington, USA,
</affiliation>
<email confidence="0.998848">
mabdulma@indiana.edu
</email>
<sectionHeader confidence="0.989525" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.979318333333333">
Although Subjectivity and Sentiment Analysis
(SSA) has been witnessing a flurry of novel re-
search, there are few attempts to build SSA
systems for Morphologically-Rich Languages
(MRL). In the current study, we report efforts
to partially fill this gap. We present a newly
developed manually annotated corpus of Mod-
ern Standard Arabic (MSA) together with a
new polarity lexicon.The corpus is a collec-
tion of newswire documents annotated on the
sentence level. We also describe an automatic
SSA tagging system that exploits the anno-
tated data. We investigate the impact of differ-
ent levels of preprocessing settings on the SSA
classification task. We show that by explicitly
accounting for the rich morphology the system
is able to achieve significantly higher levels of
performance.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992717">
Subjectivity and Sentiment Analysis (SSA) is an area
that has been witnessing a flurry of novel research.
In natural language, subjectivity refers to expression
of opinions, evaluations, feelings, and speculations
(Banfield, 1982; Wiebe, 1994) and thus incorporates
sentiment. The process of subjectivity classification
refers to the task of classifying texts into either ob-
jective (e.g., Mubarak stepped down) or subjective
(e.g., Mubarak, the hateful dictator, stepped down).
Subjective text is further classified with sentiment or
polarity. For sentiment classification, the task refers
to identifying whether the subjective text is positive
(e.g., What an excellent camera!), negative (e.g., I
hate this camera!), neutral (e.g., I believe there will
be a meeting.), or, sometimes, mixed (e.g., It is good,
but I hate it!) texts.
Most of the SSA literature has focused on En-
glish and other Indio-European languages. Very few
studies have addressed the problem for morphologi-
cally rich languages (MRL) such as Arabic, Hebrew,
</bodyText>
<email confidence="0.500774">
mkorayem@indiana.edu
</email>
<bodyText confidence="0.99983485">
Turkish, Czech, etc. (Tsarfaty et al., 2010). MRL
pose significant challenges to NLP systems in gen-
eral, and the SSA task is expected to be no excep-
tion. The problem is even more pronounced in some
MRL due to the lack in annotated resources for SSA
such as labeled corpora, and polarity lexica.
In the current paper, we investigate the task of
sentence-level SSA on Modern Standard Arabic
(MSA) texts from the newswire genre. We run
experiments on three different pre-processing set-
tings based on tokenized text from the Penn Ara-
bic Treebank (PATB) (Maamouri et al., 2004)
and employ both language-independent and Arabic-
specific, morphology-based features. Our work
shows that explicitly using morphology-based fea-
tures in our models improves the system’s perfor-
mance. We also measure the impact of using a wide
coverage polarity lexicon and show that using a tai-
lored resource results in significant improvement in
classification performance.
</bodyText>
<sectionHeader confidence="0.984023" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999816333333333">
To our knowledge, no SSA annotated MSA data ex-
ists. Hence we decided to create our own SSA an-
notated data.1
</bodyText>
<subsectionHeader confidence="0.999806">
2.1 Data set and Annotation
</subsectionHeader>
<bodyText confidence="0.967897818181818">
Corpus: Two college-educated native speakers
of Arabic annotated 2855 sentences from Part
1 V 3.0 of the PATB. The sentences make up
the first 400 documents of that part of PATB
amounting to a total of 54.5% of the PATB
Part 1 data set. For each sentence, the an-
notators assigned one of 4 possible labels: (1)
OBJECTIVE (OBJ), (2) SUBJECTIVE-POSITIVE
(S-POS), (3) SUBJECTIVE-NEGATIVE (S-NEG),
and (4) SUBJECTIVE-NEUTRAL (S-NEUT). Fol-
lowing (Wiebe et al., 1999), if the primary goal
</bodyText>
<footnote confidence="0.985778">
1The data may be obtained by contacting the first author.
</footnote>
<page confidence="0.912971">
587
</page>
<note confidence="0.6051255">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 587–591,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999395266666667">
of a sentence is judged as the objective reporting
of information, it was labeled as OBJ. Otherwise, a
sentence would be a candidate for one of the three
SUBJ classes. Inter-annotator agreement reached
88.06%.2 The distribution of classes in our data set
was as follows: 1281 OBJ, a total of 1574 SUBJ,
where 491 were deemed S-POS, 689 S-NEG, and
394 S-NEUT. Moreover, each of the sentences in our
data set is manually labeled by a domain label. The
domain labels are from the newswire genre and are
adopted from (Abdul-Mageed, 2008).
Polarity Lexicon: We manually created a lexicon
of 3982 adjectives labeled with one of the follow-
ing tags {positive, negative, neutral}. The adjectives
pertain to the newswire domain.
</bodyText>
<subsectionHeader confidence="0.996388">
2.2 Automatic Classification
</subsectionHeader>
<bodyText confidence="0.987516769230769">
Tokenization scheme and settings: We run experi-
ments on gold-tokenized text from PATB. We adopt
the PATB+Al tokenization scheme, where procli-
tics and enclitics as well as Al are segmented out
from the stem words. We experiment with three dif-
ferent pre-processing lemmatization configurations
that specifically target the stem words: (1) Surface,
where the stem words are left as is with no further
processing of the morpho-tactics that result from the
segmentation of clitics; (2) Lemma, where the stem
words are reduced to their lemma citation forms, for
instance in case of verbs it is the 3rd person mas-
culine singular perfective form; and (3) Stem, which
is the surface form minus inflectional morphemes, it
should be noted that this configuration may result in
non proper Arabic words (a la IR stemming). Ta-
ble 1 illustrates examples of the three configuration
schemes, with each underlined.
Features: The features we employed are of two
main types: Language-independent features and
Morphological features.
Language-Independent Features: This group of
features has been employed in various SSA studies.
Domain: Following (Wilson et al., 2009), we ap-
ply a feature indicating the domain of the document
to which a sentence belongs. As mentioned earlier,
each sentence has a document domain label manu-
ally associated with it.
2A detailed account of issues related to the annotation task
will appear in a separate publication.
UNIQUE: Following Wiebe et al. (2004) we ap-
ply a unique feature. Namely words that occur in our
corpus with an absolute frequency &lt; 5, are replaced
with the token ”UNIQUE”.
N-GRAM: We run experiments with N-grams &lt; 4
and all possible combinations of them.
ADJ: For subjectivity classification, we follow
Bruce &amp; Wiebe’s (1999) in adding a binary
has adjective feature indicating whether or not any
of the adjectives in our manually created polarity
lexicon exists in a sentence. For sentiment classi-
fication, we apply two features, has POS adjective
and has NEG adjective, each of these binary fea-
tures indicate whether a POS or NEG adjective oc-
curs in a sentence.
MSA-Morphological Features: MSA exhibits a
very rich morphological system that is templatic,
and agglutinative and it is based on both derivational
and inflectional features. We explicitly model mor-
phological features of person, state, gender, tense,
aspect, and number. We do not use POS informa-
tion. We assume undiacritized text in our models.
</bodyText>
<subsectionHeader confidence="0.996877">
2.3 Method: Two-stage Classification Process
</subsectionHeader>
<bodyText confidence="0.999933458333333">
In the current study, we adopt a two-stage classifica-
tion approach. In the first stage (i.e., Subjectivity),
we build a binary classifier to sort out OBJ from
SUBJ cases. For the second stage (i.e., Sentiment)
we apply binary classification that distinguishes S-
POS from S-NEG cases. We disregard the neutral
class of S-NEUT for this round of experimentation.
We use an SVM classifier, the SVMlight package
(Joachims, 2008). We experimented with various
kernels and parameter settings and found that linear
kernels yield the best performance. We ran experi-
ments with presence vectors: In each sentence vec-
tor, the value of each dimension is binary either a 1
(regardless of how many times a feature occurs) or
0.
Experimental Conditions: We first run ex-
periments using each of the three lemmatization
settings Surface, Lemma, Stem using various N-
grams and N-gram combinations and then itera-
tively add other features. The morphological fea-
tures (i.e., Morph) are added only to the Stem setting.
Language-independent features (i.e., from the fol-
lowing set {DOMAIN, ADJ, UNIQUE}) are added
to the Lemma and Stem+Morph settings. With all
</bodyText>
<page confidence="0.983685">
588
</page>
<table confidence="0.999357666666667">
Word POS Surface form Lemma Stem Gloss
AlwlAyAt Noun Al+wlAyAt Al+wlAyp Al+wlAy the states
ltblgh Verb l+tblg+h l+&gt;blg+h l+blg+h to inform him
</table>
<tableCaption confidence="0.999899">
Table 1: Examples of word lemmatization settings
</tableCaption>
<bodyText confidence="0.997423">
the three settings, clitics that are split off words are
kept as separate features in the sentence vectors.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="background">
3 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.996994909090909">
We divide our data into 80% for 5-fold cross-
validation and 20% for test. For experiments on the
test data, the 80% are used as training data. We have
two settings, a development setting (DEV) and a test
setting (TEST). In the development setting, we run
the typical 5 fold cross validation where we train on
4 folds and test on the 5th and then average the re-
sults. In the test setting, we only ran with the best
configurations yielded from the DEV conditions. In
TEST mode, we still train with 4 folds but we test on
the test data exclusively, averaging across the differ-
ent training rounds.
It is worth noting that the test data is larger than
any given dev data (20% of the overall data set for
test, vs. 16% for any DEV fold). We report results
using F-measure (F). Moreover, for TEST we re-
port only experiments on the Stem+Morph setting
and Stem+Morph+ADJ, Stem+Morph+DOMAIN,
and Stem+Morph+UNIQUE. Below, we only report
the best-performing results across the N-GRAM fea-
tures and their combinations. In each case, our base-
line is the majority class in the training set.
</bodyText>
<subsectionHeader confidence="0.997243">
3.1 Subjectivity
</subsectionHeader>
<bodyText confidence="0.999900619047619">
Among all the lemmatization settings, the Stem was
found to perform best with 73.17% F (with 1g+2g),
compared to 71.97% F (with 1g+2g+3g) for Sur-
face and 72.74% F (with 1g+2g) for Lemma. In ad-
dition, adding the inflectional morphology features
improves classification (and hence the Stem+Morph
setting, when ran under the same 1g+2g condition
as the Stem, is better by 0.15% F than the Stem
condition alone). As for the language-independent
features, we found that whereas the ADJ feature
does not help neither the Lemma nor Stem+Morph
setting, the DOMAIN feature improves the re-
sults slightly with the two settings. In addition,
the UNIQUE feature helps classification with the
Lemma, but it hurts with the Stem+Morph.
Table 2 shows that although performance on the
test set drops with all settings on Stem+Morph, re-
sults are still at least 10% higher than the bseline.
With the Stem+Morph setting, the best performance
on the TEST set is 71.54% Fand is 16.44% higher
than the baseline.
</bodyText>
<subsectionHeader confidence="0.99789">
3.2 Sentiment
</subsectionHeader>
<bodyText confidence="0.999994782608696">
Similar to the subjectivity results, the Stem set-
ting performs better than the other two lemmatiza-
tion scheme settings, with 56.87% F compared to
52.53% F for the Surface and 55.01% F for the
Lemma. These best results for the three lemmatiza-
tion schemes are all acquired with 1g. Again, adding
the morphology-based features helps improve the
classification: The Stem+Morph outperforms Stem
by about 1.00% F. We also found that whereas
adding the DOMAIN feature to both the Lemma and
the Stem+Morph settings improves the classification
slightly, the UNIQUE feature only improves classi-
fication with the Stem+Morph.
Adding the ADJ feature improves performance
significantly: An improvement of 20.88% F for the
Lemma setting and 33.09% F for the Stem+Morph
is achieved. As Table 3 shows, performance on test
data drops with applying all features except ADJ, the
latter helping improve performance by 4.60% F. The
best results we thus acquire on the 80% training data
with 5-fold cross validation is 90.93% F with 1g,
and the best performance of the system on the test
data is 95.52% F also with 1g.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.8999365">
Several sentence- and phrase-level SSA systems
have been built, e.g., (Yi et al. 2003; Hu and Liu.,
2004; Kim and Hovy., 2004; Mullen and Collier
2004; Pang and Lee 2004; Wilson et al. 2005;
Yu and Hatzivassiloglou, 2003). Yi et al. (2003)
present an NLP-based system that detects all ref-
</bodyText>
<page confidence="0.995956">
589
</page>
<table confidence="0.99968675">
DEV Stem+Morph +ADJ +DOMAIN +UNIQUE
73.32 73.30 73.43 72.92
TEST 65.60 71.54 64.67 65.66
Baseline 55.13 55.13 55.13 55.13
</table>
<tableCaption confidence="0.86078">
Table 2: Subjectivity results on Stem+Morph+language independent features
</tableCaption>
<table confidence="0.99994525">
Stem+Morph +ADJ +DOMAIN +UNIQUE
DEV 57.84 90.93 58.03 58.22
TEST 52.12 95.52 53.21 51.92
Baseline 58.38 58.38 58.38 58.38
</table>
<tableCaption confidence="0.999722">
Table 3: Sentiment results on Stem+Morph+language independent features
</tableCaption>
<bodyText confidence="0.997892970588235">
erences to a given subject, and determines senti-
ment in each of the references. Similar to (2003),
Kim &amp; Hovy (2004) present a sentence-level sys-
tem that, given a topic detects sentiment towards it.
Our approach differs from both (2003) and Kim &amp;
Hovy (2004) in that we do not detect sentiment to-
ward specific topics. Also, we make use of N-gram
features beyond unigrams and employ elaborate N-
gram combinations.
Yu &amp; Hatzivassiloglou (2003) build a document-
and sentence-level subjectivity classification system
using various N-gram-based features and a polarity
lexicon. They report about 97% F-measure on docu-
ments and about 91% F-measure on sentences from
the Wall Street Journal (WSJ) corpus. Some of our
features are similar to those used by Yu &amp; Hatzivas-
siloglou, but we exploit additional features. Wiebe
et al. (1999) train a sentence-level probabilistic
classifier on data from the WSJ to identify subjectiv-
ity in these sentences. They use POS features, lex-
ical features, and a paragraph feature and obtain an
average accuracy on subjectivity tagging of 72.17%.
Again, our feature set is richer than Wiebe et al.
(1999).
The only work on Arabic SSA we are aware of
is that of Abbasi et al. (2008). They use an en-
tropy weighted genetic algorithm for both English
and Arabic Web forums at the document level. They
exploit both syntactic and stylistic features. Abbasi
et al. use a root extraction algorithm and do not use
morphological features. They report 93.6% accu-
racy. Their system is not directly comparable to ours
due to the difference in data sets and tagging granu-
larity.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999981">
In this paper, we build a sentence-level SSA sys-
tem for MSA contrasting language independent only
features vs. combining language independent and
language-specific feature sets, namely morpholog-
ical features specific to Arabic. We also investi-
gate the level of stemming required for the task.
We show that the Stem lemmatization setting outper-
forms both Surface and Lemma settings for the SSA
task. We illustrate empirically that adding language
specific features for MRL yields improved perfor-
mance. Similar to previous studies of SSA for other
languages, we show that exploiting a polarity lexi-
con has the largest impact on performance. Finally,
as part of the contribution of this investigation, we
present a novel MSA data set annotated for SSA lay-
ered on top of the PATB data annotations that will
be made available to the community at large, in ad-
dition to a large scale polarity lexicon.
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997128">
A. Abbasi, H. Chen, and A. Salem. 2008. Sentiment
analysis in multiple languages: Feature selection for
opinion classification in web forums. ACM Trans. Inf.
Syst., 26:1–34.
M. Abdul-Mageed. 2008. Online News Sites and
Journalism 2.0: Reader Comments on Al Jazeera
Arabic. tripleC-Cognition, Communication, Co-
operation, 6(2):59.
A. Banfield. 1982. Unspeakable Sentences: Narration
</reference>
<page confidence="0.987062">
590
</page>
<reference confidence="0.9993794375">
and Representation in the Language of Fiction. Rout-
ledge Kegan Paul, Boston.
R. Bruce and J. Wiebe. 1999. Recognizing subjectivity.
a case study of manual tagging. Natural Language
Engineering, 5(2).
T. Joachims. 2008. Svmlight: Support vector ma-
chine. http://svmlight.joachims.org/, Cornell Univer-
sity, 2008.
S. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
pages 1367–1373.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The penn arabic treebank: Building a large-
scale annotated arabic corpus. In NEMLAR Confer-
ence on Arabic Language Resources and Tools, pages
102–109.
R. Tsarfaty, D. Seddah, Y. Goldberg, S. Kuebler, Y. Ver-
sley, M. Candito, J. Foster, I. Rehbein, and L. Tounsi.
2010. Statistical parsing of morphologically rich lan-
guages (spmrl) what, how and whither. In Proceedings
of the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, Los An-
geles, CA.
J. Wiebe, R. Bruce, and T. O’Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99), pages 246–
253, University of Maryland: ACL.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational
linguistics, 30(3):277–308.
J. Wiebe. 1994. Tracking point of view in narrative.
Computional Linguistics, 20(2):233–287.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399–433.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In Proceedings of the 3rd IEEE International
Conference on Data Mining, pages 427–434.
H. Yu and V. Hatzivassiloglou. 2003. The penn arabic
treebank: Building a large-scale annotated arabic cor-
pus. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 129–
136.
</reference>
<page confidence="0.998086">
591
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304238">
<title confidence="0.815094">Subjectivity and Sentiment Analysis of Modern Standard Arabic</title>
<affiliation confidence="0.996728">Columbia University, NYC,</affiliation>
<email confidence="0.999139">mdiab@ccls.columbia.edu</email>
<author confidence="0.98512">Muhammad</author>
<affiliation confidence="0.983846333333333">Department of Linguistics School of Library &amp; Info. Indiana</affiliation>
<address confidence="0.954232">Bloomington, USA,</address>
<author confidence="0.992457">T Mona</author>
<affiliation confidence="0.986529">Center for</affiliation>
<title confidence="0.816035">Learning Systems,</title>
<author confidence="0.841689">Mohammed</author>
<affiliation confidence="0.994039666666667">School of and Indiana</affiliation>
<address confidence="0.566196">Bloomington, USA,</address>
<email confidence="0.999813">mabdulma@indiana.edu</email>
<abstract confidence="0.997777947368421">and Sentiment Analysis been witnessing a flurry of novel research, there are few attempts to build SSA systems for Morphologically-Rich Languages (MRL). In the current study, we report efforts to partially fill this gap. We present a newly developed manually annotated corpus of Modern Standard Arabic (MSA) together with a new polarity lexicon.The corpus is a collection of newswire documents annotated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abbasi</author>
<author>H Chen</author>
<author>A Salem</author>
</authors>
<title>Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums.</title>
<date>2008</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<pages>26--1</pages>
<contexts>
<context position="13885" citStr="Abbasi et al. (2008)" startWordPosition="2219" endWordPosition="2222">bout 97% F-measure on documents and about 91% F-measure on sentences from the Wall Street Journal (WSJ) corpus. Some of our features are similar to those used by Yu &amp; Hatzivassiloglou, but we exploit additional features. Wiebe et al. (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. They use POS features, lexical features, and a paragraph feature and obtain an average accuracy on subjectivity tagging of 72.17%. Again, our feature set is richer than Wiebe et al. (1999). The only work on Arabic SSA we are aware of is that of Abbasi et al. (2008). They use an entropy weighted genetic algorithm for both English and Arabic Web forums at the document level. They exploit both syntactic and stylistic features. Abbasi et al. use a root extraction algorithm and do not use morphological features. They report 93.6% accuracy. Their system is not directly comparable to ours due to the difference in data sets and tagging granularity. 5 Conclusion In this paper, we build a sentence-level SSA system for MSA contrasting language independent only features vs. combining language independent and language-specific feature sets, namely morphological feat</context>
</contexts>
<marker>Abbasi, Chen, Salem, 2008</marker>
<rawString>A. Abbasi, H. Chen, and A. Salem. 2008. Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26:1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Abdul-Mageed</author>
</authors>
<date>2008</date>
<booktitle>Online News Sites and Journalism 2.0: Reader Comments on Al Jazeera Arabic. tripleC-Cognition, Communication, Cooperation,</booktitle>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="4633" citStr="Abdul-Mageed, 2008" startWordPosition="712" endWordPosition="713"> Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics of a sentence is judged as the objective reporting of information, it was labeled as OBJ. Otherwise, a sentence would be a candidate for one of the three SUBJ classes. Inter-annotator agreement reached 88.06%.2 The distribution of classes in our data set was as follows: 1281 OBJ, a total of 1574 SUBJ, where 491 were deemed S-POS, 689 S-NEG, and 394 S-NEUT. Moreover, each of the sentences in our data set is manually labeled by a domain label. The domain labels are from the newswire genre and are adopted from (Abdul-Mageed, 2008). Polarity Lexicon: We manually created a lexicon of 3982 adjectives labeled with one of the following tags {positive, negative, neutral}. The adjectives pertain to the newswire domain. 2.2 Automatic Classification Tokenization scheme and settings: We run experiments on gold-tokenized text from PATB. We adopt the PATB+Al tokenization scheme, where proclitics and enclitics as well as Al are segmented out from the stem words. We experiment with three different pre-processing lemmatization configurations that specifically target the stem words: (1) Surface, where the stem words are left as is wit</context>
</contexts>
<marker>Abdul-Mageed, 2008</marker>
<rawString>M. Abdul-Mageed. 2008. Online News Sites and Journalism 2.0: Reader Comments on Al Jazeera Arabic. tripleC-Cognition, Communication, Cooperation, 6(2):59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Banfield</author>
</authors>
<title>Unspeakable Sentences: Narration and Representation in the Language of Fiction. Routledge Kegan Paul,</title>
<date>1982</date>
<location>Boston.</location>
<contexts>
<context position="1444" citStr="Banfield, 1982" startWordPosition="205" endWordPosition="206">ire documents annotated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance. 1 Introduction Subjectivity and Sentiment Analysis (SSA) is an area that has been witnessing a flurry of novel research. In natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations (Banfield, 1982; Wiebe, 1994) and thus incorporates sentiment. The process of subjectivity classification refers to the task of classifying texts into either objective (e.g., Mubarak stepped down) or subjective (e.g., Mubarak, the hateful dictator, stepped down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) tex</context>
</contexts>
<marker>Banfield, 1982</marker>
<rawString>A. Banfield. 1982. Unspeakable Sentences: Narration and Representation in the Language of Fiction. Routledge Kegan Paul, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing subjectivity. a case study of manual tagging.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>R. Bruce and J. Wiebe. 1999. Recognizing subjectivity. a case study of manual tagging. Natural Language Engineering, 5(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Svmlight: Support vector machine. http://svmlight.joachims.org/,</title>
<date>2008</date>
<institution>Cornell University,</institution>
<contexts>
<context position="7755" citStr="Joachims, 2008" startWordPosition="1207" endWordPosition="1208">phological features of person, state, gender, tense, aspect, and number. We do not use POS information. We assume undiacritized text in our models. 2.3 Method: Two-stage Classification Process In the current study, we adopt a two-stage classification approach. In the first stage (i.e., Subjectivity), we build a binary classifier to sort out OBJ from SUBJ cases. For the second stage (i.e., Sentiment) we apply binary classification that distinguishes SPOS from S-NEG cases. We disregard the neutral class of S-NEUT for this round of experimentation. We use an SVM classifier, the SVMlight package (Joachims, 2008). We experimented with various kernels and parameter settings and found that linear kernels yield the best performance. We ran experiments with presence vectors: In each sentence vector, the value of each dimension is binary either a 1 (regardless of how many times a feature occurs) or 0. Experimental Conditions: We first run experiments using each of the three lemmatization settings Surface, Lemma, Stem using various Ngrams and N-gram combinations and then iteratively add other features. The morphological features (i.e., Morph) are added only to the Stem setting. Language-independent features</context>
</contexts>
<marker>Joachims, 2008</marker>
<rawString>T. Joachims. 2008. Svmlight: Support vector machine. http://svmlight.joachims.org/, Cornell University, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>1367--1373</pages>
<contexts>
<context position="12797" citStr="Kim &amp; Hovy (2004)" startWordPosition="2039" endWordPosition="2042"> al. 2005; Yu and Hatzivassiloglou, 2003). Yi et al. (2003) present an NLP-based system that detects all ref589 DEV Stem+Morph +ADJ +DOMAIN +UNIQUE 73.32 73.30 73.43 72.92 TEST 65.60 71.54 64.67 65.66 Baseline 55.13 55.13 55.13 55.13 Table 2: Subjectivity results on Stem+Morph+language independent features Stem+Morph +ADJ +DOMAIN +UNIQUE DEV 57.84 90.93 58.03 58.22 TEST 52.12 95.52 53.21 51.92 Baseline 58.38 58.38 58.38 58.38 Table 3: Sentiment results on Stem+Morph+language independent features erences to a given subject, and determines sentiment in each of the references. Similar to (2003), Kim &amp; Hovy (2004) present a sentence-level system that, given a topic detects sentiment towards it. Our approach differs from both (2003) and Kim &amp; Hovy (2004) in that we do not detect sentiment toward specific topics. Also, we make use of N-gram features beyond unigrams and employ elaborate Ngram combinations. Yu &amp; Hatzivassiloglou (2003) build a documentand sentence-level subjectivity classification system using various N-gram-based features and a polarity lexicon. They report about 97% F-measure on documents and about 91% F-measure on sentences from the Wall Street Journal (WSJ) corpus. Some of our features</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>S. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics, pages 1367–1373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>T Buckwalter</author>
<author>W Mekki</author>
</authors>
<title>The penn arabic treebank: Building a largescale annotated arabic corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR Conference on Arabic Language Resources and Tools,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="2836" citStr="Maamouri et al., 2004" startWordPosition="423" endWordPosition="426">MRL) such as Arabic, Hebrew, mkorayem@indiana.edu Turkish, Czech, etc. (Tsarfaty et al., 2010). MRL pose significant challenges to NLP systems in general, and the SSA task is expected to be no exception. The problem is even more pronounced in some MRL due to the lack in annotated resources for SSA such as labeled corpora, and polarity lexica. In the current paper, we investigate the task of sentence-level SSA on Modern Standard Arabic (MSA) texts from the newswire genre. We run experiments on three different pre-processing settings based on tokenized text from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and employ both language-independent and Arabicspecific, morphology-based features. Our work shows that explicitly using morphology-based features in our models improves the system’s performance. We also measure the impact of using a wide coverage polarity lexicon and show that using a tailored resource results in significant improvement in classification performance. 2 Approach To our knowledge, no SSA annotated MSA data exists. Hence we decided to create our own SSA annotated data.1 2.1 Data set and Annotation Corpus: Two college-educated native speakers of Arabic annotated 2855 sentences f</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki. 2004. The penn arabic treebank: Building a largescale annotated arabic corpus. In NEMLAR Conference on Arabic Language Resources and Tools, pages 102–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tsarfaty</author>
<author>D Seddah</author>
<author>Y Goldberg</author>
<author>S Kuebler</author>
<author>Y Versley</author>
<author>M Candito</author>
<author>J Foster</author>
<author>I Rehbein</author>
<author>L Tounsi</author>
</authors>
<title>Statistical parsing of morphologically rich languages (spmrl) what, how and whither.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="2308" citStr="Tsarfaty et al., 2010" startWordPosition="332" endWordPosition="335">down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) texts. Most of the SSA literature has focused on English and other Indio-European languages. Very few studies have addressed the problem for morphologically rich languages (MRL) such as Arabic, Hebrew, mkorayem@indiana.edu Turkish, Czech, etc. (Tsarfaty et al., 2010). MRL pose significant challenges to NLP systems in general, and the SSA task is expected to be no exception. The problem is even more pronounced in some MRL due to the lack in annotated resources for SSA such as labeled corpora, and polarity lexica. In the current paper, we investigate the task of sentence-level SSA on Modern Standard Arabic (MSA) texts from the newswire genre. We run experiments on three different pre-processing settings based on tokenized text from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and employ both language-independent and Arabicspecific, morphology-bas</context>
</contexts>
<marker>Tsarfaty, Seddah, Goldberg, Kuebler, Versley, Candito, Foster, Rehbein, Tounsi, 2010</marker>
<rawString>R. Tsarfaty, D. Seddah, Y. Goldberg, S. Kuebler, Y. Versley, M. Candito, J. Foster, I. Rehbein, and L. Tounsi. 2010. Statistical parsing of morphologically rich languages (spmrl) what, how and whither. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>T O’Hara</author>
</authors>
<title>Development and use of a gold standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proc. 37th Annual Meeting of the Assoc. for Computational Linguistics (ACL-99),</booktitle>
<pages>246--253</pages>
<publisher>ACL.</publisher>
<institution>University of Maryland:</institution>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>J. Wiebe, R. Bruce, and T. O’Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. In Proc. 37th Annual Meeting of the Assoc. for Computational Linguistics (ACL-99), pages 246– 253, University of Maryland: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="6314" citStr="Wiebe et al. (2004)" startWordPosition="973" endWordPosition="976">hree configuration schemes, with each underlined. Features: The features we employed are of two main types: Language-independent features and Morphological features. Language-Independent Features: This group of features has been employed in various SSA studies. Domain: Following (Wilson et al., 2009), we apply a feature indicating the domain of the document to which a sentence belongs. As mentioned earlier, each sentence has a document domain label manually associated with it. 2A detailed account of issues related to the annotation task will appear in a separate publication. UNIQUE: Following Wiebe et al. (2004) we apply a unique feature. Namely words that occur in our corpus with an absolute frequency &lt; 5, are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams &lt; 4 and all possible combinations of them. ADJ: For subjectivity classification, we follow Bruce &amp; Wiebe’s (1999) in adding a binary has adjective feature indicating whether or not any of the adjectives in our manually created polarity lexicon exists in a sentence. For sentiment classification, we apply two features, has POS adjective and has NEG adjective, each of these binary features indicate whether a POS or NEG adje</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin. 2004. Learning subjective language. Computational linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computional Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1458" citStr="Wiebe, 1994" startWordPosition="207" endWordPosition="208">notated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance. 1 Introduction Subjectivity and Sentiment Analysis (SSA) is an area that has been witnessing a flurry of novel research. In natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations (Banfield, 1982; Wiebe, 1994) and thus incorporates sentiment. The process of subjectivity classification refers to the task of classifying texts into either objective (e.g., Mubarak stepped down) or subjective (e.g., Mubarak, the hateful dictator, stepped down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) texts. Most of th</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>J. Wiebe. 1994. Tracking point of view in narrative. Computional Linguistics, 20(2):233–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="5996" citStr="Wilson et al., 2009" startWordPosition="920" endWordPosition="923">r lemma citation forms, for instance in case of verbs it is the 3rd person masculine singular perfective form; and (3) Stem, which is the surface form minus inflectional morphemes, it should be noted that this configuration may result in non proper Arabic words (a la IR stemming). Table 1 illustrates examples of the three configuration schemes, with each underlined. Features: The features we employed are of two main types: Language-independent features and Morphological features. Language-Independent Features: This group of features has been employed in various SSA studies. Domain: Following (Wilson et al., 2009), we apply a feature indicating the domain of the document to which a sentence belongs. As mentioned earlier, each sentence has a document domain label manually associated with it. 2A detailed account of issues related to the annotation task will appear in a separate publication. UNIQUE: Following Wiebe et al. (2004) we apply a unique feature. Namely words that occur in our corpus with an absolute frequency &lt; 5, are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams &lt; 4 and all possible combinations of them. ADJ: For subjectivity classification, we follow Bruce &amp; Wiebe’s</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yi</author>
<author>T Nasukawa</author>
<author>R Bunescu</author>
<author>W Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd IEEE International Conference on Data Mining,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="12085" citStr="Yi et al. 2003" startWordPosition="1927" endWordPosition="1930">sification with the Stem+Morph. Adding the ADJ feature improves performance significantly: An improvement of 20.88% F for the Lemma setting and 33.09% F for the Stem+Morph is achieved. As Table 3 shows, performance on test data drops with applying all features except ADJ, the latter helping improve performance by 4.60% F. The best results we thus acquire on the 80% training data with 5-fold cross validation is 90.93% F with 1g, and the best performance of the system on the test data is 95.52% F also with 1g. 4 Related Work Several sentence- and phrase-level SSA systems have been built, e.g., (Yi et al. 2003; Hu and Liu., 2004; Kim and Hovy., 2004; Mullen and Collier 2004; Pang and Lee 2004; Wilson et al. 2005; Yu and Hatzivassiloglou, 2003). Yi et al. (2003) present an NLP-based system that detects all ref589 DEV Stem+Morph +ADJ +DOMAIN +UNIQUE 73.32 73.30 73.43 72.92 TEST 65.60 71.54 64.67 65.66 Baseline 55.13 55.13 55.13 55.13 Table 2: Subjectivity results on Stem+Morph+language independent features Stem+Morph +ADJ +DOMAIN +UNIQUE DEV 57.84 90.93 58.03 58.22 TEST 52.12 95.52 53.21 51.92 Baseline 58.38 58.38 58.38 58.38 Table 3: Sentiment results on Stem+Morph+language independent features eren</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proceedings of the 3rd IEEE International Conference on Data Mining, pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>The penn arabic treebank: Building a large-scale annotated arabic corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="12221" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="1951" endWordPosition="1954">r the Lemma setting and 33.09% F for the Stem+Morph is achieved. As Table 3 shows, performance on test data drops with applying all features except ADJ, the latter helping improve performance by 4.60% F. The best results we thus acquire on the 80% training data with 5-fold cross validation is 90.93% F with 1g, and the best performance of the system on the test data is 95.52% F also with 1g. 4 Related Work Several sentence- and phrase-level SSA systems have been built, e.g., (Yi et al. 2003; Hu and Liu., 2004; Kim and Hovy., 2004; Mullen and Collier 2004; Pang and Lee 2004; Wilson et al. 2005; Yu and Hatzivassiloglou, 2003). Yi et al. (2003) present an NLP-based system that detects all ref589 DEV Stem+Morph +ADJ +DOMAIN +UNIQUE 73.32 73.30 73.43 72.92 TEST 65.60 71.54 64.67 65.66 Baseline 55.13 55.13 55.13 55.13 Table 2: Subjectivity results on Stem+Morph+language independent features Stem+Morph +ADJ +DOMAIN +UNIQUE DEV 57.84 90.93 58.03 58.22 TEST 52.12 95.52 53.21 51.92 Baseline 58.38 58.38 58.38 58.38 Table 3: Sentiment results on Stem+Morph+language independent features erences to a given subject, and determines sentiment in each of the references. Similar to (2003), Kim &amp; Hovy (2004) present a sentence-leve</context>
<context position="13121" citStr="Yu &amp; Hatzivassiloglou (2003)" startWordPosition="2093" endWordPosition="2096">Morph +ADJ +DOMAIN +UNIQUE DEV 57.84 90.93 58.03 58.22 TEST 52.12 95.52 53.21 51.92 Baseline 58.38 58.38 58.38 58.38 Table 3: Sentiment results on Stem+Morph+language independent features erences to a given subject, and determines sentiment in each of the references. Similar to (2003), Kim &amp; Hovy (2004) present a sentence-level system that, given a topic detects sentiment towards it. Our approach differs from both (2003) and Kim &amp; Hovy (2004) in that we do not detect sentiment toward specific topics. Also, we make use of N-gram features beyond unigrams and employ elaborate Ngram combinations. Yu &amp; Hatzivassiloglou (2003) build a documentand sentence-level subjectivity classification system using various N-gram-based features and a polarity lexicon. They report about 97% F-measure on documents and about 91% F-measure on sentences from the Wall Street Journal (WSJ) corpus. Some of our features are similar to those used by Yu &amp; Hatzivassiloglou, but we exploit additional features. Wiebe et al. (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. They use POS features, lexical features, and a paragraph feature and obtain an average accuracy on su</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. The penn arabic treebank: Building a large-scale annotated arabic corpus. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 129– 136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>