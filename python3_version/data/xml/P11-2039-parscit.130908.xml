<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000621">
<title confidence="0.982883">
Query Snowball: A Co-occurrence-based Approach to Multi-document
Summarization for Question Answering
</title>
<author confidence="0.959166">
Hajime Morita&apos; 2 and Tetsuya Sakai&apos; and Manabu Okumura3
</author>
<affiliation confidence="0.946178">
&apos;Microsoft Research Asia, Beijing, China
2Tokyo Institute of Technology, Tokyo, Japan
3Precision and Intelligence Laboratory, Tokyo Institute of Technology, Tokyo, Japan
</affiliation>
<email confidence="0.992897">
morita@lr.pi.titech.ac.jp,tetsuyasakai@acm.org,
oku@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998079357142857">
We propose a new method for query-oriented
extractive multi-document summarization. To
enrich the information need representation of
a given query, we build a co-occurrence graph
to obtain words that augment the original
query terms. We then formulate the sum-
marization problem as a Maximum Coverage
Problem with Knapsack Constraints based on
word pairs rather than single words. Our
experiments with the NTCIR ACLIA ques-
tion answering test collections show that our
method achieves a pyramid F3-score of up to
0.313, a 36% improvement over a baseline us-
ing Maximal Marginal Relevance.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930078431373">
Automatic text summarization aims at reducing the
amount of text the user has to read while preserv-
ing important contents, and has many applications
in this age of digital information overload (Mani,
2001). In particular, query-oriented multi-document
summarization is useful for helping the user satisfy
his information need efficiently by gathering impor-
tant pieces of information from multiple documents.
In this study, we focus on extractive summariza-
tion (Liu and Liu, 2009), in particular, on sentence
selection from a given set of source documents that
contain relevant sentences. One well-known chal-
lenge in selecting sentences relevant to the informa-
tion need is the vocabulary mismatch between the
query (i.e. information need representation) and the
candidate sentences. Hence, to enrich the informa-
tion need representation, we build a co-occurrence
graph to obtain words that augment the original
query terms. We call this method Query Snowball.
Another challenge in sentence selection for
query-oriented multi-document summarization is
how to avoid redundancy so that diverse pieces of
information (i.e. nuggets (Voorhees, 2003)) can be
covered. For penalizing redundancy across sen-
tences, using single words as the basic unit may not
always be appropriate, because different nuggets for
a given information need often have many words
in common. Figure 1 shows an example of this
word overlap problem from the NTCIR-8 ACLIA2
Japanese question answering test collection. Here,
two gold-standard nuggets for the question “Sen to
Chihiro no Kamikakushi (Spirited Away) is a full-
length animated movie from Japan. The user wants
to know how it was received overseas.” (in English
translation) is shown. Each nugget represents a par-
ticular award that the movie received, and the two
Japanese nugget strings have as many as three words
in common: “*ff (review/critic)”, “ア.:!,A (ani-
mation)” and “S (award).” Thus, if we use single
words as the basis for penalising redundancy in sen-
tence selection, it would be difficult to cover both of
these nuggets in the summary because of the word
overlaps.
We therefore use word pairs as the basic unit for
computing sentence scores, and then formulate the
summarization problem as a Maximum Cover Prob-
lem with Knapsack Constraints (MCKP) (Filatova
and Hatzivassiloglou, 2004; Takamura and Oku-
mura, 2009a). This problem is an optimization prob-
lem that maximizes the total score of words covered
by a summary under a summary length limit.
</bodyText>
<page confidence="0.986611">
223
</page>
<note confidence="0.9735475">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.785999176470588">
• Question Bosma, 2009). Unlike existing graph-based meth-
Sen to Chihiro no Kamikakushi (Spirited Away) is a full-length ods, our method explicitly computes indirect rela-
animated movie from Japan. The user wants to know how it tionships between the query and words in the docu-
was received overseas. ments to enrich the information need representation.
• Nugget example 1 To this end, our method utilizes within-sentence co-
全米 映画批評 会議 の アニメ 賞 occurrences of words.
National Board of Review of Motion Pictures Best Animated The approach taken by Jagarlamudi et al. (2005)
Feature is similar to our proposed method in that it uses word
co-occurrence and dependencies within sentences in
order to measure relevance of words to the query.
However, while their approach measures the generic
relevance of each word based on Hyperspace Ana-
logue to Language (Lund and Burgess, 1996) using
an external corpus, our method measures the rele-
vance of each word within the document contexts,
and the query relevance scores are propagated recur-
sively.
</bodyText>
<figure confidence="0.8336074">
3 Proposed Method
• Nugget example 2
ロサンゼルス 批評 家 協会 賞 の アニメ 賞
Los Angeles Film Critics Association Award for Best Ani-
mated Film
</figure>
<figureCaption confidence="0.829064">
Figure 1: Question and gold-standard nuggets example in
NTCIR-8 ACLIA2 dataset
</figureCaption>
<bodyText confidence="0.982826524590164">
We evaluate our proposed method using Japanese
complex question answering test collections from
NTCIR ACLIA–Advanced Cross-lingual Informa-
tion Access task (Mitamura et al., 2008; Mitamura
et al., 2010). However, our method can easily be
extended for handling other languages.
2 Related Work Section 3.1 introduces the Query Snowball (QSB)
method which computes the query relevance score
for each word. Then, Section 3.2 describes how
we formulate the summarization problem based on
word pairs.
3.1 Query Snowball method (QSB)
The basic idea behind QSB is to close the gap
between the query (i.e. information need rep-
resentation) and relevant sentences by enriching
the information need representation based on co-
occurrences. To this end, QSB computes a query
relevance score for each word in the source docu-
ments as described below.
Figure 2 shows the concept of QSB. Here, Q is
the set of query terms (each represented by q), R1
is the set of words (r1) that co-occur with a query
term in the same sentence, and R2 is the set of words
(r2) that co-occur with a word from R1, excluding
those that are already in R1. The imaginary root
node at the center represents the information need,
and we assume that the need is propagated through
this graph, where edges represent within-sentence
co-occurrences. Thus, to compute sentence scores,
we use not only the query terms but also the words
in R1 and R2.
Our first clue for computing a word score is
the query-independent importance of the word.
Much work has been done for generic multi-
document summarization (Takamura and Okumura,
2009a; Takamura and Okumura, 2009b; Celiky-
ilmaz and Hakkani-Tur, 2010; Lin et al., 2010a;
Lin and Bilmes, 2010). Carbonell and Goldstein
(1998) proposed the Maximal Marginal Relevance
(MMR) criteria for non-redundant sentence selec-
tion, which consist of document similarity and re-
dundancy penalty. McDonald (2007) presented
an approximate dynamic programming approach to
maximize the MMR criteria. Yih et al. (2007)
formulated the document summarization problem
as an MCKP, and proposed a supervised method.
Whereas, our method is unsupervised. Filatova
and Hatzivassiloglou (2004) also formulated sum-
marization as an MCKP, and they used two types
of concepts in documents: single words and events
(named entity pairs with a verb or a noun). While
their work was for generic summarization, our
method is designed specifically for query-oriented
summarization.
MMR-based methods are also popular for query-
oriented summarization (Jagarlamudi et al., 2005;
Li et al., 2008; Hasegawa et al., 2010; Lin et al.,
2010b). Moreover, graph-based methods for sum-
marization and sentence retrieval are popular (Otter-
bacher et al., 2005; Varadarajan and Hristidis, 2006;
224
</bodyText>
<figureCaption confidence="0.99738">
Figure 2: Co-occurrence Graph (Query Snowball)
</figureCaption>
<bodyText confidence="0.99785396">
We represent this base word score by sb(w) =
log(N/ctf (w)) or sb(w) = log(N/n(w)), where
ctf (w) is the total number of occurrences of w
within the corpus and n(w) is the document fre-
quency of w, and N is the total number of docu-
ments in the corpus. We will refer to these two ver-
sions as itf and idf, respectively. Our second clue
is the weight propagated from the center of the co-
occurence graph shown in Figure 1. Below, we de-
scribe how to compute the word scores for words in
R1 and then those for words in R2.
As Figure 2 suggests, the query relevance score
for r1 E R1 is computed based not only on its base
word score but also on the relationship between r1
and q E Q. To be more specific, let freq(w, w&apos;)
denote the within-sentence co-occurrence frequency
for words w and w&apos;, and let distance(w, w&apos;) denote
the minimum dependency distance between w and
w&apos;: A dependency distance is the path length be-
tween nodes w and w&apos; within a dependency parse
tree; the minimum dependency distance is the short-
est path length among all dependency parse trees of
source-document sentences in which w and w&apos; co-
occur. Then, the query relevance score for r1 can be
computed as:
</bodyText>
<equation confidence="0.999828">
( sb(q) ) ( freq(q, r1) )
sb(r1) (1)
sumQ distance(q, r1) + 1.0
</equation>
<bodyText confidence="0.999929357142857">
where sumQ = ∑qEQ sb(q). It can be observed that
the query relevance score sr(r1) reflects the base
word scores of both q and r1, as well as the co-
occurrence frequency freq(q, r1). Moreover, sr(r1)
depends on distance(q, r1), the minimum depen-
dency distance between q and r1, which reflects
the strength of relationship between q and r1. This
quantity is used in one of its denominators in Eq.1
as small values of distance(q, r1) imply a strong re-
lationship between q and r1. The 1.0 in the denom-
inator avoids division by zero.
Similarly, the query relevance score for r2 E R2
is computed based on the base word score of r2 and
the relationship between r2 and r1 E R1:
</bodyText>
<equation confidence="0.996215333333333">
sr (r2)
= ∑ ( s,.(r1) ( freq(r1, r2)
r1ERSb(r2) (\sumR1 distance (r1, r2) + 1.0 (2)
</equation>
<bodyText confidence="0.946837">
where sumR1 =∑r1ER1sr(r1).
</bodyText>
<subsectionHeader confidence="0.999344">
3.2 Score Maximization Using Word Pairs
</subsectionHeader>
<bodyText confidence="0.9987577">
Having determined the query relevance score, the
next step is to define the summary score. To this end,
we use word pairs rather than individual words as the
basic unit. This is because word pairs are more in-
formative for discriminating across different pieces
of information than single common words. (Re-
call the example mentioned in Section 1) Thus, the
word pair score is simply defined as: sp(w1, w2) =
sr(w1)sr(w2) and the summary score is computed
as:
</bodyText>
<equation confidence="0.7809735">
fQSBP (S) = ∑ sp(w1, w2) (3)
{w1,w2|w1=,4w2 and w1,w2Eu and uES}
</equation>
<bodyText confidence="0.9999121">
where u is a textual unit, which in our case is a
sentence. Our problem then is to select S to maxi-
mize fQSBP(S). The above function based on word
pairs is still submodular, and therefore we can apply
a greedy approximate algorithm with performance
guarantee as proposed in previous work (Khuller
et al., 1999; Takamura and Okumura, 2009a). Let
l(u) denote the length of u. Given a set of source
documents D and a length limit L for a sum-
mary,
</bodyText>
<listItem confidence="0.9072178">
Require: D, L
1: W = D,S = φ
2: while W =� φ do
3: u = arg maxuEW f (SU{
l(})−f(S)
4: if l(u) + ∑uSES l(uS) G L then
5: S = S U {u}
6: end if
7: W = W/{u}
8: end while
9: umax = arg maxuED f(u)
10: if f(umax) &gt; f(S) then
11: return umax
12: else return S
13: end if
</listItem>
<bodyText confidence="0.997395">
where f(·) is some score function such as fQSBP.
We call our proposed method QSBP: Query Snow-
ball with Word Pairs.
</bodyText>
<figure confidence="0.981530866666667">
r2
r2
r2
r2
r2
r1
r2 r2
r2
r1
q
r1
r2
r1 r1
Q
root
r2
R1
q
q
R2
r1
r2
r2
r1
r2
r2
r2
∑
sr(r1) =
qEQ
</figure>
<page confidence="0.99135">
225
</page>
<sectionHeader confidence="0.991289" genericHeader="introduction">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.813039">
4.1 Experimental Environment
</subsectionHeader>
<table confidence="0.999694333333333">
ACLIA1 ACLIA2
Development Test Test
#of questions 101 100 80*
#of avg. nuggets 5.8 12.8 11.2*
Question types DEFINITION, BIOGRAPHY,
+WHY
RELATIONSHIP, EVENT
Articles years 1998-2001 2002-2005
Documents Mainichi Newspaper
</table>
<tableCaption confidence="0.997498">
*After removing the factoid questions.
Table 1: ACLIA dataset statistics
</tableCaption>
<bodyText confidence="0.999984483333334">
We evaluate our method using Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al., 2008; Mitamura et
al., 2010). The collections contain complex ques-
tions and their answer nuggets with weights. Ta-
ble 1 shows some statistics of the data. We use the
ACLIA1 development data for tuning a parameter
for our baseline as shown in Section 4.2 (whereas
our proposed method is parameter-free), and the
ACLIA1 and ACLIA2 test data for evaluating dif-
ferent methods The results for the ACLIA1 test data
are omitted due to lack of space. As our aim is
to answer complex questions by means of multi-
document summarization, we removed factoid ques-
tions from the ACLIA2 test data.
Although the ACLIA test collections were origi-
nally designed for Japanese QA evaluation, we treat
them as query-oriented summarization test collec-
tions. We use all the candidate documents from
which nuggets were extracted as input to the multi-
document summarizers. That is, in our problem set-
ting, the relevant documents are already given, al-
though the given document sets also occasionally
contain documents that were eventually never used
for nugget extraction (Mitamura et al., 2008; Mita-
mura et al., 2010).
We preprocessed the Japanese documents basi-
cally by automatically detecting sentence bound-
aries based on Japanese punctuation marks, but we
also used regular-expression-based heuristics to de-
tect glossary of terms in articles. As the descrip-
tions of these glossaries are usually very useful for
answering BIOGRAPHY and DEFINITION ques-
tions, we treated each term description (generally
multiple sentences) as a single sentence.
We used Mecab (Kudo et al., 2004) for morpho-
logical analysis, and calculated base word scores
sb(w) using Mainichi articles from 1991 to 2005.
We also used Mecab to convert each word to its base
form and to filter using POS tags to extract content
words. As for dependency parsing for distance com-
putation, we used Cabocha (Kudo and Matsumoto,
2000). We did not use a stop word list or any other
external knowledge.
Following the NTCIR-9 one click access task
setting1, we aimed at generating summaries of
Japanese 500 characters or less. To evaluate the
summaries, we followed the practices at the TAC
summarization tasks (Dang, 2008) and NTCIR
ACLIA tasks, and computed pyramid-based preci-
sion with an allowance parameter of C, recall, Fβ
(where β is 1 or 3) scores. The value of C was
determined based on the average nugget length for
each question type of the ACLIA2 collection (Mita-
mura et al., 2010). Precision and recall are computed
based on the nuggets that the summary covered as
well as their weights. The first author of this paper
manually evaluated whether each nugget matches a
summary. The evaluation metrics are formally de-
fined as follows:
</bodyText>
<equation confidence="0.9173385">
CC · (] of matched nuggets) �
precision = min , 1 ,
summary length
sum of weights over matched nuggets
recall =
sum of weights over all nuggets ,
Fβ = (1 + β2) · precision · recall
β2 · recision + recall .
</equation>
<subsectionHeader confidence="0.821437">
4.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999764875">
MMR is a popular approach in query-oriented sum-
marization. For example, at the TAC 2008 opin-
ion summarization track, a top performer in terms
of pyramid F score used an MMR-based method.
Our own implementation of an MMR-based base-
line uses an existing algorithm to maximize the fol-
lowing summary set score function (Lin and Bilmes,
2010):
</bodyText>
<equation confidence="0.98816425">
(E fMMR(S) = γ Sim(u,vD) + E )Sim(u,vQ)
uES uES
−(1 − γ) E Sim(ui, uj) (4)
l(u;,uj)ji0j and u;,ujES}
</equation>
<bodyText confidence="0.951113333333333">
where vD is the vector representing the source docu-
ments, vQ is the vector representing the query terms,
Sim is the cosine similarity, and γ is a parameter.
</bodyText>
<footnote confidence="0.987143">
1http://research.microsoft.com/en-us/people/tesakai/1click.aspx
</footnote>
<page confidence="0.998702">
226
</page>
<bodyText confidence="0.999959636363636">
Thus, the first term of this function reflects how the
sentences reflect the entire documents; the second
term reflects the relevance of the sentences to the
query; and finally the function penalizes redundant
sentences. We set -y to 0.8 and the scaling factor
used in the algorithm to 0.3 based on a preliminary
experiment with a part of the ACLIA1 development
data. We also tried incorporating sentence position
information (Radev, 2001) to our MMR baseline but
this actually hurt performance in our preliminary ex-
periments.
</bodyText>
<subsectionHeader confidence="0.998488">
4.3 Variants of the Proposed Method
</subsectionHeader>
<bodyText confidence="0.999848909090909">
To clarify the contributions of each components, the
minimum dependency distance, QSB and the word
pair, we also evaluated the following simplified ver-
sions of QSBP. (We use the itf version by default,
and will refer to the idf version as QSBP(idf). ) To
examine the contribution of using minimum depen-
dency distance, We remove distance(w, w0) from
Eq.1 and Eq.2. We call the method QSBP(nodist).
To examine the contribution of using word pairs for
score maximization (see Section 3.2) on the perfor-
mance of QSBP, we replaced Eq.3 with:
</bodyText>
<equation confidence="0.985243">
fQSB(S) = � sr(w) . (5)
{w|w∈ui and ui∈S}
</equation>
<bodyText confidence="0.999895">
To examine the contribution of the QSB relevance
scoring (see Section 3.1) on the performance of
QSBP, we replaced Eq.3 with:
</bodyText>
<equation confidence="0.7252355">
fWP(S) = � sb(w1)sb(w2) . (6)
{w1,w2|w16�w2 and w1,w2∈ui and ui∈S}
</equation>
<bodyText confidence="0.999635">
We will refer to this as WP. Note that this relies only
on base word scores and is query-independent.
</bodyText>
<subsectionHeader confidence="0.872462">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999798518518519">
Tables 2 and 3 summarize our results. We used
the two-tailed sign test for testing statistical signif-
icance. Significant improvements over the MMR
baseline are marked with a t (α=0.05) or a t
(α=0.01); those over QSBP(nodist) are marked with
a � (α=0.05) or a (α=0.01); and those over QSB
are marked with a]• (α=0.05) or a •• (α=0.01); and
those over WP are marked with a (α=0.05) or a
?? (α=0.01). From Table 2, it can be observed that
both QSBP and QSBP(idf) significantly outperforms
QSBP(nodist), QSB, WP and the baseline in terms
of all evaluation metrics. Thus, the minimum depen-
dency distance, Query Snowball and the use of word
pairs all contribute significantly to the performance
of QSBP. Note that we are using the ACLIA data as
summarization test collections and that the official
QA results of ACLIA should not be compared with
ours.
QSBP and QSBP(idf) achieve 0.312 and 0.313 in
F3 score, and the differences between the two are
not statistically significant. Table 3 shows the F3
scores for each question type. It can be observed
that QSBP is the top performer for BIO, DEF and
REL questions on average, while QSBP(idf) is the
top performer for EVENT and WHY questions on
average. It is possible that different word scoring
methods work well for different question types.
</bodyText>
<table confidence="0.999594">
Method Precision Recall F1 score F3 score
Baseline 0.076? ? ? ? ] ] 0.370? ? ] ] 0.116? ? ? ? ] ] ?
QSBP 0.107‡• • ] ] 0.482‡• • ? ? ? ? ] ] 0.161‡• • ? ? ] ] 0.231?
QSBP(idf) 0.106‡ • • ? ? 0.485‡• • 0.161‡• • 0.312‡• • ? ? ]
QSBP(nodist) 0.083‡?? 0.396?? 0.125? ? ]
QSB 0.086‡? ? 0.400? ? 0.129‡? ? 0.313‡•• ?? ]]
WP 0.053 0.222 0.080 ?
0.248?
?
0.253† ?
0.152
</table>
<tableCaption confidence="0.991298">
Table 2: ACLIA2 test data results
</tableCaption>
<table confidence="0.999775555555556">
Type BIO DEF REL EVENT WHY
Baseline 0.207? 0.251?? 0.270 0.212 0.213
QSBP 0.315•? 0.329‡?? 0.401† 0.258† ?? ]] 0.275?]
QSBP(idf) 0.304•?] 0.328† ? ? 0.397† 0.268† ? ? ?
QSBP(nodist) 0.255 0.281? ? 0.329 0.196 0.280?
QSB 0.245?? 0.273?? 0.324 0.217 ?
WP 0.109 0.037 0.235 0.141 0.212?
0.215
0.161
</table>
<tableCaption confidence="0.992018">
Table 3: F3-scores for each question type (ACLIA2 test)
</tableCaption>
<sectionHeader confidence="0.981924" genericHeader="conclusions">
5 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.9998623125">
We proposed the Query Snowball (QSB) method for
query-oriented multi-document summarization. To
enrich the information need representation of a given
query, QSB obtains words that augment the original
query terms from a co-occurrence graph. We then
formulated the summarization problem as an MCKP
based on word pairs rather than single words. Our
method, QSBP, achieves a pyramid F3-score of up
to 0.313 with the ACLIA2 Japanese test collection,
a 36% improvement over a baseline using Maximal
Marginal Relevance.
Moreover, as the principles of QSBP are basically
language independent, we will investigate the effec-
tiveness of QSBP in other languages. Also, we plan
to extend our approach to abstractive summariza-
tion.
</bodyText>
<page confidence="0.995583">
227
</page>
<sectionHeader confidence="0.990174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997095009523809">
Wauter Bosma. 2009. Contextual salience in query-
based summarization. In Proceedings of the Interna-
tional Conference RANLP-2009, pages 39–44. Asso-
ciation for Computational Linguistics.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’98, pages 335–336. Association for Comput-
ing Machinery.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
’10, pages 815–824. Association for Computational
Linguistics.
Hoa Trang Dang. 2008. Overview of the tac 2008 opin-
ion question answering and summarization tasks. In
Proceedings of Text Analysis Conference.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ’04. Association for Computational Linguis-
tics.
Takaaki Hasegawa, Hitoshi Nishikawa, Kenji Imamura,
Genichiro Kikui, and Manabu Okumura. 2010. A
Web Page Summarization for Mobile Phones. Trans-
actions of the Japanese Society for Artificial Intelli-
gence, 25:133–143.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva
Varma. 2005. A relevance-based language modeling
approach to duc 2005. In Proceedings of Document
Understanding Conferences (along with HLT-EMNLP
2005).
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Informa-
tion Processing Letters, 70(1):39–45.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proceedings of the 2000 Joint SIGDAT
conference on Empirical methods in natural language
processing and very large corpora: held in conjunc-
tion with the 38th Annual Meeting of the Association
for Computational Linguistics, volume 13, pages 18–
25. Association for Computational Linguistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Confer-
ence on Emprical Methods in Natural Language Pro-
cessing (EMNLP 2004), volume 2004, pages 230–237.
Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008.
PolyU at TAC 2008. In Proceedings of Text Analysis
Conference.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ’10, pages 912–920. Association for Computa-
tional Linguistics.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2010a. Graph-
based submodular selection for extractive summariza-
tion. In Automatic Speech Recognition &amp; Understand-
ing, 2009. ASRU2009. IEEE Workshop on, pages 381–
386. IEEE.
Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr. 2010b.
Putting the user in the loop: interactive maximal
marginal relevance for query-focused summarization.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ’10,
pages 305–308. Association for Computational Lin-
guistics.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ’09, pages
261–264. Association for Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203–208.
Inderjeet Mani. 2001. Automatic summarization. John
Benjamins Publishing Co.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceed-
ings of the 29th European conference on IR research,
ECIR’07, pages 557–564. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki
Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song,
Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and
Noriko Kando. 2008. Overview of the NTCIR-7
ACLIA tasks: Advanced cross-lingual information ac-
cess. In Proceedings of the 7th NTCIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko
Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin,
Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee.
2010. Overview of the NTCIR-8 ACLIA tasks: Ad-
vanced cross-lingual information access. In Proceed-
ings of the 8th NTCIR Workshop.
Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sen-
tence retrieval. In Proceedings of the conference on
Human Language Technology and Empirical Methods
</reference>
<page confidence="0.974833">
228
</page>
<reference confidence="0.999732">
in Natural Language Processing, HLT ’05, pages 915–
922. Association for Computational Linguistics.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In First
Document Understanding Conference.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of the 12th
Conference of the European Chapter of the ACL
(EACL 2009), pages 781–789. Association for Com-
putational Linguistics.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceeding of the 18th ACM conference
on Information and knowledge management, CIKM
’09, pages 1589–1592. Association for Computing
Machinery.
Ramakrishna Varadarajan and Vagelis Hristidis. 2006.
A system for query-specific document summarization.
In Proceedings of the 15th ACM international con-
ference on Information and knowledge management,
CIKM ’06, pages 622–631. ACM.
Ellen M. Voorhees. 2003. Overview of the TREC
2003 Question Answering Track. In Proceedings of
the Twelfth Text REtrieval Conference (TREC 2003),
pages 54–68.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and
Hisami Suzuki. 2007. Multi-document summariza-
tion by maximizing informative content-words. In
Proceedings of the 20th international joint conference
on Artifical intelligence, pages 1776–1782. Morgan
Kaufmann Publishers Inc.
</reference>
<page confidence="0.998915">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.140300">
<title confidence="0.9670105">Query Snowball: A Co-occurrence-based Approach to Summarization for Question Answering</title>
<affiliation confidence="0.942484333333333">Research Asia, Beijing, Institute of Technology, Tokyo, and Intelligence Laboratory, Tokyo Institute of Technology, Tokyo,</affiliation>
<email confidence="0.988249">oku@pi.titech.ac.jp</email>
<abstract confidence="0.964090533333333">We propose a new method for query-oriented extractive multi-document summarization. To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. We then formulate the summarization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline using Maximal Marginal Relevance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wauter Bosma</author>
</authors>
<title>Contextual salience in querybased summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>39--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3718" citStr="Bosma, 2009" startWordPosition="551" endWordPosition="552">s. We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a). This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit. 223 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics • Question Bosma, 2009). Unlike existing graph-based methSen to Chihiro no Kamikakushi (Spirited Away) is a full-length ods, our method explicitly computes indirect relaanimated movie from Japan. The user wants to know how it tionships between the query and words in the docuwas received overseas. ments to enrich the information need representation. • Nugget example 1 To this end, our method utilizes within-sentence co全米 映画批評 会議 の アニメ 賞 occurrences of words. National Board of Review of Motion Pictures Best Animated The approach taken by Jagarlamudi et al. (2005) Feature is similar to our proposed method in that it us</context>
</contexts>
<marker>Bosma, 2009</marker>
<rawString>Wauter Bosma. 2009. Contextual salience in querybased summarization. In Proceedings of the International Conference RANLP-2009, pages 39–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’98,</booktitle>
<pages>335--336</pages>
<publisher>Association for Computing Machinery.</publisher>
<contexts>
<context position="6659" citStr="Carbonell and Goldstein (1998)" startWordPosition="1034" endWordPosition="1037">hat are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). W</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’98, pages 335–336. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>A hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>815--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6585" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="1021" endWordPosition="1025">is the set of words (r2) that co-occur with a word from R1, excluding those that are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in docume</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2010</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hybrid hierarchical model for multi-document summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 815–824. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of the tac</title>
<date>2008</date>
<booktitle>In Proceedings of Text Analysis Conference.</booktitle>
<contexts>
<context position="13835" citStr="Dang, 2008" startWordPosition="2286" endWordPosition="2287">l., 2004) for morphological analysis, and calculated base word scores sb(w) using Mainichi articles from 1991 to 2005. We also used Mecab to convert each word to its base form and to filter using POS tags to extract content words. As for dependency parsing for distance computation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge. Following the NTCIR-9 one click access task setting1, we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with an allowance parameter of C, recall, Fβ (where β is 1 or 3) scores. The value of C was determined based on the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed based on the nuggets that the summary covered as well as their weights. The first author of this paper manually evaluated whether each nugget matches a summary. The evaluation metrics are formally defined as follows: CC · (] of matched nuggets) � precision = min , 1 , summary length sum of wei</context>
</contexts>
<marker>Dang, 2008</marker>
<rawString>Hoa Trang Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks. In Proceedings of Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3327" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="492" endWordPosition="495">h translation) is shown. Each nugget represents a particular award that the movie received, and the two Japanese nugget strings have as many as three words in common: “*ff (review/critic)”, “ア.:!,A (animation)” and “S (award).” Thus, if we use single words as the basis for penalising redundancy in sentence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps. We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a). This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit. 223 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics • Question Bosma, 2009). Unlike existing graph-based methSen to Chihiro no Kamikakushi (Spirited Away) is a full-length ods, our method explicitly computes indirect relaanimated movie from Japan. The user wants to know how it tionsh</context>
<context position="7097" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="1094" endWordPosition="1097">ric multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Figure 2: Co-occurre</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Hitoshi Nishikawa</author>
<author>Kenji Imamura</author>
<author>Genichiro Kikui</author>
<author>Manabu Okumura</author>
</authors>
<title>A Web Page Summarization for Mobile Phones.</title>
<date>2010</date>
<journal>Transactions of the Japanese Society for Artificial Intelligence,</journal>
<pages>25--133</pages>
<contexts>
<context position="7508" citStr="Hasegawa et al., 2010" startWordPosition="1159" endWordPosition="1162">maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Figure 2: Co-occurrence Graph (Query Snowball) We represent this base word score by sb(w) = log(N/ctf (w)) or sb(w) = log(N/n(w)), where ctf (w) is the total number of occurrences of w within the corpus and n(w) is the document frequency of w, and N is the total number of documents in the corpus. We will refer to these two versions as itf and idf, respectively. Our second clue is the weight propagated from the center of the coo</context>
</contexts>
<marker>Hasegawa, Nishikawa, Imamura, Kikui, Okumura, 2010</marker>
<rawString>Takaaki Hasegawa, Hitoshi Nishikawa, Kenji Imamura, Genichiro Kikui, and Manabu Okumura. 2010. A Web Page Summarization for Mobile Phones. Transactions of the Japanese Society for Artificial Intelligence, 25:133–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Prasad Pingali</author>
<author>Vasudeva Varma</author>
</authors>
<title>A relevance-based language modeling approach to duc</title>
<date>2005</date>
<booktitle>In Proceedings of Document Understanding Conferences (along with HLT-EMNLP</booktitle>
<contexts>
<context position="4262" citStr="Jagarlamudi et al. (2005)" startWordPosition="638" endWordPosition="641">4, 2011. c�2011 Association for Computational Linguistics • Question Bosma, 2009). Unlike existing graph-based methSen to Chihiro no Kamikakushi (Spirited Away) is a full-length ods, our method explicitly computes indirect relaanimated movie from Japan. The user wants to know how it tionships between the query and words in the docuwas received overseas. ments to enrich the information need representation. • Nugget example 1 To this end, our method utilizes within-sentence co全米 映画批評 会議 の アニメ 賞 occurrences of words. National Board of Review of Motion Pictures Best Animated The approach taken by Jagarlamudi et al. (2005) Feature is similar to our proposed method in that it uses word co-occurrence and dependencies within sentences in order to measure relevance of words to the query. However, while their approach measures the generic relevance of each word based on Hyperspace Analogue to Language (Lund and Burgess, 1996) using an external corpus, our method measures the relevance of each word within the document contexts, and the query relevance scores are propagated recursively. 3 Proposed Method • Nugget example 2 ロサンゼルス 批評 家 協会 賞 の アニメ 賞 Los Angeles Film Critics Association Award for Best Animated Film Figur</context>
<context position="7468" citStr="Jagarlamudi et al., 2005" startWordPosition="1151" endWordPosition="1154">pproximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Figure 2: Co-occurrence Graph (Query Snowball) We represent this base word score by sb(w) = log(N/ctf (w)) or sb(w) = log(N/n(w)), where ctf (w) is the total number of occurrences of w within the corpus and n(w) is the document frequency of w, and N is the total number of documents in the corpus. We will refer to these two versions as itf and idf, respectively. Our second clue is the weig</context>
</contexts>
<marker>Jagarlamudi, Pingali, Varma, 2005</marker>
<rawString>Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma. 2005. A relevance-based language modeling approach to duc 2005. In Proceedings of Document Understanding Conferences (along with HLT-EMNLP 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samir Khuller</author>
<author>Anna Moss</author>
<author>Joseph S Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<journal>Information Processing Letters,</journal>
<volume>70</volume>
<issue>1</issue>
<contexts>
<context position="10610" citStr="Khuller et al., 1999" startWordPosition="1718" endWordPosition="1721">ative for discriminating across different pieces of information than single common words. (Recall the example mentioned in Section 1) Thus, the word pair score is simply defined as: sp(w1, w2) = sr(w1)sr(w2) and the summary score is computed as: fQSBP (S) = ∑ sp(w1, w2) (3) {w1,w2|w1=,4w2 and w1,w2Eu and uES} where u is a textual unit, which in our case is a sentence. Our problem then is to select S to maximize fQSBP(S). The above function based on word pairs is still submodular, and therefore we can apply a greedy approximate algorithm with performance guarantee as proposed in previous work (Khuller et al., 1999; Takamura and Okumura, 2009a). Let l(u) denote the length of u. Given a set of source documents D and a length limit L for a summary, Require: D, L 1: W = D,S = φ 2: while W =� φ do 3: u = arg maxuEW f (SU{ l(})−f(S) 4: if l(u) + ∑uSES l(uS) G L then 5: S = S U {u} 6: end if 7: W = W/{u} 8: end while 9: umax = arg maxuED f(u) 10: if f(umax) &gt; f(S) then 11: return umax 12: else return S 13: end if where f(·) is some score function such as fQSBP. We call our proposed method QSBP: Query Snowball with Word Pairs. r2 r2 r2 r2 r2 r1 r2 r2 r2 r1 q r1 r2 r1 r1 Q root r2 R1 q q R2 r1 r2 r2 r1 r2 r2 r2</context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>Samir Khuller, Anna Moss, and Joseph S. Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39–45.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>13</volume>
<pages>pages</pages>
<contexts>
<context position="13549" citStr="Kudo and Matsumoto, 2000" startWordPosition="2237" endWordPosition="2240">egular-expression-based heuristics to detect glossary of terms in articles. As the descriptions of these glossaries are usually very useful for answering BIOGRAPHY and DEFINITION questions, we treated each term description (generally multiple sentences) as a single sentence. We used Mecab (Kudo et al., 2004) for morphological analysis, and calculated base word scores sb(w) using Mainichi articles from 1991 to 2005. We also used Mecab to convert each word to its base form and to filter using POS tags to extract content words. As for dependency parsing for distance computation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge. Following the NTCIR-9 one click access task setting1, we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with an allowance parameter of C, recall, Fβ (where β is 1 or 3) scores. The value of C was determined based on the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed based on</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Japanese dependency structure analysis based on support vector machines. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, volume 13, pages 18– 25. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Emprical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<volume>volume</volume>
<pages>230--237</pages>
<contexts>
<context position="13233" citStr="Kudo et al., 2004" startWordPosition="2183" endWordPosition="2186">given document sets also occasionally contain documents that were eventually never used for nugget extraction (Mitamura et al., 2008; Mitamura et al., 2010). We preprocessed the Japanese documents basically by automatically detecting sentence boundaries based on Japanese punctuation marks, but we also used regular-expression-based heuristics to detect glossary of terms in articles. As the descriptions of these glossaries are usually very useful for answering BIOGRAPHY and DEFINITION questions, we treated each term description (generally multiple sentences) as a single sentence. We used Mecab (Kudo et al., 2004) for morphological analysis, and calculated base word scores sb(w) using Mainichi articles from 1991 to 2005. We also used Mecab to convert each word to its base form and to filter using POS tags to extract content words. As for dependency parsing for distance computation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge. Following the NTCIR-9 one click access task setting1, we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 200</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Emprical Methods in Natural Language Processing (EMNLP 2004), volume 2004, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjie Li</author>
<author>You Ouyang</author>
<author>Yi Hu</author>
<author>Furu Wei</author>
</authors>
<date>2008</date>
<journal>PolyU at TAC</journal>
<booktitle>In Proceedings of Text Analysis Conference.</booktitle>
<contexts>
<context position="7485" citStr="Li et al., 2008" startWordPosition="1155" endWordPosition="1158">ming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Figure 2: Co-occurrence Graph (Query Snowball) We represent this base word score by sb(w) = log(N/ctf (w)) or sb(w) = log(N/n(w)), where ctf (w) is the total number of occurrences of w within the corpus and n(w) is the document frequency of w, and N is the total number of documents in the corpus. We will refer to these two versions as itf and idf, respectively. Our second clue is the weight propagated fro</context>
</contexts>
<marker>Li, Ouyang, Hu, Wei, 2008</marker>
<rawString>Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008. PolyU at TAC 2008. In Proceedings of Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>912--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6627" citStr="Lin and Bilmes, 2010" startWordPosition="1030" endWordPosition="1033">m R1, excluding those that are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity</context>
<context position="14914" citStr="Lin and Bilmes, 2010" startWordPosition="2478" endWordPosition="2481">summary. The evaluation metrics are formally defined as follows: CC · (] of matched nuggets) � precision = min , 1 , summary length sum of weights over matched nuggets recall = sum of weights over all nuggets , Fβ = (1 + β2) · precision · recall β2 · recision + recall . 4.2 Baseline MMR is a popular approach in query-oriented summarization. For example, at the TAC 2008 opinion summarization track, a top performer in terms of pyramid F score used an MMR-based method. Our own implementation of an MMR-based baseline uses an existing algorithm to maximize the following summary set score function (Lin and Bilmes, 2010): (E fMMR(S) = γ Sim(u,vD) + E )Sim(u,vQ) uES uES −(1 − γ) E Sim(ui, uj) (4) l(u;,uj)ji0j and u;,ujES} where vD is the vector representing the source documents, vQ is the vector representing the query terms, Sim is the cosine similarity, and γ is a parameter. 1http://research.microsoft.com/en-us/people/tesakai/1click.aspx 226 Thus, the first term of this function reflects how the sentences reflect the entire documents; the second term reflects the relevance of the sentences to the query; and finally the function penalizes redundant sentences. We set -y to 0.8 and the scaling factor used in the</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 912–920. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
<author>Shasha Xie</author>
</authors>
<title>Graphbased submodular selection for extractive summarization.</title>
<date>2010</date>
<booktitle>In Automatic Speech Recognition &amp; Understanding, 2009. ASRU2009. IEEE Workshop on,</booktitle>
<pages>381--386</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6603" citStr="Lin et al., 2010" startWordPosition="1026" endWordPosition="1029">cur with a word from R1, excluding those that are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words </context>
</contexts>
<marker>Lin, Bilmes, Xie, 2010</marker>
<rawString>Hui Lin, Jeff Bilmes, and Shasha Xie. 2010a. Graphbased submodular selection for extractive summarization. In Automatic Speech Recognition &amp; Understanding, 2009. ASRU2009. IEEE Workshop on, pages 381– 386. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Putting the user in the loop: interactive maximal marginal relevance for query-focused summarization.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>305--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6603" citStr="Lin et al., 2010" startWordPosition="1026" endWordPosition="1029">cur with a word from R1, excluding those that are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words </context>
</contexts>
<marker>Lin, Madnani, Dorr, 2010</marker>
<rawString>Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr. 2010b. Putting the user in the loop: interactive maximal marginal relevance for query-focused summarization. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 305–308. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>From extractive to abstractive meeting summaries: can it be done by sentence compression?</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>261--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1485" citStr="Liu and Liu, 2009" startWordPosition="205" endWordPosition="208"> our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline using Maximal Marginal Relevance. 1 Introduction Automatic text summarization aims at reducing the amount of text the user has to read while preserving important contents, and has many applications in this age of digital information overload (Mani, 2001). In particular, query-oriented multi-document summarization is useful for helping the user satisfy his information need efficiently by gathering important pieces of information from multiple documents. In this study, we focus on extractive summarization (Liu and Liu, 2009), in particular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the information need representation, we build a co-occurrence graph to obtain words that augment the original query terms. We call this method Query Snowball. Another challenge in sentence selection for query-oriented multi-document summarization is how to avoid redundancy so tha</context>
</contexts>
<marker>Liu, Liu, 2009</marker>
<rawString>Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: can it be done by sentence compression? In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 261–264. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods,</title>
<date>1996</date>
<pages>28--203</pages>
<contexts>
<context position="4566" citStr="Lund and Burgess, 1996" startWordPosition="687" endWordPosition="690">query and words in the docuwas received overseas. ments to enrich the information need representation. • Nugget example 1 To this end, our method utilizes within-sentence co全米 映画批評 会議 の アニメ 賞 occurrences of words. National Board of Review of Motion Pictures Best Animated The approach taken by Jagarlamudi et al. (2005) Feature is similar to our proposed method in that it uses word co-occurrence and dependencies within sentences in order to measure relevance of words to the query. However, while their approach measures the generic relevance of each word based on Hyperspace Analogue to Language (Lund and Burgess, 1996) using an external corpus, our method measures the relevance of each word within the document contexts, and the query relevance scores are propagated recursively. 3 Proposed Method • Nugget example 2 ロサンゼルス 批評 家 協会 賞 の アニメ 賞 Los Angeles Film Critics Association Award for Best Animated Film Figure 1: Question and gold-standard nuggets example in NTCIR-8 ACLIA2 dataset We evaluate our proposed method using Japanese complex question answering test collections from NTCIR ACLIA–Advanced Cross-lingual Information Access task (Mitamura et al., 2008; Mitamura et al., 2010). However, our method can eas</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Co.</publisher>
<contexts>
<context position="1211" citStr="Mani, 2001" startWordPosition="168" endWordPosition="169">that augment the original query terms. We then formulate the summarization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline using Maximal Marginal Relevance. 1 Introduction Automatic text summarization aims at reducing the amount of text the user has to read while preserving important contents, and has many applications in this age of digital information overload (Mani, 2001). In particular, query-oriented multi-document summarization is useful for helping the user satisfy his information need efficiently by gathering important pieces of information from multiple documents. In this study, we focus on extractive summarization (Liu and Liu, 2009), in particular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the in</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic summarization. John Benjamins Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th European conference on IR research, ECIR’07,</booktitle>
<pages>557--564</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="6829" citStr="McDonald (2007)" startWordPosition="1059" endWordPosition="1060">sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented </context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European conference on IR research, ECIR’07, pages 557–564. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
</authors>
<title>Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and Noriko Kando.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop.</booktitle>
<marker>Mitamura, 2008</marker>
<rawString>Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and Noriko Kando. 2008. Overview of the NTCIR-7 ACLIA tasks: Advanced cross-lingual information access. In Proceedings of the 7th NTCIR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
<author>Hideki Shima</author>
<author>Tetsuya Sakai</author>
<author>Noriko Kando</author>
<author>Tatsunori Mori</author>
<author>Koichi Takeda</author>
<author>Chin-Yew Lin</author>
<author>Ruihua Song</author>
<author>Chuan-Jie Lin</author>
<author>Cheng-Wei Lee</author>
</authors>
<title>Overview of the NTCIR-8 ACLIA tasks: Advanced cross-lingual information access.</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th NTCIR Workshop.</booktitle>
<contexts>
<context position="5137" citStr="Mitamura et al., 2010" startWordPosition="778" endWordPosition="781">space Analogue to Language (Lund and Burgess, 1996) using an external corpus, our method measures the relevance of each word within the document contexts, and the query relevance scores are propagated recursively. 3 Proposed Method • Nugget example 2 ロサンゼルス 批評 家 協会 賞 の アニメ 賞 Los Angeles Film Critics Association Award for Best Animated Film Figure 1: Question and gold-standard nuggets example in NTCIR-8 ACLIA2 dataset We evaluate our proposed method using Japanese complex question answering test collections from NTCIR ACLIA–Advanced Cross-lingual Information Access task (Mitamura et al., 2008; Mitamura et al., 2010). However, our method can easily be extended for handling other languages. 2 Related Work Section 3.1 introduces the Query Snowball (QSB) method which computes the query relevance score for each word. Then, Section 3.2 describes how we formulate the summarization problem based on word pairs. 3.1 Query Snowball method (QSB) The basic idea behind QSB is to close the gap between the query (i.e. information need representation) and relevant sentences by enriching the information need representation based on cooccurrences. To this end, QSB computes a query relevance score for each word in the sourc</context>
<context position="11709" citStr="Mitamura et al., 2010" startWordPosition="1939" endWordPosition="1942"> QSBP: Query Snowball with Word Pairs. r2 r2 r2 r2 r2 r1 r2 r2 r2 r1 q r1 r2 r1 r1 Q root r2 R1 q q R2 r1 r2 r2 r1 r2 r2 r2 ∑ sr(r1) = qEQ 225 4 Experiments 4.1 Experimental Environment ACLIA1 ACLIA2 Development Test Test #of questions 101 100 80* #of avg. nuggets 5.8 12.8 11.2* Question types DEFINITION, BIOGRAPHY, +WHY RELATIONSHIP, EVENT Articles years 1998-2001 2002-2005 Documents Mainichi Newspaper *After removing the factoid questions. Table 1: ACLIA dataset statistics We evaluate our method using Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain complex questions and their answer nuggets with weights. Table 1 shows some statistics of the data. We use the ACLIA1 development data for tuning a parameter for our baseline as shown in Section 4.2 (whereas our proposed method is parameter-free), and the ACLIA1 and ACLIA2 test data for evaluating different methods The results for the ACLIA1 test data are omitted due to lack of space. As our aim is to answer complex questions by means of multidocument summarization, we removed factoid questions from the ACLIA2 test data. Although the ACLIA test collections were origin</context>
<context position="14105" citStr="Mitamura et al., 2010" startWordPosition="2332" endWordPosition="2336">r distance computation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge. Following the NTCIR-9 one click access task setting1, we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with an allowance parameter of C, recall, Fβ (where β is 1 or 3) scores. The value of C was determined based on the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed based on the nuggets that the summary covered as well as their weights. The first author of this paper manually evaluated whether each nugget matches a summary. The evaluation metrics are formally defined as follows: CC · (] of matched nuggets) � precision = min , 1 , summary length sum of weights over matched nuggets recall = sum of weights over all nuggets , Fβ = (1 + β2) · precision · recall β2 · recision + recall . 4.2 Baseline MMR is a popular approach in query-oriented summarization. For example, at the TAC 2008 opinion summarization track, a top perfo</context>
</contexts>
<marker>Mitamura, Shima, Sakai, Kando, Mori, Takeda, Lin, Song, Lin, Lee, 2010</marker>
<rawString>Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee. 2010. Overview of the NTCIR-8 ACLIA tasks: Advanced cross-lingual information access. In Proceedings of the 8th NTCIR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Using random walks for question-focused sentence retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods</booktitle>
<contexts>
<context position="7638" citStr="Otterbacher et al., 2005" startWordPosition="1178" endWordPosition="1182">d method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Figure 2: Co-occurrence Graph (Query Snowball) We represent this base word score by sb(w) = log(N/ctf (w)) or sb(w) = log(N/n(w)), where ctf (w) is the total number of occurrences of w within the corpus and n(w) is the document frequency of w, and N is the total number of documents in the corpus. We will refer to these two versions as itf and idf, respectively. Our second clue is the weight propagated from the center of the cooccurence graph shown in Figure 1. Below, we describe how to compute the word scores for words in R1 and then those for words in R2</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R. Radev. 2005. Using random walks for question-focused sentence retrieval. In Proceedings of the conference on Human Language Technology and Empirical Methods</rawString>
</citation>
<citation valid="false">
<booktitle>in Natural Language Processing, HLT ’05,</booktitle>
<pages>915--922</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker></marker>
<rawString>in Natural Language Processing, HLT ’05, pages 915– 922. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Experiments in single and multidocument summarization using mead.</title>
<date>2001</date>
<booktitle>In First Document Understanding Conference.</booktitle>
<contexts>
<context position="15681" citStr="Radev, 2001" startWordPosition="2601" endWordPosition="2602">ents, vQ is the vector representing the query terms, Sim is the cosine similarity, and γ is a parameter. 1http://research.microsoft.com/en-us/people/tesakai/1click.aspx 226 Thus, the first term of this function reflects how the sentences reflect the entire documents; the second term reflects the relevance of the sentences to the query; and finally the function penalizes redundant sentences. We set -y to 0.8 and the scaling factor used in the algorithm to 0.3 based on a preliminary experiment with a part of the ACLIA1 development data. We also tried incorporating sentence position information (Radev, 2001) to our MMR baseline but this actually hurt performance in our preliminary experiments. 4.3 Variants of the Proposed Method To clarify the contributions of each components, the minimum dependency distance, QSB and the word pair, we also evaluated the following simplified versions of QSBP. (We use the itf version by default, and will refer to the idf version as QSBP(idf). ) To examine the contribution of using minimum dependency distance, We remove distance(w, w0) from Eq.1 and Eq.2. We call the method QSBP(nodist). To examine the contribution of using word pairs for score maximization (see Sec</context>
</contexts>
<marker>Radev, 2001</marker>
<rawString>Dragomir R. Radev. 2001. Experiments in single and multidocument summarization using mead. In First Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>781--789</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3355" citStr="Takamura and Okumura, 2009" startWordPosition="496" endWordPosition="500">represents a particular award that the movie received, and the two Japanese nugget strings have as many as three words in common: “*ff (review/critic)”, “ア.:!,A (animation)” and “S (award).” Thus, if we use single words as the basis for penalising redundancy in sentence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps. We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a). This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit. 223 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics • Question Bosma, 2009). Unlike existing graph-based methSen to Chihiro no Kamikakushi (Spirited Away) is a full-length ods, our method explicitly computes indirect relaanimated movie from Japan. The user wants to know how it tionships between the query and wo</context>
<context position="6520" citStr="Takamura and Okumura, 2009" startWordPosition="1013" endWordPosition="1016">t co-occur with a query term in the same sentence, and R2 is the set of words (r2) that co-occur with a word from R1, excluding those that are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summar</context>
<context position="10638" citStr="Takamura and Okumura, 2009" startWordPosition="1722" endWordPosition="1725">ng across different pieces of information than single common words. (Recall the example mentioned in Section 1) Thus, the word pair score is simply defined as: sp(w1, w2) = sr(w1)sr(w2) and the summary score is computed as: fQSBP (S) = ∑ sp(w1, w2) (3) {w1,w2|w1=,4w2 and w1,w2Eu and uES} where u is a textual unit, which in our case is a sentence. Our problem then is to select S to maximize fQSBP(S). The above function based on word pairs is still submodular, and therefore we can apply a greedy approximate algorithm with performance guarantee as proposed in previous work (Khuller et al., 1999; Takamura and Okumura, 2009a). Let l(u) denote the length of u. Given a set of source documents D and a length limit L for a summary, Require: D, L 1: W = D,S = φ 2: while W =� φ do 3: u = arg maxuEW f (SU{ l(})−f(S) 4: if l(u) + ∑uSES l(uS) G L then 5: S = S U {u} 6: end if 7: W = W/{u} 8: end while 9: umax = arg maxuED f(u) 10: if f(umax) &gt; f(S) then 11: return umax 12: else return S 13: end if where f(·) is some score function such as fQSBP. We call our proposed method QSBP: Query Snowball with Word Pairs. r2 r2 r2 r2 r2 r1 r2 r2 r2 r1 q r1 r2 r1 r1 Q root r2 R1 q q R2 r1 r2 r2 r1 r2 r2 r2 ∑ sr(r1) = qEQ 225 4 Experi</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009a. Text summarization model based on maximum coverage problem and its variant. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 781–789. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on the budgeted median problem.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM conference on Information and knowledge management, CIKM ’09,</booktitle>
<pages>1589--1592</pages>
<publisher>Association for Computing Machinery.</publisher>
<contexts>
<context position="3355" citStr="Takamura and Okumura, 2009" startWordPosition="496" endWordPosition="500">represents a particular award that the movie received, and the two Japanese nugget strings have as many as three words in common: “*ff (review/critic)”, “ア.:!,A (animation)” and “S (award).” Thus, if we use single words as the basis for penalising redundancy in sentence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps. We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a). This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit. 223 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics • Question Bosma, 2009). Unlike existing graph-based methSen to Chihiro no Kamikakushi (Spirited Away) is a full-length ods, our method explicitly computes indirect relaanimated movie from Japan. The user wants to know how it tionships between the query and wo</context>
<context position="6520" citStr="Takamura and Okumura, 2009" startWordPosition="1013" endWordPosition="1016">t co-occur with a query term in the same sentence, and R2 is the set of words (r2) that co-occur with a word from R1, excluding those that are already in R1. The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summar</context>
<context position="10638" citStr="Takamura and Okumura, 2009" startWordPosition="1722" endWordPosition="1725">ng across different pieces of information than single common words. (Recall the example mentioned in Section 1) Thus, the word pair score is simply defined as: sp(w1, w2) = sr(w1)sr(w2) and the summary score is computed as: fQSBP (S) = ∑ sp(w1, w2) (3) {w1,w2|w1=,4w2 and w1,w2Eu and uES} where u is a textual unit, which in our case is a sentence. Our problem then is to select S to maximize fQSBP(S). The above function based on word pairs is still submodular, and therefore we can apply a greedy approximate algorithm with performance guarantee as proposed in previous work (Khuller et al., 1999; Takamura and Okumura, 2009a). Let l(u) denote the length of u. Given a set of source documents D and a length limit L for a summary, Require: D, L 1: W = D,S = φ 2: while W =� φ do 3: u = arg maxuEW f (SU{ l(})−f(S) 4: if l(u) + ∑uSES l(uS) G L then 5: S = S U {u} 6: end if 7: W = W/{u} 8: end while 9: umax = arg maxuED f(u) 10: if f(umax) &gt; f(S) then 11: return umax 12: else return S 13: end if where f(·) is some score function such as fQSBP. We call our proposed method QSBP: Query Snowball with Word Pairs. r2 r2 r2 r2 r2 r1 r2 r2 r2 r1 q r1 r2 r1 r1 Q root r2 R1 q q R2 r1 r2 r2 r1 r2 r2 r2 ∑ sr(r1) = qEQ 225 4 Experi</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009b. Text summarization model based on the budgeted median problem. In Proceeding of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 1589–1592. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramakrishna Varadarajan</author>
<author>Vagelis Hristidis</author>
</authors>
<title>A system for query-specific document summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM ’06,</booktitle>
<pages>622--631</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7671" citStr="Varadarajan and Hristidis, 2006" startWordPosition="1183" endWordPosition="1186">hod is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Figure 2: Co-occurrence Graph (Query Snowball) We represent this base word score by sb(w) = log(N/ctf (w)) or sb(w) = log(N/n(w)), where ctf (w) is the total number of occurrences of w within the corpus and n(w) is the document frequency of w, and N is the total number of documents in the corpus. We will refer to these two versions as itf and idf, respectively. Our second clue is the weight propagated from the center of the cooccurence graph shown in Figure 1. Below, we describe how to compute the word scores for words in R1 and then those for words in R2. As Figure 2 suggests, the query</context>
</contexts>
<marker>Varadarajan, Hristidis, 2006</marker>
<rawString>Ramakrishna Varadarajan and Vagelis Hristidis. 2006. A system for query-specific document summarization. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM ’06, pages 622–631. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text REtrieval Conference (TREC</booktitle>
<pages>54--68</pages>
<contexts>
<context position="2147" citStr="Voorhees, 2003" startWordPosition="303" endWordPosition="304">iven set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the information need representation, we build a co-occurrence graph to obtain words that augment the original query terms. We call this method Query Snowball. Another challenge in sentence selection for query-oriented multi-document summarization is how to avoid redundancy so that diverse pieces of information (i.e. nuggets (Voorhees, 2003)) can be covered. For penalizing redundancy across sentences, using single words as the basic unit may not always be appropriate, because different nuggets for a given information need often have many words in common. Figure 1 shows an example of this word overlap problem from the NTCIR-8 ACLIA2 Japanese question answering test collection. Here, two gold-standard nuggets for the question “Sen to Chihiro no Kamikakushi (Spirited Away) is a fulllength animated movie from Japan. The user wants to know how it was received overseas.” (in English translation) is shown. Each nugget represents a parti</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Ellen M. Voorhees. 2003. Overview of the TREC 2003 Question Answering Track. In Proceedings of the Twelfth Text REtrieval Conference (TREC 2003), pages 54–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Joshua Goodman</author>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>1776--1782</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="6931" citStr="Yih et al. (2007)" startWordPosition="1072" endWordPosition="1075">the words in R1 and R2. Our first clue for computing a word score is the query-independent importance of the word. Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). M</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In Proceedings of the 20th international joint conference on Artifical intelligence, pages 1776–1782. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>