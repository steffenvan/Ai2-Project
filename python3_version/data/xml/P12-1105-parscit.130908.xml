<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.998563">
Polarity Consistency Checking for Sentiment Dictionaries
</title>
<author confidence="0.970101">
Eduard Dragut
</author>
<affiliation confidence="0.9629175">
Cyber Center
Purdue University
</affiliation>
<email confidence="0.991209">
edragut@purdue.edu
</email>
<affiliation confidence="0.957676">
Computer Science Dept.
University of Illinois at Chicago
</affiliation>
<email confidence="0.990035">
fhwang207,cyu,sistlal@uic.edu
</email>
<affiliation confidence="0.949855">
Computer Science Dept.
Binghamton University
</affiliation>
<email confidence="0.997938">
meng@cs.binghamton.edu
</email>
<note confidence="0.711101">
Hong Wang Clement Yu Prasad Sistla Weiyi Meng
</note>
<sectionHeader confidence="0.991677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99418747368421">
Polarity classification of words is important
for applications such as Opinion Mining and
Sentiment Analysis. A number of sentiment
word/sense dictionaries have been manually
or (semi)automatically constructed. The dic-
tionaries have substantial inaccuracies. Be-
sides obvious instances, where the same word
appears with different polarities in different
dictionaries, the dictionaries exhibit complex
cases, which cannot be detected by mere man-
ual inspection. We introduce the concept of
polarity consistency of words/senses in senti-
ment dictionaries in this paper. We show that
the consistency problem is NP-complete. We
reduce the polarity consistency problem to the
satisfiability problem and utilize a fast SAT
solver to detect inconsistencies in a sentiment
dictionary. We perform experiments on four
sentiment dictionaries and WordNet.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947387096774">
The opinions expressed in various Web and media
outlets (e.g., blogs, newspapers) are an important
yardstick for the success of a product or a govern-
ment policy. For instance, a product with consis-
tently good reviews is likely to sell well. The gen-
eral approach is to summarize the semantic polarity
(i.e., positive or negative) of sentences/documents
by analysis of the orientations of the individual
words (Pang and Lee, 2004; Danescu-N.-M. et al.,
2009; Kim and Hovy, 2004; Takamura et al., 2005).
Sentiment dictionaries are utilized to facilitate the
summarization. There are numerous works that,
given a sentiment lexicon, analyze the structure of
a sentence/document to infer its orientation, the
holder of an opinion, the sentiment of the opin-
ion, etc. (Breck et al., 2007; Ding and Liu, 2010;
Kim and Hovy, 2004). Several domain indepen-
dent sentiment dictionaries have been manually or
(semi)-automatically created, e.g., General Inquirer
(GI) (Stone et al., 1996), Opinion Finder (OF) (Wil-
son et al., 2005), Appraisal Lexicon (AL) (Taboada
and Grieve, 2004), SentiWordNet (Baccianella et al.,
2010) and Q-WordNet (Agerri and Garcia-Serrano,
2010). Q-WordNet and SentiWordNet are lexical re-
sources which classify the synsets(senses) in Word-
Net according to their polarities. We call them sen-
timent sense dictionaries (SSD). OF, GI and AL
are called sentiment word dictionaries (SWD). They
consist of words manually annotated with their cor-
responding polarities. The sentiment dictionaries
have the following problems:
</bodyText>
<listItem confidence="0.991717647058824">
• They exhibit substantial (intra-dictionary) inac-
curacies. For example, the synset
{Indo-European, Indo-Aryan, Aryan} (of or re-
lating to the former Indo-European people),
has a negative polarity in Q-WordNet, while
most people would agree that this synset has a
neutral polarity instead.
• They have (inter-dictionary) inconsistencies.
For example, the adjective cheap is positive in
AL and negative in OF.
• These dictionaries do not address the concept of
polarity (in)consistency of words/synsets.
We concentrate on the concept of (in)consistency
in this paper. We define consistency among the po-
larities of words/synsets in a dictionary and give
methods to check it. A couple of examples help il-
lustrate the problem we attempt to address.
</listItem>
<page confidence="0.953929">
997
</page>
<note confidence="0.9857715">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 997–1005,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999826304347826">
The first example is the verbs confute and
disprove, which have positive and negative po-
larities, respectively, in OF. According to WordNet,
both words have a unique sense, which they share:
disprove, confute (prove to be false) ”The physicist
disproved his colleagues’ theories”
Assuming that WordNet has complete information
about the two words, it is rather strange that the
words have distinct polarities. By manually check-
ing two other authoritative English dictionaries, Ox-
ford1 and Cambridge2, we note that the information
about confute and disprove in WordNet is the
same as that in these dictionaries. So, the problem
seems to originate in OF.
The second example is the verbs tantalize
and taunt, which have positive and negative po-
larities, respectively, in OF. They also have a unique
sense in WordNet, which they share. Again, there
is a contradiction. In this case Oxford dictionary
mentions a sense of tantalize that is missing
from WordNet: “excite the senses or desires of
(someone)”. This sense conveys a positive polarity.
Hence, tantalize conveys a positive sentiment
when used with this sense.
In summary, these dictionaries have conflicting
information. Manual checking of sentiment dictio-
naries for inconsistency is a difficult endeavor. We
deem words such as confute and disprove in-
consistent. We aim to unearth these inconsistencies
in sentiment dictionaries. The presence of inconsis-
tencies found via polarity analysis is not exclusively
attributed to one party, i.e., either the sentiment dic-
tionary or WordNet. Instead, as emphasized by the
above examples, some of them lie in the sentiment
dictionaries, while others lie in WordNet. Therefore,
a by-product of our polarity consistency analysis is
that it can also locate some of the likely places where
WordNet needs linguists’ attention.
We show that the problem of checking whether
the polarities of a set of words is consistent is NP-
complete. Fortunately, the consistency problem can
be reduced to the satisfiability problem (SAT). A
fast SAT solver is utilized to detect inconsistencies
and it is known such solvers can in practice deter-
mine consistency or detect inconsistencies. Experi-
mental results show that substantial inconsistencies
</bodyText>
<footnote confidence="0.9998915">
1http://oxforddictionaries.com/
2http://dictionary.cambridge.org/
</footnote>
<bodyText confidence="0.993998">
are discovered among words with polarities within
and across sentiment dictionaries. This suggests that
some remedial work needs to be performed on these
sentiment dictionaries as well as on WordNet. The
contributions of this paper are:
</bodyText>
<listItem confidence="0.964825666666667">
• address the consistency of polarities of
words/senses. The problem has not been
addressed before;
• show that the consistency problem is NP-
complete;
• reduce the polarity consistency problem to the
satisfiability problem and utilize a fast SAT
solver to detect inconsistencies;
• give experimental results to demonstrate that our
technique identifies considerable inconsistencies
in various sentiment lexicons as well as discrep-
ancies between these lexicons and WordNet.
</listItem>
<sectionHeader confidence="0.950551" genericHeader="introduction">
2 Problem Definition
</sectionHeader>
<bodyText confidence="0.999966857142857">
The polarities of the words in a sentiment dictionary
may not necessarily be consistent (or correct). In
this paper, we focus on the detection of polarity as-
signment inconsistencies for the words and synsets
within and across dictionaries (e.g., OF vs. GI). We
attempt to pinpoint the words with polarity inconsis-
tencies and classify them (Section 3).
</bodyText>
<subsectionHeader confidence="0.963467">
2.1 WordNet
</subsectionHeader>
<bodyText confidence="0.979749761904762">
We give a formal characterization of WordNet. This
consists of words, synsets and frequency counts. A
word-synset network N is quadruple (W, S, £, f)
where W is a finite set of words, S is a finite set of
synsets, £ is a set of undirected edges between el-
ements in W and S, i.e., £ C W x S and f is a
function assigning a positive integer to each element
in £. For an edge (w, s), f(w, s) is called the fre-
quency of use of w in the sense given by s. For any
word w and synset s, we say that s is a synset of w
if (w, s) E £. Also, for any word w, we let freq(w)
denote the sum of all f(w, s) such that (w, s) E £.
If a synset has a 0 frequency of use we replace it
with 0.1, which is a standard smoothing technique
(Han, 2005). For instance, the word cheap has four
senses. The frequencies of occurrence of the word in
the four senses are f1 = 9, f2 = 1, f3 = 1 and f4 =
0, respectively. By smoothing, f4 = 0.1. Hence,
freq(cheap) = f1 + f2 + f3 + f4 = 11.1. The
relative frequency of the synset in the first sense of
cheap, which denotes the probability that the word
</bodyText>
<equation confidence="0.9956925">
is used in the first sense, is f�
freq��heap) = 9 = 0.81.
</equation>
<page confidence="0.948396">
11.1
998
</page>
<subsectionHeader confidence="0.997254">
2.2 Consistent Polarity Assignment
</subsectionHeader>
<bodyText confidence="0.9549555625">
We assume that each synset has a unique polarity.
We define the polarity of a word to be a discrete
probability distribution: P+, P_, P0 with P++P_+
P0 = 1, where they represent the “likelihoods” that
the word is positive, negative or neutral, respec-
tively. We call this distribution a polarity distribu-
tion. For instance, the word cheap has the polarity
distribution P+ = 0.81, P_ = 0.19 and P0 = 0.
The polarity distribution of a word is estimated using
the polarities of its underlying synsets. For instance
cheap has four senses, with the first sense being
positive and the last three senses being negative. The
probability that the word expresses a negative senti-
ment is P_ = f2+f3+f4 = 0.19, while the proba-
f req (cheap)
bility that the word expresses a positive sentiment is
</bodyText>
<equation confidence="0.9991215">
P+ = f�
freq(cheap) = 0.81. P0 = 1 − P+ − P_ = 0.
</equation>
<bodyText confidence="0.978513634146341">
Our view of characterizing the polarity of a word
using a polarity distribution is shared with other pre-
vious works (Kim and Hovy, 2006; Andreevskaia
and Bergler, 2006). Nonetheless, we depart from
these works in the following key aspect. We say
that a word has a (mostly) positive (negative) po-
larity if the majority sense of the word is positive
(negative). That is, a word has a mostly positive po-
larity if P+ &gt; P_ + P0 and it has a mostly nega-
tive polarity if P_ &gt; P+ + P0. Or, equivalently, if
P+ &gt; 21 or P_ &gt; 12, respectively. For example,
on majority, cheap conveys positive polarity since
P+ = .081 &gt; 12, i.e., the majority sense of the word
cheap has positive connotation.
Based on this study, we contend that GI, OF and
AL tacitly assume this property. For example, the
verb steal is assigned only negative polarity in
GI. This word has two other less frequently occur-
ring senses, which have positive polarities. The po-
larity of steal according to these two senses is not
mentioned in GI. This is the case for the overwhelm-
ing majority of the entries in the three dictionaries:
only 112 out of a total of 14,105 entries in the three
dictionaries regard words with multiple polarities.
For example, the verb arrest is mentioned with
both negative and positive polarities in GI. We re-
gard an entry in an SWD as the majority sense of the
word has the specified polarity, although the word
may carry other polarities. For instance, the adjec-
tive cheap has positive polarity in GI. The only as-
sumption we make about the word is that it has a po-
larity distribution such that P+ &gt; P_ + P0. This in-
terpretation is consistent with the senses of the word.
In this work we show that this property allows the
polarities of words in input sentiment dictionaries to
be checked. We formally state this property.
Definition 1. Let w be a word and Sw its set of
synsets. Each synset in Sw has an associated po-
larity and a relative frequency with respect to w. w
has polarity p, p E {positive, negative} if there is
a subset of synsets S′ C_ Sw such that each synset
</bodyText>
<equation confidence="0.567561">
s E S′ has polarity p and ∑sES′ f(w,s)
freq(w) &gt; 0.5. S′
</equation>
<bodyText confidence="0.965601756756757">
is called a polarity dominant subset. If there is no
such subset then w has a neutral polarity.
S′ C_ Sw is a minimally dominant subset of
synsets (MDSs) if the sum of the relative frequen-
cies of the synsets in S′ is larger than 0.5 and the
removal of any synset s from S′ will make the sum
of the relative frequencies of the synsets in S′ − {s}
smaller than or equal to 0.5.
The definition does not preclude a word from hav-
ing a polarity with a majority sense and a different
polarity with a minority sense. For example, the def-
inition does not prevent a word from having both
positive and negative senses, but it prevents a word
from concomitantly having a majority sense of being
positive and a majority sense of being negative.
Despite using a “hard-coded” constant in the def-
inition, our approach is generic and does not depen-
dent on the constant 0.5. This constant is just a lower
bound for deciding whether a word has a majority
sense with a certain polarity. It also is intuitively
appealing. The constant can be replaced with an ar-
bitrary threshold T between 0.5 and 1.
We need a formal description of polarity assign-
ments to the words and synsets in WordNet. We as-
sign polarities from the set P = {positive, negative,
neutral} to elements in W U S. Formally, a polar-
ity assignment -y for a network N is a function from
W U S to the set P. Let -y be a polarity assignment
for N. We say that -y is consistent if it satisfies the
following condition for each w E W:
For p E {positive, negative}, -y(w) = p iff the
sum of all f(w, s) such that (w, s) E E and -y(s) =
fre�(w)
p, is greater than 2 .Note that, for any w E
W, -y(w) = neutral iff the above inequality is not
satisfied for both values of p in {positive, negative}.
We contend that our approach is applicable to do-
</bodyText>
<page confidence="0.998971">
999
</page>
<tableCaption confidence="0.997995">
Table 1: Disagreement between dictionaries.
</tableCaption>
<table confidence="0.998208666666667">
Pairs of Word Polarity Disagreement
Dictionaries
Inconsistency Overlap
OF &amp; GI 90 2,924
OF &amp; AL 73 1,150
GI &amp; AL 18 712
</table>
<bodyText confidence="0.999513833333333">
main dependent sentiment dictionaries, too. We can
employ WordNet Domains (Bentivogli et al., 2004).
WordNet Domains augments WordNet with domain
labels. Hence, we can project the words/synsets in
WordNet according to a domain label and then apply
our methodology to the projection.
</bodyText>
<sectionHeader confidence="0.998057" genericHeader="method">
3 Inconsistency Classification
</sectionHeader>
<bodyText confidence="0.999333">
Polarity inconsistencies are of two types: input and
complex. We discuss them in this section.
</bodyText>
<subsectionHeader confidence="0.937896">
3.1 Input Dictionaries Polarity Inconsistency
</subsectionHeader>
<bodyText confidence="0.99820325">
Input polarity inconsistencies are of two types:
intra-dictionary and inter-dictionary inconsistencies.
The latter are obtained by comparing (1) two SWDs,
(2) an SWD with an SSD and (3) two SSDs.
</bodyText>
<sectionHeader confidence="0.441546" genericHeader="method">
3.1.1 Intra-dictionary inconsistency
</sectionHeader>
<bodyText confidence="0.999977714285714">
An SWD may have triplets of the form (w, pos, p)
and (w, pos, p′), where p 7� p′. For instance, the
verb brag has both positive and negative polarities
in OF. For these cases, we look up WordNet and ap-
ply Definition 1 to determine the polarity of word w
with part of speech pos. The verb brag has negative
polarity according to Definition 1. Such cases sim-
ply say that the team who constructs the dictionary
believes the word has multiple polarities as they do
not adopt our dominant sense principle. There are
58 occurrences of this type of inconsistency in GI,
OF and AL. Q-WordNet, a sentiment sense dictio-
nary, does not have intra-inconsistencies as it does
do not have a synset with multiple polarities.
</bodyText>
<sectionHeader confidence="0.43218" genericHeader="method">
3.1.2 Inter-dictionary inconsistency
</sectionHeader>
<bodyText confidence="0.999903166666667">
A word belongs to this category if it appears with
different polarities in different SWDs. For instance,
the adjective joyless has positive polarity in OF
and negative polarity in GI. Table 1 depicts the over-
lapping relationships between the three SWDs: e.g.,
OF has 2,933 words in common with GI. The three
dictionaries largely agree on the polarities of the
words they pairwise share. For instance, out of 2,924
words shared by OF and GI, 2,834 have the same po-
larities. However, there are also a significant number
of words which have different polarities across dic-
tionaries. Case in point, OF and GI disagree on the
polarities of 90 words. Among the three dictionar-
ies there are 181 polarity inconsistent words. These
words are manually corrected using Definition 1 be-
fore the polarity consistency checking is applied to
the union of the three dictionaries. This union is
called disagreement-free union.
</bodyText>
<subsectionHeader confidence="0.998853">
3.2 Complex Polarity Inconsistency
</subsectionHeader>
<bodyText confidence="0.999774833333333">
This kind of inconsistency is more subtle and cannot
be detected by direct comparison of words/synsets.
They consist of sets of words and/or synsets whose
polarities cannot concomitantly be satisfied. Recall
the example of the verbs confute and disprove
in OF given in Section 1. Recall our argument that
by assuming that WordNet is correct, it is not pos-
sible for the two words to have different polarities:
the sole synset, which they share, would have two
different polarities, which is a contradiction.
The occurrence of an inconsistency points out the
presence of incorrect input data:
</bodyText>
<listItem confidence="0.997476333333333">
• the information given in WordNet is incorrect, or
• the information in the given sentiment dictionary
is incorrect, or both.
</listItem>
<bodyText confidence="0.956184533333333">
Regarding WordNet, the errors may be due to (1)
a word has senses that are missing from WordNet or
(2) the frequency count of a synset is inaccurate. A
comprehensive analysis of every synset/word with
inconsistency is a tantalizing endeavor requiring not
only a careful study of multiple sources (e.g., dictio-
naries such as Oxford and Cambridge) but also lin-
guistic expertise. It is beyond the scope of this paper
to enlist all potentially inconsistent words/synsets
and the possible remedies. Instead, we limit our-
selves to drawing attention to the occurrence of these
issues through examples, welcoming experts in the
area to join the corrective efforts. We give more ex-
amples of inconsistencies in order to illustrate addi-
tional discrepancies between input dictionaries.
</bodyText>
<subsectionHeader confidence="0.681379">
3.2.1 WordNet vs. Sentiment Dictionaries
</subsectionHeader>
<bodyText confidence="0.999984625">
The adjective bully is an example of a discrep-
ancy between WordNet and a sentiment dictionary.
The word has negative polarity in OF and has a sin-
gle sense in WordNet. The sense is shared with the
word nifty, which has positive polarity in OF. By
applying Definition 1 to nifty we obtain that the
sense is positive, which in turn, by Definition 1, im-
plies that bully is positive. This contradicts the
</bodyText>
<page confidence="0.932261">
1000
</page>
<bodyText confidence="0.999938">
input polarity of bully. According to the Webster
dictionary, the word has a sense (i.e., resembling or
characteristic of a bully) which has a negative po-
larity, but it is not present in WordNet. The example
shows the presence of a discrepancy between Word-
Net and OF, namely, OF seems to assign polarity to
a word according to a sense that is not in WordNet.
</bodyText>
<subsectionHeader confidence="0.821064">
3.2.2 Across Sentiment Dictionaries
</subsectionHeader>
<bodyText confidence="0.999985076923077">
We provide examples of inconsistencies across
sentiment dictionaries here. Our first example
is obtained by comparing SWDs. The adjective
comic has negative polarity in AL and the adjective
laughable has positive polarity in OF. Through
deduction (i.e., by successive applications of Defini-
tion 1), the word risible, which is not present in
either of the dictionaries, is assigned negative polar-
ity because of comic and is assigned positive po-
larity because of laughable.
The second example illustrates that an SWD and
an SSD may have contradicting information. The
verb intoxicate has three synsets in WordNet,
each with the same frequency. Hence, their rela-
tive frequencies with respect to intoxicate are
3. On one hand, intoxicate has a negative po-
larity in GI. This means that P_ &gt; 1�. On the other
hand, two of its three synsets have positive polarity
in Q-WordNet. So, P+ = 23 &gt; 1�, which means that
P_ &lt; 1�. This is a contradiction. This example can
also be used to illustrate the presence of a discrep-
ancy between WordNet and sentiment dictionaries.
Note that all the frequencies of use of the senses of
intoxicate in WordNet are 0. The problem is
that when all the senses of a word have a 0 frequency
of use, wrong polarity inference may be produced.
</bodyText>
<subsectionHeader confidence="0.998159">
3.3 Consistent Polarity Assignment
</subsectionHeader>
<bodyText confidence="0.99961925">
Given the discussion above, it clearly is important to
find all occurrences of inconsistent words. This in
turn boils down to finding those words with the prop-
erty that there does not exist any polarity assignment
to the synsets, which is consistent with their polar-
ities. It turns out that the complexity of the prob-
lem of assigning polarities to the synsets such that
the assignment is consistent with the polarities of
the input words, called Consistent Polarity
Assignment problem, is a “hard” problem, as de-
scribed below. The problem is stated as follows:
Consider two sets of nodes of type synsets and
type words, in which each synset of a word has a
relative frequency with respect to the word. Each
synset can be assigned a positive, negative or neu-
tral polarity. A word has polarity p if it satisfies the
hypothesis of Definition 1. The question to be an-
swered is: Given an assignment of polarities to the
words, does there exist an assignment of polarities
to the synsets that agrees with that of the words?
In other words, given the polarities of a subset of
words (e.g., that given by one of the three SWDs)
the problem of finding the polarities of the synsets
that agree with this assignment is a “hard” problem.
</bodyText>
<figureCaption confidence="0.5787805">
Theorem 1. The Consistent Polarity Assignment
problem is NP-complete.
</figureCaption>
<sectionHeader confidence="0.984691" genericHeader="method">
4 Polarity Consistency Checking
</sectionHeader>
<bodyText confidence="0.999980904761905">
To “exhaustively” solve the problem of finding the
polarity inconsistencies in an SWD, we propose a
solution that reduces an instance of the problem to
an instance of CNF-SAT. We can then employ a
fast SAT solver (e.g., (Xu et al., 2008; Babic et al.,
2006)) to solve our problem. CNF-SAT is a deci-
sion problem of determining if there is an assign-
ment of True and False to the variables of a Boolean
formula 4) in conjunctive normal form (CNF) such
that 4) evaluates to True. A formula is in CNF if
it is a conjunction of one or more clauses, each of
which is a disjunction of literals. CNF-SAT is a clas-
sic NP-complete problem, but, modern SAT solvers
are capable of solving many practical instances of
the problem. Since, in general, there is no easy way
to tell the difficulty of a problem without trying it,
SAT solvers include time-outs, so they will termi-
nate even if they cannot find a solution.
We developed a method of converting an instance
of the polarity consistency checking problem into an
instance of CNF-SAT, which we will describe next.
</bodyText>
<subsectionHeader confidence="0.998874">
4.1 Conversion to CNF-SAT
</subsectionHeader>
<bodyText confidence="0.999814181818182">
The input consists of an SWD D and the word-
synset network N. We partition N into connected
components. For each synset s we define three
Boolean variables s_, s+ and s0, corresponding to
the negative, positive and neutral polarities, respec-
tively. In this section we use −, +, 0 to denote neg-
ative, positive and neutral polarities, respectively.
Let 4) be the Boolean formula for a connected
component M of the word-synset network N. We
introduce its clauses. First, for each synset s we need
a clause C(s) that expresses that the synset can have
</bodyText>
<page confidence="0.969621">
1001
</page>
<bodyText confidence="0.962333285714286">
only one of the three polarities: C(s) = (s+∧¬s−∧
¬s0) ∨ (s− ∧ ¬s+ ∧ ¬s0) ∨ (s0 ∧ ¬s− ∧ ¬s+).
Since a word has a neutral polarity if it has nei-
ther positive nor negative polarities, we have that
s0 = ¬s+ ∧ ¬s−. Replacing this expression in the
equation above and applying standard Boolean logic
formulas, we can reduce it to
</bodyText>
<equation confidence="0.998552">
C(s) = ¬s+ ∨ ¬s− (1)
</equation>
<bodyText confidence="0.92897325">
For each word w with polarity p ∈ {−, +, 0} in
D we need a clause C(w, p) that states that w has
polarity p. So, the Boolean formula for a connected
component M of the word-synset network N is:
</bodyText>
<equation confidence="0.9923325">
∧� = C(s) ∧ ∧ C(w, p). (2)
S∈M (w,p)∈D
</equation>
<bodyText confidence="0.997961947368421">
From Definition 1, w is neutral if it is neither pos-
itive nor negative. Hence, C(w, 0) = ¬C(w, −) ∧
¬C(w, +). So, we need to define only the clauses
C(w, −) and C(w, +), which correspond to w hav-
ing polarity negative and positive, respectively. So,
herein p ∈ {−, +}, unless otherwise specified.
Our method is based on the following statement
in Definition 1: w has polarity p if there exists a
polarity dominant subset among its synsets. Thus,
C(w, p) is defined by enumerating all the MDSs of
w. If at least one of them is a polarity dominant
subset then C(w, p) evaluates to True.
Exhaustive Enumeration of MDSs Method
(EEM) We now elaborate the construction of
C(w, p). We enumerate all the MDSs of w and for
each of them we introduce a clause. The clauses are
then concatenated by OR in the Boolean formula.
Let C(w, p, T) denote the clause for an MDS T of
w, when w has polarity p ∈ {−, +}. Hence,
</bodyText>
<equation confidence="0.9938385">
C(w, p) = V C(w, p, T), (3)
T∈MDS(w)
</equation>
<bodyText confidence="0.9950935">
where MDS(w) is the set of all MDSs of w.
For each MDS T of w, the clause C(w, p, T) is
the AND of the variables corresponding to polarity
p of the synsets in T. That is,
</bodyText>
<equation confidence="0.9603935">
C(w,p,T) = ∧ sp, p ∈ {−, +}. (4)
S∈T
</equation>
<bodyText confidence="0.976950411764706">
The formula 4) is not in CNF after this construc-
tion and it needs to be converted. The conversion to
CNF is a standard procedure and we omit it in this
paper. 4) in CNF is input to a SAT solver.
Example 1. Consider a connected component
consisting of the words w = cheap, v =
inexpensive and u = sleazy. cheap has
a positive polarity, whereas inexpensive and
sleazy have negative polarities. The synsets
of these words are: {s1, s2, s3, s4}, {s1} and
{s3, s4, s5}, respectively (refer to WordNet). The
relative frequencies of s3, s4 and s5 w.r.t. sleazy
are all equal to 1/3. We have 15 binary variables,
3 per synset, si−, si+, si0, 1 ≤ i ≤ 5. The only
MDS of cheap is {s1}, which coincides with that
of inexpensive. Those of sleazyare {s3, s4},
{s3, s5} and {s4, s5}. For each si we need a clause
</bodyText>
<equation confidence="0.944884571428571">
C(si). Hence, C(w, +) = s1+, C(v, −) = s1− and
C(u, −) = (s3− ∧ s4−) ∨ (s3− ∧ s5−) ∨ (s4− ∧ s5−).
Thus, � = ∧ C(si) ∧ [s1+ ∧ s1 − ∧ ((s3− ∧ s4−) ∨
i
(s3− ∧ s5−) ∨ (s4− ∧ s5−))]. 4) is not in CNF and
needs to be converted. For 4) to be True, the clauses
C(w, +) = s1+ and C(v, −) = s1− must be True.
</equation>
<bodyText confidence="0.97830375">
But, this makes C(s1) False. Hence, 4) is not satisfi-
able. The clauses C(w, +) = s1+ and C(v, −) = s1−
are unsatisfiable and thus the polarities of cheap
and inexpensive are inconsistent.
</bodyText>
<subsectionHeader confidence="0.995611">
4.2 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.999992">
The above reduction is exponential in the number
of clauses (see, Equation 3) in the worst case. A
polynomial reduction is possible, but it is signifi-
cantly more complicated to implement. We choose
to present the exponential reduction in this paper be-
cause it can handle over 97% of the words in Word-
Net and it is better suited to explain one of the main
contributions of paper: the translation from the po-
larity consistency problem to SAT.
WordNet possesses nice properties, which allows
the exponential reduction to run efficiently in prac-
tice. First, 97.2% of its (word, part-of-speech) pairs
have 4 or fewer synsets. Thus, these words add very
few clauses to a CNF formula (Equation 3). Second,
WordNet can be partitioned into 33,015 non-trivial
connected components, each of which corresponds
to a Boolean formula and they all are independently
handled. A non-trivial connected component has at
least two words. Finally, in practice, not all con-
nected components need to be considered for an in-
put sentiment dictionary D, but only those having at
least two words in D. In our experiments the largest
number of components that need to be processed is
</bodyText>
<page confidence="0.998654">
1002
</page>
<tableCaption confidence="0.991689">
Table 2: Distribution of words and synsets
</tableCaption>
<table confidence="0.917856571428571">
POS Words Synsets OF GI AL QWN
Noun117,798 82,115 1,907 1,444 2 7,403
Verb 11,529 13,767 1,501 1,041 0 4006
Adj. 21,479 18,156 2,608 1,188 1,440 4050
Adv. 4,481 3,621 775 51 317 40
Total 155,287 117,659 6,791 3,961 1,759 15,499
1,581, for the disagreement-free union dictionary.
</table>
<sectionHeader confidence="0.967647" genericHeader="method">
5 Detecting Inconsistencies
</sectionHeader>
<bodyText confidence="0.99997084375">
In this section we describe how we detect the words
with polarity inconsistencies using the output of a
SAT solver. For an unsatisfiable formula, a mod-
ern SAT solver returns a minimal unsatisfiable core
(MUC) from the original formula. An unsatisfiable
core is minimal if it becomes satisfiable whenever
any one of its clauses is removed. There are no
known practical algorithms for computing the min-
imum core (Dershowitz et al., 2006). In our prob-
lem a MUC corresponds to a set of polarity incon-
sistent words. The argument is as follows. Con-
sider W the set of words in a connected component
and Φ the CNF formula generated with the above
method. During the transformation we keep track of
the clauses introduced in Φ by each word. Suppose
Φ is inconsistent. Then, the SAT solver returns a
MUC. Each clause in a MUC is mapped back to its
corresponding word(s). We obtain the correspond-
ing subset of words W′, W′ ⊆ W. Suppose that Φ′
is the Boolean CNF formula for the words in W′.
The set of clauses in Φ′ is a subset of those in Φ.
Also, the clauses in the MUC appear in Φ′. Thus, Φ′
is unsatisfiable and the words in W′ are inconsistent.
To find all inconsistent words we ought to gener-
ate all MUCs. Unfortunately, this is a “hard” prob-
lem (Dershowitz et al., 2006) and no open source
SAT solver possesses this functionality. We how-
ever observe that the two SAT solvers we use for our
experiments (SAT4j and PicoSAT (Biere, 2008)) re-
turn different MUCs for the same formula and we
use them to find as many inconsistencies as possi-
ble.
</bodyText>
<sectionHeader confidence="0.99932" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.994889666666667">
The goal of the experimental study is to show that
our techniques can identify considerable inconsis-
tencies in various sentiment dictionaries.
</bodyText>
<tableCaption confidence="0.994949">
Table 3: Intra- and inter-dictionaries inconsistency
</tableCaption>
<table confidence="0.999110833333333">
POS OF QW GI QW AL QW UF QW
Noun 23 119 4 61 0 42 90 140
Verb 66 113 2 67 0 0 63 137
Adj. 90 170 8 48 0 0 27 177
Adv. 61 1 0 0 2 0 69 1
Total 240 403 14 176 2 42 249 455
</table>
<bodyText confidence="0.98930855">
Data sets In our experiments, we use WordNet
3.0, GI, OF, AL and Q-WordNet. Their statistics are
given in Table 2. The table shows the distribution of
the words and synsets per part of speech. Columns
2 and 3 pertain to WordNet. There are 3,961 entries
in GI, 1,759 entries in AL and 6,791 entries in OF
which appear in WordNet. Q-WordNet has 15,499
entries, i.e., synsets with polarities.
Inconsistency Detection We applied our method
to (1) each of AL, GI and OF; (2) the disagreement-
free union (UF); (3) each of AL, GI and OF together
with Q-WordNet and (4) UF and Q-WordNet. Ta-
ble 3 summarizes the outcome of the experimental
study. EEM finds 240, 14 and 2 polarity inconsis-
tent words in OF, GI and AL, respectively. The ratio
between the number of inconsistent words and the
number of input words is the highest for OF and the
lowest for AL. The union dictionary has 7,794 words
and 249 out of them are found to be polarity incon-
sistent words. Recall that we manually corrected
the polarities of 181 words, to the best of our un-
derstanding. So, in effect the three dictionaries have
249 + 181 = 430 polarity inconsistent words. As dis-
cussed in the previous section, these may not be all
the polarity inconsistencies in UF. In general, to find
all inconsistencies we need to generate all MUCs.
Generating all MUCs is an “overkill” and the SAT
solvers we use do not implement such a functional-
ity. In addition, the intention of SAT solver design-
ers is to use MUCs in a interactive manner. That
is, the errors pointed out by a MUC are corrected
and then the new improved formula is re-evaluated
by the SAT solver. If an error is still present a new
MUC is reported, and the process repeats until the
formula has no errors. Or, in our problem, until a
dictionary is consistent.
We also paired Q-WordNet with each of the
SWDs. Table 3 presents the results. Observe that po-
larities assigned to the words in AL and GI largely
agree with the polarities assigned to the synsets in
</bodyText>
<page confidence="0.955758">
1003
</page>
<bodyText confidence="0.999850770833334">
Q-WordNet. This is expected for AL because it
has only two nouns and no verb, while Q-WordNet
has only 40 adverbs. Consequently, these two dic-
tionaries have limited “overlay”. The union dictio-
nary and Q-WordNet have substantial inconsisten-
cies: the polarity of 455 words in the union dictio-
nary disagrees with the polarities assigned to their
underlying synsets in Q-WordNet.
Sentence Level Evaluation We took 10 pairs of
inconsistent words per part of speech; in total, we
collected a set IW of 80 inconsistent words. Let
(w, pos, p) E IW, p is the polarity of w. We col-
lected 5 sentences for (w, pos) from the set of snip-
pets returned by Google for query w. We parsed
the snippets and identified the first 5 occurrences of
w with the part of speech pos. Then two graduate
students with English background analyzed the po-
larities of (w, pos) in the 5 sentences. We counted
the number of times (w, pos) appears with polarity p
and polarities different from p. We defined an agree-
ment scale: total agreement (5/5), most agreement
(4/5), majority agreement (3/5), majority disagree-
ment (2/5), most disagreement (1/5), total disagree-
ment (0/5). We computed the percentage of words
per agreement category. We repeated the experiment
for 40 randomly drawn words (10 per part of speech)
from the set of consistent words. In total 600 sen-
tences were manually analyzed. Figure 1 shows the
distribution of the (in)consistent words. For exam-
ple, the annotators totally agree with the polarities
of 55% of the consistent words, whereas they only
totally agree with 16% of the polarities of the incon-
sistent words. The graph suggests that the annota-
tors disagree to some extent (total disagreement +
most disagreement + major disagreement) with 40%
of the polarities of the inconsistent words, whereas
they disagree to some extent with only 5% of the
consistent words. We also manually investigated the
senses of these words in WordNet. We noted that
36 of the 80 inconsistent words (45%) have missing
senses according to one of these English dictionar-
ies: Oxford and Cambridge.
Computational Issues We used a 4-core CPU
computer with 12GB of memory. EEM requires
10GB of memory and cannot handle words with
more than 200,000 MDSs: for UF we left the SAT
solver running for a week without ever terminating.
In contrast, it takes about 4 hours if we limit the set
</bodyText>
<figureCaption confidence="0.997899">
Figure 1: Human classification of (in)consistent words.
</figureCaption>
<bodyText confidence="0.999968285714286">
of words to those that have up to 200,000 MDSs.
EEM could not handle words such as make, give
and break. Recall however that we did not gener-
ate all MUCs. We do not know how long would that
might have taken. (The polynomial method handles
all the words in WordNet and it takes 5GB of mem-
ory and about 2 hours to finish.)
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999859631578947">
Several researchers have studied the problem of
finding opinion words (Liu, 2010). There are two
lines of work on sentiment polarity lexicon induc-
tion: corpora-based (Hatzivassiloglou and McKe-
own, 1997; Kanayama and Nasukawa, 2006; Qiu et
al., 2009; Wiebe, 2000) and dictionary-based (An-
dreevskaia and Bergler, 2006; Agerri and Garc´ıa-
Serrano, 2010; Dragut et al., 2010; Esuli and Se-
bastiani, 2005; Baccianella et al., 2010; Hu and
Liu, 2004; Kamps et al., 2004; Kim and Hovy,
2006; Rao and Ravichandran, 2009; Takamura et al.,
2005). Our work falls into the latter. Most of these
works use the lexical relations defined in WordNet
(e.g., synonym, antonym) to derive sentiment lexi-
cons. To our knowledge, none of the earlier works
studied the problem of polarity consistency check-
ing for a sentiment dictionary. Our techniques can
pinpoint the inconsistencies within individual dictio-
naries and across dictionaries.
</bodyText>
<sectionHeader confidence="0.998582" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999992333333333">
We studied the problem of checking polarity consis-
tency for sentiment word dictionaries. We proved
that this problem is NP-complete. We showed that
in practice polarity inconsistencies of words both
within a dictionary and across dictionaries can be
obtained using an SAT solver. The inconsistencies
are pinpointed and this allows the dictionaries to be
improved. We reported experiments on four senti-
ment dictionaries and their union dictionary.
</bodyText>
<page confidence="0.995815">
1004
</page>
<sectionHeader confidence="0.999543" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9952665">
This work is supported in part by the following NSF
grants: IIS-0842546 and IIS-0842608.
</bodyText>
<sectionHeader confidence="0.998066" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918274725275">
Rodrigo Agerri and Ana Garc´ıa-Serrano. 2010. Q-
wordnet: Extracting polarity from wordnet senses. In
LREC.
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for fuzzy sentiment: Sentiment tag extraction from
wordnet glosses. In EACL.
Domagoj Babic, Jesse Bingham, and Alan J. Hu. 2006.
B-cubing: New possibilities for efficient sat-solving.
TC, 55(11).
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lexical
Resource for Sentiment Analysis and Opinion Mining.
In LREC, Valletta, Malta, May.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and
Emanuele Pianta. 2004. Revising the wordnet do-
mains hierarchy: semantics, coverage and balancing.
MLR.
Armin Biere. 2008. PicoSAT essentials. JSAT, 4(2-
4):75–97.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identi-
fying expressions of opinion in context. In IJCAI.
Cristian Danescu-N.-M., Gueorgi Kossinets, Jon Klein-
berg, and Lillian Lee. 2009. How opinions are re-
ceived by online communities: a case study on ama-
zon.com helpfulness votes. In WWW, pages 141–150.
Nachum Dershowitz, Ziyad Hanna, and Er Nadel. 2006.
A scalable algorithm for minimal unsatisfiable core ex-
traction. In In Proc. SAT06. Springer.
Xiaowen Ding and Bing Liu. 2010. Resolving object and
attribute coreference in opinion mining. In COLING.
Eduard C. Dragut, Clement T. Yu, A. Prasad Sistla, and
Weiyi Meng. 2010. Construction of a sentimental
word dictionary. In CIKM, pages 1761–1764.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss
classification. In CIKM, pages 617–624.
Jiawei Han. 2005. Data Mining: Concepts and Tech-
niques. Morgan Kaufmann Publishers Inc.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In ACL, pages 174–181, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In ACM SIGKDD, pages 168–
177, New York, NY, USA. ACM.
J. Kamps, M. Marx, R. Mokken, and M. de Rijke. 2004.
Using wordnet to measure semantic orientation of ad-
jectives. In LREC.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’06, pages 355–363, Stroudsburg,
PA, USA. Association for Computational Linguistics.
M. Kim and E. Hovy. 2004. Determining the sentiment
of opinions. In COLING.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and
analyzing judgment opinions. In HLT-NAACL.
Bing Liu. 2010. Sentiment analysis and subjectivity. In
Nitin Indurkhya and Fred J. Damerau, editors, Hand-
book of Natural Language Processing, Second Edi-
tion. CRC Press, Taylor and Francis Group, Boca Ra-
ton, FL. ISBN 978-1420085921.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In IJCAI, pages 1199–1204.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In EACL.
P. Stone, D. Dunphy, M. Smith, and J. Ogilvie. 1996.
The general inquirer: A computer approach to content
analysis. In MIT Press.
M. Taboada and J. Grieve. 2004. Analyzing appraisal
automatically. In AAAI Spring Symposium.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL, pages 133–140.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735–740. AAAI Press.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT/EMNLP.
Lin Xu, Frank Hutter, Holger H. Hoos, and Kevin
Leyton-Brown. 2008. Satzilla: portfolio-based algo-
rithm selection for sat. J. Artif. Int. Res., 32:565–606,
June.
</reference>
<page confidence="0.990083">
1005
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497188">
<title confidence="0.999619">Polarity Consistency Checking for Sentiment Dictionaries</title>
<author confidence="0.863392">Eduard</author>
<affiliation confidence="0.774317">Cyber Purdue University</affiliation>
<email confidence="0.99907">edragut@purdue.edu</email>
<affiliation confidence="0.9998505">Computer Science University of Illinois at Chicago Computer Science Binghamton University</affiliation>
<email confidence="0.997229">meng@cs.binghamton.edu</email>
<author confidence="0.995795">Hong Wang Clement Yu Prasad Sistla Weiyi Meng</author>
<abstract confidence="0.9977012">Polarity classification of words is important for applications such as Opinion Mining and Sentiment Analysis. A number of sentiment word/sense dictionaries have been manually or (semi)automatically constructed. The dictionaries have substantial inaccuracies. Besides obvious instances, where the same word appears with different polarities in different dictionaries, the dictionaries exhibit complex cases, which cannot be detected by mere manual inspection. We introduce the concept of polarity consistency of words/senses in sentiment dictionaries in this paper. We show that the consistency problem is NP-complete. We reduce the polarity consistency problem to the satisfiability problem and utilize a fast SAT solver to detect inconsistencies in a sentiment dictionary. We perform experiments on four sentiment dictionaries and WordNet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rodrigo Agerri</author>
<author>Ana Garc´ıa-Serrano</author>
</authors>
<title>Qwordnet: Extracting polarity from wordnet senses.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<marker>Agerri, Garc´ıa-Serrano, 2010</marker>
<rawString>Rodrigo Agerri and Ana Garc´ıa-Serrano. 2010. Qwordnet: Extracting polarity from wordnet senses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Andreevskaia</author>
<author>S Bergler</author>
</authors>
<title>Mining wordnet for fuzzy sentiment: Sentiment tag extraction from wordnet glosses.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="9210" citStr="Andreevskaia and Bergler, 2006" startWordPosition="1471" endWordPosition="1474">+ = 0.81, P_ = 0.19 and P0 = 0. The polarity distribution of a word is estimated using the polarities of its underlying synsets. For instance cheap has four senses, with the first sense being positive and the last three senses being negative. The probability that the word expresses a negative sentiment is P_ = f2+f3+f4 = 0.19, while the probaf req (cheap) bility that the word expresses a positive sentiment is P+ = f� freq(cheap) = 0.81. P0 = 1 − P+ − P_ = 0. Our view of characterizing the polarity of a word using a polarity distribution is shared with other previous works (Kim and Hovy, 2006; Andreevskaia and Bergler, 2006). Nonetheless, we depart from these works in the following key aspect. We say that a word has a (mostly) positive (negative) polarity if the majority sense of the word is positive (negative). That is, a word has a mostly positive polarity if P+ &gt; P_ + P0 and it has a mostly negative polarity if P_ &gt; P+ + P0. Or, equivalently, if P+ &gt; 21 or P_ &gt; 12, respectively. For example, on majority, cheap conveys positive polarity since P+ = .081 &gt; 12, i.e., the majority sense of the word cheap has positive connotation. Based on this study, we contend that GI, OF and AL tacitly assume this property. For e</context>
<context position="33580" citStr="Andreevskaia and Bergler, 2006" startWordPosition="5834" endWordPosition="5838">t have up to 200,000 MDSs. EEM could not handle words such as make, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries.</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>A. Andreevskaia and S. Bergler. 2006. Mining wordnet for fuzzy sentiment: Sentiment tag extraction from wordnet glosses. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Domagoj Babic</author>
<author>Jesse Bingham</author>
<author>Alan J Hu</author>
</authors>
<title>B-cubing: New possibilities for efficient sat-solving.</title>
<date>2006</date>
<journal>TC,</journal>
<volume>55</volume>
<issue>11</issue>
<contexts>
<context position="20698" citStr="Babic et al., 2006" startWordPosition="3476" endWordPosition="3479"> to the synsets that agrees with that of the words? In other words, given the polarities of a subset of words (e.g., that given by one of the three SWDs) the problem of finding the polarities of the synsets that agree with this assignment is a “hard” problem. Theorem 1. The Consistent Polarity Assignment problem is NP-complete. 4 Polarity Consistency Checking To “exhaustively” solve the problem of finding the polarity inconsistencies in an SWD, we propose a solution that reduces an instance of the problem to an instance of CNF-SAT. We can then employ a fast SAT solver (e.g., (Xu et al., 2008; Babic et al., 2006)) to solve our problem. CNF-SAT is a decision problem of determining if there is an assignment of True and False to the variables of a Boolean formula 4) in conjunctive normal form (CNF) such that 4) evaluates to True. A formula is in CNF if it is a conjunction of one or more clauses, each of which is a disjunction of literals. CNF-SAT is a classic NP-complete problem, but, modern SAT solvers are capable of solving many practical instances of the problem. Since, in general, there is no easy way to tell the difficulty of a problem without trying it, SAT solvers include time-outs, so they will t</context>
</contexts>
<marker>Babic, Bingham, Hu, 2006</marker>
<rawString>Domagoj Babic, Jesse Bingham, and Alan J. Hu. 2006. B-cubing: New possibilities for efficient sat-solving. TC, 55(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In LREC,</title>
<date>2010</date>
<location>Valletta, Malta,</location>
<contexts>
<context position="2294" citStr="Baccianella et al., 2010" startWordPosition="326" endWordPosition="329">ura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD). They consist of words manually annotated with their corresponding polarities. The sentiment dictionaries have the following problems: • They exhibit substantial (intra-dictionary) inaccuracies. For example, the synset {Indo-European, Indo-Aryan, Aryan} (of or relating to the former Indo-European people), has a neg</context>
<context position="33688" citStr="Baccianella et al., 2010" startWordPosition="5853" endWordPosition="5856"> generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polarity consistency for sentiment word dictionaries. We pr</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In LREC, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Pamela Forner</author>
<author>Bernardo Magnini</author>
<author>Emanuele Pianta</author>
</authors>
<title>Revising the wordnet domains hierarchy: semantics, coverage and balancing.</title>
<date>2004</date>
<publisher>MLR.</publisher>
<contexts>
<context position="13206" citStr="Bentivogli et al., 2004" startWordPosition="2224" endWordPosition="2227">atisfies the following condition for each w E W: For p E {positive, negative}, -y(w) = p iff the sum of all f(w, s) such that (w, s) E E and -y(s) = fre�(w) p, is greater than 2 .Note that, for any w E W, -y(w) = neutral iff the above inequality is not satisfied for both values of p in {positive, negative}. We contend that our approach is applicable to do999 Table 1: Disagreement between dictionaries. Pairs of Word Polarity Disagreement Dictionaries Inconsistency Overlap OF &amp; GI 90 2,924 OF &amp; AL 73 1,150 GI &amp; AL 18 712 main dependent sentiment dictionaries, too. We can employ WordNet Domains (Bentivogli et al., 2004). WordNet Domains augments WordNet with domain labels. Hence, we can project the words/synsets in WordNet according to a domain label and then apply our methodology to the projection. 3 Inconsistency Classification Polarity inconsistencies are of two types: input and complex. We discuss them in this section. 3.1 Input Dictionaries Polarity Inconsistency Input polarity inconsistencies are of two types: intra-dictionary and inter-dictionary inconsistencies. The latter are obtained by comparing (1) two SWDs, (2) an SWD with an SSD and (3) two SSDs. 3.1.1 Intra-dictionary inconsistency An SWD may </context>
</contexts>
<marker>Bentivogli, Forner, Magnini, Pianta, 2004</marker>
<rawString>Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains hierarchy: semantics, coverage and balancing. MLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armin Biere</author>
</authors>
<date>2008</date>
<journal>PicoSAT essentials. JSAT,</journal>
<pages>4--2</pages>
<contexts>
<context position="28054" citStr="Biere, 2008" startWordPosition="4852" endWordPosition="4853">mapped back to its corresponding word(s). We obtain the corresponding subset of words W′, W′ ⊆ W. Suppose that Φ′ is the Boolean CNF formula for the words in W′. The set of clauses in Φ′ is a subset of those in Φ. Also, the clauses in the MUC appear in Φ′. Thus, Φ′ is unsatisfiable and the words in W′ are inconsistent. To find all inconsistent words we ought to generate all MUCs. Unfortunately, this is a “hard” problem (Dershowitz et al., 2006) and no open source SAT solver possesses this functionality. We however observe that the two SAT solvers we use for our experiments (SAT4j and PicoSAT (Biere, 2008)) return different MUCs for the same formula and we use them to find as many inconsistencies as possible. 6 Experiments The goal of the experimental study is to show that our techniques can identify considerable inconsistencies in various sentiment dictionaries. Table 3: Intra- and inter-dictionaries inconsistency POS OF QW GI QW AL QW UF QW Noun 23 119 4 61 0 42 90 140 Verb 66 113 2 67 0 0 63 137 Adj. 90 170 8 48 0 0 27 177 Adv. 61 1 0 0 2 0 69 1 Total 240 403 14 176 2 42 249 455 Data sets In our experiments, we use WordNet 3.0, GI, OF, AL and Q-WordNet. Their statistics are given in Table 2.</context>
</contexts>
<marker>Biere, 2008</marker>
<rawString>Armin Biere. 2008. PicoSAT essentials. JSAT, 4(2-4):75–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="1966" citStr="Breck et al., 2007" startWordPosition="278" endWordPosition="281"> For instance, a product with consistently good reviews is likely to sell well. The general approach is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientations of the individual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word diction</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-N-M</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: a case study on amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In WWW,</booktitle>
<pages>141--150</pages>
<marker>Danescu-N-M, Kossinets, Kleinberg, Lee, 2009</marker>
<rawString>Cristian Danescu-N.-M., Gueorgi Kossinets, Jon Kleinberg, and Lillian Lee. 2009. How opinions are received by online communities: a case study on amazon.com helpfulness votes. In WWW, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nachum Dershowitz</author>
<author>Ziyad Hanna</author>
<author>Er Nadel</author>
</authors>
<title>A scalable algorithm for minimal unsatisfiable core extraction. In</title>
<date>2006</date>
<booktitle>In Proc. SAT06.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="27058" citStr="Dershowitz et al., 2006" startWordPosition="4662" endWordPosition="4665">9 18,156 2,608 1,188 1,440 4050 Adv. 4,481 3,621 775 51 317 40 Total 155,287 117,659 6,791 3,961 1,759 15,499 1,581, for the disagreement-free union dictionary. 5 Detecting Inconsistencies In this section we describe how we detect the words with polarity inconsistencies using the output of a SAT solver. For an unsatisfiable formula, a modern SAT solver returns a minimal unsatisfiable core (MUC) from the original formula. An unsatisfiable core is minimal if it becomes satisfiable whenever any one of its clauses is removed. There are no known practical algorithms for computing the minimum core (Dershowitz et al., 2006). In our problem a MUC corresponds to a set of polarity inconsistent words. The argument is as follows. Consider W the set of words in a connected component and Φ the CNF formula generated with the above method. During the transformation we keep track of the clauses introduced in Φ by each word. Suppose Φ is inconsistent. Then, the SAT solver returns a MUC. Each clause in a MUC is mapped back to its corresponding word(s). We obtain the corresponding subset of words W′, W′ ⊆ W. Suppose that Φ′ is the Boolean CNF formula for the words in W′. The set of clauses in Φ′ is a subset of those in Φ. Al</context>
</contexts>
<marker>Dershowitz, Hanna, Nadel, 2006</marker>
<rawString>Nachum Dershowitz, Ziyad Hanna, and Er Nadel. 2006. A scalable algorithm for minimal unsatisfiable core extraction. In In Proc. SAT06. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
</authors>
<title>Resolving object and attribute coreference in opinion mining.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1986" citStr="Ding and Liu, 2010" startWordPosition="282" endWordPosition="285">duct with consistently good reviews is likely to sell well. The general approach is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientations of the individual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD). They co</context>
</contexts>
<marker>Ding, Liu, 2010</marker>
<rawString>Xiaowen Ding and Bing Liu. 2010. Resolving object and attribute coreference in opinion mining. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard C Dragut</author>
<author>Clement T Yu</author>
<author>A Prasad Sistla</author>
<author>Weiyi Meng</author>
</authors>
<title>Construction of a sentimental word dictionary.</title>
<date>2010</date>
<booktitle>In CIKM,</booktitle>
<pages>1761--1764</pages>
<contexts>
<context position="33634" citStr="Dragut et al., 2010" startWordPosition="5844" endWordPosition="5847">e, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polar</context>
</contexts>
<marker>Dragut, Yu, Sistla, Meng, 2010</marker>
<rawString>Eduard C. Dragut, Clement T. Yu, A. Prasad Sistla, and Weiyi Meng. 2010. Construction of a sentimental word dictionary. In CIKM, pages 1761–1764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss classification.</title>
<date>2005</date>
<booktitle>In CIKM,</booktitle>
<pages>617--624</pages>
<contexts>
<context position="33662" citStr="Esuli and Sebastiani, 2005" startWordPosition="5848" endWordPosition="5852">call however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polarity consistency for sentimen</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classification. In CIKM, pages 617–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiawei Han</author>
</authors>
<title>Data Mining: Concepts and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="7765" citStr="Han, 2005" startWordPosition="1204" endWordPosition="1205">twork N is quadruple (W, S, £, f) where W is a finite set of words, S is a finite set of synsets, £ is a set of undirected edges between elements in W and S, i.e., £ C W x S and f is a function assigning a positive integer to each element in £. For an edge (w, s), f(w, s) is called the frequency of use of w in the sense given by s. For any word w and synset s, we say that s is a synset of w if (w, s) E £. Also, for any word w, we let freq(w) denote the sum of all f(w, s) such that (w, s) E £. If a synset has a 0 frequency of use we replace it with 0.1, which is a standard smoothing technique (Han, 2005). For instance, the word cheap has four senses. The frequencies of occurrence of the word in the four senses are f1 = 9, f2 = 1, f3 = 1 and f4 = 0, respectively. By smoothing, f4 = 0.1. Hence, freq(cheap) = f1 + f2 + f3 + f4 = 11.1. The relative frequency of the synset in the first sense of cheap, which denotes the probability that the word is used in the first sense, is f� freq��heap) = 9 = 0.81. 11.1 998 2.2 Consistent Polarity Assignment We assume that each synset has a unique polarity. We define the polarity of a word to be a discrete probability distribution: P+, P_, P0 with P++P_+ P0 = 1</context>
</contexts>
<marker>Han, 2005</marker>
<rawString>Jiawei Han. 2005. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In ACL,</booktitle>
<pages>174--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33466" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="5817" endWordPosition="5821"> takes about 4 hours if we limit the set Figure 1: Human classification of (in)consistent words. of words to those that have up to 200,000 MDSs. EEM could not handle words such as make, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment d</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In ACL, pages 174–181, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In ACM SIGKDD,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="33706" citStr="Hu and Liu, 2004" startWordPosition="5857" endWordPosition="5860">not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polarity consistency for sentiment word dictionaries. We proved that this pro</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In ACM SIGKDD, pages 168– 177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kamps</author>
<author>M Marx</author>
<author>R Mokken</author>
<author>M de Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<marker>Kamps, Marx, Mokken, de Rijke, 2004</marker>
<rawString>J. Kamps, M. Marx, R. Mokken, and M. de Rijke. 2004. Using wordnet to measure semantic orientation of adjectives. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>355--363</pages>
<publisher>Association for</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33495" citStr="Kanayama and Nasukawa, 2006" startWordPosition="5822" endWordPosition="5825"> set Figure 1: Human classification of (in)consistent words. of words to those that have up to 200,000 MDSs. EEM could not handle words such as make, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 355–363, Stroudsburg, PA, USA. Association for Computational Linguistics. M. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="9177" citStr="Kim and Hovy, 2006" startWordPosition="1467" endWordPosition="1470">arity distribution P+ = 0.81, P_ = 0.19 and P0 = 0. The polarity distribution of a word is estimated using the polarities of its underlying synsets. For instance cheap has four senses, with the first sense being positive and the last three senses being negative. The probability that the word expresses a negative sentiment is P_ = f2+f3+f4 = 0.19, while the probaf req (cheap) bility that the word expresses a positive sentiment is P+ = f� freq(cheap) = 0.81. P0 = 1 − P+ − P_ = 0. Our view of characterizing the polarity of a word using a polarity distribution is shared with other previous works (Kim and Hovy, 2006; Andreevskaia and Bergler, 2006). Nonetheless, we depart from these works in the following key aspect. We say that a word has a (mostly) positive (negative) polarity if the majority sense of the word is positive (negative). That is, a word has a mostly positive polarity if P+ &gt; P_ + P0 and it has a mostly negative polarity if P_ &gt; P+ + P0. Or, equivalently, if P+ &gt; 21 or P_ &gt; 12, respectively. For example, on majority, cheap conveys positive polarity since P+ = .081 &gt; 12, i.e., the majority sense of the word cheap has positive connotation. Based on this study, we contend that GI, OF and AL ta</context>
<context position="33746" citStr="Kim and Hovy, 2006" startWordPosition="5865" endWordPosition="5868">e taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polarity consistency for sentiment word dictionaries. We proved that this problem is NP-complete. We showed that in p</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Identifying and analyzing judgment opinions. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>In Nitin Indurkhya</booktitle>
<pages>978--1420085921</pages>
<editor>and Fred J. Damerau, editors,</editor>
<publisher>CRC Press, Taylor</publisher>
<contexts>
<context position="1986" citStr="Liu, 2010" startWordPosition="284" endWordPosition="285"> consistently good reviews is likely to sell well. The general approach is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientations of the individual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD). They co</context>
<context position="33346" citStr="Liu, 2010" startWordPosition="5802" endWordPosition="5803">MDSs: for UF we left the SAT solver running for a week without ever terminating. In contrast, it takes about 4 hours if we limit the set Figure 1: Human classification of (in)consistent words. of words to those that have up to 200,000 MDSs. EEM could not handle words such as make, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexi</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group, Boca Raton, FL. ISBN 978-1420085921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1614" citStr="Pang and Lee, 2004" startWordPosition="223" endWordPosition="226">he satisfiability problem and utilize a fast SAT solver to detect inconsistencies in a sentiment dictionary. We perform experiments on four sentiment dictionaries and WordNet. 1 Introduction The opinions expressed in various Web and media outlets (e.g., blogs, newspapers) are an important yardstick for the success of a product or a government policy. For instance, a product with consistently good reviews is likely to sell well. The general approach is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientations of the individual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal </context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Expanding domain sentiment lexicon through double propagation.</title>
<date>2009</date>
<booktitle>In IJCAI,</booktitle>
<pages>1199--1204</pages>
<contexts>
<context position="33513" citStr="Qiu et al., 2009" startWordPosition="5826" endWordPosition="5829">cation of (in)consistent words. of words to those that have up to 200,000 MDSs. EEM could not handle words such as make, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inco</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2009</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double propagation. In IJCAI, pages 1199–1204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="33774" citStr="Rao and Ravichandran, 2009" startWordPosition="5869" endWordPosition="5872">mial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polarity consistency for sentiment word dictionaries. We proved that this problem is NP-complete. We showed that in practice polarity inconsisten</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stone</author>
<author>D Dunphy</author>
<author>M Smith</author>
<author>J Ogilvie</author>
</authors>
<title>The general inquirer: A computer approach to content analysis. In</title>
<date>1996</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2159" citStr="Stone et al., 1996" startWordPosition="306" endWordPosition="309">y analysis of the orientations of the individual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD). They consist of words manually annotated with their corresponding polarities. The sentiment dictionaries have the following problems: • They exhibit substantial (intra-dictionary) </context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1996</marker>
<rawString>P. Stone, D. Dunphy, M. Smith, and J. Ogilvie. 1996. The general inquirer: A computer approach to content analysis. In MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
<author>J Grieve</author>
</authors>
<title>Analyzing appraisal automatically. In</title>
<date>2004</date>
<publisher>AAAI Spring Symposium.</publisher>
<contexts>
<context position="2253" citStr="Taboada and Grieve, 2004" startWordPosition="321" endWordPosition="324">. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD). They consist of words manually annotated with their corresponding polarities. The sentiment dictionaries have the following problems: • They exhibit substantial (intra-dictionary) inaccuracies. For example, the synset {Indo-European, Indo-Aryan, Aryan} (of or relating to th</context>
</contexts>
<marker>Taboada, Grieve, 2004</marker>
<rawString>M. Taboada and J. Grieve. 2004. Analyzing appraisal automatically. In AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="1686" citStr="Takamura et al., 2005" startWordPosition="235" endWordPosition="238">consistencies in a sentiment dictionary. We perform experiments on four sentiment dictionaries and WordNet. 1 Introduction The opinions expressed in various Web and media outlets (e.g., blogs, newspapers) are an important yardstick for the success of a product or a government policy. For instance, a product with consistently good reviews is likely to sell well. The general approach is to summarize the semantic polarity (i.e., positive or negative) of sentences/documents by analysis of the orientations of the individual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al</context>
<context position="33798" citStr="Takamura et al., 2005" startWordPosition="5873" endWordPosition="5876">words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies within individual dictionaries and across dictionaries. 8 Conclusion We studied the problem of checking polarity consistency for sentiment word dictionaries. We proved that this problem is NP-complete. We showed that in practice polarity inconsistencies of words both withi</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In ACL, pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>735--740</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="33527" citStr="Wiebe, 2000" startWordPosition="5830" endWordPosition="5831">istent words. of words to those that have up to 200,000 MDSs. EEM could not handle words such as make, give and break. Recall however that we did not generate all MUCs. We do not know how long would that might have taken. (The polynomial method handles all the words in WordNet and it takes 5GB of memory and about 2 hours to finish.) 7 Related Work Several researchers have studied the problem of finding opinion words (Liu, 2010). There are two lines of work on sentiment polarity lexicon induction: corpora-based (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2009; Wiebe, 2000) and dictionary-based (Andreevskaia and Bergler, 2006; Agerri and Garc´ıaSerrano, 2010; Dragut et al., 2010; Esuli and Sebastiani, 2005; Baccianella et al., 2010; Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2006; Rao and Ravichandran, 2009; Takamura et al., 2005). Our work falls into the latter. Most of these works use the lexical relations defined in WordNet (e.g., synonym, antonym) to derive sentiment lexicons. To our knowledge, none of the earlier works studied the problem of polarity consistency checking for a sentiment dictionary. Our techniques can pinpoint the inconsistencies wi</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 735–740. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="2202" citStr="Wilson et al., 2005" startWordPosition="313" endWordPosition="317">vidual words (Pang and Lee, 2004; Danescu-N.-M. et al., 2009; Kim and Hovy, 2004; Takamura et al., 2005). Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. (Breck et al., 2007; Ding and Liu, 2010; Kim and Hovy, 2004). Several domain independent sentiment dictionaries have been manually or (semi)-automatically created, e.g., General Inquirer (GI) (Stone et al., 1996), Opinion Finder (OF) (Wilson et al., 2005), Appraisal Lexicon (AL) (Taboada and Grieve, 2004), SentiWordNet (Baccianella et al., 2010) and Q-WordNet (Agerri and Garcia-Serrano, 2010). Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in WordNet according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD). They consist of words manually annotated with their corresponding polarities. The sentiment dictionaries have the following problems: • They exhibit substantial (intra-dictionary) inaccuracies. For example, the synset {Indo</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Xu</author>
<author>Frank Hutter</author>
<author>Holger H Hoos</author>
<author>Kevin Leyton-Brown</author>
</authors>
<title>Satzilla: portfolio-based algorithm selection for sat.</title>
<date>2008</date>
<journal>J. Artif. Int. Res.,</journal>
<pages>32--565</pages>
<contexts>
<context position="20677" citStr="Xu et al., 2008" startWordPosition="3472" endWordPosition="3475">ent of polarities to the synsets that agrees with that of the words? In other words, given the polarities of a subset of words (e.g., that given by one of the three SWDs) the problem of finding the polarities of the synsets that agree with this assignment is a “hard” problem. Theorem 1. The Consistent Polarity Assignment problem is NP-complete. 4 Polarity Consistency Checking To “exhaustively” solve the problem of finding the polarity inconsistencies in an SWD, we propose a solution that reduces an instance of the problem to an instance of CNF-SAT. We can then employ a fast SAT solver (e.g., (Xu et al., 2008; Babic et al., 2006)) to solve our problem. CNF-SAT is a decision problem of determining if there is an assignment of True and False to the variables of a Boolean formula 4) in conjunctive normal form (CNF) such that 4) evaluates to True. A formula is in CNF if it is a conjunction of one or more clauses, each of which is a disjunction of literals. CNF-SAT is a classic NP-complete problem, but, modern SAT solvers are capable of solving many practical instances of the problem. Since, in general, there is no easy way to tell the difficulty of a problem without trying it, SAT solvers include time</context>
</contexts>
<marker>Xu, Hutter, Hoos, Leyton-Brown, 2008</marker>
<rawString>Lin Xu, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. Satzilla: portfolio-based algorithm selection for sat. J. Artif. Int. Res., 32:565–606, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>