<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001437">
<title confidence="0.985757">
Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression
</title>
<author confidence="0.756778">
Florian Boudin and Emmanuel Morin
</author>
<address confidence="0.216233">
LINA - UMR CNRS 6241, Universit´e de Nantes, France
</address>
<email confidence="0.575774">
{florian.boudin,emmanuel.morin}@univ-nantes.fr
</email>
<sectionHeader confidence="0.98459" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9980765">
Multi-Sentence Compression (MSC) is the
task of generating a short single sentence sum-
mary from a cluster of related sentences. This
paper presents an N-best reranking method
based on keyphrase extraction. Compression
candidates generated by a word graph-based
MSC approach are reranked according to the
number and relevance of keyphrases they con-
tain. Both manual and automatic evaluations
were performed using a dataset made of clus-
ters of newswire sentences. Results show that
the proposed method significantly improves
the informativity of the generated compres-
sions.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999847192307693">
Multi-Sentence Compression (MSC) can be broadly
described as the task of generating a short single sen-
tence summary from a cluster of related sentences.
It has recently attracted much attention, mostly be-
cause of its relevance to single or multi-document
extractive summarization. A standard way to gen-
erate summaries consists in ranking sentences by
importance, cluster them by similarity and select a
sentence from the top ranked clusters (Wang et al.,
2008). One difficulty is then to generate concise,
non-redundant summaries. Selected sentences al-
most always contain additional information specific
to the documents from which they came, leading to
readability issues in the summary.
Sentence Compression (SC), i.e. the task of
summarizing a sentence while retaining most of
the informational content and remaining grammat-
ical (Jing, 2000), is a straightforward solution to this
problem. Another solution would be to create, for
each cluster of related sentences, a concise and flu-
ent fusion of information, reflecting facts common
to all sentences. Originally defined as sentence fu-
sion (Barzilay and McKeown, 2005), MSC is a text-
to-text generation process in which a novel sentence
is produced as a result of summarizing common in-
formation across a set of similar sentences.
Most of the previous MSC approaches rely on
syntactic parsers for producing grammatical com-
pressions, e.g. (Filippova and Strube, 2008; El-
sner and Santhanam, 2011). Recently, (Filippova,
2010) proposed a word graph-based approach which
only requires a Part-Of-Speech (POS) tagger and a
list of stopwords. The key assumption behind her
approach is that redundancy within the set of related
sentences provides a reliable way of generating in-
formative and grammatical sentences. Although this
approach seemingly works well, 48% to 60% of the
generated sentences are missing important informa-
tion about the set of related sentences. In this study,
we aim at producing more informative sentences by
maximizing the range of topics they cover.
Keyphrases are words that capture the main top-
ics of a document. Extracting keyphrases can benefit
various Natural Language Processing tasks such as
summarization, information retrieval and question-
answering (Kim et al., 2010). In summarization,
keyphrases provide semantic metadata that represent
the content of a document. Sentences containing the
most relevant keyphrases are used to generate the
summary (D’Avanzo and Magnini, 2005). In the
same way, we hypothesize that keyphrases can be
used to better generate sentences that convey the gist
</bodyText>
<page confidence="0.965714">
298
</page>
<note confidence="0.4721245">
Proceedings of NAACL-HLT 2013, pages 298–305,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.855524727272727">
of the set of related sentences.
In this paper, we present a reranking method
of N-best multi-sentence compressions based on
keyphrase extraction and describe a series of experi-
ments conducted on a manually constructed evalua-
tion corpus. More precisely, the main contributions
of our work are as follows:
• We extend Filippova (2010)’s word graph-
based MSC approach to produce well-
punctuated and more informative compres-
sions.
</bodyText>
<listItem confidence="0.972416166666667">
• We investigate the use of automatic Machine
Translation (MT) and summarization evalua-
tion metrics to evaluate MSC performance.
• We introduce a French evaluation dataset made
of 40 sets of related sentences along with refer-
ence compressions composed by humans.
</listItem>
<bodyText confidence="0.999888">
The rest of this paper is organized as follows. We
first briefly review the previous work, followed by
a description of the method we propose. Next, we
give the details of the evaluation dataset we have
constructed and present our experiments and results.
Lastly, we conclude with a discussion and directions
for further work.
</bodyText>
<sectionHeader confidence="0.999822" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.995232">
2.1 Multi-sentence compression
</subsectionHeader>
<bodyText confidence="0.999967806451613">
MSC have received much attention recently and
many different approaches have been proposed. The
pioneering work of (Barzilay and McKeown, 2005)
introduced the framework used by many subsequent
works: input sentences are represented by depen-
dency trees, some words are aligned to merge the
trees into a lattice, and the lattice is linearized using
tree traversal to produce fusion sentences. (Filip-
pova and Strube, 2008) cast MSC as an integer linear
program, and show promising results for German.
Later, (Elsner and Santhanam, 2011) proposed a su-
pervised approach trained on examples of manually
fused sentences.
Previously described approaches require the use
of a syntactic parser to control the grammatical-
ity of the output. As an alternative, several word
graph-based approaches that only require a POS
tagger were proposed. The key assumption is
that redundancy provides a reliable way of gen-
erating grammatical sentences. First, a directed
word graph is constructed from the set of input sen-
tences in which nodes represent unique words, de-
fined as word and POS tuples, and edges express
the original structure of sentences (i.e. word order-
ing). Sentence compressions are obtained by find-
ing commonly used paths in the graph. Word graph-
based MSC approaches were used in different tasks,
such as guided microblog summarization (Sharifi
et al., 2010), opinion summarization (Ganesan et
al., 2010) and newswire summarization (Filippova,
2010).
</bodyText>
<subsectionHeader confidence="0.997753">
2.2 Keyphrase extraction
</subsectionHeader>
<bodyText confidence="0.9998904">
Keyphrases are words that are representative of the
main content of documents. Extracting keyphrases
can benefit various Natural Language Processing
tasks such as summarization, information retrieval
and question-answering (Kim et al., 2010). Previ-
ous works fall into two categories: supervised and
unsupervised methods. The idea behind supervised
methods is to recast keyphrase extraction as a binary
classification task. A model is trained using anno-
tated data to determine whether a given phrase is a
keyphrase or not (Frank et al., 1999; Turney, 2000).
Unsupervised approaches proposed so far have in-
volved a number of techniques, including language
modeling (Tomokiyo and Hurst, 2003), graph-based
ranking (Mihalcea and Tarau, 2004; Wan and Xiao,
2008) and clustering (Liu et al., 2009). While super-
vised approaches have generally proven more suc-
cessful, the need for training data and the bias to-
wards the domain on which they are trained remain
two critical issues.
</bodyText>
<sectionHeader confidence="0.97924" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9998105">
In this section, we first describe Filippova (2010)’s
word graph-based MSC approach. Then, we present
the keyphrase extraction approach we use and our
method for reranking generated compressions.
</bodyText>
<subsectionHeader confidence="0.999874">
3.1 Description of Filippova’s approach
</subsectionHeader>
<bodyText confidence="0.999982">
Let G = (V, E) be a directed graph with the set
of vertices (nodes) V and a set of directed edges E,
where E is a subset of V x V. Given a set of re-
lated sentences S = {s1, s2, ..., sn}, a word graph
is constructed by iteratively adding sentences to it.
</bodyText>
<page confidence="0.998926">
299
</page>
<figureCaption confidence="0.999475">
Figure 1: Word graph constructed from the set of related sentences, a possible compression path is also given.
</figureCaption>
<figure confidence="0.999861448275862">
to be
(3)
...
the
of
his
island
pinta
&apos;s
last kind
giant
believed
world
the
the
passed
away
-start-
tortoise
known
has
-end-
lonesome
as
died
george
a
...
(2)
</figure>
<figureCaption confidence="0.828187">
Figure 1 is an illustration of the word graph con-
</figureCaption>
<bodyText confidence="0.940564333333333">
structed from the following sentences. For clarity,
edge weights are omitted and italicized fragments
from the sentences are replaced with dots.
</bodyText>
<listItem confidence="0.9955832">
1. Lonesome George, the world’s last Pinta Island
giant tortoise, has passed away.
2. The giant tortoise known as Lonesome George
died Sunday at the Galapagos National Park in
Ecuador.
3. He was only about a hundred years old, but
the last known giant Pinta tortoise, Lonesome
George, has passed away.
4. Lonesome George, a giant tortoise believed to
be the last of his kind, has died.
</listItem>
<bodyText confidence="0.999869925">
At the first step, the graph simply represents one
sentence plus the start and end symbols (–start– and
–end– in Figure 1). A node is added to G for each
word in the sentence, and words adjacent in the sen-
tence are connected with directed edges. A word
from the following sentences is mapped onto an ex-
isting node in the graph if they have the same lower-
cased word form and POS and that no word from this
sentence has already been mapped onto this node. A
new node is created if there is no suitable candidate
in the graph.
Words are added to the graph in the following or-
der:
i. non-stopwords for which no candidate exists in
the graph or for which an unambiguous map-
ping is possible;
ii. non-stopwords for which there are either sev-
eral possible candidates in the graph or which
occur more than once in the sentence;
iii. stopwords.
For the last two groups of words where mapping
is ambiguous (i.e. there are two or more nodes in
the graph that refer to the same word/POS tuple),
the immediate context (the preceding and following
words in the sentence and the neighboring nodes in
the graph) or the frequency (i.e. the node which has
words mapped onto it) are used to select the candi-
date node. We use the stopword list included in nltk1
extended with temporal nouns (e.g. monday, yester-
day).
In Filippova’s approach, punctuation marks are
excluded. To generate well-punctuated compres-
sions, we simply added a fourth step for adding
punctuation marks in the graph. When mapping is
ambiguous, we select the candidate which has the
same immediate context.
Once the words from a sentence are added to the
graph, words adjacent in the sentence are connected
with directed edges. Edge weights are calculated us-
ing the weighting function defined in Equation 1.
</bodyText>
<footnote confidence="0.942552">
1http://nltk.org/
</footnote>
<page confidence="0.886304">
300
</page>
<equation confidence="0.993137833333333">
cohesion(i,j)
w(i,j) = (1)
freq (i) x freq (j)
freq(i) + freq(j)
cohesion(i, j) = (2)
LsES d(s,i,j)−
</equation>
<bodyText confidence="0.998212722222222">
where freq(i) is the number of words mapped to the
node i. The function d(s, i, j) refers to the distance
between the offset positions of words i and j in sen-
tence s.
The purpose of this function is two fold: i. to
generate a grammatical compression, links between
words which appear often in this order are favored
(see Equation 2); ii. to generate an informative com-
pression, the weight of edges connecting salient
nodes is decreased.
A K-shortest paths algorithm is then used to find
the 50 shortest paths from start to end nodes in the
graph. Paths shorter than eight words or that do not
contain a verb are filtered. The remaining paths are
reranked by normalizing the total path weight over
its length. The path which has the lightest average
edge weight is then considered as the best compres-
sion.
</bodyText>
<subsectionHeader confidence="0.99976">
3.2 Reranking paths using keyphrases
</subsectionHeader>
<bodyText confidence="0.999936138888889">
The main difficulty of MSC is to generate sentences
that are both informative and grammatically correct.
Here, redundancy within the set of input sentences
is used to identify important words and salient links
between words. Although this approach seemingly
works well, important information is missing in 48%
to 60% of the generated sentences (Filippova, 2010).
One of the reasons for this is that node salience
is estimated only with the frequency measure. To
tackle this issue, we propose to rerank the N-best list
of compressions using keyphrases extracted from
the set of related sentences. Intuitively, an infor-
mative sentence should contain the most relevant
keyphrases. We propose to rerank generated com-
pressions according to the number and relevance of
keyphrases they contain.
An unsupervised method based on (Wan and
Xiao, 2008) is used to extract keyphrases from each
set of related sentences. This method is based on
the assumption that a word recommends other co-
occurring words, and the strength of the recommen-
dation is recursively computed based on the im-
portance of the words making the recommendation.
Keyphrase extraction can be divided into two steps.
First, a weighted graph is constructed from the set
of related sentences, in which nodes represent words
defined as word and POS tuples. Two nodes (words)
are connected if their corresponding lexical units co-
occur within a sentence. Edge weights are the num-
ber of times two words co-occur. TextRank (Mihal-
cea and Tarau, 2004), a graph-based ranking algo-
rithm that takes into account edge weights, is ap-
plied for computing a salience score for each node.
The score for node Vi is initialized with a default
value and is computed in an iterative manner until
convergence using this equation:
</bodyText>
<equation confidence="0.985847333333333">
Y_ wji S(Vi)
S(Vi) = (1−d)+d� LVkEadj(Vi) wjk
ViEadj(Vi)
</equation>
<bodyText confidence="0.999977538461538">
where adj(Vi) denotes the neighbors of Vi and d is
the damping factor set to 0.85.
The second step consists in generating and scor-
ing keyphrase candidates. Sequences of adja-
cent words satisfying a specific syntactic pattern
are collapsed into multi-word phrases. We use
(ADJ)*(NPP NC)+(ADJ)* for French, in which
ADJ are adjectives, NPP are proper nouns and NC
are common nouns.
The score of a candidate keyphrase k is computed
by summing the salience scores of the words it con-
tains normalized by its length + 1 to favor longer
n-grams (see equation 3).
</bodyText>
<equation confidence="0.9996425">
score(k) = LwEk TextRank(w) (3)
length(k) + 1
</equation>
<bodyText confidence="0.999956">
The small vocabulary size as well as the high
redundancy within the set of related sentences are
two factors that make keyphrase extraction easier
to achieve. On the other hand, a large number
of the generated keyphrases are redundant. Some
keyphrases may be contained within larger ones,
e.g. giant tortoise and Pinta Island giant tortoise. To
solve this problem, generated keyphrases are clus-
tered using word overlap. For each cluster, we then
select the keyphrase with the highest score. This fil-
tering process enables the generation of a smaller
subset of keyphrases while having a better coverage
of the cluster content.
</bodyText>
<page confidence="0.994854">
301
</page>
<bodyText confidence="0.9993877">
Reranking techniques can suffer from the limited
scope of the N-best list, which may rule out many
potentially good candidates. For this reason, we use
a larger number of paths than the one in (Filippova,
2010). Accordingly, the K-shortest paths algorithm
is used to find the 200 shortest paths. We rerank the
paths by normalizing the total path weight over its
length multiplied by the sum of keyphrase scores it
contains. The score of a sentence compression c is
given by:
</bodyText>
<equation confidence="0.9996615">
score(c) = F—i,iEpath(c) w(i,i) (4)
length(c) x F—kEc score(k)
</equation>
<sectionHeader confidence="0.999546" genericHeader="method">
4 Experimental settings
</sectionHeader>
<subsectionHeader confidence="0.999605">
4.1 Construction of the evaluation dataset
</subsectionHeader>
<bodyText confidence="0.999980933333333">
To our knowledge, there is no dataset available to
evaluate MSC in an automatic way. The perfor-
mance of the previously described approaches was
assessed by human judges. In this work, we intro-
duce a new evaluation dataset made of 40 sets of re-
lated sentences along with reference compressions
composed by human assessors. The purpose of this
dataset is to investigate the use of existing automatic
evaluation metrics for the MSC task.
Similar to (Filippova, 2010), we collected news
articles presented in clusters on the French edition of
Google News2 over a period of three months. Clus-
ters composed of at least 20 news articles and con-
taining one single prevailing event were manually
selected. To obtain the sets of related sentences, we
extracted the first sentences from each article in the
cluster, removing duplicates. Leading sentences in
news articles are known to provide a good summary
of the article content and are used as a baseline in
summarization (Dang, 2005).
The resulting dataset contains 618 sentences (33
tokens on average) spread over 40 clusters. The
number of sentences within each cluster is on av-
erage 15, with a minimum of 7 and a maximum of
36. The word redundancy rate within the dataset,
computed as the number of unique words over the
number of words for each cluster, is 38.8%.
Three reference compressions were manually
composed for each set of sentences. Human an-
notators, all native French speakers, were asked to
</bodyText>
<footnote confidence="0.793224">
2http://news.google.fr
</footnote>
<bodyText confidence="0.999916777777778">
carefully read the set of sentences, extract the most
salient facts and generate a sentence (compression)
that summarize the set of sentences. Annotators
were also told to introduce as little new vocabu-
lary as possible in their compressions. The purpose
of this guideline is to reduce the number of possi-
ble mismatches, as existing evaluation metrics are
based on n-gram comparison. Reference compres-
sions have a compression rate of 60%.
</bodyText>
<subsectionHeader confidence="0.995749">
4.2 Automatic evaluation
</subsectionHeader>
<bodyText confidence="0.999969782608696">
The use of automatic methods for evaluating
machine-generated text has gradually become the
mainstream in Computational Linguistics. Well
known examples are the ROUGE (Lin, 2004) and
BLEU (Papineni et al., 2002) evaluation metrics used
in the summarization and MT communities. These
metrics assess the quality of a system output by com-
puting its similarity to one or more human-generated
references.
Prior work in sentence compression use the F1
measure over grammatical relations to evaluate can-
didate compressions (Riezler et al., 2003). It was
shown to correlate significantly with human judg-
ments (Clarke and Lapata, 2006) and behave sim-
ilarly to BLEU (Unno et al., 2006). However,
this metric is not entirely reliable as it depends on
parser accuracy and the type of dependency relations
used (Napoles et al., 2011). In this work, the fol-
lowing evaluation measures are considered relevant:
BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams)
and ROUGE-SU4 (bigrams with skip distance up to
4 words)4. ROUGE measures are computed using
stopword removal and French stemming 5.
</bodyText>
<subsectionHeader confidence="0.999364">
4.3 Manual evaluation
</subsectionHeader>
<bodyText confidence="0.999843428571429">
The quality of the generated compressions was as-
sessed in an experiment with human raters. Two as-
pects were considered: grammaticality and informa-
tivity. Following previous work (Barzilay and McK-
eown, 2005), we asked raters to assess grammati-
cality on a 3-points scale: perfect (2 pts), if the com-
pression is a complete grammatical sentence; almost
</bodyText>
<footnote confidence="0.9938148">
3ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a.pl
4We use the version 1.5.5 of the ROUGE package available
from http://www.berouge.com
5http://snowball.tartarus.org/
</footnote>
<page confidence="0.997881">
302
</page>
<bodyText confidence="0.9989798">
(1 pt), if it requires minor editing, e.g. one mistake
in articles, agreement or punctuation; ungrammati-
cal (0 pts), if it is none of the above. Raters were ex-
plicitly asked to ignore lack of capitalization while
evaluating grammaticality.
Informativity is evaluated according to the 3-
points scale defined in (Filippova, 2010): perfect (2
pts), if the compression conveys the gist of the main
event and is more or less like the summary the per-
son would produce himself; related (1 pt), if it is
related to the the main theme but misses something
important; unrelated (0 pts), if the compression is
not related to the main theme.
Three raters, all native French speakers, were
hired to assess the generated compressions.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999262">
To evaluate the effectiveness of our method, we
compare the compressions generated with Filip-
pova’s approach (denoted as baseline) against the
ones obtained by reranking paths using keyphrases
(denoted as KeyRank). We evaluated the agreement
between the three raters using Fleiss’s kappa (Art-
stein and Poesio, 2008). The K value is 0.56 which
denotes a moderate agreement.
Table 1 presents the average grammaticality and
informativity scores. Results achieved by the base-
line are consistent with the ones presented in (Fil-
ippova, 2010). We observe a significant improve-
ment in informativity for KeyRank. Grammaticality
scores are, however, slightly decreased. One reason
for that is the reranking we added to the shortest path
method that outputs longer compressions. The aver-
age length for our method is nevertheless drastically
shorter than the average length of the input sentences
(19 vs. 33 tokens). This corresponds to a compres-
sion rate (58%) that is close to the one observed on
reference compressions (60%).
Table 2 shows the distributions over the three
scores for both grammaticality and informativity.
We observe that 97.5% of the compressions gener-
ated with KeyRank are related to the main theme
of the cluster, and 62.5% convey the very gist of
it without missing any important information. This
represents an absolute increase of 19.2% over the
baseline. Although our reranking method has lower
grammaticality scores, 65% of the generated sen-
</bodyText>
<table confidence="0.948754">
Length
Method Gram. Info. CompR
Avg. Std.Dev.
Baseline 1.63 1.33 16.3 4.8 50%
KeyRank 1.53 1.601 19 6.1 58%
</table>
<tableCaption confidence="0.99574925">
Table 1: Average ratings over all clusters and raters along
with average compression length (in tokens), standard de-
viation and corresponding compression rate (t indicates
significance at the 0.01 level using Student’s t-test).
</tableCaption>
<table confidence="0.974696333333333">
tences are perfectly grammatical.
Gram. Info.
Method
0 1 2 0 1 2
Baseline 9.2% 18.3% 72.5% 10.0% 46.7% 43.3%
KeyRank 11.7% 23.3% 65.0% 2.5% 35.0% 62.5%
</table>
<tableCaption confidence="0.913707666666667">
Table 2: Distribution over possible manual ratings for
grammaticality and informativity. Ratings are expressed
on a scale of 0 to 2.
</tableCaption>
<bodyText confidence="0.999216384615385">
Table 3 shows the performance of the baseline
and our reranking method in terms of ROUGE and
BLEU scores. KeyRank significantly outperforms
the baseline according to the different ROUGE met-
rics. This indicates an improvement in informativity
for the compressions generated using our method.
We observe a large but not significant increase in
BLEU scores. The slightly decreased grammatical-
ity scores could be a reason for this. BLEU is essen-
tially a precision metric, and it measures how well a
compression candidate overlaps with multiple refer-
ences. Longer n-grams used by BLEU6 tend to score
for grammaticality rather than content.
</bodyText>
<table confidence="0.9985626">
Metric Baseline KeyRank
ROUGE-1 0.57441 0.65677‡
ROUGE-2 0.39212 0.44140†
ROUGE-SU4 0.37004 0.43443‡
BLEU 0.61560 0.65770
</table>
<tableCaption confidence="0.9657">
Table 3: Automatic evaluation scores (t and t indicate
significance at the 0.01 and 0.001 levels respectively us-
ing Student’s t-test)
</tableCaption>
<bodyText confidence="0.506417">
To assess the effectiveness of automatic evalua-
</bodyText>
<footnote confidence="0.923022">
6BLEU measures are computed using 4-grams.
</footnote>
<page confidence="0.99897">
303
</page>
<bodyText confidence="0.997953714285714">
tion metrics, we compute the Pearson’s correlation
coefficient between ROUGE and BLEU scores and
averaged manual ratings. According to Table 4, re-
sults show medium to strong correlation between
ROUGE scores and informativity ratings. On the
other hand, BLEU scores better correlate with gram-
maticality ratings. Overall, automatic evaluation
metrics are not highly correlated with manual rat-
ings. One reason for that may be that the manual
score assignments are arbitrary (i.e. 0, 1, 2), and that
a score of one is in fact closer to two than to zero.
Results suggest that automatic metrics do give an in-
dication of the compression quality, but can not re-
place manual evaluation.
</bodyText>
<table confidence="0.9986754">
Metric Gram. Info.
ROUGE-1 0.402 0.591
ROUGE-2 0.432 0.494
ROUGE-SU4 0.386 0.542
BLEU 0.444 0.401
</table>
<tableCaption confidence="0.9966295">
Table 4: Pearson correlation coefficients for automatic
metrics vs. average human ratings.
</tableCaption>
<sectionHeader confidence="0.997465" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999871789473684">
This paper presented a multi-sentence compres-
sion approach that uses keyphrases to generate
more informative compressions. We extended Fil-
ippova (2010)’s word graph-based MSC approach
by adding a re-reranking step that favors compres-
sions that contain the most relevant keyphrases of
the input sentence set. An implementation of the
proposed multi-sentence compression approach is
available for download7. We constructed an eval-
uation dataset made of 40 sets of related sentences
along with reference compressions composed by hu-
mans. This dataset is freely available for download8.
We performed both manual and automatic evalua-
tions and showed that our method significantly im-
proves the informativity of the generated compres-
sions. We also investigated the correlation between
manual and automatic evaluation metrics and found
that ROUGE and BLEU have a medium correlation
with manual ratings.
</bodyText>
<footnote confidence="0.999709">
7https://github.com/boudinfl/takahe
8https://github.com/boudinfl/lina-msc
</footnote>
<bodyText confidence="0.9997758">
In future work, we intend to examine how gram-
maticality of the generated compressions can be en-
hanced. Similar to the work of Hasan et al. (2006) in
the Machine Translation field, we plan to experiment
with high order POS language models reranking.
</bodyText>
<sectionHeader confidence="0.994554" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99973125">
The authors would like to thank Sebasti´an Pe˜na Sal-
darriaga and Oph´elie Lacroix for helpful comments
on this work. We thank the anonymous reviewers for
their useful comments. This work was supported by
the French Agence Nationale de la Recherche under
grant ANR-12-CORD-0027 and by the French Re-
gion Pays de Loire in the context of the DEPART
project (http://www.projetdepart.org/).
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999030272727273">
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555–596.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297–328.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 377–384, Sydney, Australia, July. Association
for Computational Linguistics.
Hoa Trang Dang. 2005. Overview of duc 2005. In Pro-
ceedings of the Document Understanding Conference.
Ernesto D’Avanzo and Bernardo Magnini. 2005. A
keyphrase-based approach to summarization: the lake
system at duc-2005. In Proceedings of the Document
Understanding Conference.
Micha Elsner and Deepak Santhanam. 2011. Learning to
fuse disparate sentences. In Proceedings of the Work-
shop on Monolingual Text-To-Text Generation, pages
54–63, Portland, Oregon, June. Association for Com-
putational Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 177–185, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Katja Filippova. 2010. Multi-Sentence Compression:
Finding Shortest Paths in Word Graphs. In Proceed-
</reference>
<page confidence="0.996926">
304
</page>
<reference confidence="0.996189733333333">
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 322–330,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A Graph Based Approach to Abstrac-
tive Summarization of Highly Redundant Opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 340–
348, Beijing, China, August. Coling 2010 Organizing
Committee.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypotheses using structural properties. In Pro-
ceedings of the EACL Workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 41–48.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the Sixth
Conference on Applied Natural Language Processing,
pages 310–315, Seattle, Washington, USA, April. As-
sociation for Computational Linguistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. Semeval-2010 task 5 : Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 21–26, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74–
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.
2009. Clustering to find exemplar terms for keyphrase
extraction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 257–266, Singapore, August. Association
for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 404–411,
Barcelona, Spain, July. Association for Computational
Linguistics.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of the Workshop on Monolingual Text-To-Text Gener-
ation, pages 91–97, Portland, Oregon, June. Associa-
tion for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 118–125. Association for Computational Lin-
guistics.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing Microblogs Automatically. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 685–
688, Los Angeles, California, June. Association for
Computational Linguistics.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33–40, Sapporo, Japan, July. Association
for Computational Linguistics.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303–336.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun’ichi Tsujii. 2006. Trimming cfg parse trees
for sentence compression using machine learning ap-
proaches. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 850–857,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Xiaojun Wan and Jianguo Xiao. 2008. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 969–976, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.
2008. Multi-document Summarization via Sentence-
Level Semantic Analysis and Symmetric Matrix Fac-
torization. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ’08, pages
307–314, New York, NY, USA. ACM.
</reference>
<page confidence="0.999239">
305
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.581816">
<title confidence="0.999738">Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression</title>
<author confidence="0.986877">Boudin Morin</author>
<affiliation confidence="0.624854">LINA - UMR CNRS 6241, Universit´e de Nantes,</affiliation>
<abstract confidence="0.995418333333333">Multi-Sentence Compression (MSC) is the task of generating a short single sentence summary from a cluster of related sentences. This paper presents an N-best reranking method based on keyphrase extraction. Compression candidates generated by a word graph-based MSC approach are reranked according to the number and relevance of keyphrases they contain. Both manual and automatic evaluations were performed using a dataset made of clusters of newswire sentences. Results show that the proposed method significantly improves the informativity of the generated compressions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="19340" citStr="Artstein and Poesio, 2008" startWordPosition="3106" endWordPosition="3110">like the summary the person would produce himself; related (1 pt), if it is related to the the main theme but misses something important; unrelated (0 pts), if the compression is not related to the main theme. Three raters, all native French speakers, were hired to assess the generated compressions. 5 Results To evaluate the effectiveness of our method, we compare the compressions generated with Filippova’s approach (denoted as baseline) against the ones obtained by reranking paths using keyphrases (denoted as KeyRank). We evaluated the agreement between the three raters using Fleiss’s kappa (Artstein and Poesio, 2008). The K value is 0.56 which denotes a moderate agreement. Table 1 presents the average grammaticality and informativity scores. Results achieved by the baseline are consistent with the ones presented in (Filippova, 2010). We observe a significant improvement in informativity for KeyRank. Grammaticality scores are, however, slightly decreased. One reason for that is the reranking we added to the shortest path method that outputs longer compressions. The average length for our method is nevertheless drastically shorter than the average length of the input sentences (19 vs. 33 tokens). This corre</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>R. Artstein and M. Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1922" citStr="Barzilay and McKeown, 2005" startWordPosition="277" endWordPosition="280">te concise, non-redundant summaries. Selected sentences almost always contain additional information specific to the documents from which they came, leading to readability issues in the summary. Sentence Compression (SC), i.e. the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000), is a straightforward solution to this problem. Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences. Originally defined as sentence fusion (Barzilay and McKeown, 2005), MSC is a textto-text generation process in which a novel sentence is produced as a result of summarizing common information across a set of similar sentences. Most of the previous MSC approaches rely on syntactic parsers for producing grammatical compressions, e.g. (Filippova and Strube, 2008; Elsner and Santhanam, 2011). Recently, (Filippova, 2010) proposed a word graph-based approach which only requires a Part-Of-Speech (POS) tagger and a list of stopwords. The key assumption behind her approach is that redundancy within the set of related sentences provides a reliable way of generating in</context>
<context position="4706" citStr="Barzilay and McKeown, 2005" startWordPosition="705" endWordPosition="708"> evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans. The rest of this paper is organized as follows. We first briefly review the previous work, followed by a description of the method we propose. Next, we give the details of the evaluation dataset we have constructed and present our experiments and results. Lastly, we conclude with a discussion and directions for further work. 2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed. The pioneering work of (Barzilay and McKeown, 2005) introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences. (Filippova and Strube, 2008) cast MSC as an integer linear program, and show promising results for German. Later, (Elsner and Santhanam, 2011) proposed a supervised approach trained on examples of manually fused sentences. Previously described approaches require the use of a syntactic parser to control the grammaticality of the output. As an alternat</context>
<context position="17973" citStr="Barzilay and McKeown, 2005" startWordPosition="2896" endWordPosition="2900">). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et al., 2011). In this work, the following evaluation measures are considered relevant: BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-SU4 (bigrams with skip distance up to 4 words)4. ROUGE measures are computed using stopword removal and French stemming 5. 4.3 Manual evaluation The quality of the generated compressions was assessed in an experiment with human raters. Two aspects were considered: grammaticality and informativity. Following previous work (Barzilay and McKeown, 2005), we asked raters to assess grammaticality on a 3-points scale: perfect (2 pts), if the compression is a complete grammatical sentence; almost 3ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v13a.pl 4We use the version 1.5.5 of the ROUGE package available from http://www.berouge.com 5http://snowball.tartarus.org/ 302 (1 pt), if it requires minor editing, e.g. one mistake in articles, agreement or punctuation; ungrammatical (0 pts), if it is none of the above. Raters were explicitly asked to ignore lack of capitalization while evaluating grammaticality. Informativity is evaluated according to </context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>377--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="17298" citStr="Clarke and Lapata, 2006" startWordPosition="2791" endWordPosition="2794">e use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to BLEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et al., 2011). In this work, the following evaluation measures are considered relevant: BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-SU4 (bigrams with skip distance up to 4 words)4. ROUGE measures are computed using stopword removal and French stemming 5. 4.3 Manual evaluation The quality of the generated compressions was assessed in an experiment with human raters. Two aspects were considered: grammatical</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 377–384, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of duc</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference.</booktitle>
<contexts>
<context position="15712" citStr="Dang, 2005" startWordPosition="2543" endWordPosition="2544">te the use of existing automatic evaluation metrics for the MSC task. Similar to (Filippova, 2010), we collected news articles presented in clusters on the French edition of Google News2 over a period of three months. Clusters composed of at least 20 news articles and containing one single prevailing event were manually selected. To obtain the sets of related sentences, we extracted the first sentences from each article in the cluster, removing duplicates. Leading sentences in news articles are known to provide a good summary of the article content and are used as a baseline in summarization (Dang, 2005). The resulting dataset contains 618 sentences (33 tokens on average) spread over 40 clusters. The number of sentences within each cluster is on average 15, with a minimum of 7 and a maximum of 36. The word redundancy rate within the dataset, computed as the number of unique words over the number of words for each cluster, is 38.8%. Three reference compressions were manually composed for each set of sentences. Human annotators, all native French speakers, were asked to 2http://news.google.fr carefully read the set of sentences, extract the most salient facts and generate a sentence (compressio</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa Trang Dang. 2005. Overview of duc 2005. In Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernesto D’Avanzo</author>
<author>Bernardo Magnini</author>
</authors>
<title>A keyphrase-based approach to summarization: the lake system at duc-2005.</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference.</booktitle>
<marker>D’Avanzo, Magnini, 2005</marker>
<rawString>Ernesto D’Avanzo and Bernardo Magnini. 2005. A keyphrase-based approach to summarization: the lake system at duc-2005. In Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Deepak Santhanam</author>
</authors>
<title>Learning to fuse disparate sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>54--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="2246" citStr="Elsner and Santhanam, 2011" startWordPosition="329" endWordPosition="333">g grammatical (Jing, 2000), is a straightforward solution to this problem. Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences. Originally defined as sentence fusion (Barzilay and McKeown, 2005), MSC is a textto-text generation process in which a novel sentence is produced as a result of summarizing common information across a set of similar sentences. Most of the previous MSC approaches rely on syntactic parsers for producing grammatical compressions, e.g. (Filippova and Strube, 2008; Elsner and Santhanam, 2011). Recently, (Filippova, 2010) proposed a word graph-based approach which only requires a Part-Of-Speech (POS) tagger and a list of stopwords. The key assumption behind her approach is that redundancy within the set of related sentences provides a reliable way of generating informative and grammatical sentences. Although this approach seemingly works well, 48% to 60% of the generated sentences are missing important information about the set of related sentences. In this study, we aim at producing more informative sentences by maximizing the range of topics they cover. Keyphrases are words that </context>
<context position="5096" citStr="Elsner and Santhanam, 2011" startWordPosition="767" endWordPosition="770">h a discussion and directions for further work. 2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed. The pioneering work of (Barzilay and McKeown, 2005) introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences. (Filippova and Strube, 2008) cast MSC as an integer linear program, and show promising results for German. Later, (Elsner and Santhanam, 2011) proposed a supervised approach trained on examples of manually fused sentences. Previously described approaches require the use of a syntactic parser to control the grammaticality of the output. As an alternative, several word graph-based approaches that only require a POS tagger were proposed. The key assumption is that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.</context>
</contexts>
<marker>Elsner, Santhanam, 2011</marker>
<rawString>Micha Elsner and Deepak Santhanam. 2011. Learning to fuse disparate sentences. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 54–63, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>177--185</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2217" citStr="Filippova and Strube, 2008" startWordPosition="325" endWordPosition="328">ational content and remaining grammatical (Jing, 2000), is a straightforward solution to this problem. Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences. Originally defined as sentence fusion (Barzilay and McKeown, 2005), MSC is a textto-text generation process in which a novel sentence is produced as a result of summarizing common information across a set of similar sentences. Most of the previous MSC approaches rely on syntactic parsers for producing grammatical compressions, e.g. (Filippova and Strube, 2008; Elsner and Santhanam, 2011). Recently, (Filippova, 2010) proposed a word graph-based approach which only requires a Part-Of-Speech (POS) tagger and a list of stopwords. The key assumption behind her approach is that redundancy within the set of related sentences provides a reliable way of generating informative and grammatical sentences. Although this approach seemingly works well, 48% to 60% of the generated sentences are missing important information about the set of related sentences. In this study, we aim at producing more informative sentences by maximizing the range of topics they cove</context>
<context position="4982" citStr="Filippova and Strube, 2008" startWordPosition="748" endWordPosition="752">ils of the evaluation dataset we have constructed and present our experiments and results. Lastly, we conclude with a discussion and directions for further work. 2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed. The pioneering work of (Barzilay and McKeown, 2005) introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences. (Filippova and Strube, 2008) cast MSC as an integer linear program, and show promising results for German. Later, (Elsner and Santhanam, 2011) proposed a supervised approach trained on examples of manually fused sentences. Previously described approaches require the use of a syntactic parser to control the grammaticality of the output. As an alternative, several word graph-based approaches that only require a POS tagger were proposed. The key assumption is that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 177–185, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-Sentence Compression: Finding Shortest Paths in Word Graphs.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>322--330</pages>
<location>Beijing, China,</location>
<contexts>
<context position="2275" citStr="Filippova, 2010" startWordPosition="335" endWordPosition="336">tforward solution to this problem. Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences. Originally defined as sentence fusion (Barzilay and McKeown, 2005), MSC is a textto-text generation process in which a novel sentence is produced as a result of summarizing common information across a set of similar sentences. Most of the previous MSC approaches rely on syntactic parsers for producing grammatical compressions, e.g. (Filippova and Strube, 2008; Elsner and Santhanam, 2011). Recently, (Filippova, 2010) proposed a word graph-based approach which only requires a Part-Of-Speech (POS) tagger and a list of stopwords. The key assumption behind her approach is that redundancy within the set of related sentences provides a reliable way of generating informative and grammatical sentences. Although this approach seemingly works well, 48% to 60% of the generated sentences are missing important information about the set of related sentences. In this study, we aim at producing more informative sentences by maximizing the range of topics they cover. Keyphrases are words that capture the main topics of a </context>
<context position="3834" citStr="Filippova (2010)" startWordPosition="571" endWordPosition="572">e summary (D’Avanzo and Magnini, 2005). In the same way, we hypothesize that keyphrases can be used to better generate sentences that convey the gist 298 Proceedings of NAACL-HLT 2013, pages 298–305, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics of the set of related sentences. In this paper, we present a reranking method of N-best multi-sentence compressions based on keyphrase extraction and describe a series of experiments conducted on a manually constructed evaluation corpus. More precisely, the main contributions of our work are as follows: • We extend Filippova (2010)’s word graphbased MSC approach to produce wellpunctuated and more informative compressions. • We investigate the use of automatic Machine Translation (MT) and summarization evaluation metrics to evaluate MSC performance. • We introduce a French evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans. The rest of this paper is organized as follows. We first briefly review the previous work, followed by a description of the method we propose. Next, we give the details of the evaluation dataset we have constructed and present our experiments a</context>
<context position="6008" citStr="Filippova, 2010" startWordPosition="911" endWordPosition="912"> The key assumption is that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph. Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (Sharifi et al., 2010), opinion summarization (Ganesan et al., 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised app</context>
<context position="11441" citStr="Filippova, 2010" startWordPosition="1837" endWordPosition="1838">o not contain a verb are filtered. The remaining paths are reranked by normalizing the total path weight over its length. The path which has the lightest average edge weight is then considered as the best compression. 3.2 Reranking paths using keyphrases The main difficulty of MSC is to generate sentences that are both informative and grammatically correct. Here, redundancy within the set of input sentences is used to identify important words and salient links between words. Although this approach seemingly works well, important information is missing in 48% to 60% of the generated sentences (Filippova, 2010). One of the reasons for this is that node salience is estimated only with the frequency measure. To tackle this issue, we propose to rerank the N-best list of compressions using keyphrases extracted from the set of related sentences. Intuitively, an informative sentence should contain the most relevant keyphrases. We propose to rerank generated compressions according to the number and relevance of keyphrases they contain. An unsupervised method based on (Wan and Xiao, 2008) is used to extract keyphrases from each set of related sentences. This method is based on the assumption that a word rec</context>
<context position="14341" citStr="Filippova, 2010" startWordPosition="2318" endWordPosition="2319"> redundant. Some keyphrases may be contained within larger ones, e.g. giant tortoise and Pinta Island giant tortoise. To solve this problem, generated keyphrases are clustered using word overlap. For each cluster, we then select the keyphrase with the highest score. This filtering process enables the generation of a smaller subset of keyphrases while having a better coverage of the cluster content. 301 Reranking techniques can suffer from the limited scope of the N-best list, which may rule out many potentially good candidates. For this reason, we use a larger number of paths than the one in (Filippova, 2010). Accordingly, the K-shortest paths algorithm is used to find the 200 shortest paths. We rerank the paths by normalizing the total path weight over its length multiplied by the sum of keyphrase scores it contains. The score of a sentence compression c is given by: score(c) = F—i,iEpath(c) w(i,i) (4) length(c) x F—kEc score(k) 4 Experimental settings 4.1 Construction of the evaluation dataset To our knowledge, there is no dataset available to evaluate MSC in an automatic way. The performance of the previously described approaches was assessed by human judges. In this work, we introduce a new ev</context>
<context position="18621" citStr="Filippova, 2010" startWordPosition="2991" endWordPosition="2992">maticality on a 3-points scale: perfect (2 pts), if the compression is a complete grammatical sentence; almost 3ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v13a.pl 4We use the version 1.5.5 of the ROUGE package available from http://www.berouge.com 5http://snowball.tartarus.org/ 302 (1 pt), if it requires minor editing, e.g. one mistake in articles, agreement or punctuation; ungrammatical (0 pts), if it is none of the above. Raters were explicitly asked to ignore lack of capitalization while evaluating grammaticality. Informativity is evaluated according to the 3- points scale defined in (Filippova, 2010): perfect (2 pts), if the compression conveys the gist of the main event and is more or less like the summary the person would produce himself; related (1 pt), if it is related to the the main theme but misses something important; unrelated (0 pts), if the compression is not related to the main theme. Three raters, all native French speakers, were hired to assess the generated compressions. 5 Results To evaluate the effectiveness of our method, we compare the compressions generated with Filippova’s approach (denoted as baseline) against the ones obtained by reranking paths using keyphrases (de</context>
<context position="23119" citStr="Filippova (2010)" startWordPosition="3700" endWordPosition="3702"> may be that the manual score assignments are arbitrary (i.e. 0, 1, 2), and that a score of one is in fact closer to two than to zero. Results suggest that automatic metrics do give an indication of the compression quality, but can not replace manual evaluation. Metric Gram. Info. ROUGE-1 0.402 0.591 ROUGE-2 0.432 0.494 ROUGE-SU4 0.386 0.542 BLEU 0.444 0.401 Table 4: Pearson correlation coefficients for automatic metrics vs. average human ratings. 6 Conclusion This paper presented a multi-sentence compression approach that uses keyphrases to generate more informative compressions. We extended Filippova (2010)’s word graph-based MSC approach by adding a re-reranking step that favors compressions that contain the most relevant keyphrases of the input sentence set. An implementation of the proposed multi-sentence compression approach is available for download7. We constructed an evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans. This dataset is freely available for download8. We performed both manual and automatic evaluations and showed that our method significantly improves the informativity of the generated compressions. We also investigate</context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-Sentence Compression: Finding Shortest Paths in Word Graphs. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 322–330, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<contexts>
<context position="6575" citStr="Frank et al., 1999" startWordPosition="993" endWordPosition="996"> 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method </context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant Opinions.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>340--348</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5963" citStr="Ganesan et al., 2010" startWordPosition="904" endWordPosition="907">ches that only require a POS tagger were proposed. The key assumption is that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph. Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (Sharifi et al., 2010), opinion summarization (Ganesan et al., 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank </context>
</contexts>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: A Graph Based Approach to Abstractive Summarization of Highly Redundant Opinions. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 340– 348, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>O Bender</author>
<author>H Ney</author>
</authors>
<title>Reranking translation hypotheses using structural properties.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<pages>41--48</pages>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>S. Hasan, O. Bender, and H. Ney. 2006. Reranking translation hypotheses using structural properties. In Proceedings of the EACL Workshop on Learning Structured Information in Natural Language Applications, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing,</booktitle>
<pages>310--315</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1645" citStr="Jing, 2000" startWordPosition="236" endWordPosition="237">gle or multi-document extractive summarization. A standard way to generate summaries consists in ranking sentences by importance, cluster them by similarity and select a sentence from the top ranked clusters (Wang et al., 2008). One difficulty is then to generate concise, non-redundant summaries. Selected sentences almost always contain additional information specific to the documents from which they came, leading to readability issues in the summary. Sentence Compression (SC), i.e. the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000), is a straightforward solution to this problem. Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences. Originally defined as sentence fusion (Barzilay and McKeown, 2005), MSC is a textto-text generation process in which a novel sentence is produced as a result of summarizing common information across a set of similar sentences. Most of the previous MSC approaches rely on syntactic parsers for producing grammatical compressions, e.g. (Filippova and Strube, 2008; Elsner and Santhanam, 2011</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of the Sixth Conference on Applied Natural Language Processing, pages 310–315, Seattle, Washington, USA, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>21--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3046" citStr="Kim et al., 2010" startWordPosition="451" endWordPosition="454"> that redundancy within the set of related sentences provides a reliable way of generating informative and grammatical sentences. Although this approach seemingly works well, 48% to 60% of the generated sentences are missing important information about the set of related sentences. In this study, we aim at producing more informative sentences by maximizing the range of topics they cover. Keyphrases are words that capture the main topics of a document. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and questionanswering (Kim et al., 2010). In summarization, keyphrases provide semantic metadata that represent the content of a document. Sentences containing the most relevant keyphrases are used to generate the summary (D’Avanzo and Magnini, 2005). In the same way, we hypothesize that keyphrases can be used to better generate sentences that convey the gist 298 Proceedings of NAACL-HLT 2013, pages 298–305, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics of the set of related sentences. In this paper, we present a reranking method of N-best multi-sentence compressions based on keyphrase extraction</context>
<context position="6276" citStr="Kim et al., 2010" startWordPosition="944" endWordPosition="947">e original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph. Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (Sharifi et al., 2010), opinion summarization (Ganesan et al., 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally prove</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21–26, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81,</booktitle>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16850" citStr="Lin, 2004" startWordPosition="2723" endWordPosition="2724">nces, extract the most salient facts and generate a sentence (compression) that summarize the set of sentences. Annotators were also told to introduce as little new vocabulary as possible in their compressions. The purpose of this guideline is to reduce the number of possible mismatches, as existing evaluation metrics are based on n-gram comparison. Reference compressions have a compression rate of 60%. 4.2 Automatic evaluation The use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to BLEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of depend</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Peng Li</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>257--266</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6826" citStr="Liu et al., 2009" startWordPosition="1030" endWordPosition="1033">on, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V and a set of directed edges E, where E is a subset of V x V. Given a set of related sentences S = {s1, </context>
</contexts>
<marker>Liu, Li, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>404--411</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6771" citStr="Mihalcea and Tarau, 2004" startWordPosition="1020" endWordPosition="1023"> various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V and a set of directed edges E, where E is a subs</context>
<context position="12578" citStr="Mihalcea and Tarau, 2004" startWordPosition="2020" endWordPosition="2024">om each set of related sentences. This method is based on the assumption that a word recommends other cooccurring words, and the strength of the recommendation is recursively computed based on the importance of the words making the recommendation. Keyphrase extraction can be divided into two steps. First, a weighted graph is constructed from the set of related sentences, in which nodes represent words defined as word and POS tuples. Two nodes (words) are connected if their corresponding lexical units cooccur within a sentence. Edge weights are the number of times two words co-occur. TextRank (Mihalcea and Tarau, 2004), a graph-based ranking algorithm that takes into account edge weights, is applied for computing a salience score for each node. The score for node Vi is initialized with a default value and is computed in an iterative manner until convergence using this equation: Y_ wji S(Vi) S(Vi) = (1−d)+d� LVkEadj(Vi) wjk ViEadj(Vi) where adj(Vi) denotes the neighbors of Vi and d is the damping factor set to 0.85. The second step consists in generating and scoring keyphrase candidates. Sequences of adjacent words satisfying a specific syntactic pattern are collapsed into multi-word phrases. We use (ADJ)*(N</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 404–411, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Evaluating sentence compression: Pitfalls and suggested remedies.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>91--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<marker>Napoles, Van Durme, Callison-Burch, 2011</marker>
<rawString>Courtney Napoles, Benjamin Van Durme, and Chris Callison-Burch. 2011. Evaluating sentence compression: Pitfalls and suggested remedies. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 91–97, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="16883" citStr="Papineni et al., 2002" startWordPosition="2727" endWordPosition="2730">t salient facts and generate a sentence (compression) that summarize the set of sentences. Annotators were also told to introduce as little new vocabulary as possible in their compressions. The purpose of this guideline is to reduce the number of possible mismatches, as existing evaluation metrics are based on n-gram comparison. Reference compressions have a compression rate of 60%. 4.2 Automatic evaluation The use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to BLEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Annie Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>118--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17210" citStr="Riezler et al., 2003" startWordPosition="2777" endWordPosition="2780">n. Reference compressions have a compression rate of 60%. 4.2 Automatic evaluation The use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to BLEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et al., 2011). In this work, the following evaluation measures are considered relevant: BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-SU4 (bigrams with skip distance up to 4 words)4. ROUGE measures are computed using stopword removal and French stemming 5. 4.3 Manual evaluation The quality of the generated compressions w</context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Stefan Riezler, Tracy H. King, Richard Crouch, and Annie Zaenen. 2003. Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 118–125. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beaux Sharifi</author>
<author>Mark-Anthony Hutton</author>
<author>Jugal Kalita</author>
</authors>
<title>Summarizing Microblogs Automatically. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>685--688</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="5917" citStr="Sharifi et al., 2010" startWordPosition="898" endWordPosition="901">n alternative, several word graph-based approaches that only require a POS tagger were proposed. The key assumption is that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph. Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (Sharifi et al., 2010), opinion summarization (Ganesan et al., 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whethe</context>
</contexts>
<marker>Sharifi, Hutton, Kalita, 2010</marker>
<rawString>Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita. 2010. Summarizing Microblogs Automatically. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 685– 688, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Tomokiyo</author>
<author>Matthew Hurst</author>
</authors>
<title>A language model approach to keyphrase extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6724" citStr="Tomokiyo and Hurst, 2003" startWordPosition="1014" endWordPosition="1017"> of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V a</context>
</contexts>
<marker>Tomokiyo, Hurst, 2003</marker>
<rawString>Takashi Tomokiyo and Matthew Hurst. 2003. A language model approach to keyphrase extraction. In Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 33–40, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="6590" citStr="Turney, 2000" startWordPosition="997" endWordPosition="998">summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking g</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2(4):303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuya Unno</author>
<author>Takashi Ninomiya</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Trimming cfg parse trees for sentence compression using machine learning approaches.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>850--857</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="17347" citStr="Unno et al., 2006" startWordPosition="2801" endWordPosition="2804">ated text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to BLEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et al., 2011). In this work, the following evaluation measures are considered relevant: BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-SU4 (bigrams with skip distance up to 4 words)4. ROUGE measures are computed using stopword removal and French stemming 5. 4.3 Manual evaluation The quality of the generated compressions was assessed in an experiment with human raters. Two aspects were considered: grammaticality and informativity. Following previous work (B</context>
</contexts>
<marker>Unno, Ninomiya, Miyao, Tsujii, 2006</marker>
<rawString>Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Trimming cfg parse trees for sentence compression using machine learning approaches. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850–857, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Collabrank: Towards a collaborative approach to single-document keyphrase extraction.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>969--976</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="6792" citStr="Wan and Xiao, 2008" startWordPosition="1024" endWordPosition="1027">Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V and a set of directed edges E, where E is a subset of V x V. Given a </context>
<context position="11920" citStr="Wan and Xiao, 2008" startWordPosition="1911" endWordPosition="1914">ds. Although this approach seemingly works well, important information is missing in 48% to 60% of the generated sentences (Filippova, 2010). One of the reasons for this is that node salience is estimated only with the frequency measure. To tackle this issue, we propose to rerank the N-best list of compressions using keyphrases extracted from the set of related sentences. Intuitively, an informative sentence should contain the most relevant keyphrases. We propose to rerank generated compressions according to the number and relevance of keyphrases they contain. An unsupervised method based on (Wan and Xiao, 2008) is used to extract keyphrases from each set of related sentences. This method is based on the assumption that a word recommends other cooccurring words, and the strength of the recommendation is recursively computed based on the importance of the words making the recommendation. Keyphrase extraction can be divided into two steps. First, a weighted graph is constructed from the set of related sentences, in which nodes represent words defined as word and POS tuples. Two nodes (words) are connected if their corresponding lexical units cooccur within a sentence. Edge weights are the number of tim</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008. Collabrank: Towards a collaborative approach to single-document keyphrase extraction. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 969–976, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
<author>Shenghuo Zhu</author>
<author>Chris Ding</author>
</authors>
<title>Multi-document Summarization via SentenceLevel Semantic Analysis and Symmetric Matrix Factorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08,</booktitle>
<pages>307--314</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1261" citStr="Wang et al., 2008" startWordPosition="179" endWordPosition="182"> clusters of newswire sentences. Results show that the proposed method significantly improves the informativity of the generated compressions. 1 Introduction Multi-Sentence Compression (MSC) can be broadly described as the task of generating a short single sentence summary from a cluster of related sentences. It has recently attracted much attention, mostly because of its relevance to single or multi-document extractive summarization. A standard way to generate summaries consists in ranking sentences by importance, cluster them by similarity and select a sentence from the top ranked clusters (Wang et al., 2008). One difficulty is then to generate concise, non-redundant summaries. Selected sentences almost always contain additional information specific to the documents from which they came, leading to readability issues in the summary. Sentence Compression (SC), i.e. the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000), is a straightforward solution to this problem. Another solution would be to create, for each cluster of related sentences, a concise and fluent fusion of information, reflecting facts common to all sentences. Origi</context>
</contexts>
<marker>Wang, Li, Zhu, Ding, 2008</marker>
<rawString>Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding. 2008. Multi-document Summarization via SentenceLevel Semantic Analysis and Symmetric Matrix Factorization. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08, pages 307–314, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>