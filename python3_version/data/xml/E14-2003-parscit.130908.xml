<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032739">
<title confidence="0.98868">
XLike Project Language Analysis Services
</title>
<author confidence="0.9965755">
Xavier Carreras*, Lluis Padr´o*, Lei Zhang�, Achim Rettinger*, Zhixing Li✶,
Esteban Garcia-Cuesta*, ˇZeljko Agi´c*, Boˇzo Bekavaca, Blaz Fortuna†, Tadej ˇStajner†
</author>
<affiliation confidence="0.9844915">
∗ Universitat Polit`ecnica de Catalunya, Barcelona, Spain. o iSOCO S.A. Madrid, Spain.
a University of Zagreb, Zagreb, Croatia. * University of Potsdam, Germany.
† Joˇzef Stefan Institute, Ljubljana, Slovenia. ✶ Tsinghua University, Beijing, China.
♠ Karlsruhe Institute of Technology, Karlsruhe, Germany.
</affiliation>
<sectionHeader confidence="0.978663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996051875">
This paper presents the linguistic analysis
infrastructure developed within the XLike
project. The main goal of the imple-
mented tools is to provide a set of func-
tionalities supporting the XLike main ob-
jectives: Enabling cross-lingual services
for publishers, media monitoring or de-
veloping new business intelligence appli-
cations. The services cover seven major
and minor languages: English, German,
Spanish, Chinese, Catalan, Slovenian, and
Croatian. These analyzers are provided
as web services following a lightweigth
SOA architecture approach, and they are
publically accessible and shared through
META-SHARE.1
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999850166666667">
Project XLike2 goal is to develop technology able
to gather documents in a variety of languages and
genres (news, blogs, tweets, etc.) and to extract
language-independent knowledge from them, in
order to provide new and better services to pub-
lishers, media monitoring, and business intelli-
gence. Thus, project use cases are provided by
STA (Slovenian Press Agency) and Bloomberg, as
well as New York Times as an associated partner.
Research partners in the project are Joˇzef Ste-
fan Institute (JSI), Karlsruhe Institute of Technol-
ogy (KIT), Universitat Polit`ecnica de Catalunya
(UPC), University of Zagreb (UZG), and Tsinghua
University (THU). The Spanish company iSOCO
is in charge of integration of all components de-
veloped in the project.
This paper deals with the language technology
developed within the project XLike to convert in-
</bodyText>
<footnote confidence="0.99258575">
1accessible and shared here means that the services are
publicly callable, not that the code is open-source.
http://www.meta-share.eu
2http://www.xlike.org
</footnote>
<bodyText confidence="0.999463083333333">
put documents into a language-independent rep-
resentation that afterwards enables knowledge ag-
gregation.
To achieve this goal, a bench of linguistic pro-
cessing pipelines is devised as the first step in the
document processing flow. Then, a cross-lingual
semantic annotation method, based on Wikipedia
and Linked Open Data (LOD), is applied. The
semantic annotation stage enriches the linguistic
anaylsis with links to knowledge bases for differ-
ent languages, or links to language independent
representations.
</bodyText>
<sectionHeader confidence="0.969675" genericHeader="method">
2 Linguistic Analyzers
</sectionHeader>
<bodyText confidence="0.999936615384615">
Apart from basic state-of-the-art tokenizers, lem-
matizers, PoS/MSD taggers, and NE recogniz-
ers, each pipeline requires deeper processors able
to build the target language-independent seman-
tic representantion. For that, we rely on three
steps: dependency parsing, semantic role label-
ing and word sense disambiguation. These three
processes, combined with multilingual ontologi-
cal resouces such as different WordNets and Pred-
icateMatrix (L´opez de la Calle et al., 2014), a
lexical semantics resource combining WordNet,
FrameNet, and VerbNet, are the key to the con-
struction of our semantic representation.
</bodyText>
<subsectionHeader confidence="0.989909">
2.1 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9989819">
We use graph-based methods for dependency
parsing, namely, MSTParser3 (McDonald et al.,
2005) is used for Chinese and Croatian, and
Treeler4 is used for the other languages. Treeler is
a library developed by the UPC team that imple-
ments several statistical methods for tagging and
parsing.
We use these tools in order to train dependency
parsers for all XLike languages using standard
available treebanks.
</bodyText>
<footnote confidence="0.999949">
3http://sourceforge.net/projects/mstparser
4http://treeler.lsi.upc.edu
</footnote>
<page confidence="0.909853">
9
</page>
<note confidence="0.882744">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 9–12,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.998085">
2.2 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.99998675">
As with syntactic parsing, we are developing SRL
methods with the Treeler library. In order to train
models, we will use the treebanks made available
by the CoNLL-2009 shared task, which provided
data annotated with predicate-argument relations
for English, Spanish, Catalan, German and Chi-
nese. No treebank annotated with semantic roles
exists for Slovene or Croatian. A prototype of
SRL has been integrated in all pipelines (except
the Slovene and Croatian pipelines). The method
implemented follows a pipeline architecture de-
scribed in (Lluis et al., 2013).
</bodyText>
<subsectionHeader confidence="0.998794">
2.3 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.9999366875">
Word sense disambiguation is performed for all
languages with a publicly available WordNet. This
includes all languages in the project except Chi-
nese. The goal of WSD is to map specific lan-
guages to a common semantic space, in this case,
WN synsets. Thanks to existing connections be-
tween WN and other resources, SUMO and Open-
CYC sense codes are also output when available.
Thanks to PredicateMatrix, the obtained con-
cepts can be projected to FrameNet, achieving a
normalization of the semantic roles produced by
the SRL (which are treebank-dependent, and thus,
not the same for all languages). The used WSD
engine is the UKB (Agirre and Soroa, 2009) im-
plementation provided by FreeLing (Padr´o and
Stanilovsky, 2012).
</bodyText>
<subsectionHeader confidence="0.998528">
2.4 Frame Extraction
</subsectionHeader>
<bodyText confidence="0.999991371428571">
The final step is to convert all the gathered linguis-
tic information into a semantic representation. Our
method is based on the notion of frame: a seman-
tic frame is a schematic representation of a situ-
ation involving various participants. In a frame,
each participant plays a role. There is a direct cor-
respondence between roles in a frame and seman-
tic roles; namely, frames correspond to predicates,
and participants correspond to the arguments of
the predicate. We distinguish three types of par-
ticipants: entities, words, and frames.
Entities are nodes in the graph connected to
real-world entities as described in Section 3.
Words are common words or concepts, linked to
general ontologies such as WordNet. Frames cor-
respond to events or predicates described in the
document. Figure 1 shows an example sentence,
the extracted frames and their arguments.
It is important to note that frames are a more
general representation than SVO-triples. While
SVO-triples represent a binary relation between
two participants, frames can represent n-ary rela-
tions (e.g. predicates with more than two argu-
ments, or with adjuncts). Frames also allow repre-
senting the sentences where one of the arguments
is in turn a frame (as is the case with plan to make
in the example).
Finally, although frames are extracted at sen-
tence level, the resulting graphs are aggregated
in a single semantic graph representing the whole
document via a very simple coreference resolution
based on detecting named entity aliases and repe-
titions of common nouns. Future improvements
include using an state-of-the-art coreference reso-
lution module for languages where it is available.
</bodyText>
<sectionHeader confidence="0.988614" genericHeader="method">
3 Cross-lingual Semantic Annotation
</sectionHeader>
<bodyText confidence="0.99987016">
This step adds further semantic annotations on top
of the results obtained by linguistic processing.
All XLike languages are covered. The goal is
to map word phrases in different languages into
the same semantic interlingua, which consists of
resources specified in knowledge bases such as
Wikipedia and Linked Open Data (LOD) sources.
Cross-lingual semantic annotation is performed in
two stages: (1) first, candidate concepts in the
knowledge base are linked to the linguistic re-
sources based on a newly developed cross-lingual
linked data lexica, called xLiD-Lexica, (2) next
the candidate concepts get disambiguated based
on the personalized PageRank algorithm by utiliz-
ing the structure of information contained in the
knowledge base.
The xLiD-Lexica is stored in RDF format and
contains about 300 million triples of cross-lingual
groundings. It is extracted from Wikipedia dumps
of July 2013 in English, German, Spanish, Cata-
lan, Slovenian and Chinese, and based on the
canonicalized datasets of DBpedia 3.8 contain-
ing triples extracted from the respective Wikipedia
whose subject and object resource have an equiv-
alent English article.
</bodyText>
<sectionHeader confidence="0.990802" genericHeader="method">
4 Web Service Architecture Approach
</sectionHeader>
<bodyText confidence="0.9999744">
The different language functionalities are imple-
mented following the service oriented architec-
ture (SOA) approach defined in the project XLike.
Therefore all the pipelines (one for each language)
have been implemented as web services and may
</bodyText>
<page confidence="0.99516">
10
</page>
<figureCaption confidence="0.9473105">
Figure 1: Graphical representation of frames in the sentence Acme, based in New York, now plans to
make computer and electronic products.
</figureCaption>
<bodyText confidence="0.999917470588235">
be requested to produce different levels of analy-
sis (e.g. tokenization, lemmatization, NERC, pars-
ing, relation extraction). This approach is very ap-
pealing due to the fact that it allows to treat ev-
ery language independently and execute the whole
language analysis process at different threads or
computers allowing an easier parallelization (e.g.
using external high perfomance platforms such as
Amazon Elastic Compute Cloud EC25) as needed.
Furthermore it also provides independent develop-
ment lifecycles for each language which is crucial
in this type of research projects. Recall that these
web services can be deployed locally or remotely,
maintaining the option of using them in a stand-
alone configuration.
The main structure for each one of the pipelines
is described below:
</bodyText>
<listItem confidence="0.989235176470588">
• Spanish, English, and Catalan: all mod-
ules are based on FreeLing (Padr´o and
Stanilovsky, 2012) and Treeler.
• German: German shallow processing is
based on OpenNLP6, Stanford POS tagger
and NE extractor (Toutanova et al., 2003;
Finkel et al., 2005). Dependency parsing,
semantic role labeling, word sense disam-
biguation, and SRL-based frame extraction
are based on FreeLing and Treeler.
• Slovene: Slovene shallow processing is pro-
vided by JSI Enrycher7 (ˇStajner et al., 2010),
which consists of the Obeliks morphosyntac-
tic analysis library (Grˇcar et al., 2012), the
LemmaGen lemmatizer (Jurˇsiˇc et al., 2010)
and a CRF-based entity extractor (ˇStajner et
al., 2012). Dependency parsing, word sense
</listItem>
<footnote confidence="0.999890666666667">
5http://aws.amazon.com/ec2/
6http://opennlp.apache.org
7http://enrycher.ijs.si
</footnote>
<bodyText confidence="0.935367">
disambiguation are based on FreeLing and
Treeler. Frame extraction is rule-based since
no SRL corpus is available for Slovene.
</bodyText>
<listItem confidence="0.788474333333333">
• Croatian: Croatian shallow processing is
based on proprietary tokenizer, POS/MSD-
tagging and lemmatisaton system (Agi´c et
al., 2008), NERC system (Bekavac and
Tadi´c, 2007) and dependency parser (Agi´c,
2012). Word sense disambiguation is based
on FreeLing. Frame extraction is rule-based
since no SRL corpus is available for Croatian.
• Chinese: Chinese shallow and deep process-
ing is based on a word segmentation compo-
nent ICTCLAS8 and a semantic dependency
parser trained on CSDN corpus. Then, rule-
based frame extraction is performed (no SRL
corpus nor WordNet are available for Chi-
nese).
</listItem>
<bodyText confidence="0.998615363636364">
Each language analysis service is able to pro-
cess thousands of words per second when per-
forming shallow analysis (up to NE recognition),
and hundreds of words per second when produc-
ing the semantic representation based on full anal-
ysis. Moreover, the web service architecture en-
ables the same server to run a different thread for
each client, thus taking advantage of multiproces-
sor capabilities.
The components of the cross-lingual semantic
annotation stage are:
</bodyText>
<listItem confidence="0.9893622">
• xLiD-Lexica: The cross-lingual groundings
in xLiD-Lexica are translated into RDF data
and are accessible through a SPARQL end-
point, based on OpenLink Virtuoso9 as the
back-end database engine.
</listItem>
<footnote confidence="0.9993935">
8http://ictclas.org/
9http://virtuoso.openlinksw.com/
</footnote>
<page confidence="0.998367">
11
</page>
<listItem confidence="0.9154448">
• Semantic Annotation: The cross-lingual se-
mantic annotation service is based on the
xLiD-Lexica for entity mention recognition
and the JUNG Framework10 for graph-based
disambiguation.
</listItem>
<sectionHeader confidence="0.967936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962260869565">
We presented the web service based architecture
used in XLike FP7 project to linguistically ana-
lyze large amounts of documents in seven differ-
ent languages. The analysis pipelines perform ba-
sic processing as tokenization, PoS-tagging, and
named entity extraction, as well as deeper analy-
sis such as dependency parsing, word sense disam-
biguation, and semantic role labelling. The result
of these linguistic analyzers is a semantic graph
capturing the main events described in the docu-
ment and their core participants.
On top of that, the cross-lingual semantic an-
notation component links the resulting linguistic
resources in one language to resources in a knowl-
edge bases in any other language or to language
independent representations. This semantic repre-
sentation is later used in XLike for document min-
ing purposes such as enabling cross-lingual ser-
vices for publishers, media monitoring or devel-
oping new business intelligence applications.
The described analysis services are currently
available via META-SHARE as callable RESTful
services.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999494">
This work was funded by the European Union
through project XLike (FP7-ICT-2011-288342).
</bodyText>
<sectionHeader confidence="0.977575" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.855040408450705">
ˇZeljko Agi´c, Marko Tadi´c, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445–451.
ˇZeljko Agi´c. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1–12,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th conference of the European
chapter of the Association for Computational Lin-
guistics (EACL-2009), Athens, Greece.
10Java Universal Network/Graph Framework
http://jung.sourceforge.net/
Boˇzo Bekavac and Marko Tadi´c. 2007. Implementa-
tion of Croatian NERC system. In Proceedings of
the Workshop on Balto-Slavonic Natural Language
Processing (BSNLP2007), Special Theme: Informa-
tion Extraction and Enabling Technologies, pages
11–18. Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL’05), pages 363–370.
Miha Grˇcar, Simon Krek, and Kaja Dobrovoljc. 2012.
Obeliks: statistiˇcni oblikoskladenjski oznaˇcevalnik
in lematizator za slovenski jezik. In Zbornik Osme
konference Jezikovne tehnologije, Ljubljana, Slove-
nia.
Matjaz Jurˇsiˇc, Igor Mozetiˇc, Tomaz Erjavec, and Nada
Lavraˇc. 2010. Lemmagen: Multilingual lemmati-
sation with induced ripple-down rules. Journal of
Universal Computer Science, 16(9):1190–1214.
Xavier Llu´ıs, Xavier Carreras, and Llu´ıs M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics, 1:219–230.
Maddalen L´opez de la Calle, Egoitz Laparra, and Ger-
man Rigau. 2014. First steps towards a predicate
matrix. In Proceedings of the Global WordNet Con-
ference (GWC 2014), Tartu, Estonia, January. GWA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 91–98, Ann Ar-
bor, Michigan, June.
Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Tadej ˇStajner, Delia Rusu, Lorand Dali, Blaˇz Fortuna,
Dunja Mladeni´c, and Marko Grobelnik. 2010. A
service oriented framework for natural language text
enrichment. Informatica, 34(3):307–313.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Lin- guistics on Human Language Technology
(NAACL’03).
Tadej ˇStajner, Tomaˇz Erjavec, and Simon Krek.
2012. Razpoznavanje imenskih entitet v slovenskem
besedilu. In In Proceedings of 15th Internation
Multiconference on Information Society - Jezikovne
Tehnologije, Ljubljana, Slovenia.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.169930">
<title confidence="0.636794">XLike Project Language Analysis Services Lluis Lei Achim Zhixing Boˇzo Blaz Tadej</title>
<author confidence="0.496123">S A Madrid</author>
<author confidence="0.496123">Spain</author>
<affiliation confidence="0.941310666666667">a University of Zagreb, Zagreb, Croatia. * University of Potsdam, Germany. Stefan Institute, Ljubljana, Slovenia. University, Beijing, China. Institute of Technology, Karlsruhe, Germany.</affiliation>
<abstract confidence="0.975005875">This paper presents the linguistic analysis infrastructure developed within the XLike project. The main goal of the implemented tools is to provide a set of functionalities supporting the XLike main objectives: Enabling cross-lingual services for publishers, media monitoring or developing new business intelligence applications. The services cover seven major and minor languages: English, German, Spanish, Chinese, Catalan, Slovenian, and Croatian. These analyzers are provided as web services following a lightweigth SOA architecture approach, and they are publically accessible and shared through</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Marko Tadi´c</author>
<author>Zdravko Dovedan</author>
</authors>
<title>Improving part-of-speech tagging accuracy for Croatian by morphological analysis.</title>
<date>2008</date>
<journal>Informatica,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Agi´c, Tadi´c, Dovedan, 2008</marker>
<rawString>ˇZeljko Agi´c, Marko Tadi´c, and Zdravko Dovedan. 2008. Improving part-of-speech tagging accuracy for Croatian by morphological analysis. Informatica, 32(4):445–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
</authors>
<title>K-best spanning tree dependency parsing with verb valency lexicon reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1--12</pages>
<location>Mumbai, India,</location>
<marker>Agi´c, 2012</marker>
<rawString>ˇZeljko Agi´c. 2012. K-best spanning tree dependency parsing with verb valency lexicon reranking. In Proceedings of COLING 2012: Posters, pages 1–12, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009), Athens, Greece. 10Java Universal Network/Graph Framework http://jung.sourceforge.net/</booktitle>
<contexts>
<context position="5277" citStr="Agirre and Soroa, 2009" startWordPosition="769" endWordPosition="772">s performed for all languages with a publicly available WordNet. This includes all languages in the project except Chinese. The goal of WSD is to map specific languages to a common semantic space, in this case, WN synsets. Thanks to existing connections between WN and other resources, SUMO and OpenCYC sense codes are also output when available. Thanks to PredicateMatrix, the obtained concepts can be projected to FrameNet, achieving a normalization of the semantic roles produced by the SRL (which are treebank-dependent, and thus, not the same for all languages). The used WSD engine is the UKB (Agirre and Soroa, 2009) implementation provided by FreeLing (Padr´o and Stanilovsky, 2012). 2.4 Frame Extraction The final step is to convert all the gathered linguistic information into a semantic representation. Our method is based on the notion of frame: a semantic frame is a schematic representation of a situation involving various participants. In a frame, each participant plays a role. There is a direct correspondence between roles in a frame and semantic roles; namely, frames correspond to predicates, and participants correspond to the arguments of the predicate. We distinguish three types of participants: en</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009), Athens, Greece. 10Java Universal Network/Graph Framework http://jung.sourceforge.net/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boˇzo Bekavac</author>
<author>Marko Tadi´c</author>
</authors>
<title>Implementation of Croatian NERC system.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing (BSNLP2007), Special Theme: Information Extraction and Enabling Technologies,</booktitle>
<pages>11--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bekavac, Tadi´c, 2007</marker>
<rawString>Boˇzo Bekavac and Marko Tadi´c. 2007. Implementation of Croatian NERC system. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing (BSNLP2007), Special Theme: Information Extraction and Enabling Technologies, pages 11–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL’05),</booktitle>
<pages>363--370</pages>
<contexts>
<context position="9650" citStr="Finkel et al., 2005" startWordPosition="1448" endWordPosition="1451">ompute Cloud EC25) as needed. Furthermore it also provides independent development lifecycles for each language which is crucial in this type of research projects. Recall that these web services can be deployed locally or remotely, maintaining the option of using them in a standalone configuration. The main structure for each one of the pipelines is described below: • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6, Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is provided by JSI Enrycher7 (ˇStajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) and a CRF-based entity extractor (ˇStajner et al., 2012). Dependency parsing, word sense 5http://aws.amazon.com/ec2/ 6http://opennlp.apache.org 7http://enrycher.ijs.si disambiguation are based on FreeLing and Treeler. Frame extraction </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL’05), pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miha Grˇcar</author>
<author>Simon Krek</author>
<author>Kaja Dobrovoljc</author>
</authors>
<title>Obeliks: statistiˇcni oblikoskladenjski oznaˇcevalnik in lematizator za slovenski jezik.</title>
<date>2012</date>
<booktitle>In Zbornik Osme konference Jezikovne tehnologije,</booktitle>
<location>Ljubljana, Slovenia.</location>
<marker>Grˇcar, Krek, Dobrovoljc, 2012</marker>
<rawString>Miha Grˇcar, Simon Krek, and Kaja Dobrovoljc. 2012. Obeliks: statistiˇcni oblikoskladenjski oznaˇcevalnik in lematizator za slovenski jezik. In Zbornik Osme konference Jezikovne tehnologije, Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matjaz Jurˇsiˇc</author>
<author>Igor Mozetiˇc</author>
<author>Tomaz Erjavec</author>
<author>Nada Lavraˇc</author>
</authors>
<title>Lemmagen: Multilingual lemmatisation with induced ripple-down rules.</title>
<date>2010</date>
<journal>Journal of Universal Computer Science,</journal>
<volume>16</volume>
<issue>9</issue>
<marker>Jurˇsiˇc, Mozetiˇc, Erjavec, Lavraˇc, 2010</marker>
<rawString>Matjaz Jurˇsiˇc, Igor Mozetiˇc, Tomaz Erjavec, and Nada Lavraˇc. 2010. Lemmagen: Multilingual lemmatisation with induced ripple-down rules. Journal of Universal Computer Science, 16(9):1190–1214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Llu´ıs</author>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Joint arc-factored parsing of syntactic and semantic dependencies.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--219</pages>
<marker>Llu´ıs, Carreras, M`arquez, 2013</marker>
<rawString>Xavier Llu´ıs, Xavier Carreras, and Llu´ıs M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic dependencies. Transactions of the Association for Computational Linguistics, 1:219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maddalen L´opez de la Calle</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>First steps towards a predicate matrix.</title>
<date>2014</date>
<booktitle>In Proceedings of the Global WordNet Conference (GWC 2014),</booktitle>
<publisher>GWA.</publisher>
<location>Tartu, Estonia,</location>
<contexts>
<context position="3140" citStr="Calle et al., 2014" startWordPosition="446" endWordPosition="449">he linguistic anaylsis with links to knowledge bases for different languages, or links to language independent representations. 2 Linguistic Analyzers Apart from basic state-of-the-art tokenizers, lemmatizers, PoS/MSD taggers, and NE recognizers, each pipeline requires deeper processors able to build the target language-independent semantic representantion. For that, we rely on three steps: dependency parsing, semantic role labeling and word sense disambiguation. These three processes, combined with multilingual ontological resouces such as different WordNets and PredicateMatrix (L´opez de la Calle et al., 2014), a lexical semantics resource combining WordNet, FrameNet, and VerbNet, are the key to the construction of our semantic representation. 2.1 Dependency Parsing We use graph-based methods for dependency parsing, namely, MSTParser3 (McDonald et al., 2005) is used for Chinese and Croatian, and Treeler4 is used for the other languages. Treeler is a library developed by the UPC team that implements several statistical methods for tagging and parsing. We use these tools in order to train dependency parsers for all XLike languages using standard available treebanks. 3http://sourceforge.net/projects/m</context>
</contexts>
<marker>Calle, Laparra, Rigau, 2014</marker>
<rawString>Maddalen L´opez de la Calle, Egoitz Laparra, and German Rigau. 2014. First steps towards a predicate matrix. In Proceedings of the Global WordNet Conference (GWC 2014), Tartu, Estonia, January. GWA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3393" citStr="McDonald et al., 2005" startWordPosition="482" endWordPosition="485">h pipeline requires deeper processors able to build the target language-independent semantic representantion. For that, we rely on three steps: dependency parsing, semantic role labeling and word sense disambiguation. These three processes, combined with multilingual ontological resouces such as different WordNets and PredicateMatrix (L´opez de la Calle et al., 2014), a lexical semantics resource combining WordNet, FrameNet, and VerbNet, are the key to the construction of our semantic representation. 2.1 Dependency Parsing We use graph-based methods for dependency parsing, namely, MSTParser3 (McDonald et al., 2005) is used for Chinese and Croatian, and Treeler4 is used for the other languages. Treeler is a library developed by the UPC team that implements several statistical methods for tagging and parsing. We use these tools in order to train dependency parsers for all XLike languages using standard available treebanks. 3http://sourceforge.net/projects/mstparser 4http://treeler.lsi.upc.edu 9 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 9–12, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computatio</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>Freeling 3.0: Towards wider multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC 2012),</booktitle>
<publisher>ELRA.</publisher>
<location>Istanbul, Turkey,</location>
<marker>Padr´o, Stanilovsky, 2012</marker>
<rawString>Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadej ˇStajner</author>
<author>Delia Rusu</author>
<author>Lorand Dali</author>
<author>Blaˇz Fortuna</author>
<author>Dunja Mladeni´c</author>
<author>Marko Grobelnik</author>
</authors>
<title>A service oriented framework for natural language text enrichment.</title>
<date>2010</date>
<journal>Informatica,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>ˇStajner, Rusu, Dali, Fortuna, Mladeni´c, Grobelnik, 2010</marker>
<rawString>Tadej ˇStajner, Delia Rusu, Lorand Dali, Blaˇz Fortuna, Dunja Mladeni´c, and Marko Grobelnik. 2010. A service oriented framework for natural language text enrichment. Informatica, 34(3):307–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Lin- guistics on Human Language Technology (NAACL’03).</booktitle>
<contexts>
<context position="9628" citStr="Toutanova et al., 2003" startWordPosition="1444" endWordPosition="1447">such as Amazon Elastic Compute Cloud EC25) as needed. Furthermore it also provides independent development lifecycles for each language which is crucial in this type of research projects. Recall that these web services can be deployed locally or remotely, maintaining the option of using them in a standalone configuration. The main structure for each one of the pipelines is described below: • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6, Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is provided by JSI Enrycher7 (ˇStajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) and a CRF-based entity extractor (ˇStajner et al., 2012). Dependency parsing, word sense 5http://aws.amazon.com/ec2/ 6http://opennlp.apache.org 7http://enrycher.ijs.si disambiguation are based on FreeLing and Tree</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Lin- guistics on Human Language Technology (NAACL’03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadej ˇStajner</author>
<author>Tomaˇz Erjavec</author>
<author>Simon Krek</author>
</authors>
<title>Razpoznavanje imenskih entitet v slovenskem besedilu. In</title>
<date>2012</date>
<booktitle>In Proceedings of 15th Internation Multiconference on Information Society - Jezikovne Tehnologije,</booktitle>
<location>Ljubljana, Slovenia.</location>
<marker>ˇStajner, Erjavec, Krek, 2012</marker>
<rawString>Tadej ˇStajner, Tomaˇz Erjavec, and Simon Krek. 2012. Razpoznavanje imenskih entitet v slovenskem besedilu. In In Proceedings of 15th Internation Multiconference on Information Society - Jezikovne Tehnologije, Ljubljana, Slovenia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>