<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9481765">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 360-367.
</note>
<title confidence="0.748013">
Bootstrapping
</title>
<author confidence="0.4955125">
Steven Abney
AT&amp;T Laboratories – Research
</author>
<keyword confidence="0.4061025">
180 Park Avenue
Florham Park, NJ, USA, 07932
</keyword>
<sectionHeader confidence="0.954479" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993375">
This paper refines the analysis of co-
training, defines and evaluates a new
co-training algorithm that has theo-
retical justification, gives a theoreti-
cal justification for the Yarowsky algo-
rithm, and shows that co-training and
the Yarowsky algorithm are based on
different independence assumptions.
</bodyText>
<sectionHeader confidence="0.9793" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999944266666667">
The term bootstrapping here refers to a prob-
lem setting in which one is given a small set of
labeled data and a large set of unlabeled data,
and the task is to induce a classifier. The plen-
itude of unlabeled natural language data, and
the paucity of labeled data, have made boot-
strapping a topic of interest in computational
linguistics. Current work has been spurred by
two papers, (Yarowsky, 1995) and (Blum and
Mitchell, 1998).
Blum and Mitchell propose a conditional in-
dependence assumption to account for the effi-
cacy of their algorithm, called co-training, and
they give a proof based on that conditional in-
dependence assumption. They also give an in-
tuitive explanation of why co-training works,
in terms of maximizing agreement on unla-
beled data between classifiers based on different
“views” of the data. Finally, they suggest that
the Yarowsky algorithm is a special case of the
co-training algorithm.
The Blum and Mitchell paper has been very
influential, but it has some shortcomings. The
proof they give does not actually apply directly
to the co-training algorithm, nor does it directly
justify the intuitive account in terms of classifier
agreement on unlabeled data, nor, for that mat-
ter, does the co-training algorithm directly seek
to find classifiers that agree on unlabeled data.
Moreover, the suggestion that the Yarowsky al-
gorithm is a special case of co-training is based
on an incidental detail of the particular applica-
tion that Yarowsky considers, not on the prop-
erties of the core algorithm.
In recent work, (Dasgupta et al., 2001) prove
that a classifier has low generalization error if it
agrees on unlabeled data with a second classifier
based on a different “view” of the data. This
addresses one of the shortcomings of the original
co-training paper: it gives a proof that justifies
searching for classifiers that agree on unlabeled
data.
I extend this work in two ways. First, (Das-
gupta et al., 2001) assume the same conditional
independence assumption as proposed by Blum
and Mitchell. I show that that independence as-
sumption is remarkably powerful, and violated
in the data; however, I show that a weaker as-
sumption suffices. Second, I give an algorithm
that finds classifiers that agree on unlabeled
data, and I report on an implementation and
empirical results.
Finally, I consider the question of the re-
lation between the co-training algorithm and
the Yarowsky algorithm. I suggest that the
Yarowsky algorithm is actually based on a dif-
ferent independence assumption, and I show
that, if the independence assumption holds, the
Yarowsky algorithm is effective at finding a
high-precision classifier.
</bodyText>
<sectionHeader confidence="0.646231" genericHeader="introduction">
2 Problem Setting and Notation
</sectionHeader>
<bodyText confidence="0.972330448275862">
A bootstrapping problem consists of a space
of instances X, a set of labels L, a function
Y : X -+ G assigning labels to instances,
and a space of rules mapping instances to la-
bels. Rules may be partial functions; we write
F(x) = + if F abstains (that is, makes no pre-
diction) on input x. “Classifier” is synonymous
with “rule”.
It is often useful to think of rules and labels
as sets of instances. A binary rule F can be
thought of as the characteristic function of the
set of instances {x : F(x) = +}. Multi-class
rules also define useful sets when a particular
target class t is understood. For any rule F,
we write F` for the set of instances {x : F(x) =
�}, or (ambiguously) for that set’s characteristic
function.
We write F` for the complement of F`, either
as a set or characteristic function. Note that
¯F` contains instances on which F abstains. We
write F¯` for {x : F(x) =� t n F(x) =� +}. When
F does not abstain, ¯F` and F¯` are identical.
Finally, in expressions like Pr[F = +|Y = +]
(with square brackets and “Pr”), the functions
F(x) and Y (x) are used as random variables.
By contrast, in the expression P(F|Y ) (with
parentheses and “P”), F is the set of instances
for which F(x) = +, and Y is the set of in-
stances for which Y (x) = +.
</bodyText>
<sectionHeader confidence="0.965917" genericHeader="method">
3 View Independence
</sectionHeader>
<bodyText confidence="0.902680125">
Blum and Mitchell assume that each instance x
consists of two “views” x1, x2. We can take this
as the assumption of functions X1 and X2 such
that X1(x) = x1 and X2(x) = x2. They propose
that views are conditionally independent given
the label.
Definition 1 A pair of views x1, x2 satisfy
view independence just in case:
</bodyText>
<equation confidence="0.999968">
Pr[X1 = x1|X2 = x2, Y = y] = Pr[X1 = x1|Y = y]
Pr[X2 = x2lX1 = x1, Y = y] = Pr[X2 = x21Y = y]
</equation>
<construct confidence="0.575069">
A classification problem instance satisfies view
independence just in case all pairs x1, x2 satisfy
view independence.
</construct>
<bodyText confidence="0.9011106">
There is a related independence assumption
that will prove useful. Let us define W1 to con-
sist of rules that are functions of X1 only, and
define W2 to consist of rules that are functions
of X2 only.
</bodyText>
<construct confidence="0.987397333333333">
Definition 2 A pair of rules F E W1, G E W2
satisfies rule independence just in case, for all
u, v, y:
</construct>
<equation confidence="0.965076">
Pr[F = u|G = v,Y = y] = Pr[F = u|Y = y]
</equation>
<construct confidence="0.68684625">
and similarly for F E W2, G E W1. A classi-
fication problem instance satisfies rule indepen-
dence just in case all opposing-view rule pairs
satisfy rule independence.
</construct>
<bodyText confidence="0.967057777777778">
If instead of generating W1 and W2 from X1
and X2, we assume a set of features F (which
can be thought of as binary rules), and take
W1 = W2 = F, rule independence reduces to
the Naive Bayes independence assumption.
The following theorem is not difficult to
prove; we omit the proof.
Theorem 1 View independence implies rule
independence.
</bodyText>
<sectionHeader confidence="0.6375825" genericHeader="method">
4 Rule Independence and
Bootstrapping
</sectionHeader>
<bodyText confidence="0.986425666666667">
Blum and Mitchell’s paper suggests that rules
that agree on unlabelled instances are useful in
bootstrapping.
</bodyText>
<construct confidence="0.5463755">
Definition 3 The agreement rate between
rules F and G is
</construct>
<equation confidence="0.963323">
Pr[F = G|F, G =�+]
</equation>
<bodyText confidence="0.917466666666667">
Note that the agreement rate between rules
makes no reference to labels; it can be deter-
mined from unlabeled data.
The algorithm that Blum and Mitchell de-
scribe does not explicitly search for rules with
good agreement; nor does agreement rate play
any direct role in the learnability proof given in
the Blum and Mitchell paper.
The second lack is emended in (Dasgupta
et al., 2001). They show that, if view inde-
pendence is satisfied, then the agreement rate
between opposing-view rules F and G upper
bounds the error of F (or G). The following
statement of the theorem is simplified and as-
sumes non-abstaining binary rules.
Theorem 2 For all F E W1, G E W2 that sat-
isfy rule independence and are nontrivial predic-
tors in the sense that minu Pr[F = u] &gt; Pr[F =�
</bodyText>
<equation confidence="0.839576333333333">
G], one of the following inequalities holds:
Pr[F =� Y ] G Pr[F =� G]
Pr[F =� Y ] G Pr[F =� G]
</equation>
<bodyText confidence="0.999852636363637">
If F agrees with G on all but E unlabelled in-
stances, then either F or F¯ predicts Y with er-
ror no greater than E. A small amount of la-
belled data suffices to choose between F and F¯.
I give a geometric proof sketch here; the
reader is referred to the original paper for a for-
mal proof. Consider figures 1 and 2. In these
diagrams, area represents probability. For ex-
ample, the leftmost box (in either diagram) rep-
resents the instances for which Y = +, and
the area of its upper left quadrant represents
Pr[F = +, G = +, Y = +]. Typically, in such
a diagram, either the horizontal or vertical line
is broken, as in figure 2. In the special case in
which rule independence is satisfied, both hori-
zontal and vertical lines are unbroken, as in fig-
ure 1.
Theorem 2 states that disagreement upper
bounds error. First let us consider a lemma, to
wit: disagreement upper bounds minority prob-
abilities. Define the minority value of F given
Y = y to be the value u with least probability
Pr[F = u|Y = y]; the minority probability is the
probability of the minority value. (Note that
minority probabilities are conditional probabili-
ties, and distinct from the marginal probability
mina Pr[F = u] mentioned in the theorem.)
In figure 1a, the areas of disagreement are
the upper right and lower left quadrants of each
box, as marked. The areas of minority values
are marked in figure 1b. It should be obvious
that the area of disagreement upper bounds the
area of minority values.
The error values of F are the values opposite
to the values of Y : the error value is − when
Y = + and + when Y = −. When minority
values are error values, as in figure 1, disagree-
ment upper bounds error, and theorem 2 follows
immediately.
However, three other cases are possible. One
possibility is that minority values are opposite
to error values. In this case, the minority val-
ues of F¯ are error values, and disagreement be-
tween F and G upper bounds the error of F¯.
</bodyText>
<figureCaption confidence="0.610147">
Figure 1: Disagreement upper-bounds minority
probabilities.
</figureCaption>
<bodyText confidence="0.999934714285714">
This case is admitted by theorem 2. In the
final two cases, minority values are the same
regardless of the value of Y . In these cases,
however, the predictors do not satisfy the “non-
triviality” condition of theorem 2, which re-
quires that mina Pr[F = u] be greater than the
disagreement between F and G.
</bodyText>
<sectionHeader confidence="0.912478" genericHeader="method">
5 The Unreasonableness of Rule
Independence
</sectionHeader>
<bodyText confidence="0.999867785714286">
Rule independence is a very strong assumption;
one remarkable consequence will show just how
strong it is. The precision of a rule F is de-
fined to be Pr[Y = +|F = +]. (We continue to
assume non-abstaining binary rules.) If rule in-
dependence holds, knowing the precision of any
one rule allows one to exactly compute the preci-
sion of every other rule given only unlabeled data
and knowledge of the size of the target concept.
Let F and G be arbitrary rules based on in-
dependent views. We first derive an expression
for the precision of F in terms of G. Note that
the second line is derived from the first by rule
independence.
</bodyText>
<equation confidence="0.9996492">
P(FG) = P(F|GY )P(GY ) + P(FJGY¯)P(GY¯)
= P(F|Y )P(GY ) + P(F|Y)P(GY¯)
P(G|F) = P(Y |F)P(G|Y ) + [1 − P(Y |F)]P(G|Y¯)
P(Y |F) = P(G|F)−P(G|
P(G|Y )−P(G|
</equation>
<bodyText confidence="0.999980875">
To compute the expression on the righthand
side of the last line, we require P(Y |G), P(Y ),
P(G|F), and P(G). The first value, the preci-
sion of G, is assumed known. The second value,
P(Y ), is also assumed known; it can at any rate
be estimated from a small amount of labeled
data. The last two values, P(G|F) and P(G),
can be computed from unlabeled data.
</bodyText>
<equation confidence="0.868686777777778">
G
−
G
+ − +
Y = + Y = −
+
F
−
(b) Minority Values
Y = + Y = −
G G
+ − + −
(a) Disagreement
+
F
−
Y¯)
Y¯)
</equation>
<bodyText confidence="0.999344285714286">
Thus, given the precision of an arbitrary rule
G, we can compute the precision of any other-
view rule F. Then we can compute the precision
of rules based on the same view as G by using
the precision of some other-view rule F. Hence
we can compute the precision of every rule given
the precision of any one.
</bodyText>
<table confidence="0.99968025">
F Co-training Yarowsky Truth
M:chairman -12.7 .068 .030
X:Company-of 10.2 .979 .989
X:court-in -.183 1.00 .981
X:Company-in 75.7 1.00 .986
X:firm-in -9.94 .952 .949
X:%-in -15.2 .875 .192
X:meeting-in -2.25 1.00 .753
</table>
<tableCaption confidence="0.999306">
Table 1: Some data
</tableCaption>
<figure confidence="0.808409454545454">
6 Some Data
Y = + Y = −
Y = + Y = −
G G
+ − + −
F +
−
(b) Negative correlation
G G
+ − + −
(a) Positive correlation
</figure>
<bodyText confidence="0.999358842105263">
The empirical investigations described here and
below use the data set of (Collins and Singer,
1999). The task is to classify names in text
as person, location, or organization. There is
an unlabeled training set containing 89,305 in-
stances, and a labeled test set containing 289
persons, 186 locations, 402 organizations, and
123 “other”, for a total of 1,000 instances.
Instances are represented as lists of features.
Intrinsic features are the words making up the
name, and contextual features are features of
the syntactic context in which the name oc-
curs. For example, consider Bruce Kaplan,
president of Metals Inc. This text snippet con-
tains two instances. The first has intrinsic fea-
tures N:Bruce-Kaplan, C:Bruce, and C:Kaplan
(“N” for the complete name, “C” for “con-
tains”), and contextual feature M:president
(“M” for “modified by”). The second instance
has intrinsic features N:Metals-Inc, C:Metals,
C:Inc, and contextual feature X:president-of
(“X” for “in the context of”).
Let us define Y (x) = + if x is a “location”
instance, and Y (x) = − otherwise. We can es-
timate P(Y ) from the test sample; it contains
186/1000 location instances, giving P(Y ) =
.186.
Let us treat each feature F as a rule predict-
ing + when F is present and − otherwise. The
precision of F is P(Y |F). The internal feature
N:New-York has precision 1. This permits us to
compute the precision of various contextual fea-
tures, as shown in the “Co-training” column of
Table 1. We note that the numbers do not even
look like probabilities. The cause is the failure
of view independence to hold in the data, com-
bined with the instability of the estimator. (The
“Yarowsky” column uses a seed rule to estimate
</bodyText>
<figure confidence="0.724071">
+
F
</figure>
<figureCaption confidence="0.9576595">
Figure 2: Deviation from conditional indepen-
dence.
</figureCaption>
<bodyText confidence="0.929209">
P(Y |F), as is done in the Yarowsky algorithm,
and the “Truth” column shows the true value
of P(Y |F).)
</bodyText>
<sectionHeader confidence="0.978422" genericHeader="method">
7 Relaxing the Assumption
</sectionHeader>
<bodyText confidence="0.999953571428572">
Nonetheless, the unreasonableness of view inde-
pendence does not mean we must abandon the-
orem 2. In this section, we introduce a weaker
assumption, one that is satisfied by the data,
and we show that theorem 2 holds under this
weaker assumption.
There are two ways in which the data can di-
verge from conditional independence: the rules
may either be positively or negatively corre-
lated, given the class value. Figure 2a illus-
trates positive correlation, and figure 2b illus-
trates negative correlation.
If the rules are negatively correlated, then
their disagreement (shaded in figure 2) is larger
than if they are conditionally independent, and
the conclusion of theorem 2 is maintained a for-
tiori. Unfortunately, in the data, they are posi-
tively correlated, so the theorem does not apply.
Let us quantify the amount of deviation from
conditional independence. We define the condi-
tional dependence of F and G given Y = y to
</bodyText>
<equation confidence="0.9718045">
be dy =
|Pr[G = v|Y = y,F = u]−Pr[G = v|Y = y]|
</equation>
<bodyText confidence="0.886243260869565">
If F and G are conditionally independent, then
dy = 0.
This permits us to state a weaker version of
rule independence:
Definition 4 Rules F and G satisfy weak rule
dependence just in case, for y E {+, −}:
dy ` p2 2p1q1
where p1 = minu Pr[F = u|Y = y], p2 =
minu Pr[G = u|Y = y], and q1 = 1 − p1.
By definition, p1 and p2 cannot exceed 0.5. If
p1 = 0.5, then weak rule dependence reduces to
independence: if p1 = 0.5 and weak rule depen-
dence is satisfied, then dy must be 0, which is
to say, F and G must be conditionally indepen-
dent. However, as p1 decreases, the permissible
amount of conditional dependence increases.
We can now state a generalized version of the-
orem 2:
Theorem 3 For all F E W1, G E W2 that
satisfy weak rule dependence and are nontrivial
predictors in the sense that minu Pr[F = u] &gt;
Pr[F =� G], one of the following inequalities
holds:
</bodyText>
<equation confidence="0.9970545">
Pr[F =� Y ] ` Pr[F =� G]
Pr[ F¯ =� Y ] ` Pr[F =� G]
</equation>
<bodyText confidence="0.971646266666667">
Consider figure 3. This illustrates the most
relevant case, in which F and G are positively
correlated given Y . (Only the case Y = + is
shown; the case Y = − is similar.) We assume
that the minority values of F are error values;
the other cases are handled as in the discussion
of theorem 2.
Let u be the minority value of G when Y = +.
In figure 3, a is the probability that G = u when
F takes its minority value, and b is the proba-
bility that G = u when F takes its majority
value.
The value r = a − b is the difference. Note
that r = 0 if F and G are conditionally inde-
pendent given Y = +. In fact, we can show
</bodyText>
<figureCaption confidence="0.953508">
Figure 3: Positive correlation, Y = +.
</figureCaption>
<equation confidence="0.99349">
that r is exactly our measure dy of conditional
dependence:
2dy = |a − p2 |+ |b − p2 |+ |(1 − a) − (1 − p2)|
+|(1 − b) − (1 − p2)|
= |a − b |+ |a − b|
= 2r
</equation>
<bodyText confidence="0.969513722222222">
Hence, in particular, we may write dy = a − b.
Observe further that p2, the minority proba-
bility of G when Y = +, is a weighted average of
a and b, namely, p2 = p1a+q1b. Combining this
with the equation dy = a−b allows us to express
a and b in terms of the remaining variables, to
wit: a = p2 + q1dy and b = p2 − p1dy.
In order to prove theorem 3, we need to show
that the area of disagreement (B U C) upper
bounds the area of the minority value of F (AU
B). This is true just in case C is larger than A,
which is to say, if bq1 &gt; ap1. Substituting our
expressions for a and b into this inequality and
solving for dy yields:
dy ` p2 2p1q1
In short, disagreement upper bounds the mi-
nority probability just in case weak rule depen-
dence is satisfied, proving the theorem.
</bodyText>
<sectionHeader confidence="0.964097" genericHeader="method">
8 The Greedy Agreement Algorithm
</sectionHeader>
<bodyText confidence="0.9661224">
Dasgupta, Littman, and McAllester suggest a
possible algorithm at the end of their paper, but
they give only the briefest suggestion, and do
not implement or evaluate it. I give here an
algorithm, the Greedy Agreement Algorithm,
</bodyText>
<figure confidence="0.961120111111111">
q
2
p
2
q1
D
✂ �
C✂�
r
☎ ✄
p✄ ☎ ✄ ☎ ☎ ✄
1 ☎ ✄ ☎ ✄B
A
b
a
1 2�
u,v
q1 − p1
q1 − p1
Input: seed rules F, G
loop
for each atomic rule H
G’ = G + H
evaluate cost of (F,G’)
keep lowest-cost G’
if G’ is worse than G, quit
swap F, G’
</figure>
<figureCaption confidence="0.999921">
Figure 4: The Greedy Agreement Algorithm
</figureCaption>
<bodyText confidence="0.942013608695652">
that constructs paired rules that agree on un-
labeled data, and I examine its performance.
The algorithm is given in figure 4. It begins
with two seed rules, one for each view. At each
iteration, each possible extension to one of the
rules is considered and scored. The best one is
kept, and attention shifts to the other rule.
A complex rule (or classifier) is a list of atomic
rules H, each associating a single feature h with
a label t. H(x) = t if x has feature h, and
H(x) = L otherwise. A given atomic rule is
permitted to appear multiple times in the list.
Each atomic rule occurrence gets one vote, and
the classifier’s prediction is the label that re-
ceives the most votes. In case of a tie, there is
no prediction.
The cost of a classifier pair (F, G) is based
on a more general version of theorem 2, that
admits abstaining rules. The following theorem
is based on (Dasgupta et al., 2001).
Theorem 4 If view independence is satisfied,
and if F and G are rules based on different
views, then one of the following holds:
</bodyText>
<equation confidence="0.979831333333333">
Pr[F =� Y JF =� L] &lt; δ
µ−δ
Pr[F¯ =� Y J F¯ =� L] &lt; δ
µ−δ
where S = Pr[F =� G|F, G =� L], and µ =
minaPr[F = u|F =� L].
</equation>
<bodyText confidence="0.9996457">
In other words, for a given binary rule F, a pes-
simistic estimate of the number of errors made
by F is S/(µ − S) times the number of instances
labeled by F, plus the number of instances left
unlabeled by F. Finally, we note that the cost
of F is sensitive to the choice of G, but the cost
of F with respect to G is not necessarily the
same as the cost of G with respect to F. To get
an overall cost, we average the cost of F with
respect to G and G with respect to F.
</bodyText>
<figure confidence="0.997180625">
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.9611445">
Figure 5: Performance of the greedy agreement
algorithm
</figureCaption>
<bodyText confidence="0.9981685">
Figure 5 shows the performance of the greedy
agreement algorithm after each iteration. Be-
cause not all test instances are labeled (some
are neither persons nor locations nor organiza-
tions), and because classifiers do not label all
instances, we show precision and recall rather
than a single error rate. The contour lines show
levels of the F-measure (the harmonic mean of
precision and recall). The algorithm is run to
convergence, that is, until no atomic rule can
be found that decreases cost. It is interesting
to note that there is no significant overtrain-
ing with respect to F-measure. The final values
are 89.2/80.4/84.5 recall/precision/F-measure,
which compare favorably with the performance
of the Yarowsky algorithm (83.3/84.6/84.0).
(Collins and Singer, 1999) add a special final
round to boost recall, yielding 91.2/80.0/85.2
for the Yarowsky algorithm and 91.3/80.1/85.3
for their version of the original co-training algo-
rithm. All four algorithms essentially perform
equally well; the advantage of the greedy agree-
ment algorithm is that we have an explanation
for why it performs well.
</bodyText>
<sectionHeader confidence="0.702156" genericHeader="method">
9 The Yarowsky Algorithm
</sectionHeader>
<bodyText confidence="0.9709372">
For Yarowsky’s algorithm, a classifier again con-
sists of a list of atomic rules. The prediction of
the classifier is the prediction of the first rule in
the list that applies. The algorithm constructs a
classifier iteratively, beginning with a seed rule.
In the variant we consider here, one atomic rule
is added at each iteration. An atomic rule F` is
chosen only if its precision, Pr[G` = +JF` = +]
(as measured using the labels assigned by the
current classifier G), exceeds a fixed threshold
θ.1
Yarowsky does not give an explicit justifica-
tion for the algorithm. I show here that the
algorithm can be justified on the basis of two
independence assumptions. In what follows, F
represents an atomic rule under consideration,
and G represents the current classifier. Recall
that Y` is the set of instances whose true label is
t, and G` is the set of instances {x : G(x) = t}.
We write G∗ for the set of instances labeled by
the current classifier, that is, {x : G(x) =� +}.
The first assumption is precision indepen-
dence.
Definition 5 Candidate rule F` and classifier
G satisfy precision independence just in case
</bodyText>
<equation confidence="0.336572">
P(Y`JF`, G∗) = P(Y`JF`)
</equation>
<construct confidence="0.9863084">
A bootstrapping problem instance satisfies pre-
cision independence just in case all rules G and
all atomic rules F` that nontrivially overlap with
G (both F` flG∗ and F` −G∗ are nonempty) sat-
isfy precision independence.
</construct>
<bodyText confidence="0.984780282051282">
Precision independence is stated here so that it
looks like a conditional independence assump-
tion, to emphasize the similarity to the analysis
of co-training. In fact, it is only “half” an in-
dependence assumption—for precision indepen-
dence, it is not necessary that P(Y`J ¯F`, G∗) =
P(Y`J
The second assumption is that classifiers
make balanced errors. That is:
P(Y`, G¯`JF`) = P(Y¯`, G`JF`)
Let us first consider a concrete (but hypo-
thetical) example. Suppose the initial classifier
correctly labels 100 out of 1000 instances, and
makes no mistakes. Then the initial precision is
1(Yarowsky, 1995), citing (Yarowsky, 1994), actually
uses a superficially different score that is, however, a
monotone transform of precision, hence equivalent to
precision, since it is used only for sorting.
1 and recall is 0.1. Suppose further that we add
an atomic rule that correctly labels 19 new in-
stances, and incorrectly labels one new instance.
The rule’s precision is 0.95. The precision of
the new classifier (the old classifier plus the new
atomic rule) is 119/120 = 0.99. Note that the
new precision lies between the old precision and
the precision of the rule. We will show that this
is always the case, given precision independence
and balanced errors.
We need to consider several quantities: the
precision of the current classifier, P(Y`JG`);
the precision of the rule under consideration,
P(Y`JF`); the precision of the rule on the cur-
rent labeled set, P(Y`JF`G∗); and the precision
of the rule as measured using estimated labels,
P(G`JF`G∗).
The assumption of balanced errors implies
that measured precision equals true precision on
labeled instances, as follows. (We assume here
that all instances have true labels, hence that
</bodyText>
<equation confidence="0.9045762">
¯Y` = Y¯`.)
P(F`G`¯Y`) = P(F`G¯`Y`)
P(F`G`Y`) + P(F`G` ¯Y`) = P(Y`F`G`) + P(F`G¯`Y`)
P(F`G`) = P(Y`F`G*)
P(G`|F`G*) = P(Y`|F`G*)
</equation>
<bodyText confidence="0.999553941176471">
This, combined with precision independence,
implies that the precision of F` as measured
on the labeled set is equal to its true precision
P(Y`JF`).
Now consider the precision of the old and new
classifiers at predicting t. Of the instances that
the old classifier labels t, let A be the num-
ber that are correctly labeled and B be the
number that are incorrectly labeled. Defining
Nt = A + B, the precision of the old classifier
is Qt = A/Nt. Let ΔA be the number of new
instances that the rule under consideration cor-
rectly labels, and let ΔB be the number that it
incorrectly labels. Defining n = ΔA + ΔB, the
precision of the rule is q = ΔA/n. The precision
of the new classifier is Qt+1 = (A + ΔA)/Nt+1,
which can be written as:
</bodyText>
<equation confidence="0.927192">
Nt n
Qt+1 = Qt + q
Nt+1 Nt+1
</equation>
<bodyText confidence="0.998286">
That is, the precision of the new classifier is
a weighted average of the precision of the old
classifier and the precision of the new rule.
</bodyText>
<figure confidence="0.993959777777778">
F`).
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.923978">
Figure 6: Performance of the Yarowsky algo-
rithm
</figureCaption>
<bodyText confidence="0.9838750625">
An immediate consequence is that, if we only
accept rules whose precision exceeds a given
threshold B, then the precision of the new classi-
fier exceeds B. Since measured precision equals
true precision under our previous assumptions,
it follows that the true precision of the final clas-
sifier exceeds B if the measured precision of ev-
ery accepted rule exceeds B.
Moreover, observe that recall can be written
as:
Nt Qt
N`
where N` is the number of instances whose true
label is t. If Qt &gt; B, then recall is bounded
below by NtB/N`, which grows as Nt grows.
Hence we have proven the following theorem.
</bodyText>
<construct confidence="0.880514285714286">
Theorem 5 If the assumptions of precision in-
dependence and balanced errors are satisfied,
then the Yarowsky algorithm with threshold B
obtains a final classifier whose precision is at
least B. Moreover, recall is bounded below by
NtB/N`, a quantity which increases at each
round.
</construct>
<bodyText confidence="0.999857714285715">
Intuitively, the Yarowsky algorithm increases
recall while holding precision above a threshold
that represents the desired precision of the final
classifier. The empirical behavior of the algo-
rithm, as shown in figure 6, is in accordance
with this analysis.
We have seen, then, that the Yarowsky algo-
rithm, like the co-training algorithm, can be jus-
tified on the basis of an independence assump-
tion, precision independence. It is important to
note, however, that the Yarowsky algorithm is
not a special case of co-training. Precision in-
dependence and view independence are distinct
assumptions; neither implies the other.2
</bodyText>
<sectionHeader confidence="0.99408" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.9999235">
To sum up, we have refined previous work on
the analysis of co-training, and given a new co-
training algorithm that is theoretically justified
and has good empirical performance.
We have also given a theoretical analysis of
the Yarowsky algorithm for the first time, and
shown that it can be justified by an indepen-
dence assumption that is quite distinct from
the independence assumption that co-training
is based on.
</bodyText>
<sectionHeader confidence="0.996122" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996690272727273">
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT:
Proceedings of the Workshop on Computational
Learning Theory. Morgan Kaufmann Publishers.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP.
Sanjoy Dasgupta, Michael Littman, and David
McAllester. 2001. PAC generalization bounds for
co-training. In Proceedings of NIPS.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution. In Proceedings ACL 32.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
189–196.
2To see that view independence does not imply pre-
cision indepence, consider an example in which G = Y
always. This is compatible with rule independence, but
it implies that P(YIFG) = 1 and P(Y |F¯G) = 0, violat-
ing precision independence.
</reference>
<figure confidence="0.941076333333333">
A
=
N`
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.018364">
<note confidence="0.998005">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 360-367.</note>
<title confidence="0.986332">Bootstrapping</title>
<author confidence="0.999949">Steven Abney</author>
<affiliation confidence="0.999898">AT&amp;T Laboratories – Research</affiliation>
<address confidence="0.997564">180 Park Avenue Florham Park, NJ, USA, 07932</address>
<abstract confidence="0.981601682370821">This paper refines the analysis of cotraining, defines and evaluates a new co-training algorithm that has theoretical justification, gives a theoretical justification for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions. 1 Overview term refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics. Current work has been spurred by two papers, (Yarowsky, 1995) and (Blum and Mitchell, 1998). Blum and Mitchell propose a conditional independence assumption to account for the effiof their algorithm, called and they give a proof based on that conditional independence assumption. They also give an intuitive explanation of why co-training works, in terms of maximizing agreement on unlabeled data between classifiers based on different “views” of the data. Finally, they suggest that the Yarowsky algorithm is a special case of the co-training algorithm. The Blum and Mitchell paper has been very influential, but it has some shortcomings. The proof they give does not actually apply directly to the co-training algorithm, nor does it directly justify the intuitive account in terms of classifier agreement on unlabeled data, nor, for that matter, does the co-training algorithm directly seek to find classifiers that agree on unlabeled data. Moreover, the suggestion that the Yarowsky algorithm is a special case of co-training is based on an incidental detail of the particular application that Yarowsky considers, not on the properties of the core algorithm. In recent work, (Dasgupta et al., 2001) prove that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on a different “view” of the data. This addresses one of the shortcomings of the original co-training paper: it gives a proof that justifies searching for classifiers that agree on unlabeled data. I extend this work in two ways. First, (Dasgupta et al., 2001) assume the same conditional independence assumption as proposed by Blum and Mitchell. I show that that independence assumption is remarkably powerful, and violated in the data; however, I show that a weaker assumption suffices. Second, I give an algorithm that finds classifiers that agree on unlabeled data, and I report on an implementation and empirical results. Finally, I consider the question of the relation between the co-training algorithm and the Yarowsky algorithm. I suggest that the Yarowsky algorithm is actually based on a different independence assumption, and I show that, if the independence assumption holds, the Yarowsky algorithm is effective at finding a high-precision classifier. 2 Problem Setting and Notation A bootstrapping problem consists of a space instances a set of labels a function -+ G labels to instances, and a space of rules mapping instances to labels. Rules may be partial functions; we write = (that is, makes no preon input “Classifier” is synonymous with “rule”. It is often useful to think of rules and labels sets of instances. A binary rule be thought of as the characteristic function of the of instances = Multi-class rules also define useful sets when a particular class understood. For any rule write the set of instances = or (ambiguously) for that set’s characteristic function. write the complement of either as a set or characteristic function. Note that instances on which We t n When not abstain, identical. in expressions like +] (with square brackets and “Pr”), the functions and are used as random variables. contrast, in the expression (with and the set of instances which = +, and the set of infor which = +. 3 View Independence and Mitchell assume that each instance of two “views” We can take this the assumption of functions = = They propose that views are conditionally independent given the label. 1 pair of views independence in case: Y = Y = A classification problem instance satisfies view just in case all pairs view independence. There is a related independence assumption will prove useful. Let us define conof rules that are functions of and consist of rules that are functions 2 pair of rules F E G E independence in case, for all u, v, y: = similarly for F E G E A classification problem instance satisfies rule independence just in case all opposing-view rule pairs satisfy rule independence. instead of generating we assume a set of features can be thought of as binary rules), and take rule independence reduces to the Naive Bayes independence assumption. The following theorem is not difficult to prove; we omit the proof. 1 independence implies rule independence. 4 Rule Independence and Bootstrapping Blum and Mitchell’s paper suggests that rules that agree on unlabelled instances are useful in bootstrapping. 3 rate rules F and G is G Note that the agreement rate between rules makes no reference to labels; it can be determined from unlabeled data. The algorithm that Blum and Mitchell describe does not explicitly search for rules with good agreement; nor does agreement rate play any direct role in the learnability proof given in the Blum and Mitchell paper. The second lack is emended in (Dasgupta et al., 2001). They show that, if view independence is satisfied, then the agreement rate opposing-view rules the error of The following statement of the theorem is simplified and assumes non-abstaining binary rules. 2 all F E G E satisfy rule independence and are nontrivial predicin the sense that one of the following inequalities holds: with all but inthen either erno greater than A small amount of ladata suffices to choose between I give a geometric proof sketch here; the reader is referred to the original paper for a formal proof. Consider figures 1 and 2. In these diagrams, area represents probability. For example, the leftmost box (in either diagram) repthe instances for which +, and the area of its upper left quadrant represents G Y +]. Typically, in such a diagram, either the horizontal or vertical line is broken, as in figure 2. In the special case in which rule independence is satisfied, both horizontal and vertical lines are unbroken, as in figure 1. Theorem 2 states that disagreement upper bounds error. First let us consider a lemma, to wit: disagreement upper bounds minority prob- Define the value be the value least probability the probability the probability of the minority value. (Note that probabilities are probabilities, and distinct from the marginal probability mentioned in the theorem.) In figure 1a, the areas of disagreement are the upper right and lower left quadrants of each box, as marked. The areas of minority values are marked in figure 1b. It should be obvious that the area of disagreement upper bounds the area of minority values. values the values opposite the values of the error value is + and + when When minority values are error values, as in figure 1, disagreement upper bounds error, and theorem 2 follows immediately. However, three other cases are possible. One possibility is that minority values are opposite to error values. In this case, the minority valof error values, and disagreement bebounds the error of Figure 1: Disagreement upper-bounds minority probabilities. This case is admitted by theorem 2. In the final two cases, minority values are the same of the value of In these cases, however, the predictors do not satisfy the “nontriviality” condition of theorem 2, which rethat be greater than the between 5 The Unreasonableness of Rule Independence Rule independence is a very strong assumption; one remarkable consequence will show just how it is. The a rule deto be +]. (We continue to assume non-abstaining binary rules.) If rule independence holds, knowing the precision of any rule allows one to compute preciof every other rule given only and knowledge of the size of the target concept. arbitrary rules based on independent views. We first derive an expression the precision of terms of Note that the second line is derived from the first by rule independence. = + + = + [1 = To compute the expression on the righthand of the last line, we require and The first value, the preciof is assumed known. The second value, is also assumed known; it can at any rate be estimated from a small amount of labeled The last two values, and can be computed from unlabeled data. G − G + − + Y = + Y = − + F − (b) Minority Values Y = + Y = − G G + − + − (a) Disagreement + F − Thus, given the precision of an arbitrary rule we can compute the precision of any otherrule Then we can compute the precision rules based on the same view as using precision of some other-view rule Hence we can compute the precision of every rule given the precision of any one. F Co-training Yarowsky Truth M:chairman -12.7 .068 .030 X:Company-of 10.2 .979 .989 X:court-in -.183 1.00 .981 X:Company-in 75.7 1.00 .986 X:firm-in -9.94 .952 .949 X:%-in -15.2 .875 .192 X:meeting-in -2.25 1.00 .753 Table 1: Some data 6 Some Data Y = + Y = − Y = + Y = − G G + − + − F + − (b) Negative correlation G G + − + − (a) Positive correlation The empirical investigations described here and below use the data set of (Collins and Singer, 1999). The task is to classify names in text as person, location, or organization. There is an unlabeled training set containing 89,305 instances, and a labeled test set containing 289 persons, 186 locations, 402 organizations, and 123 “other”, for a total of 1,000 instances. Instances are represented as lists of features. features the words making up the and features features of the syntactic context in which the name oc- For example, consider Kaplan, of Metals Inc. text snippet contains two instances. The first has intrinsic feaand (“N” for the complete name, “C” for “conand contextual feature (“M” for “modified by”). The second instance intrinsic features and contextual feature (“X” for “in the context of”). us define = + if a “location” and = We can esfrom the test sample; it contains location instances, giving = us treat each feature a rule predict- + when present and The of The internal feature precision 1. This permits us to compute the precision of various contextual features, as shown in the “Co-training” column of Table 1. We note that the numbers do not even look like probabilities. The cause is the failure of view independence to hold in the data, combined with the instability of the estimator. (The “Yarowsky” column uses a seed rule to estimate + F Figure 2: Deviation from conditional independence. as is done in the Yarowsky algorithm, and the “Truth” column shows the true value 7 Relaxing the Assumption Nonetheless, the unreasonableness of view independence does not mean we must abandon theorem 2. In this section, we introduce a weaker one that by the data, and we show that theorem 2 holds under this weaker assumption. There are two ways in which the data can diverge from conditional independence: the rules may either be positively or negatively correlated, given the class value. Figure 2a illustrates positive correlation, and figure 2b illustrates negative correlation. If the rules are negatively correlated, then their disagreement (shaded in figure 2) is larger than if they are conditionally independent, and the conclusion of theorem 2 is maintained a fortiori. Unfortunately, in the data, they are positively correlated, so the theorem does not apply. Let us quantify the amount of deviation from independence. We define the condidependence conditionally independent, then 0. This permits us to state a weaker version of rule independence: 4 F and G satisfy rule in case, for y and 1 − definition, exceed 0.5. If then weak rule dependence reduces to if and weak rule depenis satisfied, then be 0, which is say, be conditionally indepen- However, as the permissible amount of conditional dependence increases. We can now state a generalized version of theorem 2: 3 all F G satisfy weak rule dependence and are nontrivial in the sense that one of the following inequalities holds: ` ` Consider figure 3. This illustrates the most case, in which positively given (Only the case + is the case − is similar.) We assume the minority values of error values; the other cases are handled as in the discussion of theorem 2. the minority value of +. figure 3, the probability that its minority value, and the probathat its majority value. value the difference. Note 0 if conditionally indegiven +. In fact, we can show 3: Positive correlation, +. exactly our measure conditional dependence: + + |(1 − − (1 − − − (1 − + in particular, we may write further that the minority probaof +, is a weighted average of namely, Combining this the equation us to express terms of the remaining variables, to In order to prove theorem 3, we need to show the area of disagreement upper the area of the minority value of This is true just in case larger than is to say, if Substituting our for this inequality and for In short, disagreement upper bounds the minority probability just in case weak rule dependence is satisfied, proving the theorem. 8 The Greedy Agreement Algorithm Dasgupta, Littman, and McAllester suggest a possible algorithm at the end of their paper, but they give only the briefest suggestion, and do not implement or evaluate it. I give here an algorithm, the Greedy Agreement Algorithm, q 2 p 2 D ✂ � r ☎ ✄ ✄ ☎ ☎ ✄ ✄ ☎ A b a u,v Input: seed rules F, G loop for each atomic rule H G’ = G + H evaluate cost of (F,G’) keep lowest-cost G’ if G’ is worse than G, quit swap F, G’ Figure 4: The Greedy Agreement Algorithm that constructs paired rules that agree on unlabeled data, and I examine its performance. The algorithm is given in figure 4. It begins with two seed rules, one for each view. At each iteration, each possible extension to one of the rules is considered and scored. The best one is kept, and attention shifts to the other rule. A complex rule (or classifier) is a list of atomic each associating a single feature label = feature and = A given atomic rule is permitted to appear multiple times in the list. Each atomic rule occurrence gets one vote, and the classifier’s prediction is the label that receives the most votes. In case of a tie, there is no prediction. cost of a classifier pair is based on a more general version of theorem 2, that admits abstaining rules. The following theorem is based on (Dasgupta et al., 2001). 4 view independence is satisfied, and if F and G are rules based on different views, then one of the following holds: µ−δ µ−δ S G and µ other words, for a given binary rule a pessimistic estimate of the number of errors made times the number of instances by plus the number of instances left by Finally, we note that the cost sensitive to the choice of but the cost respect to not necessarily the as the cost of respect to To get overall cost, we average the cost of to respect to 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 Recall Figure 5: Performance of the greedy agreement algorithm Figure 5 shows the performance of the greedy agreement algorithm after each iteration. Because not all test instances are labeled (some are neither persons nor locations nor organizations), and because classifiers do not label all instances, we show precision and recall rather than a single error rate. The contour lines show levels of the F-measure (the harmonic mean of precision and recall). The algorithm is run to convergence, that is, until no atomic rule can be found that decreases cost. It is interesting to note that there is no significant overtraining with respect to F-measure. The final values are 89.2/80.4/84.5 recall/precision/F-measure, which compare favorably with the performance of the Yarowsky algorithm (83.3/84.6/84.0). (Collins and Singer, 1999) add a special final round to boost recall, yielding 91.2/80.0/85.2 for the Yarowsky algorithm and 91.3/80.1/85.3 for their version of the original co-training algorithm. All four algorithms essentially perform equally well; the advantage of the greedy agreement algorithm is that we have an explanation performs well. 9 The Yarowsky Algorithm For Yarowsky’s algorithm, a classifier again consists of a list of atomic rules. The prediction of the classifier is the prediction of the first rule in the list that applies. The algorithm constructs a classifier iteratively, beginning with a seed rule. In the variant we consider here, one atomic rule added at each iteration. An atomic rule only if its precision, +] (as measured using the labels assigned by the classifier exceeds a fixed threshold Yarowsky does not give an explicit justification for the algorithm. I show here that the algorithm can be justified on the basis of two assumptions. In what follows, represents an atomic rule under consideration, the current classifier. Recall the set of instances whose true label is and the set of instances = write for the set of instances labeled by current classifier, that is, first assumption is indepen- 5 rule classifier satisfy independence in case = A bootstrapping problem instance satisfies precision independence just in case all rules G and atomic rules nontrivially overlap with (both and are nonempty) satisfy precision independence. Precision independence is stated here so that it looks like a conditional independence assumption, to emphasize the similarity to the analysis of co-training. In fact, it is only “half” an independence assumption—for precision indepenit is that = The second assumption is that classifiers make balanced errors. That is: = Let us first consider a concrete (but hypothetical) example. Suppose the initial classifier correctly labels 100 out of 1000 instances, and makes no mistakes. Then the initial precision is 1995), citing (Yarowsky, 1994), actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting. 1 and recall is 0.1. Suppose further that we add an atomic rule that correctly labels 19 new instances, and incorrectly labels one new instance. The rule’s precision is 0.95. The precision of the new classifier (the old classifier plus the new atomic rule) is 119/120 = 0.99. Note that the new precision lies between the old precision and the precision of the rule. We will show that this is always the case, given precision independence and balanced errors. We need to consider several quantities: the of the current classifier, the precision of the rule under consideration, the precision of the rule on the curlabeled set, and the precision of the rule as measured using estimated labels, The assumption of balanced errors implies that measured precision equals true precision on labeled instances, as follows. (We assume here that all instances have true labels, hence that = + = + = = This, combined with precision independence, that the precision of measured on the labeled set is equal to its true precision Now consider the precision of the old and new at predicting Of the instances that old classifier labels let the numthat are correctly labeled and the number that are incorrectly labeled. Defining the precision of the old classifier Let the number of new instances that the rule under consideration corlabels, and let the number that it labels. Defining the of the rule is The precision the new classifier is which can be written as: n = That is, the precision of the new classifier is a weighted average of the precision of the old classifier and the precision of the new rule. 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 Recall Figure 6: Performance of the Yarowsky algorithm An immediate consequence is that, if we only accept rules whose precision exceeds a given then the precision of the new classiexceeds Since measured precision equals true precision under our previous assumptions, it follows that the true precision of the final clasexceeds the measured precision of evaccepted rule exceeds Moreover, observe that recall can be written as: the number of instances whose true is If then recall is bounded by which grows as Hence we have proven the following theorem. 5 the assumptions of precision independence and balanced errors are satisfied, then the Yarowsky algorithm with threshold B obtains a final classifier whose precision is at least B. Moreover, recall is bounded below by a quantity which increases at each round. Intuitively, the Yarowsky algorithm increases recall while holding precision above a threshold that represents the desired precision of the final classifier. The empirical behavior of the algorithm, as shown in figure 6, is in accordance with this analysis. We have seen, then, that the Yarowsky algorithm, like the co-training algorithm, can be justified on the basis of an independence assumption, precision independence. It is important to note, however, that the Yarowsky algorithm is not a special case of co-training. Precision independence and view independence are distinct neither implies the 10 Conclusion To sum up, we have refined previous work on the analysis of co-training, and given a new cotraining algorithm that is theoretically justified and has good empirical performance. We have also given a theoretical analysis of the Yarowsky algorithm for the first time, and shown that it can be justified by an independence assumption that is quite distinct from the independence assumption that co-training is based on.</abstract>
<note confidence="0.967419545454545">References A. Blum and T. Mitchell. 1998. Combining labeled unlabeled data with co-training. In Proceedings of the Workshop on Computational Morgan Kaufmann Publishers. Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Sanjoy Dasgupta, Michael Littman, and David McAllester. 2001. PAC generalization bounds for In of David Yarowsky. 1994. Decision lists for lexical am-</note>
<title confidence="0.892381">resolution. In ACL</title>
<author confidence="0.888324">Unsupervised word sense</author>
<abstract confidence="0.8934327">disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the for Computational pages 189–196. see that view independence does not imply preindepence, consider an example in which always. This is compatible with rule independence, but implies that = 1 and = 0, violating precision independence. A</abstract>
<intro confidence="0.796484"></intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT: Proceedings of the Workshop on Computational Learning Theory.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="981" citStr="Blum and Mitchell, 1998" startWordPosition="152" endWordPosition="155">cal justification, gives a theoretical justification for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions. 1 Overview The term bootstrapping here refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics. Current work has been spurred by two papers, (Yarowsky, 1995) and (Blum and Mitchell, 1998). Blum and Mitchell propose a conditional independence assumption to account for the efficacy of their algorithm, called co-training, and they give a proof based on that conditional independence assumption. They also give an intuitive explanation of why co-training works, in terms of maximizing agreement on unlabeled data between classifiers based on different “views” of the data. Finally, they suggest that the Yarowsky algorithm is a special case of the co-training algorithm. The Blum and Mitchell paper has been very influential, but it has some shortcomings. The proof they give does not actu</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="11304" citStr="Collins and Singer, 1999" startWordPosition="2053" endWordPosition="2056">sion of rules based on the same view as G by using the precision of some other-view rule F. Hence we can compute the precision of every rule given the precision of any one. F Co-training Yarowsky Truth M:chairman -12.7 .068 .030 X:Company-of 10.2 .979 .989 X:court-in -.183 1.00 .981 X:Company-in 75.7 1.00 .986 X:firm-in -9.94 .952 .949 X:%-in -15.2 .875 .192 X:meeting-in -2.25 1.00 .753 Table 1: Some data 6 Some Data Y = + Y = − Y = + Y = − G G + − + − F + − (b) Negative correlation G G + − + − (a) Positive correlation The empirical investigations described here and below use the data set of (Collins and Singer, 1999). The task is to classify names in text as person, location, or organization. There is an unlabeled training set containing 89,305 instances, and a labeled test set containing 289 persons, 186 locations, 402 organizations, and 123 “other”, for a total of 1,000 instances. Instances are represented as lists of features. Intrinsic features are the words making up the name, and contextual features are features of the syntactic context in which the name occurs. For example, consider Bruce Kaplan, president of Metals Inc. This text snippet contains two instances. The first has intrinsic features N:B</context>
<context position="19557" citStr="Collins and Singer, 1999" startWordPosition="3647" endWordPosition="3650">r persons nor locations nor organizations), and because classifiers do not label all instances, we show precision and recall rather than a single error rate. The contour lines show levels of the F-measure (the harmonic mean of precision and recall). The algorithm is run to convergence, that is, until no atomic rule can be found that decreases cost. It is interesting to note that there is no significant overtraining with respect to F-measure. The final values are 89.2/80.4/84.5 recall/precision/F-measure, which compare favorably with the performance of the Yarowsky algorithm (83.3/84.6/84.0). (Collins and Singer, 1999) add a special final round to boost recall, yielding 91.2/80.0/85.2 for the Yarowsky algorithm and 91.3/80.1/85.3 for their version of the original co-training algorithm. All four algorithms essentially perform equally well; the advantage of the greedy agreement algorithm is that we have an explanation for why it performs well. 9 The Yarowsky Algorithm For Yarowsky’s algorithm, a classifier again consists of a list of atomic rules. The prediction of the classifier is the prediction of the first rule in the list that applies. The algorithm constructs a classifier iteratively, beginning with a s</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC generalization bounds for co-training.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="2109" citStr="Dasgupta et al., 2001" startWordPosition="334" endWordPosition="337">per has been very influential, but it has some shortcomings. The proof they give does not actually apply directly to the co-training algorithm, nor does it directly justify the intuitive account in terms of classifier agreement on unlabeled data, nor, for that matter, does the co-training algorithm directly seek to find classifiers that agree on unlabeled data. Moreover, the suggestion that the Yarowsky algorithm is a special case of co-training is based on an incidental detail of the particular application that Yarowsky considers, not on the properties of the core algorithm. In recent work, (Dasgupta et al., 2001) prove that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on a different “view” of the data. This addresses one of the shortcomings of the original co-training paper: it gives a proof that justifies searching for classifiers that agree on unlabeled data. I extend this work in two ways. First, (Dasgupta et al., 2001) assume the same conditional independence assumption as proposed by Blum and Mitchell. I show that that independence assumption is remarkably powerful, and violated in the data; however, I show that a weaker assumption suffic</context>
<context position="6480" citStr="Dasgupta et al., 2001" startWordPosition="1130" endWordPosition="1133">le independence. 4 Rule Independence and Bootstrapping Blum and Mitchell’s paper suggests that rules that agree on unlabelled instances are useful in bootstrapping. Definition 3 The agreement rate between rules F and G is Pr[F = G|F, G =�+] Note that the agreement rate between rules makes no reference to labels; it can be determined from unlabeled data. The algorithm that Blum and Mitchell describe does not explicitly search for rules with good agreement; nor does agreement rate play any direct role in the learnability proof given in the Blum and Mitchell paper. The second lack is emended in (Dasgupta et al., 2001). They show that, if view independence is satisfied, then the agreement rate between opposing-view rules F and G upper bounds the error of F (or G). The following statement of the theorem is simplified and assumes non-abstaining binary rules. Theorem 2 For all F E W1, G E W2 that satisfy rule independence and are nontrivial predictors in the sense that minu Pr[F = u] &gt; Pr[F =� G], one of the following inequalities holds: Pr[F =� Y ] G Pr[F =� G] Pr[F =� Y ] G Pr[F =� G] If F agrees with G on all but E unlabelled instances, then either F or F¯ predicts Y with error no greater than E. A small am</context>
<context position="17965" citStr="Dasgupta et al., 2001" startWordPosition="3346" endWordPosition="3349">one is kept, and attention shifts to the other rule. A complex rule (or classifier) is a list of atomic rules H, each associating a single feature h with a label t. H(x) = t if x has feature h, and H(x) = L otherwise. A given atomic rule is permitted to appear multiple times in the list. Each atomic rule occurrence gets one vote, and the classifier’s prediction is the label that receives the most votes. In case of a tie, there is no prediction. The cost of a classifier pair (F, G) is based on a more general version of theorem 2, that admits abstaining rules. The following theorem is based on (Dasgupta et al., 2001). Theorem 4 If view independence is satisfied, and if F and G are rules based on different views, then one of the following holds: Pr[F =� Y JF =� L] &lt; δ µ−δ Pr[F¯ =� Y J F¯ =� L] &lt; δ µ−δ where S = Pr[F =� G|F, G =� L], and µ = minaPr[F = u|F =� L]. In other words, for a given binary rule F, a pessimistic estimate of the number of errors made by F is S/(µ − S) times the number of instances labeled by F, plus the number of instances left unlabeled by F. Finally, we note that the cost of F is sensitive to the choice of G, but the cost of F with respect to G is not necessarily the same as the cos</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2001</marker>
<rawString>Sanjoy Dasgupta, Michael Littman, and David McAllester. 2001. PAC generalization bounds for co-training. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution.</title>
<date>1994</date>
<booktitle>In Proceedings ACL 32.</booktitle>
<contexts>
<context position="21894" citStr="Yarowsky, 1994" startWordPosition="4038" endWordPosition="4039">n independence is stated here so that it looks like a conditional independence assumption, to emphasize the similarity to the analysis of co-training. In fact, it is only “half” an independence assumption—for precision independence, it is not necessary that P(Y`J ¯F`, G∗) = P(Y`J The second assumption is that classifiers make balanced errors. That is: P(Y`, G¯`JF`) = P(Y¯`, G`JF`) Let us first consider a concrete (but hypothetical) example. Suppose the initial classifier correctly labels 100 out of 1000 instances, and makes no mistakes. Then the initial precision is 1(Yarowsky, 1995), citing (Yarowsky, 1994), actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting. 1 and recall is 0.1. Suppose further that we add an atomic rule that correctly labels 19 new instances, and incorrectly labels one new instance. The rule’s precision is 0.95. The precision of the new classifier (the old classifier plus the new atomic rule) is 119/120 = 0.99. Note that the new precision lies between the old precision and the precision of the rule. We will show that this is always the case, given precision independ</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution. In Proceedings ACL 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="951" citStr="Yarowsky, 1995" startWordPosition="149" endWordPosition="150">thm that has theoretical justification, gives a theoretical justification for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions. 1 Overview The term bootstrapping here refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics. Current work has been spurred by two papers, (Yarowsky, 1995) and (Blum and Mitchell, 1998). Blum and Mitchell propose a conditional independence assumption to account for the efficacy of their algorithm, called co-training, and they give a proof based on that conditional independence assumption. They also give an intuitive explanation of why co-training works, in terms of maximizing agreement on unlabeled data between classifiers based on different “views” of the data. Finally, they suggest that the Yarowsky algorithm is a special case of the co-training algorithm. The Blum and Mitchell paper has been very influential, but it has some shortcomings. The</context>
<context position="21869" citStr="Yarowsky, 1995" startWordPosition="4035" endWordPosition="4036">on independence. Precision independence is stated here so that it looks like a conditional independence assumption, to emphasize the similarity to the analysis of co-training. In fact, it is only “half” an independence assumption—for precision independence, it is not necessary that P(Y`J ¯F`, G∗) = P(Y`J The second assumption is that classifiers make balanced errors. That is: P(Y`, G¯`JF`) = P(Y¯`, G`JF`) Let us first consider a concrete (but hypothetical) example. Suppose the initial classifier correctly labels 100 out of 1000 instances, and makes no mistakes. Then the initial precision is 1(Yarowsky, 1995), citing (Yarowsky, 1994), actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting. 1 and recall is 0.1. Suppose further that we add an atomic rule that correctly labels 19 new instances, and incorrectly labels one new instance. The rule’s precision is 0.95. The precision of the new classifier (the old classifier plus the new atomic rule) is 119/120 = 0.99. Note that the new precision lies between the old precision and the precision of the rule. We will show that this is always the case,</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
<citation valid="false">
<title>2To see that view independence does not imply precision indepence, consider an example in which G = Y always. This is compatible with rule independence, but it implies that P(YIFG) = 1 and P(Y |F¯G) = 0, violating precision independence.</title>
<marker></marker>
<rawString>2To see that view independence does not imply precision indepence, consider an example in which G = Y always. This is compatible with rule independence, but it implies that P(YIFG) = 1 and P(Y |F¯G) = 0, violating precision independence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>