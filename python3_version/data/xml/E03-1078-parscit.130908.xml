<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.9955275">
Transparent combination of rule-based and data-driven approaches in a
speech understanding architecture
</title>
<author confidence="0.897052">
Manny Rayner and Beth Ann Hockey
</author>
<affiliation confidence="0.909499">
RIACS, Mail Stop T27A-2
NASA Ames Research Center
</affiliation>
<address confidence="0.981871">
Moffett Field, CA 94035-1000, USA
</address>
<email confidence="0.998953">
fmrayner, bahockeyl@riacs.edu
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999828666666667">
We describe a domain-independent se-
mantic interpretation architecture suit-
able for spoken dialogue systems, which
uses a decision-list method to effect a
transparent combination of rule-based
and data-driven approaches. The ar-
chitecture has been implemented and
evaluated in the context of a medium-
vocabulary command and control task.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981636363636">
As the field of spoken language understanding be-
comes more mature, a clearer picture begins to
emerge of the tradeoffs between rule-based and
data-driven methods. Other things being equal,
there are many reasons to prefer data-driven ap-
proaches. They are more robust, and reduce the
heavy authoring costs associated with rule-based
systems; methods are moreover starting to emerge
which enable data-driven approaches to be used
in areas which previously were thought to require
rules, such as dialogue management. A good
overview of current work in this area is provided
in (Young, 2002).
Excellent as data-driven systems are, they have
one obvious drawback: they require corpus data,
usually in fairly substantial amounts. Academics
basically interested in pure research are free to
work within a domain for which the data has al-
ready been collected, and for example can decide
to use the Penn Treebank (Penn Treebank, 2002)
or the ATIS corpus (Dahl et al., March 1994). If,
on the other hand, the goal is to create a use-
ful application for a specified new domain, there
will in general be little or no available data at
the start of the project. It is possible to create
the data by using Wizard of Oz methods, or sim-
ilar. Wizard of Oz data collection is unattrac-
tive for many reasons: it is expensive and time-
consuming, and once the data has been collected
it is not easy to change the coverage of the system.
For these reasons, commercial speech recognition
platform vendors like Nuance and SpeechWorks
have focussed on rule-based approaches, which al-
low rapid prototyping of systems from only very
modest quantities of corpus data. Although most
commercial rule-based spoken language dialogue
systems use directed dialogue strategies and mod-
erately simple recognition grammars, the literature
now contains descriptions of several research sys-
tems built using rule-based methods, which suc-
cessfully use mixed-initiative strategies and com-
plex grammars (Stent et al., 1999; Rayner et al.,
2000; Lemon et al., 2001; Rayner et al., 2001b).
If a project of this kind is developed over a
substantial period of time, corpus material accu-
mulates automatically as input to the system is
logged. The more corpus material there is, the
stronger the reasons for moving towards data-
driven processing; this will however only be easy
if the architecture is originally set up to use statis-
tics as well as rules. Summarising the argument
so far, we would like an architecture which com-
bines rule-based and data-driven methods as trans-
parently as possible. This will allow us to shift
smoothly from an initial version of the system
</bodyText>
<page confidence="0.996489">
299
</page>
<bodyText confidence="0.999505076923077">
which is entirely rule-based, to a final version
which is largely data-driven.
In this paper, we will present a semantic inter-
pretation architecture which conforms to the gen-
eral model presented above. At the top level, se-
mantic interpretation is viewed as a statistical clas-
sification task. An interpretation consists of a set
of one or more semantic atoms. Each utterance is
associated with a set of features; some of these fea-
tures are defined by hand-coded rules, and some
by surface utterance characteristics like word N-
grams. The available data is used to train statistics
which evaluate each feature&apos;s reliability as a pre-
dictor of each semantic atom. When only small
amounts of data are used, most of the process-
ing relies on rule-based features; as the size of
the training corpus increases, the centre of gravity
shifts more and more strongly towards the surface
features.
The rest of the paper is structured as follows.
Section 2 describes the abstract architecture, and
Section 3 a concrete realisation built on top of the
Nuance Toolkit. Section 4 gives details of exper-
iments carried out on a medium-vocabulary com-
mand and control task from an instruction manual
domain. Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.33913" genericHeader="method">
2 Semantic analysis as classification
</sectionHeader>
<bodyText confidence="0.962198304347826">
This section describes an abstract architecture
which characterises semantic analysis as a task
slightly extending the &amp;quot;decision-list&amp;quot; classification
algorithm (Yarowsky, 1994; Carter, 2000). We
start with a set of semantic atoms, each represent-
ing a primitive domain concept, and define a se-
mantic representation to be a non-empty set of se-
mantic atoms. For example, in our sample domain
we represent the utterances
please speak up
show me the sample syringe
set an alarm for five minutes from now
no i said go to the next step
respectively as
line rease_volume}
show, sample_syringe}
{set_alarm, 5, minutes}
{correction, next_step}
where increase_volume, show, sam-
ple_syringe, set_alarm, 5, minutes,
correction and next_step are semantic
atoms. As well as specifying the permitted
semantic atoms themselves, we also define a
target model which for each atom specifies the
other atoms with which it may legitimately
combine. Thus here, for example, correction
may legitimately combine with any atom, but
minutes may only combine with correc-
tion, s et _alarm or a number. Although
a representation scheme of this type cannot
represent everything one might ideally want, it is
certainly rich enough to support many non-trivial
applications.
Training data consists of a set of utterances, in
either text or speech form, each tagged with its in-
tended semantic representation. We define a set of
feature extraction rules, each of which associates
an utterance with zero or more features. Feature
extraction rules can carry out any type of process-
ing. In particular, they may involve performing
speech recognition on speech data, parsing on text
data, application of hand-coded rules to the results
of parsing, or some combination of these. Statis-
tics are then compiled to estimate the probability
p(a f) of each semantic atom a given each sepa-
rate feature f, using the standard formula
</bodyText>
<equation confidence="0.547459">
P(a f) = (NY +1)1(N f +2)
</equation>
<bodyText confidence="0.999953444444445">
where Nf is the number of occurrences in the
training data of utterances with feature f, and N7
is the number of occurrences of utterances with
both feature f and semantic atom a.
The decoding process follows (Yarowsky, 1994)
in assuming complete dependence between the
features. Note that this is in sharp contrast with the
Naive Bayes classifier (Duda et al., 2000), which
assumes complete independence. Of course, nei-
ther assumption can be true in practice; however,
as argued in (Carter, 2000), there are good reasons
for preferring the dependence alternative as the
better option in a situation where there are many
features extracted in ways that are likely to over-
lap.
We are given an utterance u, to which we wish
to assign a representation R(u) consisting of a set
of semantic atoms, together with a target model
</bodyText>
<page confidence="0.997012">
300
</page>
<subsectionHeader confidence="0.90123">
3.1 Training and decoding
</subsectionHeader>
<bodyText confidence="0.999784666666667">
Training data for ALTERF is supplied in the form
of a text file containing one example per line, in
the format
comprising a set of rules defining which sets of
semantic atoms are consistent. The decoding pro-
cess proceeds as follows:
</bodyText>
<listItem confidence="0.999114210526316">
1. Initialise R(u) to the empty set.
2. Use the feature extraction rules and the statis-
tics compiled during training to find the set of
all triples ( f, a, p) where f is a feature associ-
ated with u, a is a semantic atom, and p is the
probability p(a f) estimated by the training
process.
3. Order the set of triples by the value of p, with
the largest probabilities first. Call the ordered
set T.
4. Remove the highest-ranked triple (f, a, p)
from T. Add a to R(u) if the following con-
ditions are fulfilled:
• p &gt; pt for some pre-specified threshold
value pt.
• Addition of a to R(u) results in a
set which is consistent with the target
model.
5. Repeat step (4) until T is empty.
</listItem>
<bodyText confidence="0.999236142857143">
Intuitively, the process is very simple. We just
walk down the list of possible semantic atoms,
starting with the most probable ones, and add them
to the semantic representation we are building up
when this does not conflict with the consistency
rules in the target model. We stop when the atoms
suggested have become sufficiently improbable.
</bodyText>
<sectionHeader confidence="0.988117" genericHeader="method">
3 The ALTERF system
</sectionHeader>
<bodyText confidence="0.999979909090909">
This section describes ALTERF, an open-source
tool which implements the abstract procedure de-
scribed in Section 2.1 ALTERF is implemented
in SICStus Prolog (Programming Systems Group,
1995), and makes use of the Nuance Toolkit plat-
form (Nuance, 2002) to perform speech recogni-
tion and parsing functions. It is compatible with
the open-source REGULUS compiler (Rayner et al.,
2001a), which can be used to build Nuance recog-
nisers from unification-grammar representations,
but does not depend on it.
</bodyText>
<equation confidence="0.731667333333333">
lAlterf can be downloaded from SourceForge at
http://sourceforge.net/projects/leonlp/
(Wavfile) (Atoms) (Transcription)
</equation>
<bodyText confidence="0.984519641025641">
where (Wavfile) is the name of a file con-
taining speech data, (Atoms) is the intended
semantic representation, and (Transcription) is
a text transcription of the speech data. Thus a
typical line might be
utt12.wav next_line go on
Training can be carried out in either speech or
text mode. In both modes, the Nuance toolkit
converts data in the relevant modality into a
parsed representation, using the bat chrec util-
ity for speech data, and the nl—tool utility for
text. Trace output from these two tools is post-
processed into internal form. For speech data, this
first processing stage produces three pieces of in-
formation for each line in the training file: a text
string, a parsed representation and a confidence
score. For text data, it yields either a parsed repre-
sentation or an annotation marking that the utter-
ance in question was outside grammar coverage.
Features can consequently be extracted from
either a text string or a parse output. The text
string is used straightforwardly to produce uni-
gram, bigram and trigram features. For example,
the utterance &amp;quot;speak up&amp;quot; will produce the N-gram
features unigram (speak) , unigram (up) ,
bigram (*start*, speak) ,
bigram (speak, up) , bigram (up,
*end*) , trigram (*start*, speak,
up) and trigram (speak, up, *end*) .
The process of deriving features from the parse
output is more interesting. In general, we assume
that the parse output will be quite distinct from
the semantic representation we are ultimately
interested in producing, and is more likely to be a
linguistically motivated form like a parse tree or a
predicate-argument structure. Thus, for example,
with the grammar used for the experiments in
Section 4 the parse output for
go to the next step
</bodyText>
<page confidence="0.944299">
301
</page>
<bodyText confidence="0.615755">
is
[ [ form (
imperative,
[ [go, term (pron, you, [ ) ] ,
[to, term (the_next, step, [1)]
</bodyText>
<page confidence="0.523566">
11
</page>
<bodyText confidence="0.984157088235294">
but the semantic representation is simply
{ next_step
We implement hand-coded rules defining patterns
which match sub-structures of these parse repre-
sentations: each such rule has the form
pattern((Pattern), (Atom), (Example)).
where (Pattern) is a pattern, (Atom) is an
atom it predicts, and (Example) is an example
of an utterance that should contains this pattern.
Continuing the example, a possible pattern might
be
pattern (term (the next, step, ) ,
next step,
&apos;go to the next step&apos;) .
which would reflect the rule-writer&apos;s (correct or
incorrect) intuition that &amp;quot;the next step&amp;quot; is a reli-
able phrase for predicting the atom next _step.
If the hand-coded rule pattern((P), (A), (E))
matches some part of the parse representation
for an utterance, this gives rise to a feature
pattern_match ( A). So for example the
pattern rule immediately above would for the
utterance
what&apos;s the next step
produce a feature
pattern_match (next_step) .
Each feature-atom pair is assigned a score dur-
ing the training process, using the standard esti-
mation formula of Section 2. In order to reduce
the size of the data files generated, it is possi-
ble to set parameters to discard entries which are
deemed sufficiently unlikely to be useful. There
are currently two possible reasons for discarding a
feature-atom-pair triple ( f , a, p):
</bodyText>
<listItem confidence="0.962288636363636">
• Low probability. The lower the estimated
probability p of a given f, the less likely the
entry is to be useful. All entries for which p
is lower than a specified threshold value are
consequently discarded; this threshold value
corresponds to the constant pt in Section 2.
The default value of pt is 0.65.
• Sparse data. If we want to increase the pre-
dictability of the model, it can be desirable
to prune statistics based on too small data
samples. A second threshold value specifies
</listItem>
<bodyText confidence="0.980740483870968">
the minimum number of positive examples,
i.e. training examples with both f and a, that
must be present for the entry to be kept. The
default value is 2.
For the experiments described in Section 4, use
of the default pruning values reduces the size of
the generated table of statistics by a factor of about
50.
The decoding process closely follows the de-
scription in Section 2. The target model is cur-
rently implemented as a two-place Prolog predi-
cate, supplied by the user, which for any semantic
atom A returns the (possibly empty) set of seman-
tic atoms which can co-occur with A. For the do-
main described in Section 4, the definition of the
target model constitutes about a page of code.
ALTERF permits the definition of backing-off
rules, which are used to address the sparse-data
problems that arise when learning associations be-
tween features representing number and time con-
structs and the corresponding semantic atoms. For
instance, the training example
uttl goto 3 I go to step three
would without backing-off rules induce an associ-
ation between the feature pattern_match (3)
and the semantic atom 3. Since the num-
bers in the feature and the atoms are the
same, the backing-off rules transform this
into a generic association between the feature
pattern_match (*number*) and the seman-
tic atom *numb e r *. Backing-off rules are used
</bodyText>
<page confidence="0.994208">
302
</page>
<bodyText confidence="0.975095076923077">
uniformly by the trainer and the decoder. At de-
coding time, any substitution of a generic token for
a specific one is stored, and the inverse substitution
is then applied on the output semantic atoms.
3.2 Writing and maintaining hand-coded
rules
ALTERF includes a pair of simple tools intended
to reduce the authoring overhead associated with
use of hand-coded rule-sets. The rule-set creation
tool starts with the data in the annotated training
corpus, and collects all unique pairs ((Atom),
(Transcription)) such that there is some training
example
</bodyText>
<equation confidence="0.763458">
(Wavfile) (Atoms) (Transcription)
</equation>
<bodyText confidence="0.99965575">
where (Atom) is a member of (Atoms). The
tool then parses the transcriptions that are within
grammar coverage, and for each of these creates
an initial pattern declaration of the form
</bodyText>
<equation confidence="0.5636">
pattern( (Parse), (Atom), (Transcription)).
</equation>
<bodyText confidence="0.999968111111111">
where (Parse) is the parse representation associ-
ated with (Transcription). The initial declarations
are sorted by the (Atom) field, and written out
to a file which is then edited by a human system
expert. The human rule-writer consequently only
needs to edit the parse-representation field in order
to keep what she considers to be the meaningful
part of the pattern. We have been able to use this
methodology to create good-quality rule-sets at a
rate of about 50 to 100 rules per hour.
In order to check for clerical errors in the rule-
creation process and version slippage between the
grammar and the rule-set, a rule validation tool is
run periodically to ascertain for each rule that the
&amp;quot;pattern&amp;quot; field still matches at least one subterm in
the representation of the &amp;quot;example&amp;quot; field. Offend-
ing examples are highlighted for human attention,
and can be rapidly corrected.
</bodyText>
<sectionHeader confidence="0.80967" genericHeader="method">
4 Target system and experiments
</sectionHeader>
<bodyText confidence="0.999971230769231">
This section describes concrete experiments car-
ried out with ALTERF on CHECKLIST, a sys-
tem related to the intelligent procedure assistant
described in (Aist et al., 2002). CHECKLIST,
which is currently being evaluated for possible
use in an astronautics domain, provides spoken
dialogue support for carrying out complex proce-
dures. The most important commands cover navi-
gation (&amp;quot;go to the next line&amp;quot;, &amp;quot;go to step fourteen&amp;quot;,
&amp;quot;go back two steps&amp;quot;, &amp;quot;where am I&amp;quot;), answering
system questions (&amp;quot;affirmative&amp;quot;, &amp;quot;no&amp;quot;), displaying
pictures of objects used in the procedure (&amp;quot;show
me the waste water bag&amp;quot;, &amp;quot;where is the syringe&amp;quot;),
recording and playing voice notes (&amp;quot;put a voice
note on that step&amp;quot;, &amp;quot;play the voice note for step
ten&amp;quot;) and setting of voice alarms (&amp;quot;set alarm for
ten minutes from now&amp;quot;, &amp;quot;cancel alarm&amp;quot;). There
are also a number of other less important func-
tionalities. The semantic representation language
currently contains a total of 46 different seman-
tic atoms, including the generic atoms (cf. Sec-
tion 3.1) *numb e r * and *time* Some exam-
ples of utterances and their associated semantic
representations are shown at the beginning of Sec-
tion 2.
The speech understanding component is imple-
mented on top of the Nuance platform (Nuance,
2002); the recognition package is compiled from
a unification grammar description using the REG-
ULUS tool (Rayner et al., 2001a). An example
of a parse representation produced by this gram-
mar appears in Section 3.1. The grammar contains
129 rules and 258 lexical items, and the compiled
recogniser achieves a word error rate of approxi-
mately 19% on unseen in-domain test data using
our normal software and hardware configuration.
Use of a grammar-based language model implies
that all utterances recognised by the system are
within the coverage of the grammar.
At the beginning of the current phase of the
project, we recorded 1302 utterances (5540 words)
of speech data, using an ad hoc data collection
methodology loosely based on two interviews with
potential users and a short videotape of a session
with a mock-up of the system; tight time con-
strains and lack of access to users made it diffi-
cult to do better than this. We transcribed and an-
notated the data using a simple Java-based tool,
randomly selecting 75% of it for use in training
and keeping the rest for testing. During the course
of the project, we routinely logged speech inter-
actions with the system, and transcribed and an-
</bodyText>
<page confidence="0.998636">
303
</page>
<bodyText confidence="0.999978">
notated a further 424 utterances (906 words) of
speech data. 75% of this was again assigned to
training and the rest saved for testing. The recog-
niser grammar was developed using only the train-
ing portion of the corpus.
</bodyText>
<subsectionHeader confidence="0.841989">
4.1 Experiments
</subsectionHeader>
<bodyText confidence="0.999988625">
With regard to the experiments themselves, we
were primarily interested in the quality of seman-
tic classification in the ALTERF semantic interpre-
tation module, defined as the proportion of the in-
domain utterances in the test set which were as-
signed a correct set of semantic atoms. We inves-
tigated how the semantic classification error rate
was affected by the following factors:
</bodyText>
<listItem confidence="0.9994824">
• Use of rule based features only, N-gram
based features only, or both rule and N-gram
based features.
• Quantity of training data.
• Modality (text or speech) of training data.
</listItem>
<bodyText confidence="0.99806268">
We randomly divided the training corpus into
ten equal pieces, and trained on subsets ranging
from 10% of the corpus to the full corpus in both
text and speech modes. We then evaluated seman-
tic classification performance on the in-domain
portion of the test data using either rules, N-gram
based statistics or a combination of the two. When
running in speech mode, we set the Nuance re-
jection threshold to zero and the beam width to
1200, which on the 1.9 MHz processor we were
using gave recognition processing speeds typically
around 0.5 times real time.
Table 1 presents the results of the first set of
experiments, using training data in speech form.
We see, not surprisingly, that for small amount
of training data the rule-based version of the sys-
tem is greatly superior to the N-gram based one.
For larger amounts of training data, however, the
N-gram version and in particular the combined
version start to overtake the pure rule-based sys-
tem. When all the training data is used, the com-
bined system outperforms the rule-based system
by 22.2% to 27.3% (19% relative), and outper-
forms the N-gram system by 22.2% to 25.6%
(12% relative). Item-by-item comparisons show
</bodyText>
<table confidence="0.999720090909091">
Data Rules NGrams Both
10% 28.7% 70.9% 33.1%
20% 28.7% 44.7% 30.4%
30% 29.0% 40.2% 28.4%
40% 27.3% 37.8% 27.6%
50% 27.6% 37.6% 29.4%
60% 27.3% 34.1% 26.9%
70% 27.3% 32.4% 27.6%
80% 27.3% 30.0% 26.6%
90% 27.3% 27.7% 24.6%
100% 27.3% 25.6% 22.2%
</table>
<tableCaption confidence="0.999797">
Table 1: Percentage semantic interpretation errors
</tableCaption>
<bodyText confidence="0.941404214285714">
on in-domain test data for different amounts of
training data and different versions of the system.
Training and test data both in speech form. &amp;quot;Data&amp;quot;
= proportion of training data used; &amp;quot;Rules&amp;quot; = sys-
tem uses rules only; &amp;quot;NGrams&amp;quot; = N-grams only;
&amp;quot;Both&amp;quot; = both rules and N-grams.
that there are 29 utterances in the test set where
the results for the combined and rule-based ver-
sions differ, split 22-7 in favour of the combined
version. This is significant at p &lt; 0.01 according
to the McNemar sign test. Although the differ-
ence between the N-gram and combined versions
is smaller, it is more one-sided (10-0), and is also
significant at p &lt; 0.01.
We could see two possible causal mechanisms
to account for the improvement in the combined
system compared to the pure rule-based one. The
obvious explanation is that the N-gram based dis-
criminants are filling in holes in the rule-set; more
subtly, they could be learning characteristic mis-
takes made by the recogniser and correcting them.
In order to separate these two effects, Table 2
presents the results of the same experiments run
with the training data in text mode. Since per-
formance of the N-gram and combined versions
only degrades a little, we conclude that the second
factor (learning to correct recogniser errors) is the
less important one.
</bodyText>
<sectionHeader confidence="0.950376" genericHeader="conclusions">
5 Summary and conclusions
</sectionHeader>
<bodyText confidence="0.999966">
We have presented a simple speech understand-
ing framework combining rule-based and data-
driven methods, which has been implemented and
</bodyText>
<page confidence="0.99713">
304
</page>
<table confidence="0.999907363636364">
Data Rules NGrams Both
10% 28.7% 70.9% 33.1%
20% 28.7% 44.7% 30.4%
30% 29.0% 40.2% 28.4%
40% 27.3% 37.8% 27.6%
50% 27.6% 37.6% 29.4%
60% 27.3% 34.1% 26.9%
70% 27.3% 32.4% 27.6%
80% 27.3% 30.0% 26.6%
90% 27.3% 27.7% 24.6%
100% 27.3% 27.0% 23.6%
</table>
<tableCaption confidence="0.963677">
Table 2: As Table 1, but with training data in text
form.
</tableCaption>
<bodyText confidence="0.999525412500001">
first effect is the more important one.
evaluated in the context of a non-trivial medium-
vocabulary command and control system. In
contrast to other work described in the literature
((Wang et al., 2002) is a recent example) rules are
treated on a basis of strict parity with other data
sources, so that the balance between them can be
entirely determined by the training data. For small
amounts of data, the rules dominate; by the time
we have on the order of 1000 training utterances,
data-driven methods are producing significant im-
provements in performance. The tables in Sec-
tion 4 suggest that performance has not yet topped
out, and will continue to improve as more training
data becomes available.
It is worth pointing out that the results are in
some ways quite surprising, since the basic situa-
tion is very favourable for rules. The recognition
grammar is rule-based, so the recogniser only pro-
duces utterances which are within grammar cov-
erage. There is not a great deal of training data
available, and the statistical methods used are sim-
ple and unsophisticated. However, we still get a
significant improvement on rules alone by adding
a trainable component.
An obvious rejoinder is that this only shows that
the rules have not been constructed with sufficient
care. It is unfortunately difficult to make any truly
general statements about combinations of rule-
based and trainable architectures, since rule-based
systems, more or less by definition, can perform
any conceivable type of processing. This is part
of the reason we have used a well-defined corpus-
based methodology for constructing our rule-set,
where it is possible to demonstrate that the rules
are at any rate consistent with the corpus examples
they were derived from.
Hand-examination of examples where the com-
bined system outperforms the rule-based one also
suggests that the problem does not lie in short-
comings of the rule-set; it is rather the case that
the trainable N-gram based component learns use-
ful heuristics for covering the portion of the train-
ing data that is outside the area of appliability of
the rules. Even though everything produced by
the recogniser is inside grammar coverage, it is
still perfectly possible to say things to the sys-
tem that are semantically in-domain but not cov-
ered by the grammar. Some of these utterances
will be completely garbled by being forced into
grammar coverage, but many will retain enough
structure to be useful. For instance, in one utter-
ance from the experiments described in Section 4,
the speaker actually said &amp;quot;say again&amp;quot;, which was
outside grammar coverage. This was recognised
as &amp;quot;say that&amp;quot;, which reasonably enough failed to
match any hand-coded rules. There was however
a strong enough association between the bigram
&amp;quot;say that&amp;quot; and the semantic atom repeat (de-
riving from two examples of the phrase &amp;quot;say that
again&amp;quot;) for the combined version to assign the cor-
rect interpretation.
It would be incorrect to conclude that N-grams
always outperform rules; as we saw in Sec-
tion 4 the combined version significantly out-
performs the version with N-grams only. Even
when there is enough data that the N-grams are
useful, there are still gaps in the N-gram cov-
erage where the rules do better. A typical ex-
ample in the converse direction was an utter-
ance where the speaker said &amp;quot;turn down the vol-
ume&amp;quot;. This was recognised correctly, but the N-
gram system had a fairly strong association be-
tween the feature bigram (volume, *end*)
and the semantic atom increase_volume; for
pragmatic reasons, requests in the training corpus
to change the volume were much more frequently
increases than decreases. The N-gram version
consequently assigned an incorrect interpretation.
In the combined version, a hand-coded pattern for
</bodyText>
<page confidence="0.997271">
305
</page>
<bodyText confidence="0.9999905">
decrease_volume scored a match. Taken as a
whole, the set of rules for decrease_volume
turned out to be quite reliable, giving a stronger
association with decrease_volume and a cor-
rect interpretation. In effect, the hand-coded rules
act as a kind of backing-off mechanism, alleviat-
ing the problem of data sparseness.
Looking ahead, we are currently investigating
several interesting extensions of the basic frame-
work. The present version of the system does not
use the acoustic confidence score returned by the
recogniser, but this clearly contains useful infor-
mation. A simple way to attempt to exploit this is
to differentiate between features associated with
high confidence scores, and features associated
with low ones. The expectation is that features
based on high confidence scores will turn out to be
better predictors of semantic atoms than features
based on low confidence scores. Similarly, it is
possible to use not just the top recognition hypoth-
esis, but several items from an N-best hypothe-
sis list, once again differentiating between features
taken from the top hypothesis and features taken
from lower hypotheses. A third idea we are con-
sidering is to include dialogue state information in
the annotated training data; this would for exam-
ple make it possible to learn that prediction of the
semantic atoms yes and no should be more con-
fident in a state where the system has just asked a
yes/no question, and less confident in other states.
</bodyText>
<sectionHeader confidence="0.997193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923716666666">
G. Aist, J. Dowding, B.A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astro-
naut training and support. In Proceedings of ACL
Demo, Philadelphia, PA.
D. Carter. 2000. Choosing between interpretations. In
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and
M. Wiren, editors, The Spoken Language Translator.
Cambridge University Press.
D. Dahl, M. Bates, M. Brown, K. Hunicke-Smith,
D. Pallet, C. Pao, A. Rudnicky, and E. Shriberg.
March 1994. Expanding the scope of the ATIS task:
The ATIS-3 corpus. In Proceedings of the ARPA
Human Language Technology Workshop, Princeton,
NJ.
R.O. Duda, RE. Hart, and H.G. Stork. 2000. Pattern
Classification. Wiley, New York.
0. Lemon, A. Bracy, A. Gruenstein, and S. Peters.
2001. Multimodal dialogues with intelligent agents
in dynamic environments: The WITAS conversa-
tional interface. In North American Assocation for
Computational Linguistics (NAACL), 2001.
Nuance, 2002. http://www.nuance.com. As of 1 Feb
2002.
Penn Treebank, 2002. The Penn Treebank Project.
http://www.cis.upenn.edu/ treebank/. As of July 3,
2002.
Programming Systems Group, 1995. SICStus Prolog
User&apos;s Manual. Swedish Institute of Computer Sci-
ence.
M. Rayner, B.A. Hockey, and F. James. 2000. A com-
pact architecture for dialogue management based on
scripts and meta-outputs. In Proceedings of ANLP
2000.
M. Rayner, J. Dowding, and B.A. Hockey. 2001a.
A baseline method for compiling typed unification
grammars into context free language models. In
Proceedings of Eurospeech 2001, pages 729-732,
Aalborg, Denmark.
M. Rayner, I. Lewin, G. Gorrell, and J. Boye. 200 lb.
Plug and play spoken language understanding. In
Proceedings of SIGDIAL 2001, Aalborg, Denmark.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and
R. Moore. 1999. The CommandTalk spoken dia-
logue system. In Proceedings of the Thirty-Seventh
Annual Meeting of the Association for Computa-
tional Linguistics, pages 183-190.
Y.-Y. Wang, A. Acero, C. Chelba, B. Frey, and
L. Wong. 2002. Combination of statistical and
rule-based approaches for spoken language under-
stadning. In Proceedings of the Seventh Interna-
tional Conference on Spoken Language Processing
(ICSLP), pages 609-612.
D. Yarowsky. 1994. Decision lists for lexical ambigu-
ity resolution. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 88-95, Las Cruces, New Mexico.
S. Young. 2002. Talking to machines (statistically
speaking). In Proceedings of the Seventh Interna-
tional Conference on Spoken Language Processing
(ICSLP), pages 9-16.
</reference>
<page confidence="0.999217">
306
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.375482">
<title confidence="0.9814415">Transparent combination of rule-based and data-driven approaches in a speech understanding architecture</title>
<author confidence="0.999881">Manny Rayner</author>
<author confidence="0.999881">Beth Ann Hockey</author>
<address confidence="0.413551">RIACS, Mail Stop T27A-2</address>
<affiliation confidence="0.996347">NASA Ames Research Center</affiliation>
<address confidence="0.998927">Moffett Field, CA 94035-1000, USA</address>
<abstract confidence="0.9935015">We describe a domain-independent searchitecture suitable for spoken dialogue systems, which uses a decision-list method to effect a transparent combination of rule-based and data-driven approaches. The architecture has been implemented and evaluated in the context of a mediumvocabulary command and control task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>J Hieronymus</author>
</authors>
<title>An intelligent procedure assistant for astronaut training and support.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Demo,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15991" citStr="Aist et al., 2002" startWordPosition="2632" endWordPosition="2635">ut 50 to 100 rules per hour. In order to check for clerical errors in the rulecreation process and version slippage between the grammar and the rule-set, a rule validation tool is run periodically to ascertain for each rule that the &amp;quot;pattern&amp;quot; field still matches at least one subterm in the representation of the &amp;quot;example&amp;quot; field. Offending examples are highlighted for human attention, and can be rapidly corrected. 4 Target system and experiments This section describes concrete experiments carried out with ALTERF on CHECKLIST, a system related to the intelligent procedure assistant described in (Aist et al., 2002). CHECKLIST, which is currently being evaluated for possible use in an astronautics domain, provides spoken dialogue support for carrying out complex procedures. The most important commands cover navigation (&amp;quot;go to the next line&amp;quot;, &amp;quot;go to step fourteen&amp;quot;, &amp;quot;go back two steps&amp;quot;, &amp;quot;where am I&amp;quot;), answering system questions (&amp;quot;affirmative&amp;quot;, &amp;quot;no&amp;quot;), displaying pictures of objects used in the procedure (&amp;quot;show me the waste water bag&amp;quot;, &amp;quot;where is the syringe&amp;quot;), recording and playing voice notes (&amp;quot;put a voice note on that step&amp;quot;, &amp;quot;play the voice note for step ten&amp;quot;) and setting of voice alarms (&amp;quot;set alarm for te</context>
</contexts>
<marker>Aist, Dowding, Hockey, Hieronymus, 2002</marker>
<rawString>G. Aist, J. Dowding, B.A. Hockey, and J. Hieronymus. 2002. An intelligent procedure assistant for astronaut training and support. In Proceedings of ACL Demo, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Carter</author>
</authors>
<title>Choosing between interpretations.</title>
<date>2000</date>
<editor>In M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. Wiren, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4667" citStr="Carter, 2000" startWordPosition="741" endWordPosition="742">y shifts more and more strongly towards the surface features. The rest of the paper is structured as follows. Section 2 describes the abstract architecture, and Section 3 a concrete realisation built on top of the Nuance Toolkit. Section 4 gives details of experiments carried out on a medium-vocabulary command and control task from an instruction manual domain. Section 5 concludes. 2 Semantic analysis as classification This section describes an abstract architecture which characterises semantic analysis as a task slightly extending the &amp;quot;decision-list&amp;quot; classification algorithm (Yarowsky, 1994; Carter, 2000). We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms. For example, in our sample domain we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as line rease_volume} show, sample_syringe} {set_alarm, 5, minutes} {correction, next_step} where increase_volume, show, sample_syringe, set_alarm, 5, minutes, correction and next_step are semantic atoms. As well as specifying the permitted semanti</context>
<context position="6882" citStr="Carter, 2000" startWordPosition="1104" endWordPosition="1105"> p(a f) of each semantic atom a given each separate feature f, using the standard formula P(a f) = (NY +1)1(N f +2) where Nf is the number of occurrences in the training data of utterances with feature f, and N7 is the number of occurrences of utterances with both feature f and semantic atom a. The decoding process follows (Yarowsky, 1994) in assuming complete dependence between the features. Note that this is in sharp contrast with the Naive Bayes classifier (Duda et al., 2000), which assumes complete independence. Of course, neither assumption can be true in practice; however, as argued in (Carter, 2000), there are good reasons for preferring the dependence alternative as the better option in a situation where there are many features extracted in ways that are likely to overlap. We are given an utterance u, to which we wish to assign a representation R(u) consisting of a set of semantic atoms, together with a target model 300 3.1 Training and decoding Training data for ALTERF is supplied in the form of a text file containing one example per line, in the format comprising a set of rules defining which sets of semantic atoms are consistent. The decoding process proceeds as follows: 1. Initialis</context>
</contexts>
<marker>Carter, 2000</marker>
<rawString>D. Carter. 2000. Choosing between interpretations. In M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. Wiren, editors, The Spoken Language Translator. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahl</author>
<author>M Bates</author>
<author>M Brown</author>
<author>K Hunicke-Smith</author>
<author>D Pallet</author>
<author>C Pao</author>
<author>A Rudnicky</author>
<author>E Shriberg</author>
</authors>
<title>Expanding the scope of the ATIS task: The ATIS-3 corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton, NJ.</location>
<marker>Dahl, Bates, Brown, Hunicke-Smith, Pallet, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>D. Dahl, M. Bates, M. Brown, K. Hunicke-Smith, D. Pallet, C. Pao, A. Rudnicky, and E. Shriberg. March 1994. Expanding the scope of the ATIS task: The ATIS-3 corpus. In Proceedings of the ARPA Human Language Technology Workshop, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hart</author>
<author>H G Stork</author>
</authors>
<title>Pattern Classification.</title>
<date>2000</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<marker>Hart, Stork, 2000</marker>
<rawString>R.O. Duda, RE. Hart, and H.G. Stork. 2000. Pattern Classification. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bracy Lemon</author>
<author>A Gruenstein</author>
<author>S Peters</author>
</authors>
<title>Multimodal dialogues with intelligent agents in dynamic environments: The WITAS conversational interface.</title>
<date>2001</date>
<booktitle>In North American Assocation for Computational Linguistics (NAACL),</booktitle>
<contexts>
<context position="2626" citStr="Lemon et al., 2001" startWordPosition="407" endWordPosition="410">of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smoothly from an initial version of the syst</context>
</contexts>
<marker>Lemon, Gruenstein, Peters, 2001</marker>
<rawString>0. Lemon, A. Bracy, A. Gruenstein, and S. Peters. 2001. Multimodal dialogues with intelligent agents in dynamic environments: The WITAS conversational interface. In North American Assocation for Computational Linguistics (NAACL), 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuance</author>
</authors>
<title>http://www.nuance.com.</title>
<date>2002</date>
<journal>As of</journal>
<volume>1</volume>
<contexts>
<context position="8764" citStr="Nuance, 2002" startWordPosition="1439" endWordPosition="1440">Intuitively, the process is very simple. We just walk down the list of possible semantic atoms, starting with the most probable ones, and add them to the semantic representation we are building up when this does not conflict with the consistency rules in the target model. We stop when the atoms suggested have become sufficiently improbable. 3 The ALTERF system This section describes ALTERF, an open-source tool which implements the abstract procedure described in Section 2.1 ALTERF is implemented in SICStus Prolog (Programming Systems Group, 1995), and makes use of the Nuance Toolkit platform (Nuance, 2002) to perform speech recognition and parsing functions. It is compatible with the open-source REGULUS compiler (Rayner et al., 2001a), which can be used to build Nuance recognisers from unification-grammar representations, but does not depend on it. lAlterf can be downloaded from SourceForge at http://sourceforge.net/projects/leonlp/ (Wavfile) (Atoms) (Transcription) where (Wavfile) is the name of a file containing speech data, (Atoms) is the intended semantic representation, and (Transcription) is a text transcription of the speech data. Thus a typical line might be utt12.wav next_line go on Tr</context>
<context position="17068" citStr="Nuance, 2002" startWordPosition="2809" endWordPosition="2810">playing voice notes (&amp;quot;put a voice note on that step&amp;quot;, &amp;quot;play the voice note for step ten&amp;quot;) and setting of voice alarms (&amp;quot;set alarm for ten minutes from now&amp;quot;, &amp;quot;cancel alarm&amp;quot;). There are also a number of other less important functionalities. The semantic representation language currently contains a total of 46 different semantic atoms, including the generic atoms (cf. Section 3.1) *numb e r * and *time* Some examples of utterances and their associated semantic representations are shown at the beginning of Section 2. The speech understanding component is implemented on top of the Nuance platform (Nuance, 2002); the recognition package is compiled from a unification grammar description using the REGULUS tool (Rayner et al., 2001a). An example of a parse representation produced by this grammar appears in Section 3.1. The grammar contains 129 rules and 258 lexical items, and the compiled recogniser achieves a word error rate of approximately 19% on unseen in-domain test data using our normal software and hardware configuration. Use of a grammar-based language model implies that all utterances recognised by the system are within the coverage of the grammar. At the beginning of the current phase of the </context>
</contexts>
<marker>Nuance, 2002</marker>
<rawString>Nuance, 2002. http://www.nuance.com. As of 1 Feb 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penn Treebank</author>
</authors>
<title>The Penn Treebank Project. http://www.cis.upenn.edu/ treebank/.</title>
<date>2002</date>
<journal>As of July</journal>
<volume>3</volume>
<contexts>
<context position="1535" citStr="Treebank, 2002" startWordPosition="228" endWordPosition="229">iated with rule-based systems; methods are moreover starting to emerge which enable data-driven approaches to be used in areas which previously were thought to require rules, such as dialogue management. A good overview of current work in this area is provided in (Young, 2002). Excellent as data-driven systems are, they have one obvious drawback: they require corpus data, usually in fairly substantial amounts. Academics basically interested in pure research are free to work within a domain for which the data has already been collected, and for example can decide to use the Penn Treebank (Penn Treebank, 2002) or the ATIS corpus (Dahl et al., March 1994). If, on the other hand, the goal is to create a useful application for a specified new domain, there will in general be little or no available data at the start of the project. It is possible to create the data by using Wizard of Oz methods, or similar. Wizard of Oz data collection is unattractive for many reasons: it is expensive and timeconsuming, and once the data has been collected it is not easy to change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on r</context>
</contexts>
<marker>Treebank, 2002</marker>
<rawString>Penn Treebank, 2002. The Penn Treebank Project. http://www.cis.upenn.edu/ treebank/. As of July 3, 2002.</rawString>
</citation>
<citation valid="true">
<title>SICStus Prolog User&apos;s</title>
<date>1995</date>
<institution>Programming Systems Group,</institution>
<marker>1995</marker>
<rawString>Programming Systems Group, 1995. SICStus Prolog User&apos;s Manual. Swedish Institute of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>F James</author>
</authors>
<title>A compact architecture for dialogue management based on scripts and meta-outputs.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP</booktitle>
<contexts>
<context position="2606" citStr="Rayner et al., 2000" startWordPosition="403" endWordPosition="406"> change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smoothly from an initial</context>
</contexts>
<marker>Rayner, Hockey, James, 2000</marker>
<rawString>M. Rayner, B.A. Hockey, and F. James. 2000. A compact architecture for dialogue management based on scripts and meta-outputs. In Proceedings of ANLP 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>J Dowding</author>
<author>B A Hockey</author>
</authors>
<title>A baseline method for compiling typed unification grammars into context free language models.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<pages>729--732</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="2647" citStr="Rayner et al., 2001" startWordPosition="411" endWordPosition="414">hese reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smoothly from an initial version of the system 299 which is entir</context>
<context position="8893" citStr="Rayner et al., 2001" startWordPosition="1457" endWordPosition="1460">able ones, and add them to the semantic representation we are building up when this does not conflict with the consistency rules in the target model. We stop when the atoms suggested have become sufficiently improbable. 3 The ALTERF system This section describes ALTERF, an open-source tool which implements the abstract procedure described in Section 2.1 ALTERF is implemented in SICStus Prolog (Programming Systems Group, 1995), and makes use of the Nuance Toolkit platform (Nuance, 2002) to perform speech recognition and parsing functions. It is compatible with the open-source REGULUS compiler (Rayner et al., 2001a), which can be used to build Nuance recognisers from unification-grammar representations, but does not depend on it. lAlterf can be downloaded from SourceForge at http://sourceforge.net/projects/leonlp/ (Wavfile) (Atoms) (Transcription) where (Wavfile) is the name of a file containing speech data, (Atoms) is the intended semantic representation, and (Transcription) is a text transcription of the speech data. Thus a typical line might be utt12.wav next_line go on Training can be carried out in either speech or text mode. In both modes, the Nuance toolkit converts data in the relevant modality</context>
<context position="17188" citStr="Rayner et al., 2001" startWordPosition="2826" endWordPosition="2829">rms (&amp;quot;set alarm for ten minutes from now&amp;quot;, &amp;quot;cancel alarm&amp;quot;). There are also a number of other less important functionalities. The semantic representation language currently contains a total of 46 different semantic atoms, including the generic atoms (cf. Section 3.1) *numb e r * and *time* Some examples of utterances and their associated semantic representations are shown at the beginning of Section 2. The speech understanding component is implemented on top of the Nuance platform (Nuance, 2002); the recognition package is compiled from a unification grammar description using the REGULUS tool (Rayner et al., 2001a). An example of a parse representation produced by this grammar appears in Section 3.1. The grammar contains 129 rules and 258 lexical items, and the compiled recogniser achieves a word error rate of approximately 19% on unseen in-domain test data using our normal software and hardware configuration. Use of a grammar-based language model implies that all utterances recognised by the system are within the coverage of the grammar. At the beginning of the current phase of the project, we recorded 1302 utterances (5540 words) of speech data, using an ad hoc data collection methodology loosely ba</context>
</contexts>
<marker>Rayner, Dowding, Hockey, 2001</marker>
<rawString>M. Rayner, J. Dowding, and B.A. Hockey. 2001a. A baseline method for compiling typed unification grammars into context free language models. In Proceedings of Eurospeech 2001, pages 729-732, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>I Lewin</author>
<author>G Gorrell</author>
<author>J Boye</author>
</authors>
<title>lb. Plug and play spoken language understanding.</title>
<date></date>
<booktitle>In Proceedings of SIGDIAL 2001,</booktitle>
<location>Aalborg, Denmark.</location>
<marker>Rayner, Lewin, Gorrell, Boye, </marker>
<rawString>M. Rayner, I. Lewin, G. Gorrell, and J. Boye. 200 lb. Plug and play spoken language understanding. In Proceedings of SIGDIAL 2001, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J Gawron</author>
<author>E Bratt</author>
<author>R Moore</author>
</authors>
<title>The CommandTalk spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the Thirty-Seventh Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="2585" citStr="Stent et al., 1999" startWordPosition="399" endWordPosition="402">ed it is not easy to change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smo</context>
</contexts>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore. 1999. The CommandTalk spoken dialogue system. In Proceedings of the Thirty-Seventh Annual Meeting of the Association for Computational Linguistics, pages 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-Y Wang</author>
<author>A Acero</author>
<author>C Chelba</author>
<author>B Frey</author>
<author>L Wong</author>
</authors>
<title>Combination of statistical and rule-based approaches for spoken language understadning.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>609--612</pages>
<contexts>
<context position="22490" citStr="Wang et al., 2002" startWordPosition="3728" endWordPosition="3731">ple speech understanding framework combining rule-based and datadriven methods, which has been implemented and 304 Data Rules NGrams Both 10% 28.7% 70.9% 33.1% 20% 28.7% 44.7% 30.4% 30% 29.0% 40.2% 28.4% 40% 27.3% 37.8% 27.6% 50% 27.6% 37.6% 29.4% 60% 27.3% 34.1% 26.9% 70% 27.3% 32.4% 27.6% 80% 27.3% 30.0% 26.6% 90% 27.3% 27.7% 24.6% 100% 27.3% 27.0% 23.6% Table 2: As Table 1, but with training data in text form. first effect is the more important one. evaluated in the context of a non-trivial mediumvocabulary command and control system. In contrast to other work described in the literature ((Wang et al., 2002) is a recent example) rules are treated on a basis of strict parity with other data sources, so that the balance between them can be entirely determined by the training data. For small amounts of data, the rules dominate; by the time we have on the order of 1000 training utterances, data-driven methods are producing significant improvements in performance. The tables in Section 4 suggest that performance has not yet topped out, and will continue to improve as more training data becomes available. It is worth pointing out that the results are in some ways quite surprising, since the basic situa</context>
</contexts>
<marker>Wang, Acero, Chelba, Frey, Wong, 2002</marker>
<rawString>Y.-Y. Wang, A. Acero, C. Chelba, B. Frey, and L. Wong. 2002. Combination of statistical and rule-based approaches for spoken language understadning. In Proceedings of the Seventh International Conference on Spoken Language Processing (ICSLP), pages 609-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="4652" citStr="Yarowsky, 1994" startWordPosition="739" endWordPosition="740">centre of gravity shifts more and more strongly towards the surface features. The rest of the paper is structured as follows. Section 2 describes the abstract architecture, and Section 3 a concrete realisation built on top of the Nuance Toolkit. Section 4 gives details of experiments carried out on a medium-vocabulary command and control task from an instruction manual domain. Section 5 concludes. 2 Semantic analysis as classification This section describes an abstract architecture which characterises semantic analysis as a task slightly extending the &amp;quot;decision-list&amp;quot; classification algorithm (Yarowsky, 1994; Carter, 2000). We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms. For example, in our sample domain we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as line rease_volume} show, sample_syringe} {set_alarm, 5, minutes} {correction, next_step} where increase_volume, show, sample_syringe, set_alarm, 5, minutes, correction and next_step are semantic atoms. As well as specifying the pe</context>
<context position="6610" citStr="Yarowsky, 1994" startWordPosition="1061" endWordPosition="1062"> out any type of processing. In particular, they may involve performing speech recognition on speech data, parsing on text data, application of hand-coded rules to the results of parsing, or some combination of these. Statistics are then compiled to estimate the probability p(a f) of each semantic atom a given each separate feature f, using the standard formula P(a f) = (NY +1)1(N f +2) where Nf is the number of occurrences in the training data of utterances with feature f, and N7 is the number of occurrences of utterances with both feature f and semantic atom a. The decoding process follows (Yarowsky, 1994) in assuming complete dependence between the features. Note that this is in sharp contrast with the Naive Bayes classifier (Duda et al., 2000), which assumes complete independence. Of course, neither assumption can be true in practice; however, as argued in (Carter, 2000), there are good reasons for preferring the dependence alternative as the better option in a situation where there are many features extracted in ways that are likely to overlap. We are given an utterance u, to which we wish to assign a representation R(u) consisting of a set of semantic atoms, together with a target model 300</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 88-95, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
</authors>
<title>Talking to machines (statistically speaking).</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1197" citStr="Young, 2002" startWordPosition="174" endWordPosition="175">rol task. 1 Introduction As the field of spoken language understanding becomes more mature, a clearer picture begins to emerge of the tradeoffs between rule-based and data-driven methods. Other things being equal, there are many reasons to prefer data-driven approaches. They are more robust, and reduce the heavy authoring costs associated with rule-based systems; methods are moreover starting to emerge which enable data-driven approaches to be used in areas which previously were thought to require rules, such as dialogue management. A good overview of current work in this area is provided in (Young, 2002). Excellent as data-driven systems are, they have one obvious drawback: they require corpus data, usually in fairly substantial amounts. Academics basically interested in pure research are free to work within a domain for which the data has already been collected, and for example can decide to use the Penn Treebank (Penn Treebank, 2002) or the ATIS corpus (Dahl et al., March 1994). If, on the other hand, the goal is to create a useful application for a specified new domain, there will in general be little or no available data at the start of the project. It is possible to create the data by us</context>
</contexts>
<marker>Young, 2002</marker>
<rawString>S. Young. 2002. Talking to machines (statistically speaking). In Proceedings of the Seventh International Conference on Spoken Language Processing (ICSLP), pages 9-16.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>