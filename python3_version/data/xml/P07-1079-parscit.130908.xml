<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001649">
<title confidence="0.896285">
HPSG Parsing with Shallow Dependency Constraints
</title>
<author confidence="0.831762">
Kenji Sagae&apos; and Yusuke Miyao&apos; and Jun’ichi Tsujii&apos;,2,3
</author>
<affiliation confidence="0.968509">
&apos;Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
2School of Computer Science, University of Manchester
3National Center for Text Mining
</affiliation>
<email confidence="0.999162">
{sagae,yusuke,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972214285714">
We present a novel framework that com-
bines strengths from surface syntactic pars-
ing and deep syntactic parsing to increase
deep parsing accuracy, specifically by com-
bining dependency and HPSG parsing. We
show that by using surface dependencies to
constrain the application of wide-coverage
HPSG rules, we can benefit from a num-
ber of parsing techniques designed for high-
accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. Our
framework results in a 1.4% absolute im-
provement over a state-of-the-art approach
for wide coverage HPSG parsing.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996124">
Several efficient, accurate and robust approaches to
data-driven dependency parsing have been proposed
recently (Nivre and Scholz, 2004; McDonald et al.,
2005; Buchholz and Marsi, 2006) for syntactic anal-
ysis of natural language using bilexical dependency
relations (Eisner, 1996). Much of the appeal of these
approaches is tied to the use of a simple formalism,
which allows for the use of efficient parsing algo-
rithms, as well as straightforward ways to train dis-
criminative models to perform disambiguation. At
the same time, there is growing interest in pars-
ing with more sophisticated lexicalized grammar
formalisms, such as Lexical Functional Grammar
(LFG) (Bresnan, 1982), Lexicalized Tree Adjoin-
ing Grammar (LTAG) (Schabes et al., 1988), Head-
driven Phrase Structure Grammar (HPSG) (Pollard
and Sag, 1994) and Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000), which represent deep
syntactic structures that cannot be expressed in a
shallower formalism designed to represent only as-
pects of surface syntax, such as the dependency
formalism used in current mainstream dependency
parsing.
We present a novel framework that combines
strengths from surface syntactic parsing and deep
syntactic parsing, specifically by combining depen-
dency and HPSG parsing. We show that, by us-
ing surface dependencies to constrain the applica-
tion of wide-coverage HPSG rules, we can bene-
fit from a number of parsing techniques designed
for high-accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. From the
point of view of HPSG parsing, accuracy can be im-
proved significantly through the use of highly ac-
curate discriminative dependency models, without
the difficulties involved in adapting these models
to a more complex and linguistically sophisticated
formalism. In addition, improvements in depen-
dency parsing accuracy are converted directly into
improvements in HPSG parsing accuracy. From the
point of view of dependency parsing, the applica-
tion of HPSG rules to structures generated by a sur-
face dependency model provides a principled and
linguistically motivated way to identify deep syntac-
tic phenomena, such as long-distance dependencies,
raising and control.
We begin by describing our dependency and
HPSG parsing approaches in section 2. In section
3, we present our framework for HPSG parsing with
shallow dependency constraints, and in section 4 we
</bodyText>
<page confidence="0.980601">
624
</page>
<note confidence="0.959377">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624–631,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999316">
Figure 1: HPSG parsing
</figureCaption>
<bodyText confidence="0.6445425">
evaluate this framework empirically. Sections 5 and
6 discuss related work and conclusions.
</bodyText>
<sectionHeader confidence="0.981603" genericHeader="method">
2 Fast dependency parsing and
wide-coverage HPSG parsing
</sectionHeader>
<subsectionHeader confidence="0.999497">
2.1 Data-driven dependency parsing
</subsectionHeader>
<bodyText confidence="0.9999411">
Because we use dependency parsing as a step in
deep parsing, it is important that we choose a pars-
ing approach that is not only accurate, but also effi-
cient. The deterministic shift/reduce classifier-based
dependency parsing approach (Nivre and Scholz,
2004) has been shown to offer state-of-the-art accu-
racy (Nivre et al., 2006) with high efficiency due to
a greedy search strategy. Our approach is based on
Nivre and Scholz’s approach, using support vector
machines for classification of shift/reduce actions.
</bodyText>
<subsectionHeader confidence="0.999338">
2.2 Wide-coverage HPSG parsing
</subsectionHeader>
<bodyText confidence="0.999857933333333">
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemas explain general
construction rules, and a large number of lexical en-
tries express word-specific syntactic/semantic con-
straints. Figure 1 shows an example of the process
of HPSG parsing. First, lexical entries are assigned
to each word in a sentence. In Figure 1, lexical
entries express subcategorization frames and pred-
icate argument structures. Parsing proceeds by ap-
plying schemas to lexical entries. In this example,
the Head-Complement Schema is applied to the lex-
ical entries of “tried” and “running”. We then obtain
a phrasal structure for “tried running”. By repeat-
edly applying schemas to lexical/phrasal structures,
</bodyText>
<figureCaption confidence="0.814128">
Figure 2: Extracting HPSG lexical entries from the
Penn Treebank
</figureCaption>
<bodyText confidence="0.999490894736842">
we finally obtain an HPSG parse tree that covers the
entire sentence.
In this paper, we use an HPSG parser developed
by Miyao and Tsujii (2005). This parser has a wide-
coverage HPSG lexicon which is extracted from the
Penn Treebank. Figure 2 illustrates their method
for extraction of HPSG lexical entries. First, given
a parse tree from the Penn Treebank (top), HPSG-
style constraints are added and an HPSG-style parse
tree is obtained (middle). Lexical entries are then ex-
tracted from the terminal nodes of the HPSG parse
tree (bottom). This way, in addition to a wide-
coverage lexicon, we also obtain an HPSG treebank,
which can be used as training data for disambigua-
tion models.
The disambiguation model of this parser is based
on a maximum entropy model (Berger et al., 1996).
The probability p(T |W) of an HPSG parse tree T
for the sentence W = (wi, ... , wn) is given as:
</bodyText>
<equation confidence="0.9725465">
p(T|W) = p(T|L,W)p(L|W)
J ri
�ifi(T ) p(lj|W ),
j
</equation>
<bodyText confidence="0.644658">
where L = (li, ... , ln) are lexical entries and
</bodyText>
<equation confidence="0.911386">
1
Z exp
i
</equation>
<page confidence="0.985737">
625
</page>
<bodyText confidence="0.999875727272727">
p(li|W) is the supertagging probability, i.e., the
probability of assignining the lexical entry li to wi
(Ninomiya et al., 2006). The probability p(T IL, W)
is a maximum entropy model on HPSG parse trees,
where Z is a normalization factor, and feature func-
tions fi(T) represent syntactic characteristics, such
as head words, lengths of phrases, and applied
schemas. Given the HPSG treebank as training data,
the model parameters Ai are estimated so as to maxi-
mize the log-likelihood of the training data (Malouf,
2002).
</bodyText>
<sectionHeader confidence="0.9946525" genericHeader="method">
3 HPSG parsing with dependency
constraints
</sectionHeader>
<bodyText confidence="0.999727714285715">
While a number of fairly straightforward models can
be applied successfully to dependency parsing, de-
signing and training HPSG parsing models has been
regarded as a significantly more complex task. Al-
though it seems intuitive that a more sophisticated
linguistic formalism should be more difficult to pa-
rameterize properly, we argue that the difference in
complexity between HPSG and dependency struc-
tures can be seen as incremental, and that the use
of accurate and efficient techniques to determine the
surface dependency structure of a sentence provides
valuable information that aids HPSG disambigua-
tion. This is largely because HPSG is based on a lex-
icalized grammar formalism, and as such its syntac-
tic structures have an underlying dependency back-
bone. However, HPSG syntactic structures includes
long-distance dependencies, and the underlying de-
pendency structure described by and HPSG structure
is a directed acyclic graph, not a dependency tree (as
used by mainstream approaches to data-driven de-
pendency parsing). This difference manifests itself
in words that have multiple heads. For example, in
the sentence I tried to run, the pronoun I is a depen-
dent of tried and of run. This makes it possible to
represent that I is the subject of both verbs, precisely
the kind of information that cannot be represented in
dependency parsing. If we ignore long-distance de-
pendencies, however, HPSG structures can be seen
as lexicalized trees that can be easily converted into
dependency trees.
Given that for an HPSG representation of the syn-
tactic structure of a sentence we can determine a
dependency tree by removing long-distance depen-
dencies, we can use dependency parsing techniques
(such as the deterministic dependency parsing ap-
proach mentioned in section 2.1) to determine the
underlying dependency trees in HPSG structures.
This is the basis for the parsing framework presented
here. In this approach, deep dependency analysis
is done in two stages. First, a dependency parser
determines the shallow dependency tree for the in-
put sentence. This shallow dependency tree corre-
sponds to the underlying dependency graph of the
HPSG structure for the input sentence, without de-
pendencies that roughly correspond to deep syntax.
The second step is to perform HPSG parsing, as
described in section 2.2, but using the shallow de-
pendency tree to constrain the application of HPSG
rules. We now discuss these two steps in more detail.
</bodyText>
<subsectionHeader confidence="0.7685495">
3.1 Determining shallow dependencies in
HPSG structures using dependency parsing
</subsectionHeader>
<bodyText confidence="0.999989068965517">
In order to apply a data-driven dependency ap-
proach to the task of identifying the shallow de-
pendency tree in HPSG structures, we first need a
corpus of such dependency trees to serve as train-
ing data. We created a dependency training corpus
based on the Penn Treebank (Marcus et al., 1993),
or more specifically on the HPSG Treebank gener-
ated from the Penn Treebank (see section 2.2). For
each HPSG structure in the HPSG Treebank, a de-
pendency tree is extracted in two steps. First, the
HPSG tree is converted into a CFG-style tree, sim-
ply by removing long-distance dependency links be-
tween nodes. A dependency tree is then extracted
from the resulting lexicalized CFG-style tree, as is
commonly done for converting constituent trees into
dependency trees after the application of a head-
percolation table (Collins, 1999).
Once a dependency training corpus is available,
it is used to train a dependency parser as described
in section 2.1. This is done by training a classifier
to determine parser actions based on local features
that represent the current state of the parser (Nivre
and Scholz, 2004; Sagae and Lavie, 2005). Train-
ing data for the classifier is obtained by applying the
parsing algorithm over the training sentences (for
which the correct dependency structures are known)
and recording the appropriate parser actions that re-
sult in the formation of the correct dependency trees,
coupled with the features that represent the state of
</bodyText>
<page confidence="0.963387">
626
</page>
<bodyText confidence="0.9875196">
the parser mentioned in section 2.1. An evaluation Soft dependency constraints can be implemented
of the resulting dependency parser and its efficacy in as explained above as a straightforward extension of
aiding HPSG parsing is presented in section 4. the parsing algorithm. In addition, it is easily inte-
3.2 Parsing with dependency constraints grated with beam thresholding methods of parsing.
Given a set of dependencies, the bottom-up process Because beam thresholding discards partial parse
of HPSG parsing can be constrained so that it does trees that have low log-probabilities, we can ex-
not violate the given dependencies. This can be pect that the parser would discard partial parse trees
achieved by a simple extension of the parsing algo- based on violation of the dependency constraints.
rithm, as follows. During parsing, we store the lex- 4 Experiments
ical head of each partial parse tree. In each schema We evaluate the accuracy of HPSG parsing with de-
application, we can determine which child is the pendency constraints on the HPSG Treebank (Miyao
head; for example, the left child is the head when et al., 2003), which is extracted from the Wall Street
we apply the Head-Complement Schema. Given this Journal portion of the Penn Treebank (Marcus et
information and lexical heads, the parser can iden- al., 1993)1. Sections 02-21 were used for training
tify the dependency produced by this schema appli- (for HPSG and dependency parsers), section 22 was
cation, and can therefore judge whether the schema used as development data, and final testing was per-
application violates the dependency constraints. formed on section 23. Following previous work on
This method forces the HPSG parser to produce wide-coverage parsing with lexicalized grammars
parse trees that strictly conform to the output of using the Penn Treebank, we evaluate the parser by
the dependency parser. However, this means that measuring the accuracy of predicate-argument rela-
the HPSG parser outputs no successful parse results tions in the parser’s output. A predicate-argument
when it cannot find the parse tree that is completely relation is defined as a tuple (Q, wh, a, wa), where
consistent with the given dependencies. This situ- a is the predicate type (e.g. adjective, intransitive
ation may occur when the dependency parser pro- verb), wh is the head word of the predicate, a is the
duces structures that are not covered in the HPSG argument label (MODARG, ARG1, ... , ARG4), and
grammar. This is especially likely with a fully data- wa is the head word of the argument. Labeled pre-
driven dependency parser that uses local classifica- cision (LP)/labeled recall (LR) is the ratio of tuples
tion, since its output may not be globally consistent correctly identified by the parser. These predicate-
grammatically. In addition, the HPSG grammar is argument relations cover the full range of syntactic
extracted from the HPSG Treebank using a corpus- dependencies produced by the HPSG parser (includ-
based procedure, and it does not necessarily cover ing, long-distance dependencies, raising and control,
all possible grammatical phenomena in unseen text in addition to surface dependencies).
(Miyao and Tsujii, 2005). In the experiments presented in this section, in-
We therefore propose an extension of this ap- put sentences were automatically tagged with parts-
proach that uses predetermined dependencies as soft of-speech with about 97% accuracy, using a max-
constraints. Violations of schema applications are imum entropy POS tagger. We also report results
detected in the same way as before, but instead of on parsing text with gold standard POS tags, where
strictly prohibiting schema applications, we penal- explicitly noted. This provides an upper-bound on
ize the log-likelihood of partial parse trees created what can be expected if a more sophisticated multi-
by schema applications that violate the dependen- tagging scheme (James R. Curran and Vadas, 2006)
cies constraints. Given a negative value a, we add is used, instead of hard assignment of single tags in
a to the log-probability of a partial parse tree when a preprocessing step as done here.
the schema application violates the dependency con-
straints. That is, when a parse tree violates n depen-
dencies, the log-probability of the parse tree is low-
ered by na. The meta parameter a is determined so
as to maximize the accuracy on the development set.
627
1The extraction software can be obtained from http://www-
tsujii.is.s.u-tokyo.ac.jp/enju.
</bodyText>
<subsectionHeader confidence="0.936253">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.998427666666667">
HPSG parsing results using the same HPSG gram-
mar and treebank have recently been reported by
Miyao and Tsujii (2005) and Ninomia et al. (2006).
By running the HPSG parser described in section 2.2
on the development data without dependency con-
straints, we obtain similar values of LP (86.8%) and
LR (85.6%) as those reported by Miyao and Tsu-
jii (Miyao and Tsujii, 2005). Using the extremely
lexicalized framework of (Ninomiya et al., 2006) by
performing supertagging before parsing, we obtain
similar accuracy as Ninomiya et al. (87.1% LP and
85.9% LR).
</bodyText>
<subsectionHeader confidence="0.677195">
4.2 Dependency constraints and the penalty
parameter
</subsectionHeader>
<bodyText confidence="0.999485666666666">
Parsing the development data with hard dependency
constraints confirmed the intuition that these con-
straints often describe dependency structures that do
not conform to HPSG schema used in parsing, re-
sulting in parse failures. To determine the upper-
bound on HPSG parsing with hard dependency con-
straints, we set the HPSG parser to disallow the ap-
plication of any rules that result in the creation of
dependencies that violate gold standard dependen-
cies. This results in high precision (96.7%), but re-
call is low (82.3%) due to parse failures caused by
lack of grammatical coverage 2. Using dependen-
cies produced by the shift-reduce SVM parser, we
obtain 91.5% LP and 65.7% LR. This represents a
large gain in precision over the baseline, but an even
greater loss in recall, which limits the usefulness of
the parser, and severely hurts the appeal of hard con-
straints.
We focus the rest of our experiments on parsing
with soft dependency constraints. As explained in
section 3, this involves setting the penalty parame-
ter α. During parsing, we subtract α from the log-
probability of applying any schema that violates the
dependency constraints given to the HPSG parser.
Figure 3 illustrates the effect of α when gold stan-
dard dependencies (and gold standard POS tags) are
used. We note that setting α = 0 causes the parser
</bodyText>
<footnote confidence="0.5485196">
2Although the HPSG grammar does not have perfect cov-
erage of unseen text, it supports complete and mostly correct
analyses for all sentences in the development set. However,
when we require completely correct analyses by using hard con-
straints, lack of coverage may cause parse failures.
</footnote>
<figureCaption confidence="0.995659">
Figure 3: The effect of α on HPSG parsing con-
strained by gold standard dependencies.
</figureCaption>
<bodyText confidence="0.991968413793104">
to ignore dependency constraints, providing base-
line performance. Conversely, setting a high enough
value (α = 30 is sufficient, in practice) causes any
substructures that violate the dependency constraints
to be used only when they are absolutely neces-
sary to produce a valid parse for the input sentence.
In figure 3, this corresponds to an upper-bound on
the accuracy of parsing with soft dependency con-
straints (94.7% f-score), since gold standard depen-
dencies are used.
We set α empirically with simple hill climbing on
the development set. Because it is expected that the
optimal value of α depends on the accuracy of the
surface dependency parser, we set separate values
for parsing with a POS tagger or with gold standard
POS tags. Figure 4 shows the accuracy of HPSG
predicate-argument relations obtained with depen-
dency constraints determined by dependency pars-
ing with gold standard POS tags. With both au-
tomatically assigned and gold standard POS tags,
we observe an improvement of about 0.6% in pre-
cision, recall and f-score, when the optimal α value
is used in each case. While this corresponds to a rel-
ative error reduction of over 6% (or 12%, if we con-
sider the upper-bound dictated by imperfect gram-
matical coverage), a more interesting aspect of this
framework is that it allows techniques designed for
improving dependency accuracy to improve HPSG
parsing accuracy directly, as we illustrate next.
</bodyText>
<figure confidence="0.968003222222222">
96
95
94
93
92
91
90
89
0 5 1
</figure>
<page confidence="0.76424">
628
</page>
<figureCaption confidence="0.992038333333333">
Figure 4: The effect of α on HPSG parsing con-
strained by the output of a dependency parser using
gold standard POS tags.
</figureCaption>
<subsectionHeader confidence="0.9275875">
4.3 Determining constraints with dependency
parser combination
</subsectionHeader>
<bodyText confidence="0.9999398">
Parser combination has been shown to be a power-
ful way to obtain very high accuracy in dependency
parsing (Sagae and Lavie, 2006). Using dependency
constraints allows us to improve HPSG parsing ac-
curacy simply by using an existing parser combina-
tion approach. As a first step, we train two addi-
tional parsers with the dependencies extracted from
the HPSG Treebank. The first uses the same shift-
reduce framework described in section 2.1, but it
process the input from right to left (RL). This has
been found to work well in previous work on depen-
dency parser combination (Zeman and ˇZabokrtsk´y,
2005; Sagae and Lavie, 2006). The second parser
is MSTParser, the large-margin maximum spanning
tree parser described in (McDonald et al., 2005)3.
We examine the use of two combination schemes:
one using two parsers, and one using three parsers.
The first combination approach is to keep only de-
pendencies for which there is agreement between the
two parsers. In other words, dependencies that are
proposed by one parser but not the other are simply
discarded. Using the left-to-right shift-reduce parser
and MSTParser, we find that this results in very high
precision of surface dependencies on the develop-
ment data. In the second approach, combination of
</bodyText>
<footnote confidence="0.930993">
3Downloaded from http://sourceforge.net/projects/mstparser
</footnote>
<bodyText confidence="0.999541583333333">
the three dependency parsers is done according to
the maximum spanning tree combination scheme of
Sagae and Lavie (2006), which results in high accu-
racy of surface dependencies. For each of the com-
bination approaches, we use the resulting dependen-
cies as constraints for HPSG parsing, determining
the optimal value of α on the development set in
the same way as done for a single parser. Table 1
summarizes our experiments on development data
using parser combinations to produce dependency
constraints 4. The two combination approaches are
denoted as C1 and C2.
</bodyText>
<table confidence="0.999788142857143">
Parser Dep α HPSG Diff
none (baseline) – – 86.5 –
LR shift-reduce 91.2 1.5 87.1 0.6
RL shift-reduce 90.1 – –
MSTParser 91.0 – –
C1 (agreement) 96.8* 2.5 87.4 0.9
C2 (MST) 92.4 2.5 87.4 0.9
</table>
<tableCaption confidence="0.998623">
Table 1: Summary of results on development data.
</tableCaption>
<bodyText confidence="0.94332875">
* The shallow accuracy of combination C1 corre-
sponds to the dependency precision (no dependen-
cies were reported for 8% of all words in the devel-
opment set).
</bodyText>
<subsectionHeader confidence="0.822365">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999198882352941">
Having determined α values on development data
for the shift-reduce dependency parser, the two-
parser agreement combination, and the three-parser
maximum spanning tree combination, we parse the
test data (section 23) using these three different
sources of dependency constraints for HPSG pars-
ing. Our final results are shown in table 2, where
we also include the results published in (Ninomiya
et al., 2006) for comparison purposes, and the result
of using dependency constraints obtained with gold
standard POS tags.
By using two unlabeled dependency parsers to
provide soft dependency constraints, we obtain a
1% absolute improvement in precision and recall of
predicate-argument identification in HPSG parsing
over a strong baseline. Our baseline approach out-
performed previously published results on this test
</bodyText>
<footnote confidence="0.997255666666667">
4The accuracy figures for the dependency parsers is ex-
pressed as unlabeled accuracy of the surface dependencies only,
and are not comparable to the HPSG parsing accuracy figures
</footnote>
<figure confidence="0.963614636363636">
90.8
90.6
90.4
90.2
89.8
89.6
89.
91
90
4
0 0.5
</figure>
<page confidence="0.99467">
629
</page>
<table confidence="0.999658272727273">
Parser LP LR F-score
HPSG Baseline 87.4 87.0 87.2
Shift-Reduce + HPSG 88.2 87.7 87.9
C1 + HPSG 88.5 88.0 88.2
C2 + HPSG 88.4 87.9 88.1
Baseline(gold) 89.8 89.4 89.6
Shift-Reduce(gold) 90.62 90.23 90.42
C1+HPSG(gold) 90.9 90.4 90.6
C2+HPSG(gold) 90.8 90.4 90.6
Miyao and Tsujii, 2005 85.0 84.3 84.6
Ninomiya et al., 2006 87.4 86.3 86.8
</table>
<tableCaption confidence="0.96076">
Table 2: Final results on test set. The first set of
</tableCaption>
<bodyText confidence="0.993682257142857">
results show our HPSG baseline and HPSG with soft
dependency constraints using three different sources
of dependency constraints. The second set of results
show the accuracy of the same parsers when gold
part-of-speech tags are used. The third set of results
is from existing published models on the same data.
set, and our best performing combination scheme
obtains an absolute improvement of 1.4% over the
best previously published results using the HPSG
Treebank. It is interesting to note that the results ob-
tained with dependency parser combinations C1 and
C2 were very similar, even though in C1 only two
parsers were used, and constraints were provided for
about 92% of shallow dependencies (with accuracy
higher than 96%). Clearly, precision is crucial in de-
pendency constraints.
Finally, although it is necessary to perform de-
pendency parsing to pre-compute dependency con-
straints, the total time required to perform the en-
tire process of HPSG parsing with dependency con-
straints is close to that of the baseline HPSG ap-
proach. This is due to two reasons: (1) the de-
pendency parsing approaches used to pre-compute
constraints are several times faster than the baseline
HPSG approach, and (2) the HPSG portion of the
process is significantly faster when dependency con-
straints are used, since the constraints help sharpen
the search space, making search more efficient. Us-
ing the baseline HPSG approach, it takes approx-
imately 25 minutes to parse the test set. The to-
tal time required to parse the test set using HPSG
with dependency constraints generated by the shift-
reduce parser is 27 minutes. With combination C1,
parsing time increases to 30 minutes, since two de-
pendency parsers are used sequentially.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.9999854">
There are other approaches that combine shallow
processing with deep parsing (Crysmann et al.,
2002; Frank et al., 2003; Daum et al., 2003) to im-
prove parsing efficiency. Typically, shallow parsing
is used to create robust minimal recursion seman-
tics, which are used as constraints to limit ambigu-
ity during parsing. Our approach, in contrast, uses
syntactic dependencies to achieve a significant im-
provement in the accuracy of wide-coverage HPSG
parsing. Additionally, our approach is in many
ways similar to supertagging (Bangalore and Joshi,
1999), which uses sequence labeling techniques as
an efficient way to pre-compute parsing constraints
(specifically, the assignment of lexical entries to in-
put words).
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999365">
We have presented a novel framework for taking ad-
vantage of the strengths of a shallow parsing ap-
proach and a deep parsing approach. We have
shown that by constraining the application of rules
in HPSG parsing according to results from a depen-
dency parser, we can significantly improve the ac-
curacy of deep parsing by using shallow syntactic
analyses.
To illustrate how this framework allows for im-
provements in the accuracy of dependency parsing
to be used directly to improve the accuracy of HPSG
parsing, we showed that by combining the results of
different dependency parsers using the search-based
parsing ensemble approach of (Sagae and Lavie,
2006), we obtain improved HPSG parsing accuracy
as a result of the improved dependency accuracy.
Although we have focused on the use of HPSG
and dependency parsing, the general framework pre-
sented here can be applied to other lexicalized gram-
mar formalisms, such as LTAG, CCG and LFG.
</bodyText>
<sectionHeader confidence="0.997775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9457495">
This research was partially supported by Grant-in-
Aid for Specially Promoted Research 18002007.
</bodyText>
<page confidence="0.996778">
630
</page>
<sectionHeader confidence="0.996135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999419041237114">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237–265.
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
Joan Bresnan. 1982. The mental representation of gram-
matical relations. MIT Press.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Natural Language
Learning. New York, NY.
M. Collins. 1999. Head-Driven Models for Natural Lan-
guage Parsing. Phd thesis, University of Pennsylva-
nia.
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan
Mueller, Guenter Neumann, Jakub Piskorski, Ulrich
Schaefer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu,
Markus Becker, and Hans-Ulrich Krieger. 2002. An
integrated architecture for shallow and deep process-
ing. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Michael Daum, Kilian A. Foth, and Wolfgang Menzel.
2003. Constraint-based integration of deep and shal-
low parsing techniques. In Proceedings of the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2003).
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the International Conference on Computational Lin-
guistics (COLING’96). Copenhagen, Denmark.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Schaefer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2003), pages
104–111.
Stephen Clark James R. Curran and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In
Proceedings of COLING/ACL 2006. Sydney, Aus-
tralia.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the 2002 Conference on Natural Language
Learning.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Ryan McDonald, Fernando Pereira, K. Ribarov, and
J. Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technolo-
gies/Empirical Methods in Natural Language Process-
ing (HLT-EMNLP). Vancouver, Canada.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 42nd Meeting of the Associ-
ation for Computational Linguistics. Ann Arbor, MI.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsu-
jii. 2003. Corpus oriented grammar development for
aquiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of the Tenth Con-
ference on Natural Language Learning.
T. Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, and
J. Tsujii. 2006. Extremely lexicalized models for ac-
curate and fast hpsg parsing. In Proceedings of the
2006 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2006).
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 64–70. Geneva, Switzerland.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Tenth Conference on Natural Language Learning.
New York, NY.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of the 2006 Meeting of
the North American ACL. New York, NY.
Yves Schabes, Anne Abeille, and Aravind Joshi. 1988.
Parsing strategies with lexicalized grammars: Appli-
cation to tree adjoining grammars. In Proceedings of
12th COLING.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Zeman and Zdenek ˇZabokrtsk´y. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In Proceedings of the International Workshop
on Parsing Technologies. Vancouver, Canada.
</reference>
<page confidence="0.998302">
631
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634241">
<title confidence="0.952739">HPSG Parsing with Shallow Dependency Constraints</title>
<affiliation confidence="0.9303925">of Computer Science University of Tokyo</affiliation>
<address confidence="0.940836">Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan</address>
<affiliation confidence="0.9251365">of Computer Science, University of Manchester Center for Text Mining</affiliation>
<abstract confidence="0.9967302">We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="25024" citStr="Bangalore and Joshi, 1999" startWordPosition="4008" endWordPosition="4011">minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to input words). 6 Conclusion We have presented a novel framework for taking advantage of the strengths of a shallow parsing approach and a deep parsing approach. We have shown that by constraining the application of rules in HPSG parsing according to results from a dependency parser, we can significantly improve the accuracy of deep parsing by using shallow syntactic analyses. To illustrate how this framework allows for improvements in the accuracy </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5832" citStr="Berger et al., 1996" startWordPosition="883" endWordPosition="886">as a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a parse tree from the Penn Treebank (top), HPSGstyle constraints are added and an HPSG-style parse tree is obtained (middle). Lexical entries are then extracted from the terminal nodes of the HPSG parse tree (bottom). This way, in addition to a widecoverage lexicon, we also obtain an HPSG treebank, which can be used as training data for disambiguation models. The disambiguation model of this parser is based on a maximum entropy model (Berger et al., 1996). The probability p(T |W) of an HPSG parse tree T for the sentence W = (wi, ... , wn) is given as: p(T|W) = p(T|L,W)p(L|W) J ri �ifi(T ) p(lj|W ), j where L = (li, ... , ln) are lexical entries and 1 Z exp i 625 p(li|W) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T IL, W) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi(T) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treeban</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>The mental representation of grammatical relations.</title>
<date>1982</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1592" citStr="Bresnan, 1982" startWordPosition="230" endWordPosition="231"> dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing. We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. We show that,</context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>Joan Bresnan. 1982. The mental representation of grammatical relations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Natural Language Learning.</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="1099" citStr="Buchholz and Marsi, 2006" startWordPosition="151" endWordPosition="154">y, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing. 1 Introduction Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven Phrase Structure Grammar (HP</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Natural Language Learning. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Models for Natural Language Parsing. Phd thesis,</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="9947" citStr="Collins, 1999" startWordPosition="1558" endWordPosition="1559">eated a dependency training corpus based on the Penn Treebank (Marcus et al., 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2). For each HPSG structure in the HPSG Treebank, a dependency tree is extracted in two steps. First, the HPSG tree is converted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train a dependency parser as described in section 2.1. This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). Training data for the classifier is obtained by applying the parsing algorithm over the training sentences (for which the correct dependency structures are known) and recording the appropriate parser actions that result in the formation of the correct dependency trees, coupled with the features</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Models for Natural Language Parsing. Phd thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berthold Crysmann</author>
<author>Anette Frank</author>
<author>Bernd Kiefer</author>
<author>Stefan Mueller</author>
<author>Guenter Neumann</author>
<author>Jakub Piskorski</author>
<author>Ulrich Schaefer</author>
<author>Melanie Siegel</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
<author>Markus Becker</author>
<author>Hans-Ulrich Krieger</author>
</authors>
<title>An integrated architecture for shallow and deep processing.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="24573" citStr="Crysmann et al., 2002" startWordPosition="3938" endWordPosition="3941"> portion of the process is significantly faster when dependency constraints are used, since the constraints help sharpen the search space, making search more efficient. Using the baseline HPSG approach, it takes approximately 25 minutes to parse the test set. The total time required to parse the test set using HPSG with dependency constraints generated by the shiftreduce parser is 27 minutes. With combination C1, parsing time increases to 30 minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to </context>
</contexts>
<marker>Crysmann, Frank, Kiefer, Mueller, Neumann, Piskorski, Schaefer, Siegel, Uszkoreit, Xu, Becker, Krieger, 2002</marker>
<rawString>Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mueller, Guenter Neumann, Jakub Piskorski, Ulrich Schaefer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu, Markus Becker, and Hans-Ulrich Krieger. 2002. An integrated architecture for shallow and deep processing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Daum</author>
<author>Kilian A Foth</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Constraint-based integration of deep and shallow parsing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<contexts>
<context position="24613" citStr="Daum et al., 2003" startWordPosition="3946" endWordPosition="3949">ster when dependency constraints are used, since the constraints help sharpen the search space, making search more efficient. Using the baseline HPSG approach, it takes approximately 25 minutes to parse the test set. The total time required to parse the test set using HPSG with dependency constraints generated by the shiftreduce parser is 27 minutes. With combination C1, parsing time increases to 30 minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to input words). 6 Conclusion We have prese</context>
</contexts>
<marker>Daum, Foth, Menzel, 2003</marker>
<rawString>Michael Daum, Kilian A. Foth, and Wolfgang Menzel. 2003. Constraint-based integration of deep and shallow parsing techniques. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING’96).</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1194" citStr="Eisner, 1996" startWordPosition="166" endWordPosition="167">strain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing. 1 Introduction Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which re</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the International Conference on Computational Linguistics (COLING’96). Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Markus Becker</author>
<author>Berthold Crysmann</author>
<author>Bernd Kiefer</author>
<author>Ulrich Schaefer</author>
</authors>
<title>Integrated shallow and deep parsing: TopP meets HPSG.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="24593" citStr="Frank et al., 2003" startWordPosition="3942" endWordPosition="3945"> is significantly faster when dependency constraints are used, since the constraints help sharpen the search space, making search more efficient. Using the baseline HPSG approach, it takes approximately 25 minutes to parse the test set. The total time required to parse the test set using HPSG with dependency constraints generated by the shiftreduce parser is 27 minutes. With combination C1, parsing time increases to 30 minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to input words). 6 Conc</context>
</contexts>
<marker>Frank, Becker, Crysmann, Kiefer, Schaefer, 2003</marker>
<rawString>Anette Frank, Markus Becker, Berthold Crysmann, Bernd Kiefer, and Ulrich Schaefer. 2003. Integrated shallow and deep parsing: TopP meets HPSG. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark James R Curran</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="14538" citStr="Curran and Vadas, 2006" startWordPosition="2285" endWordPosition="2288">ly tagged with partsproach that uses predetermined dependencies as soft of-speech with about 97% accuracy, using a maxconstraints. Violations of schema applications are imum entropy POS tagger. We also report results detected in the same way as before, but instead of on parsing text with gold standard POS tags, where strictly prohibiting schema applications, we penal- explicitly noted. This provides an upper-bound on ize the log-likelihood of partial parse trees created what can be expected if a more sophisticated multiby schema applications that violate the dependen- tagging scheme (James R. Curran and Vadas, 2006) cies constraints. Given a negative value a, we add is used, instead of hard assignment of single tags in a to the log-probability of a partial parse tree when a preprocessing step as done here. the schema application violates the dependency constraints. That is, when a parse tree violates n dependencies, the log-probability of the parse tree is lowered by na. The meta parameter a is determined so as to maximize the accuracy on the development set. 627 1The extraction software can be obtained from http://wwwtsujii.is.s.u-tokyo.ac.jp/enju. 4.1 Baseline HPSG parsing results using the same HPSG g</context>
</contexts>
<marker>Curran, Vadas, 2006</marker>
<rawString>Stephen Clark James R. Curran and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of COLING/ACL 2006. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="6562" citStr="Malouf, 2002" startWordPosition="1017" endWordPosition="1018">(L|W) J ri �ifi(T ) p(lj|W ), j where L = (li, ... , ln) are lexical entries and 1 Z exp i 625 p(li|W) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T IL, W) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi(T) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters Ai are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as a significantly more complex task. Although it seems intuitive that a more sophisticated linguistic formalism should be more difficult to parameterize properly, we argue that the difference in complexity between HPSG and dependency structures can be seen as incremental, and that the use of accurate and efficient techniques to determine the surface dependency structure of a sentence provide</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the 2002 Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewics</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="9416" citStr="Marcus et al., 1993" startWordPosition="1470" endWordPosition="1473">encies that roughly correspond to deep syntax. The second step is to perform HPSG parsing, as described in section 2.2, but using the shallow dependency tree to constrain the application of HPSG rules. We now discuss these two steps in more detail. 3.1 Determining shallow dependencies in HPSG structures using dependency parsing In order to apply a data-driven dependency approach to the task of identifying the shallow dependency tree in HPSG structures, we first need a corpus of such dependency trees to serve as training data. We created a dependency training corpus based on the Penn Treebank (Marcus et al., 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2). For each HPSG structure in the HPSG Treebank, a dependency tree is extracted in two steps. First, the HPSG tree is converted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewics, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewics. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technologies/Empirical Methods in Natural Language Processing (HLT-EMNLP).</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1072" citStr="McDonald et al., 2005" startWordPosition="147" endWordPosition="150">se deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing. 1 Introduction Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven P</context>
<context position="19770" citStr="McDonald et al., 2005" startWordPosition="3157" endWordPosition="3160">Lavie, 2006). Using dependency constraints allows us to improve HPSG parsing accuracy simply by using an existing parser combination approach. As a first step, we train two additional parsers with the dependencies extracted from the HPSG Treebank. The first uses the same shiftreduce framework described in section 2.1, but it process the input from right to left (RL). This has been found to work well in previous work on dependency parser combination (Zeman and ˇZabokrtsk´y, 2005; Sagae and Lavie, 2006). The second parser is MSTParser, the large-margin maximum spanning tree parser described in (McDonald et al., 2005)3. We examine the use of two combination schemes: one using two parsers, and one using three parsers. The first combination approach is to keep only dependencies for which there is agreement between the two parsers. In other words, dependencies that are proposed by one parser but not the other are simply discarded. Using the left-to-right shift-reduce parser and MSTParser, we find that this results in very high precision of surface dependencies on the development data. In the second approach, combination of 3Downloaded from http://sourceforge.net/projects/mstparser the three dependency parsers</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technologies/Empirical Methods in Natural Language Processing (HLT-EMNLP). Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage hpsg parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics.</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5197" citStr="Miyao and Tsujii (2005)" startWordPosition="775" endWordPosition="778">e assigned to each word in a sentence. In Figure 1, lexical entries express subcategorization frames and predicate argument structures. Parsing proceeds by applying schemas to lexical entries. In this example, the Head-Complement Schema is applied to the lexical entries of “tried” and “running”. We then obtain a phrasal structure for “tried running”. By repeatedly applying schemas to lexical/phrasal structures, Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a parse tree from the Penn Treebank (top), HPSGstyle constraints are added and an HPSG-style parse tree is obtained (middle). Lexical entries are then extracted from the terminal nodes of the HPSG parse tree (bottom). This way, in addition to a widecoverage lexicon, we also obtain an HPSG treebank, which can be used as training data for disambiguation models. The disambiguation model of this parser is based on a maximum </context>
<context position="13789" citStr="Miyao and Tsujii, 2005" startWordPosition="2169" endWordPosition="2172">n dependency parser that uses local classifica- cision (LP)/labeled recall (LR) is the ratio of tuples tion, since its output may not be globally consistent correctly identified by the parser. These predicategrammatically. In addition, the HPSG grammar is argument relations cover the full range of syntactic extracted from the HPSG Treebank using a corpus- dependencies produced by the HPSG parser (includbased procedure, and it does not necessarily cover ing, long-distance dependencies, raising and control, all possible grammatical phenomena in unseen text in addition to surface dependencies). (Miyao and Tsujii, 2005). In the experiments presented in this section, inWe therefore propose an extension of this ap- put sentences were automatically tagged with partsproach that uses predetermined dependencies as soft of-speech with about 97% accuracy, using a maxconstraints. Violations of schema applications are imum entropy POS tagger. We also report results detected in the same way as before, but instead of on parsing text with gold standard POS tags, where strictly prohibiting schema applications, we penal- explicitly noted. This provides an upper-bound on ize the log-likelihood of partial parse trees created</context>
<context position="15212" citStr="Miyao and Tsujii (2005)" startWordPosition="2397" endWordPosition="2400"> is used, instead of hard assignment of single tags in a to the log-probability of a partial parse tree when a preprocessing step as done here. the schema application violates the dependency constraints. That is, when a parse tree violates n dependencies, the log-probability of the parse tree is lowered by na. The meta parameter a is determined so as to maximize the accuracy on the development set. 627 1The extraction software can be obtained from http://wwwtsujii.is.s.u-tokyo.ac.jp/enju. 4.1 Baseline HPSG parsing results using the same HPSG grammar and treebank have recently been reported by Miyao and Tsujii (2005) and Ninomia et al. (2006). By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). Using the extremely lexicalized framework of (Ninomiya et al., 2006) by performing supertagging before parsing, we obtain similar accuracy as Ninomiya et al. (87.1% LP and 85.9% LR). 4.2 Dependency constraints and the penalty parameter Parsing the development data with hard dependency constraints confirmed the intuition that these constraints o</context>
<context position="22637" citStr="Miyao and Tsujii, 2005" startWordPosition="3620" endWordPosition="3623"> parsing over a strong baseline. Our baseline approach outperformed previously published results on this test 4The accuracy figures for the dependency parsers is expressed as unlabeled accuracy of the surface dependencies only, and are not comparable to the HPSG parsing accuracy figures 90.8 90.6 90.4 90.2 89.8 89.6 89. 91 90 4 0 0.5 629 Parser LP LR F-score HPSG Baseline 87.4 87.0 87.2 Shift-Reduce + HPSG 88.2 87.7 87.9 C1 + HPSG 88.5 88.0 88.2 C2 + HPSG 88.4 87.9 88.1 Baseline(gold) 89.8 89.4 89.6 Shift-Reduce(gold) 90.62 90.23 90.42 C1+HPSG(gold) 90.9 90.4 90.6 C2+HPSG(gold) 90.8 90.4 90.6 Miyao and Tsujii, 2005 85.0 84.3 84.6 Ninomiya et al., 2006 87.4 86.3 86.8 Table 2: Final results on test set. The first set of results show our HPSG baseline and HPSG with soft dependency constraints using three different sources of dependency constraints. The second set of results show the accuracy of the same parsers when gold part-of-speech tags are used. The third set of results is from existing published models on the same data. set, and our best performing combination scheme obtains an absolute improvement of 1.4% over the best previously published results using the HPSG Treebank. It is interesting to note t</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage hpsg parsing. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus oriented grammar development for aquiring a head-driven phrase structure grammar from the penn treebank.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Conference on Natural Language Learning.</booktitle>
<marker>Miyao, Ninomiya, Tsujii, 2003</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2003. Corpus oriented grammar development for aquiring a head-driven phrase structure grammar from the penn treebank. In Proceedings of the Tenth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ninomiya</author>
<author>T Matsuzaki</author>
<author>Y Tsuruoka</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast hpsg parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods for Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="6172" citStr="Ninomiya et al., 2006" startWordPosition="951" endWordPosition="954">l nodes of the HPSG parse tree (bottom). This way, in addition to a widecoverage lexicon, we also obtain an HPSG treebank, which can be used as training data for disambiguation models. The disambiguation model of this parser is based on a maximum entropy model (Berger et al., 1996). The probability p(T |W) of an HPSG parse tree T for the sentence W = (wi, ... , wn) is given as: p(T|W) = p(T|L,W)p(L|W) J ri �ifi(T ) p(lj|W ), j where L = (li, ... , ln) are lexical entries and 1 Z exp i 625 p(li|W) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T IL, W) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi(T) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters Ai are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as a </context>
<context position="15534" citStr="Ninomiya et al., 2006" startWordPosition="2451" endWordPosition="2454">rameter a is determined so as to maximize the accuracy on the development set. 627 1The extraction software can be obtained from http://wwwtsujii.is.s.u-tokyo.ac.jp/enju. 4.1 Baseline HPSG parsing results using the same HPSG grammar and treebank have recently been reported by Miyao and Tsujii (2005) and Ninomia et al. (2006). By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). Using the extremely lexicalized framework of (Ninomiya et al., 2006) by performing supertagging before parsing, we obtain similar accuracy as Ninomiya et al. (87.1% LP and 85.9% LR). 4.2 Dependency constraints and the penalty parameter Parsing the development data with hard dependency constraints confirmed the intuition that these constraints often describe dependency structures that do not conform to HPSG schema used in parsing, resulting in parse failures. To determine the upperbound on HPSG parsing with hard dependency constraints, we set the HPSG parser to disallow the application of any rules that result in the creation of dependencies that violate gold s</context>
<context position="21718" citStr="Ninomiya et al., 2006" startWordPosition="3474" endWordPosition="3477">ble 1: Summary of results on development data. * The shallow accuracy of combination C1 corresponds to the dependency precision (no dependencies were reported for 8% of all words in the development set). 4.4 Results Having determined α values on development data for the shift-reduce dependency parser, the twoparser agreement combination, and the three-parser maximum spanning tree combination, we parse the test data (section 23) using these three different sources of dependency constraints for HPSG parsing. Our final results are shown in table 2, where we also include the results published in (Ninomiya et al., 2006) for comparison purposes, and the result of using dependency constraints obtained with gold standard POS tags. By using two unlabeled dependency parsers to provide soft dependency constraints, we obtain a 1% absolute improvement in precision and recall of predicate-argument identification in HPSG parsing over a strong baseline. Our baseline approach outperformed previously published results on this test 4The accuracy figures for the dependency parsers is expressed as unlabeled accuracy of the surface dependencies only, and are not comparable to the HPSG parsing accuracy figures 90.8 90.6 90.4 </context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>T. Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, and J. Tsujii. 2006. Extremely lexicalized models for accurate and fast hpsg parsing. In Proceedings of the 2006 Conference on Empirical Methods for Natural Language Processing (EMNLP 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1049" citStr="Nivre and Scholz, 2004" startWordPosition="143" endWordPosition="146">tactic parsing to increase deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing. 1 Introduction Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et a</context>
<context position="3952" citStr="Nivre and Scholz, 2004" startWordPosition="579" endWordPosition="582">l Meeting of the Association of Computational Linguistics, pages 624–631, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 Fast dependency parsing and wide-coverage HPSG parsing 2.1 Data-driven dependency parsing Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemas explain general construction rules, and a large number of lexical entries express word-specific syntactic/semantic constraints. Figure 1 shows an example of the process of HPSG parsing. Fir</context>
<context position="10226" citStr="Nivre and Scholz, 2004" startWordPosition="1603" endWordPosition="1606">irst, the HPSG tree is converted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train a dependency parser as described in section 2.1. This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). Training data for the classifier is obtained by applying the parsing algorithm over the training sentences (for which the correct dependency structures are known) and recording the appropriate parser actions that result in the formation of the correct dependency trees, coupled with the features that represent the state of 626 the parser mentioned in section 2.1. An evaluation Soft dependency constraints can be implemented of the resulting dependency parser and its efficacy in as explained above as a straightforward extension of aiding HPSG parsing is presented in sect</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of the 20th International Conference on Computational Linguistics, pages 64–70. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryigit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Natural Language Learning.</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="4023" citStr="Nivre et al., 2006" startWordPosition="591" endWordPosition="594">rague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 Fast dependency parsing and wide-coverage HPSG parsing 2.1 Data-driven dependency parsing Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemas explain general construction rules, and a large number of lexical entries express word-specific syntactic/semantic constraints. Figure 1 shows an example of the process of HPSG parsing. First, lexical entries are assigned to each word in a sentence. In Figure </context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Natural Language Learning. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1726" citStr="Pollard and Sag, 1994" startWordPosition="248" endWordPosition="251">syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing. We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. We show that, by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techn</context>
<context position="4265" citStr="Pollard and Sag, 1994" startWordPosition="627" endWordPosition="630">ge HPSG parsing 2.1 Data-driven dependency parsing Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemas explain general construction rules, and a large number of lexical entries express word-specific syntactic/semantic constraints. Figure 1 shows an example of the process of HPSG parsing. First, lexical entries are assigned to each word in a sentence. In Figure 1, lexical entries express subcategorization frames and predicate argument structures. Parsing proceeds by applying schemas to lexical entries. In this example, the Head-Complement Schema is applied to the lexical entries of “tried” and “runn</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies.</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="10250" citStr="Sagae and Lavie, 2005" startWordPosition="1607" endWordPosition="1610">onverted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train a dependency parser as described in section 2.1. This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). Training data for the classifier is obtained by applying the parsing algorithm over the training sentences (for which the correct dependency structures are known) and recording the appropriate parser actions that result in the formation of the correct dependency trees, coupled with the features that represent the state of 626 the parser mentioned in section 2.1. An evaluation Soft dependency constraints can be implemented of the resulting dependency parser and its efficacy in as explained above as a straightforward extension of aiding HPSG parsing is presented in section 4. the parsing algor</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technologies. Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Meeting of the North American ACL.</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="19160" citStr="Sagae and Lavie, 2006" startWordPosition="3057" endWordPosition="3060">over 6% (or 12%, if we consider the upper-bound dictated by imperfect grammatical coverage), a more interesting aspect of this framework is that it allows techniques designed for improving dependency accuracy to improve HPSG parsing accuracy directly, as we illustrate next. 96 95 94 93 92 91 90 89 0 5 1 628 Figure 4: The effect of α on HPSG parsing constrained by the output of a dependency parser using gold standard POS tags. 4.3 Determining constraints with dependency parser combination Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). Using dependency constraints allows us to improve HPSG parsing accuracy simply by using an existing parser combination approach. As a first step, we train two additional parsers with the dependencies extracted from the HPSG Treebank. The first uses the same shiftreduce framework described in section 2.1, but it process the input from right to left (RL). This has been found to work well in previous work on dependency parser combination (Zeman and ˇZabokrtsk´y, 2005; Sagae and Lavie, 2006). The second parser is MSTParser, the large-margin maximum spanning tree parser described in (McDonald et </context>
<context position="20462" citStr="Sagae and Lavie (2006)" startWordPosition="3262" endWordPosition="3265">s, and one using three parsers. The first combination approach is to keep only dependencies for which there is agreement between the two parsers. In other words, dependencies that are proposed by one parser but not the other are simply discarded. Using the left-to-right shift-reduce parser and MSTParser, we find that this results in very high precision of surface dependencies on the development data. In the second approach, combination of 3Downloaded from http://sourceforge.net/projects/mstparser the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. For each of the combination approaches, we use the resulting dependencies as constraints for HPSG parsing, determining the optimal value of α on the development set in the same way as done for a single parser. Table 1 summarizes our experiments on development data using parser combinations to produce dependency constraints 4. The two combination approaches are denoted as C1 and C2. Parser Dep α HPSG Diff none (baseline) – – 86.5 – LR shift-reduce 91.2 1.5 87.1 0.6 RL shift-reduce 90.1 – – MSTParser 91.0 – – C1 (agreement) 96.8* 2.5 87.4</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of the 2006 Meeting of the North American ACL. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeille</author>
<author>Aravind Joshi</author>
</authors>
<title>Parsing strategies with lexicalized grammars: Application to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of 12th COLING.</booktitle>
<contexts>
<context position="1658" citStr="Schabes et al., 1988" startWordPosition="238" endWordPosition="241">Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing. We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. We show that, by using surface dependencies to constrain the application of wid</context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Yves Schabes, Anne Abeille, and Aravind Joshi. 1988. Parsing strategies with lexicalized grammars: Application to tree adjoining grammars. In Proceedings of 12th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1784" citStr="Steedman, 2000" startWordPosition="258" endWordPosition="259"> relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing. We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. We show that, by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for high-accuracy dependency parsing, while</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Zdenek ˇZabokrtsk´y</author>
</authors>
<title>Improving parsing accuracy by combining diverse dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<location>Vancouver, Canada.</location>
<marker>Zeman, ˇZabokrtsk´y, 2005</marker>
<rawString>Daniel Zeman and Zdenek ˇZabokrtsk´y. 2005. Improving parsing accuracy by combining diverse dependency parsers. In Proceedings of the International Workshop on Parsing Technologies. Vancouver, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>