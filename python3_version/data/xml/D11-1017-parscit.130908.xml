<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002397">
<title confidence="0.994847">
Training a Parser for Machine Translation Reordering
</title>
<author confidence="0.943051">
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
</author>
<affiliation confidence="0.848961">
Google
</affiliation>
<email confidence="0.998109">
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999323615384615">
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998959612244898">
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al., 1993; Collins,
1997; Charniak, 2000; McDonald et al., 2005;
Petrov et al., 2006; Nivre, 2008). A common—
and valid—criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al., 2004; Xu et al., 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
</bodyText>
<footnote confidence="0.987578">
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
</footnote>
<page confidence="0.958606">
183
</page>
<note confidence="0.9580235">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988995945946">
which we refer to as targeted self-training (Sec- Algorithm 1 Select parse that maximizes an extrin-
tion 2). Similar to self-training, a baseline model sic metric.
is used to produce predictions on an unlabeled data Input: baseline parser B
set. However, rather than directly training on the Input: sentence 5
output of the baseline model, we generate a list of Input: function COMPUTEEXTRINSIC(parse P)
hypotheses and use an external signal to select the Output: a parse for the input sentence
best candidate. The selected parse trees are added Pn = {Pi, ... , Pn} ← n-best parses of 5 by B
to the training data and the model is then retrained. maxScore = 0
The experiments in Section 5 show that this simple bestParse = ∅
procedure noticeably improves our parsers for the for k = 1 to n do
task at hand, resulting in significant improvements extrinsicScore = COMPUTEEXTRINSIC(Pk)
in downstream translation quality, as measured in a if extrinsicScore &gt; maxScore then
human evaluation on web text. maxScore = extrinsicScore
This idea is similar in vein to McClosky. et al. bestParse = Pk
(2006) and Petrov et al. (2010), except that we use an end if
extrinsic quality metric instead of a second parsing end for
model for making the selection. It is also similar to return bestParse
Burkett and Klein (2008) and Burkett et al. (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
184
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al. (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system’s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
</bodyText>
<sectionHeader confidence="0.996341" genericHeader="method">
3 The MT Reordering Task
</sectionHeader>
<bodyText confidence="0.999336181818182">
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
</bodyText>
<subsectionHeader confidence="0.9705465">
3.1 Reordering as a separately evaluable
component
</subsectionHeader>
<bodyText confidence="0.993292836734694">
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al., 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al. (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR’s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
|C |− 1
|e |− 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1−(3−1)/(5−1) = 0.5.
Talbot et al. (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
</bodyText>
<subsectionHeader confidence="0.9048385">
3.2 Reordering quality as predictor of parse
quality
</subsectionHeader>
<bodyText confidence="0.999286130434783">
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb “wear” and ob-
ject “sunscreen”) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al.
(2010b), for instance, were able to get impressive
English→Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
</bodyText>
<equation confidence="0.507762">
p(esys, eref) = 1 −
</equation>
<page confidence="0.996342">
185
</page>
<sectionHeader confidence="0.99811" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.995759">
4.1 Treebank data
</subsectionHeader>
<bodyText confidence="0.999992">
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al., 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token’s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al., 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al., 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
</bodyText>
<subsectionHeader confidence="0.998985">
4.2 Reference reordering data
</subsectionHeader>
<bodyText confidence="0.999334272727273">
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al. (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
</bodyText>
<figureCaption confidence="0.599869285714286">
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies “Wear” as
the main verb and moves it to the end of the sentence; the
bad parse analyses “Wear sunscreen” as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
</figureCaption>
<subsectionHeader confidence="0.998275">
4.3 Parsers
</subsectionHeader>
<bodyText confidence="0.998304">
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al. (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
</bodyText>
<table confidence="0.7637272">
Reordered:
15 or greater of an SPF has that
Wear sunscreen
Wear
Reordered:
15 or greater of an SPF has that sunscreen
Reordering score: 1.0 (matches reference)
Good parse
Bad parse
Reordering score: 0.78 (“Wear” is out of place)
</table>
<page confidence="0.99378">
186
</page>
<bodyText confidence="0.999943727272727">
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
</bodyText>
<subsectionHeader confidence="0.998572">
4.4 Reordering rules
</subsectionHeader>
<bodyText confidence="0.9999241">
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al. (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
</bodyText>
<subsectionHeader confidence="0.989715">
4.5 MT system
</subsectionHeader>
<bodyText confidence="0.997280464285714">
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
</bodyText>
<sectionHeader confidence="0.997203" genericHeader="method">
5 Experiments Reordering Web Text
</sectionHeader>
<bodyText confidence="0.9878475">
We experimented with parsers trained in three dif-
ferent ways:
</bodyText>
<listItem confidence="0.976387888888889">
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser’s 512-best list that
when reordered gives the highest reordering
score.3
</listItem>
<subsectionHeader confidence="0.6176325">
5.1 Standard self-training vs targeted
self-training
</subsectionHeader>
<bodyText confidence="0.997361375">
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
</bodyText>
<footnote confidence="0.993500285714286">
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
</footnote>
<page confidence="0.98869">
187
</page>
<table confidence="0.999897">
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
</table>
<tableCaption confidence="0.99463225">
Table 1: English→Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label “10x” indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
</tableCaption>
<table confidence="0.9985958">
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
</table>
<tableCaption confidence="0.940836666666667">
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. “WSJ-only” corresponds to the baseline WSJ-only shift-reduce parser; “Targeted” corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
</tableCaption>
<bodyText confidence="0.999662142857143">
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
“amod” and “advmod” are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of “red hot car” would be reordered
the same, that is, not at all.
</bodyText>
<subsectionHeader confidence="0.995872">
5.2 Translation quality improvement
</subsectionHeader>
<bodyText confidence="0.96897016">
To put the improvement of the MT system in terms
of BLEU score (Papineni et al., 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=“Perfect”, 4=“Most Meaning/Grammar”,
2=“Some Meaning/Grammar”, 0=“Nonsense”. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
</bodyText>
<subsectionHeader confidence="0.995652">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999854">
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
</bodyText>
<page confidence="0.995522">
188
</page>
<table confidence="0.998088">
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
</table>
<tableCaption confidence="0.999942">
Table 3: Counts on Web-Test of “click” tagged as a noun and verb and percentage of sentences parsed imperatively.
</tableCaption>
<bodyText confidence="0.999358">
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word “click” and imperative
sentences.
</bodyText>
<subsectionHeader confidence="0.569966">
5.3.1 Click
</subsectionHeader>
<bodyText confidence="0.999973285714286">
The word “click” appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to “click”. The WSJ-only parsers tag “click”
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag “click” as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag “click” more often as a verb.
It turns out that the shift-reduce parser’s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser’s n-best lists to never hypothesize “click”
as a verb, and self-training doesn’t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser’s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al. (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
</bodyText>
<subsectionHeader confidence="0.830239">
5.3.2 Imperatives
</subsectionHeader>
<bodyText confidence="0.99998195">
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn’t
catch on to the imperativeness and gets a low re-
ordering score.
</bodyText>
<sectionHeader confidence="0.937937" genericHeader="method">
6 Targeted Self-Training vs Training on
</sectionHeader>
<subsectionHeader confidence="0.685964">
Treebanks for Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999811714285714">
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
</bodyText>
<footnote confidence="0.971685666666667">
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
</footnote>
<page confidence="0.997006">
189
</page>
<bodyText confidence="0.984653921568628">
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al., 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English→Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to “some mean-
ing/grammar” and “4” corresponds to “most mean-
ing/grammar”.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78→69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
</bodyText>
<sectionHeader confidence="0.999715" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999991756097561">
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al.,
2006; Petrov et al., 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al. (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al. (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn’t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al., 2007; Chang et al., 2010; Clarke et al.,
2010; Ganchev et al., 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al. (2006), who train a per-
</bodyText>
<page confidence="0.98794">
190
</page>
<table confidence="0.9999217">
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
</table>
<tableCaption confidence="0.984819">
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
</tableCaption>
<table confidence="0.995715333333333">
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
</table>
<tableCaption confidence="0.987755666666667">
Table 5: BLEU scores and human evaluation results for English→Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
</tableCaption>
<bodyText confidence="0.9999627">
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al. (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al. (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999853588235294">
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system’s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9748882">
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT – a statistical part-of-speech tagger.
In ANLP ’00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ’08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ’10.
</reference>
<page confidence="0.979055">
191
</page>
<reference confidence="0.999794150000001">
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
’07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ’10.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In NAACL ’00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world’s response.
In CoNLL ’10.
M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause re-
structuring for statistical machine translation. In ACL
’05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ’97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ’06.
J. Foster. 2010. “cba to check the spelling”: Investigat-
ing parser performance on discussion forum posts. In
NAACL ’10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What’s in a translation rule? In HLT-NAACL ’04.
K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ’10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ’01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ’07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ’11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ’10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ’06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ’06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ’06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
’05.
Y. Miyao, R. Sætre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ’08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ’03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ’02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ’07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ’06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ’10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ’07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ’04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ’09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ’01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ’08.
</reference>
<page confidence="0.998183">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.650389">
<title confidence="0.999951">Training a Parser for Machine Translation Reordering</title>
<author confidence="0.982999">Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa</author>
<email confidence="0.676768">Google</email>
<abstract confidence="0.999381357142857">We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>M Osborne</author>
</authors>
<title>LRscore for evaluating lexical and reordering quality in MT.</title>
<date>2010</date>
<booktitle>In ACL-2010 WMT.</booktitle>
<contexts>
<context position="8126" citStr="Birch and Osborne, 2010" startWordPosition="1313" endWordPosition="1316">f the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually writte</context>
</contexts>
<marker>Birch, Osborne, 2010</marker>
<rawString>A. Birch and M. Osborne. 2010. LRscore for evaluating lexical and reordering quality in MT. In ACL-2010 WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In ANLP ’00.</booktitle>
<contexts>
<context position="15622" citStr="Brants, 2000" startWordPosition="2560" endWordPosition="2561">trov et al. (2006). We convert the constituency trees output by the BerkeleyParser to labeled dependency trees using the same procedure that is applied to the treebanks. While the BerkeleyParser views part-of-speech (POS) tagging as an integral part of parsing, our dependency parser requires the input to be tagged Reordered: 15 or greater of an SPF has that Wear sunscreen Wear Reordered: 15 or greater of an SPF has that sunscreen Reordering score: 1.0 (matches reference) Good parse Bad parse Reordering score: 0.78 (“Wear” is out of place) 186 with a separate POS tagger. We use the TnT tagger (Brants, 2000) in our experiments, because of its efficiency and ease of use. Tagger and parser are always trained on the same data. For all parsers, we lowercase the input at train and test time. We found that this improves performance in parsing web text. In addition to general uppercase/lowercase noisiness of the web text negatively impacting scores, we found that the baseline casesensitive parsers are especially bad at parsing imperative sentences, as discussed in Section 5.3.2. 4.4 Reordering rules In this paper we focus on English to Japanese, Korean, and Turkish translation. We use a superset of the </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT – a statistical part-of-speech tagger. In ANLP ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>D Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In EMNLP ’08.</booktitle>
<contexts>
<context position="4879" citStr="Burkett and Klein (2008)" startWordPosition="780" endWordPosition="783">ion 5 show that this simple bestParse = ∅ procedure noticeably improves our parsers for the for k = 1 to n do task at hand, resulting in significant improvements extrinsicScore = COMPUTEEXTRINSIC(Pk) in downstream translation quality, as measured in a if extrinsicScore &gt; maxScore then human evaluation on web text. maxScore = extrinsicScore This idea is similar in vein to McClosky. et al. bestParse = Pk (2006) and Petrov et al. (2010), except that we use an end if extrinsic quality metric instead of a second parsing end for model for making the selection. It is also similar to return bestParse Burkett and Klein (2008) and Burkett et al. (2010), but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection. It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings. Details of the reordering metric and the annotated data we used are given in Sections 3 and 4. While this annotation requires some effort, such annotations are much easier to obtain than full parse trees. In our experiments in Section 6 we show that we can obtain similar improvements on downstream translation quality </context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>D. Burkett and D. Klein. 2008. Two languages are better than one (for syntactic parsing). In EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>S Petrov</author>
<author>J Blitzer</author>
<author>D Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<journal>In CoNLL</journal>
<volume>10</volume>
<contexts>
<context position="4905" citStr="Burkett et al. (2010)" startWordPosition="785" endWordPosition="788">estParse = ∅ procedure noticeably improves our parsers for the for k = 1 to n do task at hand, resulting in significant improvements extrinsicScore = COMPUTEEXTRINSIC(Pk) in downstream translation quality, as measured in a if extrinsicScore &gt; maxScore then human evaluation on web text. maxScore = extrinsicScore This idea is similar in vein to McClosky. et al. bestParse = Pk (2006) and Petrov et al. (2010), except that we use an end if extrinsic quality metric instead of a second parsing end for model for making the selection. It is also similar to return bestParse Burkett and Klein (2008) and Burkett et al. (2010), but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection. It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings. Details of the reordering metric and the annotated data we used are given in Sections 3 and 4. While this annotation requires some effort, such annotations are much easier to obtain than full parse trees. In our experiments in Section 6 we show that we can obtain similar improvements on downstream translation quality by targeted self-training </context>
<context position="29605" citStr="Burkett et al. (2010)" startWordPosition="4757" endWordPosition="4760">e QuestionBank. 7 Related Work Our approach to training parsers for reordering is closely related to self/up-training (McClosky. et al., 2006; Petrov et al., 2010). However, unlike uptraining, our method does not use only the 1-best output of the first-stage parser, but has access to the n-best list. This makes it similar to the work of McClosky. et al. (2006), except that we use an extrinsic metric (MT reordering score) to select a high quality parse tree, rather than a second, reranking model that has access to additional features. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt</context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010. Learning better monolingual models with unannotated bilingual text. In CoNLL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Guiding semisupervision with constraint-driven learning.</title>
<date>2007</date>
<booktitle>In ACL ’07.</booktitle>
<contexts>
<context position="30118" citStr="Chang et al., 2007" startWordPosition="4843" endWordPosition="4846">to additional features. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build be</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semisupervision with constraint-driven learning. In ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>D Roth</author>
<author>V Srikumar</author>
</authors>
<title>Structured output learning with indirect supervision.</title>
<date>2010</date>
<booktitle>In ICML ’10.</booktitle>
<contexts>
<context position="30138" citStr="Chang et al., 2010" startWordPosition="4847" endWordPosition="4850">es. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build better structured pred</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>M. Chang, D. Goldwasser, D. Roth, and V. Srikumar. 2010. Structured output learning with indirect supervision. In ICML ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum–entropy–inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL ’00.</booktitle>
<contexts>
<context position="1005" citStr="Charniak, 2000" startWordPosition="143" endWordPosition="144">ically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum–entropy–inspired parser. In NAACL ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<journal>In CoNLL</journal>
<volume>10</volume>
<contexts>
<context position="30159" citStr="Clarke et al., 2010" startWordPosition="4851" endWordPosition="4854">aining is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build better structured predictors. In our case t</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In CoNLL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In ACL ’97.</booktitle>
<contexts>
<context position="989" citStr="Collins, 1997" startWordPosition="141" endWordPosition="142"> way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In ACL ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC ’06.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.-C. de Marneffe, B. MacCartney, and C. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
</authors>
<title>cba to check the spelling”: Investigating parser performance on discussion forum posts.</title>
<date>2010</date>
<booktitle>In NAACL ’10.</booktitle>
<contexts>
<context position="1472" citStr="Foster, 2010" startWordPosition="221" endWordPosition="222">ng has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight</context>
</contexts>
<marker>Foster, 2010</marker>
<rawString>J. Foster. 2010. “cba to check the spelling”: Investigating parser performance on discussion forum posts. In NAACL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>HLT-NAACL ’04.</booktitle>
<contexts>
<context position="2099" citStr="Galley et al., 2004" startWordPosition="324" endWordPosition="327">is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reord</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a translation rule? In HLT-NAACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research.</journal>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Genzel</author>
</authors>
<title>Automatically learning source-side reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In COLING ’10.</booktitle>
<contexts>
<context position="8855" citStr="Genzel, 2010" startWordPosition="1431" endWordPosition="1432">ne way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans </context>
<context position="16751" citStr="Genzel (2010)" startWordPosition="2747" endWordPosition="2749">s on English to Japanese, Korean, and Turkish translation. We use a superset of the reordering rules proposed by Xu et al. (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. The rules define a precedence order for the dependents of each part of speech. For example, a slightly simplified version of the precedence order of child labels for a verbal head HEADVERB is: advcl, nsubj, prep, [other children], dobj, prt, aux, neg, HEADVERB, mark, ref, compl. Alternatively, we could have used an automatic reordering-rule learning framework like that of Genzel (2010). Because the reordering accuracy metric can be computed for any source/target language pair, this would have made our approach language completely independent and applicable to any language pair. We chose to use manually written rules to eliminate the variance induced by the automatic reordering-rule learning framework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we u</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>D. Genzel. 2010. Automatically learning source-side reordering rules for large scale machine translation. In COLING ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In EMNLP ’01.</booktitle>
<contexts>
<context position="1436" citStr="Gildea, 2001" startWordPosition="216" endWordPosition="217">duction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in ma</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Corpus variation and parser performance. In EMNLP ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>Syntactic preprocessing for statistical machine translation.</title>
<date>2007</date>
<booktitle>In MTS ’07.</booktitle>
<contexts>
<context position="8840" citStr="Habash, 2007" startWordPosition="1429" endWordPosition="1430">le component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of co</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>N. Habash. 2007. Syntactic preprocessing for statistical machine translation. In MTS ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>R McDonald</author>
<author>J Katz-Brown</author>
<author>M Ringgaard</author>
</authors>
<title>Training dependency parsers by jointly optimizing multiple objectives.</title>
<date>2011</date>
<booktitle>In EMNLP ’11.</booktitle>
<contexts>
<context position="6971" citStr="Hall et al. (2011)" startWordPosition="1129" endWordPosition="1132">e parse ranked most highly in the n-best list. The motivation of this selection step is that good performance on the downstream external task, measured by the extrinsic metric, should be predictive of an intrinsically good parse. At the very least, even if the selected parse is not syntactically correct, or even if it goes against the original treebanking guidelines, it results in a higher extrinsic score and should therefore be preferred. One could imagine extending this framework by repeatedly running self-training on successively improving parsers in an EM-style algorithm. A recent work by Hall et al. (2011) on training a parser with multiple objective functions investigates a similar idea in the context of online learning. In this paper we focus our attention on machine translation as the final application, but one could envision applying our techniques to other applications such as information extraction or question answering. In particular, we explore one application of targeted self-training, where computing the extrinsic metric involves plugging the parse into an MT system’s reordering component and computing the accuracy of the reordering compared to a reference word order. We now direct ou</context>
<context position="32263" citStr="Hall et al. (2011)" startWordPosition="5192" endWordPosition="5195">ange 0 to 6) WSJ-only Targeted WSJ-only Targeted Sig. difference? Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level) Table 5: BLEU scores and human evaluation results for English→Japanese translation of the QTB-Test corpus, varying only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training 10x shift-reduce parser. ceptron model for an end-to-end MT system where the alignment parameters are updated based on selecting an alignment from a n-best list that leads to highest BLEU score. As mentioned earlier, this also makes our work similar to Hall et al. (2011) who train a perceptron algorithm on multiple objective functions with the goal of producing parsers that are optimized for extrinsic metrics. It has previously been observed that parsers often perform differently for downstream applications. Miyao et al. (2008) compared parser quality in the biomedical domain using a protein-protein interaction (PPI) identification accuracy metric. This allowed them to compare the utility of extant dependency parsers, phrase structure parsers, and deep structure parsers for the PPI identification task. One could apply the targeted self-training technique we d</context>
</contexts>
<marker>Hall, McDonald, Katz-Brown, Ringgaard, 2011</marker>
<rawString>K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard. 2011. Training dependency parsers by jointly optimizing multiple objectives. In EMNLP ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>T Hirao</author>
<author>K Duh</author>
<author>K Sudoh</author>
<author>H Tsukada</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In EMNLP ’10.</booktitle>
<contexts>
<context position="8099" citStr="Isozaki et al., 2010" startWordPosition="1309" endWordPosition="1312">omputing the accuracy of the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transforma</context>
<context position="11449" citStr="Isozaki et al. (2010" startWordPosition="1875" endWordPosition="1878"> reordering score. Could we flip this causality around, and perhaps try to identify a good parse tree based on its reordering score? With the experiments in this paper, we show that indeed a high reordering score is predictive of the underlying parse tree that was used to generate the reordering being a good parse (or, at least, being good enough for our purpose). In the case of translating English to Japanese or another SOV language, there is a large amount of reordering required, but with a relatively small number of reordering rules one can cover a large proportion of reordering phenomena. Isozaki et al. (2010b), for instance, were able to get impressive English→Japanese results with only a single reordering rule, given a suitable definition of a head. Hence, the reordering task depends crucially on a correct syntactic analysis and is extremely sensitive to parser errors. p(esys, eref) = 1 − 185 4 Experimental Setup 4.1 Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In par</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs. In EMNLP ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>K Sudoh</author>
<author>H Tsukada</author>
<author>K Duh</author>
</authors>
<title>Head finalization: A simple reordering rule for SOV languages. In</title>
<date>2010</date>
<booktitle>ACL-2010 WMT.</booktitle>
<contexts>
<context position="8099" citStr="Isozaki et al., 2010" startWordPosition="1309" endWordPosition="1312">omputing the accuracy of the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transforma</context>
<context position="11449" citStr="Isozaki et al. (2010" startWordPosition="1875" endWordPosition="1878"> reordering score. Could we flip this causality around, and perhaps try to identify a good parse tree based on its reordering score? With the experiments in this paper, we show that indeed a high reordering score is predictive of the underlying parse tree that was used to generate the reordering being a good parse (or, at least, being good enough for our purpose). In the case of translating English to Japanese or another SOV language, there is a large amount of reordering required, but with a relatively small number of reordering rules one can cover a large proportion of reordering phenomena. Isozaki et al. (2010b), for instance, were able to get impressive English→Japanese results with only a single reordering rule, given a suitable definition of a head. Hence, the reordering task depends crucially on a correct syntactic analysis and is extremely sensitive to parser errors. p(esys, eref) = 1 − 185 4 Experimental Setup 4.1 Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In par</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b. Head finalization: A simple reordering rule for SOV languages. In ACL-2010 WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Judge</author>
<author>A Cahill</author>
<author>J v Genabith</author>
</authors>
<title>QuestionBank: creating a corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>In ACL ’06.</booktitle>
<contexts>
<context position="12299" citStr="Judge et al., 2006" startWordPosition="2009" endWordPosition="2012">xtremely sensitive to parser errors. p(esys, eref) = 1 − 185 4 Experimental Setup 4.1 Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In particular, we split tokens containing hyphens into multiple tokens and, somewhat simplistically, gave the original token’s part-of-speech tag to all newly created tokens. In Section 6 we make also use of the Question Treebank (QTB) (Judge et al., 2006), as a source of syntactically annotated out-of-domain data. Though we experiment with both dependency parsers and phrase structure parsers, our MT system assumes dependency parses as input. We use the Stanford converter (de Marneffe et al., 2006) to convert phrase structure parse trees to dependency parse trees (for both treebank trees and predicted trees). 4.2 Reference reordering data We aim to build an MT system that can accurately translate typical English text that one finds on the Internet to SOV langauges. To this end, we randomly sampled 13595 English sentences from the web and create</context>
<context position="26884" citStr="Judge et al., 2006" startWordPosition="4331" endWordPosition="4334"> making new treebanks. For example, if we want to build a parser that can reorder question sentences better than our baseline WSJonly parser, we have these two options: 1. Manually construct PTB-style trees for 2000 5As an approximation, we count every parse that begins with a root verb as an imperative. 189 questions and train on the resulting treebank. 2. Create reference reorderings for 2000 questions and then do targeted self-training. To compare these approaches, we created reference reordering data for our train (2000 sentences) and test (1000 sentences) splits of the Question Treebank (Judge et al., 2006). Table 4 shows that both ways of training on QTB-Train sentences give similarly large improvements in reordering score on QTB-Test. Table 5 confirms that this corresponds to very large increases in English→Japanese BLEU score and subjective translation quality. In the human side-by-side comparison, the baseline translations achieved an average score of 2.12, while the targeted self-training translations received a score of 2.94, where a score of 2 corresponds to “some meaning/grammar” and “4” corresponds to “most meaning/grammar”. But which of the two approaches is better? In the shift-reduce</context>
</contexts>
<marker>Judge, Cahill, Genabith, 2006</marker>
<rawString>J. Judge, A. Cahill, and J. v. Genabith. 2006. QuestionBank: creating a corpus of parse-annotated questions. In ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>M Denkowski</author>
</authors>
<title>The Meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="8077" citStr="Lavie and Denkowski, 2009" startWordPosition="1305" endWordPosition="1308"> reordering component and computing the accuracy of the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rul</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>A. Lavie and M. Denkowski. 2009. The Meteor metric for automatic evaluation of machine translation. Machine Translation, 23(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL ’06.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="974" citStr="Marcus et al., 1993" startWordPosition="137" endWordPosition="140">us of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to t</context>
<context position="11911" citStr="Marcus et al., 1993" startWordPosition="1950" endWordPosition="1953"> of reordering required, but with a relatively small number of reordering rules one can cover a large proportion of reordering phenomena. Isozaki et al. (2010b), for instance, were able to get impressive English→Japanese results with only a single reordering rule, given a suitable definition of a head. Hence, the reordering task depends crucially on a correct syntactic analysis and is extremely sensitive to parser errors. p(esys, eref) = 1 − 185 4 Experimental Setup 4.1 Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In particular, we split tokens containing hyphens into multiple tokens and, somewhat simplistically, gave the original token’s part-of-speech tag to all newly created tokens. In Section 6 we make also use of the Question Treebank (QTB) (Judge et al., 2006), as a source of syntactically annotated out-of-domain data. Though we experiment with both dependency parsers and phrase structure parsers, our MT system assumes dependency parses as input. We use the Stanford c</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In NAACL ’06.</booktitle>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky., E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In NAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="1028" citStr="McDonald et al., 2005" startWordPosition="145" endWordPosition="148">the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>R Sætre</author>
<author>K Sagae</author>
<author>T Matsuzaki</author>
<author>J Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In ACL ’08.</booktitle>
<contexts>
<context position="32525" citStr="Miyao et al. (2008)" startWordPosition="5231" endWordPosition="5234">ems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training 10x shift-reduce parser. ceptron model for an end-to-end MT system where the alignment parameters are updated based on selecting an alignment from a n-best list that leads to highest BLEU score. As mentioned earlier, this also makes our work similar to Hall et al. (2011) who train a perceptron algorithm on multiple objective functions with the goal of producing parsers that are optimized for extrinsic metrics. It has previously been observed that parsers often perform differently for downstream applications. Miyao et al. (2008) compared parser quality in the biomedical domain using a protein-protein interaction (PPI) identification accuracy metric. This allowed them to compare the utility of extant dependency parsers, phrase structure parsers, and deep structure parsers for the PPI identification task. One could apply the targeted self-training technique we describe to optimize any of these parsers for the PPI task, similar to how we have optimized our parser for the MT reordering task. 8 Conclusion We introduced a variant of self-training that targets parser training towards an extrinsic evaluation metric. We use t</context>
</contexts>
<marker>Miyao, Sætre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Y. Miyao, R. Sætre, K. Sagae, T. Matsuzaki, and J. Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1063" citStr="Nivre, 2008" startWordPosition="153" endWordPosition="154">We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone t</context>
<context position="14280" citStr="Nivre, 2008" startWordPosition="2326" endWordPosition="2327">ings because we had access to bilingual annotators fluent in English and Japanese. Figure 1: Examples of good and bad parses and corresponding reorderings for translation from English to Japanese. The good parse correctly identifies “Wear” as the main verb and moves it to the end of the sentence; the bad parse analyses “Wear sunscreen” as a noun phrase and does not reorder it. This example was one of the wins in the human evaluation of Section 5.2. 4.3 Parsers The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). The parser uses the following features: word identity of the first two words on the buffer, the top word on the stack and the head of the top word on the stack (if available); part-ofspeech identities of the first four words on the buffer and top two words on the stack; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the leftmost modifier of the first word in the buffer. We also inc</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>J. Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03.</booktitle>
<contexts>
<context position="17676" citStr="Och, 2003" startWordPosition="2889" endWordPosition="2890">mework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given language pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of parallel documents. They come from various sources with a substantial portion coming from the web after using simple heuristics to identify p</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02.</booktitle>
<contexts>
<context position="21674" citStr="Papineni et al., 2002" startWordPosition="3504" endWordPosition="3507">th metrics.4 One explanation for the drops in LAS is that some parts of the parse tree are important for downstream reordering quality while others are not (or only to a lesser extent). Some distinctions between labels become less important; for example, arcs labeled “amod” and “advmod” are transformed identically by the reordering rules. Some semantic distinctions also become less important; for example, any sane interpretation of “red hot car” would be reordered the same, that is, not at all. 5.2 Translation quality improvement To put the improvement of the MT system in terms of BLEU score (Papineni et al., 2002), a widely used metric for automatic MT evaluation, we took 5000 sentences from Web-Test and had humans generate reference translations into Japanese, Korean, and 4We did not attempt this experiment for the BerkeleyParser since training was too slow. Turkish. We then trained MT systems varying only the parser used for reordering in training and decoding. Table 2 shows that targeted self-training data increases BLEU score for translation into all three languages. In addition to BLEU increase, a side-by-side human evaluation on 500 sentences (sampled from the 5000 used to compute BLEU scores) sh</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In NAACL ’07.</booktitle>
<contexts>
<context position="1606" citStr="Petrov and Klein, 2007" startWordPosition="240" endWordPosition="243">, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstrea</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In NAACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL ’06.</booktitle>
<contexts>
<context position="1049" citStr="Petrov et al., 2006" startWordPosition="149" endWordPosition="152">f a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a</context>
<context position="15027" citStr="Petrov et al. (2006)" startWordPosition="2459" endWordPosition="2462">2008). The parser uses the following features: word identity of the first two words on the buffer, the top word on the stack and the head of the top word on the stack (if available); part-ofspeech identities of the first four words on the buffer and top two words on the stack; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the leftmost modifier of the first word in the buffer. We also include conjunctions over all nonlexical features. We also give results for the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al. (2006). We convert the constituency trees output by the BerkeleyParser to labeled dependency trees using the same procedure that is applied to the treebanks. While the BerkeleyParser views part-of-speech (POS) tagging as an integral part of parsing, our dependency parser requires the input to be tagged Reordered: 15 or greater of an SPF has that Wear sunscreen Wear Reordered: 15 or greater of an SPF has that sunscreen Reordering score: 1.0 (matches reference) Good parse Bad parse Reordering score: 0.78 (“Wear” is out of place) 186 with a separate POS tagger. We use the TnT tagger (Brants, 2000) in o</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>P Chang</author>
<author>M Ringgaard H Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In EMNLP ’10.</booktitle>
<contexts>
<context position="4692" citStr="Petrov et al. (2010)" startWordPosition="747" endWordPosition="750"> candidate. The selected parse trees are added Pn = {Pi, ... , Pn} ← n-best parses of 5 by B to the training data and the model is then retrained. maxScore = 0 The experiments in Section 5 show that this simple bestParse = ∅ procedure noticeably improves our parsers for the for k = 1 to n do task at hand, resulting in significant improvements extrinsicScore = COMPUTEEXTRINSIC(Pk) in downstream translation quality, as measured in a if extrinsicScore &gt; maxScore then human evaluation on web text. maxScore = extrinsicScore This idea is similar in vein to McClosky. et al. bestParse = Pk (2006) and Petrov et al. (2010), except that we use an end if extrinsic quality metric instead of a second parsing end for model for making the selection. It is also similar to return bestParse Burkett and Klein (2008) and Burkett et al. (2010), but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection. It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings. Details of the reordering metric and the annotated data we used are given in Sections 3 and 4. While this annotation requires some e</context>
<context position="24707" citStr="Petrov et al. (2010)" startWordPosition="3983" endWordPosition="3986">b. It turns out that the shift-reduce parser’s stubbornness is not due to a fundamental problem of the parser, but due to an artifact in TnT. To increase speed, TnT restricts the choices of tags for known words to previously-seen tags. This causes the parser’s n-best lists to never hypothesize “click” as a verb, and self-training doesn’t click no matter how targeted it is. This shows that the targeted selftraining approach heavily relies on the diversity of the baseline parser’s n-best lists. It should be noted here that it would be easy to combine our approach with the uptraining approach of Petrov et al. (2010). The idea would be to use the BerkeleyParser to generate the n-best lists; perhaps we could call this targeted uptraining. This way, the shift-reduce parser could benefit both from the generally higher quality of the parse trees produced by the BerkeleyParser, as well as from the information provided by the extrinsic scoring function. 5.3.2 Imperatives As Table 3 shows, the WSJ training set contains only 0.7% imperative sentences.5 In contrast, our test sentences from the web contain approximately 10% imperatives. As a result, parsers trained exclusively on the WSJ underproduce imperative par</context>
<context position="29147" citStr="Petrov et al., 2010" startWordPosition="4678" endWordPosition="4681"> towards producing pathological trees with little intrinsic meaning. To explore this possibility, we computed reordering scores after reordering the QTB-Test treebank trees directly. Table 4 shows that this gives reordering scores similar to those of our best parsers. Therefore it is at least possible that the targeted self-training process could have resulted in a parser that achieves high reordering score by producing parses that look like those in the QuestionBank. 7 Related Work Our approach to training parsers for reordering is closely related to self/up-training (McClosky. et al., 2006; Petrov et al., 2010). However, unlike uptraining, our method does not use only the 1-best output of the first-stage parser, but has access to the n-best list. This makes it similar to the work of McClosky. et al. (2006), except that we use an extrinsic metric (MT reordering score) to select a high quality parse tree, rather than a second, reranking model that has access to additional features. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to in</context>
</contexts>
<marker>Petrov, Chang, Alshawi, 2010</marker>
<rawString>S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010. Uptraining for accurate deterministic question parsing. In EMNLP ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talbot</author>
<author>H Kazawa</author>
<author>H Ichikawa</author>
<author>J Katz-Brown</author>
<author>M Seno</author>
<author>F Och</author>
</authors>
<title>A lightweight evaluation framework for machine translation reordering.</title>
<date>2011</date>
<booktitle>In EMNLP-2011 WMT.</booktitle>
<contexts>
<context position="9044" citStr="Talbot et al. (2011)" startWordPosition="1457" endWordPosition="1460">s into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans whose content appears contiguously in the same order in the reference. The reordering score is then computed as |C |− 1 |e |− 1 . This metric assigns a score between 0 and 1 where 1 indicat</context>
<context position="13152" citStr="Talbot et al. (2011)" startWordPosition="2143" endWordPosition="2146"> et al., 2006) to convert phrase structure parse trees to dependency parse trees (for both treebank trees and predicted trees). 4.2 Reference reordering data We aim to build an MT system that can accurately translate typical English text that one finds on the Internet to SOV langauges. To this end, we randomly sampled 13595 English sentences from the web and created Japanese-word-order reference reorderings for them. We split the sentences arbitrarily into a 6268-sentence Web-Train corpus and a 7327- sentence Web-Test corpus. To make the reference alignments we used the technique suggested by Talbot et al. (2011): ask annotators to translate each English sentence to Japanese extremely literally and annotate which English words align to which Japanese words. Golden reference reorderings can be made programmatically from these annotations. Creating a large set of reference reorderings is straightforward because annotators need little special background or training, as long as they can speak both the source and target languages. We chose Japanese as the target language through which to create the English reference reorderings because we had access to bilingual annotators fluent in English and Japanese. F</context>
</contexts>
<marker>Talbot, Kazawa, Ichikawa, Katz-Brown, Seno, Och, 2011</marker>
<rawString>D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown, M. Seno, and F. Och. 2011. A lightweight evaluation framework for machine translation reordering. In EMNLP-2011 WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP ’07.</booktitle>
<contexts>
<context position="8761" citStr="Wang, 2007" startWordPosition="1415" endWordPosition="1416">mportance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the </context>
</contexts>
<marker>Wang, 2007</marker>
<rawString>C. Wang. 2007. Chinese syntactic reordering for statistical machine translation. In EMNLP ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<journal>In Coling</journal>
<volume>04</volume>
<contexts>
<context position="8826" citStr="Xia and McCord, 2004" startWordPosition="1425" endWordPosition="1428">s a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>F. Xia and M. McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Coling ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Xu</author>
<author>J Kang</author>
<author>M Ringgaard</author>
<author>F Och</author>
</authors>
<title>Using a dependency parser to improve SMT for subject-objectverb languages. In</title>
<date>2009</date>
<booktitle>NAACL-HLT ’09.</booktitle>
<contexts>
<context position="2117" citStr="Xu et al., 2009" startWordPosition="328" endWordPosition="331"> after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reordering accuracy, BL</context>
<context position="8779" citStr="Xu et al., 2009" startWordPosition="1417" endWordPosition="1420"> getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reorderi</context>
<context position="16267" citStr="Xu et al. (2009)" startWordPosition="2667" endWordPosition="2670">of its efficiency and ease of use. Tagger and parser are always trained on the same data. For all parsers, we lowercase the input at train and test time. We found that this improves performance in parsing web text. In addition to general uppercase/lowercase noisiness of the web text negatively impacting scores, we found that the baseline casesensitive parsers are especially bad at parsing imperative sentences, as discussed in Section 5.3.2. 4.4 Reordering rules In this paper we focus on English to Japanese, Korean, and Turkish translation. We use a superset of the reordering rules proposed by Xu et al. (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. The rules define a precedence order for the dependents of each part of speech. For example, a slightly simplified version of the precedence order of child labels for a verbal head HEADVERB is: advcl, nsubj, prep, [other children], dobj, prt, aux, neg, HEADVERB, mark, ref, compl. Alternatively, we could have used an automatic reordering-rule learning framework like that of Genzel (2010). Because the reordering accuracy metric can be computed for any source/target language pair, this would have made o</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a dependency parser to improve SMT for subject-objectverb languages. In NAACL-HLT ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL ’01.</booktitle>
<contexts>
<context position="2078" citStr="Yamada and Knight, 2001" startWordPosition="320" endWordPosition="323">xt (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set (Petrov and Klein, 2007). Furthermore, parsing was never meant as a stand-alone task, but is rather a means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics,</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In ACL ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In NAACL06 WMT.</booktitle>
<contexts>
<context position="17538" citStr="Zens and Ney, 2006" startWordPosition="2863" endWordPosition="2866">icable to any language pair. We chose to use manually written rules to eliminate the variance induced by the automatic reordering-rule learning framework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given language pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of par</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. Discriminative reordering models for statistical machine translation. In NAACL06 WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing.</title>
<date>2008</date>
<booktitle>In EMNLP ’08.</booktitle>
<contexts>
<context position="14412" citStr="Zhang and Clark (2008)" startWordPosition="2346" endWordPosition="2349">es and corresponding reorderings for translation from English to Japanese. The good parse correctly identifies “Wear” as the main verb and moves it to the end of the sentence; the bad parse analyses “Wear sunscreen” as a noun phrase and does not reorder it. This example was one of the wins in the human evaluation of Section 5.2. 4.3 Parsers The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). The parser uses the following features: word identity of the first two words on the buffer, the top word on the stack and the head of the top word on the stack (if available); part-ofspeech identities of the first four words on the buffer and top two words on the stack; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the leftmost modifier of the first word in the buffer. We also include conjunctions over all nonlexical features. We also give results for the latent variable parser (a.k.a. BerkeleyParser) of Petro</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing. In EMNLP ’08.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>