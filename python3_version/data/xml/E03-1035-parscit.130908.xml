<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.055666">
<title confidence="0.738226">
Learning Translations of Named-Entity Phrases from Parallel Corpora
</title>
<author confidence="0.896754">
Robert C. Moore
</author>
<affiliation confidence="0.882198">
Microsoft Research
</affiliation>
<address confidence="0.920422">
Redmond, WA 98052, USA
</address>
<email confidence="0.997965">
bobmoore@microsoft.com
</email>
<sectionHeader confidence="0.996648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998834">
We develop a new approach to learn-
ing phrase translations from parallel cor-
pora, and show that it performs with
very high coverage and accuracy in
choosing French translations of English
named-entity phrases in a test corpus of
software manuals. Analysis of a subset
of our results suggests that the method
should also perform well on more gen-
eral phrase translation tasks.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941162790698">
Machine translation can benefit greatly from aug-
menting knowledge of word translations with
knowledge of phrase translations. Multiword
phrases may have nonliteral translations, or one
of several equally valid literal translations may
be strongly preferred in practice. Automatically
learning translations of single words from parallel
corpora has been much studied over the past ten
years or so (Melamed, 2000, and references), but
learning translations of multiword phrases has re-
ceived less attention. (See Section 5 for a review
of prior work in this area.) In this paper, we de-
velop a new approach to learning phrase transla-
tions from parallel corpora, and show that it per-
forms with very high coverage and accuracy on a
named-entity phrase translation task. Moreover,
analysis of a subset of our evaluation results sug-
gests that the method should also perform well on
more general phrase translation tasks.
In our approach, we are given a sentence-
aligned parallel corpus annotated with a set of
phrases in one of the two languages (the source
language), and our goal is identify the correspond-
ing phrases in the corpus in the other language (the
target language), ranking the translation pairs in
order of confidence. Certain segments of the target
language corpus may be annotated as constituting
lexical compounds, which may or may not include
the translations of the source language phrases of
interest. Otherwise there is no annotation of the
target language text, except for its being divided
into words and sentences.
Below we describe the issues in named-entity
phrase translation motivating this research, we ex-
plain our algorithm, and we present the results of
our evaluation on a named-entity phrase transla-
tion task. We pay particular attention to the subset
of the data that lacks the special characteristics of
the named-entity task that we take advantage of
to optimize our performance, to suggest how the
algorithm might perform on more general tasks.
Finally we compare our approach and results to
previous work on learning phrase translations.
</bodyText>
<sectionHeader confidence="0.9845835" genericHeader="method">
2 The Named-Entity Phrase Translation
Task
</sectionHeader>
<bodyText confidence="0.993761142857143">
Named-entity expressions (Chinchor and Marsh,
1997) are any words or phrases that name a spe-
cific entity. While often thought of in terms of
categories such as persons, organizations, or lo-
cations, in technical text a much wider range of
types of entities are often named In software man-
uals, for example, named-entity expressions in-
</bodyText>
<page confidence="0.99615">
259
</page>
<bodyText confidence="0.999971886363637">
clude names of menu items, dialogue boxes, soft-
ware systems, etc. While named-entity expres-
sions are typically used as proper nouns, those en-
countered in technical text often do not have the
syntactic form of nouns or noun phrases. Con-
sider, Click the View Source Tables button. In
this sentence, View Source Tables has the syntactic
form of a nonfinite verb phrase, but it is used like a
proper noun. It would be difficult to recognize as a
named-entity expression, except for the fact that in
English, all or most of the words in named-entity
expressions are typically capitalized.
Capitalization conventions of French and Span-
ish, however, make it harder to recognize named-
entity phrases, because often only the first word
of the phrase is capitalized. For example, in our
data, the French translation of View Source Tables
is Afficher les tables source. Embedded in a sen-
tence, it is difficult to determine the extent of such
a named-entity expression using only monolingual
lexical information. If we could fully parse the
sentence, we might be able to recognize Afficher
les tables source as a named-entity expression;
but it is very difficult to parse a sentence where
something that looks like a nonfinite verb phrase
is used like a proper noun, unless the parser al-
ready knows that there is something special about
that phrase. Our problem, therefore, is to find the
phrases that are translations of the English expres-
sions, without necessarily having previously rec-
ognized that they are in fact complete phrases.
Our approach addresses the identification and
translation problems simultaneously. Taking En-
glish as our source language, we use capitalization
clues to identify named-entity phrases in the En-
glish portion of a sentence-aligned parallel corpus,
and then we apply statistical techniques to decide
which contiguous sequences of words in the target
language portion of the corpus are most likely to
correspond to the English phrases. We can then
add the learned named-entity phrases to a phrasal
lexicon that can be used to better parse target lan-
guage sentences, as well as adding the translation
pairs to a bilingual translation dictionary.
</bodyText>
<sectionHeader confidence="0.99091" genericHeader="method">
3 The Algorithm
</sectionHeader>
<bodyText confidence="0.999962555555556">
Our algorithm begins by computing a fairly simple
bilingual-word-association metric on the parallel
corpus, which is then used to construct three pro-
gressively more refined phrase translation mod-
els. The first model is used only to initialize the
second, which in turn is used only to initialize
the third, which is the model actually used. Al-
though the algorithm is designed to take advan-
tage of some special properities of named-entity
phrase translation, it is in no way limited to this
task, and can be applied to any phrase translation
task in which a set of fixed phrases can be inden-
tified on one side of a bilingual parallel corpus,
whose translations on the on the other side are de-
sired. A random sample of the output of our phrase
translation learner is shown in Table 1.1 All these
examples, except for the last, were judged to be
correct in context in our evaluation.
</bodyText>
<subsectionHeader confidence="0.999033">
3.1 Model 1
</subsectionHeader>
<bodyText confidence="0.99953925">
In addition to statistics derived from the corpus,
the first model embodies two nonstatistical heuris-
tics. The first is simply that we do not hy-
pothesize translations of source language phrases
that would require splitting predetermined lexical
compounds, if any, in the target language. The
second heuristic is that if the phrase whose trans-
lation is sought occurs in exactly the same form
in the target language sentence as in the source
language sentence, we assume that it is the cor-
responding phrase in that sentence with probabil-
ity 1.0. This is a very important heuristic in our
test corpus, because almost 17% of the source lan-
guage test phrases are names or technical terms
that occur untranslated in the target language text.
We start by measuring the degree of association
between a source language word s and a target
language word t (ignoring upper/lower case dis-
tinctions) in terms of the frequencies with which
s occurs in sentences of the source language part
of the corpus and t occurs in sentences of the tar-
get language part of the corpus, compared to the
frequency with which s and t co-occur in aligned
sentences of the corpus. The particular measure
we use is the log-likelihood-ratio statistic recom-
mended by Dunning (1993).
In the past we have found that this word-
association metric provides an excellent basis for
</bodyText>
<footnote confidence="0.933668">
1Words joined by &amp;quot;_&amp;quot; were indentified as compounds by
the monolingual tokenizers prior to applying our algorithm.
</footnote>
<page confidence="0.995322">
260
</page>
<figure confidence="0.995655">
MSMQ_Explorer
Highlighted_Edges
ADD_FILEGROUP
Custom_Preview_Area
Web_Proxy _Server
Windows_NT_3.5
All_Unassigned
Build_Query
Product_Support_Services_Web
Microphone_Settings_Wizard
Process_Accounting
SQL-DMO_Examples
Flexible_Data_Model
SQL_Server_Log_Reader_Agent
NT_LM_Security_Support_Provider
Flip_On_Short_Edge
Transact_SQL
Microsoft_Repository
Sort_Orders
Microsoft_Distributed_Transaction_Coordinator
explorateur MSMQ
Contours en surbrillance
ADD FILEGROUP
Apercu personnalise
serveur proxy Web
Windows NT version 3.5
Tous non assignes
Generer la requete
Web_des_Services _de_Support _Technique
Assistant_Parametres de le microphone
comptabilisation de les processus
Exemples SQL-DMO
Modele de donnees flexible
agent de lecture de le journal SQL _Server
Fournisseur de le service de securite NT _LM
Retoumer sur les bords courts
Transact_SQL
Registre de stockage de Microsoft
ordres de tri
transaction distribuee
</figure>
<tableCaption confidence="0.987398">
Table 1: Random sample of translations produced.
</tableCaption>
<bodyText confidence="0.999980395833333">
learning single-word translation relationships, and
the higher the score, the more likely the associ-
ation is to be a true translation relation. How-
ever, with this particular metric there is no obvi-
ous way to combine the scores for indvidual word
pairs into a composite score for phrase translation
candidates; so we use the scores indirectly to es-
timate some relevant probabilities, which can then
be combined to yield a composite score. To do
this, we make another pass through the parallel
corpus, and for each word s in the source lan-
guage sentence of an aligned sentence pair, we
note which word t in the target language sentence
of the pair has the strongest association with s.
If there is no word having a positive association
with s above a certain cut-off, we take the empty
word e to have the highest association with s in
the given sentence pair. We do this in both direc-
tions, since even if the word most strongly associ-
ated with s is t, the word most strongly associated
with t might be some other word s&apos;. For each pair
of words s and t, we keep a count of how many
times t occurs as the word most strongly associ-
ated with s, and vice versa. From these counts, we
estimate (using a modified form of Good-Turing
smoothing) the probability P1 (t s) that an occur-
rence of a source language word s will have a word
t as its most strongly associated word in the corre-
sponding aligned target language sentence, as well
as the probability (s that an occurrence of a
target language word t will have a word s as its
most strongly associated word in the correspond-
ing aligned source language sentence.
The key idea of our first model is that if a candi-
date substring of a target language sentence corre-
sponds to a selected source language phrase, then
the words in the candidate target language sub-
string should associate most strongly with words
of the selected target language phrase, and the
words of the target language sentence outside the
candidate substring should associate most strongly
with words of the source language sentence out-
side the selected phrase. We compute a compos-
ite score for a particular partitioning of the target
language sentence by summing the logarithms of
the association probabilities for the strongest as-
sociations we can find of words in the selected
source language phrase to words in the candidate
</bodyText>
<page confidence="0.983636">
261
</page>
<bodyText confidence="0.999970307692308">
target language substring (and vice versa), which
we call the inside score, added to the sum of the
logarithms of the association probabilities for the
strongest associations we can find for the words of
the source language sentence outside the selected
phrase to the words of the target language sentence
outside the candidate substring (and vice versa),
which we call the outside score.
Symbolically, let s, s&apos; be words in the source
language sentence S; let t, 1&apos; be words in the tar-
get language sentence T; let S&apos; be a substring of
S; let T&apos; be a substring of T conjectured to be the
translation of S&apos; . Then,
</bodyText>
<equation confidence="0.999062454545455">
inside(S1, T&apos;) =
E max log (Pi (tt Is)) +
sES&apos; e u{,}
Emax log (Pi&apos; (8&apos; t))
teTi s&apos;esiu{€}
outside(S&apos;, =
max log (Pi (t&apos; s)) +
S—S, E(T—T&apos;)U{E}
se
max log (/=)_ (s&apos; t))
tET-7-1 81 E(S—S&apos;)U{E}
</equation>
<bodyText confidence="0.985203289473684">
Thus if a target language word outside the can-
didate translation has a high probability of associ-
ating with a source language word in the selected
phrase, that candidate translation is likely to get
a lower composite score than another candidate
translation that does include that particular target
language word. While this is not actually a gener-
ative model, the probabilities being combined are
comparable, and it seems to work well in practice.
Since in named-entity translation from English
to Spanish or French, capitalization is relevant in
determining the phrase translation (and since the
word-association statistic ignores capitalization),
we add to the composite score a log probability
estimated for three capitalization possibilities: the
target language phrase begins with a capitalized
word, the target language phrase has no capitalized
words, or the target language phrase contains cap-
italized words, but does not begin with one. Let
Pcapt (T1) represent the probability that a target
language translation of a source language named-
entity expression falls into the capitalization class
of T&apos;. The final expression for the Model 1 score
of a source language phrase S&apos; and a hypothesized
target language translation T&apos; is, then,
outside (S&apos; , T&apos;) + inside (S&apos; , T&apos;) + log (Pcapt (T&apos;))
The capitalization class probabilities are ini-
tially taken to be uniform and are iteratively re-
computed by Viterbi re-estimation. In this way,
we are able to learn that an English named-entity
phrase is likely to correspond to a Spanish or
French phrase in which the first word is capital-
ized. This is only a strong probability and not a
certainty, however. In the random sample of the
output of our system that we selected for evalu-
ation, we found that 20% of the source language
phrases had hypothesized target language transla-
tions in which the first word is not capitalized.
</bodyText>
<subsectionHeader confidence="0.97896">
3.2 Model 2
</subsectionHeader>
<bodyText confidence="0.99999224137931">
Model 2 replaces the inside score and capitaliza-
tion log probability of the Model I by a new in-
side score computed as the logarithm of a holis-
tic estimate of the conditional probability of the
target language candidate occurring as the trans-
lation of the source language phrase, P2 (VI S&apos;),
times the conditional probability of the source lan-
guage phrase occuring as the translation of the tar-
get language candidate, P (Stir. This unusual
statistic was chosen to mirror as closely as possi-
ble the structure of the first model; we are simply
replacing approximations of these probabilities es-
timated from sets of single-word associations with
estimates based on occurrences of the complete
phrases.
This whole-phrase-based inside score is com-
bined with the original word-association-based
outside score, using a scale factor a to account for
the fact that the new version of the inside score
can be expected to have a different degree of vari-
ability from the one it is replacing. If we did not
do this, the exaggerated variance due to false inde-
pendence assumptions in the individual probabil-
ities combined in the computation of the outside
score would overwhelm the reduced variance of
the inside score. The scale factor a is simply the
ratio of the standard deviation of the inside scores
as estimated in the first model and the standard de-
viation of the initial estimates of the inside scores
</bodyText>
<page confidence="0.983973">
262
</page>
<bodyText confidence="0.993157">
for the second model. The Model 2 scores, then,
are of the form
outside(S&apos;, ± a log (P2 (T&apos;1S&apos;) • 13(S&apos;IT&apos;))
The initial values for the phrase translation
probabilities are estimated according to the first
model, and iteratively re-estimated using EM, by
treating the Model 2 scores as log probabilities
and normalizing them across the candidate trans-
lations in each sentence pair for each source lan-
guage phrase.
The effect of moving from Model 1 to Model
2 is to let tendencies in the translation of partic-
ular phrases across sentences influence the choice
of a translation in a particular sentence. If a given
phrase has a clearly preferred translation in several
sentences, that can be taken into account in choos-
ing a translation for the phrase in a sentence where
the individual word association probabilities leave
the translation of the phrase unclear.
</bodyText>
<subsectionHeader confidence="0.994843">
3.3 Model 3
</subsectionHeader>
<bodyText confidence="0.999988">
Model 3 consists of computing the log-likelihood-
ratio metric for all the selected phrases and can-
didate translations, based on the whole phrases
rather than the individual words composing them,
but counting as co-occurrences only pairs con-
sisting of a selected phrase and its highest scor-
ing candidate translation in a particular aligned
sentence pair. We initialize this model by find-
ing the highest scoring translation of each occur-
rence of each selected source language phrase ac-
cording to Model 2, and we iteratively recompute
the parameters using Viterbi re-estimation. When
this re-estimation converges, we have our final set
of phrase translation scores, in terms of the log-
likelihood-ratio metric for whole phrases.
The main point of Model 3 is to obtain a con-
sistent set of log-likelihood-ratio scores to use as
a confidence measure for the phrase translation
pairs. This could be computed just in a single pass,
but the Viterbi re-estimation ensures that the data
we are computing the log-likelihood-ratio scores
from is consistent with the resulting scores. That
is, it ensures that we do not count an instance in the
data of a particular translation pair, when there is
a higher scoring possibility according to the confi-
dence measure we are computing.
</bodyText>
<sectionHeader confidence="0.993416" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.998840276595745">
The algorithm was developed using English-
Spanish parallel data, and independently tested
on 192,711 English-French parallel sentence pairs
consisting mainly of computer software manu-
als. 73,108 occurrences of 12,301 unique multi-
word named-entity phrases were hypothesized in
the English data by a hand-built rule-based tagger,
mainly using capitalization clues.
We evaluated the performance of our algo-
rithm in finding translations for the hypothesized
named-entity phrases using a random sample of
1195 of the proposed translations. The correct-
ness of the correspondence between the English
phrases and their hypothesized translations was
judged by a fluent French-English bilingual, with
the aid of the sentence pair for which each hy-
pothesized translation received the highest score,
according to Model 1. (In preliminary work, we
found that it was very difficult to judge correctness
without seeing relevant examples from the data.)
In some cases, the existence of words in the French
not corresponding to anything in the English led
to multiple equally valid phrase correspondences,
any of which was judged correct. Clear cases of
partial matches, however, were always counted as
incorrect.
The results of the evaluation are shown in Ta-
ble 2. &amp;quot;Cumulative Coverage&amp;quot; means the pro-
portion of the unique phrases for which at least
one translation is proposed, proceeding in order
of strength of association from highest to lowest.
&amp;quot;Cumulative Accuracy&amp;quot; is the estimated accuracy
of the translations proposed for the top scoring
fraction of translations corresponding to &amp;quot;Cumula-
tive Coverage&amp;quot;.2 &amp;quot;Good Input&apos; Cumulative Accu-
racy&amp;quot; is the same as &amp;quot;Cumulative Accuracy&amp;quot;, but
removing 157 cases (13% of the test data) where
it was impossible choose a correct French trans-
lation for the English phrase, within the assump-
tions of the task.3 &amp;quot;Singleton Proportion&amp;quot; records
2These are essentially the same measures used by
Melamed (2000) in his work on learning single-word trans-
lations from parallel corpora. We use the coverage metric
rather than recall, because in this data, phrases often have
more than one translation, and we have no practical way of
knowing what proportion of these translations we find. Ac-
curacy is the same as precision.
</bodyText>
<page confidence="0.704056">
3 85% of these cases were en-ors (or at least inconsisten-
263
</page>
<table confidence="0.999894111111111">
Cumulative All Data &amp;quot;Hard&amp;quot; Data
Coverage &amp;quot;Good Input&amp;quot;
Cumulative
Accuracy
Cumulative &amp;quot;Good Input&amp;quot; Singleton
Accuracy Cumulative Proportion
Accuracy
0.100 0.914 0.980 0.000 0.96
0.200 0.906 0.979 0.000 0.87
0.300 0.896 0.975 0.000 0.92
0.400 0.873 0.965 0.087 0.89
0.500 0.879 0.963 0.243 0.90
0.600 0.875 0.961 0.354 0.88
0.700 0.880 0.961 0.436 0.88
0.800 0.870 0.955 0.498 0.86
0.900 0.856 0.941 0.565 0.86
0.950 0.843 0.938 0.595 0.85
0.990 0.808 0.916 0.619 0.84
</table>
<tableCaption confidence="0.999653">
Table 2: Performance of phrase translation learning algorithm.
</tableCaption>
<bodyText confidence="0.99963072">
the proportion of the English test phrases that had
only a single occurrence in the data.
These results show accuracy over 80% up to
99% coverage, with accuracy over 91% at 99%
coverage when only data free of tokenization er-
rors and missing translations is considered. More-
over, at this level 62% of the English test phrases
had only a single occurrence in the data. This level
of performance is very high compared to previous
work on phrase translation, but this task does have
several properties that probably make it easier than
a more general phrase translation task would be.
First, 17% of the English phrases were repeated
exactly in the French corpus. Second, 80% of the
French translations began with a capital letter. Fi-
nally, 16% of the French translations were already
identified as complete lexical compounds.
To test the robustness of our technique to phrase
cies) in identification of lexical compounds in English and/or
French that made it impossible to correctly identify the cor-
rect French translation of an English phrase. (Smadja et al.
[1996] similarly report the performance of their collocation-
translation learner, removing errors due to mistakes in identi-
fying the source language collocations.) These included cases
where English words were inconectly included in or omit-
ted from the phrase so that there was no single correspond-
ing French phrase, or where an incorrect identification of a
French lexical compound connected words in the translation
of the English phrase with words not in the translation. The
remaining 15% of the cases excluded from &amp;quot;good input&amp;quot; were
cases where the French sentence simply did not contain any
phrase corresponding to the English phrase, either because of
free translation or because of errors in sentence alignment.
translation learning tasks where these advantages
are lacking, we analyzed our evaluation data to
find all cases where the tokenizations were correct,
but the correct translation of the English phrase be-
gan with a lower case letter, and the translation it-
self was not identified as a lexical compound in
preprocessing. (This also guaranteed that none
of the translations was identical to the English
phrase, since all the English test phrases began
with a capital letter.) There were 240 such cases
out of our sample of 1195 hypothesized transla-
tion pairs. The performance of the algorithm on
this &amp;quot;hard&amp;quot; subset of the data is shown in the last
column of Figure 2. Compared with the results in
the third column on all the &amp;quot;good input&amp;quot; data, the
error rates go up by a factor of 2-3, but accuracy
is still a quite respectable 84% at 99% coverage.4
</bodyText>
<sectionHeader confidence="0.829102" genericHeader="method">
5 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.999950857142857">
Our work on learning phrase translations can be
classified along at least two dimensions. First, our
approach is asymmetrical in that it assumes that
a set of phrases in the source language is given,
and the task is to find their translations in the tar-
get language, for which only minimal monolin-
gual knowledge may be available. In symmetri-
</bodyText>
<footnote confidence="0.992328666666667">
4&amp;quot;Cummulative coverage&amp;quot; in this case means coverage of
the 235 English phrases that were determined to have at least
one lowercase translation.
</footnote>
<page confidence="0.99682">
264
</page>
<bodyText confidence="0.999976406250001">
cal approaches, the problem is generally viewed as
discovering phrases in both languages that are mu-
tual translations, for which equally rich (or equally
poor) analysis tools are available. Second, our ap-
proach applies only to fixed phrases, since it as-
sumes that the translation of a source language
phrase is a contiguous sequence of words in the
target language. At least one other reported ap-
proach applies to more flexible collocations.
Al-Onaizan and Knight&apos;s (2002) work is both
asymmetrical and targeted at fixed phrases, as well
as being perhaps the only other work aimed specif-
ically at named-entity phrase translation (for Ara-
bic to English). Lacking a parallel bilingual cor-
pus, however, their methods are completly differ-
ent from ours, and their reported accuracy is only
65-73%.
Dagan and Church&apos;s (1997) Termight is also
asymmetrical and targeted at fixed phrases. It is
conceived of as an automated assistant for a lexi-
cographer that proposes technical terms extracted
from a corpus using monolingual methods, and
for those approved by the user, proposes possible
translations from a parallel corpus. While appar-
ently never intended for use as a fully automatic
translation finder, its accuracy if used as such was
reported by Dagan and Church to be 40% in the
one experiment they describe in English-German
translation.
The Champ°Ilion system of Smadja et al.
(1996) is also asymmetrical, but it addresses the
harder problem of flexible collocations as well
as fixed phrases. They report accuracies of 65-
78% in four different experiments on the French-
English Canadian Hansard parliamentary proceed-
ings, for the equivalent of our &amp;quot;good input&amp;quot;. A
meaningful sense of coverage is difficult to estab-
lish, but they note that their test data includes only
source language collocations with at least 10 oc-
currences in the corpus. In comparison, our accu-
racy at 99% coverage on good input was 84-92%
(depending on whether we look at just the &amp;quot;hard&amp;quot;
data or all the data), with 62% of our source lan-
guage phrases only occurring once in the corpus.
The rest of the work on phrase translation we
have found is all of the symmetrical sort. In one
sense this makes the task more difficult, since
source language phrases have to be discovered as
well as target language phrases. On the other
hand, coverage claims are often harder to evalu-
ate since, lacking annotated test data, there is no
way to tell how many more phrases a better phrase
finder would have discovered that would be mis-
translated by the translation finder.
Kupiec (1993) seems to have carried out the first
experiments in this tradition, describing a method
for finding noun phrase translations in the Cana-
dian Hansards. Kupiec does report both accuracy
and coverage: 90% accuracy, but at only 2% cov-
erage.
Yamamoto et al. (2001) report on a symmetrical
method in which the units discovered are not in-
tended to correspond to standard syntactic phrases,
which means they could not serve one of our goals,
that of adding well-formed phrases to the target
language lexicon. They report 83% accuracy and
60% coverage on a Japanese-English task, where
coverage is ambitiously defined with respect to the
entire test corpus. Their units include single words
in addition to longer segments, however, and they
also state that the coverage is measured automat-
ically on an unseen corpus, which suggests that
they have not verfied that their &amp;quot;coverage&amp;quot; repre-
sents correct coverage.
Wu&apos;s (1995) method, like Yamamoto et al.&apos;s
produces translation units that do not always cor-
respond to standard syntactic phrases. He reports
accuracy of 81.5% for English-Chinese, but this
is for translation pairs that have survived several
heuristic filters, so coverage is once again prob-
lematical.
Finally, Melamed&apos;s (1997) work on finding non-
compositional compounds in parallel data focuses
more on phrase finding than phrase translation.
For translation finding, he simply uses previous
statistical translation methods. Like Yamamoto
et al. and Wu, his multiword compounds are not
phrases in the traditional sense, so they would not
help with our parsing problem. Finally, his goal is
not to produce a phrasal lexicon, but simply to add
phrase-like units to a statistical translation model,
and his evaluation is in terms of improved overall
performance of that model, rather than accuracy
and coverage of a list of translation terms.
None of this work resembles our approach in
much detail. Dagan and Church&apos;s translation-
</bodyText>
<page confidence="0.991338">
265
</page>
<bodyText confidence="0.9977508">
N. Chinchor and E. Marsh. 1997. MUC-7
named entity task definition. In Proceedings
of the 7th Message Understanding Conference,
hap ://w w vhaui/894.02/related _proj ects/muc.
proposing method somewhat resembles a crude
version of our Model 1, and Kupiec&apos;s method
is somewhat like our Model 3 (replacing log-
likihood-ratio scores with joint probabilities and
Viterbi re-estimation with EM); otherwise, all the
methods are quite different. Comparing perfor-
mance is virtually impossible, since all the tasks
are different and comparing coverage is extremely
problematic. Nevertheless, our high accuracies
at very high coverage for named-entity phrases
seems to compare favorably with any of this work.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999998384615385">
We have presented a new approach for automat-
ically learning phrase translations from parallel
corpora. Although we have tested it only on
named-entity phrases, the method itself is quite
general and could be applied to a wide variety
of phrase translation tasks with minimal modi fi -
cations. Our analysis of the &amp;quot;hard&amp;quot; subset of our
data suggests that it would perform well on other
tasks. The only significant change that would be
need would be to generalize (or eliminate) the cap-
italization scores to condition on the capitalization
pattern of the source language phrase, which is
currently not done, since all the source language
test phrases in our task had similar capitalization.
With that generalization, the only obvious restric-
tion on the applicability of the approach is that it
requires the target language translations of source
language phrases to be contiguous.
We plan to continue working on improving the
models, including designing a proper generative
probabilistic model using the features that have
proved successful in the current algorithm. Fi-
nally, we plan to address the selection of source
language phrases, both to correct the tokenization
errors we currently make, and to extend the appli-
cability of the method beyond named entities.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998912860465116">
Y. Al-Onaizan and K. Knight. 2002. Translat-
ing named entities using monolingual and blin-
gual resources. In Proceedings of the 40th An-
nual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Pennsylvania,
pp. 400-408.
I. Dagan and K. Church. 1997. Termight: co-
ordinating humans and machines in bilingual
terminology acquisition. Machine Translation,
12:89-107.
T. Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Compu-
tational Linguistics, 19(0:61-74.
J. Kupiec. 1993. An algorithm for finding noun
phrase correspondences in bilingual corpora.
In Proceedings of the 31st Annual Meeting of
the Association for Computational Linguistics,
Columbus, Ohio, pp. 17-22.
I. D. Melamed. 1997. Automatic discovery of
non-compositional compounds in parallel data.
In Proceedings of the 2nd Conference on Enpir-
ical Methods in Natural Language Processing
(EMNLP &apos;97), Providence, RI.
I. D. Melamed. 2000. Models of Transla-
tional Equivalence. Computational Linguistics,
26(2):221-249.
F. Smadja, K. R. McKeown, and V. Hatzivas-
siloglou. 1996. Translating collocations for
bilingual lexicons: a statistical approach. Com-
putational Linguistics, 22(1):1-38.
D. Wu. 1995. Grammarless extraction of phrasal
translation examples from parallel texts. in Pro-
ceedings of TMI-95, Sixth International Con-
ference on Theoretical and Methodological Is-
sues in Machine Translation, Leuven, Belgium,
Vol. 2, pp. 354-372.
K. Yamamoto, Y. Matsumoto, and M. Kitamura.
2001. A comparative study on translational
units for bilingual lexicon extraction. In Pro-
ceedings of the Workshop on Data-Driven Ma-
chine Translation, 39th Annual Meeting of
the Association for Computational Linguistics,
Toulouse, France, pp. 87-94.
</reference>
<page confidence="0.998482">
266
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985159">
<title confidence="0.999973">Learning Translations of Named-Entity Phrases from Parallel Corpora</title>
<author confidence="0.999899">Robert C Moore</author>
<affiliation confidence="0.999641">Microsoft Research</affiliation>
<address confidence="0.999816">Redmond, WA 98052, USA</address>
<email confidence="0.999891">bobmoore@microsoft.com</email>
<abstract confidence="0.998667636363636">We develop a new approach to learning phrase translations from parallel corpora, and show that it performs with very high coverage and accuracy in choosing French translations of English named-entity phrases in a test corpus of software manuals. Analysis of a subset of our results suggests that the method should also perform well on more general phrase translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating named entities using monolingual and blingual resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>400--408</pages>
<location>Philadelphia, Pennsylvania,</location>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Y. Al-Onaizan and K. Knight. 2002. Translating named entities using monolingual and blingual resources. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Pennsylvania, pp. 400-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K Church</author>
</authors>
<title>Termight: coordinating humans and machines in bilingual terminology acquisition.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<pages>12--89</pages>
<marker>Dagan, Church, 1997</marker>
<rawString>I. Dagan and K. Church. 1997. Termight: coordinating humans and machines in bilingual terminology acquisition. Machine Translation, 12:89-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--0</pages>
<contexts>
<context position="7312" citStr="Dunning (1993)" startWordPosition="1195" endWordPosition="1196">language test phrases are names or technical terms that occur untranslated in the target language text. We start by measuring the degree of association between a source language word s and a target language word t (ignoring upper/lower case distinctions) in terms of the frequencies with which s occurs in sentences of the source language part of the corpus and t occurs in sentences of the target language part of the corpus, compared to the frequency with which s and t co-occur in aligned sentences of the corpus. The particular measure we use is the log-likelihood-ratio statistic recommended by Dunning (1993). In the past we have found that this wordassociation metric provides an excellent basis for 1Words joined by &amp;quot;_&amp;quot; were indentified as compounds by the monolingual tokenizers prior to applying our algorithm. 260 MSMQ_Explorer Highlighted_Edges ADD_FILEGROUP Custom_Preview_Area Web_Proxy _Server Windows_NT_3.5 All_Unassigned Build_Query Product_Support_Services_Web Microphone_Settings_Wizard Process_Accounting SQL-DMO_Examples Flexible_Data_Model SQL_Server_Log_Reader_Agent NT_LM_Security_Support_Provider Flip_On_Short_Edge Transact_SQL Microsoft_Repository Sort_Orders Microsoft_Distributed_Tran</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(0:61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>17--22</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="25607" citStr="Kupiec (1993)" startWordPosition="4176" endWordPosition="4177">er we look at just the &amp;quot;hard&amp;quot; data or all the data), with 62% of our source language phrases only occurring once in the corpus. The rest of the work on phrase translation we have found is all of the symmetrical sort. In one sense this makes the task more difficult, since source language phrases have to be discovered as well as target language phrases. On the other hand, coverage claims are often harder to evaluate since, lacking annotated test data, there is no way to tell how many more phrases a better phrase finder would have discovered that would be mistranslated by the translation finder. Kupiec (1993) seems to have carried out the first experiments in this tradition, describing a method for finding noun phrase translations in the Canadian Hansards. Kupiec does report both accuracy and coverage: 90% accuracy, but at only 2% coverage. Yamamoto et al. (2001) report on a symmetrical method in which the units discovered are not intended to correspond to standard syntactic phrases, which means they could not serve one of our goals, that of adding well-formed phrases to the target language lexicon. They report 83% accuracy and 60% coverage on a Japanese-English task, where coverage is ambitiously</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, pp. 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Automatic discovery of non-compositional compounds in parallel data.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Enpirical Methods in Natural Language Processing (EMNLP &apos;97),</booktitle>
<location>Providence, RI.</location>
<marker>Melamed, 1997</marker>
<rawString>I. D. Melamed. 1997. Automatic discovery of non-compositional compounds in parallel data. In Proceedings of the 2nd Conference on Enpirical Methods in Natural Language Processing (EMNLP &apos;97), Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<date>2000</date>
<journal>Models of Translational Equivalence. Computational Linguistics,</journal>
<pages>26--2</pages>
<contexts>
<context position="957" citStr="Melamed, 2000" startWordPosition="140" endWordPosition="141"> named-entity phrases in a test corpus of software manuals. Analysis of a subset of our results suggests that the method should also perform well on more general phrase translation tasks. 1 Introduction Machine translation can benefit greatly from augmenting knowledge of word translations with knowledge of phrase translations. Multiword phrases may have nonliteral translations, or one of several equally valid literal translations may be strongly preferred in practice. Automatically learning translations of single words from parallel corpora has been much studied over the past ten years or so (Melamed, 2000, and references), but learning translations of multiword phrases has received less attention. (See Section 5 for a review of prior work in this area.) In this paper, we develop a new approach to learning phrase translations from parallel corpora, and show that it performs with very high coverage and accuracy on a named-entity phrase translation task. Moreover, analysis of a subset of our evaluation results suggests that the method should also perform well on more general phrase translation tasks. In our approach, we are given a sentencealigned parallel corpus annotated with a set of phrases i</context>
<context position="19020" citStr="Melamed (2000)" startWordPosition="3086" endWordPosition="3087">or which at least one translation is proposed, proceeding in order of strength of association from highest to lowest. &amp;quot;Cumulative Accuracy&amp;quot; is the estimated accuracy of the translations proposed for the top scoring fraction of translations corresponding to &amp;quot;Cumulative Coverage&amp;quot;.2 &amp;quot;Good Input&apos; Cumulative Accuracy&amp;quot; is the same as &amp;quot;Cumulative Accuracy&amp;quot;, but removing 157 cases (13% of the test data) where it was impossible choose a correct French translation for the English phrase, within the assumptions of the task.3 &amp;quot;Singleton Proportion&amp;quot; records 2These are essentially the same measures used by Melamed (2000) in his work on learning single-word translations from parallel corpora. We use the coverage metric rather than recall, because in this data, phrases often have more than one translation, and we have no practical way of knowing what proportion of these translations we find. Accuracy is the same as precision. 3 85% of these cases were en-ors (or at least inconsisten263 Cumulative All Data &amp;quot;Hard&amp;quot; Data Coverage &amp;quot;Good Input&amp;quot; Cumulative Accuracy Cumulative &amp;quot;Good Input&amp;quot; Singleton Accuracy Cumulative Proportion Accuracy 0.100 0.914 0.980 0.000 0.96 0.200 0.906 0.979 0.000 0.87 0.300 0.896 0.975 0.000</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. D. Melamed. 2000. Models of Translational Equivalence. Computational Linguistics, 26(2):221-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K R McKeown</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: a statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="24447" citStr="Smadja et al. (1996)" startWordPosition="3974" endWordPosition="3977">rted accuracy is only 65-73%. Dagan and Church&apos;s (1997) Termight is also asymmetrical and targeted at fixed phrases. It is conceived of as an automated assistant for a lexicographer that proposes technical terms extracted from a corpus using monolingual methods, and for those approved by the user, proposes possible translations from a parallel corpus. While apparently never intended for use as a fully automatic translation finder, its accuracy if used as such was reported by Dagan and Church to be 40% in the one experiment they describe in English-German translation. The Champ°Ilion system of Smadja et al. (1996) is also asymmetrical, but it addresses the harder problem of flexible collocations as well as fixed phrases. They report accuracies of 65- 78% in four different experiments on the FrenchEnglish Canadian Hansard parliamentary proceedings, for the equivalent of our &amp;quot;good input&amp;quot;. A meaningful sense of coverage is difficult to establish, but they note that their test data includes only source language collocations with at least 10 occurrences in the corpus. In comparison, our accuracy at 99% coverage on good input was 84-92% (depending on whether we look at just the &amp;quot;hard&amp;quot; data or all the data), </context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>F. Smadja, K. R. McKeown, and V. Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: a statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Grammarless extraction of phrasal translation examples from parallel texts.</title>
<date>1995</date>
<booktitle>in Proceedings of TMI-95, Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<volume>2</volume>
<pages>354--372</pages>
<location>Leuven, Belgium,</location>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. Grammarless extraction of phrasal translation examples from parallel texts. in Proceedings of TMI-95, Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, Leuven, Belgium, Vol. 2, pp. 354-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamamoto</author>
<author>Y Matsumoto</author>
<author>M Kitamura</author>
</authors>
<title>A comparative study on translational units for bilingual lexicon extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Data-Driven Machine Translation, 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>87--94</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="25866" citStr="Yamamoto et al. (2001)" startWordPosition="4217" endWordPosition="4220">more difficult, since source language phrases have to be discovered as well as target language phrases. On the other hand, coverage claims are often harder to evaluate since, lacking annotated test data, there is no way to tell how many more phrases a better phrase finder would have discovered that would be mistranslated by the translation finder. Kupiec (1993) seems to have carried out the first experiments in this tradition, describing a method for finding noun phrase translations in the Canadian Hansards. Kupiec does report both accuracy and coverage: 90% accuracy, but at only 2% coverage. Yamamoto et al. (2001) report on a symmetrical method in which the units discovered are not intended to correspond to standard syntactic phrases, which means they could not serve one of our goals, that of adding well-formed phrases to the target language lexicon. They report 83% accuracy and 60% coverage on a Japanese-English task, where coverage is ambitiously defined with respect to the entire test corpus. Their units include single words in addition to longer segments, however, and they also state that the coverage is measured automatically on an unseen corpus, which suggests that they have not verfied that thei</context>
</contexts>
<marker>Yamamoto, Matsumoto, Kitamura, 2001</marker>
<rawString>K. Yamamoto, Y. Matsumoto, and M. Kitamura. 2001. A comparative study on translational units for bilingual lexicon extraction. In Proceedings of the Workshop on Data-Driven Machine Translation, 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France, pp. 87-94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>