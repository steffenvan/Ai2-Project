<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.996481">
Identifying Non-explicit Citing Sentences for Citation-based
Summarization
</title>
<author confidence="0.988953">
Vahed Qazvinian
</author>
<affiliation confidence="0.9973305">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.930047">
Ann Arbor, MI
</address>
<email confidence="0.998885">
vahed@umich.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835833333333">
Identifying background (context) informa-
tion in scientific articles can help schol-
ars understand major contributions in their
research area more easily. In this paper,
we propose a general framework based
on probabilistic inference to extract such
context information from scientific papers.
We model the sentences in an article and
their lexical similarities as a Markov Ran-
dom Field tuned to detect the patterns that
context data create, and employ a Belief
Propagation mechanism to detect likely
context sentences. We also address the
problem of generating surveys of scien-
tific papers. Our experiments show greater
pyramid scores for surveys generated us-
ing such context information rather than
citation sentences alone.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967843882352941">
In scientific literature, scholars use citations to re-
fer to external sources. These secondary sources
are essential in comprehending the new research.
Previous work has shown the importance of cita-
tions in scientific domains and indicated that ci-
tations include survey-worthy information (Sid-
dharthan and Teufel, 2007; Elkiss et al., 2008;
Qazvinian and Radev, 2008; Mohammad et al.,
2009; Mei and Zhai, 2008).
A citation to a paper in a scientific article may
contain explicit information about the cited re-
search. The following example is an excerpt from
a CoNLL paper1 that contains information about
Eisner’s work on bottom-up parsers and the notion
of span in parsing:
“Another use of bottom-up is due to Eisner
(1996), who introduced the notion of a span.”
</bodyText>
<footnote confidence="0.796006">
1Buchholz and Marsi “CoNLL-X Shared Task On Multi-
lingual Dependency Parsing”, CoNLL 2006
</footnote>
<author confidence="0.758433">
Dragomir R. Radev
</author>
<affiliation confidence="0.989875333333333">
Department of EECS and
School of Information
University of Michigan
</affiliation>
<address confidence="0.869175">
Ann Arbor, MI
</address>
<email confidence="0.992537">
radev@umich.edu
</email>
<bodyText confidence="0.983416384615385">
However, the citation to a paper may not always
include explicit information about the cited paper:
“This approach is one of those described in Eis-
ner (1996)”
Although this sentence alone does not provide any
information about the cited paper, it suggests that
its surrounding sentences describe the proposed
approach in Eisner’s paper:
“...
In an all pairs approach, every possible
pair of two tokens in a sentence is considered
and some score is assigned to the possibility of
this pair having a (directed) dependency rela-
tion. Using that information as building blocks,
the parser then searches for the best parse for
the sentence. This approach is one of those de-
scribed in Eisner (1996).”
We refer to such implicit citations that contain
information about a specific secondary source but
do not explicitly cite it, as sentences with con-
text information or context sentences for short.
We look at the patterns that such sentences cre-
ate and observe that context sentences occur with-
ing a small neighborhood of explicit citations. We
also discuss the problem of extracting context sen-
tences for a source-reference article pair. We pro-
pose a general framework that looks at each sen-
tence as a random variable whose value deter-
mines its state about the target paper. In summary,
our proposed model is based on the probabilistic
inference of these random variables using graphi-
cal models. Finally we give evidence on how such
sentences can help us produce better surveys of re-
search areas. The rest of this paper is organized as
follows. Preceded by a review of prior work in
Section 2, we explain the data collection and our
annotation process in Section 3. Section 4 explains
our methodology and is followed by experimental
setup in Section 5.
</bodyText>
<page confidence="0.970597">
555
</page>
<note confidence="0.998126">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
ACL-ID Author Title Year all #Refs # Sents
AAN
P08-2026 McClosky &amp; Charniak Self-Training for Biomedical Parsing 2008 12 8 102
N07-1025∗ Mihalcea Using Wikipedia for Automatic ... 2007 21 12 153
N07-3002 Wang Learning Structured Classifiers ... 2007 22 14 74
P06-1101 Snow et, al. Semantic Taxonomy Induction ... 2006 19 9 138
P06-1116 Abdalla &amp; Teufel A Bootstrapping Approach To ... 2006 24 10 231
W06-2933 Nivre et, al. Labeled Pseudo-Projective Dependency ... 2006 27 5 84
P05-1044 Smith &amp; Eisner Contrastive Estimation: Training Log-Linear ... 2005 30 13 262
P05-1073 Toutanova et, al. Joint Learning Improves Semantic Role Labeling 2005 14 10 185
N03-1003 Barzilay &amp; Lee Learning To Paraphrase: An Unsupervised ... 2003 26 13 203
N03-2016∗ Kondrak et, al. Cognates Can Improve Statistical Translation ... 2003 8 5 92
</note>
<tableCaption confidence="0.905401333333333">
Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publi-
cation year, number of references (in AAN) and number of sentences. Papers marked with * are used to
calculate inter-judge agreement.
</tableCaption>
<sectionHeader confidence="0.994991" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999871367346939">
Analyzing the structure of scientific articles and
their relations has received a lot of attention re-
cently. The structure of citation and collaboration
networks has been studied in (Teufel et al., 2006;
Newman, 2001), and summarization of scientific
documents is discussed in (Teufel and Moens,
2002). In addition, there is some previous work
on the importance of citation sentences. Elkiss et
al, (Elkiss et al., 2008) perform a large-scale study
on citations in the free PubMed Central (PMC)
and show that they contain information that may
not be present in abstracts. In other work, Nanba
et al, (Nanba and Okumura, 1999; Nanba et al.,
2004b; Nanba et al., 2004a) analyze citation sen-
tences and automatically categorize them in order
to build a tool for survey generation.
The text of scientific citations has been used in
previous research. Bradshaw (Bradshaw, 2002;
Bradshaw, 2003) uses citations to determine the
content of articles. Similarly, the text of cita-
tion sentences has been directly used to produce
summaries of scientific papers in (Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Mohammad
et al., 2009). Determining the scientific attribu-
tion of an article has also been studied before.
Siddharthan and Teufel (Siddharthan and Teufel,
2007; Teufel, 2005) categorize sentences accord-
ing to their role in the author’s argument into pre-
defined classes: Own, Other, Background, Tex-
tual, Aim, Basis, Contrast.
Little work has been done on automatic cita-
tion extraction from research papers. Kaplan et
al, (Kaplan et al., 2009) introduces “citation-site”
as a block of text in which the cited text is dis-
cussed. The mentioned work uses a machine
learning method for extracting citations from re-
search papers and evaluates the result using 4 an-
notated articles.
In our work we use graphical models to ex-
tract context sentences. Graphical models have
a number of properties and corresponding tech-
niques and have been used before on Information
Retrieval tasks. Romanello et al, (Romanello et
al., 2009) use Conditional Random Fields (CRF)
to extract references from unstructured text in dig-
ital libraries of classic texts. Similar work include
term dependency extraction (Metzler and Croft,
2005), query expansion (Metzler and Croft, 2007),
and automatic feature selection (Metzler, 2007).
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999684111111111">
The ACL Anthology Network (AAN)2 is a col-
lection of papers from the ACL Anthology3 pub-
lished in the Computational Linguistics journal
and proceedings from ACL conferences and work-
shops and includes more than 14, 000 papers over
a period of four decades (Radev et al., 2009).
AAN includes the citation network of the papers
in the ACL Anthology. The papers in AAN are
publicly available in text format retrieved by an
OCR process from the original pdf files, and are
segmented into sentences.
To build a corpus for our experiments we picked
10 recently published papers from various areas
in NLP4, each of which had references for a to-
tal of 203 candidate paper-reference pairs. Table 1
lists these papers together with their authors, titles,
publication year, number of references, number of
references within AAN, and the number of sen-
</bodyText>
<footnote confidence="0.999948">
2http://clair.si.umich.edu/clair/anthology/
3http://www.aclweb.org/anthology-new/
4Regardless of data selection, the methodology in this
work is applicable to any of the papers in AAN.
</footnote>
<page confidence="0.994721">
556
</page>
<table confidence="0.99935153125">
L&amp;PS &amp;al Sentence
� � �
C C Jacquemin (1999) and Barzilay and McKeown (2001) identify
phrase level paraphrases, while Lin and Pantel (2001) and
Shinyama et al. (2002) acquire structural paraphrases encoded
as templates.
1 1 These latter are the most closely related to the sentence-level para-
phrases we desire, and so we focus in this section on template-
induction approaches.
C 0 Lin and Pantel (2001) extract inference rules, which are related
to paraphrases (for example, X wrote Y implies X is the author of
Y), to improve question answering.
1 0 They assume that paths in dependency trees that take similar argu-
ments (leaves) are close in meaning.
1 0 However, only two-argument templates are considered.
0 C Shinyama et al. (2002) also use dependency-tree information to
extract templates of a limited form (in their case, determined by
the underlying information extraction application).
1 1 Like us (and unlike Lin and Pantel, who employ a single large
corpus), they use articles written about the same event in different
newspapers as data.
1 1 Our approach shares two characteristics with the two methods just
described: pattern comparison by analysis of the patterns respec-
tive arguments, and use of nonparallel corpora as a data source.
0 0 However, extraction methods are not easily extended to generation
methods.
1 1 One problem is that their templates often only match small frag-
ments of a sentence.
1 1 While this is appropriate for other applications, deciding whether
to use a given template to generate a paraphrase requires informa-
tion about the surrounding context provided by the entire sentence.
� � �
</table>
<tableCaption confidence="0.987627">
Table 2: Part of the annotation for N03-1003 with
</tableCaption>
<bodyText confidence="0.976737">
respect to two of its references “Lin and Pan-
tel (2001)” (the first column) “Shinyama et al.
(2002)” (the second column). Cs indicate explicit
citations, 1s indicate implicit citations and 0s are
none.
tences.
</bodyText>
<subsectionHeader confidence="0.994958">
3.1 Annotation Process
</subsectionHeader>
<bodyText confidence="0.999951357142857">
We annotated the sentences in each paper from Ta-
ble 1. Each annotation instance in our setting cor-
responds to a paper-reference pair, and is a vec-
tor in which each dimension corresponds to a sen-
tence and is marked with a C if it explicitly cites
the reference, and with a 1 if it implicitly talks
about it. All other sentences are marked with 0s.
Table 2 shows a portion of two separate annota-
tion instances of N03-1003 corresponding to two
of its references. Our annotation has resulted in
203 annotation instances each corresponding to
one paper-reference pair. The goal of this work
is to automatically identify all context sentences,
which are marked as “1”.
</bodyText>
<sectionHeader confidence="0.492998" genericHeader="method">
3.1.1 Inter-judge Agreement
</sectionHeader>
<bodyText confidence="0.99994275">
We also asked a neutral annotator5 to annotate
two of our datasets that are marked with * in Ta-
ble 1. For each paper-reference pair, the annotator
was provided with a vector in which explicit cita-
</bodyText>
<footnote confidence="0.6001325">
5Someone not involved with the paper but an expert in
NLP.
</footnote>
<table confidence="0.991494">
ACL-ID vector size # Annotations r,
N07-1025∗ 153 21 0.889 f 0.30
N03-2016∗ 92 8 0.853 f 0.35
</table>
<tableCaption confidence="0.9602325">
Table 3: Average K coefficient as inter-judge
agreement for annotations of two sets
</tableCaption>
<bodyText confidence="0.999500777777778">
tions were already marked with Cs. The annota-
tion guidelines instructed the annotator to look at
each explicit citation sentence, and read up to 15
sentences before and after, then mark context sen-
tences around that sentence with 1s. Next, the 29
annotation instances done by the external annota-
tor were compared with the corresponding anno-
tations that we did, and the Kappa coefficient (K)
was calculated. The K statistic is formulated as
</bodyText>
<equation confidence="0.9660275">
Pr(a) − Pr(e)
1 − Pr(e)
</equation>
<bodyText confidence="0.999980214285714">
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that an-
notators agree by chance if each annotator is ran-
domly assigning categories. To calculate K, we ig-
nored all explicit citations (since they were pro-
vided to the external annotator) and used the bi-
nary categories (i.e., 1 for context sentences, and
0 otherwise) for all other sentences. Table 3 shows
the annotation vector size (i.e., number of sen-
tences), number of annotation instances (i.e., num-
ber of references), and average K for each set. The
average K is above 0.85 in both cases, suggest-
ing that the annotation process has a low degree
of subjectivity and can be considered reliable.
</bodyText>
<subsectionHeader confidence="0.999492">
3.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999992117647059">
In this section we describe our analysis. First,
we look at the number of explicit citations each
reference has received in a paper. Figure 1 (a)
shows the histogram corresponding to this distri-
bution. It indicates that the majority of references
get cited in only 1 sentence in a scientific arti-
cle, while the maximum being 9 in our collected
dataset with only 1 instance (i.e., there is only 1
reference that gets cited 9 times in a paper). More-
over, the data exhibits a highly positive-skewed
distribution. This is illustrated on a log-log scale
in Figure 1 (b). This highly skewed distribution
indicates that the majority of references get cited
only once in a citing paper. The very small number
of citing sentences can not make a full inventory of
the contributions of the cited paper, and therefore,
extracting explicit citations alone without context
</bodyText>
<equation confidence="0.980296">
K _
</equation>
<page confidence="0.936665">
557
</page>
<table confidence="0.59904">
gap size 0 1 2 4 9 10 15 16
instance 273 14 2 1 2 1 1 1
</table>
<tableCaption confidence="0.6158045">
Table 4: The distribution of gaps in the annotated
data
sentences may result in information loss about the
contributions of the cited paper.
</tableCaption>
<figureCaption confidence="0.780528">
Figure 1: (a) Histogram of the number of differ-
</figureCaption>
<bodyText confidence="0.985477818181818">
ent citations to each reference in a paper. (b) The
distribution observed for the number of different
citations on a log-log scale.
Next, we investigate the distance between con-
text sentences and the closest citations. For each
context sentence, we find its distance to the clos-
ets context sentence or explicit citation. Formally,
we define the gap to be the number of sentences
between a context sentence (marked with 1) and
the closest context sentence or explicit citation
(marked with either C or 1) to it. For example,
the second column of Table 2 shows that there is a
gap of size 1 in the 9th sentence in the set of con-
text and citation sentences about Shinyama et al.
(2002). Table 4 shows the distribution of gap sizes
in the annotated data. This observation suggests
that the majority of context sentences directly oc-
cur after or before a citation or another context
sentence. However, it shows that gaps between
sentences describing a cited paper actually exist,
and a proposed method should have the capability
to capture them.
</bodyText>
<sectionHeader confidence="0.994939" genericHeader="method">
4 Proposed Method
</sectionHeader>
<bodyText confidence="0.999883979166666">
In this section we propose our methodology that
enables us to identify the context information of a
cited paper. Particularly, the task is to assign a bi-
nary label XC to each sentence Si from a paper S,
where XC = 1 shows a context sentence related
to a given cited paper, C. To solve this problem
we propose a systematic way to model the net-
work level relationship between consecutive sen-
tences. In summary, each sentence is represented
with a node and is given two scores (context, non-
context), and we update these scores to be in har-
mony with the neighbors’ scores.
A particular class of graphical models known
as Markov Random Fields (MRFs) are suited for
solving inference problems with uncertainty in ob-
served data. The data is modeled as an undirected
graph with two types of nodes: hidden and ob-
served. Observed nodes represent values that are
known from the data. Each hidden node xu, cor-
responding to an observed node yu, represents the
true state underlying the observed value. The state
of a hidden node is related to the value of its cor-
responding observed node as well as the states of
its neighboring hidden nodes.
The local Markov property of an MRF indi-
cates that a variable is conditionally independent
on all other variables given its neighbors: xv ⊥
⊥ xV \cl(v)|xne(v), where ne(v) is the set of neigh-
bors of v, and cl(v) = {v} ∪ ne(v) is the closed
neighborhood of v. Thus, the state of a node is as-
sumed to statistically depend only upon its hidden
node and each of its neighbors, and independent
of any other node in the graph given its neighbors.
Dependencies in an MRF are represented using
two functions: Compatibility function (ψ) and Po-
tential function (φ). ψuv(xc, xd) shows the edge
potential of an edge between two nodes u, v of
classes xc and xd. Large values of ψuv would
indicate a strong association between xc and xd
at nodes u, v. The Potential function, φi(xc, yc),
shows the statistical dependency between xc and
yc at each node i assumed by the MRF model.
In order to find the marginal probabilities of
xis in a MRF we can use Belief Propagation
(BP) (Yedidia et al., 2003). If we assume the yis
are fixed and show φi(xi, yi) by φi(xi), we can
find the joint probability distribution for unknown
variables xi as
</bodyText>
<equation confidence="0.9995415">
1 �
p({x}) = Z
</equation>
<bodyText confidence="0.999964777777778">
In the BP algorithm a set of new variables m is
introduced where mij(xj) is the message passed
from i to j about what state xj should be in. Each
message, mij(xj), is a vector with the same di-
mensionality of xj in which each dimension shows
i’s opinion about j being in the corresponding
class. Therefore each message could be consid-
ered as a probability distribution and its compo-
nents should sum up to 1. The final belief at a
</bodyText>
<figure confidence="0.962535238095238">
1 2 3 4 5 6 7 8 9
cit
100
10−1
10−2
10−3
100 101
cit
140
120
100
80
60
40
20
0
a b
p(cit)
alpha = 3.13; D=0.02
ψij(xi, xj) � φi(xi)
ij i
</figure>
<page confidence="0.684286">
558
</page>
<figureCaption confidence="0.996525833333333">
Figure 2: The illustration of the message updating
rule. Elements that make up the message from a
node i to another node j: messages from i’s neigh-
bors, local evidence at i, and propagation function
between i, j summed over all possible states of
node i.
</figureCaption>
<bodyText confidence="0.731244">
node i, in the BP algorithm, is also a vector with
the same dimensionality of messages, and is pro-
portional to the local evidence as well as all mes-
sages from the node’s neighbors:
</bodyText>
<equation confidence="0.9944195">
bi(xi) � k�i(xi) 11 mji(xi) (1)
jEne(i)
</equation>
<bodyText confidence="0.9999302">
where k is the normalization factor of the be-
liefs about different classes. The message passed
from i to j is proportional to the propagation func-
tion between i, j, the local evidence at i, and all
messages sent to i from its neighbors except j:
</bodyText>
<equation confidence="0.883707">
mij(xj) � � �i(xi)�ij(xi,xj) 11 mki(xi)
2z kEne(i)\j
</equation>
<bodyText confidence="0.936282888888889">
Figure 2 illustrates the message update rule.
Convergence can be determined based on a va-
riety of criteria. It can occur when the maximum
change of any message between iteration steps is
less than some threshold. Convergence is guaran-
teed for trees but not for general graphs. However,
it typically occurs in practice (McGlohon et al.,
2009). Upon convergence, belief scores are deter-
mined by Equation 1.
</bodyText>
<subsectionHeader confidence="0.983414">
4.1 MRF construction
</subsectionHeader>
<bodyText confidence="0.99931975">
To find the sentences from a paper that form the
context information of a given cited paper, we
build an MRF in which a hidden node xi and
an observed node yi correspond to each sentence.
The structure of the graph associated with the
MRF is dependent upon the validity of a basic as-
sumption. This assumption indicates that the gen-
eration of a sentence (in form of its words) only
</bodyText>
<figure confidence="0.994923">
(a) (b)
</figure>
<figureCaption confidence="0.739788333333333">
Figure 3: The structure of the MRF constructed
based on the independence of non-adjacent sen-
tences; (a) left, each sentence is independent on
all other sentences given its immediate neighbors.
(b) right, sentences have dependency relationship
with each other regardless of their position.
</figureCaption>
<bodyText confidence="0.999495235294118">
depends on its surrounding sentences. Said dif-
ferently, each sentence is written independently of
all other sentences given a number of its neigh-
bors. This local dependence assumption can result
in a number of different MRFs, each built assum-
ing a dependency between a sentence and all sen-
tences within a particular distance. Figure 3 shows
the structure of the two MRFs at either extreme of
the local dependence assumption. In Figure 3 a,
each sentence only depends on one following and
one preceding sentence, while Figure 3 b shows
an MRF in which sentences are dependent on each
other regardless of their position. We refer to the
(2)former by BP1, and to the latter by BPI. Gen-
erally, we use BPi to denote an MRF in which
each sentence is connected to i sentences before
and after.
</bodyText>
<equation confidence="0.99823">
Oij(xc, xd) xd = 0 xd = 1
xc = 0 0.5 0.5
xc = 1 1 — Sij Sij
</equation>
<tableCaption confidence="0.842258666666667">
Table 5: The compatibility function 0 between
any two nodes in the MRFs from the sentences in
scientific papers
</tableCaption>
<subsectionHeader confidence="0.989986">
4.2 Compatibility Function
</subsectionHeader>
<bodyText confidence="0.999662142857143">
The compatibility function of an MRF represents
the association between the hidden node classes.
A node’s belief to be in class 1 is its probability to
be included in the context. The belief of a node i,
about its neighbor j to be in either classes is as-
sumed to be 0.5 if i is in class 0. In other words, if
a node is not part of the context itself, we assume
</bodyText>
<page confidence="0.996629">
559
</page>
<bodyText confidence="0.9995241">
it has no effect on its neighbors’ classes. In con-
trast, if i is in class 1 its belief about its neighbor
j is determined by their mutual lexical similarity.
If this similarity is close to 1 it indicates a stronger
tie between i, j. However, if i, j are not similar,
i’s probability of being in class 1, should not af-
fect that of j’s. To formalize this assumption we
use the sigmoid of the cosine similarity of two sen-
tences to build 0. More formally, we define S to
be
</bodyText>
<equation confidence="0.925191">
= 1
Sij 1 + e−cosine(i,j)
</equation>
<bodyText confidence="0.999972">
The sigmoid function obtains a value of 0.5 for
a cosine of 0 indicating that there is no bias in the
association of the two sentences. The matrix in Ta-
ble 5 shows the compatibility function built based
on the above arguments.
</bodyText>
<subsectionHeader confidence="0.994945">
4.3 Potential Function
</subsectionHeader>
<bodyText confidence="0.999834171428571">
The node potential function of an MRF can incor-
porate some other features observable from data.
Here, the goal is to find all sentences that are about
a specific cited paper, without having explicit cita-
tions. To build the node potential function of the
observed nodes, we use some sentence level fea-
tures. First, we use the explicit citation as an im-
portant feature of a sentence. This feature can af-
fect the belief of the corresponding hidden node,
which can in turn affect its neighbors’ beliefs. For
a given paper-reference pair, we flag (with a 1)
each sentence that has an explicit citation to the
reference.
The second set of features that we are inter-
ested in are discourse-based features. In particu-
lar we match each sentence with specific patterns
and flag those that match. The first pattern is a bi-
gram in which the first term matches any of “this;
that; those; these; his; her; their; such; previ-
ous”, and the second term matches any of “work;
approach; system; method; technique; result; ex-
ample”. The second pattern includes all sentences
that start with “this; such”.
Finally, the similarity of each sentence to the
reference is observable from the data and can be
used as a sentence-level feature. Intuitively, if a
sentence has higher similarity with the reference
paper, it should have a higher potential of being
in class 1 or C. The flag of each sentence here is
a value between 0 and 1 and is determined by its
cosine similarity to the reference. Once the flags
for each sentence, Si are determined, we calculate
normalized fi as the unweighted linear combina-
tion of individual features. Based on fis, we com-
pute the potential function, 0, as shown in Table 6.
</bodyText>
<equation confidence="0.956379">
Oi(xc, yc) xc = 0
1 − fi fi
</equation>
<bodyText confidence="0.90698575">
Table 6: The node potential function 0 for each
node in the MRFs from the sentences in scientific
papers is built using the sentences’ flags computed
using sentence level features.
</bodyText>
<sectionHeader confidence="0.999573" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999852">
The intrinsic evaluation of our methodology
means to directly compare the output of our
method with the gold standards obtained from the
annotated data. Our methodology finds the sen-
tences that cite a reference implicitly. Therefore
the output of the inference method is a vector, v,
of 1’s and 0’s, whereby a 1 at element i means
that sentence i in the source document is a con-
text sentence about the reference while a 0 means
an explicit citation or neither. The gold standard
for each paper-reference pair, w (obtained from the
annotated vectors in Section 3.1 by changing all
Cs to 0s), is also a vector of the same format and
dimensionality.
Precision, recall, and F� for this task can be de-
fined as
</bodyText>
<equation confidence="0.9969895">
υ · ω
1 ; FR= (1 //+�β2)p r (3)
ω
/�1p + r
</equation>
<bodyText confidence="0.9998565">
where 1 is a vector of 1’s with the same dimen-
sionality and Q is a non-negative real number.
</bodyText>
<subsectionHeader confidence="0.792125">
5.1 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999084428571429">
The first baseline that we use is an IR-based
method. This baseline, B1, takes explicit citations
as an input but use them to find context sentences.
Given a paper-reference pair, for each explicit ci-
tation sentence, marked with C, B1 picks its pre-
ceding and following sentences if their similarities
to that sentence is greater than a cutoff (the median
of all such similarities), and repeats this for neigh-
boring sentences of newly marked sentences. In-
tuitively, B1 tries to find the best chain (window)
around citing sentences.
As the second baseline, we use the hand-crafted
discourse based features used in MRF’s potential
function. Particularly, this baseline, B2, marks
</bodyText>
<equation confidence="0.997983333333333">
xc = 1
υ · ω
p = υ · 1 ; r =
</equation>
<page confidence="0.991865">
560
</page>
<table confidence="0.999921454545455">
paper B1 B2 SVM BP1 BP4 BPn
P08-2026 0.441 0.237 0.249 0.470 0.613 0.285
N07-1025 0.388 0.102 0.124 0.313 0.466 0.138
N07-3002 0.521 0.339 0.232 0.742 0.627 0.315
P06-1101 0.125 0.388 0.127 0.649 0.889 0.193
P06-1116 0.283 0.104 0.100 0.307 0.341 0.130
W06-2933 0.313 0.100 0.176 0.338 0.413 0.160
P05-1044 0.225 0.100 0.060 0.172 0.586 0.094
P05-1073 0.144 0.100 0.144 0.433 0.518 0.171
N03-1003 0.245 0.249 0.126 0.523 0.466 0.125
N03-2016 0.100 0.181 0.224 0.439 0.482 0.185
</table>
<tableCaption confidence="0.9157385">
Table 7: Average F,a=3 for similarity based baseline (B1), discourse-based baseline (B2), a supervised
method (SVM) and three MRF-based methods.
</tableCaption>
<bodyText confidence="0.999862389830509">
each sentence that is within a particular distance
(4 in our experiments) of an explicit citation and
matches one of the two patterns mentioned in Sec-
tion 4.3. After marking all such sentences, B2
also marks all sentences between them and the
closest explicit citation, which is no farther than
4 sentences away. This baseline helps us under-
stand how effectively this sentence level feature
can work in the absence of other features and the
network structure.
Finally, we use a supervised method, SVM,
to classify sentences as context/non-context. We
use 4 features to train the SVM model. These
4 features comprise the 3 sentence level features
used in MRF’s potential function (i.e., similar-
ity to reference, explicit citation, matching certain
regular-expressions) and a network level feature:
distance to the closes explicit citation. For each
source paper, P, we use all other source papers
and their source-reference annotation instances to
train a model. We then use this model to clas-
sify all instances in P. Although the number of
references and thus source-reference pairs are dif-
ferent for different papers, this can be considered
similar to a 10-fold cross validation scheme, since
for each source paper the model is built using all
source-reference pairs of all other 9 papers.
We compare these baselines with 3 MRF-based
systems each with a different assumption about in-
dependence of sentences. BP1 denotes an MRF
in which each sentence is only connected to 1 sen-
tence before and after. In BP4 locality is more
relaxed and each sentence is connected to 4 sen-
tences on each sides. BPn denotes an MRF in
which all sentences are connected to each other
regardless of their position in the paper.
Table 7 shows FO=3 for our experiments and
shows how BP4 outperforms the other methods
on average. The value 4 may suggest the fact that
although sentences might be independent of dis-
tant sentences, they depend on more than one sen-
tence on each side.
The final experiment we do to intrinsically eval-
uate the MRF-base method is to compare differ-
ent sentence-level features. The first feature used
to build the potential function is explicit citations.
This feature does not directly affect context sen-
tences (i.e., it affects the marginal probability of
context sentences through the MRF network con-
nections). Therefore, we do not alter this fea-
ture in comparing different features. However, we
look at the effect of the second and the third fea-
tures: hand-crafted regular expression-based fea-
tures and similarity to the reference. For each pa-
per, we use BP4 to perform 3 experiments: two in
absence of each feature and one including all fea-
tures. Figure 4 shows the average F,3=3 for each
experiment. This plot shows that the features lead
to better results when used together.
</bodyText>
<sectionHeader confidence="0.964374" genericHeader="method">
6 Impact on Survey Generation
</sectionHeader>
<bodyText confidence="0.999943727272727">
We also performed an extrinsic evaluation of
our context extraction methodology. Here we
show how context sentences add important survey-
worthy information to explicit citations. Previous
work that generate surveys of scientific topics use
the text of citation sentences alone (Mohammad
et al., 2009; Qazvinian and Radev, 2008). Here,
we show how the surveys generated using citations
and their context sentences are better than those
generated using citation sentences alone.
We use the data from (Mohammad et al., 2009)
</bodyText>
<page confidence="0.994493">
561
</page>
<bodyText confidence="0.991032157894737">
... Naturally, our current work on question answering for the reading comprehension task is most related to those of
(Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000 ; Wang et al. , 2000). In fact, all of this
body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the
same development set of stories. The work of (Hirschman et al. , 1999) initiated this series of work, and it reported
an accuracy of 36.3% on answering the questions in the test stories. Subsequently, the work of (Riloffand Thelen ,
2000) and (Chaxniak et al. , 2000) improved the accuracy further to 39.7% and 41%, respectively. However, all
of these three systems used handcrafted, deterministic rules and algorithms...
...The cross-model comparison showed that the performance ranking of these models was: U-SVM &gt; PatternM
&gt; S-SVM &gt; Retrieval-M. Compared with retrieval-based [Yang et al. 2003], pattern-based [Ravichandran et al. 2002
and Soubbotin et al. 2002], and deep NLP-based [Moldovan et al. 2002, Hovy et al. 2001; and Pasca et al. 2001]
answer selection, machine learning techniques are more effective in constructing QA components from scratch. These
techniques suffer, however, from the problem of requiring an adequate number of handtagged question-answer
training pairs. It is too expensive and labor intensive to collect such training pairs for supervised machine
learning techniques ...
... As expected, the definition and person-bio answer types are covered well by these resources. The web has
been employed for pattern acquisition (Ravichandran et al. , 2003), document retrieval (Dumais et al. , 2002), query
expansion (Yang et al. , 2003), structured information extraction, and answer validation (Magnini et al. , 2002). Some
of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a
less complex approach to find correct answers ...
</bodyText>
<tableCaption confidence="0.991751">
Table 8: A portion of the QA survey generated by LexRank using the context information.
</tableCaption>
<table confidence="0.774471333333333">
citation survey context survey
QA
CT nuggets 0.416 0.634
AB nuggets 0.397 0.594
DP
CT nuggets 0.324 0.379
</table>
<tableCaption confidence="0.775787">
Table 9: Pyramid F,a=3 scores of automatic
</tableCaption>
<bodyText confidence="0.4477985">
surveys of QA and DP data. The QA surveys
are evaluated using nuggets drawn from citation
texts (CT), or abstracts (AB), and DP surveys are
evaluated using nuggets from citation texts (CT).
</bodyText>
<figureCaption confidence="0.897898">
Figure 4: Average F,a=3 for BP4 employing dif-
ferent features.
</figureCaption>
<bodyText confidence="0.999708540540541">
that contains two sets of cited papers and corre-
sponding citing sentences, one on Question An-
swering (QA) with 10 papers and the other on De-
pendency Parsing (DP) with 16 papers. The QA
set contains two different sets of nuggets extracted
by experts respectively from paper abstracts and
citation sentences. The DP set includes nuggets
extracted only from citation sentences. We use
these nugget sets, which are provided in form of
regular expressions, to evaluate automatically gen-
erated summaries. To perform this experiment we
needed to build a new corpus that includes con-
text sentences. For each citation sentence, BP4 is
used on the citing paper to extract the proper con-
text. Here, we limit the context size to be 4 on
each side. That is, we attach to a citing sentence
any of its 4 preceding and following sentences if
BP4 marks them as context sentences. Therefore,
we build a new corpus in which each explicit ci-
tation sentence is replaced with the same sentence
attached to at most 4 sentence on each side.
After building the context corpus, we use
LexRank (Erkan and Radev, 2004) to generate 2
QA and 2 DP surveys using the citation sentences
only, and the new context corpus explained above.
LexRank is a multidocument summarization sys-
tem, which first builds a cosine similarity graph of
all the candidate sentences. Once the network is
built, the system finds the most central sentences
by performing a random walk on the graph. We
limit these surveys to be of a maximum length of
1000 words. Table 8 shows a portion of the sur-
vey generated from the QA context corpus. This
example shows how context sentences add mean-
ingful and survey-worthy information along with
citation sentences. Table 9 shows the Pyramid
F,a=3 score of automatic surveys of QA and DP
</bodyText>
<page confidence="0.989756">
562
</page>
<bodyText confidence="0.999897285714286">
data. The QA surveys are evaluated using nuggets
drawn from citation texts (CT), or abstracts (AB),
and DP surveys are evaluated using nuggets from
citation texts (CT). In all evaluation instances the
surveys generated with the context corpora excel
at covering nuggets drawn from abstracts or cita-
tion sentences.
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99946725">
In this paper we proposed a framework based on
probabilistic inference to extract sentences that
appear in the scientific literature, and which are
about a secondary source, but which do not con-
tain explicit citations to that secondary source.
Our methodology is based on inference in an MRF
built using the similarity of sentences and their
lexical features. We show, by numerical exper-
iments, that an MRF in which each sentence is
connected to only a few adjacent sentences prop-
erly fits this problem. We also investigate the use-
fulness of such sentences in generating surveys of
scientific literature. Our experiments on generat-
ing surveys for Question Answering and Depen-
dency Parsing show how surveys generated using
such context information along with citation sen-
tences have higher quality than those built using
citations alone.
Generating fluent scientific surveys is difficult
in absence of sufficient background information.
Our future goal is to combine summarization
and bibliometric techniques towards building au-
tomatic surveys that employ context information
as an important part of the generated surveys.
</bodyText>
<sectionHeader confidence="0.998676" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999849583333333">
The authors would like to thank Arzucan ¨Ozg¨ur
from University of Michigan for annotations.
This paper is based upon work supported by the
National Science Foundation grant ”iOPENER: A
Flexible Framework to Support Rapid Learning in
Unfamiliar Research Domains”, jointly awarded
to University of Michigan and University of Mary-
land as IIS 0705832. Any opinions, findings, and
conclusions or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the National Science Foun-
dation.
</bodyText>
<sectionHeader confidence="0.996132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999686611111112">
Shannon Bradshaw. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Shannon Bradshaw. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
Conference on Research and Advanced Technology
for Digital Libraries.
Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51–62.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009.
Automatic extraction of citation contexts for re-
search paper summarization: A coreference-chain
based approach. In Proceedings of the 2009 Work-
shop on Text and Citation Analysis for Scholarly
Digital Libraries, pages 88–95, Suntec City, Sin-
gapore, August. Association for Computational Lin-
guistics.
Mary McGlohon, Stephen Bay, Markus G. Anderle,
David M. Steier, and Christos Faloutsos. 2009.
Snare: a link analytic system for graph labeling and
risk detection. In KDD ’09: Proceedings of the 15th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1265–1274.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings ofACL ’08, pages 816–824.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In SI-
GIR ’05: Proceedings of the 28th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 472–479.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In SI-
GIR ’07: Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 311–318.
Donald A. Metzler. 2007. Automatic feature selection
in the markov random field model for information
retrieval. In CIKM ’07: Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 253–262.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
</reference>
<page confidence="0.986723">
563
</page>
<reference confidence="0.999671436363636">
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 584–592, Boulder, Colorado, June.
Association for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926–931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004a. Bilingual presri:
Integration of multiple research paper databases. In
Proceedings of RIAO 2004, pages 195–211, Avi-
gnon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Oku-
mura. 2004b. Classification of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation. In Proceedings
of the 11th SIG Classification Research Workshop,
pages 117–134, Chicago, USA.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404–409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In ACL workshop on Natural Language
Processing and Information Retrievalfor Digital Li-
braries.
Matteo Romanello, Federico Boschetti, and Gregory
Crane. 2009. Citations in the digital library of clas-
sics: Extracting canonical references by using con-
ditional random fields. In Proceedings of the 2009
Workshop on Text and Citation Analysis for Schol-
arly Digital Libraries, pages 80–87, Suntec City,
Singapore, August. Association for Computational
Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summarizing
scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409–445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the EMNLP, Sydney, Australia,
July.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159–
170.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss. 2003. Understanding belief propagation and
its generalizations. pages 239–269.
</reference>
<page confidence="0.998208">
564
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775770">
<title confidence="0.999253">Identifying Non-explicit Citing Sentences for Citation-based Summarization</title>
<author confidence="0.987304">Vahed Qazvinian</author>
<affiliation confidence="0.9998145">Department of EECS University of Michigan</affiliation>
<address confidence="0.79476">Ann Arbor, MI</address>
<email confidence="0.998898">vahed@umich.edu</email>
<abstract confidence="0.99948547368421">Identifying background (context) information in scientific articles can help scholars understand major contributions in their research area more easily. In this paper, we propose a general framework based on probabilistic inference to extract such context information from scientific papers. We model the sentences in an article and lexical similarities as a Ran- Field to detect the patterns that data create, and employ a to detect likely context sentences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shannon Bradshaw</author>
</authors>
<title>Reference Directed Indexing: Indexing Scientific Literature in the Context of Its Use.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Northwestern University.</institution>
<contexts>
<context position="5759" citStr="Bradshaw, 2002" startWordPosition="915" endWordPosition="916">ents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citatio</context>
</contexts>
<marker>Bradshaw, 2002</marker>
<rawString>Shannon Bradshaw. 2002. Reference Directed Indexing: Indexing Scientific Literature in the Context of Its Use. Ph.D. thesis, Northwestern University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shannon Bradshaw</author>
</authors>
<title>Reference directed indexing: Redeeming relevance for subject search in citation indexes.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th European Conference on Research and Advanced Technology for Digital Libraries.</booktitle>
<contexts>
<context position="5776" citStr="Bradshaw, 2003" startWordPosition="917" endWordPosition="918">d in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from</context>
</contexts>
<marker>Bradshaw, 2003</marker>
<rawString>Shannon Bradshaw. 2003. Reference directed indexing: Redeeming relevance for subject search in citation indexes. In Proceedings of the 7th European Conference on Research and Advanced Technology for Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Elkiss</author>
<author>Siwei Shen</author>
<author>Anthony Fader</author>
<author>G¨unes¸ Erkan</author>
<author>David States</author>
<author>Dragomir R Radev</author>
</authors>
<title>Blind men and elephants: What do citation summaries tell us about a research article?</title>
<date>2008</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="1251" citStr="Elkiss et al., 2008" startWordPosition="178" endWordPosition="181">Propagation mechanism to detect likely context sentences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 1 Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 Dragomir R. Radev Department of EECS and School of Information University of Michigan </context>
<context position="5309" citStr="Elkiss et al., 2008" startWordPosition="839" endWordPosition="842"> for the evaluation corpus, together with their publication year, number of references (in AAN) and number of sentences. Papers marked with * are used to calculate inter-judge agreement. 2 Prior Work Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce sum</context>
</contexts>
<marker>Elkiss, Shen, Fader, Erkan, States, Radev, 2008</marker>
<rawString>Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸ Erkan, David States, and Dragomir R. Radev. 2008. Blind men and elephants: What do citation summaries tell us about a research article? Journal of the American Society for Information Science and Technology, 59(1):51–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="32381" citStr="Erkan and Radev, 2004" startWordPosition="5494" endWordPosition="5497">matically generated summaries. To perform this experiment we needed to build a new corpus that includes context sentences. For each citation sentence, BP4 is used on the citing paper to extract the proper context. Here, we limit the context size to be 4 on each side. That is, we attach to a citing sentence any of its 4 preceding and following sentences if BP4 marks them as context sentences. Therefore, we build a new corpus in which each explicit citation sentence is replaced with the same sentence attached to at most 4 sentence on each side. After building the context corpus, we use LexRank (Erkan and Radev, 2004) to generate 2 QA and 2 DP surveys using the citation sentences only, and the new context corpus explained above. LexRank is a multidocument summarization system, which first builds a cosine similarity graph of all the candidate sentences. Once the network is built, the system finds the most central sentences by performing a random walk on the graph. We limit these surveys to be of a maximum length of 1000 words. Table 8 shows a portion of the survey generated from the QA context corpus. This example shows how context sentences add meaningful and survey-worthy information along with citation s</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dain Kaplan</author>
<author>Ryu Iida</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Automatic extraction of citation contexts for research paper summarization: A coreference-chain based approach.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>88--95</pages>
<institution>Suntec City, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="6429" citStr="Kaplan et al., 2009" startWordPosition="1018" endWordPosition="1021">content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Simila</context>
</contexts>
<marker>Kaplan, Iida, Tokunaga, 2009</marker>
<rawString>Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009. Automatic extraction of citation contexts for research paper summarization: A coreference-chain based approach. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 88–95, Suntec City, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary McGlohon</author>
<author>Stephen Bay</author>
<author>Markus G Anderle</author>
<author>David M Steier</author>
<author>Christos Faloutsos</author>
</authors>
<title>Snare: a link analytic system for graph labeling and risk detection.</title>
<date>2009</date>
<booktitle>In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1265--1274</pages>
<contexts>
<context position="18541" citStr="McGlohon et al., 2009" startWordPosition="3120" endWordPosition="3123">malization factor of the beliefs about different classes. The message passed from i to j is proportional to the propagation function between i, j, the local evidence at i, and all messages sent to i from its neighbors except j: mij(xj) � � �i(xi)�ij(xi,xj) 11 mki(xi) 2z kEne(i)\j Figure 2 illustrates the message update rule. Convergence can be determined based on a variety of criteria. It can occur when the maximum change of any message between iteration steps is less than some threshold. Convergence is guaranteed for trees but not for general graphs. However, it typically occurs in practice (McGlohon et al., 2009). Upon convergence, belief scores are determined by Equation 1. 4.1 MRF construction To find the sentences from a paper that form the context information of a given cited paper, we build an MRF in which a hidden node xi and an observed node yi correspond to each sentence. The structure of the graph associated with the MRF is dependent upon the validity of a basic assumption. This assumption indicates that the generation of a sentence (in form of its words) only (a) (b) Figure 3: The structure of the MRF constructed based on the independence of non-adjacent sentences; (a) left, each sentence is</context>
</contexts>
<marker>McGlohon, Bay, Anderle, Steier, Faloutsos, 2009</marker>
<rawString>Mary McGlohon, Stephen Bay, Markus G. Anderle, David M. Steier, and Christos Faloutsos. 2009. Snare: a link analytic system for graph labeling and risk detection. In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1265–1274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating impact-based summaries for scientific literature.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL ’08,</booktitle>
<pages>816--824</pages>
<contexts>
<context position="1322" citStr="Mei and Zhai, 2008" startWordPosition="190" endWordPosition="193">ss the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 1 Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 Dragomir R. Radev Department of EECS and School of Information University of Michigan Ann Arbor, MI radev@umich.edu However, the citation to a paper may not </context>
<context position="5986" citStr="Mei and Zhai, 2008" startWordPosition="949" endWordPosition="952">d Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citatio</context>
</contexts>
<marker>Mei, Zhai, 2008</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2008. Generating impact-based summaries for scientific literature. In Proceedings ofACL ’08, pages 816–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>A markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>472--479</pages>
<contexts>
<context position="7096" citStr="Metzler and Croft, 2005" startWordPosition="1124" endWordPosition="1127">ext in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). 3 Data The ACL Anthology Network (AAN)2 is a collection of papers from the ACL Anthology3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences. To buil</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2005. A markov random field model for term dependencies. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 472–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>Latent concept expansion using markov random fields.</title>
<date>2007</date>
<booktitle>In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="7139" citStr="Metzler and Croft, 2007" startWordPosition="1130" endWordPosition="1133">he mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). 3 Data The ACL Anthology Network (AAN)2 is a collection of papers from the ACL Anthology3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences. To build a corpus for our experiments we picked 10</context>
</contexts>
<marker>Metzler, Croft, 2007</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2007. Latent concept expansion using markov random fields. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald A Metzler</author>
</authors>
<title>Automatic feature selection in the markov random field model for information retrieval.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>253--262</pages>
<contexts>
<context position="7188" citStr="Metzler, 2007" startWordPosition="1138" endWordPosition="1139">ting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). 3 Data The ACL Anthology Network (AAN)2 is a collection of papers from the ACL Anthology3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences. To build a corpus for our experiments we picked 10 recently published papers from various areas in </context>
</contexts>
<marker>Metzler, 2007</marker>
<rawString>Donald A. Metzler. 2007. Automatic feature selection in the markov random field model for information retrieval. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 253–262.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>584--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1301" citStr="Mohammad et al., 2009" startWordPosition="186" endWordPosition="189">entences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 1 Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 Dragomir R. Radev Department of EECS and School of Information University of Michigan Ann Arbor, MI radev@umich.edu However, the citatio</context>
<context position="6010" citStr="Mohammad et al., 2009" startWordPosition="953" endWordPosition="956">show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers </context>
<context position="28615" citStr="Mohammad et al., 2009" startWordPosition="4875" endWordPosition="4878">sed features and similarity to the reference. For each paper, we use BP4 to perform 3 experiments: two in absence of each feature and one including all features. Figure 4 shows the average F,3=3 for each experiment. This plot shows that the features lead to better results when used together. 6 Impact on Survey Generation We also performed an extrinsic evaluation of our context extraction methodology. Here we show how context sentences add important surveyworthy information to explicit citations. Previous work that generate surveys of scientific topics use the text of citation sentences alone (Mohammad et al., 2009; Qazvinian and Radev, 2008). Here, we show how the surveys generated using citations and their context sentences are better than those generated using citation sentences alone. We use the data from (Mohammad et al., 2009) 561 ... Naturally, our current work on question answering for the reading comprehension task is most related to those of (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000 ; Wang et al. , 2000). In fact, all of this body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the same development set of s</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 584–592, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In IJCAI1999,</booktitle>
<pages>926--931</pages>
<contexts>
<context position="5513" citStr="Nanba and Okumura, 1999" startWordPosition="874" endWordPosition="877">nalyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Te</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In IJCAI1999, pages 926–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Takeshi Abekawa</author>
<author>Manabu Okumura</author>
<author>Suguru Saito</author>
</authors>
<title>Bilingual presri: Integration of multiple research paper databases.</title>
<date>2004</date>
<booktitle>In Proceedings of RIAO 2004,</booktitle>
<pages>195--211</pages>
<location>Avignon, France.</location>
<contexts>
<context position="5533" citStr="Nanba et al., 2004" startWordPosition="878" endWordPosition="881"> scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan an</context>
</contexts>
<marker>Nanba, Abekawa, Okumura, Saito, 2004</marker>
<rawString>Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura, and Suguru Saito. 2004a. Bilingual presri: Integration of multiple research paper databases. In Proceedings of RIAO 2004, pages 195–211, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Noriko Kando</author>
<author>Manabu Okumura</author>
</authors>
<title>Classification of research papers using citation links and citation types: Towards automatic review article generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 11th SIG Classification Research Workshop,</booktitle>
<pages>117--134</pages>
<location>Chicago, USA.</location>
<contexts>
<context position="5533" citStr="Nanba et al., 2004" startWordPosition="878" endWordPosition="881"> scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan an</context>
</contexts>
<marker>Nanba, Kando, Okumura, 2004</marker>
<rawString>Hidetsugu Nanba, Noriko Kando, and Manabu Okumura. 2004b. Classification of research papers using citation links and citation types: Towards automatic review article generation. In Proceedings of the 11th SIG Classification Research Workshop, pages 117–134, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E J Newman</author>
</authors>
<title>The structure of scientific collaboration networks.</title>
<date>2001</date>
<tech>PNAS, 98(2):404–409.</tech>
<contexts>
<context position="5106" citStr="Newman, 2001" startWordPosition="809" endWordPosition="810">e Learning To Paraphrase: An Unsupervised ... 2003 26 13 203 N03-2016∗ Kondrak et, al. Cognates Can Improve Statistical Translation ... 2003 8 5 92 Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publication year, number of references (in AAN) and number of sentences. Papers marked with * are used to calculate inter-judge agreement. 2 Prior Work Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has bee</context>
</contexts>
<marker>Newman, 2001</marker>
<rawString>Mark E. J. Newman. 2001. The structure of scientific collaboration networks. PNAS, 98(2):404–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In COLING 2008,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1278" citStr="Qazvinian and Radev, 2008" startWordPosition="182" endWordPosition="185"> to detect likely context sentences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 1 Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 Dragomir R. Radev Department of EECS and School of Information University of Michigan Ann Arbor, MI radev@umich.e</context>
<context position="5966" citStr="Qazvinian and Radev, 2008" startWordPosition="945" endWordPosition="948">citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method fo</context>
<context position="28643" citStr="Qazvinian and Radev, 2008" startWordPosition="4879" endWordPosition="4882">rity to the reference. For each paper, we use BP4 to perform 3 experiments: two in absence of each feature and one including all features. Figure 4 shows the average F,3=3 for each experiment. This plot shows that the features lead to better results when used together. 6 Impact on Survey Generation We also performed an extrinsic evaluation of our context extraction methodology. Here we show how context sentences add important surveyworthy information to explicit citations. Previous work that generate surveys of scientific topics use the text of citation sentences alone (Mohammad et al., 2009; Qazvinian and Radev, 2008). Here, we show how the surveys generated using citations and their context sentences are better than those generated using citation sentences alone. We use the data from (Mohammad et al., 2009) 561 ... Naturally, our current work on question answering for the reading comprehension task is most related to those of (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000 ; Wang et al. , 2000). In fact, all of this body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the same development set of stories. The work of (Hirschm</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In COLING 2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The ACL anthology network corpus.</title>
<date>2009</date>
<booktitle>In ACL workshop on Natural Language Processing and Information Retrievalfor Digital Libraries.</booktitle>
<contexts>
<context position="7470" citStr="Radev et al., 2009" startWordPosition="1185" endWordPosition="1188">al tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). 3 Data The ACL Anthology Network (AAN)2 is a collection of papers from the ACL Anthology3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences. To build a corpus for our experiments we picked 10 recently published papers from various areas in NLP4, each of which had references for a total of 203 candidate paper-reference pairs. Table 1 lists these papers together with their authors, titles, publication year, number of references, number of references within AAN, and the number of sen2http://clair.si.umich.edu/clair/anth</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The ACL anthology network corpus. In ACL workshop on Natural Language Processing and Information Retrievalfor Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Romanello</author>
<author>Federico Boschetti</author>
<author>Gregory Crane</author>
</authors>
<title>Citations in the digital library of classics: Extracting canonical references by using conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>80--87</pages>
<institution>Suntec City, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="6902" citStr="Romanello et al., 2009" startWordPosition="1096" endWordPosition="1099">d, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). 3 Data The ACL Anthology Network (AAN)2 is a collection of papers from the ACL Anthology3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation netw</context>
</contexts>
<marker>Romanello, Boschetti, Crane, 2009</marker>
<rawString>Matteo Romanello, Federico Boschetti, and Gregory Crane. 2009. Citations in the digital library of classics: Extracting canonical references by using conditional random fields. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 80–87, Suntec City, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Simone Teufel</author>
</authors>
<title>Whose idea was this, and why does it matter? attributing scientific work to citations.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT-07.</booktitle>
<contexts>
<context position="1230" citStr="Siddharthan and Teufel, 2007" startWordPosition="173" endWordPosition="177">a create, and employ a Belief Propagation mechanism to detect likely context sentences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 1 Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 Dragomir R. Radev Department of EECS and School of Information Un</context>
<context position="6147" citStr="Siddharthan and Teufel, 2007" startWordPosition="973" endWordPosition="976">a et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models h</context>
</contexts>
<marker>Siddharthan, Teufel, 2007</marker>
<rawString>Advaith Siddharthan and Simone Teufel. 2007. Whose idea was this, and why does it matter? attributing scientific work to citations. In Proceedings of NAACL/HLT-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="5190" citStr="Teufel and Moens, 2002" startWordPosition="819" endWordPosition="822">drak et, al. Cognates Can Improve Statistical Translation ... 2003 8 5 92 Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publication year, number of references (in AAN) and number of sentences. Papers marked with * are used to calculate inter-judge agreement. 2 Prior Work Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citation</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Comput. Linguist., 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="5091" citStr="Teufel et al., 2006" startWordPosition="805" endWordPosition="808">03-1003 Barzilay &amp; Lee Learning To Paraphrase: An Unsupervised ... 2003 26 13 203 N03-2016∗ Kondrak et, al. Cognates Can Improve Statistical Translation ... 2003 8 5 92 Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publication year, number of references (in AAN) and number of sentences. Papers marked with * are used to calculate inter-judge agreement. 2 Prior Work Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific ci</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classification of citation function. In Proceedings of the EMNLP, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Argumentative Zoning for Improved Citation Indexing. Computing Attitude and Affect in Text: Theory and Applications,</title>
<date>2005</date>
<pages>159--170</pages>
<contexts>
<context position="6162" citStr="Teufel, 2005" startWordPosition="977" endWordPosition="978"> 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citations to determine the content of articles. Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (Siddharthan and Teufel, 2007; Teufel, 2005) categorize sentences according to their role in the author’s argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of</context>
</contexts>
<marker>Teufel, 2005</marker>
<rawString>Simone Teufel. 2005. Argumentative Zoning for Improved Citation Indexing. Computing Attitude and Affect in Text: Theory and Applications, pages 159– 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan S Yedidia</author>
<author>William T Freeman</author>
<author>Yair Weiss</author>
</authors>
<title>Understanding belief propagation and its generalizations.</title>
<date>2003</date>
<pages>239--269</pages>
<contexts>
<context position="16701" citStr="Yedidia et al., 2003" startWordPosition="2772" endWordPosition="2775">neighbors, and independent of any other node in the graph given its neighbors. Dependencies in an MRF are represented using two functions: Compatibility function (ψ) and Potential function (φ). ψuv(xc, xd) shows the edge potential of an edge between two nodes u, v of classes xc and xd. Large values of ψuv would indicate a strong association between xc and xd at nodes u, v. The Potential function, φi(xc, yc), shows the statistical dependency between xc and yc at each node i assumed by the MRF model. In order to find the marginal probabilities of xis in a MRF we can use Belief Propagation (BP) (Yedidia et al., 2003). If we assume the yis are fixed and show φi(xi, yi) by φi(xi), we can find the joint probability distribution for unknown variables xi as 1 � p({x}) = Z In the BP algorithm a set of new variables m is introduced where mij(xj) is the message passed from i to j about what state xj should be in. Each message, mij(xj), is a vector with the same dimensionality of xj in which each dimension shows i’s opinion about j being in the corresponding class. Therefore each message could be considered as a probability distribution and its components should sum up to 1. The final belief at a 1 2 3 4 5 6 7 8 9</context>
</contexts>
<marker>Yedidia, Freeman, Weiss, 2003</marker>
<rawString>Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. 2003. Understanding belief propagation and its generalizations. pages 239–269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>