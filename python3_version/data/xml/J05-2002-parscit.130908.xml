<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999727">
A General Technique to Train Language
Models on Language Models
</title>
<author confidence="0.997391">
Mark-Jan Nederhof*
</author>
<affiliation confidence="0.989223">
University of Groningen
</affiliation>
<bodyText confidence="0.988876833333333">
We show that under certain conditions, a language model can be trained on the basis of a
second language model. The main instance of the technique trains a finite automaton on the
basis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between
grammar and trained automaton is provably minimal. This is a substantial generalization of
an existing algorithm to train an n-gram model on the basis of a probabilistic context-free
grammar.
</bodyText>
<sectionHeader confidence="0.996885" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99997825">
In this article, the term language model is used to refer to any description that assigns
probabilities to strings over a certain alphabet. Language models have important
applications in natural language processing, and in particular, in speech recognition
systems (Manning and Sch¨utze 1999).
Language models often consist of a symbolic description of a language, such as
a finite automaton (FA) or a context-free grammar (CFG), extended by a probability
assignment to, for example, the transitions of the FA or the rules of the CFG, by which
we obtain a probabilistic finite automaton (PFA) or probabilistic context-free grammar
(PCFG), respectively. For certain applications, one may first determine the symbolic part
of the automaton or grammar and in a second phase try to find reliable probability
estimates for the transitions or rules. The current article is involved with the second
problem, that of extending FAs or CFGs to become PFAs or PCFGs. We refer to this
process as training.
Training is often done on the basis of a corpus of actual language use in a certain
domain. If each sentence in this corpus is annotated by a list of transitions of an
FA recognizing the sentence or a parse tree for a CFG generating the sentence, then
training may consist simply in relative frequency estimation. This means that we estimate
probabilities of transitions or rules by counting their frequencies in the corpus, relative
to the frequencies of the start states of transitions or to the frequencies of the left-hand
side nonterminals of rules, respectively. By this estimation, the likelihood of the corpus
is maximized.
The technique we introduce in this article is different in that training is done on
the basis not of a finite corpus, but of an input language model. Our goal is to find
estimations for the probabilities of transitions or rules of the input FA or CFG such that
</bodyText>
<note confidence="0.786792">
* Faculty of Arts, Humanities Computing, P.O. Box 716, NL-9700 AS Groningen, The Netherlands.
</note>
<email confidence="0.844337">
E-mail: markjan@let.rug.nl.
</email>
<note confidence="0.9429435">
Submission received: 20th January 2004; Revised submission received: 5th August 2004; Accepted for
publication: 19th September 2004
© 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 2
</note>
<bodyText confidence="0.999824891304348">
the resulting PFA or PCFG approximates the input language model as well as possible,
or more specifically, such that the Kullback-Leibler (KL) distance (or relative entropy)
between the input model and the trained model is minimized. The input FA or CFG to
be trained may be structurally unrelated to the input language model.
This technique has several applications. One is an extension with probabilities
of existing work on approximation of CFGs by means of FAs (Nederhof 2000). The
motivation for this work was that application of FAs is generally less costly than
application of CFGs, which is an important benefit when the input is very large, as
is often the case in, for example, speech recognition systems. The practical relevance of
this work was limited, however, by the fact that in practice one is more interested in
the probabilities of sentences than in a purely Boolean distinction between grammatical
and ungrammatical sentences.
Several approaches were discussed by Mohri and Nederhof (2001) to extend this
work to approximation of PCFGs by means of PFAs. A first approach is to directly map
rules with attached probabilities to transitions with attached probabilities. Although
this is computationally the easiest approach, the resulting PFA may be a very inaccurate
approximation of the probability distribution described by the input PCFG. In particu-
lar, there may be assignments of probabilities to the transitions of the same FA that lead
to more accurate approximating language models.
A second approach is to train the approximating FA by means of a corpus. If
the input PCFG was itself obtained by training on a corpus, then we already possess
training material. However, this may not always be the case, and no training material
may be available. Furthermore, as a determinized approximating FA may be much
larger than the input PCFG, the sparse-data problem may be more severe for the
automaton than it was for the grammar.1 Hence, even if sufficient material was available
to train the CFG, it may not be sufficient to accurately train the FA.
A third approach is to construct a training corpus from the PCFG by means of
a (pseudo)random generator of sentences, such that sentences that are more likely
according to the PCFG are generated with greater likelihood. This has been proposed
by Jurafsky et al. (1994), for the special case of bigrams, extending a nonprobabilistic
technique by Zue et al. (1991). It is not clear, however, whether this idea is feasible
for training of finite-state models that are larger than bigrams. The reason is that
very large corpora would have to be generated in order to obtain accurate probability
estimates for the PFA. Note that the number of parameters of a bigram model is
bounded by the square of the size of the lexicon; such a bound does not exist for
general PFAs.
The current article discusses a fourth approach. In the limit, it is equivalent to the
third approach above, as if an infinite corpus were constructed on which the PFA is
trained, but we have found a way to avoid considering sentences individually. The key
idea that allows us to handle an infinite set of strings generated by the PCFG is that we
construct a new grammar that represents the intersection of the languages described by
the input PCFG and the FA. Within this new grammar, we can compute the expected
frequencies of transitions of the FA, using a fairly standard analysis of PCFGs. These
expected frequencies then allow us to determine the assignment of probabilities to
transitions of the FA that minimizes the KL distance between the PCFG and the resulting
PFA.
</bodyText>
<footnote confidence="0.985393">
1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized
approximating FAs that can be much larger than the input CFGs.
</footnote>
<page confidence="0.993953">
174
</page>
<note confidence="0.629992">
Nederhof Training Models on Models
</note>
<bodyText confidence="0.99997978125">
The only requirement is that the FA to be trained be unambiguous, by which we
mean that each input string can be recognized by at most one computation of the FA.
The special case of n-grams has already been formulated by Stolcke and Segal (1994),
realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is
here seen as a (P)FA that contains exactly one state for each possible history of the n − 1
previously read symbols. It is clear that such an FA is unambiguous (even deterministic)
and that our technique therefore properly subsumes the technique by Stolcke and Segal
(1994), although the way that the two techniques are formulated is rather different. Also
note that the FA underlying an n-gram model accepts any input string over the alphabet,
which does not hold for general (unambiguous) FAs.
Another application of our work involves determinization and minimization of
PFAs. As shown by Mohri (1997), PFAs cannot always be determinized, and no practical
algorithms are known to minimize arbitrary nondeterministic (P)FAs. This can be a
problem when deterministic or small PFAs are required. We can, however, always
compute a minimal deterministic FA equivalent to an input FA. The new results in this
article offer a way to extend this determinized FA to a PFA such that it approximates
the probability distribution described by the input PFA as well as possible, in terms of
the KL distance.
Although the proposed technique has some limitations, in particular, that the model
to be trained is unambiguous, it is by no means restricted to language models based on
finite automata or context-free grammars, as several other probabilistic grammatical
formalisms can be treated in a similar manner.
The structure of this article is as follows. We provide some preliminary definitions
in Section 2. Section 3 discusses how the expected frequency of a rule in a PCFG can be
computed. This is an auxiliary step in the algorithms to be discussed below. Section 4
defines a way to combine a PFA and a PCFG into a new PCFG that extends a well-known
representation of the intersection of a regular and a context-free language. Thereby
we merge the input model and the model to be trained into a single structure. This
structure is the foundation for a number of algorithms, presented in section 5, which
allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1),
training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an
unambiguous FA on the basis of a PFA (section 5.3).
</bodyText>
<sectionHeader confidence="0.974172" genericHeader="keywords">
2. Preliminaries
</sectionHeader>
<bodyText confidence="0.9969622">
Many of the definitions on probabilistic context-free grammars are based on Santos
(1972) and Booth and Thompson (1973), and the definitions on probabilistic finite
automata are based on Paz (1971) and Starke (1972).
A context-free grammar g is a 4-tuple (E, N, S, R), where E and N are two finite
disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and
R is a finite set of rules, each of the form A --� o, where A E N and o E (E U N)*. A
probabilistic context-free grammar g is a 5-tuple (E, N, S, R, pg), where E, N, S and R
are as above, and pg is a function from rules in R to probabilities.
In what follows, symbol a ranges over the set E, symbols w, v range over the
set E*, symbols A, B range over the set N, symbol X ranges over the set E U N,
symbols o, (3,-y range over the set (E U N)*, symbol p ranges over the set R, and
symbols d,e range over the set R*. With slight abuse of notation, we treat a rule
p = (A --� o) E R as an atomic symbol when it occurs within a string dpe E R*.
The symbol c denotes the empty string. String concatenation is represented by
operator · or by empty space.
</bodyText>
<page confidence="0.993995">
175
</page>
<note confidence="0.301759">
Computational Linguistics Volume 31, Number 2
</note>
<bodyText confidence="0.99804825">
For a fixed (P)CFG G, we define the relation ⇒ on triples consisting of two strings
α, β ∈ (Σ ∪ N)∗ and a rule ρ ∈ R by α ⇒ρ β, if and only if α is of the form wAδ and β
is of the form wγδ, for some w ∈ Σ∗ and δ ∈ (Σ ∪ N)∗, and ρ = (A → γ). A leftmost
derivation (in G) is a string d = ρ1 · · · ρm, m ≥ 0, such that α0 ⇒ α1
</bodyText>
<equation confidence="0.93969375">
ρ1 ⇒ · · · ρm
ρ2 ⇒ αm, for
some α0,..., αm ∈ (Σ ∪ N)∗; d = c is always a leftmost derivation. In the remainder
of this article, we let the term derivation refer to leftmost derivation, unless spec-
ified otherwise. If α0 ⇒ · · · ρm
ρ1 ⇒ αm for some α0,..., αm ∈ (Σ ∪ N)∗, then we say that
d = ρ1 · · · ρm derives αm from α0, and we write α0 ⇒ αm; � derives any α0 ∈ (Σ ∪ N)∗
d
</equation>
<bodyText confidence="0.955618393939394">
from itself. A derivation d such that S ⇒d w, for some w ∈ Σ∗, is called a complete
derivation. We say that G is unambiguous if for each w ∈ Σ∗, S ⇒ d w for at most
one d ∈ R∗.
Let G be a fixed PCFG (Σ, N, S, R, pG). For α, β ∈ (Σ ∪ N)∗ and d = ρ1 · · · ρm ∈ R∗,
m ≥ 0, we define pG(α ⇒d β) = fIm 1 pG (ρi) if α ⇒d β, and pG (α d⇒ β) = 0 otherwise. The
probability pG(w) of a string w ∈ Σ∗ is defined to be Ed pG(S d⇒ w).
PCFG G is said to be proper if E ρ,α pG(A ⇒ρ α) = 1 for all A ∈ N, that is, if the
probabilities of all rules ρ = (A → α) with left-hand side A sum to one. PCFG G is said to
be consistent if Ew pG(w) = 1. Consistency implies that the PCFG defines a probability
distribution on the set of terminal strings. There is a practical sufficient condition for
consistency that is decidable (Booth and Thompson 1973).
A PCFG is said to be reduced if for each nonterminal A, there are d1, d2 ∈ R∗,
w1,w2 ∈ Σ∗, and β ∈ (Σ ∪ N)∗ such that pG(S d1⇒ w1Aβ) · pG(w1Aβ d2⇒ w1w2) &gt; 0. In
words, if a PCFG is reduced, then for each nonterminal A, there is at least one derivation
d1d2 with nonzero probability that derives a string w1w2 from S and that includes
some rule with left-hand side A. A PCFG G that is not reduced can be turned into
one that is reduced and that describes the same probability distribution, provided that
Ew pG(w) &gt; 0. This reduction consists in removing from the grammar any nonterminal
A for which the above conditions do not hold, together with any rule that contains
such a nonterminal; see Aho and Ullman (1972) for reduction of CFGs, which is very
similar.
A finite automaton M is a 5-tuple (Σ, Q, q0, qf, T), where Σ and Q are two
finite sets of terminals and states, respectively, q0, qf ∈ Q are the initial and final
states, respectively, and T is a finite set of transitions, each of the form r a
�→ s, where
r ∈ Q − {qf},s ∈ Q, and a ∈ Σ.2 A probabilistic finite automaton M is a 6-tuple (Σ, Q,
q0, qf, T, pM), where Σ, Q, q0, qf, and T are as above, and pM is a function from transitions
in T to probabilities.
In what follows, symbols q, r, s range over the set Q, symbol τ ranges over the set T,
and symbol c ranges over the set T∗.
For a fixed (P)FA M, we define a configuration to be an element of Q × Σ∗, and we
define the relation �- on triples consisting of two configurations and a transition τ ∈ T
by (r, w) τ� (s, w~) if and only if w is of the form aw�, for some a ∈ Σ, and τ = (r a
</bodyText>
<equation confidence="0.954437">
�→ s).
τ1
�- (r1, w1)
</equation>
<bodyText confidence="0.8888866">
2 That we only allow one final state is not a serious restriction with regard to the set of strings we can
process; only when the empty string is to be recognized could this lead to difficulties. Lifting the
restriction would encumber the presentation with treatment of additional cases without affecting,
however, the validity of the main results.
A computation (in M) is a string c = τ1 · · · τm, m ≥ 0, such that (r0,w0)
</bodyText>
<equation confidence="0.819476666666667">
τ2 τm
� · · · �(rm, wm), for some (r0, w0), ..., (rm, wm) ∈ Q × Σ∗; c = c is always a compu-
τ1 τm
</equation>
<bodyText confidence="0.922851">
tation. If (r0, w0) � · · · � (rm, wm) for some (r0, w0), ... , (rm, wm) ∈ Q × Σ∗ and c = τ1 · · ·
τm ∈ T∗, then we write (r0,w0) �c- (rm,wm). We say that c recognizes w if (q0, w) �c- (qf, c).
</bodyText>
<page confidence="0.975694">
176
</page>
<note confidence="0.563884">
Nederhof Training Models on Models
</note>
<bodyText confidence="0.870642076923077">
Let M be a fixed FA (E, Q, q0, qf, T). The language L(M) accepted by M is
defined to be {w ∈ E∗  |∃c[(qo,w) c� (qf, c)]}. We say M is unambiguous if for each
w ∈ E∗, (q0,w) �c- (qf, c) for at most one c ∈ T∗. We say M is deterministic if for each
(r,w) ∈ Q × E∗, there is at most one combination of τ ∈ T and (s,w&apos;) ∈ Q × E∗ such
that (r,w) τ (s,w&apos;). Turning a given FA into one that is deterministic and accepts the
same language is called determinization. All FAs can be determinized. Turning a given
(deterministic) FA into the smallest (deterministic) FA that accepts the same language
is called minimization. There are effective algorithms for minimization of deterministic
FAs.
Let M be a fixed PFA (E, Q, q0, qf, T, pM). For (r, w), (s, v) ∈ Q × E∗ and
c = τ1 · · · τm ∈ T∗, we define pM((r, w) �c- (s, v)) = flmi=1 pM(τi) if (r, w) �c- (s, v), and
pM((r, w) �c- (s, v)) = 0 otherwise. The probability pM(w) of a string w ∈ E∗ is defined
to be EcpM((q0,w) �c- (qf, c)).
</bodyText>
<equation confidence="0.9964685">
PFA M is said to be proper if Er,a,s: r=(r a
�→s)∈T pM(τ) = 1 for all r ∈ Q − {qf}.
</equation>
<sectionHeader confidence="0.957649" genericHeader="introduction">
3. Expected Frequencies of Rules
</sectionHeader>
<bodyText confidence="0.9993385">
Let G be a PCFG (E, N, S, R, pG). We assume without loss of generality that S does not
occur in the right-hand side of any rule from R. For each rule ρ, we define
</bodyText>
<equation confidence="0.84977825">
� pG(S dρdI
E(ρ) = ⇒ w) (1)
d,dI,w
If G is proper and consistent, (1) is the expected frequency of ρ in a complete derivation.
Each complete derivation dρd~ can be written as dρd&amp;quot;d&amp;quot;&apos;, with d&apos; = d&amp;quot;d&amp;quot;&apos;, where
Sd⇒w&apos;Aβ,A⇒ρα,α dII ⇒ w��,βdIII ⇒ w~~~ (2)
for some A, α, β, w&apos;, w&amp;quot;, and w&amp;quot;&apos;. Therefore
E(ρ) = outer(A) · pG(ρ) · inner(α) (3)
</equation>
<bodyText confidence="0.986539">
where we define
</bodyText>
<equation confidence="0.998024">
�outer(A) = pG(S d⇒ w&apos;Aβ) · pG(β dIII⇒ w&amp;quot;i) (4)
d,wI,P,dIII,wIII
�inner(α) = pG(α dII⇒ w&amp;quot;) (5)
dII,wII
for each A ∈ N and α ∈ (E ∪ N)∗. From the definition of inner, we can easily derive the
following equations:
inner(a) = 1 (6)
�inner(A) = pG(ρ) · inner(α) (7)
ρ,α:
ρ=(A—α)
inner(Xβ) = inner(X) · inner(β) (8)
</equation>
<page confidence="0.975643">
177
</page>
<note confidence="0.476011">
Computational Linguistics Volume 31, Number 2
</note>
<bodyText confidence="0.9724625">
This can be taken as a recursive definition of inner, assuming R =~ C in (8). Similarly, we
can derive a recursive definition of outer:
</bodyText>
<equation confidence="0.99901225">
outer(S) = 1 (9)
�outer(A) = outer(B) · pG(p) · inner(o) · inner(p) (10)
P,B,α,R:
P=(B→αAR)
</equation>
<bodyText confidence="0.973503333333333">
for A =~ S.
In general, there may be cyclic dependencies in the equations for inner and outer;
that is, for certain nonterminals A, inner(A) and outer(A) may be defined in terms
of themselves. There may even be no closed-form expression for inner(A). However,
one may approximate the solutions to arbitrary precision by means of fixed-point
iteration.
</bodyText>
<sectionHeader confidence="0.908466" genericHeader="method">
4. Intersection of Context-Free and Regular Languages
</sectionHeader>
<bodyText confidence="0.999022428571429">
We recall a construction from Bar-Hillel, Perles, and Shamir (1964) that computes the
intersection of a context-free language and a regular language. The input consists of a
CFG G = (Σ, N, S, R) and an FA M = (Σ, Q, q0, qf, T); note that we assume, without loss
of generality, that G and M share the same set of terminals Σ.
The output of the construction is CFG Gn = (Σ, Nn, Sn, Rn), where Nn = Q ×
(Σ ∪ N) × Q, Sn = (q0, S, qf ), and Rn consists of the set of rules that is obtained as
follows:
</bodyText>
<listItem confidence="0.906776857142857">
• For each rule p = (A → X1 · · · Xm) ∈ R, m ≥ 0, and each sequence of states
r0, ... , rm ∈ Q, let the rule pn = ((r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm))
be in Rn; for m = 0, Rn contains a rule pn = ((r0,A,r0) → C) for each
state r0.
• For each transition T = (r a
�→ s) ∈ T, let the rule pn = ((r,a,s) → a) be
in Rn.
</listItem>
<bodyText confidence="0.995353285714286">
Note that for each rule (r0, A, rm) → (r0, X1, r1) · · · (rm−1, Xm, rm) from Rn, there is a
unique rule A → X1 · · · Xm from R from which it has been constructed by the above.
Similarly, each rule (r, a, s) → a uniquely identifies a transition r a
�→ s. This means that if
we take a derivation dn in Gn, we can extract a sequence h1(dn) of rules from G and a
sequence h2(dn) of transitions from M, where h1 and h2 are string homomorphisms that
we define pointwise as
</bodyText>
<equation confidence="0.9772315">
h1(pn) = p if pn = ((r0,A,rm) → (r0,X1,r1)··· (rm−1, Xm, rm))
and p = (A → X1 ··· Xm)
C if pn = ((r,a,s) → a)
�→ s)
h2(pn) = T if pn = ((r,a,s) → a) and T = (r a
C if pn = ((r0,A,rm) → (r0,X1,r1)··· (rm−1, Xm, rm))
</equation>
<page confidence="0.986596">
178
</page>
<note confidence="0.796166">
Nederhof Training Models on Models
</note>
<bodyText confidence="0.996076571428571">
We define h(dn) = (h1(dn),h2(dn)). It can be easily shown that if h(dn) = (d,c) and
Sn d∩� w, then for the same w, we have S 4 w and (q0, w) �c_ (qf, C). Conversely, if for some
w, d, and c we have S d� w and (q0, w) c� (qf, C), then there is precisely one derivation dn
such that h(dn) = (d, c) and Sn d∩� w.
It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a
compact representation of all parse trees according to G that derive strings recognized
by M. The construction can be generalized to, for example, tree-adjoining grammars
(Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000;
Bertsch and Nederhof 2001). The construction for the latter also has implications for
linear context-free rewriting systems (Seki et al. 1991).
The construction has been extended by Nederhof and Satta (2003) to apply to a
PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is a
PCFG Gn = (Σ, Nn, Sn, Rn, pn), where Nn, Sn, and Rn are as before, and pn is
defined by
</bodyText>
<equation confidence="0.999201333333333">
pn((r0,A,rm) → (r0,X1,r1)··· (rm−1,Xm,rm)) = pG(A → X1 ···Xm) (15)
pn((r,a,s) → a) = pM(r a
�→ s) (16)
</equation>
<bodyText confidence="0.831663">
If dn, d, and c are such that h(dn) = (d, c), then clearly pn(dn) = pG(d) · pM(c).
</bodyText>
<sectionHeader confidence="0.807605" genericHeader="method">
5. Training Models on Models
</sectionHeader>
<bodyText confidence="0.999196">
We restrict ourselves to a few cases of the general technique of training a model on the
basis of another model.
</bodyText>
<subsectionHeader confidence="0.991864">
5.1 Training a PFA on a PCFG
</subsectionHeader>
<bodyText confidence="0.999934769230769">
Let us assume we have a proper and consistent PCFG G = (Σ, N, S, R, pG) and an FA
M = (Σ, Q, q0, qf, T) that is unambiguous. This FA may have resulted from (nonprob-
abilistic) approximation of CFG (Σ, N, S, R), but it may also be totally unrelated to G.
Note that an FA is guaranteed to be unambiguous if it is deterministic; any FA can be
determinized. Our goal is now to assign probabilities to the transitions from FA M to
obtain a proper PFA that approximates the probability distribution described by G as
well as possible.
Let us define 1 as the function that maps each transition from T to one. This means
that for each r, w, c and s, 1((r, w) �c_ (s, C)) = 1 if (r, w) �c_ (s, C), and 1((r, w) �c_ (s, C)) = 0
otherwise.
Of the set of strings generated by G, a subset is recognized by computations of M;
note again that there can be at most one such computation for each string. The expected
frequency of a transition -r in such computations is given by
</bodyText>
<equation confidence="0.9434576">
�E(-r) = pG(w) · 1((q0,w) cτc&apos;
w,c,c&apos; �- (qf, C)) (17)
Now we construct the PCFG Gn as explained in section 4 from the PCFG G and the
PFA (Σ, Q, q0, qf, T, 1). Let -r = (r a
�→ s) ∈ T and p = ((r, a, s) → a). On the basis of the
</equation>
<page confidence="0.98454">
179
</page>
<note confidence="0.417171">
Computational Linguistics Volume 31, Number 2
</note>
<bodyText confidence="0.544875">
properties of function h, we can now rewrite E(-r) as
</bodyText>
<equation confidence="0.976760666666667">
EE(-r) = c&apos;rc,
d,w,c,cl pG(S ⇒d w) · 1((q0,w) �- (qf, c))
E= c&apos;rc,
pG(S d⇒ w) · 1((q0,w) �- (qf, c))
e,d,w,c,cI:
h(e)=(d,cτcI)
E= eρe�
e,el,w pn(Sn ⇒ w)
= E(p) (18)
</equation>
<bodyText confidence="0.946294333333333">
Hereby we have expressed the expected frequency of a transition -r = (r a
�→ s) in
terms of the expected frequency of rule p = ((r,a,s) → a) in derivations in PCFG Gn.
It was explained in section 3 how such a value can be computed. Note that since
by definition I(-r) = 1, also pn(p) = 1. Furthermore, for the right-hand side a of p,
inner(a) = 1. Therefore,
</bodyText>
<equation confidence="0.898102">
E(-r) = outer((r, a, s)) · pn(p) · inner(a)
= outer((r, a, s)) (19)
</equation>
<bodyText confidence="0.990046">
To obtain the required PFA (E, Q, q0, qf, T, pM), we now define the probability
function pM for each -r = (r a
</bodyText>
<equation confidence="0.97859975">
�→ s) ∈ T as
outer((r, a, s))
pM(-r) = (20)
a�,s�:(r a•��s )�T outer((r, a&apos;, s&apos;))
</equation>
<bodyText confidence="0.950840333333333">
That such a relative frequency estimator pM minimizes the KL distance between pG and
pM on the domain L(M) is proven in the appendix.
An example with finite languages is given in Figure 1. We have, for example,
</bodyText>
<equation confidence="0.99246">
a outer((q0,a,q1))
pM(q0 �→ q1) =
outer((q0, a, q1)) + outer((q0, c, q1))
1
= (21)
</equation>
<page confidence="0.73529">
3
</page>
<figure confidence="0.9174448">
1
3
=
1 + 2
3 3
</figure>
<subsectionHeader confidence="0.996375">
5.2 Training a PCFG on a PFA
</subsectionHeader>
<bodyText confidence="0.99945">
Similarly to section 5.1, we now assume we have a proper PFA M = (E, Q, q0,
qf, T, pM) and a CFG G = (E, N, S, R) that is unambiguous. Our goal is to find a
function pG that lets proper and consistent PCFG (E, N, S, R, pG) approximate M as
well as possible. Although CFGs used for natural language processing are usually
ambiguous, there may be cases in other fields in which we may assume grammars are
unambiguous.
</bodyText>
<page confidence="0.995197">
180
</page>
<note confidence="0.863871">
Nederhof Training Models on Models
</note>
<figureCaption confidence="0.993152">
Figure 1
</figureCaption>
<bodyText confidence="0.9030732">
Example of input PCFG G, with rule probabilities between square brackets, input FA M, the
reduced PCFG Gn, and the resulting trained PFA.
Let us define 1 as the function that maps each rule from R to one. Of the set of
strings recognized by M, a subset can be derived in G. The expected frequency of a rule
p in those derivations is given by
</bodyText>
<equation confidence="0.9906321">
� pM(w) · 1(S dρd&apos;
E(p) = ⇒ w) (22)
d,dl,w
Now we construct the PCFG Gn from the PCFG G = (Σ, N, S, R, 1) and the
PFA M as explained in section 4. Analogously to section 5.1, we obtain for each
p = (A → X1 ··· Xm)
E(p) = � E((r0,A,rm) → (r0,X1,r1)··· (rm−1, Xm, rm))
r0,r1,...,rm
�= outer((r0, A, rm)) · inner((r0, X1, r1) ··· (rm−1, Xm, rm)) (23)
r0,r1,...,rm
</equation>
<bodyText confidence="0.9956335">
To obtain the required PCFG (Σ, N, S, R, pG), we now define the probability function
pG for each p = (A → o) as
</bodyText>
<equation confidence="0.994796333333333">
E(p)
pG(p) = � (24)
ρ&apos;=(A—α&apos;)ER E(pl)
</equation>
<bodyText confidence="0.985914666666667">
The proof that this relative frequency estimator pG minimizes the KL distance between
pM and pG on the domain L(G) is almost identical to the proof in the appendix for a
similar claim from section 5.1.
</bodyText>
<subsectionHeader confidence="0.965172">
5.3 Training a PFA on a PFA
</subsectionHeader>
<bodyText confidence="0.906857">
We now assume we have a proper PFA M1 = (Σ, Q1, q0,1, qf,1, T1, p1) and an FA
M2 = (Σ, Q2, q0,2, qf,2, T2) that is unambiguous. Our goal is to find a function p2 so that
</bodyText>
<page confidence="0.991875">
181
</page>
<note confidence="0.511937">
Computational Linguistics Volume 31, Number 2
</note>
<bodyText confidence="0.9428831">
proper PFA (Σ, Q2, q0,2, qf,2, T2, p2) approximates M1 as well as possible, minimizing
the KL distance between p1 and p2 on the domain L(M2).
One way to solve this problem is to map M2 to an equivalent right-linear CFG G and
then to apply the algorithm from section 5.2. The obtained probability function pG can
be translated back to an appropriate function p2. For this special case, the construction
from section 4 can be simplified to the “cross-product” construction of finite automata
(see, e.g., Aho and Ullman 1972). The simplified forms of the functions inner and outer
from section 3 are commonly called forward and backward, respectively, and they are
defined by systems of linear equations. As a result, we can compute exact solutions, as
opposed to approximate solutions by iteration.
</bodyText>
<sectionHeader confidence="0.950778" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.998387">
We now prove that the choice of pM in section 5.1 is such that it minimizes the Kullback-
Leibler distance between pG and pM, restricted to the domain L(M). Without this
restriction, the KL distance is given by
</bodyText>
<equation confidence="0.985701666666667">
� pG(w) · log pG(w) (25)
D(pG�pM) = pM(w)
w
</equation>
<bodyText confidence="0.9888606">
This can be used for many applications mentioned in section 1. For example, an FA M
approximating a CFG G is guaranteed to be such that L(M) ⊇ L(G) in the case of most
practical approximation algorithms. However, if there are strings w such that w ∈/ L(M)
and pG(w) &gt; 0, then (25) is infinite, regardless of the choice of pM. We therefore restrict
pG to the domain L(M) and normalize it to obtain
</bodyText>
<equation confidence="0.992909666666667">
pG|M(w) = pG(w)
Z , if w ∈ L(M) (26)
0, otherwise (27)
</equation>
<bodyText confidence="0.9998495">
where Z = Ew:w∈L(M) pG(w). Note that pG|M = pG if L(M) ⊇ L(G). Our goal is now to
show that our choice of pM minimizes
</bodyText>
<equation confidence="0.84100825">
� pG|M(w) · log pG|M(w)
D(pG|M�pM) = pM(w)
w:w∈L(M)
� pG(w) · log pG(w) (28)
=log Z 1+ Z 1 pM(w)
w:w∈L(M)
As Z is independent of pM, it is sufficient to show that our choice of pM minimizes
� pG(w) · log p pG
w:w∈L(M) ((w ) (29)
Now consider the expression
TT pM(τ)E(τ) (30)
τ
</equation>
<page confidence="0.991847">
182
</page>
<note confidence="0.850324">
Nederhof Training Models on Models
</note>
<bodyText confidence="0.9786">
By the usual proof technique with Lagrange multipliers, it is easy to show that our
choice of pM in section 5.1, given by
</bodyText>
<equation confidence="0.975668875">
E(T)
pM(T) =
T ( )
Eτ ,a ,s τ : =(r F E
-,s&apos;)ET (,) 31
for each T = (r a
�→ s) ∈ T, is such that it maximizes (30), under the constraint of
properness.
</equation>
<bodyText confidence="0.565744">
For T ∈ T and w ∈ Σ*, we define #τ(w) to be zero, if w ∈� L(M), and otherwise to be
the number of occurrences of T in the (unique) computation that recognizes w. Formally,
#τ (w) = Ec,c, 1((q0,w)cTc (qf, e)). We rewrite (30) as
</bodyText>
<equation confidence="0.98074475">
ri �pM(T)E(τ) = pM (T) Ew pG(w)·#τ (w)
τ τ
pM(T)pG(w)·#τ(w)
ri
τ
ri=
w
� (rI
= pM(T)#τ(w)
wτ
�pG(w)
�= pM(w)pG (w)
w:pM(w)&gt;0
= ri 2pG(w)·log pM(w)
w:pM(w)&gt;0
ri
=
w:pM(w)&gt;0
ri 2−pG(w)·logpG(w)
= pM(w) +pG(w)·log pG(w)
w:pM(w)&gt;0
�
= 2−�w:pM(w)&gt;0 pG(w)·log pG (w)
pM(w) · 2 w:pM(w)&gt;0 pG(w)·log pG(w) (32)
</equation>
<bodyText confidence="0.831375">
We have already seen that the choice of
that maximizes (30) is given by (31), and
&gt; 0 for all w such that w
</bodyText>
<figure confidence="0.710489214285714">
L(M) and
&gt; 0. Since
&gt; 0 is
impossible for w
L(M),
pM
pM(w)
∈
pG(w)
pM(w)
∈�
the value of
pM (w)&gt;0 pG (w)·logpG(w) (33)
is determined solely by
</figure>
<figureCaption confidence="0.168759">
and by the condition that
</figureCaption>
<bodyText confidence="0.923848">
&gt; 0 for all w such that
</bodyText>
<equation confidence="0.761942071428571">
w
L(M) and
&gt; 0. This implies that (30) is maximized by choosing
pG
pM(w)
∈
pG(w)
pM such
that
2− Ew:pM(w)&gt;0 pG (w)·log pG (w)
pM(w) (34)
2pG (w)·log pM(w)−pG (w)·log pG (w)+pG (w)·log pG (w)
(31) implies
2Ew:
</equation>
<page confidence="0.916035">
183
</page>
<table confidence="0.35023775">
Computational Linguistics Volume 31, Number 2
is maximized, or alternatively that
� pg(w) · log pg(w) (35)
w:pm(w)&gt;0 pm(w)
</table>
<bodyText confidence="0.991430166666667">
is minimized, under the constraint that pm(w) &gt; 0 for all w such that w E L(m) and
pg(w) &gt; 0. For this choice of pm, (29) equals (35).
Conversely, if a choice of pm minimizes (29), we may assume that pm(w) &gt; 0 for
all w such that w E L(m) and pg(w) &gt; 0, since otherwise (29) is infinite. Again, for this
choice of pm, (29) equals (35). It follows that the choice of pm that minimizes (29) concurs
with the choice of pm that maximizes (30), which concludes our proof.
</bodyText>
<sectionHeader confidence="0.993737" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.686505285714286">
Comments by Khalil Sima’an, Giorgio Satta,
Yuval Krymolowski, and anonymous
reviewers are gratefully acknowledged. The
author is supported by the PIONIER Project
Algorithms for Linguistic Processing, funded
by NWO (Dutch Organization for Scientific
Research).
</bodyText>
<sectionHeader confidence="0.966809" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998150636363636">
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
Parsing, volume 1 of The Theory of Parsing,
Translation and Compiling. Prentice Hall,
Englewood Cliffs, NJ.
Bar-Hillel, Yehoshua, M. Perles, and
E. Shamir. 1964. On formal properties of
simple phrase structure grammars. In
Yehoshua Bar-Hillel, editor, Language and
Information: Selected Essays on Their Theory
and Application. Addison-Wesley, Reading,
MA, pages 116–150.
Bertsch, Eberhard and Mark-Jan Nederhof.
2001. On the complexity of some
extensions of RCG parsing. In Proceedings
of the Seventh International Workshop on
Parsing Technologies, pages 66–77, Beijing,
October.
Booth, Taylor L. and Richard A. Thompson.
1973. Applying probabilistic measures to
abstract languages. IEEE Transactions on
Computers, C-22(5):442–450.
Boullier, Pierre. 2000. Range concatenation
grammars. In Proceedings of the Sixth
International Workshop on Parsing
Technologies, pages 53–64, Trento, Italy,
February.
Jurafsky, Daniel, Chuck Wooters, Gary
Tajchman, Jonathan Segal, Andreas
Stolcke, Eric Fosler, and Nelson Morgan.
1994. The Berkeley Restaurant Project. In
Proceedings of the International Conference on
Spoken Language Processing (ICSLP-94),
pages 2139–2142, Yokohama, Japan.
Lang, Bernard. 1994. Recognition can be
harder than parsing. Computational
Intelligence, 10(4):486–494.
Manning, Christopher D. and Hinrich
Sch¨utze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23(2):269–311.
Mohri, Mehryar and Mark-Jan Nederhof.
2001. Regular approximation of
context-free grammars through
transformation. In J.-C. Junqua and G. van
Noord, editors, Robustness in Language and
Speech Technology. Kluwer Academic,
pages 153–163.
Nederhof, Mark-Jan. 2000. Practical
experiments with regular approximation
of context-free languages. Computational
Linguistics, 26(1):17–44.
Nederhof, Mark-Jan and Giorgio Satta. 2003.
Probabilistic parsing as intersection. In
Proceedings of the Eighth International
Workshop on Parsing Technologies, pages
137–148, Laboratoire Lorrain de recherche
en informatique et ses applications
(LORIA), Nancy, France, April.
Paz, Azaria. 1971. Introduction to Probabilistic
Automata. Academic Press, New York.
Rimon, Mori and J. Herz. 1991. The
recognition capacity of local syntactic
constraints. In Proceedings of the Fifth
Conference of the European Chapter of the
ACL, pages 155–160, Berlin, April.
Santos, Eugene S. 1972. Probabilistic
grammars and automata. Information and
Control, 21:27–47.
Seki, Hiroyuki, Takashi Matsumura,
Mamoru Fujii, and Tadao Kasami.
1991. On multiple context-free grammars.
Theoretical Computer Science,
88:191–229.
</reference>
<page confidence="0.993604">
184
</page>
<note confidence="0.806339">
Nederhof Training Models on Models
</note>
<reference confidence="0.999651409090909">
Starke, Peter H. 1972. Abstract Automata.
North-Holland, Amsterdam.
Stolcke, Andreas and Jonathan Segal. 1994.
Precise N-gram probabilities from
stochastic context-free grammars. In
Proceedings of the 32nd Annual Meeting
of the ACL, pages 74–79, Las Cruces,
NM, June.
Vijay-Shanker, K. and David J. Weir.
1993. The use of shared forests in
tree adjoining grammar parsing. In
Proceedings of the Sixth Conference of the
European Chapter of the ACL, pages 384–393,
Utrecht, The Netherlands, April.
Zue, Victor, James Glass, David Goodine,
Hong Leung, Michael Phillips, Joseph
Polifroni, and Stephanie Seneff. 1991.
Integration of speech recognition and
natural language processing in the MIT
Voyager system. In Proceedings of the
ICASSP-91, Toronto, volume 1, pages
713–716.
</reference>
<page confidence="0.998848">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551392">
<title confidence="0.9631895">A General Technique to Train Language Models on Language Models</title>
<affiliation confidence="0.797645">University of Groningen</affiliation>
<abstract confidence="0.9589735">We show that under certain conditions, a language model can be trained on the basis of a second language model. The main instance of the technique trains a finite automaton on the basis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between grammar and trained automaton is provably minimal. This is a substantial generalization of existing algorithm to train an model on the basis of a probabilistic context-free grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<journal>Parsing,</journal>
<booktitle>of The Theory of Parsing, Translation and Compiling.</booktitle>
<volume>1</volume>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="12647" citStr="Aho and Ullman (1972)" startWordPosition="2258" endWordPosition="2261">d β ∈ (Σ ∪ N)∗ such that pG(S d1⇒ w1Aβ) · pG(w1Aβ d2⇒ w1w2) &gt; 0. In words, if a PCFG is reduced, then for each nonterminal A, there is at least one derivation d1d2 with nonzero probability that derives a string w1w2 from S and that includes some rule with left-hand side A. A PCFG G that is not reduced can be turned into one that is reduced and that describes the same probability distribution, provided that Ew pG(w) &gt; 0. This reduction consists in removing from the grammar any nonterminal A for which the above conditions do not hold, together with any rule that contains such a nonterminal; see Aho and Ullman (1972) for reduction of CFGs, which is very similar. A finite automaton M is a 5-tuple (Σ, Q, q0, qf, T), where Σ and Q are two finite sets of terminals and states, respectively, q0, qf ∈ Q are the initial and final states, respectively, and T is a finite set of transitions, each of the form r a �→ s, where r ∈ Q − {qf},s ∈ Q, and a ∈ Σ.2 A probabilistic finite automaton M is a 6-tuple (Σ, Q, q0, qf, T, pM), where Σ, Q, q0, qf, and T are as above, and pM is a function from transitions in T to probabilities. In what follows, symbols q, r, s range over the set Q, symbol τ ranges over the set T, and sy</context>
<context position="24553" citStr="Aho and Ullman 1972" startWordPosition="4693" endWordPosition="4696">uous. Our goal is to find a function p2 so that 181 Computational Linguistics Volume 31, Number 2 proper PFA (Σ, Q2, q0,2, qf,2, T2, p2) approximates M1 as well as possible, minimizing the KL distance between p1 and p2 on the domain L(M2). One way to solve this problem is to map M2 to an equivalent right-linear CFG G and then to apply the algorithm from section 5.2. The obtained probability function pG can be translated back to an appropriate function p2. For this special case, the construction from section 4 can be simplified to the “cross-product” construction of finite automata (see, e.g., Aho and Ullman 1972). The simplified forms of the functions inner and outer from section 3 are commonly called forward and backward, respectively, and they are defined by systems of linear equations. As a result, we can compute exact solutions, as opposed to approximate solutions by iteration. Appendix We now prove that the choice of pM in section 5.1 is such that it minimizes the KullbackLeibler distance between pG and pM, restricted to the domain L(M). Without this restriction, the KL distance is given by � pG(w) · log pG(w) (25) D(pG�pM) = pM(w) w This can be used for many applications mentioned in section 1. </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Alfred V. and Jeffrey D. Ullman. 1972. Parsing, volume 1 of The Theory of Parsing, Translation and Compiling. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>In Yehoshua Bar-Hillel, editor, Language and Information: Selected Essays on Their Theory and Application.</booktitle>
<pages>116--150</pages>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA,</location>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Bar-Hillel, Yehoshua, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Yehoshua Bar-Hillel, editor, Language and Information: Selected Essays on Their Theory and Application. Addison-Wesley, Reading, MA, pages 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eberhard Bertsch</author>
<author>Mark-Jan Nederhof</author>
</authors>
<title>On the complexity of some extensions of RCG parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies,</booktitle>
<pages>66--77</pages>
<location>Beijing,</location>
<contexts>
<context position="19124" citStr="Bertsch and Nederhof 2001" startWordPosition="3610" endWordPosition="3613"> be easily shown that if h(dn) = (d,c) and Sn d∩� w, then for the same w, we have S 4 w and (q0, w) �c_ (qf, C). Conversely, if for some w, d, and c we have S d� w and (q0, w) c� (qf, C), then there is precisely one derivation dn such that h(dn) = (d, c) and Sn d∩� w. It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a compact representation of all parse trees according to G that derive strings recognized by M. The construction can be generalized to, for example, tree-adjoining grammars (Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000; Bertsch and Nederhof 2001). The construction for the latter also has implications for linear context-free rewriting systems (Seki et al. 1991). The construction has been extended by Nederhof and Satta (2003) to apply to a PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is a PCFG Gn = (Σ, Nn, Sn, Rn, pn), where Nn, Sn, and Rn are as before, and pn is defined by pn((r0,A,rm) → (r0,X1,r1)··· (rm−1,Xm,rm)) = pG(A → X1 ···Xm) (15) pn((r,a,s) → a) = pM(r a �→ s) (16) If dn, d, and c are such that h(dn) = (d, c), then clearly pn(dn) = pG(d) · pM(c). 5. Training Models on Models We restrict ourselves </context>
</contexts>
<marker>Bertsch, Nederhof, 2001</marker>
<rawString>Bertsch, Eberhard and Mark-Jan Nederhof. 2001. On the complexity of some extensions of RCG parsing. In Proceedings of the Seventh International Workshop on Parsing Technologies, pages 66–77, Beijing, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
<author>Richard A Thompson</author>
</authors>
<title>Applying probabilistic measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="9334" citStr="Booth and Thompson (1973)" startWordPosition="1526" endWordPosition="1529">-known representation of the intersection of a regular and a context-free language. Thereby we merge the input model and the model to be trained into a single structure. This structure is the foundation for a number of algorithms, presented in section 5, which allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1), training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an unambiguous FA on the basis of a PFA (section 5.3). 2. Preliminaries Many of the definitions on probabilistic context-free grammars are based on Santos (1972) and Booth and Thompson (1973), and the definitions on probabilistic finite automata are based on Paz (1971) and Starke (1972). A context-free grammar g is a 4-tuple (E, N, S, R), where E and N are two finite disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and R is a finite set of rules, each of the form A --� o, where A E N and o E (E U N)*. A probabilistic context-free grammar g is a 5-tuple (E, N, S, R, pg), where E, N, S and R are as above, and pg is a function from rules in R to probabilities. In what follows, symbol a ranges over the set E, symbols w, v range over the set E*, sym</context>
<context position="11931" citStr="Booth and Thompson 1973" startWordPosition="2121" endWordPosition="2124"> S, R, pG). For α, β ∈ (Σ ∪ N)∗ and d = ρ1 · · · ρm ∈ R∗, m ≥ 0, we define pG(α ⇒d β) = fIm 1 pG (ρi) if α ⇒d β, and pG (α d⇒ β) = 0 otherwise. The probability pG(w) of a string w ∈ Σ∗ is defined to be Ed pG(S d⇒ w). PCFG G is said to be proper if E ρ,α pG(A ⇒ρ α) = 1 for all A ∈ N, that is, if the probabilities of all rules ρ = (A → α) with left-hand side A sum to one. PCFG G is said to be consistent if Ew pG(w) = 1. Consistency implies that the PCFG defines a probability distribution on the set of terminal strings. There is a practical sufficient condition for consistency that is decidable (Booth and Thompson 1973). A PCFG is said to be reduced if for each nonterminal A, there are d1, d2 ∈ R∗, w1,w2 ∈ Σ∗, and β ∈ (Σ ∪ N)∗ such that pG(S d1⇒ w1Aβ) · pG(w1Aβ d2⇒ w1w2) &gt; 0. In words, if a PCFG is reduced, then for each nonterminal A, there is at least one derivation d1d2 with nonzero probability that derives a string w1w2 from S and that includes some rule with left-hand side A. A PCFG G that is not reduced can be turned into one that is reduced and that describes the same probability distribution, provided that Ew pG(w) &gt; 0. This reduction consists in removing from the grammar any nonterminal A for which </context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Booth, Taylor L. and Richard A. Thompson. 1973. Applying probabilistic measures to abstract languages. IEEE Transactions on Computers, C-22(5):442–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>Range concatenation grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Workshop on Parsing Technologies,</booktitle>
<pages>53--64</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="19096" citStr="Boullier 2000" startWordPosition="3608" endWordPosition="3609">h2(dn)). It can be easily shown that if h(dn) = (d,c) and Sn d∩� w, then for the same w, we have S 4 w and (q0, w) �c_ (qf, C). Conversely, if for some w, d, and c we have S d� w and (q0, w) c� (qf, C), then there is precisely one derivation dn such that h(dn) = (d, c) and Sn d∩� w. It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a compact representation of all parse trees according to G that derive strings recognized by M. The construction can be generalized to, for example, tree-adjoining grammars (Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000; Bertsch and Nederhof 2001). The construction for the latter also has implications for linear context-free rewriting systems (Seki et al. 1991). The construction has been extended by Nederhof and Satta (2003) to apply to a PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is a PCFG Gn = (Σ, Nn, Sn, Rn, pn), where Nn, Sn, and Rn are as before, and pn is defined by pn((r0,A,rm) → (r0,X1,r1)··· (rm−1,Xm,rm)) = pG(A → X1 ···Xm) (15) pn((r,a,s) → a) = pM(r a �→ s) (16) If dn, d, and c are such that h(dn) = (d, c), then clearly pn(dn) = pG(d) · pM(c). 5. Training Models on M</context>
</contexts>
<marker>Boullier, 2000</marker>
<rawString>Boullier, Pierre. 2000. Range concatenation grammars. In Proceedings of the Sixth International Workshop on Parsing Technologies, pages 53–64, Trento, Italy, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Chuck Wooters</author>
<author>Gary Tajchman</author>
<author>Jonathan Segal</author>
<author>Andreas Stolcke</author>
<author>Eric Fosler</author>
<author>Nelson Morgan</author>
</authors>
<title>The Berkeley Restaurant Project.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP-94),</booktitle>
<pages>2139--2142</pages>
<location>Yokohama, Japan.</location>
<contexts>
<context position="5167" citStr="Jurafsky et al. (1994)" startWordPosition="823" endWordPosition="826">e the case, and no training material may be available. Furthermore, as a determinized approximating FA may be much larger than the input PCFG, the sparse-data problem may be more severe for the automaton than it was for the grammar.1 Hence, even if sufficient material was available to train the CFG, it may not be sufficient to accurately train the FA. A third approach is to construct a training corpus from the PCFG by means of a (pseudo)random generator of sentences, such that sentences that are more likely according to the PCFG are generated with greater likelihood. This has been proposed by Jurafsky et al. (1994), for the special case of bigrams, extending a nonprobabilistic technique by Zue et al. (1991). It is not clear, however, whether this idea is feasible for training of finite-state models that are larger than bigrams. The reason is that very large corpora would have to be generated in order to obtain accurate probability estimates for the PFA. Note that the number of parameters of a bigram model is bounded by the square of the size of the lexicon; such a bound does not exist for general PFAs. The current article discusses a fourth approach. In the limit, it is equivalent to the third approach </context>
</contexts>
<marker>Jurafsky, Wooters, Tajchman, Segal, Stolcke, Fosler, Morgan, 1994</marker>
<rawString>Jurafsky, Daniel, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stolcke, Eric Fosler, and Nelson Morgan. 1994. The Berkeley Restaurant Project. In Proceedings of the International Conference on Spoken Language Processing (ICSLP-94), pages 2139–2142, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Recognition can be harder than parsing.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="18797" citStr="Lang (1994)" startWordPosition="3561" endWordPosition="3562">ine pointwise as h1(pn) = p if pn = ((r0,A,rm) → (r0,X1,r1)··· (rm−1, Xm, rm)) and p = (A → X1 ··· Xm) C if pn = ((r,a,s) → a) �→ s) h2(pn) = T if pn = ((r,a,s) → a) and T = (r a C if pn = ((r0,A,rm) → (r0,X1,r1)··· (rm−1, Xm, rm)) 178 Nederhof Training Models on Models We define h(dn) = (h1(dn),h2(dn)). It can be easily shown that if h(dn) = (d,c) and Sn d∩� w, then for the same w, we have S 4 w and (q0, w) �c_ (qf, C). Conversely, if for some w, d, and c we have S d� w and (q0, w) c� (qf, C), then there is precisely one derivation dn such that h(dn) = (d, c) and Sn d∩� w. It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a compact representation of all parse trees according to G that derive strings recognized by M. The construction can be generalized to, for example, tree-adjoining grammars (Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000; Bertsch and Nederhof 2001). The construction for the latter also has implications for linear context-free rewriting systems (Seki et al. 1991). The construction has been extended by Nederhof and Satta (2003) to apply to a PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is </context>
</contexts>
<marker>Lang, 1994</marker>
<rawString>Lang, Bernard. 1994. Recognition can be harder than parsing. Computational Intelligence, 10(4):486–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="7575" citStr="Mohri (1997)" startWordPosition="1233" endWordPosition="1234">model is here seen as a (P)FA that contains exactly one state for each possible history of the n − 1 previously read symbols. It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs. Another application of our work involves determinization and minimization of PFAs. As shown by Mohri (1997), PFAs cannot always be determinized, and no practical algorithms are known to minimize arbitrary nondeterministic (P)FAs. This can be a problem when deterministic or small PFAs are required. We can, however, always compute a minimal deterministic FA equivalent to an input FA. The new results in this article offer a way to extend this determinized FA to a PFA such that it approximates the probability distribution described by the input PFA as well as possible, in terms of the KL distance. Although the proposed technique has some limitations, in particular, that the model to be trained is unamb</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mohri, Mehryar. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2):269–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Regular approximation of context-free grammars through transformation.</title>
<date>2001</date>
<booktitle>Robustness in Language and Speech Technology.</booktitle>
<pages>153--163</pages>
<editor>In J.-C. Junqua and G. van Noord, editors,</editor>
<publisher>Kluwer Academic,</publisher>
<contexts>
<context position="3835" citStr="Mohri and Nederhof (2001)" startWordPosition="603" endWordPosition="606">ion with probabilities of existing work on approximation of CFGs by means of FAs (Nederhof 2000). The motivation for this work was that application of FAs is generally less costly than application of CFGs, which is an important benefit when the input is very large, as is often the case in, for example, speech recognition systems. The practical relevance of this work was limited, however, by the fact that in practice one is more interested in the probabilities of sentences than in a purely Boolean distinction between grammatical and ungrammatical sentences. Several approaches were discussed by Mohri and Nederhof (2001) to extend this work to approximation of PCFGs by means of PFAs. A first approach is to directly map rules with attached probabilities to transitions with attached probabilities. Although this is computationally the easiest approach, the resulting PFA may be a very inaccurate approximation of the probability distribution described by the input PCFG. In particular, there may be assignments of probabilities to the transitions of the same FA that lead to more accurate approximating language models. A second approach is to train the approximating FA by means of a corpus. If the input PCFG was itse</context>
</contexts>
<marker>Mohri, Nederhof, 2001</marker>
<rawString>Mohri, Mehryar and Mark-Jan Nederhof. 2001. Regular approximation of context-free grammars through transformation. In J.-C. Junqua and G. van Noord, editors, Robustness in Language and Speech Technology. Kluwer Academic, pages 153–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="3306" citStr="Nederhof 2000" startWordPosition="521" endWordPosition="522">d for publication: 19th September 2004 © 2005 Association for Computational Linguistics Computational Linguistics Volume 31, Number 2 the resulting PFA or PCFG approximates the input language model as well as possible, or more specifically, such that the Kullback-Leibler (KL) distance (or relative entropy) between the input model and the trained model is minimized. The input FA or CFG to be trained may be structurally unrelated to the input language model. This technique has several applications. One is an extension with probabilities of existing work on approximation of CFGs by means of FAs (Nederhof 2000). The motivation for this work was that application of FAs is generally less costly than application of CFGs, which is an important benefit when the input is very large, as is often the case in, for example, speech recognition systems. The practical relevance of this work was limited, however, by the fact that in practice one is more interested in the probabilities of sentences than in a purely Boolean distinction between grammatical and ungrammatical sentences. Several approaches were discussed by Mohri and Nederhof (2001) to extend this work to approximation of PCFGs by means of PFAs. A firs</context>
<context position="6462" citStr="Nederhof (2000)" startWordPosition="1048" endWordPosition="1049">t we have found a way to avoid considering sentences individually. The key idea that allows us to handle an infinite set of strings generated by the PCFG is that we construct a new grammar that represents the intersection of the languages described by the input PCFG and the FA. Within this new grammar, we can compute the expected frequencies of transitions of the FA, using a fairly standard analysis of PCFGs. These expected frequencies then allow us to determine the assignment of probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA. 1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized approximating FAs that can be much larger than the input CFGs. 174 Nederhof Training Models on Models The only requirement is that the FA to be trained be unambiguous, by which we mean that each input string can be recognized by at most one computation of the FA. The special case of n-grams has already been formulated by Stolcke and Segal (1994), realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is here seen as a (P)FA that contains exactly one state for each possible history of the n − </context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Nederhof, Mark-Jan. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1):17–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<location>Nancy, France,</location>
<contexts>
<context position="19305" citStr="Nederhof and Satta (2003)" startWordPosition="3637" endWordPosition="3640">, C), then there is precisely one derivation dn such that h(dn) = (d, c) and Sn d∩� w. It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a compact representation of all parse trees according to G that derive strings recognized by M. The construction can be generalized to, for example, tree-adjoining grammars (Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000; Bertsch and Nederhof 2001). The construction for the latter also has implications for linear context-free rewriting systems (Seki et al. 1991). The construction has been extended by Nederhof and Satta (2003) to apply to a PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is a PCFG Gn = (Σ, Nn, Sn, Rn, pn), where Nn, Sn, and Rn are as before, and pn is defined by pn((r0,A,rm) → (r0,X1,r1)··· (rm−1,Xm,rm)) = pG(A → X1 ···Xm) (15) pn((r,a,s) → a) = pM(r a �→ s) (16) If dn, d, and c are such that h(dn) = (d, c), then clearly pn(dn) = pG(d) · pM(c). 5. Training Models on Models We restrict ourselves to a few cases of the general technique of training a model on the basis of another model. 5.1 Training a PFA on a PCFG Let us assume we have a proper and consistent PCFG G = (Σ, N,</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>Nederhof, Mark-Jan and Giorgio Satta. 2003. Probabilistic parsing as intersection. In Proceedings of the Eighth International Workshop on Parsing Technologies, pages 137–148, Laboratoire Lorrain de recherche en informatique et ses applications (LORIA), Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azaria Paz</author>
</authors>
<title>Introduction to Probabilistic Automata.</title>
<date>1971</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="9412" citStr="Paz (1971)" startWordPosition="1540" endWordPosition="1541">e merge the input model and the model to be trained into a single structure. This structure is the foundation for a number of algorithms, presented in section 5, which allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1), training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an unambiguous FA on the basis of a PFA (section 5.3). 2. Preliminaries Many of the definitions on probabilistic context-free grammars are based on Santos (1972) and Booth and Thompson (1973), and the definitions on probabilistic finite automata are based on Paz (1971) and Starke (1972). A context-free grammar g is a 4-tuple (E, N, S, R), where E and N are two finite disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and R is a finite set of rules, each of the form A --� o, where A E N and o E (E U N)*. A probabilistic context-free grammar g is a 5-tuple (E, N, S, R, pg), where E, N, S and R are as above, and pg is a function from rules in R to probabilities. In what follows, symbol a ranges over the set E, symbols w, v range over the set E*, symbols A, B range over the set N, symbol X ranges over the set E U N, symbols o,</context>
</contexts>
<marker>Paz, 1971</marker>
<rawString>Paz, Azaria. 1971. Introduction to Probabilistic Automata. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mori Rimon</author>
<author>J Herz</author>
</authors>
<title>The recognition capacity of local syntactic constraints.</title>
<date>1991</date>
<booktitle>In Proceedings of the Fifth Conference of the European Chapter of the ACL,</booktitle>
<pages>155--160</pages>
<location>Berlin,</location>
<contexts>
<context position="6951" citStr="Rimon and Herz (1991)" startWordPosition="1128" endWordPosition="1131">f probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA. 1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized approximating FAs that can be much larger than the input CFGs. 174 Nederhof Training Models on Models The only requirement is that the FA to be trained be unambiguous, by which we mean that each input string can be recognized by at most one computation of the FA. The special case of n-grams has already been formulated by Stolcke and Segal (1994), realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is here seen as a (P)FA that contains exactly one state for each possible history of the n − 1 previously read symbols. It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs. Another application of our work involves determinization and minimization of PFAs. </context>
</contexts>
<marker>Rimon, Herz, 1991</marker>
<rawString>Rimon, Mori and J. Herz. 1991. The recognition capacity of local syntactic constraints. In Proceedings of the Fifth Conference of the European Chapter of the ACL, pages 155–160, Berlin, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene S Santos</author>
</authors>
<title>Probabilistic grammars and automata.</title>
<date>1972</date>
<journal>Information and Control,</journal>
<pages>21--27</pages>
<contexts>
<context position="9304" citStr="Santos (1972)" startWordPosition="1523" endWordPosition="1524">hat extends a well-known representation of the intersection of a regular and a context-free language. Thereby we merge the input model and the model to be trained into a single structure. This structure is the foundation for a number of algorithms, presented in section 5, which allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1), training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an unambiguous FA on the basis of a PFA (section 5.3). 2. Preliminaries Many of the definitions on probabilistic context-free grammars are based on Santos (1972) and Booth and Thompson (1973), and the definitions on probabilistic finite automata are based on Paz (1971) and Starke (1972). A context-free grammar g is a 4-tuple (E, N, S, R), where E and N are two finite disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and R is a finite set of rules, each of the form A --� o, where A E N and o E (E U N)*. A probabilistic context-free grammar g is a 5-tuple (E, N, S, R, pg), where E, N, S and R are as above, and pg is a function from rules in R to probabilities. In what follows, symbol a ranges over the set E, symbols w</context>
</contexts>
<marker>Santos, 1972</marker>
<rawString>Santos, Eugene S. 1972. Probabilistic grammars and automata. Information and Control, 21:27–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple context-free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>88--191</pages>
<contexts>
<context position="19240" citStr="Seki et al. 1991" startWordPosition="3627" endWordPosition="3630">if for some w, d, and c we have S d� w and (q0, w) c� (qf, C), then there is precisely one derivation dn such that h(dn) = (d, c) and Sn d∩� w. It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a compact representation of all parse trees according to G that derive strings recognized by M. The construction can be generalized to, for example, tree-adjoining grammars (Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000; Bertsch and Nederhof 2001). The construction for the latter also has implications for linear context-free rewriting systems (Seki et al. 1991). The construction has been extended by Nederhof and Satta (2003) to apply to a PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is a PCFG Gn = (Σ, Nn, Sn, Rn, pn), where Nn, Sn, and Rn are as before, and pn is defined by pn((r0,A,rm) → (r0,X1,r1)··· (rm−1,Xm,rm)) = pG(A → X1 ···Xm) (15) pn((r,a,s) → a) = pM(r a �→ s) (16) If dn, d, and c are such that h(dn) = (d, c), then clearly pn(dn) = pG(d) · pM(c). 5. Training Models on Models We restrict ourselves to a few cases of the general technique of training a model on the basis of another model. 5.1 Training a PFA on a P</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Seki, Hiroyuki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple context-free grammars. Theoretical Computer Science, 88:191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter H Starke</author>
</authors>
<title>Abstract Automata.</title>
<date>1972</date>
<publisher>North-Holland,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="9430" citStr="Starke (1972)" startWordPosition="1543" endWordPosition="1544">ut model and the model to be trained into a single structure. This structure is the foundation for a number of algorithms, presented in section 5, which allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1), training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an unambiguous FA on the basis of a PFA (section 5.3). 2. Preliminaries Many of the definitions on probabilistic context-free grammars are based on Santos (1972) and Booth and Thompson (1973), and the definitions on probabilistic finite automata are based on Paz (1971) and Starke (1972). A context-free grammar g is a 4-tuple (E, N, S, R), where E and N are two finite disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and R is a finite set of rules, each of the form A --� o, where A E N and o E (E U N)*. A probabilistic context-free grammar g is a 5-tuple (E, N, S, R, pg), where E, N, S and R are as above, and pg is a function from rules in R to probabilities. In what follows, symbol a ranges over the set E, symbols w, v range over the set E*, symbols A, B range over the set N, symbol X ranges over the set E U N, symbols o, (3,-y range over </context>
</contexts>
<marker>Starke, 1972</marker>
<rawString>Starke, Peter H. 1972. Abstract Automata. North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jonathan Segal</author>
</authors>
<title>Precise N-gram probabilities from stochastic context-free grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL,</booktitle>
<pages>74--79</pages>
<location>Las Cruces, NM,</location>
<contexts>
<context position="6885" citStr="Stolcke and Segal (1994)" startWordPosition="1118" endWordPosition="1121">hese expected frequencies then allow us to determine the assignment of probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA. 1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized approximating FAs that can be much larger than the input CFGs. 174 Nederhof Training Models on Models The only requirement is that the FA to be trained be unambiguous, by which we mean that each input string can be recognized by at most one computation of the FA. The special case of n-grams has already been formulated by Stolcke and Segal (1994), realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is here seen as a (P)FA that contains exactly one state for each possible history of the n − 1 previously read symbols. It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs. Another applicati</context>
</contexts>
<marker>Stolcke, Segal, 1994</marker>
<rawString>Stolcke, Andreas and Jonathan Segal. 1994. Precise N-gram probabilities from stochastic context-free grammars. In Proceedings of the 32nd Annual Meeting of the ACL, pages 74–79, Las Cruces, NM, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>The use of shared forests in tree adjoining grammar parsing.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the ACL,</booktitle>
<pages>384--393</pages>
<location>Utrecht, The Netherlands,</location>
<contexts>
<context position="19048" citStr="Vijay-Shanker and Weir 1993" startWordPosition="3600" endWordPosition="3603">8 Nederhof Training Models on Models We define h(dn) = (h1(dn),h2(dn)). It can be easily shown that if h(dn) = (d,c) and Sn d∩� w, then for the same w, we have S 4 w and (q0, w) �c_ (qf, C). Conversely, if for some w, d, and c we have S d� w and (q0, w) c� (qf, C), then there is precisely one derivation dn such that h(dn) = (d, c) and Sn d∩� w. It was observed by Lang (1994) that Gn can be seen as a parse forest, that is, a compact representation of all parse trees according to G that derive strings recognized by M. The construction can be generalized to, for example, tree-adjoining grammars (Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000; Bertsch and Nederhof 2001). The construction for the latter also has implications for linear context-free rewriting systems (Seki et al. 1991). The construction has been extended by Nederhof and Satta (2003) to apply to a PCFG G = (Σ, N, S, R, pG) and a PFA M = (Σ, Q, q0, qf, T, pM). The output is a PCFG Gn = (Σ, Nn, Sn, Rn, pn), where Nn, Sn, and Rn are as before, and pn is defined by pn((r0,A,rm) → (r0,X1,r1)··· (rm−1,Xm,rm)) = pG(A → X1 ···Xm) (15) pn((r,a,s) → a) = pM(r a �→ s) (16) If dn, d, and c are such that h(dn) = (d, c), then clearly</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>Vijay-Shanker, K. and David J. Weir. 1993. The use of shared forests in tree adjoining grammar parsing. In Proceedings of the Sixth Conference of the European Chapter of the ACL, pages 384–393, Utrecht, The Netherlands, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>James Glass</author>
<author>David Goodine</author>
<author>Hong Leung</author>
<author>Michael Phillips</author>
<author>Joseph Polifroni</author>
<author>Stephanie Seneff</author>
</authors>
<title>Integration of speech recognition and natural language processing in the MIT Voyager system.</title>
<date>1991</date>
<booktitle>In Proceedings of the ICASSP-91,</booktitle>
<volume>1</volume>
<pages>713--716</pages>
<location>Toronto,</location>
<contexts>
<context position="5261" citStr="Zue et al. (1991)" startWordPosition="838" endWordPosition="841"> FA may be much larger than the input PCFG, the sparse-data problem may be more severe for the automaton than it was for the grammar.1 Hence, even if sufficient material was available to train the CFG, it may not be sufficient to accurately train the FA. A third approach is to construct a training corpus from the PCFG by means of a (pseudo)random generator of sentences, such that sentences that are more likely according to the PCFG are generated with greater likelihood. This has been proposed by Jurafsky et al. (1994), for the special case of bigrams, extending a nonprobabilistic technique by Zue et al. (1991). It is not clear, however, whether this idea is feasible for training of finite-state models that are larger than bigrams. The reason is that very large corpora would have to be generated in order to obtain accurate probability estimates for the PFA. Note that the number of parameters of a bigram model is bounded by the square of the size of the lexicon; such a bound does not exist for general PFAs. The current article discusses a fourth approach. In the limit, it is equivalent to the third approach above, as if an infinite corpus were constructed on which the PFA is trained, but we have foun</context>
</contexts>
<marker>Zue, Glass, Goodine, Leung, Phillips, Polifroni, Seneff, 1991</marker>
<rawString>Zue, Victor, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneff. 1991. Integration of speech recognition and natural language processing in the MIT Voyager system. In Proceedings of the ICASSP-91, Toronto, volume 1, pages 713–716.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>