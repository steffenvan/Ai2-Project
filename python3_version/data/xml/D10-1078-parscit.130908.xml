<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000164">
<title confidence="0.994379">
Latent-Descriptor Clustering for Unsupervised POS Induction
</title>
<author confidence="0.993869">
Michael Lamar
</author>
<affiliation confidence="0.8987505">
Department of Mathematics and Computer Science
Saint Louis University
</affiliation>
<address confidence="0.904912">
220 N. Grand Blvd.
St.Louis, MO 63103, USA
</address>
<email confidence="0.999789">
mlamar@slu.edu
</email>
<author confidence="0.994749">
Yariv Maron
</author>
<affiliation confidence="0.9913675">
Gonda Brain Research Center
Bar-Ilan University
</affiliation>
<address confidence="0.691545">
Ramat-Gan 52900, Israel
</address>
<email confidence="0.998166">
syarivm@yahoo.com
</email>
<author confidence="0.997774">
Elie Bienenstock
</author>
<affiliation confidence="0.88399425">
Division of Applied Mathematics
and Department of Neuroscience
Brown University
Providence, RI 02912, USA
</affiliation>
<email confidence="0.997061">
elie@brown.edu
</email>
<sectionHeader confidence="0.993068" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999849588235294">
We present a novel approach to distributional-
only, fully unsupervised, POS tagging, based on
an adaptation of the EM algorithm for the esti-
mation of a Gaussian mixture. In this approach,
which we call Latent-Descriptor Clustering
(LDC), word types are clustered using a series
of progressively more informative descriptor
vectors. These descriptors, which are computed
from the immediate left and right context of
each word in the corpus, are updated based on
the previous state of the cluster assignments.
The LDC algorithm is simple and intuitive. Us-
ing standard evaluation criteria for unsupervised
POS tagging, LDC shows a substantial im-
provement in performance over state-of-the-art
methods, along with a several-fold reduction in
computational cost.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999454894736842">
Part-of-speech (POS) tagging is a fundamental
natural-language-processing problem, and POS
tags are used as input to many important appli-
cations. While state-of-the-art supervised POS
taggers are more than 97% accurate (Toutanova
et al., 2003; Tsuruoka and Tsujii, 2005), unsu-
pervised POS taggers continue to lag far behind.
Several authors addressed this gap using limited
supervision, such as a dictionary of tags for each
word (Goldwater and Griffiths, 2007; Ravi and
Knight, 2009), or a list of word prototypes for
each tag (Haghighi and Klein, 2006). Even in
light of all these advancements, there is still in-
terest in a completely unsupervised method for
POS induction for several reasons. First, most
languages do not have a tag dictionary. Second,
the preparation of such resources is error-prone.
Third, while several widely used tag sets do ex-
ist, researchers do not agree upon any specific
set of tags across languages or even within one
language. Since tags are used as basic features
for many important NLP applications (e.g.
Headden et al. 2008), exploring new, statistically
motivated, tag sets may also be useful. For these
reasons, a fully unsupervised induction algo-
rithm has both a practical and a theoretical val-
ue.
In the past decade, there has been a steady
improvement on the completely unsupervised
version of POS induction (Schütze, 1995; Clark,
2001; Clark, 2003; Johnson, 2007; Gao and
Johnson, 2008; Graça et al., 2009; Abend et al.,
2010; Lamar et al., 2010; Reichart et al., 2010;
Berg-Kirkpatrick et al., 2010). Some of these
methods use morphological cues (Clark, 2001;
Clark, 2003; Abend et al., 2010; Reichart et al.,
2010; Berg-Kirkpatrick et al., 2010), but all rely
heavily on distributional information, i.e., bi-
</bodyText>
<page confidence="0.977042">
799
</page>
<note confidence="0.824303">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997651988505748">
gram statistics. Two recent papers advocate non-
disambiguating models (Abend et al., 2010;
Lamar et al., 2010): these assign the same tag to
all tokens of a word type, rather than attempting
to disambiguate words in context. Lamar et al.
(2010) motivate this choice by showing how
removing the disambiguation ability from a
state-of-the-art disambiguating model results in
increasing its accuracy.
In this paper, we present a novel approach to
non-disambiguating, distributional-only, fully
unsupervised, POS tagging. As in all non-
disambiguating distributional approaches, the
goal, loosely stated, is to assign the same tag to
words whose contexts in the corpus are similar.
Our approach, which we call Latent-Descriptor
Clustering, or LDC, is an iterative algorithm, in
the spirit of the K-means clustering algorithm
and of the EM algorithm for the estimation of a
mixture of Gaussians.
In conventional K-means clustering, one is
given a collection of N objects described as N
data points in an r-dimensional Euclidean space,
and one seeks a clustering that minimizes the
sum of intra-cluster squared distances, i.e., the
sum, over all data points, of the squared distance
between that point and the centroid of its as-
signed cluster. In LDC, we similarly state our
goal as one of finding a tagging, i.e., cluster as-
signment, A, that minimizes the sum of intra-
cluster squared distances. However, unlike in
conventional K-means, the N objects to be clus-
tered are themselves described by vectors—in a
suitable manifold—that depend on the clustering
A. We call these vectors latent descriptors.
Specifically, each object to be clustered, i.e.,
each word type w, is described in terms of its
left-tag context and right-tag context. These con-
text vectors are the counts of the K different tags
occurring, under tagging A, to the left and right
of tokens of word type w in the corpus. We nor-
malize each of these context vectors to unit
length, producing, for each word type w, two
points LA(w) and RA(w) on the (K–1)-
dimensional unit sphere. The latent descriptor
for w consists of the pair (LA(w), RA(w))—more
details in Section 2.
A straightforward approach to this latent-
descriptor K-means problem is to adapt the clas-
sical iterative K-means algorithm so as to handle
the latent descriptors. Specifically, in each itera-
tion, given the assignment A obtained from the
previous iteration, one first computes the latent
descriptors for all word types as defined above,
and then proceeds in the usual way to update
cluster centroids and to find a new assignment A
to be used in the next iteration.
For reasons to be discussed in Section 5, we
instead prefer a soft-assignment strategy, in-
spired from the EM algorithm for the estimation
of a mixture of Gaussians. Thus, rather than the
hard assignment A, we use a soft-assignment
matrix P. Pwk, interpreted as the probability of
assigning word w to cluster k, is, essentially,
proportional to exp{– dwk2/2σ2}, where dwk is the
distance between the latent descriptor for w and
the centroid, i.e., Gaussian mean, for k. Unlike
the Gaussian-mixture model however, we use
the same mixture coefficient and the same Gaus-
sian width for all k. Further, we let the Gaussian
width σ decrease gradually during the iterative
process. As is well-known, the EM algorithm for
Gaussian mixtures reduces in the limit of small σ
to the simpler K-means clustering algorithm. As
a result, the last few iterations of LDC effec-
tively implement the hard-assignment K-means-
style algorithm outlined in the previous para-
graph. The soft assignment used earlier in the
process lends robustness to the algorithm.
The LDC approach is shown to yield substantial
improvement over state-of-the-art methods for
the problem of fully unsupervised, distributional
only, POS tagging. The algorithm is conceptu-
ally simple and easy to implement, requiring less
than 30 lines of Matlab code. It runs in a few
seconds of computation time, as opposed to
hours or days for the training of HMMs.
</bodyText>
<sectionHeader confidence="0.847549" genericHeader="introduction">
2 Notations and Statement of Problem
</sectionHeader>
<bodyText confidence="0.9999253">
The LDC algorithm is best understood in the
context of the latent-descriptor K-means optimi-
zation problem. In this section, we set up our
notations and define this problem in detail. For
simplicity, induced tags are henceforth referred
to as labels, while tags will be reserved for the
gold-standard tags, to be used later for evalua-
tion.
Let W denote the set of word types w1,...,wN,
and let T denote the set of labels, i.e., induced
</bodyText>
<page confidence="0.989697">
800
</page>
<bodyText confidence="0.985328741935484">
tags. The sizes of these sets are |W |= N and |T |=
K. In the experiments presented in Section 4, N
is 43,766 and K is either 50 or 17. For any word
token t in the corpus, we denote the word type of
t by w(t). The frequency of word type w in the
corpus is denoted f(w); thus, Y-w f(w) = 1.
For a word type w1, the left-word context of
w1, L(w1), is defined as the N-dimensional vector
whose n-th component is the number of bigrams,
i.e., pairs of consecutive tokens (ti–1, ti) in the
corpus, such that w(ti) = w1 and w(ti–1) = n. Simi-
larly, we define the right-word context of w1,
R(w1), as the N-dimensional vector whose n-th
component is the number of bigrams (ti, ti+1)
such that w(ti) = w1 and w(ti+1) = n. We let L
(resp. R) be the N×N matrix whose w-th row is
L(w) (resp. R(w)).
SK–1 is the unit sphere in the K-dimensional
Euclidean space ℝK. For any xEℝK, we denote
by A(x) the projection of x on SK–1, i.e., A(x) =
x/||x||.
A labeling is a map A: W —&gt; T. Given a labeling
~
A, we define ( )
LA w1 , the left-label context of
word type w1, as the K-dimensional vector
whose k-th component is the number of bigrams
(ti–1, ti) in the corpus such that w(ti) = w1 and
A(w(ti–1)) = k. We define the left descriptor of
word type w as:
~
</bodyText>
<equation confidence="0.830678">
LA (w)= A(LA w .
( ))
</equation>
<bodyText confidence="0.999000666666667">
We similarly define the right-label context of w1,
~
RA (w1) , as the K-dimensional vector whose k-
th component is the number of bigrams (ti, ti+1)
such that w(ti) = w1 and A(w(ti+1)) = k, and we
define the right descriptor of word type w as:
</bodyText>
<equation confidence="0.86215">
~
RA (w)= A(RA w .
( ))
</equation>
<bodyText confidence="0.9997844">
In short, any labeling A defines two maps, LA
and RA, each from W to SK–1.
For any function g(w) defined on W, &lt;g(w)&gt; will
be used to denote the average of g(w) weighted
by the frequency of word type w in the corpus:
</bodyText>
<equation confidence="0.566878666666667">
&lt;g(w)&gt; = Y-w f(w)g(w).
For any label k, we define:
CL(k) = A(&lt; LA(w): A(w) = k &gt;).
</equation>
<bodyText confidence="0.99590775">
Thus, CL(k) is the projection on SK–1 of the
weighted average of the left descriptors of the
word types labeled k. We sometimes refer to
CL(k) as the left centroid of cluster k. Note that
CL(k) depends on A in two ways, first in that the
average is taken on words w such that A(w) = k,
and second through the global dependency of LA
on A. We similarly define the right centroids:
</bodyText>
<equation confidence="0.810115">
CR(k)= A(&lt; RA(w): A(w) = k &gt;).
</equation>
<bodyText confidence="0.999814785714286">
Informally, we seek a labeling A such that, for
any two word types w1 and w2 in W, w1 and w2
are labeled the same if and only if LA(w1) and
LA(w2) are close to each other on SK–1 and so are
RA(w1) and RA(w2). Formally, our goal is to find
a labeling A that minimizes the objective func-
tion:
F(A)=&lt;||LA(w)–CL(A(w))||2+||RA(w)–CR(A(w))||2&gt;.
Note that, just as in conventional K-means clus-
tering, F(A) is the sum of the intra-cluster
squared distances. However, unlike conventional
K-means clustering, the descriptors of the ob-
jects to be clustered depend themselves on the
clustering. We accordingly refer to LA and RA as
latent descriptors, and to the method described
in the next section as Latent-Descriptor Clus-
tering, or LDC.
Note, finally, that we do not seek the global
minimum of F(A). This global minimum, 0, is
obtained by the trivial assignment that maps all
word types to a unique label. Instead, we seek a
minimum under the constraint that the labeling
be non-trivial. As we shall see, this constraint
need not be imposed explicitly: the iterative
LDC algorithm, when suitably initialized and
parameterized, converges to non-trivial local
minima of F(A)—and these are shown to pro-
vide excellent taggers.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.998856">
Recall that a mixture of Gaussians is a genera-
tive model for a random variable taking values
</bodyText>
<page confidence="0.985787">
801
</page>
<bodyText confidence="0.74416">
in a Euclidean space 118r. With K Gaussians, the
model is parameterized by:
</bodyText>
<listItem confidence="0.99928525">
• K mixture parameters, i.e., K non-
negative numbers adding up to 1;
• K means, i.e., K points µ1,...,µK in 118r;
• K variance-covariance d×d matrices.
</listItem>
<bodyText confidence="0.999825722222222">
The collection of all parameters defining the
model is denoted by B. EM is an iterative algo-
rithm used to find a (local) maximizer of the
likelihood of N observed data points x1,...,xN E
118r. Each iteration of the algorithm includes an E
phase and an M phase. The E phase consists of
computing, based on the current B, a probabilis-
tic assignment of each of the N observations to
the K Gaussian distributions. These probabilistic
assignments form an NxK stochastic matrix P,
i.e., a matrix of non-negative numbers in which
each row sums to 1. The M phase consists of
updating the model parameters θ, based on the
current assignments P. For more details, see,
e.g., Bishop (2006).
The structure of the LDC algorithm is very simi-
lar to that of the EM algorithm. Thus, each itera-
tion of LDC consists of an E phase and an M
phase. As observations are replaced by latent
descriptors, an iteration of LDC is best viewed
as starting with the M phase. The M phase first
starts by building a pair of latent-descriptor ma-
trices LP and RP, from the soft assignments ob-
tained in the previous iteration. Note that these
descriptors are now indexed by P, the matrix of
probabilistic assignments, rather than by hard
assignments A as in the previous section.
LP and RP are obtained by a straightforward ad-
aptation of the definition given in the previous
section to the case of probabilistic assignments.
Thus, the latent descriptors consist of the left-
word and right-word contexts (recall that these
are given by matrices L and R), mapped into
left-label and right-label contexts through multi-
plication by the assignment matrix P, and scaled
to unit length:
</bodyText>
<equation confidence="0.967158">
LP = λ(LP)
RP = λ(RP).
</equation>
<bodyText confidence="0.999970190476191">
With these latent descriptors in hand, we pro-
ceed with the M phase of the algorithm as usual.
Thus, the left mean µL k for Gaussian k is the
weighted average of the left latent descriptors
LP(w), scaled to unit length. The weight used in
this weighted average is Pwkxf(w) (remember
that f(w) is the frequency of word type w in the
corpus). Note that the definition of the Gaussian
mean µL k parallels the definition of the cluster
centroid CL(k) given in the previous section; if
the assignment P happens to be a hard assign-
ment, µLk is actually identical to CL(k). The right
Gaussian mean µRk is computed in a similar
fashion. As mentioned, we do not estimate any
mixture coefficients or variance-covariance ma-
trices.
The E phase of the iteration takes the latent de-
scriptors and the Gaussian means, and computes
a new NxK matrix of probabilistic assignments
P. These new assignments are given by:
with Z a normalization constant such that
Ek Pwk = 1. σ is a parameter of the model, which,
as mentioned, is gradually decreased to enforce
convergence of P to a hard assignment.
The description of the M phase given above
does not apply to the first iteration, since the M
phase uses P from the previous iteration. To ini-
tialize the algorithm, i.e., create a set of left and
right descriptor vectors in the M phase of the
first iteration, we use the left-word and right-
word contexts L and R. These matrices however
are of very high dimension (NxN), and thus
sparse and noisy. We therefore reduce their di-
mensionality, using reduced-rank singular-value
decomposition. This yields two Nxr1 matrices,
L1 and R1. A natural choice for r1 is r1 = K, and
this was indeed used for K = 17. For K = 50, we
also use r1 = 17. The left and right descriptors
for the first iteration are obtained by scaling
each row of matrices L1 and R1 to unit length.
The Gaussian centers µL k and µRk, k = 1,...,K, are
set equal to the left and right descriptors of the K
</bodyText>
<equation confidence="0.504329">
Pk - Z pt �k P �k
1 ex — Pl L w— L 2+ l R w— R 2 / 262
}
</equation>
<page confidence="0.985718">
802
</page>
<bodyText confidence="0.999930365853658">
most frequent words in the corpus. This com-
pletes the description of the LDC algorithm.1
While this algorithm is intuitive and simple, it
does not easily lend itself to mathematical
analysis; indeed there is no a priori guarantee
that it will behave as desired. Even for the sim-
pler, hard-assignment, K-means-style version of
LDC outlined in the previous section, there is no
equivalent to the statement—valid for the con-
ventional K-means algorithm—that each itera-
tion lowers the intra-cluster sum of squared dis-
tances F(A); this is a mere consequence of the
fact that the descriptors themselves are updated
on each iteration. The soft-assignment version of
LDC does not directly attempt to minimize F(A),
nor can it be viewed as likelihood maximiza-
tion—as is EM for a Gaussian mixture—since
the use of latent descriptors precludes the defini-
tion of a generative model for the data. This
theoretical difficulty is compounded by the use
of a variable σ.
Empirically however, as shown in the next sec-
tion, we find that the LDC algorithm is very well
behaved. Two simple tools will be used to aid in
the description of the behavior of LDC.
The first tool is an objective function G(P)
that parallels the definition of F(A) for hard as-
signments. For a probabilistic assignment P, we
define G(P) to be the weighted average, over all
w and all k, of ||LP(w) – µLk||2 + ||RP(w) – µRk||2;
the weight used in this average is Pwkxxf(w), just
as in the computation of the Gaussian means.
Clearly, G is identical to F on any P that hap-
pens to be a hard assignment. Thus, G is actually
an extension of the objective function F to soft
assignments.
The second tool will allow us to compute a
tagging accuracy for soft assignments. For this
purpose, we simply create, for any probabilistic
assignment P, the obvious labeling A = A*(P)
that maps w to k with highest Pwk.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9962725">
In order to evaluate the performance of LDC, we
apply it to the Wall Street Journal portion of the
</bodyText>
<footnote confidence="0.979289">
1 The LDC code, including tagging accuracy evaluation, is
available at http://www.dam.brown.edu/people/elie/code/.
</footnote>
<bodyText confidence="0.9968225">
Penn Treebank corpus (1,173,766 tokens, all
lower-case, resulting in N = 43,766 word types).
We compare the induced labels with two gold-
standard tagsets:
</bodyText>
<listItem confidence="0.984596166666667">
• PTB45, the standard 45-tag PTB tagset.
When using PTB45 as the gold standard,
models induce 50 labels, to allow com-
parison with Gao and Johnson (2008)
and Lamar et al. (2010).
• PTB17, the PTB tagset coarse-grained
</listItem>
<bodyText confidence="0.8672305">
to 17 tags (Smith and Eisner 2005).
When using PTB17 as the gold standard,
models induce 17 labels.
In order to compare the labels generated by the
unsupervised model with the tags of each tagset,
we use two map-based criteria:
</bodyText>
<listItem confidence="0.826756">
• MTO: many-to-one tagging accuracy,
</listItem>
<bodyText confidence="0.926521545454545">
i.e., fraction of correctly-tagged tokens
in the corpus under the so-called many-
to-one mapping, which takes each in-
duced tag to the gold-standard POS tag
with which it co-occurs most frequently.
This is the most prevalent metric in use
for unsupervised POS tagging, and we
find it the most reliable of all criteria
currently in use. Accordingly, the study
presented here emphasizes the use of
MTO.
</bodyText>
<listItem confidence="0.998124">
• OTO: best tagging accuracy achievable
under a so-called one-to-one mapping,
i.e., a mapping such that at most one in-
duced tag is sent to any POS tag. The
optimal one-to-one mapping is found
through the Hungarian algorithm2.
</listItem>
<footnote confidence="0.829593333333333">
2 Code by Markus Beuhren is available at
http://www.mathworks.com/matlabcentral/fileexchange/65
43-functions-for-the-rectangular-assignment-problem
</footnote>
<page confidence="0.998112">
803
</page>
<figureCaption confidence="0.987093444444444">
Figure 1: Convergence of LDC with K = 17. Bottom
curve: a -schedule, i.e., sequence of Gaussian widths
employed. Middle curve: Objective function G(P)
(see Section 3). Top curve: Many-to-one tagging
accuracy of labeling A*(P), evaluated against
PTB17.
Figure 2: Same as Figure 1 but with K = 50. Top curve
shows the MTO accuracy of the labeling evaluated
against PTB45.
</figureCaption>
<bodyText confidence="0.999871">
Figures 1 and 2 show the behavior of the LDC
algorithm for K = 17 and K = 50 respectively.
From the G curves as well as from the MTO
scoring curves (using the labeling A*(P) defined
at the end of Section 3), it is clear that the algo-
rithm converges. The figures show only the first
15 iterations, as little change is observed after
that. The schedule of the a parameter was given
the simple form (t) = 1exp{–c(t–1)}, t =
1,2,..., and the parameters 1 and c were ad-
justed so as to get the best MTO accuracy. With
the -schedules used in these experiments, P
typically converges to a hard assignment in
about 45 iterations,  being then 10–5.
While the objective function G(P) mostly de-
creases, it does show a hump for K = 50 around
iteration 9. This may be due to the use of latent
descriptors, or of a variable , or both. The
MTO score sometimes decreases by a small
fraction of a percent, after having reached its
peak around the 15th iteration.
Note that we start  at 0.4 for K = 17, and at
0.5 for K = 50. Although we chose two slightly
different a schedules for the two tagsets in order
to achieve optimal performance on each tagset,
an identical sequence of a can be used for both
with only a 1% drop in PTB17 score.
As the width of the Gaussians narrows, each
vector is steadily pushed toward a single choice
of cluster. This forced choice, in turn, produces
more coherent descriptor vectors for all word
types, and yields a steady increase in tagging
accuracy.
</bodyText>
<page confidence="0.996967">
804
</page>
<table confidence="0.999968090909091">
Criterion Model PTB17 PTB45
MTO LDC 0.751 0.708
SVD2 0.740 0.658
HMM-EM 0.647 0.621
HMM-VB 0.637 0.605
HMM-GS 0.674 0.660
OTO LDC 0.593 0.483
SVD2 0.541 0.473
HMM-EM 0.431 0.405
HMM-VB 0.514 0.461
HMM-GS 0.466 0.499
</table>
<tableCaption confidence="0.9741325">
Table 1. Tagging accuracy comparison between
several models for two tagsets and two mapping
</tableCaption>
<figureCaption confidence="0.6956196">
criteria. Note that LDC significantly outperforms
all HMMs (Gao and Johnson, 2008) in every case
except PTB45 under the OTO mapping. LDC also
outperforms SVD2 (Lamar et al., 2010) in all
cases.
</figureCaption>
<bodyText confidence="0.998338692307692">
Table 1 compares the tagging accuracy of LDC
with several recent models of Gao and Johnson
(2008) and Lamar et al. (2010).
The LDC results shown in the top half of the
table, which uses the MTO criterion, were ob-
tained with the same -schedules as used in Fig-
ures 1 and 2. Note that the LDC algorithm is
deterministic. However, the randomness in the
sparse-matrix implementation of reduced-rank
SVD used in the initialization step causes a
small variability in performance (the standard
deviation of the MTO score is 0.0004 for PTB17
and 0.003 for PTB45). The LDC results reported
are averages over 20 runs. Each run was halted
at iteration 15, and the score reported uses the
labeling A*(P) defined at the end of Section 3.
The LDC results shown in the bottom half of
the table, which uses the OTO criterion, were
obtained with a variant of the LDC algorithm, in
which the M phase estimates not only the Gaus-
sian means but also the mixture coefficients.
Also, different -schedules were used,3
For both PTB17 and PTB45, and under both
criteria, LDC&apos;s performance nearly matches or
exceeds (often by a large margin) the results
achieved by the other models. We find the large
</bodyText>
<footnote confidence="0.9404435">
3 All details are included in the code available at
http://www.dam.brown.edu/people/elie/code/.
</footnote>
<bodyText confidence="0.999745363636364">
increase achieved by LDC in the MTO perform-
ance under the PTB45 tagset particularly com-
pelling. It should be noted that Abend et al.
(2010) report 71.6% MTO accuracy for PTB45,
but they treat all punctuation tags differently in
their evaluation and therefore these results can-
not be directly compared. Berg-Kirkpatrick et al.
(2010) report 75.5% MTO accuracy for PTB45
by incorporating other features such as mor-
phology; Table 1 is limited to distributional-only
methods.
</bodyText>
<figureCaption confidence="0.957724083333333">
Figure 3: Mislabeled words per tag, using the
PTB17 tagset. Black bars indicate mislabeled words
when 17 clusters are used. Gray bars indicate words
that continue to be mislabeled even when every word
type is free to choose its own label, as if each type
were in its own cluster—which defines the theoreti-
cally best possible non-disambiguating model. Top:
fraction of the corpus mislabeled, broken down by
gold tags. Bottom: fraction of tokens of each tag that
are mislabeled. Many of the infrequent tags are
100% mislabeled because no induced label is
mapped to these tags under MTO.
</figureCaption>
<bodyText confidence="0.991390692307693">
Figure 3 demonstrates the mistakes made by
LDC under the MTO mapping. From the top
graph, it is clear that the majority of the missed
tokens are open-class words – most notably ad-
jectives and adverbs. Over 8% of the tokens in
the corpus are mislabeled adjectives – roughly
one-third of all total mislabeled tokens (25.8%).
Furthermore, the corresponding bar in the bot-
tom graph indicates that over half of the adjec-
tives are labeled incorrectly. Similarly, nearly
4% of the mislabeled tokens are adverbs, but
every adverb in the corpus is mislabeled because
no label is mapped to this tag – a common oc-
</bodyText>
<page confidence="0.998207">
805
</page>
<figureCaption confidence="0.995263">
Figure 4: The confusion matrix for LDC&apos;s labeling under PTB17. The area of a black square indicates the number
of tokens in each element of the confusion matrix. The diamonds indicate the induced tag under the MTO map-
ping. Several labels are mapped to N (Noun), and one of these labels causes appreciable confusion between nouns
and adjectives. Because multiple labels are dedicated to a single tag (N, V and PREP), several tags (in this case 7)
are left with no label.
</figureCaption>
<bodyText confidence="0.994190733333333">
currence under MTO, shared by seven of the
seventeen tags.
To further illuminate the errors made by LDC,
we construct the confusion matrix (figure 4).
Element (i,j) of this matrix stores the fraction of
all tokens of POS tag i that are given label j by
the model. In a perfect labeling, exactly one
element of each row and each column would be
non-zero. As illustrated in figure 4, the confu-
sion matrices produced by LDC are far from
perfect. LDC consistently splits the Nouns into
several labels and often confuses Nouns and Ad-
jectives under a single label. These types of
mistakes have been observed as well in models
that use supervision (Haghighi and Klein, 2006).
</bodyText>
<sectionHeader confidence="0.998276" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999852176470588">
When devising a model for unsupervised POS
induction, one challenge is to choose a model of
adequate complexity, this choice being related to
the bias-variance dilemma ubiquitous in statisti-
cal estimation problems. While large datasets are
available, they are typically not large enough to
allow efficient unsupervised learnability in mod-
els that are powerful enough to capture complex
features of natural languages. Ambiguity is one
of these features. Here we propose a new ap-
proach to this set of issues: start with a model
that explicitly entertains ambiguity, and gradu-
ally constrain it so that it eventually converges
to an unambiguous tagger.
Thus, although the algorithm uses probabilis-
tic assignments, of Gaussian-mixture type, the
goal is the construction of hard assignments. By
</bodyText>
<page confidence="0.996264">
806
</page>
<bodyText confidence="0.99998656701031">
requiring the Gaussians to be isotropic with uni-
form width and by allowing that width to shrink
to zero, the algorithm forces the soft assign-
ments to converge to a set of hard assignments.
Based on its performance, this simulated-
annealing-like approach appears to provide a
good compromise in the choice of model com-
plexity.
LDC bears some similarities with the algorithm
of Ney, Essen and Kneser (1994), further im-
plemented, with extensions, by Clark (2003).
Both models use an iterative approach to mini-
mize an objective function, and both initialize
with frequent words. However, the model of
Ney et al. is, in essence, an HMM where each
word type is constrained to belong to a single
class (i.e., in HMM terminology, be emitted by a
single hidden state). Accordingly, the objective
function is the data likelihood under this con-
strained HMM. This takes into account only the
rightward transition probabilities. Our approach
is conceptually rather different from an HMM. It
is more similar to the approach of Schütze
(1995) and Lamar et al. (2010), where each
word type is mapped into a descriptor vector
derived from its left and right tag contexts. Ac-
cordingly, the objective function is that of the K-
means clustering problem, namely a sum of in-
tra-cluster squared distances. This objective
function, unlike the likelihood under an HMM,
takes into account both left and right contexts. It
also makes use in a crucial way of cluster cen-
troids (or Gaussian means), a notion that has no
counterpart in the HMM approach. We note that
LDC achieves much better results (by about
10%) than a recent implementation of the Ney et
al. approach (Reichart et al. 2010).
The only parameters in LDC are the two pa-
rameters used to define the σ schedule, and r1
used in the first iteration. Performance was gen-
erally found to degrade gracefully with changes
in these parameters away from their optimal val-
ues. When σ was made too large in the first few
iterations, it was found that the algorithm con-
verges to the trivial minimum of the objective
function F(A), which maps all word types to a
unique label (see section 2). An alternative
would be to estimate the variance for each Gaus-
sian separately, as is usually done in EM for
Gaussian mixtures. This would not necessarily
preclude the use of an iteration-dependent scal-
ing factor, which would achieve the goal of
gradually forcing the tagging to become deter-
ministic. Investigating this and related options is
left for future work.
Reduced-rank SVD is used in the initialization
of the descriptor vectors, for the optimization to
get off the ground. The details of this initializa-
tion step do not seem to be too critical, as wit-
nessed by robustness against many parameter
changes. For instance, using only the 400 most
frequent words in the corpus—instead of all
words—in the construction of the left-word and
right-word context vectors in iteration 1 causes
no appreciable change in performance.
The probabilistic-assignment algorithm was
found to be much more robust against parameter
changes than the hard-assignment version of
LDC, which parallels the classical K-means
clustering algorithm (see Section 1). We ex-
perimented with this hard-assignment latent-
descriptor clustering algorithm (data not shown),
and found that a number of additional devices
were necessary in order to make it work prop-
erly. In particular, we found it necessary to use
reduced-rank SVD on each iteration of the algo-
rithm—as opposed to just the first iteration in
the version presented here—and to gradually
increase the rank r. Further, we found it neces-
sary to include only the most frequent words at
the beginning, and only gradually incorporate
rare words in the algorithm. Both of these de-
vices require fine tuning. Provided they are in-
deed appropriately tuned, the same level of per-
formance as in the probabilistic-assignment ver-
sion could be achieved. However, as mentioned,
the behavior is much less robust with hard clus-
tering.
Central to the success of LDC is the dynamic
interplay between the progressively harder clus-
ter assignments and the updated latent descriptor
vectors. We operate under the assumption that if
all word types were labeled optimally, words
that share a label should have similar descriptor
vectors arising from this optimal labeling.
These similar vectors would continue to be clus-
tered together, producing a stable equilibrium in
</bodyText>
<page confidence="0.989785">
807
</page>
<bodyText confidence="0.99995735">
the dynamic process. The LDC algorithm dem-
onstrates that, despite starting far from this op-
timal labeling, the alternation between vector
updates and assignment updates is able to pro-
duce steadily improving clusters, as seen by the
steady increase of tagging accuracy.
We envision the possibility of extending this
approach in several ways. It is a relatively sim-
ple matter to extend the descriptor vectors to
include context outside the nearest neighbors,
which may well improve performance. In view
of the computational efficiency of LDC, which
runs in under one minute on a desktop PC, the
added computational burden of working with the
extended context is not likely to be prohibitive.
LDC could also be extended to include morpho-
logical or other features, rather than relying ex-
clusively on context. Again, we would antici-
pate a corresponding increase in accuracy from
this additional linguistic information.
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900986486487">
Omri Abend, Roi Reichart and Ari Rappoport. Im-
proved Unsupervised POS Induction through Pro-
totype Discovery. 2010. In Proceedings of the
48th Annual Meeting of the ACL.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer-Verlag, New
York, LLC.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In proceedings
of NAACL 2010.
Alexander Clark. 2001. The unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. In CoNLL.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech in-
duction. In 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 59–66.
Jianfeng Gao and Mark Johnson. 2008. A comparison
of bayesian estimators for unsupervised Hidden
Markov Model POS taggers. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 344–352.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics, pages 744–751.
João V. Graça, Kuzman Ganchev, Ben Taskar, and
Fernando Pereira. 2009. Posterior vs. Parameter
Sparsity in Latent Variable Models. Neural Infor-
mation Processing Systems Conference (NIPS).
Michael Lamar, Yariv Maron, Mark Johnson, Elie
Bienenstock. 2010. SVD and Clustering for Unsu-
pervised POS Tagging. In Proceedings of the 48th
Annual Meeting of the ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Main Conference, pages 320–
327, New York City, USA, June. Association for
Computational Linguistics.
William P. Headden, David McClosky, and Eugene
Charniak. 2008. Evaluating unsupervised part-of-
speech tagging for grammar induction. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING ’08).
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages
296–305.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependences in
stochastic language modelling. Computer Speech
and Language, 8, 1-38.
Roi Reichart, Raanan Fattal and Ari Rappoport.
2010. Improved Unsupervised POS Induction Us-
ing Intrinsic Clustering Quality and a Zipfian Con-
straint. CoNLL.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
Proceedings of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP, pages
504–512.
Hinrich Schütze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
on European chapter of the Association for Com-
putational Linguistics, pages 141–148.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unla-
beled data. In Proceedings of the 43rd Annual
</reference>
<page confidence="0.981297">
808
</page>
<reference confidence="0.996824727272727">
Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 354–362.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of HLT-NAACL 2003, pages
252-259.
Yoshimasa Tsuruoka and Jun&apos;ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy
for Tagging Sequence Data. In Proceedings of
HLT/EMNLP, pp. 467-474.
</reference>
<page confidence="0.99889">
809
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.098558">
<title confidence="0.999607">Latent-Descriptor Clustering for Unsupervised POS Induction</title>
<author confidence="0.998192">Michael</author>
<affiliation confidence="0.999296">Department of Mathematics and Computer</affiliation>
<address confidence="0.940853333333333">Saint Louis 220 N. Grand St.Louis, MO 63103, USA</address>
<email confidence="0.999347">mlamar@slu.edu</email>
<author confidence="0.445028">Yariv Gonda Brain Research</author>
<affiliation confidence="0.551029">Bar-Ilan</affiliation>
<address confidence="0.778465">Ramat-Gan 52900, Israel</address>
<email confidence="0.99915">syarivm@yahoo.com</email>
<author confidence="0.914714">Elie</author>
<affiliation confidence="0.989000666666667">Division of Applied and Department of Brown</affiliation>
<address confidence="0.953918">Providence, RI 02912,</address>
<email confidence="0.999922">elie@brown.edu</email>
<abstract confidence="0.994509388888889">We present a novel approach to distributionalonly, fully unsupervised, POS tagging, based on an adaptation of the EM algorithm for the estimation of a Gaussian mixture. In this approach, which we call Latent-Descriptor Clustering (LDC), word types are clustered using a series of progressively more informative descriptor vectors. These descriptors, which are computed from the immediate left and right context of each word in the corpus, are updated based on the previous state of the cluster assignments. The LDC algorithm is simple and intuitive. Using standard evaluation criteria for unsupervised POS tagging, LDC shows a substantial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Improved Unsupervised POS Induction through Prototype Discovery.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2688" citStr="Abend et al., 2010" startWordPosition="403" endWordPosition="406">do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisambiguating models (Abend et al., 2010; Lamar et al., 2010): these</context>
<context position="22350" citStr="Abend et al. (2010)" startWordPosition="3835" endWordPosition="3838">he OTO criterion, were obtained with a variant of the LDC algorithm, in which the M phase estimates not only the Gaussian means but also the mixture coefficients. Also, different -schedules were used,3 For both PTB17 and PTB45, and under both criteria, LDC&apos;s performance nearly matches or exceeds (often by a large margin) the results achieved by the other models. We find the large 3 All details are included in the code available at http://www.dam.brown.edu/people/elie/code/. increase achieved by LDC in the MTO performance under the PTB45 tagset particularly compelling. It should be noted that Abend et al. (2010) report 71.6% MTO accuracy for PTB45, but they treat all punctuation tags differently in their evaluation and therefore these results cannot be directly compared. Berg-Kirkpatrick et al. (2010) report 75.5% MTO accuracy for PTB45 by incorporating other features such as morphology; Table 1 is limited to distributional-only methods. Figure 3: Mislabeled words per tag, using the PTB17 tagset. Black bars indicate mislabeled words when 17 clusters are used. Gray bars indicate words that continue to be mislabeled even when every word type is free to choose its own label, as if each type were in its </context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2010</marker>
<rawString>Omri Abend, Roi Reichart and Ari Rappoport. Improved Unsupervised POS Induction through Prototype Discovery. 2010. In Proceedings of the 48th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>New York, LLC.</location>
<contexts>
<context position="12106" citStr="Bishop (2006)" startWordPosition="2045" endWordPosition="2046"> B. EM is an iterative algorithm used to find a (local) maximizer of the likelihood of N observed data points x1,...,xN E 118r. Each iteration of the algorithm includes an E phase and an M phase. The E phase consists of computing, based on the current B, a probabilistic assignment of each of the N observations to the K Gaussian distributions. These probabilistic assignments form an NxK stochastic matrix P, i.e., a matrix of non-negative numbers in which each row sums to 1. The M phase consists of updating the model parameters θ, based on the current assignments P. For more details, see, e.g., Bishop (2006). The structure of the LDC algorithm is very similar to that of the EM algorithm. Thus, each iteration of LDC consists of an E phase and an M phase. As observations are replaced by latent descriptors, an iteration of LDC is best viewed as starting with the M phase. The M phase first starts by building a pair of latent-descriptor matrices LP and RP, from the soft assignments obtained in the previous iteration. Note that these descriptors are now indexed by P, the matrix of probabilistic assignments, rather than by hard assignments A as in the previous section. LP and RP are obtained by a straig</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer-Verlag, New York, LLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Côté</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless Unsupervised Learning with Features.</title>
<date>2010</date>
<booktitle>In proceedings of NAACL</booktitle>
<contexts>
<context position="2763" citStr="Berg-Kirkpatrick et al., 2010" startWordPosition="415" endWordPosition="418">s across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisambiguating models (Abend et al., 2010; Lamar et al., 2010): these assign the same tag to all tokens of a word type, rather than attempting t</context>
<context position="22543" citStr="Berg-Kirkpatrick et al. (2010)" startWordPosition="3864" endWordPosition="3867">chedules were used,3 For both PTB17 and PTB45, and under both criteria, LDC&apos;s performance nearly matches or exceeds (often by a large margin) the results achieved by the other models. We find the large 3 All details are included in the code available at http://www.dam.brown.edu/people/elie/code/. increase achieved by LDC in the MTO performance under the PTB45 tagset particularly compelling. It should be noted that Abend et al. (2010) report 71.6% MTO accuracy for PTB45, but they treat all punctuation tags differently in their evaluation and therefore these results cannot be directly compared. Berg-Kirkpatrick et al. (2010) report 75.5% MTO accuracy for PTB45 by incorporating other features such as morphology; Table 1 is limited to distributional-only methods. Figure 3: Mislabeled words per tag, using the PTB17 tagset. Black bars indicate mislabeled words when 17 clusters are used. Gray bars indicate words that continue to be mislabeled even when every word type is free to choose its own label, as if each type were in its own cluster—which defines the theoretically best possible non-disambiguating model. Top: fraction of the corpus mislabeled, broken down by gold tags. Bottom: fraction of tokens of each tag that</context>
</contexts>
<marker>Berg-Kirkpatrick, Bouchard-Côté, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté, John DeNero, and Dan Klein. 2010. Painless Unsupervised Learning with Features. In proceedings of NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>The unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="2597" citStr="Clark, 2001" startWordPosition="389" endWordPosition="390">aration of such resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two rece</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. The unsupervised induction of stochastic context-free grammars using distributional clustering. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="2610" citStr="Clark, 2003" startWordPosition="391" endWordPosition="392">ch resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers adv</context>
<context position="26277" citStr="Clark (2003)" startWordPosition="4491" endWordPosition="4492">s tagger. Thus, although the algorithm uses probabilistic assignments, of Gaussian-mixture type, the goal is the construction of hard assignments. By 806 requiring the Gaussians to be isotropic with uniform width and by allowing that width to shrink to zero, the algorithm forces the soft assignments to converge to a set of hard assignments. Based on its performance, this simulatedannealing-like approach appears to provide a good compromise in the choice of model complexity. LDC bears some similarities with the algorithm of Ney, Essen and Kneser (1994), further implemented, with extensions, by Clark (2003). Both models use an iterative approach to minimize an objective function, and both initialize with frequent words. However, the model of Ney et al. is, in essence, an HMM where each word type is constrained to belong to a single class (i.e., in HMM terminology, be emitted by a single hidden state). Accordingly, the objective function is the data likelihood under this constrained HMM. This takes into account only the rightward transition probabilities. Our approach is conceptually rather different from an HMM. It is more similar to the approach of Schütze (1995) and Lamar et al. (2010), where </context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of bayesian estimators for unsupervised Hidden Markov Model POS taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="2648" citStr="Gao and Johnson, 2008" startWordPosition="395" endWordPosition="398"> Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisambiguating models (Abend </context>
<context position="17484" citStr="Gao and Johnson (2008)" startWordPosition="3002" endWordPosition="3005">obabilistic assignment P, the obvious labeling A = A*(P) that maps w to k with highest Pwk. 4 Results In order to evaluate the performance of LDC, we apply it to the Wall Street Journal portion of the 1 The LDC code, including tagging accuracy evaluation, is available at http://www.dam.brown.edu/people/elie/code/. Penn Treebank corpus (1,173,766 tokens, all lower-case, resulting in N = 43,766 word types). We compare the induced labels with two goldstandard tagsets: • PTB45, the standard 45-tag PTB tagset. When using PTB45 as the gold standard, models induce 50 labels, to allow comparison with Gao and Johnson (2008) and Lamar et al. (2010). • PTB17, the PTB tagset coarse-grained to 17 tags (Smith and Eisner 2005). When using PTB17 as the gold standard, models induce 17 labels. In order to compare the labels generated by the unsupervised model with the tags of each tagset, we use two map-based criteria: • MTO: many-to-one tagging accuracy, i.e., fraction of correctly-tagged tokens in the corpus under the so-called manyto-one mapping, which takes each induced tag to the gold-standard POS tag with which it co-occurs most frequently. This is the most prevalent metric in use for unsupervised POS tagging, and </context>
<context position="20827" citStr="Gao and Johnson, 2008" startWordPosition="3574" endWordPosition="3577">e Gaussians narrows, each vector is steadily pushed toward a single choice of cluster. This forced choice, in turn, produces more coherent descriptor vectors for all word types, and yields a steady increase in tagging accuracy. 804 Criterion Model PTB17 PTB45 MTO LDC 0.751 0.708 SVD2 0.740 0.658 HMM-EM 0.647 0.621 HMM-VB 0.637 0.605 HMM-GS 0.674 0.660 OTO LDC 0.593 0.483 SVD2 0.541 0.473 HMM-EM 0.431 0.405 HMM-VB 0.514 0.461 HMM-GS 0.466 0.499 Table 1. Tagging accuracy comparison between several models for two tagsets and two mapping criteria. Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. LDC also outperforms SVD2 (Lamar et al., 2010) in all cases. Table 1 compares the tagging accuracy of LDC with several recent models of Gao and Johnson (2008) and Lamar et al. (2010). The LDC results shown in the top half of the table, which uses the MTO criterion, were obtained with the same -schedules as used in Figures 1 and 2. Note that the LDC algorithm is deterministic. However, the randomness in the sparse-matrix implementation of reduced-rank SVD used in the initialization step causes a small variability in performance (the standard d</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of bayesian estimators for unsupervised Hidden Markov Model POS taggers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="1682" citStr="Goldwater and Griffiths, 2007" startWordPosition="236" endWordPosition="239">POS tagging, LDC shows a substantial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost. 1 Introduction Part-of-speech (POS) tagging is a fundamental natural-language-processing problem, and POS tags are used as input to many important applications. While state-of-the-art supervised POS taggers are more than 97% accurate (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), unsupervised POS taggers continue to lag far behind. Several authors addressed this gap using limited supervision, such as a dictionary of tags for each word (Goldwater and Griffiths, 2007; Ravi and Knight, 2009), or a list of word prototypes for each tag (Haghighi and Klein, 2006). Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons. First, most languages do not have a tag dictionary. Second, the preparation of such resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008),</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>João V Graça</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Fernando Pereira</author>
</authors>
<title>Posterior vs.</title>
<date>2009</date>
<booktitle>Parameter Sparsity in Latent Variable Models. Neural Information Processing Systems Conference (NIPS).</booktitle>
<contexts>
<context position="2668" citStr="Graça et al., 2009" startWordPosition="399" endWordPosition="402">idely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisambiguating models (Abend et al., 2010; Lamar </context>
</contexts>
<marker>Graça, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>João V. Graça, Kuzman Ganchev, Ben Taskar, and Fernando Pereira. 2009. Posterior vs. Parameter Sparsity in Latent Variable Models. Neural Information Processing Systems Conference (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lamar</author>
<author>Yariv Maron</author>
<author>Mark Johnson</author>
<author>Elie Bienenstock</author>
</authors>
<title>SVD and Clustering for Unsupervised POS Tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2708" citStr="Lamar et al., 2010" startWordPosition="407" endWordPosition="410">s do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisambiguating models (Abend et al., 2010; Lamar et al., 2010): these assign the same tag</context>
<context position="17508" citStr="Lamar et al. (2010)" startWordPosition="3007" endWordPosition="3010">he obvious labeling A = A*(P) that maps w to k with highest Pwk. 4 Results In order to evaluate the performance of LDC, we apply it to the Wall Street Journal portion of the 1 The LDC code, including tagging accuracy evaluation, is available at http://www.dam.brown.edu/people/elie/code/. Penn Treebank corpus (1,173,766 tokens, all lower-case, resulting in N = 43,766 word types). We compare the induced labels with two goldstandard tagsets: • PTB45, the standard 45-tag PTB tagset. When using PTB45 as the gold standard, models induce 50 labels, to allow comparison with Gao and Johnson (2008) and Lamar et al. (2010). • PTB17, the PTB tagset coarse-grained to 17 tags (Smith and Eisner 2005). When using PTB17 as the gold standard, models induce 17 labels. In order to compare the labels generated by the unsupervised model with the tags of each tagset, we use two map-based criteria: • MTO: many-to-one tagging accuracy, i.e., fraction of correctly-tagged tokens in the corpus under the so-called manyto-one mapping, which takes each induced tag to the gold-standard POS tag with which it co-occurs most frequently. This is the most prevalent metric in use for unsupervised POS tagging, and we find it the most reli</context>
<context position="20924" citStr="Lamar et al., 2010" startWordPosition="3591" endWordPosition="3594">hoice, in turn, produces more coherent descriptor vectors for all word types, and yields a steady increase in tagging accuracy. 804 Criterion Model PTB17 PTB45 MTO LDC 0.751 0.708 SVD2 0.740 0.658 HMM-EM 0.647 0.621 HMM-VB 0.637 0.605 HMM-GS 0.674 0.660 OTO LDC 0.593 0.483 SVD2 0.541 0.473 HMM-EM 0.431 0.405 HMM-VB 0.514 0.461 HMM-GS 0.466 0.499 Table 1. Tagging accuracy comparison between several models for two tagsets and two mapping criteria. Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. LDC also outperforms SVD2 (Lamar et al., 2010) in all cases. Table 1 compares the tagging accuracy of LDC with several recent models of Gao and Johnson (2008) and Lamar et al. (2010). The LDC results shown in the top half of the table, which uses the MTO criterion, were obtained with the same -schedules as used in Figures 1 and 2. Note that the LDC algorithm is deterministic. However, the randomness in the sparse-matrix implementation of reduced-rank SVD used in the initialization step causes a small variability in performance (the standard deviation of the MTO score is 0.0004 for PTB17 and 0.003 for PTB45). The LDC results reported are </context>
<context position="26869" citStr="Lamar et al. (2010)" startWordPosition="4588" endWordPosition="4591">extensions, by Clark (2003). Both models use an iterative approach to minimize an objective function, and both initialize with frequent words. However, the model of Ney et al. is, in essence, an HMM where each word type is constrained to belong to a single class (i.e., in HMM terminology, be emitted by a single hidden state). Accordingly, the objective function is the data likelihood under this constrained HMM. This takes into account only the rightward transition probabilities. Our approach is conceptually rather different from an HMM. It is more similar to the approach of Schütze (1995) and Lamar et al. (2010), where each word type is mapped into a descriptor vector derived from its left and right tag contexts. Accordingly, the objective function is that of the Kmeans clustering problem, namely a sum of intra-cluster squared distances. This objective function, unlike the likelihood under an HMM, takes into account both left and right contexts. It also makes use in a crucial way of cluster centroids (or Gaussian means), a notion that has no counterpart in the HMM approach. We note that LDC achieves much better results (by about 10%) than a recent implementation of the Ney et al. approach (Reichart e</context>
</contexts>
<marker>Lamar, Maron, Johnson, Bienenstock, 2010</marker>
<rawString>Michael Lamar, Yariv Maron, Mark Johnson, Elie Bienenstock. 2010. SVD and Clustering for Unsupervised POS Tagging. In Proceedings of the 48th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototypedriven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="1776" citStr="Haghighi and Klein, 2006" startWordPosition="253" endWordPosition="256">ng with a several-fold reduction in computational cost. 1 Introduction Part-of-speech (POS) tagging is a fundamental natural-language-processing problem, and POS tags are used as input to many important applications. While state-of-the-art supervised POS taggers are more than 97% accurate (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), unsupervised POS taggers continue to lag far behind. Several authors addressed this gap using limited supervision, such as a dictionary of tags for each word (Goldwater and Griffiths, 2007; Ravi and Knight, 2009), or a list of word prototypes for each tag (Haghighi and Klein, 2006). Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons. First, most languages do not have a tag dictionary. Second, the preparation of such resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a ful</context>
<context position="25011" citStr="Haghighi and Klein, 2006" startWordPosition="4288" endWordPosition="4291"> seventeen tags. To further illuminate the errors made by LDC, we construct the confusion matrix (figure 4). Element (i,j) of this matrix stores the fraction of all tokens of POS tag i that are given label j by the model. In a perfect labeling, exactly one element of each row and each column would be non-zero. As illustrated in figure 4, the confusion matrices produced by LDC are far from perfect. LDC consistently splits the Nouns into several labels and often confuses Nouns and Adjectives under a single label. These types of mistakes have been observed as well in models that use supervision (Haghighi and Klein, 2006). 5 Discussion When devising a model for unsupervised POS induction, one challenge is to choose a model of adequate complexity, this choice being related to the bias-variance dilemma ubiquitous in statistical estimation problems. While large datasets are available, they are typically not large enough to allow efficient unsupervised learnability in models that are powerful enough to capture complex features of natural languages. Ambiguity is one of these features. Here we propose a new approach to this set of issues: start with a model that explicitly entertains ambiguity, and gradually constra</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototypedriven learning for sequence models. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 320– 327, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden</author>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Evaluating unsupervised part-ofspeech tagging for grammar induction.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING ’08).</booktitle>
<contexts>
<context position="2281" citStr="Headden et al. 2008" startWordPosition="337" endWordPosition="340">r and Griffiths, 2007; Ravi and Knight, 2009), or a list of word prototypes for each tag (Haghighi and Klein, 2006). Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons. First, most languages do not have a tag dictionary. Second, the preparation of such resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; B</context>
</contexts>
<marker>Headden, McClosky, Charniak, 2008</marker>
<rawString>William P. Headden, David McClosky, and Eugene Charniak. 2008. Evaluating unsupervised part-ofspeech tagging for grammar induction. In Proceedings of the International Conference on Computational Linguistics (COLING ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<contexts>
<context position="2625" citStr="Johnson, 2007" startWordPosition="393" endWordPosition="394">is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisamb</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependences in stochastic language modelling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<volume>8</volume>
<pages>1--38</pages>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in stochastic language modelling. Computer Speech and Language, 8, 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Raanan Fattal</author>
<author>Ari Rappoport</author>
</authors>
<title>Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint.</title>
<date>2010</date>
<publisher>CoNLL.</publisher>
<contexts>
<context position="2731" citStr="Reichart et al., 2010" startWordPosition="411" endWordPosition="414">any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statistics. Two recent papers advocate nondisambiguating models (Abend et al., 2010; Lamar et al., 2010): these assign the same tag to all tokens of a wor</context>
<context position="27480" citStr="Reichart et al. 2010" startWordPosition="4694" endWordPosition="4697">al. (2010), where each word type is mapped into a descriptor vector derived from its left and right tag contexts. Accordingly, the objective function is that of the Kmeans clustering problem, namely a sum of intra-cluster squared distances. This objective function, unlike the likelihood under an HMM, takes into account both left and right contexts. It also makes use in a crucial way of cluster centroids (or Gaussian means), a notion that has no counterpart in the HMM approach. We note that LDC achieves much better results (by about 10%) than a recent implementation of the Ney et al. approach (Reichart et al. 2010). The only parameters in LDC are the two parameters used to define the σ schedule, and r1 used in the first iteration. Performance was generally found to degrade gracefully with changes in these parameters away from their optimal values. When σ was made too large in the first few iterations, it was found that the algorithm converges to the trivial minimum of the objective function F(A), which maps all word types to a unique label (see section 2). An alternative would be to estimate the variance for each Gaussian separately, as is usually done in EM for Gaussian mixtures. This would not necessa</context>
</contexts>
<marker>Reichart, Fattal, Rappoport, 2010</marker>
<rawString>Roi Reichart, Raanan Fattal and Ari Rappoport. 2010. Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>504--512</pages>
<contexts>
<context position="1706" citStr="Ravi and Knight, 2009" startWordPosition="240" endWordPosition="243">ntial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost. 1 Introduction Part-of-speech (POS) tagging is a fundamental natural-language-processing problem, and POS tags are used as input to many important applications. While state-of-the-art supervised POS taggers are more than 97% accurate (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), unsupervised POS taggers continue to lag far behind. Several authors addressed this gap using limited supervision, such as a dictionary of tags for each word (Goldwater and Griffiths, 2007; Ravi and Knight, 2009), or a list of word prototypes for each tag (Haghighi and Klein, 2006). Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons. First, most languages do not have a tag dictionary. Second, the preparation of such resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statisti</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 504–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="2584" citStr="Schütze, 1995" startWordPosition="387" endWordPosition="388">econd, the preparation of such resources is error-prone. Third, while several widely used tag sets do exist, researchers do not agree upon any specific set of tags across languages or even within one language. Since tags are used as basic features for many important NLP applications (e.g. Headden et al. 2008), exploring new, statistically motivated, tag sets may also be useful. For these reasons, a fully unsupervised induction algorithm has both a practical and a theoretical value. In the past decade, there has been a steady improvement on the completely unsupervised version of POS induction (Schütze, 1995; Clark, 2001; Clark, 2003; Johnson, 2007; Gao and Johnson, 2008; Graça et al., 2009; Abend et al., 2010; Lamar et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010). Some of these methods use morphological cues (Clark, 2001; Clark, 2003; Abend et al., 2010; Reichart et al., 2010; Berg-Kirkpatrick et al., 2010), but all rely heavily on distributional information, i.e., bi799 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799–809, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics gram statist</context>
<context position="26845" citStr="Schütze (1995)" startWordPosition="4585" endWordPosition="4586"> implemented, with extensions, by Clark (2003). Both models use an iterative approach to minimize an objective function, and both initialize with frequent words. However, the model of Ney et al. is, in essence, an HMM where each word type is constrained to belong to a single class (i.e., in HMM terminology, be emitted by a single hidden state). Accordingly, the objective function is the data likelihood under this constrained HMM. This takes into account only the rightward transition probabilities. Our approach is conceptually rather different from an HMM. It is more similar to the approach of Schütze (1995) and Lamar et al. (2010), where each word type is mapped into a descriptor vector derived from its left and right tag contexts. Accordingly, the objective function is that of the Kmeans clustering problem, namely a sum of intra-cluster squared distances. This objective function, unlike the likelihood under an HMM, takes into account both left and right contexts. It also makes use in a crucial way of cluster centroids (or Gaussian means), a notion that has no counterpart in the HMM approach. We note that LDC achieves much better results (by about 10%) than a recent implementation of the Ney et </context>
</contexts>
<marker>Schütze, 1995</marker>
<rawString>Hinrich Schütze. 1995. Distributional part-of-speech tagging. In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>354--362</pages>
<contexts>
<context position="17583" citStr="Smith and Eisner 2005" startWordPosition="3020" endWordPosition="3023">ts In order to evaluate the performance of LDC, we apply it to the Wall Street Journal portion of the 1 The LDC code, including tagging accuracy evaluation, is available at http://www.dam.brown.edu/people/elie/code/. Penn Treebank corpus (1,173,766 tokens, all lower-case, resulting in N = 43,766 word types). We compare the induced labels with two goldstandard tagsets: • PTB45, the standard 45-tag PTB tagset. When using PTB45 as the gold standard, models induce 50 labels, to allow comparison with Gao and Johnson (2008) and Lamar et al. (2010). • PTB17, the PTB tagset coarse-grained to 17 tags (Smith and Eisner 2005). When using PTB17 as the gold standard, models induce 17 labels. In order to compare the labels generated by the unsupervised model with the tags of each tagset, we use two map-based criteria: • MTO: many-to-one tagging accuracy, i.e., fraction of correctly-tagged tokens in the corpus under the so-called manyto-one mapping, which takes each induced tag to the gold-standard POS tag with which it co-occurs most frequently. This is the most prevalent metric in use for unsupervised POS tagging, and we find it the most reliable of all criteria currently in use. Accordingly, the study presented her</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich partof-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="1464" citStr="Toutanova et al., 2003" startWordPosition="202" endWordPosition="205">t and right context of each word in the corpus, are updated based on the previous state of the cluster assignments. The LDC algorithm is simple and intuitive. Using standard evaluation criteria for unsupervised POS tagging, LDC shows a substantial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost. 1 Introduction Part-of-speech (POS) tagging is a fundamental natural-language-processing problem, and POS tags are used as input to many important applications. While state-of-the-art supervised POS taggers are more than 97% accurate (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), unsupervised POS taggers continue to lag far behind. Several authors addressed this gap using limited supervision, such as a dictionary of tags for each word (Goldwater and Griffiths, 2007; Ravi and Knight, 2009), or a list of word prototypes for each tag (Haghighi and Klein, 2006). Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons. First, most languages do not have a tag dictionary. Second, the preparation of such resources is error-prone. Third, while several widely used tag</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning and Yoram Singer. 2003. Feature-rich partof-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>467--474</pages>
<contexts>
<context position="1492" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="206" endWordPosition="209">ach word in the corpus, are updated based on the previous state of the cluster assignments. The LDC algorithm is simple and intuitive. Using standard evaluation criteria for unsupervised POS tagging, LDC shows a substantial improvement in performance over state-of-the-art methods, along with a several-fold reduction in computational cost. 1 Introduction Part-of-speech (POS) tagging is a fundamental natural-language-processing problem, and POS tags are used as input to many important applications. While state-of-the-art supervised POS taggers are more than 97% accurate (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), unsupervised POS taggers continue to lag far behind. Several authors addressed this gap using limited supervision, such as a dictionary of tags for each word (Goldwater and Griffiths, 2007; Ravi and Knight, 2009), or a list of word prototypes for each tag (Haghighi and Klein, 2006). Even in light of all these advancements, there is still interest in a completely unsupervised method for POS induction for several reasons. First, most languages do not have a tag dictionary. Second, the preparation of such resources is error-prone. Third, while several widely used tag sets do exist, researchers </context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun&apos;ichi Tsujii. 2005. Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data. In Proceedings of HLT/EMNLP, pp. 467-474.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>