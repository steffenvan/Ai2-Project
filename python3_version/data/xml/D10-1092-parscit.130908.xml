<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.944506">
Automatic Evaluation of Translation Quality for Distant Language Pairs
</title>
<author confidence="0.733603">
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada
</author>
<affiliation confidence="0.5752">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.73052">
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
</address>
<email confidence="0.998135">
{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.994854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998255678571429">
Automatic evaluation of Machine Translation
(MT) quality is essential to developing high-
quality MT systems. Various evaluation met-
rics have been proposed, and BLEU is now
used as the de facto standard metric. How-
ever, when we consider translation between
distant language pairs such as Japanese and
English, most popular metrics (e.g., BLEU,
NIST, PER, and TER) do not work well. It
is well known that Japanese and English have
completely different word orders, and special
care must be paid to word order in transla-
tion. Otherwise, translations with wrong word
order often lead to misunderstanding and in-
comprehensibility. For instance, SMT-based
Japanese-to-English translators tend to trans-
late ‘A because B’ as ‘B because A.’ Thus,
word order is the most important problem
for distant language translation. However,
conventional evaluation metrics do not sig-
nificantly penalize such word order mistakes.
Therefore, locally optimizing these metrics
leads to inadequate translations. In this pa-
per, we propose an automatic evaluation met-
ric based on rank correlation coefficients mod-
ified with precision. Our meta-evaluation of
the NTCIR-7 PATMT JE task data shows that
this metric outperforms conventional metrics.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96264987804878">
Automatic evaluation of machine translation (MT)
quality is essential to developing high-quality ma-
chine translation systems because human evaluation
is time consuming, expensive, and irreproducible. If
we have a perfect automatic evaluation metric, we
can tune our translation system for the metric.
BLEU (Papineni et al., 2002b; Papineni et al.,
2002a) showed high correlation with human judg-
ments and is still used as the de facto standard au-
tomatic evaluation metric. However, Callison-Burch
et al. (2006) argued that the MT community is overly
reliant on BLEU by showing examples of poor per-
formance. For Japanese-to-English (JE) translation,
Echizen-ya et al. (2009) showed that the popular
BLEU and NIST do not work well by using the sys-
tem outputs of the NTCIR-7 PATMT (patent transla-
tion) JE task (Fujii et al., 2008). On the other hand,
ROUGE-L (Lin and Hovy, 2003), Word Error Rate
(WER), and IMPACT (Echizen-ya and Araki, 2007)
worked better.
In these studies, Pearson’s correlation coefficient
and Spearman’s rank correlation p with human eval-
uation scores are used to measure how closely an
automatic evaluation method correlates with human
evaluation. This evaluation of automatic evaluation
methods is called meta-evaluation. In human eval-
uation, people judge the adequacy and the fluency of
each translation.
Denoual and Lepage (2005) pointed out that
BLEU assumes word boundaries, which is ambigu-
ous in Japanese and Chinese. Here, we assume
the word boundaries given by ChaSen, one of the
standard morphological analyzers (http://chasen-
legacy.sourceforge.jp/) following Fujii et al.
(2008)
In JE translation, most Statistical Machine Trans-
lation (SMT) systems translate the Japanese sen-
tence
(J0) kare wa sono hon wo yonda node
sekaishi ni kyoumi ga atta
which means
</bodyText>
<page confidence="0.952609">
944
</page>
<note confidence="0.865594">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.880601583333333">
(R0) he was interested in world
history because he read the book
into an English sentence such as
(H0) he read the book because he was
interested in world history
in which the cause and the effect are swapped. Why
does this happen? The former half of (J0) means “He
read the book,” and the latter half means “(he) was
interested in world history.” The middle word
“node” between them corresponds to “because.”
Therefore, SMT systems output sentences like (H0).
On the other hand, Rule-based Machine Translation
(RBMT) systems correctly give (R0).
In order to find (R0), SMT systems have to search
a very large space because we cannot restrict its
search space with a small distortion limit. Most
SMT systems thus fail to find (R0).
Consequently, the global word order is essential
for translation between distant language pairs, and
wrong word order can easily lead to misunderstand-
ing or incomprehensibility. Perhaps, some readers
do not understand why we emphasize word order
from this example alone. A few more examples
will clarify what happens when SMT is applied to
Japanese-to-English translation. Even the most fa-
mous SMT service available on the web failed to
translate the following very simple sentence at the
time of writing this paper.
Japanese: meari wa jon wo koroshita.
Reference: Mary killed John.
SMT output: John killed Mary.
Since it cannot translate such a simple sentence, it
obviously cannot translate more complex sentences
correctly.
Japanese: bobu ga katta hon wo jon wa yonda.
Reference: John read a book that Bob bought.
SMT output: Bob read the book John bought.
Another example is:
Japanese: bobu wa meari ni yubiwa wo kau
tameni, jon no mise ni itta.
Reference: Bob went to John’s store to buy a
ring for Mary.
SMT output: Bob Mary to buy the ring, John
went to the store.
In this way, this SMT service usually gives incom-
prehensible or misleading translations, and thus peo-
ple prefer RBMT services. Other SMT systems also
tend to make similar word order mistakes, and spe-
cial care should be paid to the translation between
distant language pairs such as Japanese and English.
Even Japanese people cannot solve this word or-
der problem easily: It is well known that Japanese
people are not good at speaking English.
From this point of view, conventional automatic
evaluation metrics of translation quality disregard
word order mistakes too much. Single-reference
BLEU is defined by a geometrical mean of n-gram
precisions p,,, and is modified by Brevity Penalty
(BP) min(1, exp(1− r/h)), where r is the length of
the reference and h is the length of the hypothesis.
</bodyText>
<equation confidence="0.98692">
BLEU = BP x (p1p2p3p4)1/4.
</equation>
<bodyText confidence="0.997376266666667">
Its range is [0, 1]. The BLEU score of (H0) with ref-
erence (R0) is 1.0x(11/11x9/10x6/9x4/8)1/4 =
0.740. Therefore, BLEU gives a very good score to
this inadequate translation because it checks only n-
grams and does not regard global word order.
Since (R0) and (H0) look similar in terms of flu-
ency, adequacy is more important than fluency in
the translation between distant language pairs.
Similarly, other popular scores such as NIST,
PER, and TER (Snover et al., 2006) also give
relatively good scores to this translation. NIST
also considers only local word orders (n-grams).
PER (Position-Independent Word Error Rate) was
designed to disregard word order completely.
TER (Snover et al., 2006) was designed to allow
phrase movements without large penalties. There-
fore, these standard metrics are not optimal for eval-
uating translation between distant language pairs.
In this paper, we propose an alternative automatic
evaluation metric appropriate for distant language
pairs. Our method is based on rank correlation co-
efficients. We use them to compare the word ranks
in the reference with those in the hypothesis.
There are two popular rank correlation coeffi-
cients: Spearman’s p and Kendall’s r (Kendall,
1975). In Isozaki et al. (2010), we used Kendall’s r
to measure the effectiveness of our Head Finaliza-
tion rule as a preprocessor for English-to-Japanese
translation, but we measured the quality of transla-
tion by using conventional metrics.
</bodyText>
<page confidence="0.998144">
945
</page>
<bodyText confidence="0.998288833333333">
It is not clear how well T works as an automatic
evaluation metric of translation quality. Moreover,
Spearman’s p might work better than Kendall’s T.
As we discuss later, T considers only the direction
of the rank change, whereas p considers the distance
of the change.
The first objective of this paper is to examine
which is the better metric for distant language pairs.
The second objective is to find improvements of
these rank correlation-metrics.
Spearman’s p is based on Pearson’s correlation
coefficients. Suppose we have two lists of numbers
</bodyText>
<equation confidence="0.943584">
x = [0.1, 0.4, 0.2, 0.6],
y = [0.9, 0.6, 0.2, 0.7].
</equation>
<bodyText confidence="0.925521285714286">
To obtain Pearson’s coefficients between x and y,
we use the raw values in these lists. If we substitute
their ranks for their raw values, we get
x&apos; = [1, 3, 2, 4] and y&apos; = [4, 2, 1, 3].
Then, Spearman’s p between x and y is given by
Pearson’s coefficients between x&apos; and y&apos;. This p
can be rewritten as follows when there is no tie:
</bodyText>
<equation confidence="0.992086333333333">
� i d2 i
p = 1 − .
n+1C3
</equation>
<bodyText confidence="0.999931785714286">
Here, di indicates the difference in the ranks of the
i-th element. Rank distances are squared in this
formula. Because of this square, we expect that p
decreases drastically when there is an element that
significantly changes in rank. But we are also afraid
that p may be too severe for alternative good trans-
lations.
Since Pearson’s correlation metric assumes lin-
earity, nonlinear monotonic functions can change
its score. On the other hand, Spearman’s p and
Kendall’s T uses ranks instead of raw evaluation
scores, and simple application of monotonic func-
tions cannot change them (use of other operations
such as averaging sentence scores can change them).
</bodyText>
<sectionHeader confidence="0.998751" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.997274">
2.1 Word alignment for rank correlations
</subsectionHeader>
<bodyText confidence="0.9979185">
We have to determine word ranks to obtain rank cor-
relation coefficients. Suppose we have:
</bodyText>
<listItem confidence="0.3983285">
(R1) John hit Bob yesterday
(H1) Bob hit John yesterday
</listItem>
<bodyText confidence="0.999424111111111">
The 1st word “John” in R1 becomes the 3rd word
in H1. The 2nd word “hit” in R1 becomes the 2nd
word in H1. The 3rd word “Bob” in R1 becomes the
1st word in H1. The 4th word “yesterday” in R1 be-
comes the 4th word in H1. Thus, we get H1’s word
order list [3, 2, 1, 4]. The number of all pairs of in-
tegers in this list is 4C2 = 6. It has three increasing
pairs: (3,4), (2,4), and (1,4). Since Kendall’s T is
given by:
</bodyText>
<equation confidence="0.996714">
T = 2 ×
</equation>
<bodyText confidence="0.99195003030303">
the number of increasing pairs
the number of all pairs
H1’s T is 2 × 3/6 − 1 = 0.0.
In this case, we can obtain Spearman’s p as fol-
lows: “John” moved by d1 = 2 words, “hit” moved
by d2 = 0 words, “Bob” moved by d3 = 2 words,
and “yesterday” moved by d4 = 0 words. Therefore,
H1’s pis 1 − (22 + 02 + 22 + 02)/5C3 = 0.2.
Thus, T considers only the direction of the move-
ment, whereas p considers the distance of the move-
ment. Both p and T have the same range [−1, 1]. The
main objective of this paper is to clarify which rank
correlation is closer to human evaluation scores.
We have to consider the limitation of the rank cor-
relation metrics. They are defined only when there
is one-to-one correspondence. However, a refer-
ence sentence and a hypothesis sentence may have
different numbers of words. They may have two or
more occurrences of the same word in one sentence.
Sometimes, a word in the reference does not appear
in the hypothesis, or a word in the hypothesis does
not appear in the reference. Therefore, we cannot
calculate T and p following the above definitions in
general.
Here, we determine the correspondence of words
between hypotheses and references as follows. First,
we find one-to-one corresponding words. That is,
we find words that appear in both sentences and only
once in each sentence. Suppose we have:
(R2) the boy read the book
(H2) the book was read by the boy
By removing non-aligned words by one-to-one cor-
respondence, we get:
</bodyText>
<equation confidence="0.992813">
− 1,
</equation>
<page confidence="0.767673">
946
</page>
<figure confidence="0.365621">
(R3) boy read book
(H3) book read boy
</figure>
<bodyText confidence="0.928642957446808">
Thus, we lost “the.” We relax this one-to-one cor-
respondence constraint by using one-to-one corre-
sponding bigrams. (R2) and (H2) share “the boy”
and “the book,” and we can align these instances of
“the” correctly.
(R4) the1 boye read3 the4 books
(H4) the4 books read3 the1 boye
Now, we have five aligned words, and H4’s word
order is represented by [4, 5, 3, 1, 2].
In returning to H0 and R0, we find that each of
these sentences has eleven words. Almost all words
are aligned by one-to-one correspondence but “he”
is not aligned because it appears twice in each sen-
tence. By considering one-to-one corresponding bi-
grams (“he was” and “he read”), “he” is aligned as
follows.
(R5) he1 wase interested3 in4 worlds
historys because7 hea ready the10
book11
(H5) hea ready the10 book11 because7
he1 wase interested3 in4 worlds
historys
H5’s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].
The number of increasing pairs is: 4C2 = 6 pairs in
[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,
6]. Then we obtain T = 2 x (6 + 15)/11C2 − 1 =
−0.236. On the other hand, Ei d2i = 52 x 6 + 22 +
72 x 4 = 350, and we obtain p = 1 − 350/12C3 =
−0.591.
Therefore, both Spearman’s p and Kendall’s T
give very bad scores to the misleading translation
H0. This fact implies they are much better metrics
than BLEU, which gave a good score to it. p is much
lower than T as we expected.
In general, we can use higher-order n-grams for
this alignment, but here we use only unigrams and
bigrams for simplicity. This algnment algorithm is
given in Figure 1. Since some hypothesis words do
not have corresponding reference words, the output
integer list worder is sometimes shorter than the
evaluated sentence. Therefore, we should not use
worder[i] − i as di directly. We have to renumber
the list by rank as we did in Section 1.
Read a hypothesis sentence h = h1h2 ... hm
and its reference sentence r = r1r2 ... rte,.
Initialize worder with an empty list.
For each word hi in h:
</bodyText>
<listItem confidence="0.97546">
• If hi appears only once each in h and r, append j
s.t. rj = hi to worder.
• Otherwise, if the bigram hihi+1 appears only once
each in h and r, append j s.t. rjrj+1 = hihi+1 to
worder.
• Otherwise, if the bigram hi_1hi appears only once
each in h and r, append j s.t. rj_1rj = hi_1hi to
worder.
</listItem>
<figure confidence="0.379515">
Return worder.
</figure>
<figureCaption confidence="0.998054">
Figure 1: Word alignment algorithm for rank correlation
</figureCaption>
<subsectionHeader confidence="0.7804055">
2.2 Word order metrics and meta-evaluation
metrics
</subsectionHeader>
<bodyText confidence="0.9724965">
These rank correlation metrics sometimes have neg-
ative values. In order to make them just like other
automatic evaluation metrics, we normalize them as
follows.
</bodyText>
<listItem confidence="0.999887">
• Normalized Kendall’s T: NKT = (T + 1)/2.
• Normalized Spearman’s p: NSR = (p + 1)/2.
</listItem>
<bodyText confidence="0.998718714285714">
Accordingly, NKT is 0.382 and NSR is 0.205.
These metrics are defined only when the number
of aligned words is two or more. We define both
NKT and NSR as zero when the number is one or
less. Consequently, these normalized metrics have
the same range [0, 1].
In order to avoid confusion, we use these abbre-
viations (NKT and NSR) when we use rank corre-
lations as word order metrics, because these cor-
relation metrics are also used in the machine trans-
lation community for meta-evaluation. For meta-
evaluation, we use Spearman’s p and Pearson’s cor-
relation coefficient and call them “Spearman” and
“Pearson,” respectively.
</bodyText>
<subsectionHeader confidence="0.99852">
2.3 Overestimation problem
</subsectionHeader>
<bodyText confidence="0.9995485">
Since we measure the rank correlation of only cor-
responding words, these metrics will overestimate
the correlation. For instance, a hypothesis sentence
might have only two corresponding words among
</bodyText>
<page confidence="0.982155">
947
</page>
<table confidence="0.967088965517242">
? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ?? ? ?
? ? ? ? ? ? ?
? ? ? ? ? ? ? ??
? ? ? ? ? ? ? ? ? ?
? ? ?? ? ? ? ? ? ? ? ?? ?
? ? ? ? ? ? ? ? ?
? ? ? ? ?
?? ?? ? ??? ?
? ?
? ? ? ? ?? ? ??
? ? ? ? ? ? ? ? ?? ? ? ? ? ? ?
???? ? ? ?? ? ? ? ? ?
0 0.2 0.4 0.6 0.8 1.0
? ?? ? ? ? ? ? ????? ? ? ? ? ? ? ? ? ? ? ? ?
????? ? ? ? ? ???? ?????
?? ? ????????????? ? ?
? ???? ??????? ????
??????????? ??
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ??? ? ??
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ?
? ?
??? ?? ??????? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ??
? ?????? ??????????
? ? ??? ? ? ? ? ? ?????? ? ? ? ???
? ? ? ? ?? ?
?
</table>
<figure confidence="0.964727529411765">
0 0.2 0.4 0.6 0.8 1.0
? ? ??????
normalized average adequacy
1.0
0.8
0.6
0.4
0.2
0
normalized average adequacy
1.0
0.8
0.6
0.4
0.2
0
BP (brevity penalty) P (precision)
</figure>
<figureCaption confidence="0.9960165">
Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each * corresponds to one sentence generated by one MT system)
</figureCaption>
<bodyText confidence="0.99987455">
dozens of words. In this case, these two words
determine the score of the whole sentence. If the
two words appear in their order in the reference,
the whole sentence obtains the best score, NSR =
NKT = 1.0, in spite of the fact that only two words
matched.
Solving this overestimation problem is the second
objective of this paper. BLEU uses “Brevity Penalty
(BP)” (Section 1) to reduce the scores of too-short
sentences. We can combine the above word order
metrics with BP, e.g., NKT x BP and NSR x BP.
However, we cannot very much expect from this
solution because BP scores do not correlate with
human judgments well. The left graph of Figure
2 shows a scatter plot of BP and “normalized av-
erage adequacy.” This graph has 15 (systems) x
100 (sentences) dots. Each dot (*) corresponds to
one sentence from one translation system.
In the NTCIR-7 data, three human judges gave
five-point scores (1, 2, 3, 4, 5) for “adequacy” and
“fluency” of each translated sentence. Although
each system translated 1,381 sentences, only 100
sentences were evaluated by the judges.
For each translated sentence, we averaged three
judges’ adequacy scores and normalized this aver-
age x by (x −1)/4. This is our “normalized average
adequacy,” and the dots appears only at multiples of
1/3 x 1/4.
This graph shows that BP has very little correla-
tion with adequacy, and we cannot expect BP to im-
prove the meta-evaluation performance very much.
Perhaps, BP’s poor performance was caused by the
fact that most MT systems output almost the same
number of words, and if the number exceeds the
length of the reference, BP=1.0 holds.
Therefore, we have to consider other modifiers
for this overestimation problem. We can use other
common metrics such as precision, recall, and F-
measure to reduce the overestimation of NSR and
NKT.
</bodyText>
<listItem confidence="0.999541333333333">
• Precision: P = c/|h|, where c is the number of
corresponding words and |h |is the number of
words in the hypothesis sentence h.
• Recall: R = c/r, where |r |is the number of
words in the reference sentence r.
• F-measure: F,� = (1 + Q2)PR/(Q2P + R),
</listItem>
<bodyText confidence="0.995176375">
where Q is a parameter.
In (R2)&amp;(H2)’s case, precision is 5/7 = 0.714 and
recall is 5/5 = 1.000.
Which metric should we use? Our preliminary
experiments with NTCIR-7 data showed that preci-
sion correlated best with adequacy among these
three metrics (P, R, and F,a=1). In addition, BLEU
is essentially made for precision. Therefore, preci-
sion seems the most promising modifier.
The right graph of Figure 2 shows a scatter plot
of precision and normalized average adequacy. The
graph shows that precision has more correlation with
adequacy than BP. We can observe that sentences
with very small P values usually obtain very low
adequacy scores but those with mediocre P values
often obtain good adequacy scores.
</bodyText>
<page confidence="0.996433">
948
</page>
<bodyText confidence="0.978208833333333">
If we multiply P directly by NSR or NKT, those
sentences with mediocre P values will lose too
much of their scores. The use of \/x will miti-
gate this problem. Since \/P is closer to 1.0 than
P itself, multiplication of \/P instead of P itself
will save these sentences. If we apply \/x twice
</bodyText>
<equation confidence="0.9383175">
�\/ \/
( P = 4 P), it will further save them. There-
</equation>
<bodyText confidence="0.99147725">
fore, we expect x \/P and x , /P to work better than
xP. Now, we propose two new metrics:
NSRP&apos; and NKTP&apos;,
where α is a parameter (0 &lt; α &lt; 1).
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999974">
3.1 Meta-evaluation with NTCIR-7 data
</subsectionHeader>
<bodyText confidence="0.999293352941176">
In order to compare automatic translation evalua-
tion methods, we use submissions to the NTCIR-7
Patent Translation (PATMT) task (Fujii et al., 2008).
Fourteen MT systems participated in the Japanese-
English intrinsic evaluation. There were two Rule-
Based MT (RMBT) systems and one Example-
based MT (EBMT) system. All other systems were
Statistical MT (SMT) systems. The task organiz-
ers provided a baseline SMT system. These 15 sys-
tems translated 1,381 Japanese sentences into En-
glish. The organizers evaluated these translations by
using BLEU and human judgments. In the human
judgements, three experts independently evaluated
100 selected sentences in terms of ‘adequacy’ and
‘fluency.’
For automatic evaluation, we used a single refer-
ence sentence for each of these 100 manually evalu-
ated sentences. Echizen-ya et al. (2009) used multi-
reference data, but their data is not publicly available
yet.
For this meta-evaluation, we measured the
corpus-level correlation between the human evalua-
tion scores and the automatic evaluation scores. We
simply averaged scores of 100 sentences for the pro-
posed metrics. For existing metrics such as BLEU,
we followed their definitions for corpus-level eval-
uation instead of simple averages of sentence-level
scores. We used default settings for conventional
metrics, but we tuned GTM (Melamed et al., 2007)
with -e option. This option controls preferences
on longer word runs. We also used the para-
phrase database TERp (http://www.umiacs.umd.
edu/˜snover/terp) for METEOR (Banerjee and
Lavie, 2005).
</bodyText>
<subsectionHeader confidence="0.999929">
3.2 Meta-evaluation with WMT-07 data
</subsectionHeader>
<bodyText confidence="0.999995">
We developed our metric mainly for automatic eval-
uation of translation quality for distant language
pairs such as Japanese-English, but we also want
to know how well the metric works for similar lan-
guage pairs. Therefore, we also use the WMT-
07 data (Callison-Burch et al., 2007) that covers
only European language pairs. Callison-Burch et al.
(2007) tried different human evaluation methods and
showed detailed evaluation scores. The Europarl test
set has 2,000 sentences, and The News Commentary
test set has 2,007 sentences.
This data has different language pairs: Spanish,
French, German ==&gt;. English. We exclude Czech-
English because there were so few systems (See the
footnote of p. 146 in their paper).
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999981">
4.1 Meta-evaluation with NTCIR-7 data
</subsectionHeader>
<bodyText confidence="0.99956984">
Table 1 shows the main results of this paper. The
left part has corpus-level meta-evaluation with ade-
quacy. Error metrics, WER, PER, and TER, have
negative correlation coefficients, but we did not
show their minus signs here.
Both NSR-based metrics and NKT-based metrics
perform better than conventional metrics for this NT-
CIR PATMT JE translation data. As we expected,
xBP and xP(1/1) performed badly. Spearman of
BP itself is zero.
NKT performed slightly better than NSR. Per-
haps, NSR penalized alternative good translations
too much. However, one of the NSR-based metrics,
NSRP1/4, gave the best Spearman score of 0.947,
and the difference between NSRP&apos; and NKTP&apos;
was small. Modification with P led to this improve-
ment.
NKT gave the best Pearson score of 0.922. How-
ever, Pearson measures linearity and we can change
its score through a nonlinear monotonic function
without changing Spearman very much. For in-
stance, (NSRP1/4)1.5 also has Spearman of 0.947
but its Pearson is 0.931, which is better than NKT’s
0.922. Thus, we think Spearman is a better meta-
evaluation metric than Pearson.
</bodyText>
<page confidence="0.999037">
949
</page>
<tableCaption confidence="0.980152">
Table 1: NTCIR-7 Meta-evaluation: correlation with hu-
man judgments (Spm = Spearman, Prs = Pearson)
</tableCaption>
<table confidence="0.999891857142857">
human judge Adequacy Fluency
eval\ meta-eval Spm Prs Spm Prs
P 0.615 0.704 0.672 0.876
R 0.436 0.669 0.461 0.854
Fp=1 0.525 0.692 0.543 0.871
BP 0.000 0.515 -0.007 0.742
NSR 0.904 0.906 0.869 0.910
NSRP1/8 0.937 0.905 0.890 0.934
NSRP1/4 0.947 0.900 0.901 0.944
NSRP1/2 0.937 0.890 0.926 0.949
NSRP1/1 0.883 0.872 0.883 0.939
NSR x BP 0.851 0.874 0.769 0.910
NKT 0.940 0.922 0.887 0.931
NKTP1/8 0.940 0.913 0.908 0.944
NKTP1/4 0.940 0.904 0.908 0.949
NKTP1/2 0.929 0.890 0.897 0.949
NKTP1/1 0.897 0.869 0.879 0.936
NKT x BP 0.829 0.878 0.726 0.918
ROUGE-L 0.903 0.874 0.889 0.932
ROUGE-S(4) 0.593 0.757 0.640 0.869
IMPACT 0.797 0.813 0.751 0.932
WER 0.894 0.822 0.836 0.926
TER 0.854 0.806 0.372 0.856
PER 0.375 0.642 0.393 0.842
METEOR(TERp) 0.490 0.708 0.508 0.878
GTM(-e 12) 0.618 0.723 0.601 0.850
NIST 0.343 0.661 0.372 0.856
BLEU 0.515 0.653 0.500 0.795
</table>
<bodyText confidence="0.9959898125">
The right part of Table 1 shows correlation with
fluency, but adequacy is more important, because
our motivation is to provide a metric that is useful to
reduce incomprehensible or misunderstanding out-
puts of MT systems. Again, the correlation-based
metrics gave better scores than conventional metrics,
and BP performed badly. NSR-based metrics proved
to be as good as NKT-based metrics.
Meta-evaluation scores of the de facto standard
BLEU is much lower than those of other metrics.
Echizen-ya et al. (2009) reported that IMPACT per-
formed very well for sentence-level evaluation of
NTCIR-7 PATMT JE data. This corpus-level result
also shows that IMPACT works better than BLEU,
but ROUGE-L, WER, and our methods give better
scores than IMPACT.
</bodyText>
<tableCaption confidence="0.903022666666667">
Table 2: WMT-07 meta-evaluation: Each source lan-
guage has two columns: the left one is News Corpus and
the right one is Europarl.
</tableCaption>
<table confidence="0.999837071428571">
Spearman’s p with human “rank”
source French Spanish German
NSR 0.775 0.837 0.523 0.766 0.700 0.593
NSRP1/8 0.821 0.857 0.786 0.595 0.400 0.685
NSRP1/4 0.821 0.857 0.786 0.455 0.400 0.714
NSRP1/2 0.821 0.857 0.786 0.347 0.400 0.714
NKT 0.845 0.857 0.607 0.838 0.700 0.630
NKTP1/8 0.793 0.857 0.786 0.595 0.400 0.714
NKTP1/4 0.793 0.857 0.786 0.524 0.400 0.714
NKTP1/2 0.793 0.857 0.786 0.347 0.400 0.714
BLEU 0.786 0.679 0.750 0.595 0.400 0.821
WER 0.607 0.857 0.750 0.429 0.000 0.500
ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857
ROUGES 0.883 0.679 0.786 0.690 0.400 0.929
</table>
<subsectionHeader confidence="0.977684">
4.2 Meta-evaluation with WMT-07 data
</subsectionHeader>
<bodyText confidence="0.999955888888889">
Callison-Burch et al. (2007) have performed differ-
ent human evaluation methods for different language
pairs and different corpora. Their Table 5 shows
inter-annotator agreements for the human evaluation
methods. According to their table, the “sentence
ranking” (or “rank”) method obtained better agree-
ment than “adequacy.” Therefore, we show Spear-
man’s p for “rank.” We used the scores given in
their Tables 9, 10, and 11. (The “constituent” meth-
ods obtained the best inter-annotator agreement, but
these methods focus on local translation quality and
have nothing to do with global word order, which we
are discussing here.)
Table 2 shows that our metrics designed for
distant language pairs are comparable to conven-
tional methods even for similar language pairs, but
ROUGE-L and ROUGE-S performed better than
ours for French News Corpus and German Europarl.
BLEU scores in this table agree with those in Table
17 of Callison-Burch et al. (2007) within rounding
errors.
After some experiments, we noticed that the use
of R instead of P often gives better scores for WMT-
07, but it degrades NTCIR-7 scores. We can extend
our metric by F,3, weighted harmonic mean of P and
R, or any other interpolation, but the introduction
of new parameters into our metric makes it difficult
</bodyText>
<page confidence="0.99045">
950
</page>
<bodyText confidence="0.57903825">
to control. Improvement without new parameters is
beyond the scope of this paper.
Table 3: NTCIR-7 meta-evaluation: Effects of square
root (b(x) = 1 − √1 − x)
</bodyText>
<sectionHeader confidence="0.997653" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.998477536585366">
It has come to our attention that Birch et al. (2010)
has independently proposed an automatic evaluation
method based on Kendall’s T. First, they started
with Kendall’s T distance, which can be written as
“1 − NKT” in our terminology, and then subtracted
it from one. Thus, their metric is nothing but NKT.
Then, they proposed application of the square root
to get better Pearson by improving “the sensitivity
to small reorderings.” Since they used “Kendall’s T”
and “Kendall’s T distance” interchangeably, it is not
clear what they mean by “√ Kendall’s T,” but per-
haps they mean 1 − √1 − NKT because √NKT is
more insensitive to small reorderings. Table 3 shows
the performance of these metrics for NTCIR-7 data.
Pearson’s correlation coefficient with adequacy was
improved by 1 − √1 − NKT, but other scores were
degraded in this experiment.
The difference between our method and Birch et
al. (2010)’s method comes from the fact that we
used Japanese-English translation data and Spear-
man’s correlation for meta-evaluation, whereas they
used Chinese-English translation data and only Pear-
son’s correlation for meta-evaluation. Chinese word
order is different from English, but Chinese is a
Subject-Verb-Object (SVO) language and thus is
much closer to English word order than Japanese,
a typical SOV language.
We preferred NSR because it penalizes global
word order mistakes much more than does NKT, and
as discussed above, global word order mistakes of-
ten lead to incomprehensibility and misunderstand-
ing.
On the other hand, they also tried Hamming dis-
tance, and summarized their experiments as follows:
However, the Hamming distance seems to
be more informative than Kendall’s tau for
small amounts of reordering.
This sentence and the introduction of the square root
to NKT imply that Chinese word order is close to
that of English, and they have to measure subtle
word order mistakes.
</bodyText>
<table confidence="0.9963714">
NKT √ NKT b(NKT)
Spearman w/ adequacy 0.940 0.940 0.922
Pearson w/ adequacy 0.922 0.817 0.941
Spearman w/ fluency 0.887 0.865 0.858
Pearson w/ fluency 0.931 0.917 0.833
</table>
<bodyText confidence="0.9997825">
In spite of these differences, the two groups inde-
pendently recognized the usefulness of rank correla-
tions for automatic evaluation of translation quality
for distant language pairs.
In their WMT-2010 paper (Birch and Osborne,
2010), they multiplied NKT with the brevity penalty
and interpolated it with BLEU for the WMT-2010
shared task. This fact implies that incomprehensible
or misleading word order mistakes are rare in trans-
lation among European languages.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99923276">
When Statistical Machine Translation is applied to
distant language pairs such as Japanese and English,
word order becomes an important problem. SMT
systems often fail to find an appropriate translation
because of a large search space. Therefore, they
often output misleading or incomprehensible sen-
tences such as “A because B” vs. “B because A.” To
penalize such inadequate translations, we presented
an automatic evaluation method based on rank corre-
lation. There were two questions for this approach.
First, which correlation coefficient should we use:
Spearman’s p or Kendall’s T? Second, how should
we solve the overestimation problem caused by the
nature of one-to-one correspondence?
We answered these questions through our exper-
iments using the NTCIR-7 PATMT JE translation
data. For the first question, T was slightly better
than p, but p was improved by precision. For the
second question, it turned out that BLEU’s Brevity
Penalty was counter-productive. A precision-based
penalty gave a better solution. With this precision-
based penalty, both p and T worked well and they
outperformed conventional methods for NTCIR-7
data. For similar language pairs, our method was
comparable to conventional evaluation methods. Fu-
</bodyText>
<page confidence="0.993682">
951
</page>
<bodyText confidence="0.999565666666667">
ture work includes extension of the method so that it
can outperform conventional methods even for sim-
ilar language pairs.
</bodyText>
<sectionHeader confidence="0.994212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999794210526316">
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgements. In Proc. of ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and Summarization, pages 65–72.
Alexandra Birch and Miles Osborne. 2010. LRscore for
evaluating lexical and reordering quality in MT. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 327–
332.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15–26.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluatiing the role of Bleu in ma-
chine translation research. In Proc. of the Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249–256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Chrstof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of machine translation. In Proc. of
the Workshop on Machine Translation (WMT), pages
136–158.
Etienne Denoual and Yves Lepage. 2005. BLEU in char-
acters: towards automatic MT evaluation in languages
without word delimiters. In Companion Volume to the
Proceedings of the Second International Joint Confer-
ence on Natural Language Processing, pages 81–86.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In Proceedings of MT Summit XII Workshop on Patent
Translation, pages 151–158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, and Noriko Kando. 2009. Meta-
evaluation of automatic evaluation methods for ma-
chine translation using patent translation data in ntcir-
7. In Proceedings of the 3rd Workshop on Patent
Translation, pages 9–16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389–400.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 250–257.
Maurice G. Kendall. 1975. Rank Correlation Methods.
Charles Griffin.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 71–78.
Dan Melamed, Ryan Green, and Joseph P. Turian. 2007.
Precision and recall of machine translation. In Proc.
of NAACL-HLT, pages 61–63.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish Results. In
Proc. of the International Conference on Human Lan-
guage Technology Research (HLT), pages 132–136.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 311–318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation forMachine Translation
in the Americas.
</reference>
<page confidence="0.997564">
952
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.772889">
<title confidence="0.99993">Automatic Evaluation of Translation Quality for Distant Language Pairs</title>
<author confidence="0.827316">Hideki Isozaki</author>
<author confidence="0.827316">Tsutomu Hirao</author>
<author confidence="0.827316">Kevin Duh</author>
<author confidence="0.827316">Katsuhito Sudoh</author>
<author confidence="0.827316">Hajime</author>
<affiliation confidence="0.897087">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.932826">2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237,</address>
<abstract confidence="0.999482068965517">Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ‘A because B’ as ‘B because A.’ Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgements.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="20631" citStr="Banerjee and Lavie, 2005" startWordPosition="3644" endWordPosition="3647">a-evaluation, we measured the corpus-level correlation between the human evaluation scores and the automatic evaluation scores. We simply averaged scores of 100 sentences for the proposed metrics. For existing metrics such as BLEU, we followed their definitions for corpus-level evaluation instead of simple averages of sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language p</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgements. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>LRscore for evaluating lexical and reordering quality in MT.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>327--332</pages>
<contexts>
<context position="28705" citStr="Birch and Osborne, 2010" startWordPosition="4953" endWordPosition="4956">an Kendall’s tau for small amounts of reordering. This sentence and the introduction of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. NKT √ NKT b(NKT) Spearman w/ adequacy 0.940 0.940 0.922 Pearson w/ adequacy 0.922 0.817 0.941 Spearman w/ fluency 0.887 0.865 0.858 Pearson w/ fluency 0.931 0.917 0.833 In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs. In their WMT-2010 paper (Birch and Osborne, 2010), they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages. 6 Conclusions When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as “A because B” vs. “B because A.” To penal</context>
</contexts>
<marker>Birch, Osborne, 2010</marker>
<rawString>Alexandra Birch and Miles Osborne. 2010. LRscore for evaluating lexical and reordering quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT evaluation: evaluating reordering.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="26468" citStr="Birch et al. (2010)" startWordPosition="4594" endWordPosition="4597">agree with those in Table 17 of Callison-Burch et al. (2007) within rounding errors. After some experiments, we noticed that the use of R instead of P often gives better scores for WMT07, but it degrades NTCIR-7 scores. We can extend our metric by F,3, weighted harmonic mean of P and R, or any other interpolation, but the introduction of new parameters into our metric makes it difficult 950 to control. Improvement without new parameters is beyond the scope of this paper. Table 3: NTCIR-7 meta-evaluation: Effects of square root (b(x) = 1 − √1 − x) 5 Discussion It has come to our attention that Birch et al. (2010) has independently proposed an automatic evaluation method based on Kendall’s T. First, they started with Kendall’s T distance, which can be written as “1 − NKT” in our terminology, and then subtracted it from one. Thus, their metric is nothing but NKT. Then, they proposed application of the square root to get better Pearson by improving “the sensitivity to small reorderings.” Since they used “Kendall’s T” and “Kendall’s T distance” interchangeably, it is not clear what they mean by “√ Kendall’s T,” but perhaps they mean 1 − √1 − NKT because √NKT is more insensitive to small reorderings. Table</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, 24(1):15–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluatiing the role of Bleu in machine translation research.</title>
<date>2006</date>
<booktitle>In Proc. of the Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="2071" citStr="Callison-Burch et al. (2006)" startWordPosition="293" endWordPosition="296">a-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation p with human evaluation scores are used to measure how closely an auto</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluatiing the role of Bleu in machine translation research. In Proc. of the Conference of the European Chapter of the Association for Computational Linguistics, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Chrstof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-)Evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Machine Translation (WMT),</booktitle>
<pages>136--158</pages>
<contexts>
<context position="20948" citStr="Callison-Burch et al., 2007" startWordPosition="3696" endWordPosition="3699">es of sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language pairs: Spanish, French, German ==&gt;. English. We exclude CzechEnglish because there were so few systems (See the footnote of p. 146 in their paper). 4 Results 4.1 Meta-evaluation with NTCIR-7 data Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics,</context>
<context position="24991" citStr="Callison-Burch et al. (2007)" startWordPosition="4349" endWordPosition="4352">’s p with human “rank” source French Spanish German NSR 0.775 0.837 0.523 0.766 0.700 0.593 NSRP1/8 0.821 0.857 0.786 0.595 0.400 0.685 NSRP1/4 0.821 0.857 0.786 0.455 0.400 0.714 NSRP1/2 0.821 0.857 0.786 0.347 0.400 0.714 NKT 0.845 0.857 0.607 0.838 0.700 0.630 NKTP1/8 0.793 0.857 0.786 0.595 0.400 0.714 NKTP1/4 0.793 0.857 0.786 0.524 0.400 0.714 NKTP1/2 0.793 0.857 0.786 0.347 0.400 0.714 BLEU 0.786 0.679 0.750 0.595 0.400 0.821 WER 0.607 0.857 0.750 0.429 0.000 0.500 ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857 ROUGES 0.883 0.679 0.786 0.690 0.400 0.929 4.2 Meta-evaluation with WMT-07 data Callison-Burch et al. (2007) have performed different human evaluation methods for different language pairs and different corpora. Their Table 5 shows inter-annotator agreements for the human evaluation methods. According to their table, the “sentence ranking” (or “rank”) method obtained better agreement than “adequacy.” Therefore, we show Spearman’s p for “rank.” We used the scores given in their Tables 9, 10, and 11. (The “constituent” methods obtained the best inter-annotator agreement, but these methods focus on local translation quality and have nothing to do with global word order, which we are discussing here.) Ta</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Chrstof Monz, and Josh Schroeder. 2007. (Meta-)Evaluation of machine translation. In Proc. of the Workshop on Machine Translation (WMT), pages 136–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Etienne Denoual</author>
<author>Yves Lepage</author>
</authors>
<title>BLEU in characters: towards automatic MT evaluation in languages without word delimiters.</title>
<date>2005</date>
<booktitle>In Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing,</booktitle>
<pages>81--86</pages>
<contexts>
<context position="2913" citStr="Denoual and Lepage (2005)" startWordPosition="426" endWordPosition="429">l by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation p with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al. (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means 944 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Associat</context>
</contexts>
<marker>Denoual, Lepage, 2005</marker>
<rawString>Etienne Denoual and Yves Lepage. 2005. BLEU in characters: towards automatic MT evaluation in languages without word delimiters. In Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing, pages 81–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Echizen-ya</author>
<author>Kenji Araki</author>
</authors>
<title>Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XII Workshop on Patent Translation,</booktitle>
<pages>151--158</pages>
<contexts>
<context position="2501" citStr="Echizen-ya and Araki, 2007" startWordPosition="367" endWordPosition="370">eni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation p with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers</context>
</contexts>
<marker>Echizen-ya, Araki, 2007</marker>
<rawString>Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum. In Proceedings of MT Summit XII Workshop on Patent Translation, pages 151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Echizen-ya</author>
</authors>
<title>Terumasa Ehara, Sayori Shimohata, Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, and Noriko Kando.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd Workshop on Patent Translation,</booktitle>
<pages>9--16</pages>
<marker>Echizen-ya, 2009</marker>
<rawString>Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata, Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, and Noriko Kando. 2009. Metaevaluation of automatic evaluation methods for machine translation using patent translation data in ntcir7. In Proceedings of the 3rd Workshop on Patent Translation, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Overview of the patent translation task at the NTCIR-7 workshop.</title>
<date>2008</date>
<booktitle>In Working Notes of the NTCIR Workshop Meeting (NTCIR),</booktitle>
<pages>389--400</pages>
<contexts>
<context position="2388" citStr="Fujii et al., 2008" startWordPosition="348" endWordPosition="351">ave a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation p with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japan</context>
<context position="19246" citStr="Fujii et al., 2008" startWordPosition="3434" endWordPosition="3437">cre P values will lose too much of their scores. The use of \/x will mitigate this problem. Since \/P is closer to 1.0 than P itself, multiplication of \/P instead of P itself will save these sentences. If we apply \/x twice �\/ \/ ( P = 4 P), it will further save them. Therefore, we expect x \/P and x , /P to work better than xP. Now, we propose two new metrics: NSRP&apos; and NKTP&apos;, where α is a parameter (0 &lt; α &lt; 1). 3 Experiments 3.1 Meta-evaluation with NTCIR-7 data In order to compare automatic translation evaluation methods, we use submissions to the NTCIR-7 Patent Translation (PATMT) task (Fujii et al., 2008). Fourteen MT systems participated in the JapaneseEnglish intrinsic evaluation. There were two RuleBased MT (RMBT) systems and one Examplebased MT (EBMT) system. All other systems were Statistical MT (SMT) systems. The task organizers provided a baseline SMT system. These 15 systems translated 1,381 Japanese sentences into English. The organizers evaluated these translations by using BLEU and human judgments. In the human judgements, three experts independently evaluated 100 selected sentences in terms of ‘adequacy’ and ‘fluency.’ For automatic evaluation, we used a single reference sentence f</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Working Notes of the NTCIR Workshop Meeting (NTCIR), pages 389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head Finalization: A simple reordering rule for SOV languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>250--257</pages>
<contexts>
<context position="7410" citStr="Isozaki et al. (2010)" startWordPosition="1157" endWordPosition="1160">) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman’s p and Kendall’s r (Kendall, 1975). In Isozaki et al. (2010), we used Kendall’s r to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. 945 It is not clear how well T works as an automatic evaluation metric of translation quality. Moreover, Spearman’s p might work better than Kendall’s T. As we discuss later, T considers only the direction of the rank change, whereas p considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant language pairs. The second ob</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A simple reordering rule for SOV languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 250–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>Rank Correlation Methods. Charles Griffin.</title>
<date>1975</date>
<contexts>
<context position="7384" citStr="Kendall, 1975" startWordPosition="1154" endWordPosition="1155">ent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman’s p and Kendall’s r (Kendall, 1975). In Isozaki et al. (2010), we used Kendall’s r to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. 945 It is not clear how well T works as an automatic evaluation metric of translation quality. Moreover, Spearman’s p might work better than Kendall’s T. As we discuss later, T considers only the direction of the rank change, whereas p considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant lan</context>
</contexts>
<marker>Kendall, 1975</marker>
<rawString>Maurice G. Kendall. 1975. Rank Correlation Methods. Charles Griffin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proc. of the North American Chapter of the Association of Computational Linguistics (NAACL),</booktitle>
<pages>71--78</pages>
<contexts>
<context position="2437" citStr="Lin and Hovy, 2003" startWordPosition="357" endWordPosition="360"> tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation p with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundar</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. of the North American Chapter of the Association of Computational Linguistics (NAACL), pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>61--63</pages>
<contexts>
<context position="20439" citStr="Melamed et al., 2007" startWordPosition="3617" endWordPosition="3620"> single reference sentence for each of these 100 manually evaluated sentences. Echizen-ya et al. (2009) used multireference data, but their data is not publicly available yet. For this meta-evaluation, we measured the corpus-level correlation between the human evaluation scores and the automatic evaluation scores. We simply averaged scores of 100 sentences for the proposed metrics. For existing metrics such as BLEU, we followed their definitions for corpus-level evaluation instead of simple averages of sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different hum</context>
</contexts>
<marker>Melamed, Green, Turian, 2007</marker>
<rawString>Dan Melamed, Ryan Green, and Joseph P. Turian. 2007. Precision and recall of machine translation. In Proc. of NAACL-HLT, pages 61–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>John Henderson</author>
<author>Florence Reeder</author>
</authors>
<title>Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish Results.</title>
<date>2002</date>
<booktitle>In Proc. of the International Conference on Human Language Technology Research (HLT),</booktitle>
<pages>132--136</pages>
<contexts>
<context position="1890" citStr="Papineni et al., 2002" startWordPosition="264" endWordPosition="267">ese metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and A</context>
</contexts>
<marker>Papineni, Roukos, Ward, Henderson, Reeder, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002a. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish Results. In Proc. of the International Conference on Human Language Technology Research (HLT), pages 132–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1890" citStr="Papineni et al., 2002" startWordPosition="264" endWordPosition="267">ese metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and A</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002b. BLEU: a method for automatic evaluation of machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas.</booktitle>
<contexts>
<context position="6639" citStr="Snover et al., 2006" startWordPosition="1039" endWordPosition="1042"> by Brevity Penalty (BP) min(1, exp(1− r/h)), where r is the length of the reference and h is the length of the hypothesis. BLEU = BP x (p1p2p3p4)1/4. Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0x(11/11x9/10x6/9x4/8)1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order. Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al., 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation forMachine Translation in the Americas.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>