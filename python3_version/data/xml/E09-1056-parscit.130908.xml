<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.9440335">
Improvements in Analogical Learning:
Application to Translating multi-Terms of the Medical Domain
</title>
<author confidence="0.906457">
Philippe Langlais Franc¸ois Yvon and Pierre Zweigenbaum
</author>
<affiliation confidence="0.902034">
DIRO LIMSI-CNRS
Univ. of Montreal, Canada Univ. Paris-Sud XI, France
</affiliation>
<email confidence="0.994517">
felipe@iro.umontreal.ca {yvon,pz}@limsi.fr
</email>
<sectionHeader confidence="0.994669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998704375">
Handling terminology is an important
matter in a translation workflow. However,
current Machine Translation (MT) sys-
tems do not yet propose anything proactive
upon tools which assist in managing termi-
nological databases. In this work, we in-
vestigate several enhancements to analog-
ical learning and test our implementation
on translating medical terms. We show
that the analogical engine works equally
well when translating from and into a mor-
phologically rich language, or when deal-
ing with language pairs written in differ-
ent scripts. Combining it with a phrase-
based statistical engine leads to significant
improvements.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971089285715">
If machine translation is to meet commercial
needs, it must offer a sensible approach to trans-
lating terms. Currently, MT systems offer at best
database management tools which allow a human
(typically a translator, a terminologist or even the
vendor of the system) to specify bilingual ter-
minological entries. More advanced tools are
meant to identify inconsistencies in terminological
translations and might prove useful in controlled-
language situations (Itagaki et al., 2007).
One approach to translate terms consists in us-
ing a domain-specific parallel corpus with stan-
dard alignment techniques (Brown et al., 1993) to
mine new translations. Massive amounts of par-
allel data are certainly available in several pairs
of languages for domains such as parliament de-
bates or the like. However, having at our disposal
a domain-specific (e.g. computer science) bitext
with an adequate coverage is another issue. One
might argue that domain-specific comparable (or
perhaps unrelated) corpora are easier to acquire,
in which case context-vector techniques (Rapp,
1995; Fung and McKeown, 1997) can be used
to identify the translation of terms. We certainly
agree with that point of view to a certain extent,
but as discussed by Morin et al. (2007), for many
specific domains and pairs of languages, such re-
sources simply do not exist. Furthermore, the task
of translation identification is more difficult and
error-prone.
Analogical learning has recently regained some
interest in the NLP community. Lepage and De-
noual (2005) proposed a machine translation sys-
tem entirely based on the concept of formal anal-
ogy, that is, analogy on forms. Stroppa and
Yvon (2005) applied analogical learning to sev-
eral morphological tasks also involving analogies
on words. Langlais and Patry (2007) applied it to
the task of translating unknown words in several
European languages, an idea investigated as well
by Denoual (2007) for a Japanese to English trans-
lation task.
In this study, we improve the state-of-the-art of
analogical learning by (i) proposing a simple yet
effective implementation of an analogical solver;
(ii) proposing an efficient solution to the search is-
sue embedded in analogical learning, (iii) investi-
gating whether a classifier can be trained to recog-
nize bad candidates produced by analogical learn-
ing. We evaluate our analogical engine on the task
of translating terms of the medical domain; a do-
main well-known for its tendency to create new
words, many of which being complex lexical con-
structions. Our experiments involve five language
pairs, including languages with very different mor-
phological systems.
</bodyText>
<note confidence="0.922973">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487–495,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99775">
487
</page>
<bodyText confidence="0.999897285714286">
In the remainder of this paper, we first present
in Section 2 the principle of analogical learn-
ing. Practical issues in analogical learning are
discussed in Section 3 along with our solutions.
In Section 4, we report on experiments we con-
ducted with our analogical device. We conclude
this study and discuss future work in Section 5.
</bodyText>
<sectionHeader confidence="0.949907" genericHeader="method">
2 Analogical Learning
</sectionHeader>
<subsectionHeader confidence="0.750058">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.996038066666667">
A proportional analogy, or analogy for short, is a
relation between four items noted [x : y = z : t]
which reads as “x is to y as z is to t”. Among pro-
portional analogies, we distinguish formal analo-
gies, that is, those we can identify at a graphemic
level, such as [adrenergic beta-agonists, adren-
ergic beta-antagonists, adrenergic alpha-agonists,
adrenergic alpha-antagonists].
Formal analogies can be defined in terms of
factorizations1. Let x be a string over an alpha-
bet E, a factorization of x, noted fx, is a se-
quence of n factors fx = (fX, ... , fnx), such that
x = fX O fX O ... O fnx, where O denotes the
concatenation operator. After (Stroppa and Yvon,
2005) we thus define a formal analogy as:
</bodyText>
<construct confidence="0.822315666666667">
Definition 1 ∀(x, y, z, t) ∈ E?4, [x : y = z : t] iff
there exist factorizations (fx, fy, fz, ft) ∈ (E?d),
of (x, y, z, t) such that, ∀i ∈ [1, d], (fiy, fiz) ∈
{ 1
(fix, fit ), (fit , fix) . The smallest d for which this
definition holds is called the degree of the analogy.
</construct>
<bodyText confidence="0.997714153846154">
Intuitively, this definition states that (x, y, z, t)
are made up of a common set of alternating sub-
strings. It is routine to check that it captures the
exemplar analogy introduced above, based on the
following set of factorizations:
fx ≡ (adrenergic bet, a-agonists)
fy ≡ (adrenergic bet, a-antagonists)
fz ≡ (adrenergic alph, a-agonists)
ft ≡ (adrenergic alph, a-antagonists)
As no smaller factorization can be found, the de-
gree of this analogy is 2. In the sequel, we call
an analogical equation an analogy where one item
(usually the fourth) is missing and we note it [x :
</bodyText>
<equation confidence="0.979083">
y = z : ?].
</equation>
<bodyText confidence="0.7845388">
1Factorizations of strings correspond to segmentations.
We keep the former term, to emphasize the genericity of the
definition, which remains valid for other algebraic structures,
for which factorization and segmentation are no longer syno-
mymous.
</bodyText>
<subsectionHeader confidence="0.999383">
2.2 Analogical Inference
</subsectionHeader>
<bodyText confidence="0.999155142857143">
Let L = {(i, o)  |i ∈ I, o ∈ O} be a learning set
of observations, where I (O) is the set of possible
forms of the input (output) linguistic system under
study. We denote I(u) (O(u)) the projection of u
into the input (output) space; that is, if u = (i, o),
then I(u) ≡ i and O(u) ≡ o. For an incomplete
observation u = (i, ?), the inference procedure is:
</bodyText>
<listItem confidence="0.999223888888889">
1. building Ez(u) = {(x, y, z) ∈ L3  |[I(x) :
I(y) = I(z) : I(u)]}, the set of input triplets
that define an analogy with I(u) .
2. building EO(u) = {o ∈ O  |∃(x, y, z) ∈
Ez(u) s.t. [O(x) : O(y) = O(z) : o]} the set
of solutions to the equations obtained by pro-
jecting the triplets of Ez(u) into the output
space.
3. selecting candidates among EO(u).
</listItem>
<bodyText confidence="0.999143857142857">
In the sequel, we distinguish the generator
which implements the first two steps, from the se-
lector which implements step 3.
To give an example, assume L contains
the following entries: (beeta-agonistit, adren-
ergic beta-agonists), (beetasalpaajat, adrenergic
beta-antagonists) and (alfa-agonistit, adrener-
gic alpha-agonists). We might translate the
Finnish term alfasalpaajat into the English term
adrenergic alpha-antagonists by 1) identifying
the input triplet: (beeta-agonistit, beetasalpaa-
jat, alfa-agonistit); 2) projecting it into the equa-
tion [adrenergic beta-agonists : adrenergic beta-
antagonists = adrenergic alpha-agonists : ?]; and
solving it: adrenergic alpha-antagonists is one of
its solutions.
During inference, analogies are recognized in-
dependently in the input and the output space, and
nothing pre-establishes which subpart of one in-
put form corresponds to which subpart of the out-
put one. This “knowledge” is passively captured
thanks to the inductive bias of the learning strat-
egy (an analogy in the input space corresponds to
one in the output space). Also worth mentioning,
this procedure does not rely on any pre-defined no-
tion of word. This might come at an advantage for
languages that are hard to segment (Lepage and
Lardilleux, 2007).
</bodyText>
<sectionHeader confidence="0.991961" genericHeader="method">
3 Practical issues
</sectionHeader>
<bodyText confidence="0.994132">
Each step of analogical learning, that is, search-
ing for input triplets, solving output equations and
</bodyText>
<page confidence="0.998034">
488
</page>
<bodyText confidence="0.99991975">
selecting good candidates involves some practical
issues. Since searching for input triplets might in-
volve the need for solving (input) equations, we
discuss the solver first.
</bodyText>
<subsectionHeader confidence="0.999839">
3.1 The solver
</subsectionHeader>
<bodyText confidence="0.999032230769231">
Lepage (1998) proposed an algorithm for solving
an analogical equation [x : y = z : ?]. An
alignment between x and y and between x and z
is first computed (by edit-distance) as illustrated
in Figure 1. Then, the three strings are synchro-
nized using x as a backbone of the synchroniza-
tion. The algorithm can be seen as a deterministic
finite-state machine where a state is defined by the
two edit-operations being visited in the two tables.
This is schematized by the two cursors in the fig-
ure. Two actions are allowed: copy one symbol
from y or z into the solution and move one or both
cursors.
</bodyText>
<figure confidence="0.8172045">
x:
read e r read e r
readable doer
4 4
</figure>
<figureCaption confidence="0.9544725">
Figure 1: Illustration of the synchronization done
by the solver described in (Lepage, 1998).
</figureCaption>
<bodyText confidence="0.994421071428572">
There are two things to realize with this algo-
rithm. First, since several (minimal-cost) align-
ments can be found between two strings, several
synchronizations are typically carried out while
solving an equation, leading to (possibly many)
different solutions. Indeed, in adverse situations,
an exponential number of synchronizations will
have to be computed. Second, the algorithm fails
to deliver an expected form in a rather frequent
situation where two identical symbols align fortu-
itously in two strings. This is for instance the case
in our running example where the symbol d in
doer aligns to the one in reader, which puzzles the
synchronization. Indeed, dabloe is the only form
proposed to [reader : readable = doer : ?], while
the expected one is doable. The algorithm would
have no problem, however, to produce the form
writable out of the equation [reader : readable =
writer : ?].
Yvon et al. (2004) proposed an analogical
solver which is not exposed to the latter prob-
lem. It consists in building a finite state transducer
which generates the solutions to [x : y = z : ?]
while recognizing the form x.
Theorem 1 t is a solution to [x : y = z : ?] iff
t belongs to {y o z}\x.
shuffle and complement are two rational op-
erations. The shuffle of two strings w and
v, noted w o v, is the regular language con-
taining the strings obtained by selecting (with-
out replacement) alternatively in w and v, se-
quences of characters in a left-to-right man-
ner. For instance, spondyondontilalgiatis and
ondspondonylaltitisgia are two strings belong-
ing to spondylalgia o ondontitis). The comple-
mentary set of w with respect to v, noted w\v, is
the set of strings formed by removing from w, in
a left-to-right manner, the symbols in v. For in-
stance, spondylitis and spydoniltis are belong-
ing to spondyondontilalgiatis \ ondontalgia.
Our implementation of the two rational operations
are sketched in Algorithm 1.
Because the shuffle of two strings may con-
tain an exponential number of elements with re-
spect to the length of those strings, building such
an automaton may face combinatorial problems.
Our solution simply consists in randomly sam-
pling strings in the shuffle set. Our solver, depicted
in Algorithm 2, is thus controlled by a sampling
size s, the impact of which is illustrated in Ta-
ble 1. By increasing s, the solver generates more
(mostly spurious) solutions, but also increases the
relative frequency with which the expected output
is generated. In practice, provided a large enough
sampling size,2 the expected form very often ap-
pears among the most frequent ones.
</bodyText>
<equation confidence="0.9938525">
s nb (solution,frequency)
10 11 (doable,7) (dabloe,3) (adbloe,3)
102 22 (doable,28) (dabloe,21) (abldoe,21)
103 29 (doable,333) (dabloe,196) (abldoe,164)
</equation>
<tableCaption confidence="0.678542">
Table 1: The 3-most frequent solutions generated
</tableCaption>
<bodyText confidence="0.998841">
by our solver, for different sampling sizes s, for
the equation [reader : readable = doer : ?]. nb
indicates the number of (different) solutions gen-
erated. According to our definition, there are 32
distinct solutions to this equation. Note that our
solver has no problem producing doable.
</bodyText>
<subsectionHeader confidence="0.998292">
3.2 Searching for input triplets
</subsectionHeader>
<bodyText confidence="0.999817">
A brute-force approach to identifying the input
triplets that define an analogy with the incom-
plete observation u = (t, ?) consists in enumerat-
ing triplets in the input space and checking for an
</bodyText>
<footnote confidence="0.777297">
2We used s = 2000 in this study.
</footnote>
<page confidence="0.996265">
489
</page>
<bodyText confidence="0.887098333333333">
function shuffle(y,z)
Input: hy, zi two forms
Output: a random word in y ◦ z
ify=cthen
return z
else
</bodyText>
<equation confidence="0.703462">
n ← rand(1,|y|)
return y[1:n] . shuffle(z,y[n+1:])
function complementary(m,x,r,s)
Input: m ∈ y ◦ z, x
Output: the set m \ x
if (m = c) then
if (x = c) then
s ← s ∪ r
else
complementary(m[2:],x,r.m[1],s)
if m[1] = x[1] then
complementary(m[2:],x[2:],r,s)
</equation>
<bodyText confidence="0.985906833333333">
Algorithm 1: Simulation of the two rational op-
erations required by the solver. x[a:b] denotes the
sequence of symbols x starting from index a to
index b inclusive. x[a:] denotes the suffix of x
starting at index a.
analogical relation with t. This amounts to check
o(|I|3) analogies, which is manageable for toy
problems only. Instead, Langlais and Patry (2007)
proposed to solve analogical equations [y : x = t :
?] for some pairs hx, yi belonging to the neighbor-
hood3 of I(u), denoted N(t). Those solutions that
belong to the input space are the z-forms retained;
</bodyText>
<equation confidence="0.939505">
E-T(u) = { hx, y, zi : x ∈ N(t) , y ∈ N(x),
z ∈ [y : x = t : ?] ∩ I }
</equation>
<bodyText confidence="0.95955925">
This strategy (hereafter named LP) directly fol-
lows from a symmetrical property of an analogy
([x : y = z : t] ⇔ [y : x = t : z]), and reduces
the search procedure to the resolution of a number
of analogical equations which is quadratic with the
number of pairs hx, yi sampled.
We found this strategy to be of little use for
input spaces larger than a few tens of thousands
forms. To solve this problem, we exploit a prop-
erty on symbol counts that an analogical relation
must fulfill (Lepage, 1998):
[x : y = z : t] ⇒ |x|, + |t|, = |y|, + |z|, ∀c ∈ A
</bodyText>
<footnote confidence="0.9341985">
3The authors proposed to sample x and y among the clos-
est forms in terms of edit-distance to I(u).
</footnote>
<bodyText confidence="0.961194">
function solver(hx, y, zi, s)
Input: hx, y, zi, a triplet, s the sampling size
Output: a set of solutions to [x : y = z : ?]
</bodyText>
<equation confidence="0.924678285714286">
sol ← φ
for i ← 1 to s do
ha, bi ← odd(rand(0, 1))? hz, yi : hy, zi
m ← shuffle(a,b)
c ← complementary(m,x,c,{})
sol ← sol ∪ c
return sol
</equation>
<listItem confidence="0.46410425">
Algorithm 2: A Stroppa&amp;Yvon flavored solver.
rand(a, b) returns a random integer between a
and b (included). The ternary operator ?: is to
be understood as in the C language.
</listItem>
<bodyText confidence="0.999134916666667">
where A is the alphabet on which the forms are
built, and |x|, stands for the number of occur-
rences of symbol c in x.
Our search strategy (named TC) begins by se-
lecting an x-form in the input space. This en-
forces a set of necessary constraints on the counts
of characters that any two forms y and z must sat-
isfy for [x : y = z : t] to be true. By considering
all forms x in turn,4 we collect a set of candidate
triplets for t. A verification of those that define
with t an analogy must then be carried out. For-
mally, we built:
</bodyText>
<equation confidence="0.996001">
E-T(u) = { hx, y, zi : x ∈ I,
hy, zi ∈ C(hx, ti),
[x : y = z : t] }
</equation>
<bodyText confidence="0.999984090909091">
where C(hx, ti) denotes the set of pairs hy, zi
which satisfy the count property.
This strategy will only work if (i) the number
of quadruplets to check is much smaller than the
number of triplets we can form in the input space
(which happens to be the case in practice), and
if (ii) we can efficiently identify the pairs hy, zi
that satisfy a set of constraints on character counts.
To this end, we proposed in (Langlais and Yvon,
2008) to organize the input space into a data struc-
ture which supports efficient runtime retrieval.
</bodyText>
<subsectionHeader confidence="0.998577">
3.3 The selector
</subsectionHeader>
<bodyText confidence="0.999544">
Step 3 of analogical learning consists in selecting
one or several solutions from the set of candidate
forms produced by the generator. We trained in
a supervised manner a binary classifier to distin-
guish good translation candidates (as defined by
</bodyText>
<footnote confidence="0.975511">
4Anagram forms do not have to be considered separately.
</footnote>
<page confidence="0.995465">
490
</page>
<bodyText confidence="0.9994561">
a reference) from spurious ones. We applied to
this end the voted-perceptron algorithm described
by Freund and Schapire (1999). Online voted-
perceptrons have been reported to work well in a
number of NLP tasks (Collins, 2002; Liang et al.,
2006). Training such a classifier is mainly a matter
of feature engineering. An example e is a pair of
source-target analogical relations (r, r) identified
by the generator, and which elects t as a transla-
tion for the term t:
</bodyText>
<equation confidence="0.865053">
e = (r, r) = ([x : y = z : t],[x : y�= z�: fl)
</equation>
<bodyText confidence="0.999800666666667">
where x, y, and z� are respectively the projections
of the source terms x, y and z. We investigated
many features including (i) the degree of r and r,
</bodyText>
<listItem confidence="0.7645802">
(ii) the frequency with which a form is generated,5
(iii) length ratios between t and t, (iv) likelihoods
scores (min, max, avg.) computed by a character-
based n-gram model trained on a large general cor-
pus (without overlap to DEV or TRAIN), etc.
</listItem>
<sectionHeader confidence="0.998975" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999754">
4.1 Calibrating the engine
</subsectionHeader>
<bodyText confidence="0.999991473684211">
We compared the two aforementioned searching
strategies on a task of identifying triplets in an
input space of French words for 1000 randomly
selected test words. We considered input spaces
of various sizes. The results are reported in Ta-
ble 2. TC clearly outperforms LP by systemati-
cally identifying more triplets in much less time.
For the largest input space of 84 000 forms, TC
could identify an average of 746 triplets for 946
test words in 1.2 seconds, while the best compro-
mise we could settle with LP allows the identifi-
cation of 56 triplets on average for 889 words in
6.3 seconds on average. Note that in this exper-
iment, LP was calibrated for each input space so
that the best compromise between recall (%s) and
speed could be found. Reducing the size of the
neighborhood in LP improves computation time,
but significantly affects recall. In the following,
we only consider the TC search strategy.
</bodyText>
<subsectionHeader confidence="0.870261">
4.2 Experimental Protocol
</subsectionHeader>
<bodyText confidence="0.99899775">
Datasets The data we used in this study comes
from the Medical Subject Headings (MeSH) the-
saurus. This thesaurus is used by the US National
Library of Medicine to index the biomedical sci-
</bodyText>
<footnote confidence="0.476144">
5A form t may be generated thanks to many examples.
</footnote>
<table confidence="0.9937415">
s %s (s) s %s (s) s %s (s)
TC 34 83.1 0.2 261 94.1 0.5 746 96.4 1.2
LP 17 71.7 7.4 46 85.0 7.6 56 88.9 6.3
|I |20 000 50 000 84 076
</table>
<tableCaption confidence="0.990978">
Table 2: Average number s of input analogies
</tableCaption>
<bodyText confidence="0.988237615384616">
found over 1000 test words as a function of the
size of the input space. %s stands for the percent-
age of source forms for which (at least) one source
triplet is found; and (s) indicates the average time
(counted in seconds) to treat one form.
entific literature in the MEDLINE database.6 Its
preferred terms are called ”Main Headings”. We
collected pairs of source and target Main Head-
ings (TTY = ’MH’) with the same MeSH identi-
fiers (SDUI).
We considered five language pairs with three
relatively close European languages (English-
French, English-Spanish and English-Swedish), a
more distant one (English-Finnish) and one pair
involving different scripts (English-Russian).7
The material was split in three randomly se-
lected parts, so that the development and test ma-
terial contain exactly 1000 terms each. The char-
acteristics of this material are reported in Table 3.
For the Finnish-English and Swedish-English lan-
guage pairs, the ratio of uni-terms in the Foreign
language (uf%) is twice the ratio of uni-terms in
the English counterpart. This is simply due to
the agglutinative nature of these two languages.
For instance, according to MeSH, the English
multi-term speech articulation tests corresponds
to the Finnish uni-term ¨a¨ant¨amiskokeet and to the
Swedish one artikulationstester. The ratio of out-
of-vocabulary forms (space-separated words un-
seen in TRAIN) in the TEST material is rather
high: between 36% and 68% for all Foreign-
to-English translation directions, but Finnish-to-
English, where surprisingly, only 6% of the word
forms are unknown.
Evaluation metrics For each experimental con-
dition, we compute the following measures:
Coverage the fraction of input words for which
the system can generate translations. If Nt words
receive translations among N, coverage is Nt/N.
</bodyText>
<footnote confidence="0.999213833333333">
6The MeSH thesaurus and its translations are included in
the UMLS Metathesaurus.
7Russian MeSH is normally written in Cyrillic, but some
terms are simply English terms written in uppercase Latin
script (e.g., ACHROMOBACTER for English Achromobac-
ter). We removed those terms.
</footnote>
<page confidence="0.994335">
491
</page>
<table confidence="0.996923857142857">
f TRAIN TEST DEV TEST
nb uf% ue% nb uf% uf% oov%
FI 19 787 63.7 33.7 1000 64.2 64.0 5.7
FR 17 230 29.8 29.3 1000 30.8 28.3 36.3
RU 21407 38.6 38.6 1000 38.5 40.2 44.4
SP 19 021 31.1 31.1 1000 31.7 33.3 36.6
SW 17 090 67.9 32.5 1000 67.4 67.9 68.4
</table>
<tableCaption confidence="0.9016515">
Table 3: Main characteristics of our datasets. nb
indicates the number of pairs of terms in a bi-
text, uf% (ue%) stands for the percentage of uni-
terms in the Foreign (English) part. oov% indi-
cates the percentage of out-of-vocabulary forms
(space-separated forms of TEST unseen in TRAIN).
</tableCaption>
<table confidence="0.999971636363636">
Cov P1 R1 P100 R100 R.
-* FI 47.1 31.6 14.9 57.7 27.2 31.9
FR 41.2 35.4 14.6 60.4 24.9 26.5
RU 46.2 40.5 18.7 69.9 32.3 34.8
SP 47.0 41.5 19.5 69.1 32.5 35.9
SW 42.8 36.0 15.4 66.8 28.6 31.9
+- FI 44.8 36.6 16.4 66.7 29.9 33.2
FR 38.5 47.0 18.1 69.9 26.9 29.4
RU 42.1 49.4 20.8 70.3 29.6 32.3
SP 42.6 47.7 20.3 75.1 32.0 33.7
SW 44.6 40.8 18.2 69.5 31.0 32.9
</table>
<tableCaption confidence="0.995715">
Table 4: Main characteristics of the generator, as a
</tableCaption>
<bodyText confidence="0.963732642857143">
function of the translation directions (TEST).
Precision among the Nt words for which the
system proposes an answer, precision is the pro-
portion of those for which a correct translation is
output. Depending on the number of output trans-
lations k that one is willing to examine, a correct
translation will be output for Nk input words. Pre-
cision at rank k is thus defined as Pk = Nk/Nt.
Recall is the proportion of the N input words
for which a correct translation is output. Recall at
rank k is defined as Rk = Nk/N.
In all our experiments, candidate translations
are sorted in decreasing order of frequency with
which they were generated.
</bodyText>
<subsectionHeader confidence="0.999175">
4.3 The generator
</subsectionHeader>
<bodyText confidence="0.999967513513514">
The performances of the generator on the 10
translation sessions are reported in Table 4.
The coverage of the generator varies between
38.5% (French-to-English) and 47.1% (English-
to-Finnish), which is rather low. In most cases, the
silence of the generator is due to a failure to iden-
tify analogies in the input space (step 1). The last
column of Table 4 reports the maximum recall we
can obtain if we consider all the candidates output
by the generator. The relative accuracy of the gen-
erator, expressed by the ratio of R,,,, to cov, ranges
from 64.3% (English-French) to 79.1% (Spanish-
to-English), for an average value of 73.8% over
all translation directions. This roughly means that
one fourth of the test terms with at least one solu-
tion do not contain the reference.
Overall, we conclude that analogical learning
offers comparable performances for all transla-
tion directions, although some fluctuations are ob-
served. We do not observe that the approach is
affected by language pairs which do not share the
same script (Russian/English). The best (worse)
case (as far as R,,,, is concerned) corresponds to
translating into Spanish (French).
Admittedly, the largest recall and R,,,, values re-
ported in Table 4 are disappointing. Clearly, for
analogical learning to work efficiently, enough lin-
guistic phenomena must be attested in the TRAIN
material. To illustrate this, we collected for the
Spanish-English language pair a set of medical
terms from the Medical Drug Regulatory Activi-
ties thesaurus (MedDRA) which contains roughly
three times more terms than the Spanish-English
material used in this study. This extra material al-
lows to raise the coverage to 73.4% (Spanish to
English) and 79.7% (English to Spanish), an abso-
lute improvement of more than 30%.
</bodyText>
<subsectionHeader confidence="0.99963">
4.4 The selector
</subsectionHeader>
<bodyText confidence="0.999993875">
We trained our classifiers on the several millions
of examples generated while translating the devel-
opment material. Since we considered numerous
feature representations in this study, this implies
saving many huge datafiles on disk. In order to
save some space, we decided to remove forms that
were generated less than 3 times.8 Each classifier
was trained using 20 epochs.
It is important to note that we face a very unbal-
anced task. For instance, for the English to Finnish
task, the generator produces no less than 2.7 mil-
lions of examples, among which only 4150 are
positive ones. Clearly, classifying all the examples
as negative will achieve a very high classification
accuracy, but will be of no practical use. There-
fore, we measure the ability of a classifier to iden-
</bodyText>
<footnote confidence="0.995719">
8Averaged over all translation directions, this incurs an
absolute reduction of the coverage of 3.4%.
</footnote>
<page confidence="0.990036">
492
</page>
<table confidence="0.99972775">
FI-*EN FR-*EN RU-*EN SP-*EN SW-*EN
p r p r p r p r p r
argmax-f1 41.3 56.7 46.7 63.9 48.1 65.6 49.2 63.4 43.2 61.0
s-best 53.6 61.3 57.5 68.4 61.9 66.7 64.3 70.0 53.1 64.4
</table>
<tableCaption confidence="0.999841">
Table 5: Precision (p) and recall (r) of some classifiers on the TEST material.
</tableCaption>
<bodyText confidence="0.999970708333334">
tify the few positive forms among the set of candi-
dates. We measure precision as the percentage of
forms selected by the classifier that are sanctioned
by the reference lexicon, and recall as the percent-
age of forms selected by the classifier over the to-
tal number of sanctioned forms that the classifier
could possibly select. (Recall that the generator
often fails to produce oracle forms.)
The performance measured on the TEST mate-
rial of the best classifier we monitored on DEV
are reported in Table 5 for the Foreign-to-English
translation directions (we made consistent obser-
vations on the reverse directions). For compari-
son purposes, we implemented a baseline classi-
fier (lines argmax-f1) which selects the most-
frequent candidate form. This is the selector
used as a default in several studies on analogi-
cal learning (Lepage and Denoual, 2005; Stroppa
and Yvon, 2005). The baseline identifies between
56.7% to 65.6% of the sanctioned forms, at pre-
cision rates ranging from 41.3% to 49.2%. We
observe for all translation directions that the best
classifier we trained systematically outperforms
this baseline, both in terms of precision and recall.
</bodyText>
<subsectionHeader confidence="0.910835">
4.4.1 The overall system
</subsectionHeader>
<bodyText confidence="0.999886363636364">
Table 6 shows the overall performance of the ana-
logical translation device in terms of precision, re-
call and coverage rates as defined in Section 4.2.
Overall, our best configuration (the one embed-
ding the s-best classifier) translates between
19.3% and 22.5% of the test material, with a preci-
sion ranging from 50.4% to 63.2%. This is better
than the variant which always proposes the most
frequent generated form (argmax-f1). Allowing
more answers increases both precision and recall.
If we allow up to 10 candidates per source term,
the analogical translator translates one fourth of
the terms (26.1%) with a precision of 70.9%, aver-
aged over all translation directions. The oracle
variant, which looks at the reference for select-
ing the good candidates produced by the genera-
tor, gives an upper bound of the performance that
could be obtained with our approach: less than
a third of the source terms can be translated cor-
rectly. Recall however that increasing the TRAIN
material leads to drastic improvements in cover-
age.
</bodyText>
<subsectionHeader confidence="0.999779">
4.5 Comparison with a PB-SMT engine
</subsectionHeader>
<bodyText confidence="0.999871647058823">
To put these figures in perspective, we mea-
sured the performance of a phrase-based statisti-
cal MT (PB-SMT) engine trained to handle the
same translation task. We trained a phrase table
on TRAIN, using the standard approach.9 How-
ever, because of the small training size, and the
rather huge OOV rate of the translation tasks we
address, we did not train translation models on
word-tokens, but at the character level. There-
fore a phrase is indeed a sequence of charac-
ters. This idea has been successively investigated
in a Catalan-to-Spanish translation task by Vi-
lar et al. (2007). We tuned the 8 coefficients of
the so-called log-linear combination maximized
at decoding time on the first 200 pairs of terms
of the DEV corpora. On the DEV set, BLEU
scores10 range from 67.2 (English-to-Finnish) to
77.0 (Russian-to-English).
Table 7 reports the precision and recall of both
translation engines. Note that because the SMT
engine always propose a translation, its precision
equals its recall. First, we observe that the preci-
sion of the SMT engine is not high (between 17%
and 31%), which demonstrates the difficulty of
the task. The analogical device does better for all
translation directions (see Table 6), but at a much
lower recall, remaining silent more than half of
the time. This suggests that combining both sys-
tems could be advantageous. To verify this, we
ran a straightforward combination: whenever the
analogical device produces a translation, we pick
it; otherwise, the statistical output is considered.
The gains of the resulting system over the SMT
alone are reported in column AB. Averaged over
</bodyText>
<footnote confidence="0.9997375">
9We used the scripts distributed by Philipp Koehn to train
the phrase-table, and Pharaoh (Koehn, 2004) for producing
the translations.
10We computed BLEU scores at the character level.
</footnote>
<page confidence="0.994926">
493
</page>
<table confidence="0.999641142857143">
k FI-*EN FR-*EN RU-*EN SP-*EN SW-*EN
Pk Rk Pk Rk Pk Rk Pk Rk Pk Rk
argmax-f 1 41.3 17.3 46.7 16.8 47.8 18.6 48.7 19.2 43.4 18.1
10 61.6 25.8 62.8 22.6 61.7 24.0 69.3 27.3 62.1 25.9
s-best 1 53.5 20.8 56.9 19.3 58.5 20.3 63.2 22.5 50.4 21
10 69.4 27.0 69.0 23.4 71.8 24.9 78.4 27.9 65.7 27.4
oracle 1 100 30.5 100 26.3 100 28.5 100 30.6 100 29.5
</table>
<tableCaption confidence="0.999394">
Table 6: Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST).
</tableCaption>
<bodyText confidence="0.98191665">
all translation directions, BLEU scores increase on
TEST from 66.2 to 71.5, that is, an absolute im-
provement of 5.3 points.
Table 7: Translation performances on TEST. Psmt
stands for the precision and recall of the SMT en-
gine. AB indicates the absolute gain in BLEU
score of the combined system.
We noticed a tendency of the statistical engine
to produce literal translations; a default the ana-
logical device does not show. For instance, the
Spanish term instituciones de atenci´on ambulato-
ria is translated word for word by Pharaoh into
institutions, atention ambulatory while analogical
learning produces ambulatory care facilities. We
also noticed that analogical learning sometimes
produces wrong translations based on morpholog-
ical regularities that are applied blindly. This is,
for instance, the case in a Russian/English exam-
ple where mouthal manifestations is produced, in-
stead of oral manifestations.
</bodyText>
<sectionHeader confidence="0.994528" genericHeader="conclusions">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.99996128">
In this study, we proposed solutions to practical is-
sues involved in analogical learning. A simple yet
effective implementation of a solver is described.
A search strategy is proposed which outperforms
the one described in (Langlais and Patry, 2007).
Also, we showed that a classifier trained to se-
lect good candidate translations outperforms the
most-frequently-generated heuristic used in sev-
eral works on analogical learning.
Our analogical device was used to translate
medical terms in different language pairs. The
approach rates comparably across the 10 transla-
tion directions we considered. In particular, we
do not see a drop in performance when trans-
lating into a morphology rich language (such as
Finnish), or when translating into languages with
different scripts. Averaged over all translation di-
rections, the best variant could translate in first po-
sition 21% of the terms with a precision of 57%,
while at best, one could translate 30% of the terms
with a perfect precision. We show that the ana-
logical translations are of better quality than those
produced by a phrase-based engine trained at the
character level, albeit with much lower recall. A
straightforward combination of both approaches
led an improvement of 5.3 BLEU points over the
SMT alone. Better SMT performance could be
obtained with a system based on morphemes, see
for instance (Toutanova et al., 2008). However,
since lists of morphemes specific to the medical
domain do not exist for all the languages pairs we
considered here, unsupervised methods for acquir-
ing morphemes would be necessary, which is left
as a future work. In any case, this comparison is
meaningful, since both the SMT and the analogi-
cal device work at the character level.
This work opens up several avenues. First, we
will test our approach on terminologies from dif-
ferent domains, varying the size of the training
material. Second, analyzing the segmentation in-
duced by analogical learning would be interesting.
Third, we need to address the problem of com-
bining the translations produced by analogy into a
front-end statistical translation engine. Last, there
is no reason to constrain ourselves to translating
terminology only. We targeted this task in the first
place, because terminology typically plugs trans-
lation systems, but we think that analogical learn-
ing could be useful for translating infrequent enti-
ties.
</bodyText>
<figure confidence="0.9972006">
-* EN
&lt;-- EN
FI
FR
RU
SP
SW
Psmt
20.2
19.9
24.1
22.1
25.9
AB
+7.4
+5.3
+3.1
+4.9
+4.2
Psmt AB
21.6 +6.4
17.0 +6.0
28.0 +6.4
26.4 +5.5
31.6 +3.2
</figure>
<page confidence="0.997528">
494
</page>
<sectionHeader confidence="0.993643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999972742857143">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–311.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In EMNLP, pages 1–8, Mor-
ristown, NJ, USA.
E. Denoual. 2007. Analogical translation of unknown
words in a statistical machine translation framework.
In MT Summit, XI, pages 10–14, Copenhagen.
Y. Freund and R. E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Mach.
Learn., 37(3):277–296.
P. Fung and K. McKeown. 1997. Finding terminology
translations from non-parallel corpora. In 5th An-
nual Workshop on Very Large Corpora, pages 192–
202, Hong Kong.
M. Itagaki, T. Aikawa, and X. He. 2007. Auto-
matic validation of terminology translation consis-
tency with statistical method. In MT Summit XI,
pages 269–274, Copenhagen, Denmark.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models.
In AMTA, pages 115–124, Washington, DC, USA.
P. Langlais and A. Patry. 2007. Translating unknown
words by analogical learning. In EMNLP-CoNLL,
pages 877–886, Prague, Czech Republic.
P. Langlais and F. Yvon. 2008. Scaling up analogi-
cal learning. In 22nd International Conference on
Computational Linguistics (COLING 2008), pages
51–54, Manchester, United Kingdom.
Y. Lepage and E. Denoual. 2005. ALEPH: an EBMT
system based on the preservation of proportion-
nal analogies between sentences across languages.
In International Workshop on Statistical Language
Translation (IWSLT), Pittsburgh, PA, October.
Y. Lepage and A. Lardilleux. 2007. The GREYC Ma-
chine Translation System for the IWSLT 2007 Eval-
uation Campaign. In IWLST, pages 49–53, Trento,
Italy.
Y. Lepage. 1998. Solving analogies on words: an algo-
rithm. In COLING-ACL, pages 728–734, Montreal,
Canada.
P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In 21st COLING and 44th ACL,
pages 761–768, Sydney, Australia.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In 45th ACL, pages
664–671, Prague, Czech Republic.
R. Rapp. 1995. Identifying word translation in non-
parallel texts. In 33rd ACL, pages 320–322, Cam-
bridge,Massachusetts, USA.
N. Stroppa and F. Yvon. 2005. An analogical learner
for morphological analysis. In 9th CoNLL, pages
120–127, Ann Arbor, MI.
K Toutanova, H. Suzuki, and A. Ruopp. 2008. Ap-
plying morphology generation models to machine
translation. In ACL-8 HLT, pages 514–522, Colom-
bus, Ohio, USA.
D. Vilar, J. Peter, and H. Ney. 2007. Can we trans-
late letters? In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 33–
39, Prague, Czech Republic, June.
F. Yvon, N. Stroppa, A. Delhay, and L. Miclet. 2004.
Solving analogical equations on words. Techni-
cal Report D005, ´Ecole Nationale Sup´erieure des
T´el´ecommunications, Paris, France, July.
</reference>
<page confidence="0.999131">
495
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.849775">
<title confidence="0.978805">Improvements in Analogical Learning: Application to Translating multi-Terms of the Medical Domain</title>
<author confidence="0.990751">Langlais Yvon</author>
<author confidence="0.990751">Pierre Zweigenbaum</author>
<affiliation confidence="0.9641985">DIRO LIMSI-CNRS Univ. of Montreal, Canada Univ. Paris-Sud XI, France</affiliation>
<abstract confidence="0.996732882352941">Handling terminology is an important matter in a translation workflow. However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1530" citStr="Brown et al., 1993" startWordPosition="219" endWordPosition="222"> If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16017" citStr="Collins, 2002" startWordPosition="2764" endWordPosition="2765">nto a data structure which supports efficient runtime retrieval. 3.3 The selector Step 3 of analogical learning consists in selecting one or several solutions from the set of candidate forms produced by the generator. We trained in a supervised manner a binary classifier to distinguish good translation candidates (as defined by 4Anagram forms do not have to be considered separately. 490 a reference) from spurious ones. We applied to this end the voted-perceptron algorithm described by Freund and Schapire (1999). Online votedperceptrons have been reported to work well in a number of NLP tasks (Collins, 2002; Liang et al., 2006). Training such a classifier is mainly a matter of feature engineering. An example e is a pair of source-target analogical relations (r, r) identified by the generator, and which elects t as a translation for the term t: e = (r, r) = ([x : y = z : t],[x : y�= z�: fl) where x, y, and z� are respectively the projections of the source terms x, y and z. We investigated many features including (i) the degree of r and r, (ii) the frequency with which a form is generated,5 (iii) length ratios between t and t, (iv) likelihoods scores (min, max, avg.) computed by a characterbased n</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP, pages 1–8, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Denoual</author>
</authors>
<title>Analogical translation of unknown words in a statistical machine translation framework.</title>
<date>2007</date>
<booktitle>In MT Summit, XI,</booktitle>
<pages>10--14</pages>
<location>Copenhagen.</location>
<contexts>
<context position="2819" citStr="Denoual (2007)" startWordPosition="425" endWordPosition="426">such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms of the medical domain; a domain well-known for its tendency to create new words, many of which being complex lexical constructions.</context>
</contexts>
<marker>Denoual, 2007</marker>
<rawString>E. Denoual. 2007. Analogical translation of unknown words in a statistical machine translation framework. In MT Summit, XI, pages 10–14, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<journal>Mach. Learn.,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="15920" citStr="Freund and Schapire (1999)" startWordPosition="2745" endWordPosition="2748">aints on character counts. To this end, we proposed in (Langlais and Yvon, 2008) to organize the input space into a data structure which supports efficient runtime retrieval. 3.3 The selector Step 3 of analogical learning consists in selecting one or several solutions from the set of candidate forms produced by the generator. We trained in a supervised manner a binary classifier to distinguish good translation candidates (as defined by 4Anagram forms do not have to be considered separately. 490 a reference) from spurious ones. We applied to this end the voted-perceptron algorithm described by Freund and Schapire (1999). Online votedperceptrons have been reported to work well in a number of NLP tasks (Collins, 2002; Liang et al., 2006). Training such a classifier is mainly a matter of feature engineering. An example e is a pair of source-target analogical relations (r, r) identified by the generator, and which elects t as a translation for the term t: e = (r, r) = ([x : y = z : t],[x : y�= z�: fl) where x, y, and z� are respectively the projections of the source terms x, y and z. We investigated many features including (i) the degree of r and r, (ii) the frequency with which a form is generated,5 (iii) lengt</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Mach. Learn., 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K McKeown</author>
</authors>
<title>Finding terminology translations from non-parallel corpora.</title>
<date>1997</date>
<booktitle>In 5th Annual Workshop on Very Large Corpora,</booktitle>
<pages>192--202</pages>
<location>Hong Kong.</location>
<contexts>
<context position="1999" citStr="Fung and McKeown, 1997" startWordPosition="290" endWordPosition="293">et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning t</context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>P. Fung and K. McKeown. 1997. Finding terminology translations from non-parallel corpora. In 5th Annual Workshop on Very Large Corpora, pages 192– 202, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Itagaki</author>
<author>T Aikawa</author>
<author>X He</author>
</authors>
<title>Automatic validation of terminology translation consistency with statistical method.</title>
<date>2007</date>
<booktitle>In MT Summit XI,</booktitle>
<pages>269--274</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1389" citStr="Itagaki et al., 2007" startWordPosition="197" endWordPosition="200">nguage pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements. 1 Introduction If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKe</context>
</contexts>
<marker>Itagaki, Aikawa, He, 2007</marker>
<rawString>M. Itagaki, T. Aikawa, and X. He. 2007. Automatic validation of terminology translation consistency with statistical method. In MT Summit XI, pages 269–274, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA,</booktitle>
<pages>115--124</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="28639" citStr="Koehn, 2004" startWordPosition="4908" endWordPosition="4909">the difficulty of the task. The analogical device does better for all translation directions (see Table 6), but at a much lower recall, remaining silent more than half of the time. This suggests that combining both systems could be advantageous. To verify this, we ran a straightforward combination: whenever the analogical device produces a translation, we pick it; otherwise, the statistical output is considered. The gains of the resulting system over the SMT alone are reported in column AB. Averaged over 9We used the scripts distributed by Philipp Koehn to train the phrase-table, and Pharaoh (Koehn, 2004) for producing the translations. 10We computed BLEU scores at the character level. 493 k FI-*EN FR-*EN RU-*EN SP-*EN SW-*EN Pk Rk Pk Rk Pk Rk Pk Rk Pk Rk argmax-f 1 41.3 17.3 46.7 16.8 47.8 18.6 48.7 19.2 43.4 18.1 10 61.6 25.8 62.8 22.6 61.7 24.0 69.3 27.3 62.1 25.9 s-best 1 53.5 20.8 56.9 19.3 58.5 20.3 63.2 22.5 50.4 21 10 69.4 27.0 69.0 23.4 71.8 24.9 78.4 27.9 65.7 27.4 oracle 1 100 30.5 100 26.3 100 28.5 100 30.6 100 29.5 Table 6: Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST). all translation directions, BLEU scores increase on TEST from 66.2 t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA, pages 115–124, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
<author>A Patry</author>
</authors>
<title>Translating unknown words by analogical learning.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>877--886</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2689" citStr="Langlais and Patry (2007)" startWordPosition="402" endWordPosition="405">gree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms o</context>
<context position="12979" citStr="Langlais and Patry (2007)" startWordPosition="2158" endWordPosition="2161">n z else n ← rand(1,|y|) return y[1:n] . shuffle(z,y[n+1:]) function complementary(m,x,r,s) Input: m ∈ y ◦ z, x Output: the set m \ x if (m = c) then if (x = c) then s ← s ∪ r else complementary(m[2:],x,r.m[1],s) if m[1] = x[1] then complementary(m[2:],x[2:],r,s) Algorithm 1: Simulation of the two rational operations required by the solver. x[a:b] denotes the sequence of symbols x starting from index a to index b inclusive. x[a:] denotes the suffix of x starting at index a. analogical relation with t. This amounts to check o(|I|3) analogies, which is manageable for toy problems only. Instead, Langlais and Patry (2007) proposed to solve analogical equations [y : x = t : ?] for some pairs hx, yi belonging to the neighborhood3 of I(u), denoted N(t). Those solutions that belong to the input space are the z-forms retained; E-T(u) = { hx, y, zi : x ∈ N(t) , y ∈ N(x), z ∈ [y : x = t : ?] ∩ I } This strategy (hereafter named LP) directly follows from a symmetrical property of an analogy ([x : y = z : t] ⇔ [y : x = t : z]), and reduces the search procedure to the resolution of a number of analogical equations which is quadratic with the number of pairs hx, yi sampled. We found this strategy to be of little use for </context>
<context position="30360" citStr="Langlais and Patry, 2007" startWordPosition="5193" endWordPosition="5196">ulatory while analogical learning produces ambulatory care facilities. We also noticed that analogical learning sometimes produces wrong translations based on morphological regularities that are applied blindly. This is, for instance, the case in a Russian/English example where mouthal manifestations is produced, instead of oral manifestations. 5 Discussion and future work In this study, we proposed solutions to practical issues involved in analogical learning. A simple yet effective implementation of a solver is described. A search strategy is proposed which outperforms the one described in (Langlais and Patry, 2007). Also, we showed that a classifier trained to select good candidate translations outperforms the most-frequently-generated heuristic used in several works on analogical learning. Our analogical device was used to translate medical terms in different language pairs. The approach rates comparably across the 10 translation directions we considered. In particular, we do not see a drop in performance when translating into a morphology rich language (such as Finnish), or when translating into languages with different scripts. Averaged over all translation directions, the best variant could translat</context>
</contexts>
<marker>Langlais, Patry, 2007</marker>
<rawString>P. Langlais and A. Patry. 2007. Translating unknown words by analogical learning. In EMNLP-CoNLL, pages 877–886, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
<author>F Yvon</author>
</authors>
<title>Scaling up analogical learning.</title>
<date>2008</date>
<booktitle>In 22nd International Conference on Computational Linguistics (COLING</booktitle>
<pages>51--54</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="15374" citStr="Langlais and Yvon, 2008" startWordPosition="2658" endWordPosition="2661">te triplets for t. A verification of those that define with t an analogy must then be carried out. Formally, we built: E-T(u) = { hx, y, zi : x ∈ I, hy, zi ∈ C(hx, ti), [x : y = z : t] } where C(hx, ti) denotes the set of pairs hy, zi which satisfy the count property. This strategy will only work if (i) the number of quadruplets to check is much smaller than the number of triplets we can form in the input space (which happens to be the case in practice), and if (ii) we can efficiently identify the pairs hy, zi that satisfy a set of constraints on character counts. To this end, we proposed in (Langlais and Yvon, 2008) to organize the input space into a data structure which supports efficient runtime retrieval. 3.3 The selector Step 3 of analogical learning consists in selecting one or several solutions from the set of candidate forms produced by the generator. We trained in a supervised manner a binary classifier to distinguish good translation candidates (as defined by 4Anagram forms do not have to be considered separately. 490 a reference) from spurious ones. We applied to this end the voted-perceptron algorithm described by Freund and Schapire (1999). Online votedperceptrons have been reported to work w</context>
</contexts>
<marker>Langlais, Yvon, 2008</marker>
<rawString>P. Langlais and F. Yvon. 2008. Scaling up analogical learning. In 22nd International Conference on Computational Linguistics (COLING 2008), pages 51–54, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lepage</author>
<author>E Denoual</author>
</authors>
<title>ALEPH: an EBMT system based on the preservation of proportionnal analogies between sentences across languages.</title>
<date>2005</date>
<booktitle>In International Workshop on Statistical Language Translation (IWSLT),</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="2431" citStr="Lepage and Denoual (2005)" startWordPosition="360" endWordPosition="364">another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an</context>
<context position="25534" citStr="Lepage and Denoual, 2005" startWordPosition="4400" endWordPosition="4403">y the classifier over the total number of sanctioned forms that the classifier could possibly select. (Recall that the generator often fails to produce oracle forms.) The performance measured on the TEST material of the best classifier we monitored on DEV are reported in Table 5 for the Foreign-to-English translation directions (we made consistent observations on the reverse directions). For comparison purposes, we implemented a baseline classifier (lines argmax-f1) which selects the mostfrequent candidate form. This is the selector used as a default in several studies on analogical learning (Lepage and Denoual, 2005; Stroppa and Yvon, 2005). The baseline identifies between 56.7% to 65.6% of the sanctioned forms, at precision rates ranging from 41.3% to 49.2%. We observe for all translation directions that the best classifier we trained systematically outperforms this baseline, both in terms of precision and recall. 4.4.1 The overall system Table 6 shows the overall performance of the analogical translation device in terms of precision, recall and coverage rates as defined in Section 4.2. Overall, our best configuration (the one embedding the s-best classifier) translates between 19.3% and 22.5% of the te</context>
</contexts>
<marker>Lepage, Denoual, 2005</marker>
<rawString>Y. Lepage and E. Denoual. 2005. ALEPH: an EBMT system based on the preservation of proportionnal analogies between sentences across languages. In International Workshop on Statistical Language Translation (IWSLT), Pittsburgh, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lepage</author>
<author>A Lardilleux</author>
</authors>
<title>Evaluation Campaign. In</title>
<date>2007</date>
<booktitle>The GREYC Machine Translation System for the IWSLT</booktitle>
<pages>49--53</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="7887" citStr="Lepage and Lardilleux, 2007" startWordPosition="1292" endWordPosition="1295">: ?]; and solving it: adrenergic alpha-antagonists is one of its solutions. During inference, analogies are recognized independently in the input and the output space, and nothing pre-establishes which subpart of one input form corresponds to which subpart of the output one. This “knowledge” is passively captured thanks to the inductive bias of the learning strategy (an analogy in the input space corresponds to one in the output space). Also worth mentioning, this procedure does not rely on any pre-defined notion of word. This might come at an advantage for languages that are hard to segment (Lepage and Lardilleux, 2007). 3 Practical issues Each step of analogical learning, that is, searching for input triplets, solving output equations and 488 selecting good candidates involves some practical issues. Since searching for input triplets might involve the need for solving (input) equations, we discuss the solver first. 3.1 The solver Lepage (1998) proposed an algorithm for solving an analogical equation [x : y = z : ?]. An alignment between x and y and between x and z is first computed (by edit-distance) as illustrated in Figure 1. Then, the three strings are synchronized using x as a backbone of the synchroniz</context>
</contexts>
<marker>Lepage, Lardilleux, 2007</marker>
<rawString>Y. Lepage and A. Lardilleux. 2007. The GREYC Machine Translation System for the IWSLT 2007 Evaluation Campaign. In IWLST, pages 49–53, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lepage</author>
</authors>
<title>Solving analogies on words: an algorithm.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>728--734</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="8218" citStr="Lepage (1998)" startWordPosition="1345" endWordPosition="1346"> the learning strategy (an analogy in the input space corresponds to one in the output space). Also worth mentioning, this procedure does not rely on any pre-defined notion of word. This might come at an advantage for languages that are hard to segment (Lepage and Lardilleux, 2007). 3 Practical issues Each step of analogical learning, that is, searching for input triplets, solving output equations and 488 selecting good candidates involves some practical issues. Since searching for input triplets might involve the need for solving (input) equations, we discuss the solver first. 3.1 The solver Lepage (1998) proposed an algorithm for solving an analogical equation [x : y = z : ?]. An alignment between x and y and between x and z is first computed (by edit-distance) as illustrated in Figure 1. Then, the three strings are synchronized using x as a backbone of the synchronization. The algorithm can be seen as a deterministic finite-state machine where a state is defined by the two edit-operations being visited in the two tables. This is schematized by the two cursors in the figure. Two actions are allowed: copy one symbol from y or z into the solution and move one or both cursors. x: read e r read e</context>
<context position="13752" citStr="Lepage, 1998" startWordPosition="2319" endWordPosition="2320">to the input space are the z-forms retained; E-T(u) = { hx, y, zi : x ∈ N(t) , y ∈ N(x), z ∈ [y : x = t : ?] ∩ I } This strategy (hereafter named LP) directly follows from a symmetrical property of an analogy ([x : y = z : t] ⇔ [y : x = t : z]), and reduces the search procedure to the resolution of a number of analogical equations which is quadratic with the number of pairs hx, yi sampled. We found this strategy to be of little use for input spaces larger than a few tens of thousands forms. To solve this problem, we exploit a property on symbol counts that an analogical relation must fulfill (Lepage, 1998): [x : y = z : t] ⇒ |x|, + |t|, = |y|, + |z|, ∀c ∈ A 3The authors proposed to sample x and y among the closest forms in terms of edit-distance to I(u). function solver(hx, y, zi, s) Input: hx, y, zi, a triplet, s the sampling size Output: a set of solutions to [x : y = z : ?] sol ← φ for i ← 1 to s do ha, bi ← odd(rand(0, 1))? hz, yi : hy, zi m ← shuffle(a,b) c ← complementary(m,x,c,{}) sol ← sol ∪ c return sol Algorithm 2: A Stroppa&amp;Yvon flavored solver. rand(a, b) returns a random integer between a and b (included). The ternary operator ?: is to be understood as in the C language. where A is</context>
</contexts>
<marker>Lepage, 1998</marker>
<rawString>Y. Lepage. 1998. Solving analogies on words: an algorithm. In COLING-ACL, pages 728–734, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In 21st COLING and 44th ACL,</booktitle>
<pages>761--768</pages>
<location>Sydney, Australia.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In 21st COLING and 44th ACL, pages 761–768, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Morin</author>
<author>B Daille</author>
<author>K Takeuchi</author>
<author>K Kageura</author>
</authors>
<title>Bilingual terminology mining - using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In 45th ACL,</booktitle>
<pages>664--671</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2153" citStr="Morin et al. (2007)" startWordPosition="319" endWordPosition="322">mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several </context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>E. Morin, B. Daille, K. Takeuchi, and K. Kageura. 2007. Bilingual terminology mining - using brain, not brawn comparable corpora. In 45th ACL, pages 664–671, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Identifying word translation in nonparallel texts.</title>
<date>1995</date>
<booktitle>In 33rd ACL,</booktitle>
<pages>320--322</pages>
<location>Cambridge,Massachusetts, USA.</location>
<contexts>
<context position="1974" citStr="Rapp, 1995" startWordPosition="288" endWordPosition="289">ns (Itagaki et al., 2007). One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) appl</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>R. Rapp. 1995. Identifying word translation in nonparallel texts. In 33rd ACL, pages 320–322, Cambridge,Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stroppa</author>
<author>F Yvon</author>
</authors>
<title>An analogical learner for morphological analysis.</title>
<date>2005</date>
<booktitle>In 9th CoNLL,</booktitle>
<pages>120--127</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2569" citStr="Stroppa and Yvon (2005)" startWordPosition="385" endWordPosition="388">ector techniques (Rapp, 1995; Fung and McKeown, 1997) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007), for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone. Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task. In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recogn</context>
<context position="4749" citStr="Stroppa and Yvon, 2005" startWordPosition="747" endWordPosition="750">is a relation between four items noted [x : y = z : t] which reads as “x is to y as z is to t”. Among proportional analogies, we distinguish formal analogies, that is, those we can identify at a graphemic level, such as [adrenergic beta-agonists, adrenergic beta-antagonists, adrenergic alpha-agonists, adrenergic alpha-antagonists]. Formal analogies can be defined in terms of factorizations1. Let x be a string over an alphabet E, a factorization of x, noted fx, is a sequence of n factors fx = (fX, ... , fnx), such that x = fX O fX O ... O fnx, where O denotes the concatenation operator. After (Stroppa and Yvon, 2005) we thus define a formal analogy as: Definition 1 ∀(x, y, z, t) ∈ E?4, [x : y = z : t] iff there exist factorizations (fx, fy, fz, ft) ∈ (E?d), of (x, y, z, t) such that, ∀i ∈ [1, d], (fiy, fiz) ∈ { 1 (fix, fit ), (fit , fix) . The smallest d for which this definition holds is called the degree of the analogy. Intuitively, this definition states that (x, y, z, t) are made up of a common set of alternating substrings. It is routine to check that it captures the exemplar analogy introduced above, based on the following set of factorizations: fx ≡ (adrenergic bet, a-agonists) fy ≡ (adrenergic bet</context>
<context position="25559" citStr="Stroppa and Yvon, 2005" startWordPosition="4404" endWordPosition="4407">total number of sanctioned forms that the classifier could possibly select. (Recall that the generator often fails to produce oracle forms.) The performance measured on the TEST material of the best classifier we monitored on DEV are reported in Table 5 for the Foreign-to-English translation directions (we made consistent observations on the reverse directions). For comparison purposes, we implemented a baseline classifier (lines argmax-f1) which selects the mostfrequent candidate form. This is the selector used as a default in several studies on analogical learning (Lepage and Denoual, 2005; Stroppa and Yvon, 2005). The baseline identifies between 56.7% to 65.6% of the sanctioned forms, at precision rates ranging from 41.3% to 49.2%. We observe for all translation directions that the best classifier we trained systematically outperforms this baseline, both in terms of precision and recall. 4.4.1 The overall system Table 6 shows the overall performance of the analogical translation device in terms of precision, recall and coverage rates as defined in Section 4.2. Overall, our best configuration (the one embedding the s-best classifier) translates between 19.3% and 22.5% of the test material, with a preci</context>
</contexts>
<marker>Stroppa, Yvon, 2005</marker>
<rawString>N. Stroppa and F. Yvon. 2005. An analogical learner for morphological analysis. In 9th CoNLL, pages 120–127, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H Suzuki</author>
<author>A Ruopp</author>
</authors>
<title>Applying morphology generation models to machine translation.</title>
<date>2008</date>
<booktitle>In ACL-8 HLT,</booktitle>
<pages>514--522</pages>
<location>Colombus, Ohio, USA.</location>
<contexts>
<context position="31494" citStr="Toutanova et al., 2008" startWordPosition="5374" endWordPosition="5377">ferent scripts. Averaged over all translation directions, the best variant could translate in first position 21% of the terms with a precision of 57%, while at best, one could translate 30% of the terms with a perfect precision. We show that the analogical translations are of better quality than those produced by a phrase-based engine trained at the character level, albeit with much lower recall. A straightforward combination of both approaches led an improvement of 5.3 BLEU points over the SMT alone. Better SMT performance could be obtained with a system based on morphemes, see for instance (Toutanova et al., 2008). However, since lists of morphemes specific to the medical domain do not exist for all the languages pairs we considered here, unsupervised methods for acquiring morphemes would be necessary, which is left as a future work. In any case, this comparison is meaningful, since both the SMT and the analogical device work at the character level. This work opens up several avenues. First, we will test our approach on terminologies from different domains, varying the size of the training material. Second, analyzing the segmentation induced by analogical learning would be interesting. Third, we need t</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>K Toutanova, H. Suzuki, and A. Ruopp. 2008. Applying morphology generation models to machine translation. In ACL-8 HLT, pages 514–522, Colombus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vilar</author>
<author>J Peter</author>
<author>H Ney</author>
</authors>
<title>Can we translate letters?</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>33--39</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="27506" citStr="Vilar et al. (2007)" startWordPosition="4723" endWordPosition="4727">ge. 4.5 Comparison with a PB-SMT engine To put these figures in perspective, we measured the performance of a phrase-based statistical MT (PB-SMT) engine trained to handle the same translation task. We trained a phrase table on TRAIN, using the standard approach.9 However, because of the small training size, and the rather huge OOV rate of the translation tasks we address, we did not train translation models on word-tokens, but at the character level. Therefore a phrase is indeed a sequence of characters. This idea has been successively investigated in a Catalan-to-Spanish translation task by Vilar et al. (2007). We tuned the 8 coefficients of the so-called log-linear combination maximized at decoding time on the first 200 pairs of terms of the DEV corpora. On the DEV set, BLEU scores10 range from 67.2 (English-to-Finnish) to 77.0 (Russian-to-English). Table 7 reports the precision and recall of both translation engines. Note that because the SMT engine always propose a translation, its precision equals its recall. First, we observe that the precision of the SMT engine is not high (between 17% and 31%), which demonstrates the difficulty of the task. The analogical device does better for all translati</context>
</contexts>
<marker>Vilar, Peter, Ney, 2007</marker>
<rawString>D. Vilar, J. Peter, and H. Ney. 2007. Can we translate letters? In Proceedings of the Second Workshop on Statistical Machine Translation, pages 33– 39, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Yvon</author>
<author>N Stroppa</author>
<author>A Delhay</author>
<author>L Miclet</author>
</authors>
<title>Solving analogical equations on words.</title>
<date>2004</date>
<booktitle>Technical Report D005, ´Ecole Nationale Sup´erieure des T´el´ecommunications,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="9843" citStr="Yvon et al. (2004)" startWordPosition="1626" endWordPosition="1629">n exponential number of synchronizations will have to be computed. Second, the algorithm fails to deliver an expected form in a rather frequent situation where two identical symbols align fortuitously in two strings. This is for instance the case in our running example where the symbol d in doer aligns to the one in reader, which puzzles the synchronization. Indeed, dabloe is the only form proposed to [reader : readable = doer : ?], while the expected one is doable. The algorithm would have no problem, however, to produce the form writable out of the equation [reader : readable = writer : ?]. Yvon et al. (2004) proposed an analogical solver which is not exposed to the latter problem. It consists in building a finite state transducer which generates the solutions to [x : y = z : ?] while recognizing the form x. Theorem 1 t is a solution to [x : y = z : ?] iff t belongs to {y o z}\x. shuffle and complement are two rational operations. The shuffle of two strings w and v, noted w o v, is the regular language containing the strings obtained by selecting (without replacement) alternatively in w and v, sequences of characters in a left-to-right manner. For instance, spondyondontilalgiatis and ondspondonyla</context>
</contexts>
<marker>Yvon, Stroppa, Delhay, Miclet, 2004</marker>
<rawString>F. Yvon, N. Stroppa, A. Delhay, and L. Miclet. 2004. Solving analogical equations on words. Technical Report D005, ´Ecole Nationale Sup´erieure des T´el´ecommunications, Paris, France, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>