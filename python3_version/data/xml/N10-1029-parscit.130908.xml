<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052444">
<title confidence="0.9984755">
Task-based Evaluation of Multiword Expressions:
a Pilot Study in Statistical Machine Translation
</title>
<author confidence="0.997718">
Marine Carpuat Mona Diab
</author>
<affiliation confidence="0.972146">
Columbia University
Center for Computational Learning Systems
</affiliation>
<address confidence="0.536705">
475 Riverside Drive, New York, NY 10115
</address>
<email confidence="0.999625">
{marine,mdiab}@ccls.columbia.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801375">
We conduct a pilot study for task-oriented
evaluation of Multiword Expression (MWE)
in Statistical Machine Translation (SMT). We
propose two different integration strategies for
MWE in SMT, which take advantage of differ-
ent degrees of MWE semantic compositional-
ity and yield complementary improvements in
SMT quality on a large-scale translation task.1
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992">
A multiword expression (MWE) generally refers to
a multiword unit or a collocation of words that co-
occur together statistically more than chance. A
MWE is a cover term for different types of colloca-
tions which vary in their transparency and fixedness.
Identifying MWEs and understanding their meaning
is considered essential to language understanding,
and of crucial importance for any Natural Language
Processing (NLP) applications that aim at handling
robust language meaning and use. In fact, the sem-
inal paper (Sag et al., 2002) refers to this problem
as a key issue for the development of high-quality
NLP applications. (Villavicencio et al., 2005) iden-
tify Machine Translation as an application of partic-
ular interest since “ recognition of MWEs is neces-
sary for systems to preserve the meaning and pro-
duce appropriate translations and avoid the genera-
tion of unnatural or nonsensical sentences in the tar-
get language.”
However, statistical machine translation (SMT)
typically does not model MWEs explicitly. SMT
</bodyText>
<footnote confidence="0.918638">
1The research was partially funded by IBM under the
DARPA GALE project.
</footnote>
<bodyText confidence="0.999714470588235">
units are typically phrasal translations, defined with-
out any direct syntactic or lexical semantic motiva-
tion: they are simply n-grams that are consistently
translated in parallel corpora. Phrasal translations
might indirectly capture MWEs, but they are not dis-
tinguished from any other n-gram.
As a result, the usefulness of explicitly modeling
MWEs in the SMT framework has not yet been stud-
ied systematically. Previous work has focused on
automatically learning and integrating translations
of very specific MWE categories, such as, for in-
stance, idiomatic Chinese four character expressions
(Bai et al., 2009) or domain specific MWEs (Ren et
al., 2009). MWEs have also been defined not from
a lexical semantics perspective but from a SMT er-
ror reduction perspective, as phrases that are hard
to align during SMT training (Lambert and Banchs,
2005). For each of these particular cases, translation
quality improved by augmenting the SMT transla-
tion lexicon with the learned bilingual MWEs either
directly or through improved word alignments.
In this paper, we consider a more general prob-
lem: we view SMT as an extrinsic evaluation of
the usefulness of monolingual MWEs as used per-
vasively in natural language regardless of domain,
idiomaticity and compositionality. A MWE is com-
positional if its meaning as a unit can be predicted
from the meaning of its component words such as in
make a decision meaning to decide. Some MWEs
are more predictable than others, for instance, kick
the bucket, when used idiomatically to mean to die,
has nothing in common with the literal meaning of
either kick or bucket, while make a decision is very
clearly related to to decide. These expressions are
</bodyText>
<page confidence="0.95396">
242
</page>
<subsubsectionHeader confidence="0.579203">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 242–245,
</subsubsectionHeader>
<subsectionHeader confidence="0.276442">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999804411764706">
both considered MWEs but have varying degrees of
compositionality and predictability.
We explore strategies for integrating all MWEs
along this continuum in SMT. Given a monolingual
MWE lexicon, we propose (1) a static integration
strategy that segments training and test sentences ac-
cording to the MWE vocabulary, and (2) a dynamic
integration strategy that adds a new MWE-based
feature in SMT translation lexicons.
In a pilot study of the impact of WordNet MWEs
on a large-scale English to Arabic SMT system, we
show that static and dynamic strategies both improve
translation quality and that their impact is not the
same for different types of MWEs. This suggests
that the proposed framework would be an interest-
ing testbed for a task-driven evaluation of automatic
MWE extraction.
</bodyText>
<sectionHeader confidence="0.963565" genericHeader="introduction">
2 Static integration of MWE in SMT
</sectionHeader>
<bodyText confidence="0.999924909090909">
The first strategy for integration can be seen as
a generalization of word segmentation for MWEs.
Given a MWE lexicon, we identify MWEs in run-
ning text and turn them into a single unit by un-
derscoring. We call this integration method static,
since, once segmented, all MWEs are considered
frozen from the perspective of the SMT system.
During training and decoding, MWEs are handled
as distinct words regardless of their compositional-
ity, and all knowledge of the MWE components is
lost.
</bodyText>
<sectionHeader confidence="0.997239" genericHeader="method">
3 Dynamic integration of MWE in SMT
</sectionHeader>
<bodyText confidence="0.999342833333333">
The second strategy attempts to encourage cohesive
translations of MWEs without ignoring their com-
ponents. Word alignment and phrasal translation
extraction are conducted without any MWE knowl-
edge, so that the SMT system can learn word-for-
word translations from consistently translated com-
positional MWEs. MWE knowledge is integrated as
a feature in the translation lexicon. For each entry,
in addition to the standard phrasal translation proba-
bilities, we define a count feature that represents the
number of MWEs in the input language phrase.
We refer to this integration strategy as dynamic,
because the SMT system decides at decoding time
how to segment the input sentence. The MWE fea-
ture biases the system towards using phrases that do
not break MWEs. This can be seen as a generaliza-
tion of the binary MWE feature in (Ren et al., 2009),
repurposed for monolingual MWEs.
</bodyText>
<sectionHeader confidence="0.992055" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.99980752">
We evaluate the impact of MWEs in SMT on a large-
scale English-Arabic translation task.
Using two languages from different families is a
challenging testbed for MWEs in SMT. In contrast,
very closely related languages such as English and
French might present less divergence in lexicaliza-
tion.
In addition, Arabic-English is a well-studied lan-
guage pair in SMT, with large amounts of data avail-
able. However, we tackle the less common English
to Arabic direction in order to take advantage of the
rich lexical resources available for English on the in-
put side.
Our test set consists of the 813 newswire sen-
tences of the 2008 NIST Open Machine Transla-
tion Evaluation, which is standard evaluation data
for Arabic-English translation. The first English ref-
erence translation is used as the input to our SMT
system, and the single Arabic translation is used as
the unique reference2. Translation quality is eval-
uated using two automatic evaluation metrics: (1)
BLEUr1n4 (Papineni et al., 2002), which is based
on n-gram precisions for n = LA, and (2) Trans-
lation Edit Rate (TER) (Snover et al., 2006), which
generalizes edit distance beyond single-word edits.
</bodyText>
<subsectionHeader confidence="0.991313">
4.1 SMT system
</subsectionHeader>
<bodyText confidence="0.99980825">
We use the open-source Moses toolkit (Koehn et al.,
2007) to build a standard phrase-based SMT system.
Our training data consists of 2.5M sentence pairs
from mostly newswire parallel corpora distributed
by the Linguistic Data Consortium. The English
side is tokenized using simple punctuation-based
rules. The Arabic side is segmented according to the
Arabic Treebank v3 tokenization scheme using the
MADA+TOKAN morphological analyzer and tok-
enizer (Habash et al., 2009).
The parallel corpus is word-aligned using
GIZA++ in both translation directions, which are
</bodyText>
<footnote confidence="0.931126666666667">
2We exclude weblog text since it consists of an informal mix
of Modern Standard Arabic and Dialectal Arabic which is sub-
optimal as a reference translation.
</footnote>
<page confidence="0.997411">
243
</page>
<bodyText confidence="0.998795833333333">
combined by intersection and the grow-diag-final-
and heuristic (Koehn et al., 2007). Phrase transla-
tions of up to 10 words are extracted in the Moses
phrase-table. We use a 5-gram language model with
modified Kneser-Ney smoothing. Feature weights
are tuned on NIST-MT06.
</bodyText>
<subsectionHeader confidence="0.932117">
4.2 English MWE
</subsectionHeader>
<bodyText confidence="0.999941533333333">
Our main source of English MWE is the WordNet
3.0 lexical database (Fellbaum, 1998). We use sim-
ple rules to augment WordNet entries with morpho-
logical variations (e.g., keep one’s eyes peeled is ex-
panded into keep her eyes peeled, etc.). In addi-
tion when marking MWEs in text, we allow matches
not only with surface forms, but also with lemma-
tized forms (Schmid, 1994) to account for inflec-
tions. This results in a total of about 900 MWE to-
kens and 500 types in our evaluation test set. MWE
identification in running text is performed using a
straightforward maximum forward match algorithm.
Second, in order to contrast the impact of MWEs
with that of frequent collocations in our dynamic in-
tegration strategy, we consider the top 500 most fre-
quent n-grams from the SMT test set, so that the
same number of n-gram types and WordNet MWEs
are marked in the test set. Unlike WordNet MWEs,
these n-gram represent cohesive units, but are not
necessarily frozen or even a single concept. We con-
sider n-grams up to length 10 from the phrase-table,
and compute their frequency in the English side of
the parallel corpus. The top 500 most frequent n-
grams and the WordNet MWEs yield two very dif-
ferent lexicons. Only the following 10 entries ap-
pear in both: at the same time, deputy prime minis-
ter, for the first time, in the south, in the wake of, in-
ternational atomic energy agency, islamic resistance
movement, on the other hand, osama bin laden, sec-
retary of state.
</bodyText>
<sectionHeader confidence="0.995686" genericHeader="method">
5 Static MWE Integration Improves SMT
</sectionHeader>
<bodyText confidence="0.999782142857143">
As seen in Table 1, the static integration of the Word-
Net MWE lexicon by segmentation of English train-
ing and test sentences improves BLEU and TER
compared to the SMT baseline. This suggests that
WordNet MWEs represent useful units of meaning
for alignment and translation into Arabic despite the
fact that they are monolingually defined.
</bodyText>
<table confidence="0.9991934">
MWE integration TER BLEU
Baseline — 59.43 30.49
Top 500 n-grams dynamic 59.07 30.98
WordNet MWE dynamic 58.89 31.07
WordNet MWE static 58.98 31.27
</table>
<tableCaption confidence="0.985325">
Table 1: Impact of MWE integration measured on NIST
MT08
</tableCaption>
<bodyText confidence="0.999622625">
Consider, for instance, the following input sen-
tence: the special envoy of the secretary-general will
submit an oral report to the international security
council rather than a written report. With static in-
tegration, the MWE written report is correctly trans-
lated as tqryrA mktwbA, while the baseline produces
the incorrect translation ktb Altqryr (writing the re-
port or book of report).
</bodyText>
<sectionHeader confidence="0.991062" genericHeader="method">
6 Dynamic MWE Integration Improves
SMT
</sectionHeader>
<bodyText confidence="0.999835571428571">
Dynamic integration of the WordNet MWE lexicon
and the top 500 n-grams both improve BLEU and
TER (Table 1), but WordNet MWEs yield slightly
better scores. This confirms the ability of the dy-
namic integration method to handle compositional
MWEs, since the most frequent n-grams are highly
compositional by definition.
</bodyText>
<sectionHeader confidence="0.998342" genericHeader="evaluation">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999386611111111">
At the corpus-level, static integration yields a
slightly better BLEU score than dynamic with
WordNet MWEs, while the opposite effect is ob-
served on TER. This suggests that the two integra-
tion strategies impact translation in different ways.
Sentence-level scores indeed reveal that dynamic
and static integration strategies have an opposite im-
pact on 27% of the test set (Table 2).
For instance, the dynamic approach fails for
phrasal verbs such as take out. In who were then
allowed to take out as many unsecured loans as they
wanted, take out is realized as b+ AlHSwl (acquire)
with the static approach, while it is entirely dropped
from the dynamic translation.
In the static approach, translation quality is often
degraded when our simple dictionary matching ap-
proach incorrectly detects MWE. For instance, in the
sentence the perpetration of this heinous act on our
</bodyText>
<page confidence="0.995176">
244
</page>
<table confidence="0.99695775">
Dynamic integration helps hurts
Static integration
helps 45% 16%
hurts 11% 28%
</table>
<tableCaption confidence="0.979329666666667">
Table 2: Percentage of sentences where each integration
strategy helps or hurts both BLEU and TER compared to
the baseline SMT system.
</tableCaption>
<bodyText confidence="0.99892075">
soil, act on is incorrectly identified as a MWE which
degrades translation fluency. This suggests that fur-
ther gains in translation quality could be obtained
with a more sophisticated MWE detection method.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999948941176471">
We have proposed a framework of two comple-
mentary integration strategies for MWEs in SMT,
which allows extrinsic evaluation of the usefulness
of MWEs of varying degree of compositionality.
We conducted a pilot study using manually defined
WordNet MWE and a dictionary matching approach
to MWE detection. This simple model improves
English-Arabic translation quality, even on a large
SMT system trained on more than 2 Million sen-
tence pairs.
This result suggests that standard SMT phrases
do not implicitly capture all useful MWE informa-
tion. It would therefore be interesting to conduct
this study on a larger scale, using more general
MWE definitions such as automatically learned col-
locations (Smadja, 1993) or verb-noun constructions
(Diab and Bhutada, 2009).
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863136363636">
Ming-Hong Bai, Jia-Ming You, Keh-Jiann Chen, and Ja-
son S. Chang. 2009. Acquiring translation equiva-
lences of multiword expressions by normalized corre-
lation frequencies. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 478–486, Singapore, August.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction MWE token classification. In Proceedings of
the Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applications,
pages 17–22, Singapore, August.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceedings
of the 2nd International Conference on Arabic Lan-
guage Resources and Tools (MEDAR), Cairo, Egypt.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), demonstration session, Prague, Czech Re-
public, June.
Patrik Lambert and Rafael Banchs. 2005. Data inferred
multi-word expressions for statistical machine transla-
tion. In Machine Translation Summit X, pages 396–
403, Phuket, Thailand.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expressions.
In Proceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambiguation
and Applications, pages 47–54, Singapore, August.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceed-
ings of the Third International Conference on Com-
putational Linguistics and Intelligent Text Processing,
pages 1–15, London, UK. Springer-Verlag.
Helmut Schmid. 1994. Probabilistic part–of–speech
tagging using decision trees. In Proceedings of the
Conference on New Methods in Language Processing,
pages 44–49, Manchester, UK.
Frank A. Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143–
177.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of AMTA, pages 223–231, Boston, MA.
Association for Machine Translation in the Americas.
Aline Villavicencio, Francis Bond, Anna Korhonen, and
Diana McCarthy. 2005. Introduction to the special
issue on multiword expressions: Having a crack at a
hard nut. Computer Speech &amp; Language, 19(4):365 –
377. Special issue on Multiword Expression.
</reference>
<page confidence="0.998713">
245
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698857">
<title confidence="0.978372">Task-based Evaluation of Multiword a Pilot Study in Statistical Machine Translation</title>
<author confidence="0.986326">Marine Carpuat Mona</author>
<affiliation confidence="0.988823">Columbia Center for Computational Learning</affiliation>
<address confidence="0.999855">475 Riverside Drive, New York, NY</address>
<abstract confidence="0.998111625">We conduct a pilot study for task-oriented evaluation of Multiword Expression (MWE) in Statistical Machine Translation (SMT). We propose two different integration strategies for MWE in SMT, which take advantage of different degrees of MWE semantic compositionality and yield complementary improvements in</abstract>
<intro confidence="0.762483">quality on a large-scale translation</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ming-Hong Bai</author>
<author>Jia-Ming You</author>
<author>Keh-Jiann Chen</author>
<author>Jason S Chang</author>
</authors>
<title>Acquiring translation equivalences of multiword expressions by normalized correlation frequencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>478--486</pages>
<location>Singapore,</location>
<contexts>
<context position="2338" citStr="Bai et al., 2009" startWordPosition="345" endWordPosition="348">ct. units are typically phrasal translations, defined without any direct syntactic or lexical semantic motivation: they are simply n-grams that are consistently translated in parallel corpora. Phrasal translations might indirectly capture MWEs, but they are not distinguished from any other n-gram. As a result, the usefulness of explicitly modeling MWEs in the SMT framework has not yet been studied systematically. Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai et al., 2009) or domain specific MWEs (Ren et al., 2009). MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. In this paper, we consider a more general problem: we view SMT as an extrinsic evaluation of the usefulness of monolingual MWEs as used pervasively in natural language </context>
</contexts>
<marker>Bai, You, Chen, Chang, 2009</marker>
<rawString>Ming-Hong Bai, Jia-Ming You, Keh-Jiann Chen, and Jason S. Chang. 2009. Acquiring translation equivalences of multiword expressions by normalized correlation frequencies. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 478–486, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Pravin Bhutada</author>
</authors>
<title>Verb noun construction MWE token classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications,</booktitle>
<pages>17--22</pages>
<location>Singapore,</location>
<marker>Diab, Bhutada, 2009</marker>
<rawString>Mona Diab and Pravin Bhutada. 2009. Verb noun construction MWE token classification. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, pages 17–22, Singapore, August.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR),</booktitle>
<location>Cairo, Egypt.</location>
<contexts>
<context position="7504" citStr="Habash et al., 2009" startWordPosition="1183" endWordPosition="1186">sions for n = LA, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 4.1 SMT system We use the open-source Moses toolkit (Koehn et al., 2007) to build a standard phrase-based SMT system. Our training data consists of 2.5M sentence pairs from mostly newswire parallel corpora distributed by the Linguistic Data Consortium. The English side is tokenized using simple punctuation-based rules. The Arabic side is segmented according to the Arabic Treebank v3 tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer (Habash et al., 2009). The parallel corpus is word-aligned using GIZA++ in both translation directions, which are 2We exclude weblog text since it consists of an informal mix of Modern Standard Arabic and Dialectal Arabic which is suboptimal as a reference translation. 243 combined by intersection and the grow-diag-finaland heuristic (Koehn et al., 2007). Phrase translations of up to 10 words are extracted in the Moses phrase-table. We use a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned on NIST-MT06. 4.2 English MWE Our main source of English MWE is the WordNet 3.0 lexical dat</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR), Cairo, Egypt.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session,</booktitle>
<location>Alexandra</location>
<contexts>
<context position="7091" citStr="Koehn et al., 2007" startWordPosition="1123" endWordPosition="1126">es of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2. Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = LA, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 4.1 SMT system We use the open-source Moses toolkit (Koehn et al., 2007) to build a standard phrase-based SMT system. Our training data consists of 2.5M sentence pairs from mostly newswire parallel corpora distributed by the Linguistic Data Consortium. The English side is tokenized using simple punctuation-based rules. The Arabic side is segmented according to the Arabic Treebank v3 tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer (Habash et al., 2009). The parallel corpus is word-aligned using GIZA++ in both translation directions, which are 2We exclude weblog text since it consists of an informal mix of Modern Standard Arabic and Dia</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Rafael Banchs</author>
</authors>
<title>Data inferred multi-word expressions for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<pages>396--403</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="2576" citStr="Lambert and Banchs, 2005" startWordPosition="386" endWordPosition="389"> capture MWEs, but they are not distinguished from any other n-gram. As a result, the usefulness of explicitly modeling MWEs in the SMT framework has not yet been studied systematically. Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai et al., 2009) or domain specific MWEs (Ren et al., 2009). MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. In this paper, we consider a more general problem: we view SMT as an extrinsic evaluation of the usefulness of monolingual MWEs as used pervasively in natural language regardless of domain, idiomaticity and compositionality. A MWE is compositional if its meaning as a unit can be predicted from the meaning of its component words such as in make a decision meaning to decide. Some MWEs are more predictable</context>
</contexts>
<marker>Lambert, Banchs, 2005</marker>
<rawString>Patrik Lambert and Rafael Banchs. 2005. Data inferred multi-word expressions for statistical machine translation. In Machine Translation Summit X, pages 396– 403, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6852" citStr="Papineni et al., 2002" startWordPosition="1082" endWordPosition="1085">arge amounts of data available. However, we tackle the less common English to Arabic direction in order to take advantage of the rich lexical resources available for English on the input side. Our test set consists of the 813 newswire sentences of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2. Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = LA, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 4.1 SMT system We use the open-source Moses toolkit (Koehn et al., 2007) to build a standard phrase-based SMT system. Our training data consists of 2.5M sentence pairs from mostly newswire parallel corpora distributed by the Linguistic Data Consortium. The English side is tokenized using simple punctuation-based rules. The Arabic side is segmented according to the Arabic Treebank v3 tokenization scheme using the MADA+TOKAN morpho</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhixiang Ren</author>
<author>Yajuan L¨u</author>
<author>Jie Cao</author>
<author>Qun Liu</author>
<author>Yun Huang</author>
</authors>
<title>Improving statistical machine translation using domain bilingual multiword expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications,</booktitle>
<pages>47--54</pages>
<location>Singapore,</location>
<marker>Ren, L¨u, Cao, Liu, Huang, 2009</marker>
<rawString>Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, pages 47–54, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann A Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="1166" citStr="Sag et al., 2002" startWordPosition="165" endWordPosition="168">ntary improvements in SMT quality on a large-scale translation task.1 1 Introduction A multiword expression (MWE) generally refers to a multiword unit or a collocation of words that cooccur together statistically more than chance. A MWE is a cover term for different types of collocations which vary in their transparency and fixedness. Identifying MWEs and understanding their meaning is considered essential to language understanding, and of crucial importance for any Natural Language Processing (NLP) applications that aim at handling robust language meaning and use. In fact, the seminal paper (Sag et al., 2002) refers to this problem as a key issue for the development of high-quality NLP applications. (Villavicencio et al., 2005) identify Machine Translation as an application of particular interest since “ recognition of MWEs is necessary for systems to preserve the meaning and produce appropriate translations and avoid the generation of unnatural or nonsensical sentences in the target language.” However, statistical machine translation (SMT) typically does not model MWEs explicitly. SMT 1The research was partially funded by IBM under the DARPA GALE project. units are typically phrasal translations,</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, pages 1–15, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part–of–speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="8411" citStr="Schmid, 1994" startWordPosition="1335" endWordPosition="1336">g-finaland heuristic (Koehn et al., 2007). Phrase translations of up to 10 words are extracted in the Moses phrase-table. We use a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned on NIST-MT06. 4.2 English MWE Our main source of English MWE is the WordNet 3.0 lexical database (Fellbaum, 1998). We use simple rules to augment WordNet entries with morphological variations (e.g., keep one’s eyes peeled is expanded into keep her eyes peeled, etc.). In addition when marking MWEs in text, we allow matches not only with surface forms, but also with lemmatized forms (Schmid, 1994) to account for inflections. This results in a total of about 900 MWE tokens and 500 types in our evaluation test set. MWE identification in running text is performed using a straightforward maximum forward match algorithm. Second, in order to contrast the impact of MWEs with that of frequent collocations in our dynamic integration strategy, we consider the top 500 most frequent n-grams from the SMT test set, so that the same number of n-gram types and WordNet MWEs are marked in the test set. Unlike WordNet MWEs, these n-gram represent cohesive units, but are not necessarily frozen or even a s</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part–of–speech tagging using decision trees. In Proceedings of the Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank A Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<journal>Xtract. Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>177</pages>
<marker>Smadja, 1993</marker>
<rawString>Frank A. Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143– 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>223--231</pages>
<publisher>Association</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="6959" citStr="Snover et al., 2006" startWordPosition="1103" endWordPosition="1106">ke advantage of the rich lexical resources available for English on the input side. Our test set consists of the 813 newswire sentences of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2. Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = LA, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 4.1 SMT system We use the open-source Moses toolkit (Koehn et al., 2007) to build a standard phrase-based SMT system. Our training data consists of 2.5M sentence pairs from mostly newswire parallel corpora distributed by the Linguistic Data Consortium. The English side is tokenized using simple punctuation-based rules. The Arabic side is segmented according to the Arabic Treebank v3 tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer (Habash et al., 2009). The parallel corpus is word-aligned using GIZA++ in b</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA, pages 223–231, Boston, MA. Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
<author>Francis Bond</author>
<author>Anna Korhonen</author>
<author>Diana McCarthy</author>
</authors>
<title>Introduction to the special issue on multiword expressions: Having a crack at a hard nut.</title>
<date>2005</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>19</volume>
<issue>4</issue>
<note>Special issue on Multiword Expression.</note>
<contexts>
<context position="1287" citStr="Villavicencio et al., 2005" startWordPosition="184" endWordPosition="187">E) generally refers to a multiword unit or a collocation of words that cooccur together statistically more than chance. A MWE is a cover term for different types of collocations which vary in their transparency and fixedness. Identifying MWEs and understanding their meaning is considered essential to language understanding, and of crucial importance for any Natural Language Processing (NLP) applications that aim at handling robust language meaning and use. In fact, the seminal paper (Sag et al., 2002) refers to this problem as a key issue for the development of high-quality NLP applications. (Villavicencio et al., 2005) identify Machine Translation as an application of particular interest since “ recognition of MWEs is necessary for systems to preserve the meaning and produce appropriate translations and avoid the generation of unnatural or nonsensical sentences in the target language.” However, statistical machine translation (SMT) typically does not model MWEs explicitly. SMT 1The research was partially funded by IBM under the DARPA GALE project. units are typically phrasal translations, defined without any direct syntactic or lexical semantic motivation: they are simply n-grams that are consistently trans</context>
</contexts>
<marker>Villavicencio, Bond, Korhonen, McCarthy, 2005</marker>
<rawString>Aline Villavicencio, Francis Bond, Anna Korhonen, and Diana McCarthy. 2005. Introduction to the special issue on multiword expressions: Having a crack at a hard nut. Computer Speech &amp; Language, 19(4):365 – 377. Special issue on Multiword Expression.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>