<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006815">
<title confidence="0.993717">
Efficient Search for Transformation-based Inference
</title>
<author confidence="0.952588">
Asher Stern§, Roni Sternt, Ido Dagan§, Ariel Felnert
</author>
<affiliation confidence="0.8388975">
§ Computer Science Department, Bar-Ilan University
$ Information Systems Engineering, Ben Gurion University
</affiliation>
<email confidence="0.96795725">
astern7@gmail.com
roni.stern@gmail.com
dagan@cs.biu.ac.il
felner@bgu.ac.il
</email>
<sectionHeader confidence="0.995484" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999608833333333">
This paper addresses the search problem in
textual inference, where systems need to infer
one piece of text from another. A prominent
approach to this task is attempts to transform
one text into the other through a sequence
of inference-preserving transformations, a.k.a.
a proof, while estimating the proof’s valid-
ity. This raises a search challenge of find-
ing the best possible proof. We explore this
challenge through a comprehensive investi-
gation of prominent search algorithms and
propose two novel algorithmic components
specifically designed for textual inference: a
gradient-style evaluation function, and a local-
lookahead node expansion method. Evalua-
tions, using the open-source system, BIUTEE,
show the contribution of these ideas to search
efficiency and proof quality.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937595744681">
In many NLP settings it is necessary to identify
that a certain semantic inference relation holds be-
tween two pieces of text. For example, in para-
phrase recognition it is necessary to identify that the
meanings of two text fragments are roughly equiva-
lent. In passage retrieval for question answering, it
is needed to detect text passages from which a sat-
isfying answer can be inferred. A generic formula-
tion for the inference relation between two texts is
given by the Recognizing Textual Entailment (RTE)
paradigm (Dagan et al., 2005), which is adapted here
for our investigation. In this setting, a system is
given two text fragments, termed “text” (T) and “hy-
pothesis” (H), and has to recognize whether the hy-
pothesis is entailed by (inferred from) the text.
An appealing approach to such textual inferences
is to explicitly transform T into H, using a sequence
of transformations (Bar-Haim et al., 2007; Harmel-
ing, 2009; Mehdad, 2009; Wang and Manning,
2010; Heilman and Smith, 2010; Stern and Dagan,
2011). Examples of such possible transformations
are lexical substitutions (e.g. “letter” —* “message”)
and predicate-template substitutions (e.g. “X [verb-
active] Y” —* “Y [verb-passive] by X”), which are
based on available knowledge resources. Another
example is coreference substitutions, such as replac-
ing “he” with “the employee” if a coreference re-
solver has detected that these two expressions core-
fer. Table 1 exemplifies this approach for a particu-
lar T-H pair. The rationale behind this approach is
that each transformation step should preserve infer-
ence validity, such that each text generated along this
process is indeed inferred from the preceding one.
An inherent aspect in transformation-based infer-
ence is modeling the certainty that each inference
step is valid. This is usually achieved by a cost-
based or probabilistic model, which quantifies con-
fidence in the validity of each individual transfor-
mation and consequently of the complete chain of
inference.
Given a set of possible transformations, there may
be many transformation sequences that would trans-
form T to H. This creates a very large search space,
where systems have to find the “best” transformation
sequence – the one of lowest cost, or of highest prob-
ability. To the best of our knowledge, this search
challenge has not been investigated yet in a substan-
</bodyText>
<page confidence="0.982367">
283
</page>
<note confidence="0.951309777777778">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 283–291,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
# Operation Generated text
0 - He received the letter from the secretary.
1 Coreference substitution The employee received the letter from the secretary.
2 X received Y from Z → Y was sent to X by Z The letter was sent to the employee by the secretary.
3 Y [verb-passive] by X → X [verb-active] Y The secretary sent the letter to the employee.
4 X send Y → X deliver Y The secretary delivered the letter to the employee.
5 letter → message The secretary delivered the message to the employee.
</note>
<tableCaption confidence="0.990038666666667">
Table 1: A sequence of transformations that transform the text “He received the letter from the secretary.” into the
hypothesis “The secretary delivered the message to the employee.”. The knowledge required for such transformations
is often obtained from available knowledge resources and NLP tools.
</tableCaption>
<bodyText confidence="0.9998705">
tial manner: each of the above-cited works described
the search method they used, but none of them tried
alternative methods while evaluating search perfor-
mance. Furthermore, while experimenting with our
own open-source inference system, BIUTEE1, we
observed that search efficiency is a major issue, of-
ten yielding practically unsatisfactory run-times.
This paper investigates the search problem in
transformation-based textual inference, naturally
falling within the framework of heuristic AI (Ar-
tificial Intelligence) search. To facilitate such in-
vestigation, we formulate a generic search scheme
which incorporates many search variants as special
cases and enable a meaningful comparison between
the algorithms. Under this framework, we identify
special characteristics of the textual inference search
space, that lead to the development of two novel al-
gorithmic components: a special lookahead method
for node expansion, named local lookahead, and a
gradient-based evaluation function. Together, they
yield a new search algorithm, which achieved sub-
stantially superior search performance in our evalu-
ations.
The remainder of this paper is organized as
follows. Section 2 provides an overview of
transformation-based inference systems, AI search
algorithms, and search methods realized in prior in-
ference systems. Section 3 formulates the generic
search scheme that we have investigated, which cov-
ers a broad range of known algorithms, and presents
our own algorithmic contributions. These new algo-
rithmic contributions were implemented in our sys-
tem, BIUTEE. In Section 4 we evaluate them empir-
ically, and show that they improve search efficiency
as well as solution’s quality. Search performance is
evaluated on two recent RTE benchmarks, in terms
</bodyText>
<footnote confidence="0.86721">
1www.cs.biu.ac.il/˜nlp/downloads/biutee
</footnote>
<bodyText confidence="0.9847855">
of runtime, ability to find lower-cost transformation
chains and impact on overall inference.
</bodyText>
<sectionHeader confidence="0.982053" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999929583333333">
Applying sequences of transformations to recognize
textual inference was suggested by several works.
Such a sequence may be referred to as a proof, in
the sense that it is used to “prove” the hypothesis
from the text. Although various works along this
line differ from each other in several respects, many
of them share the common challenge of finding an
optimal proof. The following paragraphs review the
major research approaches in this direction. We fo-
cus on methods that perform transformations over
parse trees, and highlight the search challenge with
which they are faced.
</bodyText>
<subsectionHeader confidence="0.99588">
2.1 Transformation-based textual inference
</subsectionHeader>
<bodyText confidence="0.999969105263158">
Several researchers suggested using various types
of transformations in order to derive H from T.
Some suggested a set of predefined transforma-
tions, for example, insertion, deletion and substitu-
tion of parse-tree nodes, by which any tree can be
transformed to any other tree. These transforma-
tions were used by the open-source system EDITS
(Mehdad, 2009), and by (Wang and Manning, 2010).
Since the above mentioned transformations are lim-
ited in capturing certain interesting and prevalent
semantic phenomena, an extended set of tree edit
operations (e.g., relabel-edge, move-sibling, etc.)
was proposed by Heilman and Smith (2010). Simi-
larly, Harmeling (2009) suggested a heuristic set of
28 transformations, which include various types of
node-substitutions as well as restructuring of the en-
tire parse-tree.
In contrast to such predefined sets of transfor-
mations, knowledge oriented approaches were sug-
</bodyText>
<page confidence="0.995938">
284
</page>
<bodyText confidence="0.99992826">
gested by Bar-Haim et al. (2007) and de Salvo Braz
et al. (2005). Their transformations are defined by
knowledge resources that contain a large amount of
entailment rules, or rewrite rules, which are pairs of
parse-tree fragments that entail one another. Typical
examples for knowledge resources of such rules are
DIRT (Lin and Pantel, 2001), and TEASE (Szpek-
tor et al., 2004), as well as syntactic transforma-
tions constructed manually. In addition, they used
knowledge-based lexical substitutions.
However, when only knowledge-based transfor-
mations are allowed, transforming the text into the
hypothesis is impossible in many cases. This limi-
tation is dealt by our open-source integrated frame-
work, BIUTEE (Stern and Dagan, 2011), which
incorporates knowledge-based transformations (en-
tailment rules) with a set of predefined tree-edits.
Motivated by the richer structure and search space
provided by BIUTEE, we adopted it for our empiri-
cal investigations.
The semantic validity of transformation-based in-
ference is usually modeled by defining a cost or
a probability estimation for each transformation.
Costs may be defined manually (Kouylekov and
Magnini, 2005), but are usually learned automati-
cally (Harmeling, 2009; Mehdad, 2009; Wang and
Manning, 2010; Heilman and Smith, 2010; Stern
and Dagan, 2011). A global cost (or probability esti-
mation) for a complete sequence of transformations
is typically defined as the sum of the costs of the
involved transformations.
Finding the lowest cost proof, as needed for de-
termining inference validity, is the focus of our re-
search. Textual inference systems limited to the
standard tree-edit operations (insertion, deletion,
substitution) can use an exact algorithm that finds
the optimal solution in polynomial time under cer-
tain constraints (Bille, 2005). Nevertheless, for the
extended set of transformations it is unlikely that ef-
ficient exact algorithms for finding lowest-cost se-
quences are available (Heilman and Smith, 2010).
In this harder case, the problem can be viewed
as an AI search problem. Each state in the search
space is a parse-tree, where the initial state is the text
parse-tree, the goal state is the hypothesis parse-tree,
and we search for the shortest (in terms of costs)
path of transformations from the initial state to the
goal state. Next we briefly review major concepts
from the field of AI search and summarize some rel-
evant proposed solutions.
</bodyText>
<subsectionHeader confidence="0.999601">
2.2 Search Algorithms
</subsectionHeader>
<bodyText confidence="0.996188052631579">
Search algorithms find a path from an initial state to
a goal state by expanding and generating states in
a search space. The term generating a state refers
to creating a data structure that represents it, while
expanding a state means generating all its immedi-
ate derivations. In our domain, each state is a parse
tree, which is expanded by performing all applicable
transformations.
Best-first search is a common search framework.
It maintains an open list (denoted hereafter as
OPEN) containing all the generated states that have
not been expanded yet. States in OPEN are prior-
itized by an evaluation function, f(s). A best-first
search algorithm iteratively removes the best state
(according to f(s)) from OPEN, and inserts new
states being generated by expanding this best state.
The evaluation function is usually a linear combina-
tion of the shortest path found from the start state to
state s, denoted by g(s), and a heuristic function, de-
noted by h(s), which estimates the cost of reaching
a goal state from s.
Many search algorithms can be viewed as spe-
cial cases or variations of best-first search. The
well-known A* (Hart et al., 1968). algorithm is
a best-first search that uses an evaluation function
f(s) = g(s) + h(s). Weighted A* (Pohl, 1970)
uses an evaluation function f(s) = w · g(s) + h(s),
where w is a parameter, while pure heuristic search
uses f(s) = h(s). K-BFS (Felner et al., 2003) ex-
pands k states in each iteration. Beam search (Furcy
and Koenig, 2005; Zhou and Hansen, 2005) limits
the number of states stored in OPEN, while Greedy
search limits OPEN to contain only the single best
state generated in the current iteration.
The search algorithm has crucial impact on the
quality of proof found by a textual inference system,
as well as on its efficiency. Next, we describe search
strategies used in prior works for textual inference.
</bodyText>
<subsectionHeader confidence="0.998051">
2.3 Search in prior inference models
</subsectionHeader>
<bodyText confidence="0.99985375">
In spite of being a fundamental problem, prior so-
lutions to the search challenge in textual inference
were mostly ad-hoc. Furthermore, there was no in-
vestigation of alternative search methods, and no
</bodyText>
<page confidence="0.987816">
285
</page>
<bodyText confidence="0.999977269230769">
evaluation of search efficiency and quality was re-
ported. For example, in (Harmeling, 2009) the order
by which the transformations are performed is pre-
determined, and in addition many possible deriva-
tions are discarded, to prevent exponential explo-
sion. Handling the search problem in (Heilman and
Smith, 2010) was by a variant of greedy search,
driven by a similarity measure between the current
parse-tree and the hypothesis, while ignoring the
cost already paid. In addition, several constraints on
the search space were implemented. In the earlier
version of BIUTEE (Stern and Dagan, 2011)2, a ver-
sion of beam search was incorporated, named here-
after BIUTEE-orig. This algorithm uses the evalua-
tion function f(s) = g(s) + wi · h(s), where in each
iteration (i) the value of w is increased, to ensure
successful termination of the search. Nevertheless,
its efficiency and quality were not investigated.
In this paper we consider several prominent
search algorithms and evaluate their quality. The
evaluation concentrates on two measures: the run-
time required to find a proof, and proof quality (mea-
sured by its cost). In addition to evaluating standard
search algorithms we propose two novel compo-
nents specifically designed for proof-based textual-
inference and evaluate their contribution.
</bodyText>
<sectionHeader confidence="0.996793" genericHeader="method">
3 Search for Textual Inference
</sectionHeader>
<bodyText confidence="0.999984714285714">
In this section we formalize our search problem and
specify a unifying search scheme by which we test
several search algorithms in a systematic manner.
Then we propose two novel algorithmic components
specifically designed for our problem. We conclude
by presenting our new search algorithm which com-
bines these two ideas.
</bodyText>
<subsectionHeader confidence="0.995612">
3.1 Inference and search space formalization
</subsectionHeader>
<bodyText confidence="0.996546125">
Let t be a parse tree, and let o be a transforma-
tion. Applying o on t, yielding t&apos;, is denoted by
t Xo t&apos;. If the underlying meaning of t&apos; can in-
deed be inferred from the underlying meaning of t,
then we refer to the application of o as valid. Let
O = (o1, o2,... on) be a sequence of transforma-
tions, such that t0 Xo1 t1 Xo2 t2 ... Xon tn. We
write t0 XO tn, and say that tn can be proven from
</bodyText>
<footnote confidence="0.9892605">
2More details in www.cs.biu.ac.il/˜nlp/
downloads/biutee/search_ranlp_2011.pdf
</footnote>
<bodyText confidence="0.99787596">
t0 by applying the sequence O. The proof might be
valid, if all the transformations involved are valid, or
invalid otherwise.
An inference system specifies a cost, C(o), for
each transformation o. In most systems the costs
are automatically learned. The interpretation of a
high cost is that it is unlikely that applying o will be
valid. The cost of a sequence O = (o1, o2,... on)
is defined as Eni=1 C(o) (or ,in some systems,
�n i=1 C(o)). Denoting by tT and tH the text parse
tree and the hypothesis parse tree, a proof system
has to find a sequence O with minimal cost such that
tT XO tH. This forms a search problem of finding
the lowest-cost proof among all possible proofs.
The search space is defined as follows. A state
s is a parse-tree. The start state is tT and the goal
state is tH. In some systems any state s in which tH
is embedded is considered as goal as well.
Given a state s, let {o(1), o(2) ... o(m)I be m
transformations that can be applied on it. Expand-
ing s means generating m new states, s(j), j =
1... m, such that s Xo(;) s(j). The number m is
called branching factor. Our empirical observations
on BIUTEE showed that its branching factor ranges
from 2-3 for some states to about 30 for other states.
</bodyText>
<subsectionHeader confidence="0.999848">
3.2 Search Scheme
</subsectionHeader>
<bodyText confidence="0.9999345">
Our empirical investigation compares a range
prominent search algorithms, described in Section 2.
To facilitate such investigation, we formulate them
in the following unifying scheme (Algorithm 1).
</bodyText>
<table confidence="0.881421833333333">
Algorithm 1 Unified Search Scheme
Parameters: f(·): state evaluation function
expand(·): state generation function
Input: kexpand: # states expanded in each iteration
kmaintain: # states in OPEN in each iteration
sinit: initial state
</table>
<listItem confidence="0.964737142857143">
1: OPEN +— {sinit}
2: repeat
3: BEST +— kexpand best (according to f) states in OPEN
4: GENERATED &apos;-- UsEBEST expand(s)
5: OPEN (OPEN \ Best) U GENERATED
6: OPEN kmaintain best (according to f) states in OPEN
7: until BEST contains the goal state
</listItem>
<bodyText confidence="0.99985">
Initially, the open list, OPEN contains the initial
state. Then, the best kexpand states from OPEN are
chosen, according to the evaluation function f(s)
</bodyText>
<page confidence="0.991515">
286
</page>
<table confidence="0.9970132">
Algorithm f() expand() kmaintain kexpand
regular ∞ 1
regular ∞ 1
regular ∞ k &gt; 1
regular ∞ 1
regular 1 1
regular k &gt; 1 k &gt; 1
regular k &gt; 1 k &gt; 1
local- 1 1
lookahead
</table>
<tableCaption confidence="0.997666">
Table 2: Search algorithm mapped to the unified search
scheme. “Regular” means generating all the states which
can be generated by applying a single transformation. Al-
ternative greedy implementations use f = h.
</tableCaption>
<bodyText confidence="0.99986275">
(line 3), and expanded using the expansion func-
tion expand(s). In classical search algorithms,
expand(s) means generating a set of states by ap-
plying all the possible state transition operators to s.
Next, we remove from OPEN the states which were
expanded, and add the newly generated states. Fi-
nally, we keep in OPEN only the best kmaintain states,
according to the evaluation function f(s) (line 6).
This process repeats until the goal state is found in
BEST (line 7). Table 2 specifies how known search
algorithms, described in Section 2, fit into the uni-
fied search scheme.
Since runtime efficiency is crucial in our domain,
we focused on improving one of the simple but fast
algorithms, namely, greedy search. To improve the
quality of the proof found by greedy search, we in-
troduce new algorithmic components for the expan-
sion and evaluation functions, as described in the
next two subsections, while maintaining efficiency
by keeping kmaintain=kexpand= 1
</bodyText>
<subsectionHeader confidence="0.995355">
3.3 Evaluation function
</subsectionHeader>
<bodyText confidence="0.996622545454546">
In most domains, the heuristic function h(s) esti-
mates the cost of the minimal-cost path from a cur-
rent state, s, to a goal state. Having such a function,
the value g(s) + h(s) estimates the expected total
cost of a search path containing s. In our domain, it
is yet unclear how to calculate such a heuristic func-
tion. Given a state s, systems typically estimate the
difference (the gap) between s and the hypothesis
tH (the goal state). In BIUTEE this is quantified by
the number of parse-tree nodes and edges of tH that
do not exist in s. However, this does not give an
estimation for the expected cost of the path (the se-
quence of transformations) from s to the goal state.
This is because the number of nodes and edges that
can be changed by a single transformation can vary
from a single node to several nodes (e.g., by a lexi-
cal syntactic entailment rule). Moreover, even if two
transformations change the same number of nodes
and edges, their costs might be significantly differ-
ent. Consequently, the measurement of the cost ac-
cumulated so far (g(s)) and the remaining gap to tH
(h(s)) are unrelated. We note that a more sophisti-
cated heuristic function was suggested by Heilman
and Smith (2010), based on tree-kernels. Neverthe-
less, this heuristic function, serving as h(s), is still
unrelated to the transformation costs (g(s)).
We therefore propose a novel gradient-style func-
tion to overcome this difficulty. Our function is
designed for a greedy search in which OPEN al-
ways contains a single state, s. Let si be a state
generated from s, the cost of deriving si from s
is Ag(si) = g(si) − g(s). Similarly, the reduc-
tion in the value of the heuristic function is de-
</bodyText>
<equation confidence="0.8678765">
fined Ah(si) = h(s) − h(si). Now, we define
fΔ(si) = Δg(�j)
</equation>
<bodyText confidence="0.956980375">
Δh(�j). Informally, this function mea-
sures how costly it is to derive si relative to the
obtained decrease in the remaining gap to the goal
state. For the edge case in which h(s) − h(si) G 0,
we define fΔ(si) = oo. Empirically, we show in our
experiments that the function fΔ(s) performs better
than the traditional functions f(s) = g(s) + h(s)
and f,,,(s) = g(s) + w · h(s) in our domain.
</bodyText>
<subsectionHeader confidence="0.82203">
3.4 Node expansion method
</subsectionHeader>
<bodyText confidence="0.999735857142857">
When examining the proofs produced by the above
mentioned algorithms, we observed that in many
cases a human could construct proofs that exhibit
some internal structure, but were not revealed by the
algorithms. Observe, for example, the proof in Ta-
ble 1. It can be seen that transformations 2,3 and
4 strongly depend on each other. Applying trans-
formation 3 requires first applying transformation 2,
and similarly 4 could not be applied unless 2 and 3
are first applied. Moreover, there is no gain in apply-
ing transformations 2 and 3, unless transformation 4
is applied as well. On the other hand, transformation
1 does not depend on any other transformation. It
may be performed at any point along the proof, and
</bodyText>
<figure confidence="0.9992421">
A* g + h
Weighted A* g +w ·h
K-Weighted A* g +w ·h
Pure Heuristic h
Greedy g+w ·h
Beam g + h
BIUTEE-orig g+wi·h
09
LLGS
Ah
</figure>
<page confidence="0.989305">
287
</page>
<bodyText confidence="0.990032033898305">
moreover, changing all other transformations would
not affect it.
Carefully examining many examples, we general-
ized this phenomenon as follows. Often, a sequence
of transformations can be decomposed into a set of
coherent subsequences of transformations, where in
each subsequence the transformations strongly de-
pend on each other, while different subsequences are
independent. This phenomenon can be utilized in
the following way: instead of searching for a com-
plete sequence of transformations that transform tT
into tH, we can iteratively search for independent co-
herent subsequences of transformations, such that a
combination of these subsequences will transform
tT into tH. This is somewhat similar to the tech-
nique of applying macro operators, which is used in
automated planning (Botea et al., 2005) and puzzle
solving (Korf, 1985).
One technique for finding such subsequences is
to perform, for each state being expanded, a brute-
force depth-limited search, also known as looka-
head (Russell and Norvig, 2010; Bulitko and Lus-
trek, 2006; Korf, 1990; Stern et al., 2010). How-
ever, performing such lookahead might be slow if
the branching factor is large. Fortunately, in our
domain, coherent subsequences have the following
characteristic which can be leveraged: typically, a
transformation depends on a previous one only if
it is performed over some nodes which were af-
fected by the previous transformation. Accordingly,
our proposed algorithm searches for coherent subse-
quences, in which each subsequent transformation
must be applied to nodes that were affected by the
previous transformation.
Formally, let o be a transformation that has been
applied on a tree t, yielding t&apos;. Qaffected(o, t&apos;) denotes
the subset of nodes in t&apos; which were affected (modi-
fied or created) by the application of o.
Next, for a transformation o, applied on a parse
tree t, we define Qrequired(t, o) as the subset of t’s
nodes required for applying o (i.e., in the absence of
these nodes, o could not be applied).
Finally, let t be a parse-tree and Q be a subset of
its nodes. enabled ops(t, Q) is a function that re-
turns the set of the transformations that can be ap-
plied on t, which require at least one of the nodes
in Q. Formally, enabled ops(t, Q) - {o E 0 :
Q n Qrequired(t, o) =� 01, where 0 is the set of trans-
formations that can be applied on t. In our algo-
rithm, Q is the set of nodes that were affected by the
preceding transformation of the constructed subse-
quence.
The recursive procedure described in Algorithm 2
generates all coherent subsequences of lengths up to
d. It should be initially invoked with t - the current
state (parse tree) being expanded, Q - the set of all its
nodes, d - the maximal required length, and 0 as an
empty initial sequence. We use O·o as concatenation
of an operation o to a subsequence O.
</bodyText>
<equation confidence="0.549398333333333">
Algorithm 2 local-lookahead (t,Q,d,O)
1: if d = 0 then
2: return 0 (empty-set)
</equation>
<listItem confidence="0.97287175">
3: end if
4: SUBSEQUENCES +— 0
5: for all o E enabled ops(t, u) do
6: Lett �-a t&apos;
7: Add {O·o}Ulocal-lookahead(t&apos;, uaffected(o, t&apos;), d−1, O·
o) to SUBSEQUENCES
8: end for
9: return SUBSEQUENCES
</listItem>
<bodyText confidence="0.999734833333333">
The loop in lines 5 - 8 iterates over transforma-
tions that can be applied on the input tree, t, requir-
ing the same nodes that were affected by the pre-
vious transformation of the subsequence being con-
structed. Note that in the first call enabled ops(t, Q)
contain all operations that can be applied on t, with
no restriction. Applying an operation o results in a
new subsequence O · o. This subsequence will be
part of the set of subsequences found by the proce-
dure. In addition, it will be used in the next recur-
sive call as the prefix of additional (longer) subse-
quences.
</bodyText>
<subsectionHeader confidence="0.997125">
3.5 Local-lookahead gradient search
</subsectionHeader>
<bodyText confidence="0.990994125">
We are now ready to define our new algorithm
LOCAL-LOOKAHEAD GRADIENT SEARCH
(LLGS). In LLGS, like in greedy search,
kmaintain=kexpand= L expand(s) is defined to
return all states generated by subsequences found
by the local-lookahead procedure, while the evalua-
tion function is defined as f = fA (see last row of
Table 2).
</bodyText>
<sectionHeader confidence="0.997353" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9681245">
In this section we first evaluate the search perfor-
mance in terms of efficiency (run time), the quality
</bodyText>
<page confidence="0.993759">
288
</page>
<bodyText confidence="0.9999895">
of the found proofs (as measured by proof cost), and
overall inference performance achieved through var-
ious search algorithms. Finally we analyze the con-
tribution of our two novel components.
</bodyText>
<subsectionHeader confidence="0.998393">
4.1 Evaluation settings
</subsectionHeader>
<bodyText confidence="0.999964974358974">
We performed our experiments on the last two
published RTE datasets: RTE-5 (2009) and RTE-
6 (2010). The RTE-5 dataset is composed of a
training and test corpora, each containing 600 text-
hypothesis pairs, where in half of them the text en-
tails the hypothesis and in the other half it does
not. In RTE-6, each of the training and test cor-
pora consists of 10 topics, where each topic con-
tains 10 documents. Each corpus contains a set of
hypotheses (211 in the training dataset, and 243 in
the test dataset), along with a set of candidate en-
tailing sentences for each hypothesis. The system
has to find for each hypothesis which candidate sen-
tences entail it. To improve speed and results, we
used the filtering mechanism suggested by (Mirkin
et al., 2009), which filters the candidate sentences
by the Lucene IR engine3. Thus, only top 20 candi-
dates per hypothesis were tested
Evaluation of each of the algorithms was
performed by running BIUTEE while replacing
BIUTEE-orig with this algorithm. We employed a
comprehensive set of knowledge resources (avail-
able in BIUTEE’s web site): WordNet (Fellbaum,
1998), Directional similarity (Kotlerman et al.,
2010), DIRT (Lin and Pantel, 2001) and generic syn-
tactic rules. In addition, we used coreference substi-
tutions, detected by ArkRef4.
We evaluated several known algorithms, de-
scribed in Table 2 above, as well as BIUTEE-orig.
The latter is a strong baseline, which outperforms
known search algorithms in generating low cost
proofs. We compared all the above mentioned al-
gorithms to our novel one, LLGS.
We used the training dataset for parameter tun-
ing, which controls the trade-off between speed and
quality. For weighted A*, as well as for greedy
search, we used w = 6.0, since, for a few instances,
lower values of w resulted in prohibitive runtime.
For beam search we used k = 150, since higher val-
</bodyText>
<footnote confidence="0.984570333333333">
3http://lucene.apache.org
4www.ark.cs.cmu.edu/ARKref/ See (Haghighi and
Klein, 2009)
</footnote>
<bodyText confidence="0.9990075625">
ues of k did not improve the proof cost on the train-
ing dataset. The value of d in LLGS was set to 3.
d = 4 yielded the same proof costs, but was about 3
times slower.
Since lower values of w could be used by
weighted A* for most instances, we also ran ex-
periments where we varied the value of w accord-
ing to the dovetailing method suggested in (Valen-
zano et al., 2010) (denoted dovetailing WA*) as fol-
lows. When weighted A* has found a solution, we
reran it with a new value of w, set to half of the
previous value. The idea is to guide the search for
lower cost solutions. This process was halted when
the total number of states generated by all weighted
A* instances exceeded a predefined constant (set to
10, 000).
</bodyText>
<subsectionHeader confidence="0.999282">
4.2 Search performance
</subsectionHeader>
<bodyText confidence="0.999983904761905">
This experiment evaluates the search algorithms in
both efficiency (run-time) and proof quality. Effi-
ciency is measured by the average CPU (Intel Xeon
2.5 GHz) run-time (in seconds) for finding a com-
plete proof for a text-hypothesis instance, and by the
average number of generated states along the search.
Proof quality is measured by its cost.
The comparison of costs requires that all experi-
ments are performed on the same model which was
learned during training. Thus, in the training phase
we used the original search of BIUTEE, and then ran
the test phase with each algorithm separately. The
results, presented in Table 3, show that our novel
algorithm, LLGS, outperforms all other algorithms
in finding lower cost proofs. The second best is
BIUTEE-orig which is much slower by a factor of
3 (on RTE-5) to 8 (on RTE-6)5. While inherently
fast algorithms, particularly greedy and pure heuris-
tic, achieve faster running times, they achieve lower
proof quality, as well as lower overall inference per-
formance (see next subsection).
</bodyText>
<subsectionHeader confidence="0.993966">
4.3 Overall inference performance
</subsectionHeader>
<bodyText confidence="0.99960975">
In this experiment we test whether, and how much,
finding better proofs, by a better search algorithm,
improves overall success rate of the RTE system.
Table 4 summarizes the results (accuracy in RTE-5
</bodyText>
<footnote confidence="0.669166">
5Calculating T-test, we found that runtime improvement is
statistically significant with p &lt; 0.01, and p &lt; 0.052 for cost
improvement over BIUTEE-orig.
</footnote>
<page confidence="0.989349">
289
</page>
<table confidence="0.998752">
Algorithm Avg. time Avg. Avg. cost
generated
Weighted A* 0.22 / 0.09 301 / 143 1.11 / 10.52
Dovetailing 7.85 / 8.53 9797 / 9979 1.05 / 10.28
WA*
Greedy 0.20 / 0.10 468 / 158 1.10 / 10.55
Pure heuristic 0.09 / 0.10 123 / 167 1.35 / 12.51
Beam search 20.53 / 9.48 43925 / 18992 1.08 / 10.52
BIUTEE-orig 7.86 / 14.61 14749 / 22795 1.03 / 10.28
LLGS 2.76 / 1.72 1722 / 842 0.95 / 10.14
</table>
<tableCaption confidence="0.999975">
Table 3: Comparison of algorithms on RTE-5 / RTE-6
</tableCaption>
<bodyText confidence="0.998602714285714">
and F1 in RTE-6). We see that in RTE-5 LLGS out-
performs all other algorithms, and BIUTEE-orig is
the second best. This result is statistically significant
with p &lt; 0.02 according to McNemar test. In RTE-
6 we see that although LLGS tends to finds lower
cost proofs, as shown in Table 3, BIUTEE obtains
slightly lower results when utilizing this algorithm.
</bodyText>
<table confidence="0.999375">
Algorithm RTE-5 accuracy % RTE-6 F1 %
Weighted A* 59.50 48.20
Dovetailing WA* 60.83 49.01
Greedy 60.50 48.56
Pure heuristic 60.83 45.70
Beam search 61.33 48.58
BIUTEE-orig 60.67 49.25
LLGS 64.00 49.09
</table>
<tableCaption confidence="0.999457">
Table 4: Impact of algorithms on system success rate
</tableCaption>
<subsectionHeader confidence="0.985885">
4.4 Component evaluation
</subsectionHeader>
<bodyText confidence="0.9999652">
In this experiment we examine separately our two
novel components. We examined fA by running
LLGS with alternative evaluation functions. The re-
sults, displayed in Table 5, show that using fA yields
better proofs and also improves run time.
</bodyText>
<table confidence="0.76346225">
f Avg. time Avg. cost Accuracy %
f = g + h 3.28 1.06 61.50
f = g + w · h 3.30 1.07 61.33
f = fA 2.76 0.95 64.0
</table>
<tableCaption confidence="0.9962125">
Table 5: Impact of fA on RTE-5. w = 6.0. Accuracy
obtained by retraining with corresponding f.
</tableCaption>
<bodyText confidence="0.999393285714286">
Our local-lookahead (Subsection 3.4) was exam-
ined by running LLGS with alternative node expan-
sion methods. One alternative to local-lookahead
is standard expansion by generating all immediate
derivations. Another alternative is to use the stan-
dard lookahead, in which a brute-force depth-limited
search is performed in each iteration, termed here
“exhaustive lookahead”. The results, presented in
Table 6, show that by avoiding any type of looka-
head one can achieve fast runtime, while compro-
mising proof quality. On the other hand, both ex-
haustive and local lookahead yield better proofs and
accuracy, while local lookahead is more than 4 times
faster than exhaustive lookahead.
</bodyText>
<table confidence="0.99161925">
lookahead Avg. time Avg. cost Accuracy (%)
exhaustive 13.22 0.95 64.0
local 2.76 0.95 64.0
none 0.24 0.97 62.0
</table>
<tableCaption confidence="0.996308">
Table 6: Impact of local and global lookahead on RTE-5.
Accuracy obtained by retraining with the corresponding
lookahead method.
</tableCaption>
<sectionHeader confidence="0.998947" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99967895">
In this paper we investigated the efficiency and proof
quality obtained by various search algorithms. Con-
sequently, we observed special phenomena of the
search space in textual inference and proposed two
novel components yielding a new search algorithm,
targeted for our domain. We have shown empirically
that (1) this algorithm improves run time by factors
of 3-8 relative to BIUTEE-orig, and by similar fac-
tors relative to standard AI-search algorithms that
achieve similar proof quality; and (2) outperforms
all other algorithms in finding low cost proofs.
In future work we plan to investigate other search
paradigms, e.g., Monte-Carlo style approaches
(Kocsis and Szepesv´ari, 2006), which do not fall
under the AI search scheme covered in this paper.
In addition, while our novel components were moti-
vated by the search space of textual inference, we
foresee their potential utility in other application
areas for search, such as automated planning and
scheduling.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.951837">
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
</bodyText>
<page confidence="0.991859">
290
</page>
<sectionHeader confidence="0.995868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999543239130435">
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science.
Adi Botea, Markus Enzenberger, Martin M¨uller, and
Jonathan Schaeffer. 2005. Macro-FF: Improving ai
planning with automatically learned macro-operators.
J. Artif. Intell. Res. (JAIR), 24:581–621.
Vadim Bulitko and Mitja Lustrek. 2006. Lookahead
pathology in real-time path-finding. In proceedings of
AAAI.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of MLCW.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, May.
Ariel Felner, Sarit Kraus, and Richard E. Korf. 2003.
KBFS: K-best-first search. Ann. Math. Artif. Intell.,
39(1-2):19–39.
David Furcy and Sven Koenig. 2005. Limited discrep-
ancy beam search. In proceedings of IJCAI.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Stefan Harmeling. 2009. Inferring textual entailment
with a probabilistically sound calculus. Natural Lan-
guage Engineering.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968. A formal basis for the heuristic determination
of minimum cost paths. IEEE Transactions on Sys-
tems Science and Cybernetics, SSC-4(2):100–107.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL.
Levente Kocsis and Csaba Szepesv´ari. 2006. Bandit
based monte-carlo planning. In proceedings of ECML.
Richard E. Korf. 1985. Macro-operators: A weak
method for learning. Artif. Intell., 26(1):35–77.
Richard E. Korf. 1990. Real-time heuristic search. Artif.
Intell., 42(2-3):189–211.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of Pascal Challenges Work-
shop on Recognising Textual Entailment.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP.
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009. Addressing discourse and document structure in
the rte search task. In Proceedings of TAC.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(3-4):193 – 204.
Stuart Russell and Peter Norvig. 2010. Artificial Intel-
ligence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, 3rd edition edition.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In Pro-
ceedings of RANLP.
Roni Stern, Tamar Kulberis, Ariel Felner, and Robert
Holte. 2010. Using lookaheads with optimal best-first
search. In proceedings of AAAI.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Richard Anthony Valenzano, Nathan R. Sturtevant,
Jonathan Schaeffer, Karen Buro, and Akihiro Kishi-
moto. 2010. Simultaneously searching with multiple
settings: An alternative to parameter tuning for subop-
timal single-agent search algorithms. In proceedings
of ICAPS.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING.
Rong Zhou and Eric A. Hansen. 2005. Beam-stack
search: Integrating backtracking with beam search. In
proceedings of ICAPS.
</reference>
<page confidence="0.997858">
291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523882">
<title confidence="0.997819">Efficient Search for Transformation-based Inference</title>
<author confidence="0.768165333333333">Roni Ido Ariel Science Department</author>
<author confidence="0.768165333333333">Bar-Ilan Systems Engineering</author>
<author confidence="0.768165333333333">Ben Gurion</author>
<email confidence="0.995009">felner@bgu.ac.il</email>
<abstract confidence="0.999378421052632">This paper addresses the search problem in textual inference, where systems need to infer one piece of text from another. A prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations, a.k.a. a proof, while estimating the proof’s validity. This raises a search challenge of finding the best possible proof. We explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference: a gradient-style evaluation function, and a locallookahead node expansion method. Evaluausing the open-source system, show the contribution of these ideas to search efficiency and proof quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
</authors>
<title>Ido Dagan, Iddo Greental, and Eyal Shnarch.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<marker>Bar-Haim, 2007</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal Shnarch. 2007. Semantic inference at the lexicalsyntactic level. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Bille</author>
</authors>
<title>A survey on tree edit distance and related problems.</title>
<date>2005</date>
<journal>Theoretical Computer Science.</journal>
<contexts>
<context position="9664" citStr="Bille, 2005" startWordPosition="1468" endWordPosition="1469">utomatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or probability estimation) for a complete sequence of transformations is typically defined as the sum of the costs of the involved transformations. Finding the lowest cost proof, as needed for determining inference validity, is the focus of our research. Textual inference systems limited to the standard tree-edit operations (insertion, deletion, substitution) can use an exact algorithm that finds the optimal solution in polynomial time under certain constraints (Bille, 2005). Nevertheless, for the extended set of transformations it is unlikely that efficient exact algorithms for finding lowest-cost sequences are available (Heilman and Smith, 2010). In this harder case, the problem can be viewed as an AI search problem. Each state in the search space is a parse-tree, where the initial state is the text parse-tree, the goal state is the hypothesis parse-tree, and we search for the shortest (in terms of costs) path of transformations from the initial state to the goal state. Next we briefly review major concepts from the field of AI search and summarize some relevan</context>
</contexts>
<marker>Bille, 2005</marker>
<rawString>Philip Bille. 2005. A survey on tree edit distance and related problems. Theoretical Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adi Botea</author>
<author>Markus Enzenberger</author>
<author>Martin M¨uller</author>
<author>Jonathan Schaeffer</author>
</authors>
<title>Macro-FF: Improving ai planning with automatically learned macro-operators.</title>
<date>2005</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>24--581</pages>
<marker>Botea, Enzenberger, M¨uller, Schaeffer, 2005</marker>
<rawString>Adi Botea, Markus Enzenberger, Martin M¨uller, and Jonathan Schaeffer. 2005. Macro-FF: Improving ai planning with automatically learned macro-operators. J. Artif. Intell. Res. (JAIR), 24:581–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vadim Bulitko</author>
<author>Mitja Lustrek</author>
</authors>
<title>Lookahead pathology in real-time path-finding.</title>
<date>2006</date>
<booktitle>In proceedings of AAAI.</booktitle>
<contexts>
<context position="22062" citStr="Bulitko and Lustrek, 2006" startWordPosition="3584" endWordPosition="3588">following way: instead of searching for a complete sequence of transformations that transform tT into tH, we can iteratively search for independent coherent subsequences of transformations, such that a combination of these subsequences will transform tT into tH. This is somewhat similar to the technique of applying macro operators, which is used in automated planning (Botea et al., 2005) and puzzle solving (Korf, 1985). One technique for finding such subsequences is to perform, for each state being expanded, a bruteforce depth-limited search, also known as lookahead (Russell and Norvig, 2010; Bulitko and Lustrek, 2006; Korf, 1990; Stern et al., 2010). However, performing such lookahead might be slow if the branching factor is large. Fortunately, in our domain, coherent subsequences have the following characteristic which can be leveraged: typically, a transformation depends on a previous one only if it is performed over some nodes which were affected by the previous transformation. Accordingly, our proposed algorithm searches for coherent subsequences, in which each subsequent transformation must be applied to nodes that were affected by the previous transformation. Formally, let o be a transformation that</context>
</contexts>
<marker>Bulitko, Lustrek, 2006</marker>
<rawString>Vadim Bulitko and Mitja Lustrek. 2006. Lookahead pathology in real-time path-finding. In proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of MLCW.</booktitle>
<contexts>
<context position="1630" citStr="Dagan et al., 2005" startWordPosition="235" endWordPosition="238">contribution of these ideas to search efficiency and proof quality. 1 Introduction In many NLP settings it is necessary to identify that a certain semantic inference relation holds between two pieces of text. For example, in paraphrase recognition it is necessary to identify that the meanings of two text fragments are roughly equivalent. In passage retrieval for question answering, it is needed to detect text passages from which a satisfying answer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” —* “message”) and predicate-template sub</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of MLCW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Salvo Braz</author>
<author>Roxana Girju</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="7925" citStr="Braz et al. (2005)" startWordPosition="1207" endWordPosition="1210"> and by (Wang and Manning, 2010). Since the above mentioned transformations are limited in capturing certain interesting and prevalent semantic phenomena, an extended set of tree edit operations (e.g., relabel-edge, move-sibling, etc.) was proposed by Heilman and Smith (2010). Similarly, Harmeling (2009) suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transformations are defined by knowledge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Lin and Pantel, 2001), and TEASE (Szpektor et al., 2004), as well as syntactic transformations constructed manually. In addition, they used knowledge-based lexical substitutions. However, when only knowledge-based transformations are allowed, transforming the text into the hypothesis is impossible in many cases. This limitation is dealt by ou</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok, Dan Roth, and Mark Sammons. 2005. An inference model for semantic entailment in natural language. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Felner</author>
<author>Sarit Kraus</author>
<author>Richard E Korf</author>
</authors>
<date>2003</date>
<journal>KBFS: K-best-first search. Ann. Math. Artif. Intell.,</journal>
<pages>39--1</pages>
<contexts>
<context position="11716" citStr="Felner et al., 2003" startWordPosition="1813" endWordPosition="1816"> evaluation function is usually a linear combination of the shortest path found from the start state to state s, denoted by g(s), and a heuristic function, denoted by h(s), which estimates the cost of reaching a goal state from s. Many search algorithms can be viewed as special cases or variations of best-first search. The well-known A* (Hart et al., 1968). algorithm is a best-first search that uses an evaluation function f(s) = g(s) + h(s). Weighted A* (Pohl, 1970) uses an evaluation function f(s) = w · g(s) + h(s), where w is a parameter, while pure heuristic search uses f(s) = h(s). K-BFS (Felner et al., 2003) expands k states in each iteration. Beam search (Furcy and Koenig, 2005; Zhou and Hansen, 2005) limits the number of states stored in OPEN, while Greedy search limits OPEN to contain only the single best state generated in the current iteration. The search algorithm has crucial impact on the quality of proof found by a textual inference system, as well as on its efficiency. Next, we describe search strategies used in prior works for textual inference. 2.3 Search in prior inference models In spite of being a fundamental problem, prior solutions to the search challenge in textual inference were</context>
</contexts>
<marker>Felner, Kraus, Korf, 2003</marker>
<rawString>Ariel Felner, Sarit Kraus, and Richard E. Korf. 2003. KBFS: K-best-first search. Ann. Math. Artif. Intell., 39(1-2):19–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Furcy</author>
<author>Sven Koenig</author>
</authors>
<title>Limited discrepancy beam search.</title>
<date>2005</date>
<booktitle>In proceedings of IJCAI.</booktitle>
<contexts>
<context position="11788" citStr="Furcy and Koenig, 2005" startWordPosition="1826" endWordPosition="1829">ath found from the start state to state s, denoted by g(s), and a heuristic function, denoted by h(s), which estimates the cost of reaching a goal state from s. Many search algorithms can be viewed as special cases or variations of best-first search. The well-known A* (Hart et al., 1968). algorithm is a best-first search that uses an evaluation function f(s) = g(s) + h(s). Weighted A* (Pohl, 1970) uses an evaluation function f(s) = w · g(s) + h(s), where w is a parameter, while pure heuristic search uses f(s) = h(s). K-BFS (Felner et al., 2003) expands k states in each iteration. Beam search (Furcy and Koenig, 2005; Zhou and Hansen, 2005) limits the number of states stored in OPEN, while Greedy search limits OPEN to contain only the single best state generated in the current iteration. The search algorithm has crucial impact on the quality of proof found by a textual inference system, as well as on its efficiency. Next, we describe search strategies used in prior works for textual inference. 2.3 Search in prior inference models In spite of being a fundamental problem, prior solutions to the search challenge in textual inference were mostly ad-hoc. Furthermore, there was no investigation of alternative s</context>
</contexts>
<marker>Furcy, Koenig, 2005</marker>
<rawString>David Furcy and Sven Koenig. 2005. Limited discrepancy beam search. In proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="27317" citStr="Haghighi and Klein, 2009" startWordPosition="4492" endWordPosition="4495"> algorithms, described in Table 2 above, as well as BIUTEE-orig. The latter is a strong baseline, which outperforms known search algorithms in generating low cost proofs. We compared all the above mentioned algorithms to our novel one, LLGS. We used the training dataset for parameter tuning, which controls the trade-off between speed and quality. For weighted A*, as well as for greedy search, we used w = 6.0, since, for a few instances, lower values of w resulted in prohibitive runtime. For beam search we used k = 150, since higher val3http://lucene.apache.org 4www.ark.cs.cmu.edu/ARKref/ See (Haghighi and Klein, 2009) ues of k did not improve the proof cost on the training dataset. The value of d in LLGS was set to 3. d = 4 yielded the same proof costs, but was about 3 times slower. Since lower values of w could be used by weighted A* for most instances, we also ran experiments where we varied the value of w according to the dovetailing method suggested in (Valenzano et al., 2010) (denoted dovetailing WA*) as follows. When weighted A* has found a solution, we reran it with a new value of w, set to half of the previous value. The idea is to guide the search for lower cost solutions. This process was halted </context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Harmeling</author>
</authors>
<title>Inferring textual entailment with a probabilistically sound calculus. Natural Language Engineering.</title>
<date>2009</date>
<contexts>
<context position="2018" citStr="Harmeling, 2009" startWordPosition="301" endWordPosition="303">eeded to detect text passages from which a satisfying answer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” —* “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” —* “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each </context>
<context position="7612" citStr="Harmeling (2009)" startWordPosition="1159" endWordPosition="1160">ypes of transformations in order to derive H from T. Some suggested a set of predefined transformations, for example, insertion, deletion and substitution of parse-tree nodes, by which any tree can be transformed to any other tree. These transformations were used by the open-source system EDITS (Mehdad, 2009), and by (Wang and Manning, 2010). Since the above mentioned transformations are limited in capturing certain interesting and prevalent semantic phenomena, an extended set of tree edit operations (e.g., relabel-edge, move-sibling, etc.) was proposed by Heilman and Smith (2010). Similarly, Harmeling (2009) suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transformations are defined by knowledge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Lin and Pantel, 2001), and TEAS</context>
<context position="9081" citStr="Harmeling, 2009" startWordPosition="1378" endWordPosition="1379">is impossible in many cases. This limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic validity of transformation-based inference is usually modeled by defining a cost or a probability estimation for each transformation. Costs may be defined manually (Kouylekov and Magnini, 2005), but are usually learned automatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or probability estimation) for a complete sequence of transformations is typically defined as the sum of the costs of the involved transformations. Finding the lowest cost proof, as needed for determining inference validity, is the focus of our research. Textual inference systems limited to the standard tree-edit operations (insertion, deletion, substitution) can use an exact algorithm that finds the optimal solution in polynomial time under certain constraints (Bille, 2005). Nevertheless, f</context>
<context position="12505" citStr="Harmeling, 2009" startWordPosition="1945" endWordPosition="1946">to contain only the single best state generated in the current iteration. The search algorithm has crucial impact on the quality of proof found by a textual inference system, as well as on its efficiency. Next, we describe search strategies used in prior works for textual inference. 2.3 Search in prior inference models In spite of being a fundamental problem, prior solutions to the search challenge in textual inference were mostly ad-hoc. Furthermore, there was no investigation of alternative search methods, and no 285 evaluation of search efficiency and quality was reported. For example, in (Harmeling, 2009) the order by which the transformations are performed is predetermined, and in addition many possible derivations are discarded, to prevent exponential explosion. Handling the search problem in (Heilman and Smith, 2010) was by a variant of greedy search, driven by a similarity measure between the current parse-tree and the hypothesis, while ignoring the cost already paid. In addition, several constraints on the search space were implemented. In the earlier version of BIUTEE (Stern and Dagan, 2011)2, a version of beam search was incorporated, named hereafter BIUTEE-orig. This algorithm uses the</context>
</contexts>
<marker>Harmeling, 2009</marker>
<rawString>Stefan Harmeling. 2009. Inferring textual entailment with a probabilistically sound calculus. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Hart</author>
<author>Nils J Nilsson</author>
<author>Bertram Raphael</author>
</authors>
<title>A formal basis for the heuristic determination of minimum cost paths.</title>
<date>1968</date>
<journal>IEEE Transactions on Systems Science and Cybernetics,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="11454" citStr="Hart et al., 1968" startWordPosition="1765" endWordPosition="1768">have not been expanded yet. States in OPEN are prioritized by an evaluation function, f(s). A best-first search algorithm iteratively removes the best state (according to f(s)) from OPEN, and inserts new states being generated by expanding this best state. The evaluation function is usually a linear combination of the shortest path found from the start state to state s, denoted by g(s), and a heuristic function, denoted by h(s), which estimates the cost of reaching a goal state from s. Many search algorithms can be viewed as special cases or variations of best-first search. The well-known A* (Hart et al., 1968). algorithm is a best-first search that uses an evaluation function f(s) = g(s) + h(s). Weighted A* (Pohl, 1970) uses an evaluation function f(s) = w · g(s) + h(s), where w is a parameter, while pure heuristic search uses f(s) = h(s). K-BFS (Felner et al., 2003) expands k states in each iteration. Beam search (Furcy and Koenig, 2005; Zhou and Hansen, 2005) limits the number of states stored in OPEN, while Greedy search limits OPEN to contain only the single best state generated in the current iteration. The search algorithm has crucial impact on the quality of proof found by a textual inferenc</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, SSC-4(2):100–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2081" citStr="Heilman and Smith, 2010" startWordPosition="310" endWordPosition="313">nswer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” —* “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” —* “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation step should preserve inference validity, such th</context>
<context position="7583" citStr="Heilman and Smith (2010)" startWordPosition="1153" endWordPosition="1156">researchers suggested using various types of transformations in order to derive H from T. Some suggested a set of predefined transformations, for example, insertion, deletion and substitution of parse-tree nodes, by which any tree can be transformed to any other tree. These transformations were used by the open-source system EDITS (Mehdad, 2009), and by (Wang and Manning, 2010). Since the above mentioned transformations are limited in capturing certain interesting and prevalent semantic phenomena, an extended set of tree edit operations (e.g., relabel-edge, move-sibling, etc.) was proposed by Heilman and Smith (2010). Similarly, Harmeling (2009) suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transformations are defined by knowledge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Li</context>
<context position="9144" citStr="Heilman and Smith, 2010" startWordPosition="1386" endWordPosition="1389">y our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic validity of transformation-based inference is usually modeled by defining a cost or a probability estimation for each transformation. Costs may be defined manually (Kouylekov and Magnini, 2005), but are usually learned automatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or probability estimation) for a complete sequence of transformations is typically defined as the sum of the costs of the involved transformations. Finding the lowest cost proof, as needed for determining inference validity, is the focus of our research. Textual inference systems limited to the standard tree-edit operations (insertion, deletion, substitution) can use an exact algorithm that finds the optimal solution in polynomial time under certain constraints (Bille, 2005). Nevertheless, for the extended set of transformations it is unlikely that effi</context>
<context position="12724" citStr="Heilman and Smith, 2010" startWordPosition="1977" endWordPosition="1980">we describe search strategies used in prior works for textual inference. 2.3 Search in prior inference models In spite of being a fundamental problem, prior solutions to the search challenge in textual inference were mostly ad-hoc. Furthermore, there was no investigation of alternative search methods, and no 285 evaluation of search efficiency and quality was reported. For example, in (Harmeling, 2009) the order by which the transformations are performed is predetermined, and in addition many possible derivations are discarded, to prevent exponential explosion. Handling the search problem in (Heilman and Smith, 2010) was by a variant of greedy search, driven by a similarity measure between the current parse-tree and the hypothesis, while ignoring the cost already paid. In addition, several constraints on the search space were implemented. In the earlier version of BIUTEE (Stern and Dagan, 2011)2, a version of beam search was incorporated, named hereafter BIUTEE-orig. This algorithm uses the evaluation function f(s) = g(s) + wi · h(s), where in each iteration (i) the value of w is increased, to ensure successful termination of the search. Nevertheless, its efficiency and quality were not investigated. In t</context>
<context position="19229" citStr="Heilman and Smith (2010)" startWordPosition="3100" endWordPosition="3103">tion for the expected cost of the path (the sequence of transformations) from s to the goal state. This is because the number of nodes and edges that can be changed by a single transformation can vary from a single node to several nodes (e.g., by a lexical syntactic entailment rule). Moreover, even if two transformations change the same number of nodes and edges, their costs might be significantly different. Consequently, the measurement of the cost accumulated so far (g(s)) and the remaining gap to tH (h(s)) are unrelated. We note that a more sophisticated heuristic function was suggested by Heilman and Smith (2010), based on tree-kernels. Nevertheless, this heuristic function, serving as h(s), is still unrelated to the transformation costs (g(s)). We therefore propose a novel gradient-style function to overcome this difficulty. Our function is designed for a greedy search in which OPEN always contains a single state, s. Let si be a state generated from s, the cost of deriving si from s is Ag(si) = g(si) − g(s). Similarly, the reduction in the value of the heuristic function is defined Ah(si) = h(s) − h(si). Now, we define fΔ(si) = Δg(�j) Δh(�j). Informally, this function measures how costly it is to der</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levente Kocsis</author>
<author>Csaba Szepesv´ari</author>
</authors>
<title>Bandit based monte-carlo planning.</title>
<date>2006</date>
<booktitle>In proceedings of ECML.</booktitle>
<marker>Kocsis, Szepesv´ari, 2006</marker>
<rawString>Levente Kocsis and Csaba Szepesv´ari. 2006. Bandit based monte-carlo planning. In proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard E Korf</author>
</authors>
<title>Macro-operators: A weak method for learning.</title>
<date>1985</date>
<journal>Artif. Intell.,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="21859" citStr="Korf, 1985" startWordPosition="3554" endWordPosition="3555">uences of transformations, where in each subsequence the transformations strongly depend on each other, while different subsequences are independent. This phenomenon can be utilized in the following way: instead of searching for a complete sequence of transformations that transform tT into tH, we can iteratively search for independent coherent subsequences of transformations, such that a combination of these subsequences will transform tT into tH. This is somewhat similar to the technique of applying macro operators, which is used in automated planning (Botea et al., 2005) and puzzle solving (Korf, 1985). One technique for finding such subsequences is to perform, for each state being expanded, a bruteforce depth-limited search, also known as lookahead (Russell and Norvig, 2010; Bulitko and Lustrek, 2006; Korf, 1990; Stern et al., 2010). However, performing such lookahead might be slow if the branching factor is large. Fortunately, in our domain, coherent subsequences have the following characteristic which can be leveraged: typically, a transformation depends on a previous one only if it is performed over some nodes which were affected by the previous transformation. Accordingly, our proposed</context>
</contexts>
<marker>Korf, 1985</marker>
<rawString>Richard E. Korf. 1985. Macro-operators: A weak method for learning. Artif. Intell., 26(1):35–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard E Korf</author>
</authors>
<title>Real-time heuristic search.</title>
<date>1990</date>
<journal>Artif. Intell.,</journal>
<pages>42--2</pages>
<contexts>
<context position="22074" citStr="Korf, 1990" startWordPosition="3589" endWordPosition="3590">earching for a complete sequence of transformations that transform tT into tH, we can iteratively search for independent coherent subsequences of transformations, such that a combination of these subsequences will transform tT into tH. This is somewhat similar to the technique of applying macro operators, which is used in automated planning (Botea et al., 2005) and puzzle solving (Korf, 1985). One technique for finding such subsequences is to perform, for each state being expanded, a bruteforce depth-limited search, also known as lookahead (Russell and Norvig, 2010; Bulitko and Lustrek, 2006; Korf, 1990; Stern et al., 2010). However, performing such lookahead might be slow if the branching factor is large. Fortunately, in our domain, coherent subsequences have the following characteristic which can be leveraged: typically, a transformation depends on a previous one only if it is performed over some nodes which were affected by the previous transformation. Accordingly, our proposed algorithm searches for coherent subsequences, in which each subsequent transformation must be applied to nodes that were affected by the previous transformation. Formally, let o be a transformation that has been ap</context>
</contexts>
<marker>Korf, 1990</marker>
<rawString>Richard E. Korf. 1990. Real-time heuristic search. Artif. Intell., 42(2-3):189–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference. Natural Language Engineering.</title>
<date>2010</date>
<contexts>
<context position="26538" citStr="Kotlerman et al., 2010" startWordPosition="4366" endWordPosition="4369">ate entailing sentences for each hypothesis. The system has to find for each hypothesis which candidate sentences entail it. To improve speed and results, we used the filtering mechanism suggested by (Mirkin et al., 2009), which filters the candidate sentences by the Lucene IR engine3. Thus, only top 20 candidates per hypothesis were tested Evaluation of each of the algorithms was performed by running BIUTEE while replacing BIUTEE-orig with this algorithm. We employed a comprehensive set of knowledge resources (available in BIUTEE’s web site): WordNet (Fellbaum, 1998), Directional similarity (Kotlerman et al., 2010), DIRT (Lin and Pantel, 2001) and generic syntactic rules. In addition, we used coreference substitutions, detected by ArkRef4. We evaluated several known algorithms, described in Table 2 above, as well as BIUTEE-orig. The latter is a strong baseline, which outperforms known search algorithms in generating low cost proofs. We compared all the above mentioned algorithms to our novel one, LLGS. We used the training dataset for parameter tuning, which controls the trade-off between speed and quality. For weighted A*, as well as for greedy search, we used w = 6.0, since, for a few instances, lower</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Pascal Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="9025" citStr="Kouylekov and Magnini, 2005" startWordPosition="1368" endWordPosition="1371">ansformations are allowed, transforming the text into the hypothesis is impossible in many cases. This limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic validity of transformation-based inference is usually modeled by defining a cost or a probability estimation for each transformation. Costs may be defined manually (Kouylekov and Magnini, 2005), but are usually learned automatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or probability estimation) for a complete sequence of transformations is typically defined as the sum of the costs of the involved transformations. Finding the lowest cost proof, as needed for determining inference validity, is the focus of our research. Textual inference systems limited to the standard tree-edit operations (insertion, deletion, substitution) can use an exact algorithm that finds the optimal solution in polynomial time </context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In Proceedings of Pascal Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="8202" citStr="Lin and Pantel, 2001" startWordPosition="1249" endWordPosition="1252">0). Similarly, Harmeling (2009) suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transformations are defined by knowledge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Lin and Pantel, 2001), and TEASE (Szpektor et al., 2004), as well as syntactic transformations constructed manually. In addition, they used knowledge-based lexical substitutions. However, when only knowledge-based transformations are allowed, transforming the text into the hypothesis is impossible in many cases. This limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical</context>
<context position="26567" citStr="Lin and Pantel, 2001" startWordPosition="4371" endWordPosition="4374">h hypothesis. The system has to find for each hypothesis which candidate sentences entail it. To improve speed and results, we used the filtering mechanism suggested by (Mirkin et al., 2009), which filters the candidate sentences by the Lucene IR engine3. Thus, only top 20 candidates per hypothesis were tested Evaluation of each of the algorithms was performed by running BIUTEE while replacing BIUTEE-orig with this algorithm. We employed a comprehensive set of knowledge resources (available in BIUTEE’s web site): WordNet (Fellbaum, 1998), Directional similarity (Kotlerman et al., 2010), DIRT (Lin and Pantel, 2001) and generic syntactic rules. In addition, we used coreference substitutions, detected by ArkRef4. We evaluated several known algorithms, described in Table 2 above, as well as BIUTEE-orig. The latter is a strong baseline, which outperforms known search algorithms in generating low cost proofs. We compared all the above mentioned algorithms to our novel one, LLGS. We used the training dataset for parameter tuning, which controls the trade-off between speed and quality. For weighted A*, as well as for greedy search, we used w = 6.0, since, for a few instances, lower values of w resulted in proh</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of inference rules from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
</authors>
<title>Automatic cost estimation for tree edit distance using particle swarm optimization.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP.</booktitle>
<contexts>
<context position="2032" citStr="Mehdad, 2009" startWordPosition="304" endWordPosition="305">ext passages from which a satisfying answer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” —* “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” —* “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation</context>
<context position="7306" citStr="Mehdad, 2009" startWordPosition="1115" endWordPosition="1116">oof. The following paragraphs review the major research approaches in this direction. We focus on methods that perform transformations over parse trees, and highlight the search challenge with which they are faced. 2.1 Transformation-based textual inference Several researchers suggested using various types of transformations in order to derive H from T. Some suggested a set of predefined transformations, for example, insertion, deletion and substitution of parse-tree nodes, by which any tree can be transformed to any other tree. These transformations were used by the open-source system EDITS (Mehdad, 2009), and by (Wang and Manning, 2010). Since the above mentioned transformations are limited in capturing certain interesting and prevalent semantic phenomena, an extended set of tree edit operations (e.g., relabel-edge, move-sibling, etc.) was proposed by Heilman and Smith (2010). Similarly, Harmeling (2009) suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo</context>
<context position="9095" citStr="Mehdad, 2009" startWordPosition="1380" endWordPosition="1381">many cases. This limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic validity of transformation-based inference is usually modeled by defining a cost or a probability estimation for each transformation. Costs may be defined manually (Kouylekov and Magnini, 2005), but are usually learned automatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or probability estimation) for a complete sequence of transformations is typically defined as the sum of the costs of the involved transformations. Finding the lowest cost proof, as needed for determining inference validity, is the focus of our research. Textual inference systems limited to the standard tree-edit operations (insertion, deletion, substitution) can use an exact algorithm that finds the optimal solution in polynomial time under certain constraints (Bille, 2005). Nevertheless, for the extende</context>
</contexts>
<marker>Mehdad, 2009</marker>
<rawString>Yashar Mehdad. 2009. Automatic cost estimation for tree edit distance using particle swarm optimization. In Proceedings of the ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Roy Bar-Haim</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Eyal Shnarch</author>
<author>Asher Stern</author>
<author>Idan Szpektor</author>
</authors>
<title>Addressing discourse and document structure in the rte search task.</title>
<date>2009</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="26136" citStr="Mirkin et al., 2009" startWordPosition="4306" endWordPosition="4309"> a training and test corpora, each containing 600 texthypothesis pairs, where in half of them the text entails the hypothesis and in the other half it does not. In RTE-6, each of the training and test corpora consists of 10 topics, where each topic contains 10 documents. Each corpus contains a set of hypotheses (211 in the training dataset, and 243 in the test dataset), along with a set of candidate entailing sentences for each hypothesis. The system has to find for each hypothesis which candidate sentences entail it. To improve speed and results, we used the filtering mechanism suggested by (Mirkin et al., 2009), which filters the candidate sentences by the Lucene IR engine3. Thus, only top 20 candidates per hypothesis were tested Evaluation of each of the algorithms was performed by running BIUTEE while replacing BIUTEE-orig with this algorithm. We employed a comprehensive set of knowledge resources (available in BIUTEE’s web site): WordNet (Fellbaum, 1998), Directional similarity (Kotlerman et al., 2010), DIRT (Lin and Pantel, 2001) and generic syntactic rules. In addition, we used coreference substitutions, detected by ArkRef4. We evaluated several known algorithms, described in Table 2 above, as </context>
</contexts>
<marker>Mirkin, Bar-Haim, Berant, Dagan, Shnarch, Stern, Szpektor, 2009</marker>
<rawString>Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor. 2009. Addressing discourse and document structure in the rte search task. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ira Pohl</author>
</authors>
<title>Heuristic search viewed as path finding in a graph.</title>
<date>1970</date>
<journal>Artificial Intelligence,</journal>
<pages>1--3</pages>
<contexts>
<context position="11566" citStr="Pohl, 1970" startWordPosition="1786" endWordPosition="1787">hm iteratively removes the best state (according to f(s)) from OPEN, and inserts new states being generated by expanding this best state. The evaluation function is usually a linear combination of the shortest path found from the start state to state s, denoted by g(s), and a heuristic function, denoted by h(s), which estimates the cost of reaching a goal state from s. Many search algorithms can be viewed as special cases or variations of best-first search. The well-known A* (Hart et al., 1968). algorithm is a best-first search that uses an evaluation function f(s) = g(s) + h(s). Weighted A* (Pohl, 1970) uses an evaluation function f(s) = w · g(s) + h(s), where w is a parameter, while pure heuristic search uses f(s) = h(s). K-BFS (Felner et al., 2003) expands k states in each iteration. Beam search (Furcy and Koenig, 2005; Zhou and Hansen, 2005) limits the number of states stored in OPEN, while Greedy search limits OPEN to contain only the single best state generated in the current iteration. The search algorithm has crucial impact on the quality of proof found by a textual inference system, as well as on its efficiency. Next, we describe search strategies used in prior works for textual infe</context>
</contexts>
<marker>Pohl, 1970</marker>
<rawString>Ira Pohl. 1970. Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1(3-4):193 – 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, 3rd edition edition.</title>
<date>2010</date>
<contexts>
<context position="22035" citStr="Russell and Norvig, 2010" startWordPosition="3580" endWordPosition="3583">on can be utilized in the following way: instead of searching for a complete sequence of transformations that transform tT into tH, we can iteratively search for independent coherent subsequences of transformations, such that a combination of these subsequences will transform tT into tH. This is somewhat similar to the technique of applying macro operators, which is used in automated planning (Botea et al., 2005) and puzzle solving (Korf, 1985). One technique for finding such subsequences is to perform, for each state being expanded, a bruteforce depth-limited search, also known as lookahead (Russell and Norvig, 2010; Bulitko and Lustrek, 2006; Korf, 1990; Stern et al., 2010). However, performing such lookahead might be slow if the branching factor is large. Fortunately, in our domain, coherent subsequences have the following characteristic which can be leveraged: typically, a transformation depends on a previous one only if it is performed over some nodes which were affected by the previous transformation. Accordingly, our proposed algorithm searches for coherent subsequences, in which each subsequent transformation must be applied to nodes that were affected by the previous transformation. Formally, let</context>
</contexts>
<marker>Russell, Norvig, 2010</marker>
<rawString>Stuart Russell and Peter Norvig. 2010. Artificial Intelligence: A Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, 3rd edition edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Ido Dagan</author>
</authors>
<title>A confidence model for syntactically-motivated entailment proofs.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="2105" citStr="Stern and Dagan, 2011" startWordPosition="314" endWordPosition="317">generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” —* “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” —* “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation step should preserve inference validity, such that each text generated a</context>
<context position="8591" citStr="Stern and Dagan, 2011" startWordPosition="1306" endWordPosition="1309">edge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Lin and Pantel, 2001), and TEASE (Szpektor et al., 2004), as well as syntactic transformations constructed manually. In addition, they used knowledge-based lexical substitutions. However, when only knowledge-based transformations are allowed, transforming the text into the hypothesis is impossible in many cases. This limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic validity of transformation-based inference is usually modeled by defining a cost or a probability estimation for each transformation. Costs may be defined manually (Kouylekov and Magnini, 2005), but are usually learned automatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or pro</context>
<context position="13007" citStr="Stern and Dagan, 2011" startWordPosition="2022" endWordPosition="2025">e search methods, and no 285 evaluation of search efficiency and quality was reported. For example, in (Harmeling, 2009) the order by which the transformations are performed is predetermined, and in addition many possible derivations are discarded, to prevent exponential explosion. Handling the search problem in (Heilman and Smith, 2010) was by a variant of greedy search, driven by a similarity measure between the current parse-tree and the hypothesis, while ignoring the cost already paid. In addition, several constraints on the search space were implemented. In the earlier version of BIUTEE (Stern and Dagan, 2011)2, a version of beam search was incorporated, named hereafter BIUTEE-orig. This algorithm uses the evaluation function f(s) = g(s) + wi · h(s), where in each iteration (i) the value of w is increased, to ensure successful termination of the search. Nevertheless, its efficiency and quality were not investigated. In this paper we consider several prominent search algorithms and evaluate their quality. The evaluation concentrates on two measures: the runtime required to find a proof, and proof quality (measured by its cost). In addition to evaluating standard search algorithms we propose two nove</context>
</contexts>
<marker>Stern, Dagan, 2011</marker>
<rawString>Asher Stern and Ido Dagan. 2011. A confidence model for syntactically-motivated entailment proofs. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Stern</author>
<author>Tamar Kulberis</author>
<author>Ariel Felner</author>
<author>Robert Holte</author>
</authors>
<title>Using lookaheads with optimal best-first search.</title>
<date>2010</date>
<booktitle>In proceedings of AAAI.</booktitle>
<contexts>
<context position="22095" citStr="Stern et al., 2010" startWordPosition="3591" endWordPosition="3594"> a complete sequence of transformations that transform tT into tH, we can iteratively search for independent coherent subsequences of transformations, such that a combination of these subsequences will transform tT into tH. This is somewhat similar to the technique of applying macro operators, which is used in automated planning (Botea et al., 2005) and puzzle solving (Korf, 1985). One technique for finding such subsequences is to perform, for each state being expanded, a bruteforce depth-limited search, also known as lookahead (Russell and Norvig, 2010; Bulitko and Lustrek, 2006; Korf, 1990; Stern et al., 2010). However, performing such lookahead might be slow if the branching factor is large. Fortunately, in our domain, coherent subsequences have the following characteristic which can be leveraged: typically, a transformation depends on a previous one only if it is performed over some nodes which were affected by the previous transformation. Accordingly, our proposed algorithm searches for coherent subsequences, in which each subsequent transformation must be applied to nodes that were affected by the previous transformation. Formally, let o be a transformation that has been applied on a tree t, yi</context>
</contexts>
<marker>Stern, Kulberis, Felner, Holte, 2010</marker>
<rawString>Roni Stern, Tamar Kulberis, Ariel Felner, and Robert Holte. 2010. Using lookaheads with optimal best-first search. In proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8237" citStr="Szpektor et al., 2004" startWordPosition="1255" endWordPosition="1259">ggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transformations are defined by knowledge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Lin and Pantel, 2001), and TEASE (Szpektor et al., 2004), as well as syntactic transformations constructed manually. In addition, they used knowledge-based lexical substitutions. However, when only knowledge-based transformations are allowed, transforming the text into the hypothesis is impossible in many cases. This limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic valid</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Anthony Valenzano</author>
<author>Nathan R Sturtevant</author>
<author>Jonathan Schaeffer</author>
<author>Karen Buro</author>
<author>Akihiro Kishimoto</author>
</authors>
<title>Simultaneously searching with multiple settings: An alternative to parameter tuning for suboptimal single-agent search algorithms.</title>
<date>2010</date>
<booktitle>In proceedings of ICAPS.</booktitle>
<contexts>
<context position="27687" citStr="Valenzano et al., 2010" startWordPosition="4568" endWordPosition="4572">l as for greedy search, we used w = 6.0, since, for a few instances, lower values of w resulted in prohibitive runtime. For beam search we used k = 150, since higher val3http://lucene.apache.org 4www.ark.cs.cmu.edu/ARKref/ See (Haghighi and Klein, 2009) ues of k did not improve the proof cost on the training dataset. The value of d in LLGS was set to 3. d = 4 yielded the same proof costs, but was about 3 times slower. Since lower values of w could be used by weighted A* for most instances, we also ran experiments where we varied the value of w according to the dovetailing method suggested in (Valenzano et al., 2010) (denoted dovetailing WA*) as follows. When weighted A* has found a solution, we reran it with a new value of w, set to half of the previous value. The idea is to guide the search for lower cost solutions. This process was halted when the total number of states generated by all weighted A* instances exceeded a predefined constant (set to 10, 000). 4.2 Search performance This experiment evaluates the search algorithms in both efficiency (run-time) and proof quality. Efficiency is measured by the average CPU (Intel Xeon 2.5 GHz) run-time (in seconds) for finding a complete proof for a text-hypot</context>
</contexts>
<marker>Valenzano, Sturtevant, Schaeffer, Buro, Kishimoto, 2010</marker>
<rawString>Richard Anthony Valenzano, Nathan R. Sturtevant, Jonathan Schaeffer, Karen Buro, and Akihiro Kishimoto. 2010. Simultaneously searching with multiple settings: An alternative to parameter tuning for suboptimal single-agent search algorithms. In proceedings of ICAPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2056" citStr="Wang and Manning, 2010" startWordPosition="306" endWordPosition="309">rom which a satisfying answer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” —* “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” —* “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation step should preserve in</context>
<context position="7339" citStr="Wang and Manning, 2010" startWordPosition="1119" endWordPosition="1122">graphs review the major research approaches in this direction. We focus on methods that perform transformations over parse trees, and highlight the search challenge with which they are faced. 2.1 Transformation-based textual inference Several researchers suggested using various types of transformations in order to derive H from T. Some suggested a set of predefined transformations, for example, insertion, deletion and substitution of parse-tree nodes, by which any tree can be transformed to any other tree. These transformations were used by the open-source system EDITS (Mehdad, 2009), and by (Wang and Manning, 2010). Since the above mentioned transformations are limited in capturing certain interesting and prevalent semantic phenomena, an extended set of tree edit operations (e.g., relabel-edge, move-sibling, etc.) was proposed by Heilman and Smith (2010). Similarly, Harmeling (2009) suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were sug284 gested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transf</context>
<context position="9119" citStr="Wang and Manning, 2010" startWordPosition="1382" endWordPosition="1385">is limitation is dealt by our open-source integrated framework, BIUTEE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by BIUTEE, we adopted it for our empirical investigations. The semantic validity of transformation-based inference is usually modeled by defining a cost or a probability estimation for each transformation. Costs may be defined manually (Kouylekov and Magnini, 2005), but are usually learned automatically (Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). A global cost (or probability estimation) for a complete sequence of transformations is typically defined as the sum of the costs of the involved transformations. Finding the lowest cost proof, as needed for determining inference validity, is the focus of our research. Textual inference systems limited to the standard tree-edit operations (insertion, deletion, substitution) can use an exact algorithm that finds the optimal solution in polynomial time under certain constraints (Bille, 2005). Nevertheless, for the extended set of transformations</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zhou</author>
<author>Eric A Hansen</author>
</authors>
<title>Beam-stack search: Integrating backtracking with beam search.</title>
<date>2005</date>
<booktitle>In proceedings of ICAPS.</booktitle>
<contexts>
<context position="11812" citStr="Zhou and Hansen, 2005" startWordPosition="1830" endWordPosition="1833"> state to state s, denoted by g(s), and a heuristic function, denoted by h(s), which estimates the cost of reaching a goal state from s. Many search algorithms can be viewed as special cases or variations of best-first search. The well-known A* (Hart et al., 1968). algorithm is a best-first search that uses an evaluation function f(s) = g(s) + h(s). Weighted A* (Pohl, 1970) uses an evaluation function f(s) = w · g(s) + h(s), where w is a parameter, while pure heuristic search uses f(s) = h(s). K-BFS (Felner et al., 2003) expands k states in each iteration. Beam search (Furcy and Koenig, 2005; Zhou and Hansen, 2005) limits the number of states stored in OPEN, while Greedy search limits OPEN to contain only the single best state generated in the current iteration. The search algorithm has crucial impact on the quality of proof found by a textual inference system, as well as on its efficiency. Next, we describe search strategies used in prior works for textual inference. 2.3 Search in prior inference models In spite of being a fundamental problem, prior solutions to the search challenge in textual inference were mostly ad-hoc. Furthermore, there was no investigation of alternative search methods, and no 28</context>
</contexts>
<marker>Zhou, Hansen, 2005</marker>
<rawString>Rong Zhou and Eric A. Hansen. 2005. Beam-stack search: Integrating backtracking with beam search. In proceedings of ICAPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>