<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000161">
<title confidence="0.893949">
Improving Testsuites via Instrumentation
</title>
<author confidence="0.758454">
Norbert Broker
</author>
<address confidence="0.435142666666667">
Eschenweg 3
69231 Rauenberg
Germany
</address>
<email confidence="0.991209">
norbert.broeker@sap.com
</email>
<sectionHeader confidence="0.97932" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998734083333333">
This paper explores the usefulness of a technique
from software engineering, namely code instrumen-
tation, for the development of large-scale natural
language grammars. Information about the usage
of grammar rules in test sentences is used to detect
untested rules, redundant test sentences, and likely
causes of overgeneration. Results show that less
than half of a large-coverage grammar for German is
actually tested by two large testsuites, and that 10-
30% of testing time is redundant. The methodology
applied can be seen as a re-use of grammar writing
knowledge for testsuite compilation.
</bodyText>
<sectionHeader confidence="0.995359" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973975928571429">
Computational Linguistics (CL) has moved towards
the marketplace: One finds programs employing CL-
techniques in every software shop: Speech Recogni-
tion, Grammar and Style Checking, and even Ma-
chine Translation are available as products. While
this demonstrates the applicability of the research
done, it also calls for a rigorous development
methodology of such CL application products.
In this paper,1I describe the adaptation of a tech-
nique from Software Engineering, namely code in-
strumentation, to grammar development. Instru-
mentation is based on the simple idea of marking
any piece of code used in processing, and evaluating
this usage information afterwards. The application
I present here is the evaluation and improvement of
grammar and testsuites; other applications are pos-
sible.
1.1 Software Engineering vs. Grammar
Engineering
Both software and grammar development are simi-
lar processes: They result in a system transforming
some input into some output, based on a functional
specification (e.g., cf. (Ciravegna et al., 1998) for the
application of a particular software design method-
ology to linguistic engineering). Although Grammar
&apos;The work reported here was conducted during my time
at the Institut fiir Maschinelle Sprachverarbeitung (IMS),
Stuttgart University, Germany.
Engineering usually is not based on concrete specifi-
cations, research from linguistics provides an infor-
mal specification.
Software Engineering developed many methods to
assess the quality of a program, ranging from static
analysis of the program code to dynamic testing of
the program&apos;s behavior. Here, we adapt dynamic
testing, which means running the implemented pro-
gram against a set of test cases. The test cases
are designed to maximize the probability of detect-
ing errors in the program, i.e., incorrect conditions,
incompatible assumptions on subsequent branches,
etc. (for overviews, cf. (Hetzel, 1988; Liggesmeyer,
1990)).
</bodyText>
<sectionHeader confidence="0.532611" genericHeader="introduction">
1.2 Instrumentation in Grammar
Engineering
</sectionHeader>
<bodyText confidence="0.999906928571429">
How can we fruitfully apply the idea of measuring
the coverage of a set of test cases to grammar devel-
opment? I argue that by exploring the relation be-
tween grammar and testsuite, one can improve both
of them. Even the traditional usage of testsuites to
indicate grammar gaps or overgeneration can profit
from a precise indication of the grammar rules used
to parse the sentences (cf. Sec.4). Conversely, one
may use the grammar to improve the testsuite, both
in terms of its coverage (cf. Sec.3.1) and its economy
(cf. Sec.3.2).
Viewed this way, testsuite writing can benefit from
grammar development because both describe the
syntactic constructions of a natural language. Test-
suites systematically list these constructions, while
grammars give generative procedures to construct
them. Since there are currently many more gram-
mars than testsuites, we may re-use the work that
has gone into the grammars for the improvement of
testsuites.
The work reported here is situated in a large coop-
erative project aiming at the development of large-
coverage grammars for three languages. The gram-
mars have been developed over years by different
people, which makes the existence of tools for navi-
gation, testing, and documentation mandatory. Al-
though the sample rules given below are in the for-
mat of LFG, nothing of the methodology relies on
</bodyText>
<page confidence="0.957793">
325
</page>
<equation confidence="0.96704675">
VP #. V
OBJ);
PP*{4.= (tOBL);
I4.E (f ADJUNCT);).
</equation>
<figureCaption confidence="0.997002">
Figure 1: Sample Rule
</figureCaption>
<bodyText confidence="0.817181">
the choice of linguistic or computational paradigm.
</bodyText>
<sectionHeader confidence="0.990306" genericHeader="method">
2 Grammar Instrumentation
</sectionHeader>
<bodyText confidence="0.997748444444445">
Measures from Software Engineering cannot be sim-
ply transferred to Grammar Engineering, because
the structure of programs is different from that of
unification grammars. Nevertheless, the structure
of a grammar allows the derivation of suitable mea-
sures, similar to the structure of programs; this is
discussed in Sec.2.1. The actual instrumentation of
the grammar depends on the formalism used, and is
discussed in Sec.2.2.
</bodyText>
<subsectionHeader confidence="0.986931">
2.1 Coverage Criteria
</subsectionHeader>
<bodyText confidence="0.999160666666666">
Consider the LFG grammar rule in Fig. 1.2 On first
view, one could require of a testsuite that each such
rule is exercised at least once. Further thought will
indicate that there are hidden alternatives, namely
the optionality of the NP and the PP. The rule can
only be said to be thoroughly tested if test cases exist
which test both presence and absence of optional
constituents (requiring 4 test cases for this rule).
In addition to context-free rules, unification gram-
mars contain equations of various sorts, as illus-
trated in Fig. 1. Since these annotations may also
contain disjunctions, a testsuite with complete rule
coverage is not guaranteed to exercise all equation
alternatives. The phrase-structure-based criterion
defined above must be refined to cover all equation
alternatives in the rule (requiring two test cases for
the PP annotation). Even if we assume that (as,
e.g., in LFG) there is at least one equation associ-
ated with each constituent, equation coverage does
not subsume rule coverage: Optional constituents
introduce a rule disjunct (without the constituent)
that is not characterizable by an equation. A mea-
sure might thus be defined as follows:
disjunct coverage The disjunct coverage of a test-
suite is the quotient
number of disjuncts tested
number of disjuncts in grammar
</bodyText>
<footnote confidence="0.624617">
2Notation: ?poi+ represent optionality/iteration includ-
ing/excluding zero occurrences on categories. Annotations
to a category specify equality (=) or set membership (E) of
feature values, or non-existence of features (--.); they are ter-
minated by a semicolon ( ; ). Disjunctions are given in braces
({ ... I ...}). -t- (4,) are metavariables representing the feature
structure corresponding to the mother (daughter) of the rule.
Comments are enclosed in quotation marks (&amp;quot; . &amp;quot;). Cf. (Ka-
plan and Bresnan, 1982) for an introduction to LFG notation.
</footnote>
<bodyText confidence="0.999888586206896">
where a disjunct is either a phrase-structure al-
ternative, or an annotation alternative. Op-
tional constituents (and equations, if the for-
malism allows them) have to be treated as a
disjunction of the constituent and an empty cat-
egory (cf. the instrumented rule in Fig.2 for an
example).
Instead of considering disjuncts in isolation, one
might take their interaction into account. The most
complete test criterion, doing this to the fullest ex-
tent possible, can be defined as follows:
interaction coverage The interaction coverage of
a testsuite is the quotient
Tinter = number of disjunct combinations tested
number of legal disjunct combinations
There are methodological problems in this cri-
terion, however. First, the set of legal com-
binations may not be easily definable, due to
far-reaching dependencies between disjuncts in
different rules, and second, recursion leads to
infinitely many legal disjunct combinations as
soon as we take the number of usages of a dis-
junct into account. Requiring complete inter-
action coverage is infeasible in practice, similar
to the path coverage criterion in Software En-
gineering.
We will say that an analysis (and the sentence
receiving this analysis) relies on a grammar disjunct
if this disjunct was used in constructing the analysis.
</bodyText>
<subsectionHeader confidence="0.981872">
2.2 Instrumentation
</subsectionHeader>
<bodyText confidence="0.965166956521739">
Basically, grammar instrumentation is identical to
program instrumentation: For each disjunct in a
given source grammar, we add grammar code that
will identify this disjunct in the solution produced,
if that disjunct has been used in constructing the
solution.
Assuming a unique numbering of disjuncts, an an-
notation of the form DISJUNCT-nn = + can be used
for marking. To determine whether a certain dis-
junct was used in constructing a solution, one only
needs to check whether the associated feature oc-
curs (at some level of embedding) in the solution.
Alternatively, if set-valued features are available,
one can use a set-valued feature DISJUNCTS to col-
lect atomic symbols representing one disjunct each:
DISJUNCT-nn E DISJUNCTS.
One restriction is imposed by using the unification
formalism, though: One occurrence of the mark can-
not be distinguished from two occurrences, since the
second application of the equation introduces no new
information. The markers merely unify, and there is
no way of counting.
Tdis
</bodyText>
<page confidence="0.655569">
326
</page>
<equation confidence="0.995957444444444">
VP =V .1.=-1;
e DISJUNCT-001 E 0*;
I NP 1.=( OBJ)
DISJUNCT-002 E o*;
{ e DISJUNCT-003 E o*;
I PP+{.1,= OBL)
DISJUNCT-004 E o*;
I (f ADJUNCT)
DISJUNCT-005 E o*J.
</equation>
<figureCaption confidence="0.997441">
Figure 2: Instrumented rule
</figureCaption>
<bodyText confidence="0.999873066666667">
Therefore, we have used a special feature of our
grammar development environment: Following the
LFG spirit of different representation levels associ-
ated with each solution (so-called projections), it
provides for a multiset of symbols associated with
the complete solution, where structural embedding
plays no role (so-called optimality projection; see
(Frank et al., 1998)). In this way, from the root
node of each solution the set of all disjuncts used
can be collected, together with a usage count.
Fig. 2 shows the rule from Fig.1 with such an in-
strumentation; equations of the form DISJUNCT-nnE
o* express membership of the disjunct-specific atom
DISJUNCT-nn in the sentence&apos;s multiset of disjunct
markers.
</bodyText>
<subsectionHeader confidence="0.999247">
2.3 Processing Tools
</subsectionHeader>
<bodyText confidence="0.9999762">
Tool support is mandatory for a scenario such as
instrumentation: Nobody will manually add equa-
tions such as those in Fig. 2 to several hundred rules.
Based on the format of the grammar rules, an algo-
rithm instrumenting a grammar can be written down
easily.
Given a grammar and a testsuite or corpus to com-
pare, first an instrumented grammar must be con-
structed using such an algorithm. This instrumented
grammar is then used to parse the testsuite, yielding
a set of solutions associated with information about
usage of grammar disjuncts. Up to this point, the
process is completely automatic. The following two
sections discuss two possibilities to evaluate this in-
formation.
</bodyText>
<sectionHeader confidence="0.937633" genericHeader="method">
3 Quality of Testsuites
</sectionHeader>
<bodyText confidence="0.999983">
This section addresses the aspects of completeness
(&amp;quot;does the testsuite exercise all disjuncts in the
grammar?&amp;quot;) and economy of a testsuite (&amp;quot;is it min-
imal?&amp;quot;).
Complementing other work on testsuite construc-
tion (cf. Sec.5), we will assume that a grammar
is already available, and that a testsuite has to be
constructed or extended. While one may argue that
grammar and testsuite should be developed in paral-
lel, such that the coding of a new grammar disjunct
is accompanied by the addition of suitable test cases,
and vice versa, this is seldom the case. Apart from
the existence of grammars which lack a testsuite,
and to which this procedure could be usefully ap-
plied, there is the more principled obstacle of the
evolution of the grammar, leading to states where
previously necessary rules silently loose their use-
fulness, because their function is taken over by some
other rules, structured differently. This is detectable
by instrumentation, as discussed in Sec.3.1.
On the other hand, once there is a testsuite, you
want to use it in the most economic way, avoiding
redundant tests. Sec.3.2 shows that there are dif-
ferent levels of redundancy in a testsuite, dependent
on the specific grammar used. Reduction of this re-
dundancy can speed up the test activity, and give a
clearer picture of the grammar&apos;s performance.
</bodyText>
<subsectionHeader confidence="0.991898">
3.1 Testsuite Completeness
</subsectionHeader>
<bodyText confidence="0.999975064516129">
If the disjunct coverage of a testsuite is 1 for some
grammar, the testsuite is complete w.r.t. this gram-
mar. Such a testsuite can reliably be used to mon-
itor changes in the grammar: Any reduction in the
grammar&apos;s coverage will show up in the failure of
some test case (for negative test cases, cf. Sec.4).
If there is no complete testsuite, one can - via in-
strumentation - identify disjuncts in the grammar
for which no test case exists. There might be either
(i) appropriate, but untested, disjuncts calling for
the addition of a test case, or (ii) inappropriate dis-
juncts, for which one cannot construct a grammat-
ical test case relying on them (e.g., left-overs from
rearranging the grammar). Grammar instrumenta-
tion singles out all untested disjuncts automatically,
but cases (i) and (ii) have to be distinguished man-
ually.
Checking completeness of our local testsuite of
1787 items, we found that only 1456 out of 3730
grammar disjuncts in our German grammar were
tested, yielding Tdis = 0.39 (the TSNLP testsuite
containing 1093 items tests only 1081 disjuncts,
yielding Tdis = 0.28).3 Fig.3 shows an example
of a gap in our testsuite (there are no examples of
circumpositions), while Fig.4 shows an inapproppri-
ate disjunct thus discovered (the category ADVadj
has been eliminated in the lexicon, but not in all
rules). Another error class is illustrated by Fig.5,
which shows a rule that can never be used due to an
LFG coherence violation; the grammar is inconsis-
tent here.4
</bodyText>
<footnote confidence="0.992466571428572">
3There are, of course, unparsed but grammatical test cases
in both testsuites, which have not been taken into account
in these figures. This explains the difference to the overall
number of 1582 items in the German TSNLP testsuite.
4Test cases using a free dative pronoun may be in the test-
suite, but receive no analysis since the grammatical function
FREEDAT is not defined as such in the configuration section.
</footnote>
<page confidence="0.991033">
327
</page>
<figure confidence="0.560792">
PPstd. Pprae 4.=1.;
NPstd OBJ);
{ e DISJUNCT-011 E o*;
Pcircum4.=1;
DISJUNCT-012 E o*
&amp;quot;unused disjunct&amp;quot;;
</figure>
<figureCaption confidence="0.975472">
Figure 3: Appropriate untested disjunct
</figureCaption>
<figure confidence="0.951518444444444">
ADVP e DISJUNCT-021 E o*;
ADVadj4.=t
DISJUNCT-022 E o*
&amp;quot;unused disjunct&amp;quot;;
ADVstd 1=T
DISJUNCT-023 E o*
&amp;quot;unused disjunct&amp;quot;;
I
}
</figure>
<figureCaption confidence="0.999446">
Figure 4: Inappropriate disjunct
</figureCaption>
<subsectionHeader confidence="0.999208">
3.2 Testsuite Economy
</subsectionHeader>
<bodyText confidence="0.999917125">
Besides being complete, a testsuite must be econom-
ical, i.e., contain as few items as possible without
sacrificing its diagnostic capabilities. Instrumenta-
tion can identify redundant test cases. Three criteria
can be applied in determining whether a test case is
redundant:
similarity There is a set of other test cases which
jointly rely on all disjunct on which the test case
under consideration relies.
equivalence There is a single test case which relies
on exactly the same combination(s) of disjuncts.
strict equivalence There is a single test case
which is equivalent to and, additionally, relies
on the disjuncts exactly as often as, the test
case under consideration.
For all criteria, lexical and structural ambiguities
must be taken into account. Fig.6 shows some equiv-
alent test cases derived from our testsuite: Exam-
ple 1 illustrates the distinction between equivalence
and strict equivalence; the test cases contain differ-
ent numbers of attributive adjectives, but are nev-
ertheless considered equivalent. Example 2 shows
that our grammar does not make any distinction be-
tween adverbial usage and secondary (subject or ob-
ject) predication. Example 3 shows test cases which
should not be considered equivalent, and is discussed
below.
The reduction we achieved in size and processing
time is shown in Table 1, which contains measure-
ments for a test run containing only the parseable
test cases, one without equivalent test cases (for ev-
ery set of equivalent test cases, one was arbitrar-
</bodyText>
<figure confidence="0.916191571428571">
VPargs {
I PRONstd1.= FREEDAT)
(4. CASE) = dat
(4. PRON-TYPE) =-- pers
OBJ2)
DISJUNCT-041 E o*
&amp;quot;unused disjunct&amp;quot;;
</figure>
<figureCaption confidence="0.982979">
Figure 5: Inconsistent disjunct
</figureCaption>
<figure confidence="0.9923074">
1 em guter alter Wein
em guter alter trockener Wein
&apos;a good old (dry) wine&apos;
2 Er ii3t das Schnitzel roh.
Er ifft das Schnitzel nackt.
Er ifft das Schnitzel schnell.
&apos;He eats the schnitzel naked/raw/quickly.&apos;
3 Otto versucht oft zu lachen.
Otto versucht zu lachen.
&apos;Otto (often) tries to laugh.&apos;
</figure>
<figureCaption confidence="0.999992">
Figure 6: Sets of equivalent test cases
</figureCaption>
<bodyText confidence="0.999889875">
ily selected), and one without similar test cases.
The last was constructed using a simple heuristic:
Starting with the sentence relying on the most dis-
juncts, working towards sentences relying on fewer
disjuncts, a sentence was selected only if it relied on
a disjunct on which no previously selected sentence
relied. Assuming that a disjunct working correctly
once will work correctly more than once, we did not
consider strict equivalence.
We envisage the following use of this redundancy
detection: There clearly are linguistic reasons to dis-
tinguish all test cases in example 2, so they cannot
simply be deleted from the testsuite. Rather, their
equivalence indicates that the grammar is not yet
perfect (or never will be, if it remains purely syn-
tactic). Such equivalences could be interpreted as
</bodyText>
<figure confidence="0.498989833333333">
a) CD Q.) c.)
v.) a.) &gt;
&lt;13 &apos;4:5 •
cn ca 4:4
G)
TSNLP testsuite
</figure>
<bodyText confidence="0.782975428571429">
parseable 1093 100% 1537 100% 3561
no equivalents 783 71% 665.3 43%
no similar cases 214 19% 128.5 8%
local testsuite
parseable 1787 100% 1213 100% 5480
no equivalents 1600 89% 899.5 74%
no similar cases 331 18% 175.0 14%
</bodyText>
<tableCaption confidence="0.998038">
Table 1: Reduction of Testsuites
</tableCaption>
<page confidence="0.975373">
328
</page>
<figure confidence="0.999003307692308">
1 Der Test fallt leicht .
Die schlafen .
3 Man schlafen.
Dieser schlafen .
Ich schlafen .
Der schlafen .
Jeder schlafen .
Derjenige schlafen .
Jener schlafen .
Keiner schlafen .
Derselbe schlafen .
Er schlafen .
Irgendjemand schlafen
2 Dieselbe schlafen .
Das schlafen
Eines schlafen .
Jede schlafen .
Dieses schlafen .
Eine schlafen .
Meins schlafen .
Dasjenige schlafen.
Jedes schlafen .
Diejenige schlafen .
Jenes schlafen .
Keines schlafen .
Dasselbe schlafen .
</figure>
<bodyText confidence="0.987953222222222">
a reminder which linguistic distinctions need to be
incorporated into the grammar. Thus, this level of
redundancy may drive your grammar development
agenda. The level of equivalence can be taken as
a limited interaction test: These test cases repre-
sent one complete selection of grammar disjuncts,
and (given the grammar) there is nothing we can
gain by checking a test case if an equivalent one was
tested. Thus, this level of redundancy may be used
for ensuring the quality of grammar changes prior
to their incorporation into the production version of
the grammar. The level of similarity contains much
less test cases, and does not test any (systematic)
interaction between disjuncts. Thus, it may be used
during development as a quick rule-of-thumb proce-
dure detecting serious errors only.
Coming back to example 3 in Fig.6, building
equivalence classes also helps in detecting grammar
errors: If, according to the grammar, two cases are
equivalent which actually aren&apos;t, the grammar is in-
correct. Example 3 shows two test cases which are
syntactically different in that the first contains the
adverbial oft, while the other doesn&apos;t. The reason
why they are equivalent is an incorrect rule that as-
signs an incorrect reading to the second test case,
where the infinitival particle &amp;quot;zu&amp;quot; functions as an
adverbial.
</bodyText>
<sectionHeader confidence="0.997312" genericHeader="method">
4 Negative Test Cases
</sectionHeader>
<bodyText confidence="0.999638821428572">
To control overgeneration, appropriately marked un-
grammatical sentences are important in every test-
suite. Instrumentation as proposed here only looks
at successful parses, but can still be applied in this
context: If an ungrammatical test case receives an
analysis, instrumentation informs us about the dis-
juncts used in the incorrect analysis. One (or more)
of these disjuncts must be incorrect, or the sentence
would not have received a solution. We exploit this
information by accumulation across the entire test
suite, looking for disjuncts that appear in unusu-
ally high proportion in parseable ungrammatical test
cases.
In this manner, six grammar disjuncts are singled
out by the parseable ungrammatical test cases in the
TSNLP testsuite. The most prominent disjunct ap-
pears in 26 sentences (listed in Fig.7), of which group
1 is really grammatical and the rest fall into two
groups: A partial VP with object NP, interpreted
as an imperative sentence (group 2), and a weird
interaction with the tokenizer incorrectly handling
capitalization (group 3).
Far from being conclusive, the similarity of these
sentences derived from a suspicious grammar dis-
junct, and the clear relation of the sentences to only
two exactly specifiable grammar errors make it plau-
sible that this approach is very promising in reducing
overgeneration.
</bodyText>
<figureCaption confidence="0.957587">
Figure 7: Sentences relying on a suspicious disjunct
</figureCaption>
<sectionHeader confidence="0.9760065" genericHeader="method">
5 Other Approaches to Testsuite
Construction
</sectionHeader>
<bodyText confidence="0.999916428571429">
Although there are a number of efforts to construct
reusable large-coverage testsuites, none has to my
knowledge explored how existing grammars could be
used for this purpose.
Starting with (Flickinger et al., 1987), testsuites
have been drawn up from a linguistic viewpoint, &amp;quot;in-
formed by [the] study of linguistics and [reflecting]
the grammatical issues that linguists have concerned
themselves with&amp;quot; (Flickinger et al., 1987„ p.4). Al-
though the question is not explicitly addressed in
(Balkan et al., 1994), all the testsuites reviewed there
also seem to follow the same methodology. The
TSNLP project (Lehmann and Oepen, 1996) and
its successor DiET (Netter et al., 1998), which built
large multilingual testsuites, likewise fall into this
category.
The use of corpora (with various levels of annota-
tion) has been studied, but even here the recommen-
dations are that mucfi manual work is required to
turn corpus examples into test cases (e.g., (Balkan
and Fouvry, 1995)). The reason given is that cor-
pus sentences neither contain linguistic phenomena
in isolation, nor do they contain systematic varia-
tion. Corpora thus are used only as an inspiration.
(Oepen and Flickinger, 1998) stress the inter-
dependence between application and testsuite, but
don&apos;t comment on the relation between grammar
and testsuite.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999571">
The approach presented tries to make available the
linguistic knowledge that went into the grammar for
development of testsuites. Grammar development
and testsuite compilation are seen as complemen-
tary and interacting processes, not as isolated mod-
ules. We have seen that even large testsuites cover
only a fraction of existing large-coverage grammars,
</bodyText>
<page confidence="0.996727">
329
</page>
<bodyText confidence="0.999920961538462">
and presented evidence that there is a considerable
amount of redundancy within existing testsuites.
To empirically validate that the procedures out-
lined above improve grammar and testsuite, careful
grammar development is required. Based on the in-
formation derived from parsing with instrumented
grammars, the changes and their effects need to be
evaluated. In addition to this empirical work, instru-
mentation can be applied to other areas in Gram-
mar Engineering, e.g., to detect sources of spurious
ambiguities, to select sample sentences relying on a
disjunct for documentation, or to assist in the con-
struction of additional test cases. Methodological
work is also required for the definition of a practical
and intuitive criterion to measure limited interaction
coverage.
Each existing grammar development environment
undoubtely offers at least some basic tools for com-
paring the grammar&apos;s coverage with a testsuite. Re-
grettably, these tools are seldomly presented pub-
licly (which accounts for the short list of such refer-
ences). It is my belief that the thorough discussion
of such infrastructure items (tools and methods) is
of more immediate importance to the quality of the
lingware than the discussion of open linguistic prob-
lems.
</bodyText>
<sectionHeader confidence="0.997518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995245777777778">
L. Balkan and F. Fouvry. 1995. Corpus-based test
suite generation. TSNLP-WP 2.2, University of
Essex.
L. Balkan, S. Meijer, D. Arnold, D. Estival, and
K. Falkedal. 1994. Test Suite Design Annotation
Scheme. TSNLP-WP2.2, University of Essex.
F. Ciravegna, A. LaveIli, D. Petrelli, and F. Pianesi.
1998. Developing language reesources and appli-
cations with geppetto. In Proc. 1st Int&apos;l Conf. on
Language Resources and Evaluation, pages 619-
625. Granada/Spain, 28-30 May 1998.
D. Flickinger, J. Nerbonne, I. Sag, and T. Wa-
sow. 1987. Toward Evaluation of NLP Systems.
Hewlett-Packard Laboratories, Palo Alto/CA.
A. Frank, T.H. King, J. Kuhn, and J. Maxwell.
1998. Optimality theory style constraint ranking
in large-scale lfg gramma. In Proc. of the LFG98
Conference. Brisbane/AUS, Aug 1998, CSLI On-
line Publications.
W.C. Hetzel. 1988. The complete guide to software
testing. QED Information Sciences, Inc. Welles-
ley/MA 02181.
R.M. Kaplan and J. Bresnan. 1982. Lexical-
functional grammar: A formal system for gram-
matical representation. In J. Bresnan and R.M.
Kaplan, editors, The Mental Representation of
Grammatical Relations, pages 173-281. Cam-
bridge, MA: MIT Press.
S. Lehmann and S. Oepen. 1996. TSNLP - Test
Suites for Natural Language Processing. In Proc.
16th Int&apos;l Conf. on Computational Linguistics,
pages 711-716. Copenhagen/DK.
P. Liggesmeyer. 1990. Modultest und Modulverifika-
hon. Angewandte Informatik 4. Mannheim: BI
Wissenschaftsverlag.
K. Netter, S. Armstrong, T. Kiss, J. Klein, and
S. Lehman. 1998. Diet - diagnostic and eval-
uation tools for nlp applications. In Proc. 1st
Int&apos;l Conf. on Language Resources and Evalua-
tion, pages 573-579. Granada/Spain, 28-30 May
1998.
S. Oepen and D.P. Flickinger. 1998. Towards sys-
tematic grammar profiling:test suite techn. 10
years afte. Journal of Computer Speech and Lan-
guage, 12:411-435.
</reference>
<page confidence="0.99836">
330
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797860">
<title confidence="0.999943">Improving Testsuites via Instrumentation</title>
<author confidence="0.997615">Norbert Broker</author>
<address confidence="0.933490333333333">Eschenweg 3 69231 Rauenberg Germany</address>
<email confidence="0.999948">norbert.broeker@sap.com</email>
<abstract confidence="0.999556230769231">This paper explores the usefulness of a technique from software engineering, namely code instrumentation, for the development of large-scale natural language grammars. Information about the usage of grammar rules in test sentences is used to detect untested rules, redundant test sentences, and likely causes of overgeneration. Results show that less than half of a large-coverage grammar for German is actually tested by two large testsuites, and that 10testing time is redundant. The methodology applied can be seen as a re-use of grammar writing knowledge for testsuite compilation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Balkan</author>
<author>F Fouvry</author>
</authors>
<title>Corpus-based test suite generation. TSNLP-WP 2.2,</title>
<date>1995</date>
<institution>University of Essex.</institution>
<contexts>
<context position="21320" citStr="Balkan and Fouvry, 1995" startWordPosition="3396" endWordPosition="3399"> issues that linguists have concerned themselves with&amp;quot; (Flickinger et al., 1987„ p.4). Although the question is not explicitly addressed in (Balkan et al., 1994), all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that mucfi manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995)). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don&apos;t comment on the relation between grammar and testsuite. 6 Conclusion The approach presented tries to make available the linguistic knowledge that went into the grammar for development of testsuites. Grammar development and testsuite compilation are seen as complementary and interacting processes, not as isolated mo</context>
</contexts>
<marker>Balkan, Fouvry, 1995</marker>
<rawString>L. Balkan and F. Fouvry. 1995. Corpus-based test suite generation. TSNLP-WP 2.2, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Balkan</author>
<author>S Meijer</author>
<author>D Arnold</author>
<author>D Estival</author>
<author>K Falkedal</author>
</authors>
<title>Test Suite Design Annotation Scheme.</title>
<date>1994</date>
<tech>TSNLP-WP2.2,</tech>
<institution>University of Essex.</institution>
<contexts>
<context position="20857" citStr="Balkan et al., 1994" startWordPosition="3321" endWordPosition="3324">rgeneration. Figure 7: Sentences relying on a suspicious disjunct 5 Other Approaches to Testsuite Construction Although there are a number of efforts to construct reusable large-coverage testsuites, none has to my knowledge explored how existing grammars could be used for this purpose. Starting with (Flickinger et al., 1987), testsuites have been drawn up from a linguistic viewpoint, &amp;quot;informed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with&amp;quot; (Flickinger et al., 1987„ p.4). Although the question is not explicitly addressed in (Balkan et al., 1994), all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that mucfi manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995)). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation.</context>
</contexts>
<marker>Balkan, Meijer, Arnold, Estival, Falkedal, 1994</marker>
<rawString>L. Balkan, S. Meijer, D. Arnold, D. Estival, and K. Falkedal. 1994. Test Suite Design Annotation Scheme. TSNLP-WP2.2, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ciravegna</author>
<author>A LaveIli</author>
<author>D Petrelli</author>
<author>F Pianesi</author>
</authors>
<title>Developing language reesources and applications with geppetto.</title>
<date>1998</date>
<booktitle>In Proc. 1st Int&apos;l Conf. on Language Resources and Evaluation,</booktitle>
<pages>619--625</pages>
<contexts>
<context position="1772" citStr="Ciravegna et al., 1998" startWordPosition="257" endWordPosition="260">e adaptation of a technique from Software Engineering, namely code instrumentation, to grammar development. Instrumentation is based on the simple idea of marking any piece of code used in processing, and evaluating this usage information afterwards. The application I present here is the evaluation and improvement of grammar and testsuites; other applications are possible. 1.1 Software Engineering vs. Grammar Engineering Both software and grammar development are similar processes: They result in a system transforming some input into some output, based on a functional specification (e.g., cf. (Ciravegna et al., 1998) for the application of a particular software design methodology to linguistic engineering). Although Grammar &apos;The work reported here was conducted during my time at the Institut fiir Maschinelle Sprachverarbeitung (IMS), Stuttgart University, Germany. Engineering usually is not based on concrete specifications, research from linguistics provides an informal specification. Software Engineering developed many methods to assess the quality of a program, ranging from static analysis of the program code to dynamic testing of the program&apos;s behavior. Here, we adapt dynamic testing, which means runni</context>
</contexts>
<marker>Ciravegna, LaveIli, Petrelli, Pianesi, 1998</marker>
<rawString>F. Ciravegna, A. LaveIli, D. Petrelli, and F. Pianesi. 1998. Developing language reesources and applications with geppetto. In Proc. 1st Int&apos;l Conf. on Language Resources and Evaluation, pages 619-625. Granada/Spain, 28-30 May 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>J Nerbonne</author>
<author>I Sag</author>
<author>T Wasow</author>
</authors>
<title>Toward Evaluation of NLP Systems. Hewlett-Packard Laboratories,</title>
<date>1987</date>
<location>Palo Alto/CA.</location>
<contexts>
<context position="20563" citStr="Flickinger et al., 1987" startWordPosition="3276" endWordPosition="3279"> handling capitalization (group 3). Far from being conclusive, the similarity of these sentences derived from a suspicious grammar disjunct, and the clear relation of the sentences to only two exactly specifiable grammar errors make it plausible that this approach is very promising in reducing overgeneration. Figure 7: Sentences relying on a suspicious disjunct 5 Other Approaches to Testsuite Construction Although there are a number of efforts to construct reusable large-coverage testsuites, none has to my knowledge explored how existing grammars could be used for this purpose. Starting with (Flickinger et al., 1987), testsuites have been drawn up from a linguistic viewpoint, &amp;quot;informed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with&amp;quot; (Flickinger et al., 1987„ p.4). Although the question is not explicitly addressed in (Balkan et al., 1994), all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has be</context>
</contexts>
<marker>Flickinger, Nerbonne, Sag, Wasow, 1987</marker>
<rawString>D. Flickinger, J. Nerbonne, I. Sag, and T. Wasow. 1987. Toward Evaluation of NLP Systems. Hewlett-Packard Laboratories, Palo Alto/CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frank</author>
<author>T H King</author>
<author>J Kuhn</author>
<author>J Maxwell</author>
</authors>
<title>Optimality theory style constraint ranking in large-scale lfg gramma.</title>
<date>1998</date>
<booktitle>In Proc. of the LFG98 Conference. Brisbane/AUS,</booktitle>
<publisher>CSLI Online Publications.</publisher>
<contexts>
<context position="9369" citStr="Frank et al., 1998" startWordPosition="1449" endWordPosition="1452"> merely unify, and there is no way of counting. Tdis 326 VP =V .1.=-1; e DISJUNCT-001 E 0*; I NP 1.=( OBJ) DISJUNCT-002 E o*; { e DISJUNCT-003 E o*; I PP+{.1,= OBL) DISJUNCT-004 E o*; I (f ADJUNCT) DISJUNCT-005 E o*J. Figure 2: Instrumented rule Therefore, we have used a special feature of our grammar development environment: Following the LFG spirit of different representation levels associated with each solution (so-called projections), it provides for a multiset of symbols associated with the complete solution, where structural embedding plays no role (so-called optimality projection; see (Frank et al., 1998)). In this way, from the root node of each solution the set of all disjuncts used can be collected, together with a usage count. Fig. 2 shows the rule from Fig.1 with such an instrumentation; equations of the form DISJUNCT-nnE o* express membership of the disjunct-specific atom DISJUNCT-nn in the sentence&apos;s multiset of disjunct markers. 2.3 Processing Tools Tool support is mandatory for a scenario such as instrumentation: Nobody will manually add equations such as those in Fig. 2 to several hundred rules. Based on the format of the grammar rules, an algorithm instrumenting a grammar can be wri</context>
</contexts>
<marker>Frank, King, Kuhn, Maxwell, 1998</marker>
<rawString>A. Frank, T.H. King, J. Kuhn, and J. Maxwell. 1998. Optimality theory style constraint ranking in large-scale lfg gramma. In Proc. of the LFG98 Conference. Brisbane/AUS, Aug 1998, CSLI Online Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Hetzel</author>
</authors>
<title>The complete guide to software testing.</title>
<date>1988</date>
<journal>QED Information Sciences, Inc. Wellesley/MA</journal>
<pages>02181</pages>
<contexts>
<context position="2635" citStr="Hetzel, 1988" startWordPosition="385" endWordPosition="386">. Engineering usually is not based on concrete specifications, research from linguistics provides an informal specification. Software Engineering developed many methods to assess the quality of a program, ranging from static analysis of the program code to dynamic testing of the program&apos;s behavior. Here, we adapt dynamic testing, which means running the implemented program against a set of test cases. The test cases are designed to maximize the probability of detecting errors in the program, i.e., incorrect conditions, incompatible assumptions on subsequent branches, etc. (for overviews, cf. (Hetzel, 1988; Liggesmeyer, 1990)). 1.2 Instrumentation in Grammar Engineering How can we fruitfully apply the idea of measuring the coverage of a set of test cases to grammar development? I argue that by exploring the relation between grammar and testsuite, one can improve both of them. Even the traditional usage of testsuites to indicate grammar gaps or overgeneration can profit from a precise indication of the grammar rules used to parse the sentences (cf. Sec.4). Conversely, one may use the grammar to improve the testsuite, both in terms of its coverage (cf. Sec.3.1) and its economy (cf. Sec.3.2). View</context>
</contexts>
<marker>Hetzel, 1988</marker>
<rawString>W.C. Hetzel. 1988. The complete guide to software testing. QED Information Sciences, Inc. Wellesley/MA 02181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexicalfunctional grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In J. Bresnan and R.M. Kaplan, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="6449" citStr="Kaplan and Bresnan, 1982" startWordPosition="988" endWordPosition="992">age The disjunct coverage of a testsuite is the quotient number of disjuncts tested number of disjuncts in grammar 2Notation: ?poi+ represent optionality/iteration including/excluding zero occurrences on categories. Annotations to a category specify equality (=) or set membership (E) of feature values, or non-existence of features (--.); they are terminated by a semicolon ( ; ). Disjunctions are given in braces ({ ... I ...}). -t- (4,) are metavariables representing the feature structure corresponding to the mother (daughter) of the rule. Comments are enclosed in quotation marks (&amp;quot; . &amp;quot;). Cf. (Kaplan and Bresnan, 1982) for an introduction to LFG notation. where a disjunct is either a phrase-structure alternative, or an annotation alternative. Optional constituents (and equations, if the formalism allows them) have to be treated as a disjunction of the constituent and an empty category (cf. the instrumented rule in Fig.2 for an example). Instead of considering disjuncts in isolation, one might take their interaction into account. The most complete test criterion, doing this to the fullest extent possible, can be defined as follows: interaction coverage The interaction coverage of a testsuite is the quotient </context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>R.M. Kaplan and J. Bresnan. 1982. Lexicalfunctional grammar: A formal system for grammatical representation. In J. Bresnan and R.M. Kaplan, editors, The Mental Representation of Grammatical Relations, pages 173-281. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lehmann</author>
<author>S Oepen</author>
</authors>
<title>TSNLP - Test Suites for Natural Language Processing.</title>
<date>1996</date>
<booktitle>In Proc. 16th Int&apos;l Conf. on Computational Linguistics,</booktitle>
<pages>711--716</pages>
<publisher>Copenhagen/DK.</publisher>
<contexts>
<context position="20978" citStr="Lehmann and Oepen, 1996" startWordPosition="3340" endWordPosition="3343">gh there are a number of efforts to construct reusable large-coverage testsuites, none has to my knowledge explored how existing grammars could be used for this purpose. Starting with (Flickinger et al., 1987), testsuites have been drawn up from a linguistic viewpoint, &amp;quot;informed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with&amp;quot; (Flickinger et al., 1987„ p.4). Although the question is not explicitly addressed in (Balkan et al., 1994), all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that mucfi manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995)). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between applicatio</context>
</contexts>
<marker>Lehmann, Oepen, 1996</marker>
<rawString>S. Lehmann and S. Oepen. 1996. TSNLP - Test Suites for Natural Language Processing. In Proc. 16th Int&apos;l Conf. on Computational Linguistics, pages 711-716. Copenhagen/DK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liggesmeyer</author>
</authors>
<title>Modultest und Modulverifikahon. Angewandte Informatik 4.</title>
<date>1990</date>
<publisher>BI Wissenschaftsverlag.</publisher>
<location>Mannheim:</location>
<contexts>
<context position="2655" citStr="Liggesmeyer, 1990" startWordPosition="387" endWordPosition="388">usually is not based on concrete specifications, research from linguistics provides an informal specification. Software Engineering developed many methods to assess the quality of a program, ranging from static analysis of the program code to dynamic testing of the program&apos;s behavior. Here, we adapt dynamic testing, which means running the implemented program against a set of test cases. The test cases are designed to maximize the probability of detecting errors in the program, i.e., incorrect conditions, incompatible assumptions on subsequent branches, etc. (for overviews, cf. (Hetzel, 1988; Liggesmeyer, 1990)). 1.2 Instrumentation in Grammar Engineering How can we fruitfully apply the idea of measuring the coverage of a set of test cases to grammar development? I argue that by exploring the relation between grammar and testsuite, one can improve both of them. Even the traditional usage of testsuites to indicate grammar gaps or overgeneration can profit from a precise indication of the grammar rules used to parse the sentences (cf. Sec.4). Conversely, one may use the grammar to improve the testsuite, both in terms of its coverage (cf. Sec.3.1) and its economy (cf. Sec.3.2). Viewed this way, testsui</context>
</contexts>
<marker>Liggesmeyer, 1990</marker>
<rawString>P. Liggesmeyer. 1990. Modultest und Modulverifikahon. Angewandte Informatik 4. Mannheim: BI Wissenschaftsverlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Netter</author>
<author>S Armstrong</author>
<author>T Kiss</author>
<author>J Klein</author>
<author>S Lehman</author>
</authors>
<title>Diet - diagnostic and evaluation tools for nlp applications.</title>
<date>1998</date>
<booktitle>In Proc. 1st Int&apos;l Conf. on Language Resources and Evaluation,</booktitle>
<pages>573--579</pages>
<contexts>
<context position="21023" citStr="Netter et al., 1998" startWordPosition="3348" endWordPosition="3351">sable large-coverage testsuites, none has to my knowledge explored how existing grammars could be used for this purpose. Starting with (Flickinger et al., 1987), testsuites have been drawn up from a linguistic viewpoint, &amp;quot;informed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with&amp;quot; (Flickinger et al., 1987„ p.4). Although the question is not explicitly addressed in (Balkan et al., 1994), all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that mucfi manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995)). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don&apos;t comment on the rel</context>
</contexts>
<marker>Netter, Armstrong, Kiss, Klein, Lehman, 1998</marker>
<rawString>K. Netter, S. Armstrong, T. Kiss, J. Klein, and S. Lehman. 1998. Diet - diagnostic and evaluation tools for nlp applications. In Proc. 1st Int&apos;l Conf. on Language Resources and Evaluation, pages 573-579. Granada/Spain, 28-30 May 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>D P Flickinger</author>
</authors>
<title>Towards systematic grammar profiling:test suite techn. 10 years afte.</title>
<date>1998</date>
<journal>Journal of Computer Speech and Language,</journal>
<pages>12--411</pages>
<contexts>
<context position="21532" citStr="Oepen and Flickinger, 1998" startWordPosition="3429" endWordPosition="3432"> follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that mucfi manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995)). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don&apos;t comment on the relation between grammar and testsuite. 6 Conclusion The approach presented tries to make available the linguistic knowledge that went into the grammar for development of testsuites. Grammar development and testsuite compilation are seen as complementary and interacting processes, not as isolated modules. We have seen that even large testsuites cover only a fraction of existing large-coverage grammars, 329 and presented evidence that there is a considerable amount of redundancy within existing testsuites. T</context>
</contexts>
<marker>Oepen, Flickinger, 1998</marker>
<rawString>S. Oepen and D.P. Flickinger. 1998. Towards systematic grammar profiling:test suite techn. 10 years afte. Journal of Computer Speech and Language, 12:411-435.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>