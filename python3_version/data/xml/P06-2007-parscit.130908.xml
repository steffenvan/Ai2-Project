<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.935768">
N Semantic Classes are Harder than Two
</title>
<figure confidence="0.3803318">
Rosie Jones
Yahoo! Research
3333 Empire Ave.
Burbank, CA 91504
jonesr@yahoo-inc.com
</figure>
<author confidence="0.677886">
Ben Carterette*
</author>
<affiliation confidence="0.7572275">
CIIR
University of Massachusetts
</affiliation>
<address confidence="0.708695">
Amherst, MA 01003
</address>
<email confidence="0.997363">
carteret@cs.umass.edu
</email>
<author confidence="0.944628">
Wiley Greiner*
</author>
<affiliation confidence="0.83154">
Los Angeles Software Inc.
</affiliation>
<address confidence="0.874524">
1329 Pine Street
Santa Monica, CA 90405
</address>
<email confidence="0.998012">
w.greiner@lasoft.com
</email>
<author confidence="0.832892">
Cory Barr
</author>
<affiliation confidence="0.653915">
Yahoo! Research
3333 Empire Ave.
</affiliation>
<address confidence="0.898595">
Burbank, CA 91504
</address>
<email confidence="0.92">
barrc@yahoo-inc.com
</email>
<sectionHeader confidence="0.991161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857076923077">
We show that we can automatically clas-
sify semantically related phrases into 10
classes. Classification robustness is im-
proved by training with multiple sources
of evidence, including within-document
cooccurrence, HTML markup, syntactic
relationships in sentences, substitutability
in query logs, and string similarity. Our
work provides a benchmark for automatic
n-way classification into WordNet’s se-
mantic classes, both on a TREC news cor-
pus and on a corpus of substitutable search
query phrases.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952235294118">
Identifying semantically related phrases has been
demonstrated to be useful in information retrieval
(Anick, 2003; Terra and Clarke, 2004) and spon-
sored search (Jones et al., 2006). Work on seman-
tic entailment often includes lexical entailment as
a subtask (Dagan et al., 2005).
We draw a distinction between the task of iden-
tifying terms which are topically related and iden-
tifying the specific semantic class. For example,
the terms “dog”, “puppy”, “canine”, “schnauzer”,
“cat” and “pet” are highly related terms, which
can be identified using techniques that include
distributional similarity (Lee, 1999) and within-
document cooccurrence measures such as point-
wise mutual information (Turney et al., 2003).
These techniques, however, do not allow us to dis-
tinguish the more specific relationships:
</bodyText>
<listItem confidence="0.927024">
• hypernym(dog,puppy)
</listItem>
<footnote confidence="0.4209715">
*This work was carried out while these authors were at
Yahoo! Research.
</footnote>
<listItem confidence="0.998755">
• hyponym(dog,canine)
• coordinate(dog,cat)
</listItem>
<bodyText confidence="0.997734">
Lexical resources such as WordNet (Miller,
1995) are extremely useful, but are limited by be-
ing manually constructed. They do not contain se-
mantic class relationships for the many new terms
we encounter in text such as web documents, for
example “mp3 player” or “ipod”. We can use
WordNet as training data for such classification to
the extent that the training on pairs found in Word-
Net and testing on pairs found outside WordNet
provides accurate generalization.
We describe a set of features used to train n-
way supervised machine-learned classification of
semantic classes for arbitrary pairs of phrases. Re-
dundancy in the sources of our feature informa-
tion means that we are able to provide coverage
over an extremely large vocabulary of phrases. We
contrast this with techniques that require parsing
of natural language sentences (Snow et al., 2005)
which, while providing reasonable performance,
can only be applied to a restricted vocabulary of
phrases cooccuring in sentences.
Our contributions are:
</bodyText>
<listItem confidence="0.998421538461538">
• Demonstration that binary classification re-
moves the difficult cases of classification into
closely related semantic classes
• Demonstration that dependency parser paths
are inadequate for semantic classification into
7 WordNet classes on TREC news corpora
• A benchmark of 10-class semantic classifica-
tion over highly substitutable query phrases
• Demonstration that training a classifier us-
ing WordNet for labeling does not generalize
well to query pairs
• Demonstration that much of the performance
in classification can be attained using only
</listItem>
<page confidence="0.992373">
49
</page>
<note confidence="0.7791815">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 49–56,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.687726">
syntactic features
</bodyText>
<listItem confidence="0.992111">
• A learning curve for classification of query
phrase pairs that suggests the primary bottle-
neck is manually labeled training instances:
we expect our benchmark to be surpassed.
</listItem>
<sectionHeader confidence="0.643656" genericHeader="introduction">
2 Relation to Previous Work
</sectionHeader>
<bodyText confidence="0.998359769230769">
Snow et al. (2005) demonstrated binary classi-
fication of hypernyms and non-hypernyms using
WordNet (Miller, 1995) as a source of training la-
bels. Using dependency parse tree paths as fea-
tures, they were able to generalize from WordNet
labelings to human labelings.
Turney et al. (2003) combined features to an-
swer multiple-choice synonym questions from the
TOEFL test and verbal analogy questions from
the SAT college entrance exam. The multiple-
choice questions typically do not consist of mul-
tiple closely related terms. A typical example is
given by Turney:
</bodyText>
<listItem confidence="0.985641">
• hidden:: (a) laughable (c) ancient
(b) veiled (d) revealed
</listItem>
<bodyText confidence="0.939041857142857">
Note that only (b) and (d) are at all related to the
term, so the algorithm only needs to distinguish
antonyms from synonyms, not synonyms from say
hypernyms.
We use as input phrase pairs recorded in query
logs that web searchers substitute during search
sessions. We find much more closely related
</bodyText>
<listItem confidence="0.99557875">
phrases: (a) secret (e) hiden
• hidden:: (b) hidden camera (f) voyeur
(c) hidden cam (g) hide
(d) spy
</listItem>
<bodyText confidence="0.998749363636364">
This set contains a context-dependent synonym,
topically related verbs and nouns, and a spelling
correction. All of these could cooccur on web
pages, so simple cooccurrence statistics may not
be sufficient to classify each according to the se-
mantic type.
We show that the techniques used to perform
binary semantic classification do not work as well
when extended to a full n-way semantic classifi-
cation. We show that using a variety of features
performs better than any feature alone.
</bodyText>
<sectionHeader confidence="0.975964" genericHeader="method">
3 Identifying Candidate Phrases for
Classification
</sectionHeader>
<bodyText confidence="0.9713155">
In this section we introduce the two data sources
we use to extract sets of candidate related phrases
for classification: a TREC-WordNet intersection
and query logs.
</bodyText>
<subsectionHeader confidence="0.9991065">
3.1 Noun-Phrase Pairs Cooccuring in TREC
News Sentences
</subsectionHeader>
<bodyText confidence="0.999982476190476">
The first is a data-set derived from TREC news
corpora and WordNet used in previous work for
binary semantic class classification (Snow et al.,
2005). We extract two sets of candidate-related
pairs from these corpora, one restricted and one
more complete set.
Snow et al. obtained training data from the inter-
section of noun-phrases cooccuring in sentences in
a TREC news corpus and those that can be labeled
unambiguously as hypernyms or non-hypernyms
using WordNet. We use a restricted set since in-
stances selected in the previous work are a subset
of the instances one is likely to encounter in text.
The pairs are generally either related in one type
of relationship, or completely unrelated.
In general we may be able to identify related
phrases (for example with distributional similarity
(Lee, 1999)), but would like to be able to automat-
ically classify the related phrases by the type of
the relationship. For this task we identify a larger
set of candidate-related phrases.
</bodyText>
<subsectionHeader confidence="0.999869">
3.2 Query Log Data
</subsectionHeader>
<bodyText confidence="0.999963428571429">
To find phrases that are similar or substitutable for
web searchers, we turn to logs of user search ses-
sions. We look at query reformulations: a pair
of successive queries issued by a single user on
a single day. We collapse repeated searches for
the same terms, as well as query pair sequences
repeated by the same user on the same day.
</bodyText>
<subsectionHeader confidence="0.988689">
3.2.1 Substitutable Query Segments
</subsectionHeader>
<bodyText confidence="0.998237666666667">
Whole queries tend to consist of several con-
cepts together, for example “new york  |maps” or
“britney spears  |mp3s”. We identify segments or
phrases using a measure over adjacent terms sim-
ilar to mutual information. Substitutions occur at
the level of segments. For example, a user may
initially search for “britney spears  |mp3s”, then
search for “britney spears  |music”. By aligning
query pairs with a single substituted segment, we
generate pairs of phrases which a user has substi-
tuted. In this example, the phrase “mp3s” was sub-
stituted by the phrase “music”.
Aggregating substitutable pairs over millions of
users and millions of search sessions, we can cal-
culate the probability of each such rewrite, then
</bodyText>
<page confidence="0.991908">
50
</page>
<bodyText confidence="0.997239090909091">
test each pair for statistical significance to elim-
inate phrase rewrites which occurred in a small
number of sessions, perhaps by chance. To test
for statistical significance we use the pair inde-
pendence likelihood ratio, or log-likelihood ratio,
test. This metric tests the hypothesis that the prob-
ability of phrase Q is the same whether phrase α
has been seen or not by calculating the likelihood
of the observed data under a binomial distribution
using probabilities derived using each hypothesis
(Dunning, 1993).
</bodyText>
<equation confidence="0.999906333333333">
L (P(�|α) = P(�|-α))
logA = log
L (P(0|α) =� P(0|-α))
</equation>
<bodyText confidence="0.999708">
A high negative value for A suggests a strong
dependence between query α and query Q.
</bodyText>
<sectionHeader confidence="0.6191975" genericHeader="method">
4 Labeling Phrase Pairs for Supervised
Learning
</sectionHeader>
<bodyText confidence="0.9998636">
We took a random sample of query segment sub-
stitutions from our query logs to be labeled. The
sampling was limited to pairs that were frequent
substitutions for each other to ensure a high prob-
ability of the segments having some relationship.
</bodyText>
<subsectionHeader confidence="0.999372">
4.1 WordNet Labeling
</subsectionHeader>
<bodyText confidence="0.999996">
WordNet is a large lexical database of English
words. In addition to defining several hun-
dred thousand words, it defines synonym sets, or
synsets, of words that represent some underly-
ing lexical concept, plus relationships between
synsets. The most frequent relationships between
noun-phrases are synonym, hyponym, hypernym,
and coordinate, defined in Table 1. We also may
use meronym and holonym, defined as the PART-OF
relationship.
We used WordNet to automatically label the
subset of our sample for which both phrases occur
in WordNet. Any sense of the first segment having
a relationship to any sense of the second would re-
sult in the pair being labeled. Since WordNet con-
tains many other relationships in addition to those
listed above, we group the rest into the other cate-
gory. If the segments had no relationship in Word-
Net, they were labeled no relationship.
</bodyText>
<subsectionHeader confidence="0.998305">
4.2 Segment Pair Labels
</subsectionHeader>
<bodyText confidence="0.99956775">
Phrase pairs passing a statistical test are com-
mon reformulations, but can be of many seman-
tic types. Rieh and Xie (2001) categorized types
of query reformulations, defining 10 general cat-
egories: specification, generalization, synonym,
parallel movement, term variations, operator us-
age, error correction, general resource, special re-
source, and site URLs. We redefine these slightly
to apply to query segments. The summary of the
definitions is shown in Table 1, along with the dis-
tribution in the data of pairs passing the statistical
test.
</bodyText>
<subsectionHeader confidence="0.719677">
4.2.1 Hand Labeling
</subsectionHeader>
<bodyText confidence="0.99997">
More than 90% of phrases in query logs do not
appear in WordNet due to being spelling errors,
web site URLs, proper nouns of a temporal nature,
etc. Six annotators labeled 2,463 segment pairs
selected randomly from our sample. Annotators
agreed on the label of 78% of pairs, with a Kappa
statistic of .74.
</bodyText>
<sectionHeader confidence="0.993574" genericHeader="method">
5 Automatic Classification
</sectionHeader>
<bodyText confidence="0.999906333333333">
We wish to perform supervised classification of
pairs of phrases into semantic classes. To do this,
we will assign features to each pair of phrases,
which may be predictive of their semantic rela-
tionship, then use a machine-learned classifier to
assign weights to these features. In Section 7 we
will look at the learned weights and discuss which
features are most significant for identifying which
semantic classes.
</bodyText>
<subsectionHeader confidence="0.841423">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9986775">
Features for query substitution pairs are extracted
from query logs and web pages.
</bodyText>
<subsubsectionHeader confidence="0.988506">
5.1.1 Web Page / Document Features
</subsubsectionHeader>
<bodyText confidence="0.999680444444444">
We submit the two segments to a web search
engine as a conjunctive query and download the
top 50 results. Each result is converted into an
HTML Document Object Model (DOM) tree and
segmented into sentences.
Dependency Tree Paths The path from the first
segment to the second in a dependency parse
tree generated by MINIPAR (Lin, 1998)
from sentences in which both segments ap-
pear. These were previously used by Snow
et al. (2005). These features were extracted
from web pages in all experiments, except
where we identify that we used TREC news
stories (the same data as used by Snow et al.).
HTML Paths The paths from DOM tree nodes
the first segment appears in to nodes the sec-
ond segment appears in. The value is the
number of times the path occurs with the pair.
</bodyText>
<page confidence="0.995505">
51
</page>
<table confidence="0.7090505">
Class Description Example %
synonym one phrase can be used in place of the other without loss in meaning low cost; cheap 4.2
hypernym X is a hypernym of Y if and only if Y is a X muscle car; mustang 2.0
hyponym X is a hyponym of Y if and only if X is a Y (inverse of hypernymy) lotus; flowers 2.0
coordinate there is some Z such that X and Y are both Zs aquarius; gemini 13.9
generalization X is a generalization of Y if X contains less information about the topic lyrics; santana lyrics 4.8
specialization X is a specification of Y if X contains more information about the topic credit card; card 4.7
spelling change spelling errors, typos, punctuation changes, spacing changes peopl; people 14.9
stemmed form X and Y have the same lemmas ant; ants 3.4
URL change X and Y are related and X or Y is a URL alliance; alliance.com 29.8
other relationship X and Y are related in some other way flagpoles; flags 9.8
no relationship X and Y are not related in any obvious way crypt; tree 10.4
</table>
<tableCaption confidence="0.960779">
Table 1: Semantic relationships between phrases rewritten in query reformulation sessions, along with their prevalence in our
data.
</tableCaption>
<bodyText confidence="0.986964625">
Lexico-syntactic Patterns (Hearst, 1992) A sub-
string occurring between the two segments
extracted from text in nodes in which both
segments appear. In the example fragment
“authors such as Shakespeare”, the feature
is “such as” and the value is the number of
times the substring appears between “author”
and “Shakespeare”.
</bodyText>
<subsubsectionHeader confidence="0.623812">
5.1.2 Query Pair Features
</subsubsectionHeader>
<bodyText confidence="0.994007333333333">
Table 2 summarizes features that are induced
from the query strings themselves or calculated
from query log data.
</bodyText>
<subsectionHeader confidence="0.999374">
5.2 Additional Training Pairs
</subsectionHeader>
<bodyText confidence="0.998520545454545">
We can double our training set by adding for each
pair u1, u2 a new pair u2, u1. The class of the new
pair is the same as the old in all cases but hyper-
nym, hyponym, specification, and generalization,
which are inverted. Features are reversed from
f(u1, u2) to f(u2, u1).
A pair and its inverse have different sets of fea-
tures, so splitting the set randomly into training
and testing sets should not result in resubstitution
error. Nonetheless, we ensure that a pair and its
inverse are not separated for training and testing.
</bodyText>
<subsectionHeader confidence="0.957138">
5.3 Classifier
</subsectionHeader>
<bodyText confidence="0.99979825">
For each class we train a binary one-vs.-all linear-
kernel support vector machine (SVM) using the
optimization algorithm of Keerthi and DeCoste
(2005).
</bodyText>
<subsectionHeader confidence="0.648407">
5.3.1 Meta-Classifier
</subsectionHeader>
<bodyText confidence="0.7708024">
For n-class classification, we calibrate SVM
scores to probabilities using the method described
by Platt (2000). This gives us P(classlpair) for
each pair. The final classification for a pair is
argmaxcla33P(class pair).
</bodyText>
<table confidence="0.9998825">
Source Snow (NIPS 2005) Experiment
Task binary hypernym binary hypernym
Data WordNet-TREC WordNet-TREC
Instance Count 752,311 752,311
Features minipar paths minipar paths
Feature Count 69,592 69,592
Classifier logistic Regression linear SVM
maxF 0.348 0.453
</table>
<tableCaption confidence="0.986583">
Table 3: Snow et al’s (2005) reported performance using lin-
ear regression, and our reproduction of the same experiment,
using a support vector machine (SVM).
</tableCaption>
<subsectionHeader confidence="0.403168">
5.3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999884375">
Binary classifiers are evaluated by ranking in-
stances by classification score and finding the Max
F1 (the harmonic mean of precision and recall;
ranges from 0 to 1) and area under the ROC curve
(AUC; ranges from 0.5 to 1 with at least 0.8 being
“good”). The meta-classifier is evaluated by pre-
cision and recall of each class and classification
accuracy of all instances.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.944414">
6.1 Baseline Comparison to Snow et al.’s
Previous Hypernym Classification on
WordNet-TREC data
</subsectionHeader>
<bodyText confidence="0.9999585">
Snow et al. (2005) evaluated binary classifi-
cation of noun-phrase pairs as hypernyms or
non-hypernyms. When training and testing on
WordNet-labeled pairs from TREC sentences,
they report classifier Max F of 0.348, using de-
pendency path features and logistic regression. To
justify our choice of an SVM for classification, we
replicated their work. Snow et al. provided us with
their data. With our SVM we achieved a Max F of
0.453, 30% higher than they reported.
</bodyText>
<subsectionHeader confidence="0.9222265">
6.2 Extending Snow et al.’s WordNet-TREC
Binary Classification to N Classes
</subsectionHeader>
<bodyText confidence="0.9926115">
Snow et al. select pairs that are “Known Hyper-
nyms” (the first sense of the first word is a hy-
</bodyText>
<page confidence="0.992368">
52
</page>
<table confidence="0.999734444444444">
Feature Description
Levenshtein Distance # character insertions/deletions/substitutions to change query α to query ,3 (Levenshtein, 1966).
Word Overlap Percent # words the two queries have in common, divided by num. words in the longer query.
Possible Stem 1 if the two segments stem to the same root using the Porter stemmer.
Substring Containment 1 if the first segment is a substring of the second.
Is URL 1 if either segment matches a handmade URL regexp.
Query Pair Frequency # times the pair was seen in the entire unlabeled corpus of query pairs.
Log Likelihood Ratio The Log Likelihood Ratio described in Section 3.2.1 Formula 3.2.1
Dice and Jaccard Coefficients Measures of the similarity of substitutes for and by the two phrases.
</table>
<tableCaption confidence="0.999075">
Table 2: Syntactic and statistical features over pairs of phrases.
</tableCaption>
<bodyText confidence="0.999962052631579">
ponym of the first sense of the second and both
have no more than one tagged sense in the Brown
corpus) and “Known Non-Hypernyms” (no sense
of the first word is a hyponym of any sense of the
second). We wished to test whether making the
classes less cleanly separable would affect the re-
sults, and also whether we could use these features
for n-way classification.
From the same TREC corpus we extracted
known synonym, known hyponym, known coordi-
nate, known meronym, and known holonym pairs.
Each of these classes is defined analogously to the
known hypernym class; we selected these six rela-
tionships because they are the six most common.
A pair is labeled known no-relationship if no sense
of the first word has any relationship to any sense
of the second word. The class distribution was se-
lected to match as closely as possible that observed
in query logs. We labeled 50,000 pairs total.
Results are shown in Table 4(a). Although AUC
is fairly high for all classes, MaxF is low for all
but two. MaxF has degraded quite a bit for hyper-
nyms from Table 3. Removing all instances except
hypernym and no relationship brings MaxF up to
0.45, suggesting that the additional classes make it
harder to separate hypernyms.
Metaclassifier accuracy is very good, but this is
due to high recall of no relationship and coordi-
nate pairs: more than 80% of instances with some
relationship are predicted to be coordinates, and
most of the rest are predicted no relationship. It
seems that we are only distinguishing between no
vs. some relationship.
The size of the no relationship class may be bi-
asing the results. We removed those instances, but
performance of the n-class classifier did not im-
prove (Table 4(b)). MaxF of binary classifiers did
improve, even though AUC is much worse.
</bodyText>
<subsectionHeader confidence="0.788404">
6.3 N-Class Classification of Query Pairs
</subsectionHeader>
<bodyText confidence="0.993838">
We now use query pairs rather than TREC pairs.
</bodyText>
<subsectionHeader confidence="0.750719">
6.3.1 Classification Using Only Dependency
Paths
</subsectionHeader>
<bodyText confidence="0.9999416">
We first limit features to dependency paths in
order to compare to the prior results. Dependency
paths cannot be obtained for all query phrase pairs,
since the two phrases must appear in the same sen-
tence together. We used only the pairs for which
we could get path features, about 32% of the total.
Table 5(a) shows results of binary classification
and metaclassification on those instances using de-
pendency path features only. We can see that de-
pendency paths do not perform very well on their
own: most instances are assigned to the “coordi-
nate” class that comprises a plurality of instances.
A comparison of Tables 5(a) and 4(a) suggests
that classifying query substitution pairs is harder
than classifying TREC phrases.
Table 5(b) shows the results of binary clas-
sification and metaclassification on the same in-
stances using all features. Using all features im-
proves performance dramatically on each individ-
ual binary classifier as well as the metaclassifier.
</bodyText>
<subsectionHeader confidence="0.820595">
6.3.2 Classification on All Query Pairs Using
All Features
</subsectionHeader>
<bodyText confidence="0.99979325">
We now expand to all of our hand-labeled pairs.
Table 6(a) shows results of binary and meta classi-
fication; Figure 1 shows precision-recall curves for
10 binary classifiers (excluding URLs). Our clas-
sifier does quite well on every class but hypernym
and hyponym. These two make up a very small
percentage of the data, so it is not surprising that
performance would be so poor.
The metaclassifier achieved 71% accuracy. This
is significantly better than random or majority-
class baselines, and close to our 78% interanno-
tator agreement. Thresholding the metaclassifier
to pairs with greater than .5 max class probability
(68% of instances) gives 85% accuracy.
Next we wish to see how much of the perfor-
mance can be maintained without using the com-
</bodyText>
<page confidence="0.998492">
53
</page>
<table confidence="0.973079409090909">
class binary AUC n-way rec data
maxF prec %
no rel .980 .986 .979 .985 80.0
synonym .028 .856 0 0 0.3
hypernym .185 .888 .512 .019 2.1
hyponym .193 .890 .462 .016 2.1
coordinate .808 .971 .714 .931 14.8
meronym .158 .905 .615 .050 0.3
holonym .120 .883 .909 .062 0.3
metaclassifier accuracy .927
(a) All seven WordNet classes. The high accuracy is
mostly due to high recall of no rel and coordinate classes.
binary AUC n-way rec data
maxF prec %
– – – – 0
.086 .683 0 0 1.7
.337 .708 .563 .077 10.6
.341 .720 .527 .080 10.6
.857 .737 .757 .986 74.1
.251 .777 .500 .068 1.5
.277 .767 .522 .075 1.5
– .749
</table>
<tableCaption confidence="0.822951833333333">
(b) Removing no relationship instances
improves MaxF and recall of all classes,
but performance is generally worse.
Table 4: Performance of 7 binary classifier and metaclassifiers on phrase-pairs cooccuring in TREC data labeled with WordNet
classes, using minipar dependency features. These features do not seem to be adequate for distinguishing classes other than
coordinate and no-relationship.
</tableCaption>
<table confidence="0.9984482">
class binary auc prec n-way
maxf rec
no rel .281 .611 .067 .006
synonym .269 .656 .293 .167
hypernym .140 .626 0 0
hyponym .121 .610 0 0
coordinate .506 .760 .303 .888
spelling .288 .677 .121 .022
stemmed .571 .834 .769 .260
URL .742 .919 .767 .691
generalization .082 .547 0 0
specification .085 .528 0 0
other .393 .681 .384 .364
metaclassifier accuracy .385
(a) Dependency tree paths only.
binary auc prec n-way % data
maxf rec % full
.602 .883 .639 .497 10.6 3.5
.477 .851 .571 .278 4.5 1.5
.167 .686 .125 .017 3.7 1.2
.136 .660 0 0 3.7 1.2
.747 .935 .624 .862 21.0 6.9
.814 .970 .703 .916 11.0 3.6
.781 .972 .788 .675 4.8 1.6
1 1 1 1 16.2 5.3
.490 .883 .489 .393 3.5 1.1
.584 .854 .600 .589 3.5 1.1
.641 .895 .603 .661 17.5 5.7
– .692 —
(b) All features.
</table>
<tableCaption confidence="0.9951285">
Table 5: Binary and metaclassifier performance on the 32% of hand-labeled instances with dependency path features. Adding
all our features significantly improves performance over just using dependency paths.
</tableCaption>
<bodyText confidence="0.999843">
putationally expensive syntactic parsing of depen-
dency paths. To estimate the marginal gain of the
other features over the dependency paths, we ex-
cluded the latter features and retrained our clas-
sifiers. Results are shown in Table 6(b). Even
though binary and meta-classifier performance de-
creases on all classes but generalizations and spec-
ifications, much of the performance is maintained.
Because URL changes are easily identifiable by
the IsURL feature, we removed those instances
and retrained the classifiers. Results are shown in
Table 6(c). Although overall accuracy is worse,
individual class performance is still high, allow-
ing us to conclude our results are not only due to
the ease of classifying URLs.
We generated a learning curve by randomly
sampling instances, training the binary classifiers
on that subset, and training the metaclassifier on
the results of the binary classifiers. The curve is
shown in Figure 2. With 10% of the instances, we
have a metaclassifier accuracy of 59%; with 100%
of the data, accuracy is 71%. Accuracy shows no
sign of falling off with more instances.
</bodyText>
<subsectionHeader confidence="0.998658">
6.4 Training on WordNet-Labeled Pairs Only
</subsectionHeader>
<bodyText confidence="0.9977242">
Figure 2 implies that more labeled instances will
lead to greater accuracy. However, manually la-
beled instances are generally expensive to obtain.
Here we look to other sources of labeled instances
for additional training pairs.
</bodyText>
<subsectionHeader confidence="0.72307">
6.4.1 Training and Testing on WordNet
</subsectionHeader>
<bodyText confidence="0.999985583333333">
We trained and tested five classifiers using 10-
fold cross validation on our set of WordNet-
labeled query segment pairs. Results for each class
are shown in Table 7. We seem to have regressed
to predicting no vs. some relationship.
Because these results are not as good as the
human-labeled results, we believe that some of our
performance must be due to peculiarities of our
data. That is not unexpected: since words that ap-
pear in WordNet are very common, features are
much noisier than features associated with query
entities that are often structured within web pages.
</bodyText>
<page confidence="0.997286">
54
</page>
<table confidence="0.994629466666667">
class binary auc prec n-way binary auc n-way rec data binary auc prec n-way
maxf rec maxf prec % maxf rec
no rel .531 .878 .616 .643 .466 .764 .549 .482 10.4 .512 .808 .502 .486
synonym .355 .820 .506 .212 .351 .745 .493 .178 4.2 .350 .759 .478 .212
hypernym .173 .821 .100 .020 .133 .728 0 0 2.0 .156 .710 .250 .020
hyponym .173 .797 .059 .010 .163 .733 0 0 2.0 .187 .739 .125 .020
coordinate .635 .921 .590 .703 .539 .832 .565 .732 13.9 .634 .885 .587 .706
spelling .778 .960 .625 .904 .723 .917 .628 .902 14.9 .774 .939 .617 .906
stemmed .703 .973 .786 .589 .656 .964 .797 .583 3.4 .717 .967 .802 .601
URL 1 1 1 1 1 1 1 1 29.8 – – – –
generalization .565 .916 .575 .483 .492 .852 .604 .604 4.8 .581 .885 .598 .634
specification .661 .926 .652 .506 .578 .869 .670 .644 4.7 .665 .906 .657 .468
other .539 .898 .575 .483 .436 .790 .550 .444 9.8 .529 .847 .559 .469
metaclassifier accuracy .714 – .714 – .587
(a) All features. (b) Dependency path features removed. (c) URL class removed.
</table>
<tableCaption confidence="0.920838666666667">
Table 6: Binary and metaclassifier performance on all classes and all hand-labeled instances. Table (a) provides a benchmark
for 10-class classification over highly substitutable query phrases. Table (b) shows that a lot of our performance can be achieved
without computationally-expensive parsing.
</tableCaption>
<table confidence="0.997260111111111">
class binary auc meta rec data
maxf prec %
no rel .758 .719 .660 .882 57.8
synonym .431 .901 .617 .199 2.4
hypernym .284 .803 .367 .061 1.8
hyponym .212 .804 .415 .056 1.6
coordinate .588 .713 .615 .369 35.5
other .206 .739 .375 .019 0.8
metaclassifier accuracy .648
</table>
<tableCaption confidence="0.8929665">
Table 7: Binary and metaclassifier performance on WordNet-
labeled instances with all features.
</tableCaption>
<figure confidence="0.9909535">
0.72
0.7
Metaclassifier accuracy
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Number of query pairs
</figure>
<table confidence="0.918285875">
class binary auc meta rec data
maxf prec %
no rel .525 .671 .485 .354 31.9
synonym .381 .671 .684 .125 13.0
hypernym .211 .605 0 0 6.2
hyponym .125 .501 0 0 6.2
coordinate .623 .628 .485 .844 42.6
metaclassifier accuracy .490
</table>
<tableCaption confidence="0.989937333333333">
Table 8: Training on WordNet-labeled pairs and testing on
hand-labeled pairs. Classifiers trained on WordNet do not
generalize well.
</tableCaption>
<subsectionHeader confidence="0.846356">
6.4.2 Training on WordNet, Testing on
WordNet and Hand-Labeled Pairs
</subsectionHeader>
<bodyText confidence="0.9989921">
We took the five classes for which human and
WordNet definitions agreed (synonyms, coordi-
nates, hypernyms, hyponyms, and no relationship)
and trained classifiers on all WordNet-labeled in-
stances. We tested the classifiers on human-
labeled instances from just those five classes. Re-
sults are shown in Table 8. Performance was
not very good, reinforcing the idea that while our
features can distinguish between query segments,
they cannot distinguish between common words.
</bodyText>
<figureCaption confidence="0.99046">
Figure 2: Meta-classifier accuracy as a function of number of
labeled instances for training.
</figureCaption>
<sectionHeader confidence="0.998914" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.99986935">
Almost all high-weighted features are either
HTML paths or query log features; these are the
ones that are easiest to obtain. Many of the
highest-weight HTML tree features are symmet-
ric, e.g. both words appear in cells of the same ta-
ble, or as items in the same list. Here we note a
selection of the more interesting predictors.
synonym —“X or Y” expressed as a dependency
path was a high-weight feature.
hyper/hyponym —“Y and other X” as a depen-
dency path has highest weight. An interesting
feature is X in a table cell and Y appearing in
text outside but nearby the table.
sibling —many symmetric HTML features. “X to
the Y” as in “80s to the 90s”. “X and Y”, “X,
Y, and Z” highly-weighted minipar paths.
general/specialization —the top three features
are substring containment, word subset dif-
ference count, and prefix overlap.
spelling change —many negative features, indi-
</bodyText>
<page confidence="0.988232">
55
</page>
<figure confidence="0.998360394736842">
Precision
Precision
0 0.2 0.4 0.6 0.8 1
Recall
0 0.2 0.4 0.6 0.8 1
Recall
0.8
0.6
0.4
0.2
0
1
F=0.354
F=0.172
F=0.173
no relationship
sibling
synonym
hyponym
hypernym
F=0.531
F=0.634
0.8
0.6
0.4
0.2
0
1
spelling change
related in some other way
stemmed form
generalization
specification
F=0.565
F=0.538
F=0.661
F=0.702
F=0.777
</figure>
<figureCaption confidence="0.999983">
Figure 1: Precision-recall curves for 10 binary classifiers on all hand-labeled instances with all features.
</figureCaption>
<bodyText confidence="0.998045111111111">
cating that two words that cooccur in a web
page are not likely to be spelling differences.
other —many symmetric HTML features. Two
words emphasized in the same way (e.g. both
bolded) may indicate some relationship.
none —many asymmetric HTML features, e.g.
one word in a blockquote, the other bolded
in a different paragraph. Dice coefficient is a
good negative features.
</bodyText>
<sectionHeader confidence="0.997838" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999672">
We have provided the first benchmark for n-
class semantic classification of highly substi-
tutable query phrases. There is much room for im-
provement, and we expect that this baseline will
be surpassed.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999897909090909">
Thanks to Chris Manning and Omid Madani for
helpful comments, to Omid Madani for providing
the classification code, to Rion Snow for providing
the hypernym data, and to our labelers.
This work was supported in part by the CIIR
and in part by the Defense Advanced Research
Projects Agency (DARPA) under contract number
HR001-06-C-0023. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983725">
Peter G. Anick. 2003. Using terminological feedback for
web search refinement: a log-based study. In SIGIR 2003,
pages 88–95.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
PASCAL Challenges Workshop on Recognising Textual
Entailment.
Ted E. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61–74.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of Coling 1992,
pages 539–545.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In 15th
International World Wide Web Conference (WWW-2006),
Edinburgh.
Sathiya Keerthi and Dennis DeCoste. 2005. A modified fi-
nite newton method for fast solution of large scale linear
svms. Journal ofMachine Learning Research, 6:341–361,
March.
Lillian Lee. 1999. Measures of distributional similarity. In
37th Annual Meeting of the Association for Computational
Linguistics, pages 25–32.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Cybernetics
and Control Theory, 10(8):707–710. Original in Doklady
AkademiiNaukSSSR 163(4): 845–848 (1965).
Dekang Lin. 1998. Dependency-based evaluation of mini-
par. In Workshop on the Evaluation of Parsing Systems.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications oftheACM, 38(11):39–41.
J. Platt. 2000. Probabilistic outputs for support vector ma-
chines and comparison to regularized likelihood methods.
pages 61–74.
Soo Young Rieh and Hong Iris Xie. 2001. Patterns and se-
quences of multiple query reformulations in web search-
ing: A preliminary study. In Proceedings of the 64th An-
nual Meeting of the American Society for Information Sci-
ence and Technology Vol. 38, pages 246–255.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery.
In Proceedings of the Nineteenth Annual Conference on
Neural Information Processing Systems (NIPS 2005).
Egidio Terra and Charles L. A. Clarke. 2004. Scoring miss-
ing terms in information retrieval tasks. In CIKM 2004,
pages 50–58.
P.D Turney, M.L. Littman, J. Bigham, and V. Shnayder, 2003.
Recent Advances in Natural Language Processing III: Se-
lected Papers from RANLP 2003, chapter Combining in-
dependent modules in lexical multiple-choice problems,
pages 101–110. John Benjamins.
</reference>
<page confidence="0.998422">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.869870">
<title confidence="0.993145">N Semantic Classes are Harder than Two</title>
<author confidence="0.985964">Rosie Jones</author>
<affiliation confidence="0.983583">Yahoo! Research</affiliation>
<address confidence="0.996548">3333 Empire Ave. Burbank, CA 91504</address>
<email confidence="0.999571">jonesr@yahoo-inc.com</email>
<affiliation confidence="0.9685365">CIIR University of Massachusetts</affiliation>
<address confidence="0.999962">Amherst, MA 01003</address>
<email confidence="0.999698">carteret@cs.umass.edu</email>
<affiliation confidence="0.995834">Los Angeles Software Inc.</affiliation>
<address confidence="0.999777">1329 Pine Street Santa Monica, CA 90405</address>
<email confidence="0.999413">w.greiner@lasoft.com</email>
<author confidence="0.997626">Cory Barr</author>
<affiliation confidence="0.988591">Yahoo! Research</affiliation>
<address confidence="0.996541">3333 Empire Ave. Burbank, CA 91504</address>
<email confidence="0.999653">barrc@yahoo-inc.com</email>
<abstract confidence="0.999463928571429">We show that we can automatically classify semantically related phrases into 10 classes. Classification robustness is improved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic into WordNet’s semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter G Anick</author>
</authors>
<title>Using terminological feedback for web search refinement: a log-based study.</title>
<date>2003</date>
<booktitle>In SIGIR</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1032" citStr="Anick, 2003" startWordPosition="141" endWordPosition="142">an automatically classify semantically related phrases into 10 classes. Classification robustness is improved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic n-way classification into WordNet’s semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases. 1 Introduction Identifying semantically related phrases has been demonstrated to be useful in information retrieval (Anick, 2003; Terra and Clarke, 2004) and sponsored search (Jones et al., 2006). Work on semantic entailment often includes lexical entailment as a subtask (Dagan et al., 2005). We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). Thes</context>
</contexts>
<marker>Anick, 2003</marker>
<rawString>Peter G. Anick. 2003. Using terminological feedback for web search refinement: a log-based study. In SIGIR 2003, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1196" citStr="Dagan et al., 2005" startWordPosition="167" endWordPosition="170"> including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic n-way classification into WordNet’s semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases. 1 Introduction Identifying semantically related phrases has been demonstrated to be useful in information retrieval (Anick, 2003; Terra and Clarke, 2004) and sponsored search (Jones et al., 2006). Work on semantic entailment often includes lexical entailment as a subtask (Dagan et al., 2005). We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). These techniques, however, do not allow us to distinguish the more specific relationships: • hypernym(dog,puppy) *This work was carried out while these authors were at </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8176" citStr="Dunning, 1993" startWordPosition="1280" endWordPosition="1281">rs and millions of search sessions, we can calculate the probability of each such rewrite, then 50 test each pair for statistical significance to eliminate phrase rewrites which occurred in a small number of sessions, perhaps by chance. To test for statistical significance we use the pair independence likelihood ratio, or log-likelihood ratio, test. This metric tests the hypothesis that the probability of phrase Q is the same whether phrase α has been seen or not by calculating the likelihood of the observed data under a binomial distribution using probabilities derived using each hypothesis (Dunning, 1993). L (P(�|α) = P(�|-α)) logA = log L (P(0|α) =� P(0|-α)) A high negative value for A suggests a strong dependence between query α and query Q. 4 Labeling Phrase Pairs for Supervised Learning We took a random sample of query segment substitutions from our query logs to be labeled. The sampling was limited to pairs that were frequent substitutions for each other to ensure a high probability of the segments having some relationship. 4.1 WordNet Labeling WordNet is a large lexical database of English words. In addition to defining several hundred thousand words, it defines synonym sets, or synsets,</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted E. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>539--545</pages>
<contexts>
<context position="12896" citStr="Hearst, 1992" startWordPosition="2086" endWordPosition="2087"> more information about the topic credit card; card 4.7 spelling change spelling errors, typos, punctuation changes, spacing changes peopl; people 14.9 stemmed form X and Y have the same lemmas ant; ants 3.4 URL change X and Y are related and X or Y is a URL alliance; alliance.com 29.8 other relationship X and Y are related in some other way flagpoles; flags 9.8 no relationship X and Y are not related in any obvious way crypt; tree 10.4 Table 1: Semantic relationships between phrases rewritten in query reformulation sessions, along with their prevalence in our data. Lexico-syntactic Patterns (Hearst, 1992) A substring occurring between the two segments extracted from text in nodes in which both segments appear. In the example fragment “authors such as Shakespeare”, the feature is “such as” and the value is the number of times the substring appears between “author” and “Shakespeare”. 5.1.2 Query Pair Features Table 2 summarizes features that are induced from the query strings themselves or calculated from query log data. 5.2 Additional Training Pairs We can double our training set by adding for each pair u1, u2 a new pair u2, u1. The class of the new pair is the same as the old in all cases but </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of Coling 1992, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In 15th International World Wide Web Conference (WWW-2006),</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="1099" citStr="Jones et al., 2006" startWordPosition="151" endWordPosition="154"> 10 classes. Classification robustness is improved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic n-way classification into WordNet’s semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases. 1 Introduction Identifying semantically related phrases has been demonstrated to be useful in information retrieval (Anick, 2003; Terra and Clarke, 2004) and sponsored search (Jones et al., 2006). Work on semantic entailment often includes lexical entailment as a subtask (Dagan et al., 2005). We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). These techniques, however, do not allow us to distinguish the more spec</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In 15th International World Wide Web Conference (WWW-2006), Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sathiya Keerthi</author>
<author>Dennis DeCoste</author>
</authors>
<title>A modified finite newton method for fast solution of large scale linear svms.</title>
<date>2005</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>6--341</pages>
<contexts>
<context position="14040" citStr="Keerthi and DeCoste (2005)" startWordPosition="2276" endWordPosition="2279">ew pair u2, u1. The class of the new pair is the same as the old in all cases but hypernym, hyponym, specification, and generalization, which are inverted. Features are reversed from f(u1, u2) to f(u2, u1). A pair and its inverse have different sets of features, so splitting the set randomly into training and testing sets should not result in resubstitution error. Nonetheless, we ensure that a pair and its inverse are not separated for training and testing. 5.3 Classifier For each class we train a binary one-vs.-all linearkernel support vector machine (SVM) using the optimization algorithm of Keerthi and DeCoste (2005). 5.3.1 Meta-Classifier For n-class classification, we calibrate SVM scores to probabilities using the method described by Platt (2000). This gives us P(classlpair) for each pair. The final classification for a pair is argmaxcla33P(class pair). Source Snow (NIPS 2005) Experiment Task binary hypernym binary hypernym Data WordNet-TREC WordNet-TREC Instance Count 752,311 752,311 Features minipar paths minipar paths Feature Count 69,592 69,592 Classifier logistic Regression linear SVM maxF 0.348 0.453 Table 3: Snow et al’s (2005) reported performance using linear regression, and our reproduction o</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite newton method for fast solution of large scale linear svms. Journal ofMachine Learning Research, 6:341–361, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="1526" citStr="Lee, 1999" startWordPosition="218" endWordPosition="219">on Identifying semantically related phrases has been demonstrated to be useful in information retrieval (Anick, 2003; Terra and Clarke, 2004) and sponsored search (Jones et al., 2006). Work on semantic entailment often includes lexical entailment as a subtask (Dagan et al., 2005). We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). These techniques, however, do not allow us to distinguish the more specific relationships: • hypernym(dog,puppy) *This work was carried out while these authors were at Yahoo! Research. • hyponym(dog,canine) • coordinate(dog,cat) Lexical resources such as WordNet (Miller, 1995) are extremely useful, but are limited by being manually constructed. They do not contain semantic class relationships for the many new terms we encounter in text such as web documents, for example “mp3 player” or “ipod”.</context>
<context position="6378" citStr="Lee, 1999" startWordPosition="982" endWordPosition="983">s from these corpora, one restricted and one more complete set. Snow et al. obtained training data from the intersection of noun-phrases cooccuring in sentences in a TREC news corpus and those that can be labeled unambiguously as hypernyms or non-hypernyms using WordNet. We use a restricted set since instances selected in the previous work are a subset of the instances one is likely to encounter in text. The pairs are generally either related in one type of relationship, or completely unrelated. In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. For this task we identify a larger set of candidate-related phrases. 3.2 Query Log Data To find phrases that are similar or substitutable for web searchers, we turn to logs of user search sessions. We look at query reformulations: a pair of successive queries issued by a single user on a single day. We collapse repeated searches for the same terms, as well as query pair sequences repeated by the same user on the same day. 3.2.1 Substitutable Query Segments Whole queries tend to consist o</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<booktitle>Cybernetics and Control Theory, 10(8):707–710. Original in Doklady AkademiiNaukSSSR</booktitle>
<volume>163</volume>
<issue>4</issue>
<pages>845--848</pages>
<contexts>
<context position="15971" citStr="Levenshtein, 1966" startWordPosition="2576" endWordPosition="2577">rom TREC sentences, they report classifier Max F of 0.348, using dependency path features and logistic regression. To justify our choice of an SVM for classification, we replicated their work. Snow et al. provided us with their data. With our SVM we achieved a Max F of 0.453, 30% higher than they reported. 6.2 Extending Snow et al.’s WordNet-TREC Binary Classification to N Classes Snow et al. select pairs that are “Known Hypernyms” (the first sense of the first word is a hy52 Feature Description Levenshtein Distance # character insertions/deletions/substitutions to change query α to query ,3 (Levenshtein, 1966). Word Overlap Percent # words the two queries have in common, divided by num. words in the longer query. Possible Stem 1 if the two segments stem to the same root using the Porter stemmer. Substring Containment 1 if the first segment is a substring of the second. Is URL 1 if either segment matches a handmade URL regexp. Query Pair Frequency # times the pair was seen in the entire unlabeled corpus of query pairs. Log Likelihood Ratio The Log Likelihood Ratio described in Section 3.2.1 Formula 3.2.1 Dice and Jaccard Coefficients Measures of the similarity of substitutes for and by the two phras</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and Control Theory, 10(8):707–710. Original in Doklady AkademiiNaukSSSR 163(4): 845–848 (1965).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="11302" citStr="Lin, 1998" startWordPosition="1796" endWordPosition="1797">atures. In Section 7 we will look at the learned weights and discuss which features are most significant for identifying which semantic classes. 5.1 Features Features for query substitution pairs are extracted from query logs and web pages. 5.1.1 Web Page / Document Features We submit the two segments to a web search engine as a conjunctive query and download the top 50 results. Each result is converted into an HTML Document Object Model (DOM) tree and segmented into sentences. Dependency Tree Paths The path from the first segment to the second in a dependency parse tree generated by MINIPAR (Lin, 1998) from sentences in which both segments appear. These were previously used by Snow et al. (2005). These features were extracted from web pages in all experiments, except where we identify that we used TREC news stories (the same data as used by Snow et al.). HTML Paths The paths from DOM tree nodes the first segment appears in to nodes the second segment appears in. The value is the number of times the path occurs with the pair. 51 Class Description Example % synonym one phrase can be used in place of the other without loss in meaning low cost; cheap 4.2 hypernym X is a hypernym of Y if and onl</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications oftheACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1905" citStr="Miller, 1995" startWordPosition="272" endWordPosition="273">entifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). These techniques, however, do not allow us to distinguish the more specific relationships: • hypernym(dog,puppy) *This work was carried out while these authors were at Yahoo! Research. • hyponym(dog,canine) • coordinate(dog,cat) Lexical resources such as WordNet (Miller, 1995) are extremely useful, but are limited by being manually constructed. They do not contain semantic class relationships for the many new terms we encounter in text such as web documents, for example “mp3 player” or “ipod”. We can use WordNet as training data for such classification to the extent that the training on pairs found in WordNet and testing on pairs found outside WordNet provides accurate generalization. We describe a set of features used to train nway supervised machine-learned classification of semantic classes for arbitrary pairs of phrases. Redundancy in the sources of our feature</context>
<context position="3904" citStr="Miller, 1995" startWordPosition="578" endWordPosition="579">t generalize well to query pairs • Demonstration that much of the performance in classification can be attained using only 49 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 49–56, Sydney, July 2006. c�2006 Association for Computational Linguistics syntactic features • A learning curve for classification of query phrase pairs that suggests the primary bottleneck is manually labeled training instances: we expect our benchmark to be surpassed. 2 Relation to Previous Work Snow et al. (2005) demonstrated binary classification of hypernyms and non-hypernyms using WordNet (Miller, 1995) as a source of training labels. Using dependency parse tree paths as features, they were able to generalize from WordNet labelings to human labelings. Turney et al. (2003) combined features to answer multiple-choice synonym questions from the TOEFL test and verbal analogy questions from the SAT college entrance exam. The multiplechoice questions typically do not consist of multiple closely related terms. A typical example is given by Turney: • hidden:: (a) laughable (c) ancient (b) veiled (d) revealed Note that only (b) and (d) are at all related to the term, so the algorithm only needs to di</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications oftheACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparison to regularized likelihood methods.</title>
<date>2000</date>
<pages>61--74</pages>
<contexts>
<context position="14175" citStr="Platt (2000)" startWordPosition="2296" endWordPosition="2297">erted. Features are reversed from f(u1, u2) to f(u2, u1). A pair and its inverse have different sets of features, so splitting the set randomly into training and testing sets should not result in resubstitution error. Nonetheless, we ensure that a pair and its inverse are not separated for training and testing. 5.3 Classifier For each class we train a binary one-vs.-all linearkernel support vector machine (SVM) using the optimization algorithm of Keerthi and DeCoste (2005). 5.3.1 Meta-Classifier For n-class classification, we calibrate SVM scores to probabilities using the method described by Platt (2000). This gives us P(classlpair) for each pair. The final classification for a pair is argmaxcla33P(class pair). Source Snow (NIPS 2005) Experiment Task binary hypernym binary hypernym Data WordNet-TREC WordNet-TREC Instance Count 752,311 752,311 Features minipar paths minipar paths Feature Count 69,592 69,592 Classifier logistic Regression linear SVM maxF 0.348 0.453 Table 3: Snow et al’s (2005) reported performance using linear regression, and our reproduction of the same experiment, using a support vector machine (SVM). 5.3.2 Evaluation Binary classifiers are evaluated by ranking instances by </context>
</contexts>
<marker>Platt, 2000</marker>
<rawString>J. Platt. 2000. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo Young Rieh</author>
<author>Hong Iris Xie</author>
</authors>
<title>Patterns and sequences of multiple query reformulations in web searching: A preliminary study.</title>
<date>2001</date>
<booktitle>In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology</booktitle>
<volume>38</volume>
<pages>246--255</pages>
<contexts>
<context position="9644" citStr="Rieh and Xie (2001)" startWordPosition="1525" endWordPosition="1528">m, defined as the PART-OF relationship. We used WordNet to automatically label the subset of our sample for which both phrases occur in WordNet. Any sense of the first segment having a relationship to any sense of the second would result in the pair being labeled. Since WordNet contains many other relationships in addition to those listed above, we group the rest into the other category. If the segments had no relationship in WordNet, they were labeled no relationship. 4.2 Segment Pair Labels Phrase pairs passing a statistical test are common reformulations, but can be of many semantic types. Rieh and Xie (2001) categorized types of query reformulations, defining 10 general categories: specification, generalization, synonym, parallel movement, term variations, operator usage, error correction, general resource, special resource, and site URLs. We redefine these slightly to apply to query segments. The summary of the definitions is shown in Table 1, along with the distribution in the data of pairs passing the statistical test. 4.2.1 Hand Labeling More than 90% of phrases in query logs do not appear in WordNet due to being spelling errors, web site URLs, proper nouns of a temporal nature, etc. Six anno</context>
</contexts>
<marker>Rieh, Xie, 2001</marker>
<rawString>Soo Young Rieh and Hong Iris Xie. 2001. Patterns and sequences of multiple query reformulations in web searching: A preliminary study. In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology Vol. 38, pages 246–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In Proceedings of the Nineteenth Annual Conference on Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context position="2711" citStr="Snow et al., 2005" startWordPosition="403" endWordPosition="406">r example “mp3 player” or “ipod”. We can use WordNet as training data for such classification to the extent that the training on pairs found in WordNet and testing on pairs found outside WordNet provides accurate generalization. We describe a set of features used to train nway supervised machine-learned classification of semantic classes for arbitrary pairs of phrases. Redundancy in the sources of our feature information means that we are able to provide coverage over an extremely large vocabulary of phrases. We contrast this with techniques that require parsing of natural language sentences (Snow et al., 2005) which, while providing reasonable performance, can only be applied to a restricted vocabulary of phrases cooccuring in sentences. Our contributions are: • Demonstration that binary classification removes the difficult cases of classification into closely related semantic classes • Demonstration that dependency parser paths are inadequate for semantic classification into 7 WordNet classes on TREC news corpora • A benchmark of 10-class semantic classification over highly substitutable query phrases • Demonstration that training a classifier using WordNet for labeling does not generalize well to</context>
<context position="5721" citStr="Snow et al., 2005" startWordPosition="873" endWordPosition="876"> perform binary semantic classification do not work as well when extended to a full n-way semantic classification. We show that using a variety of features performs better than any feature alone. 3 Identifying Candidate Phrases for Classification In this section we introduce the two data sources we use to extract sets of candidate related phrases for classification: a TREC-WordNet intersection and query logs. 3.1 Noun-Phrase Pairs Cooccuring in TREC News Sentences The first is a data-set derived from TREC news corpora and WordNet used in previous work for binary semantic class classification (Snow et al., 2005). We extract two sets of candidate-related pairs from these corpora, one restricted and one more complete set. Snow et al. obtained training data from the intersection of noun-phrases cooccuring in sentences in a TREC news corpus and those that can be labeled unambiguously as hypernyms or non-hypernyms using WordNet. We use a restricted set since instances selected in the previous work are a subset of the instances one is likely to encounter in text. The pairs are generally either related in one type of relationship, or completely unrelated. In general we may be able to identify related phrase</context>
<context position="11397" citStr="Snow et al. (2005)" startWordPosition="1811" endWordPosition="1814"> most significant for identifying which semantic classes. 5.1 Features Features for query substitution pairs are extracted from query logs and web pages. 5.1.1 Web Page / Document Features We submit the two segments to a web search engine as a conjunctive query and download the top 50 results. Each result is converted into an HTML Document Object Model (DOM) tree and segmented into sentences. Dependency Tree Paths The path from the first segment to the second in a dependency parse tree generated by MINIPAR (Lin, 1998) from sentences in which both segments appear. These were previously used by Snow et al. (2005). These features were extracted from web pages in all experiments, except where we identify that we used TREC news stories (the same data as used by Snow et al.). HTML Paths The paths from DOM tree nodes the first segment appears in to nodes the second segment appears in. The value is the number of times the path occurs with the pair. 51 Class Description Example % synonym one phrase can be used in place of the other without loss in meaning low cost; cheap 4.2 hypernym X is a hypernym of Y if and only if Y is a X muscle car; mustang 2.0 hyponym X is a hyponym of Y if and only if X is a Y (inve</context>
<context position="15216" citStr="Snow et al. (2005)" startWordPosition="2454" endWordPosition="2457"> linear regression, and our reproduction of the same experiment, using a support vector machine (SVM). 5.3.2 Evaluation Binary classifiers are evaluated by ranking instances by classification score and finding the Max F1 (the harmonic mean of precision and recall; ranges from 0 to 1) and area under the ROC curve (AUC; ranges from 0.5 to 1 with at least 0.8 being “good”). The meta-classifier is evaluated by precision and recall of each class and classification accuracy of all instances. 6 Experiments 6.1 Baseline Comparison to Snow et al.’s Previous Hypernym Classification on WordNet-TREC data Snow et al. (2005) evaluated binary classification of noun-phrase pairs as hypernyms or non-hypernyms. When training and testing on WordNet-labeled pairs from TREC sentences, they report classifier Max F of 0.348, using dependency path features and logistic regression. To justify our choice of an SVM for classification, we replicated their work. Snow et al. provided us with their data. With our SVM we achieved a Max F of 0.453, 30% higher than they reported. 6.2 Extending Snow et al.’s WordNet-TREC Binary Classification to N Classes Snow et al. select pairs that are “Known Hypernyms” (the first sense of the fir</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Proceedings of the Nineteenth Annual Conference on Neural Information Processing Systems (NIPS 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egidio Terra</author>
<author>Charles L A Clarke</author>
</authors>
<title>Scoring missing terms in information retrieval tasks.</title>
<date>2004</date>
<booktitle>In CIKM</booktitle>
<pages>50--58</pages>
<contexts>
<context position="1057" citStr="Terra and Clarke, 2004" startWordPosition="143" endWordPosition="146">lly classify semantically related phrases into 10 classes. Classification robustness is improved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic n-way classification into WordNet’s semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases. 1 Introduction Identifying semantically related phrases has been demonstrated to be useful in information retrieval (Anick, 2003; Terra and Clarke, 2004) and sponsored search (Jones et al., 2006). Work on semantic entailment often includes lexical entailment as a subtask (Dagan et al., 2005). We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). These techniques, however, do</context>
</contexts>
<marker>Terra, Clarke, 2004</marker>
<rawString>Egidio Terra and Charles L. A. Clarke. 2004. Scoring missing terms in information retrieval tasks. In CIKM 2004, pages 50–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>M L Littman</author>
<author>J Bigham</author>
<author>V Shnayder</author>
</authors>
<title>chapter Combining independent modules in lexical multiple-choice problems,</title>
<date>2003</date>
<booktitle>Recent Advances in Natural Language Processing III: Selected Papers from RANLP</booktitle>
<pages>101--110</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="1626" citStr="Turney et al., 2003" startWordPosition="231" endWordPosition="234">ion retrieval (Anick, 2003; Terra and Clarke, 2004) and sponsored search (Jones et al., 2006). Work on semantic entailment often includes lexical entailment as a subtask (Dagan et al., 2005). We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class. For example, the terms “dog”, “puppy”, “canine”, “schnauzer”, “cat” and “pet” are highly related terms, which can be identified using techniques that include distributional similarity (Lee, 1999) and withindocument cooccurrence measures such as pointwise mutual information (Turney et al., 2003). These techniques, however, do not allow us to distinguish the more specific relationships: • hypernym(dog,puppy) *This work was carried out while these authors were at Yahoo! Research. • hyponym(dog,canine) • coordinate(dog,cat) Lexical resources such as WordNet (Miller, 1995) are extremely useful, but are limited by being manually constructed. They do not contain semantic class relationships for the many new terms we encounter in text such as web documents, for example “mp3 player” or “ipod”. We can use WordNet as training data for such classification to the extent that the training on pair</context>
<context position="4076" citStr="Turney et al. (2003)" startWordPosition="606" endWordPosition="609"> Conference Poster Sessions, pages 49–56, Sydney, July 2006. c�2006 Association for Computational Linguistics syntactic features • A learning curve for classification of query phrase pairs that suggests the primary bottleneck is manually labeled training instances: we expect our benchmark to be surpassed. 2 Relation to Previous Work Snow et al. (2005) demonstrated binary classification of hypernyms and non-hypernyms using WordNet (Miller, 1995) as a source of training labels. Using dependency parse tree paths as features, they were able to generalize from WordNet labelings to human labelings. Turney et al. (2003) combined features to answer multiple-choice synonym questions from the TOEFL test and verbal analogy questions from the SAT college entrance exam. The multiplechoice questions typically do not consist of multiple closely related terms. A typical example is given by Turney: • hidden:: (a) laughable (c) ancient (b) veiled (d) revealed Note that only (b) and (d) are at all related to the term, so the algorithm only needs to distinguish antonyms from synonyms, not synonyms from say hypernyms. We use as input phrase pairs recorded in query logs that web searchers substitute during search sessions.</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>P.D Turney, M.L. Littman, J. Bigham, and V. Shnayder, 2003. Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003, chapter Combining independent modules in lexical multiple-choice problems, pages 101–110. John Benjamins.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>