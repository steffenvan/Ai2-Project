<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.699643">
ACL Lifetime Achievement Award
Word Play
</title>
<author confidence="0.992409">
Lauri Karttunen*
</author>
<affiliation confidence="0.9881785">
Palo Alto Research Center
Stanford University
</affiliation>
<note confidence="0.398113">
This article is a perspective on some important developments in semantics and in computational
linguistics over the past forty years. It reviews two lines of research that lie at opposite ends of
the field: semantics and morphology. The semantic part deals with issues from the 1970s such
</note>
<bodyText confidence="0.593149428571429">
as discourse referents, implicative verbs, presuppositions, and questions. The second
part presents a brief history of the application of finite-state transducers to linguistic analysis
starting with the advent of two-level morphology in the early 1980s and culminating in
successful commercial applications in the 1990s. It offers some commentary on the relationship,
or the lack thereof, between computational and paper-and-pencil linguistics. The final section
returns to the semantic issues and their application to currently popular tasks such as textual
inference and question answering.
</bodyText>
<sectionHeader confidence="0.864147" genericHeader="abstract">
1. Prologue
</sectionHeader>
<bodyText confidence="0.999638">
Thirty-eight years ago, in the summer of 1969 at the second meeting of COLING in
S˚anga-S¨aby in Sweden, I stood for the first time in front of a computational audience
and started my talk on Discourse Referents by reading the following passage (Karttunen
1976):
Consider a device designed to read a text in some natural language, interpret it,
and store the content in some manner, say, for the purpose of being able to answer
questions about it. To accomplish this task, the machine will have to fulfill at least the
following basic requirement. It has to be able to build a file that consists of records of all
individuals, that is, events, objects, etc., mentioned in the text and, for each individual,
record whatever is said about it. Of course, for the time being at least, it seems that such
a text interpreter is not a practical idea, but this should not discourage us from studying
in the abstract what kind of capabilities the machine would have to possess, provided
that our study provides us with some insight into natural language in general.
The paper went on to discuss the circumstances that allow a pronoun or a definite
description to refer to an object introduced by an indefinite noun phrase. For example,
in (1a), the pronoun It can refer to Bill’s car, but in (1b) it cannot.
</bodyText>
<footnote confidence="0.656218666666667">
* Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94062, USA. E-mail:
karttunen@parc.com. This article is the text of the talk given on receipt of the ACL’s Lifetime
Achievement Award in 2007.
</footnote>
<note confidence="0.7386745">
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
</note>
<listItem confidence="0.3600885">
(1) a. Bill has a cari. Iti/The cari is black.
b. Bill doesn’t have a cari. *Iti/*The cari is black. 1
</listItem>
<bodyText confidence="0.958891076923077">
A year later in 1970, I gave my first ACL presentation at the 8th annual meeting in
Columbus, Ohio. The title of the invited talk was The Logic of English Predicate Comple-
ment Constructions. It started off with the following declaration (Karttunen 1971b):
It is evident that logical relations between main sentences and their complements are of
great significance in any system of automatic data processing that depends on natural
language. For this reason a systematic study of such relations, of which this paper is an
example, will certainly have a great practical value, in addition to what it may
contribute to the theory of the semantics of natural languages.
The paper presented a classification of verbs and constructions that take sentential
complements, that-clauses and infinitival complements, based on whether the sentence
commits the author to the truth or falsity of the complement clause. For example, all the
sentences in (2) imply, for different reasons, that the complement is true while all the
sentences in (3) imply that the complement is false.2
</bodyText>
<listItem confidence="0.995154666666667">
(2) a. John forgot that Mary was sick. Mary was sick.
b. Bill managed to solve the problem. Bill solved the problem.
c. Harry forced Ed to leave. Ed left.
(3) a. John pretended that Mary was sick. Mary was not sick.
b. Bill failed to solve the problem. Bill did not solve the problem.
c. Harry prevented Ed from leaving. Ed did not leave.
</listItem>
<bodyText confidence="0.999939">
Neither one of these two papers would have been accepted at this 2007 ACL conference.
There was no implementation, no evaluation, and very little discussion of related work.
In the happy childhood of computational linguistics even the most junior person in the
field, like myself, was allowed—even invited—to give a talk at the main COLING/ACL
session about uncharted linguistic phenomena. It was a small field then.
A future historian of the field might be puzzled by the 1969 and 1970 papers.
They were written by a postdoctoral research associate at the University of Texas at
Austin, who had arrived from Finland in 1964 by way of the University of Indiana at
Bloomington where he had just received a Ph.D. in Linguistics. Where did the young
man acquire, and why was he spouting, that kind of computational rhetoric, when the
record shows that for the next ten years he never laid his hands on a computer?
The fact is that I did have a brush with computational linguistics before settling
down to do pure semantics in the 1970s. I wanted to do linguistics because of Syntactic
Structures (Chomsky 1957) and when the Uralic and Altaic Studies Department in
Bloomington offered me a job in 1964 as a “native informant” in Finnish I accepted
and managed to get into the Linguistics department as a graduate student. My job title
turned out not to be accurate. During my first two years in Bloomington I was teaching
Finnish on my own for nine hours per week. Luckily, I signed up for a course on
computational linguistics taught by an excellent teacher and mentor, Robert E. Wall. Bob
Wall had participated in an early MT project at Harvard and in a project on automatic
</bodyText>
<footnote confidence="0.97042025">
1 The subscripts i and j are referential indices. Two noun phrases with the same referential index are
supposed to be coreferential, that is, they should refer to the same object.
2 I am using the word imply as a generic cover term for entail, presuppose, and conventionally implicate. More
about this in Sections 2.2 and 2.3.
</footnote>
<page confidence="0.993494">
444
</page>
<note confidence="0.600504">
Karttunen Word Play
</note>
<bodyText confidence="0.970643326530612">
summarization at IBM. In his course, we learned formal language theory from notes
that eventually became a book (Wall 1972), a bit of Fortran and COMIT, a language
developed by Victor Yngve at MIT. I wrote a program on punched cards to randomly
generate sentences from a small grammar of Finnish. Thanks to Bob, I was rescued from
my indentured servitude in the Uralic and Altaic Studies. In my third and final year in
Bloomington, I worked as a research assistant in the Computer Center with no specific
duties other than to be a liaison to the Linguistics Department. My only accomplishment
in that role was to save piles of anthropological data from obsolescence by writing a
program to transform rolls of 5-channel paper tape to 6-channel magnetic tapes. By
doing that, I became one of the few linguists who could explain the joke, There are 10
kinds of linguists: those who know binary and those who don’t. I suspect that the data on my
tapes for the Control Data 3600 computer have now been lost. We still have the data on
manuscripts hundreds of years old but much of the content created in the first decades
of the computer age is gone forever.
Just as I was starting to work on my dissertation in 1967, I had the good fortune of
getting a one-year fellowship at the RAND corporation in Santa Monica, California, in
the group headed by David G. Hays, the author of the first textbook in our field (Hays
1967), and the founder of the Association for Machine Translation and Computational
Linguistics (AMTCL, the predecessor of our ACL). The main focus of Hays’s team was
Russian-to-English machine translation. Remarkably, Hays was also one of the authors
of the infamous 1966 ALPAC report that inexorably caused the shutdown of all gov-
ernmentally funded MT projects, including the one at RAND. Because the term machine
translation had acquired a bad odor, the 1968 meeting of AMTCL dropped MT from its
name and became ACL. At the 1970 meeting, the first one that I attended, people were
still bitterly arguing about the matter.
In Hays’s group at RAND I again met Martin Kay, who had been my teacher in a
course on parsing at the 1966 Linguistic Institute at UCLA. My term paper for Martin’s
course had been on the computational analysis of Finnish morphology, a topic to
which I would eventually return some fifteen years later. Happy to become Martin’s
student again, I learned Algol, an elegant new programming language, and got an
understanding of the beauty of recursive algorithms. Martin was running an exciting
weekly colloquium series. I teamed up with an intern by the name of Ronald Kaplan
for a small study project and we gave a joint presentation about our findings. Ron and
I agree that we did this together but neither one remembers what we said. It probably
was about the similarities and differences between pronouns and logical variables, the
topic of my first published paper (Karttunen 1969b).
At the time the prevailing assumption was that symbolic logic provides an appro-
priate system for semantic representation within transformational grammar (McCawley
1970). But as I showed in the 1969 CSL paper, even cases as simple as (4a) and (4b) could
not be treated adequately within the proposed framework.
(4) a. The mani who loved hisi wifej kissed herj.
b. I gave each student a cookiei. Some of them ate iti right away.
c. The piloti who shot at itj hit the Migj that chased himi.
The problem with (4a) is that the phrase hisi wife has to be treated in situ as it cannot
be replaced by another coreferential noun phrase, say Mary, without changing the
meaning. (4a) implies that only one man in some group of men loved his wife, which is
not the same as there being just one man who loved Mary even if the two noun phrases
pick out the same individual. For this reason there was no way in a system such as
McCawley’s to link hisi wifej to herj. In the case of (4b), the problem is that there is no
</bodyText>
<page confidence="0.997599">
445
</page>
<note confidence="0.799337">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.999728590909091">
unique cookie to serve as the referent of it. Being an anaphoric pronoun linked to an
antecedent does not necessarily mean that the two are coreferential, at least not in any
naive sense of coreference.
Even worse problem cases were known. (4c) is a typical “Bach-and-Peters sen-
tence,” named after its inventors, Emmon Bach and Stanley Peters. As I was going
to devote a chapter of my dissertation to this topic, I was lucky to run into them at a
conference in San Diego. I found the two very intimidating in their suits and crew cuts.
They looked like Haldeman and Ehrlichman, a pair of Nixon aides. But at least I found
out that the problem had not been solved.
As the year at RAND when on, I spent less and less time at the computer and a lot
of time walking up and down the Santa Monica pier just down the cliff from my office
thinking about pronouns, variables, reference, and definiteness. I went up to UCLA a
few times to discuss these issues with Barbara Partee and gave a talk in her seminar.
The topic of my dissertation was Problems of Reference in Syntax, in principle due before
I left RAND but finished half-a-year later (Karttunen 1969a).
By the summer of 1968 I had two job offers. David Hays was leaving RAND for SUNY
in Buffalo and he offered me a job there. But I chose to become a Faculty Associate
in the Linguistics Department at the University of Texas at Austin. Climate was one
consideration, but, more importantly, Austin was where Emmon Bach and Stanley
Peters were. My Indiana mentor, Bob Wall, had just moved into the same department.
For the next ten years I had very little contact with Martin Kay and Ronald Kaplan but
they became very important people in Act II of my life.
</bodyText>
<sectionHeader confidence="0.920829" genericHeader="keywords">
2. Act I: Framing Problems
</sectionHeader>
<bodyText confidence="0.999911">
I started my career in Austin in the fall of 1968 and became a regular faculty member
in 1970. My work on discourse referents was largely done when I arrived in Austin.
I went on to study so-called implicative verbs such as manage and fail, a subtopic in
the discourse referents paper, and branched to other types of verbs that take sentential
complements. One important semantic class of verbs with sentential complements,
called factives, had already been identified and discussed by Zeno Vendler (1967) and
Paul and Carol Kiparsky (Kiparsky and Kiparsky 1971) at MIT. Factive verbs were said
to presuppose that the complement clause is true.
As it happened, I started to think about factives and presuppositions at MIT in the
Fall of 1972. In the spring before I had a surprise phone call from Paul Kiparsky who
said that the MIT Department was still looking for a one-year replacement for David
Perlmutter who was going on a sabbatical. Would I be interested? Of course I was. I had
come to the U.S. seven years earlier to study linguistics because of Noam Chomsky
and now I had an office just across from hall from his. But during the year I was
there, I lost interest in transformational syntax. I found Chomsky’s Thursday lectures
of that year, on themes later published as Conditions on Transformations (Chomsky 1973),
uncompelling.
My sense of what was interesting had changed. The “Linguistic Wars” (Harris
1995) between generative (George Lakoff, John Ross, James D. McCawley, Paul Postal,
and others) and interpretive semantics (Ray Jackendoff and others) had been won by
Chomsky for the interpretivists, although Chomsky himself was, and still is, skeptical
of any kind of formal theory of meaning. My sympathies were with the losing side.
But I sensed that both camps were essentially doing syntax, albeit in different ways.
Barbara Partee had convinced me in our discussions about pronouns and variables at
UCLA that model theory and intensional logic was the right approach to semantics. But
</bodyText>
<page confidence="0.997988">
446
</page>
<subsectionHeader confidence="0.287966">
Karttunen Word Play
</subsectionHeader>
<bodyText confidence="0.999689055555555">
it was going to take a while before I could do anything original within that emerging
paradigm. At MIT I gave a “formal methods” course for a few linguistic students starting
with Bob Wall’s textbook (Wall 1972) and finishing with Montague Grammar that I was
just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on
my own topics: discourse referents, implicative verbs, and presuppositions. I had one
star student in the seminar by the name of Mark Liberman, who wrote a Master’s Thesis
poking holes in my emerging ideas about presuppositions.
In the 1970s, the Linguistics Department in Austin was an excellent place for a
young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee
Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excellent semantics
and syntax students. David Dowty, Per-Kristian Halvorsen, Roland Hausser, Orvokki
Hein¨am¨aki, Jim McCloskey, and Hans Uszkoreit got their degrees from UT Austin while
I was there. Orvokki was my first Ph.D. student. She wrote an insightful thesis on the
meaning of before and other temporal connectives (Hein¨am¨aki 1974).3
In the 1970s I worked on four general topics: discourse referents, implicative verbs,
presuppositions, and questions. This is not an occasion to deep-end into any of these
topics but I will discuss each of them briefly in the following sections to give a general
idea of what I think my contributions were.
</bodyText>
<subsectionHeader confidence="0.989975">
2.1 Discourse Referents
</subsectionHeader>
<bodyText confidence="0.99997395">
The obvious difference between definite and indefinite noun phrases is that garden-
variety definite NPs such as the car in simple main clauses imply the existence of an
individual or an object but indefinite noun phrases such as a car often do not. In that
respect, definite NPs are similar to definite pronouns such as it in contexts where the
pronoun does not play the role of a bound variable. The reason for the incoherence of
(1b) is that it tells us explicitly that there is no such car.
Given an indefinite noun phrase, when is there supposed to be a corresponding
individual that it describes? This was the question I tried to answer in the 1969 paper
on discourse referents, excerpted from the first chapter of my Indiana dissertation. The
conclusion I came to was that a pronoun or a definite description could refer back to
(or be an anaphor for) an indefinite NP (the antecedent) just in case the existence of
the individual was semantically implied by the text. Put in this simple way, the answer
seems obvious but it gave rise to many problems some of which remain unsolved to
this day. The novelty of the approach was that it rephrased the problem of pronom-
inalization in semantic terms. Up to that point in transformational grammar, all the
discussion about anaphors and antecedents had been about the constraints on their
syntactic configurations.
Simple affirmative sentences such as Bill has a car in (1a) obviously imply existence
and simple negative sentences such as Bill doesn’t have a car in (1b) imply the opposite.
The type of the verb matters, as seen in (5).
</bodyText>
<listItem confidence="0.590723">
(5) a. The director is looking at an innocent blondei. Shei is from Bean Blossom.
</listItem>
<bodyText confidence="0.9837825">
b. The director is looking for an innocent blondei. #Shei is from Bean Blossom.
So-called intensional verbs such as look for, need, and want introduce an ambiguity.
In (5b), the phrase an innocent blonde may be understood in two ways. In the specific
sense it describes a particular individual that we can refer to as she. But (5b) can also be
</bodyText>
<page confidence="0.874865">
3 As I was preparing this talk, I heard the sad news from Finland that Orvokki Hein¨am¨aki had died.
447
</page>
<note confidence="0.296513">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.994473526315789">
interpreted nonspecifically, describing the type of girl the director is looking for. In that
sense, the continuation is incoherent because there is not yet any individual to refer to.
(5a) has no such ambiguity; it entails the existence of an innocent blonde in the actual
world and we can talk about her.
But matters are more complicated. Although (5b) under a nonspecific reading of
an innocent blonde does not establish a discourse referent in the actual world, we can
nevertheless have one in a modal or hypothetical context, as in (6).
(6) The director is looking for an innocent blondei. Shei must be 17 years old.
There is another problem here. If we interpret an innocent blonde nonspecifically in (6),
then must has a deontic reading. It is a requirement that she be 17 years old. However,
on the specific reading must gets an epistemic interpretation. That is, we have made an
inference about the age of the girl in question from her looks or other evidence.
My work on discourse referents was a harbinger of the vast literature yet to come on
this topic including Bonnie Webber’s 1978 dissertation (Webber 1978), Irene Heim’s file
change semantics (Heim 1982), and the theory of discourse representation structures
(DR(S) Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993).
Looking back at my old paper, I am amused by the youthful innocence with which it
approached the topic but I am also impressed by the fact that some of the problems it
uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved.
</bodyText>
<subsectionHeader confidence="0.999948">
2.2 Semantics of Complementation
</subsectionHeader>
<bodyText confidence="0.9999415">
An indefinite noun phrase creates a stable discourse referent just in case the clause it
is bound to is implied to be true by the context in which it appears.4 That was the
main idea in the 1969 paper. In the course of seeking evidence for this thesis, I came
across an interesting class of verbs and constructions that give rise to such implications
(Karttunen 1971a, 1971b). For example, the contrast between (7a) and (7b) is explained
by the semantic properties of the two verbs, manage and fail.
</bodyText>
<listItem confidence="0.974705">
(7) a. John managed to get a sabbaticali. Iti starts in September.
b. John failed to get a sabbaticali. *Iti starts in September.
(7a) entails that John got a sabbatical, (7b) entails that he didn’t. The interesting fact
about these verbs is that when we change the polarity from positive to negative we still
get an entailment, but of the opposite polarity as seen in (8).
(8) a. John didn’t manage to get a sabbaticali. *Iti starts in September.
b. John didn’t fail to get a sabbaticali. Iti starts in September.
</listItem>
<bodyText confidence="0.996992833333333">
There exists quite a number of such two-way implicatives that yield an entailment in
both positive and negative contexts. Verbs like manage yield a positive entailment in
positive contexts (++) and a negative entailment in negative contexts (−−). Let us call
them ++/−−−− implicatives. Two-way implicatives like fail flip the polarity, so we call
them +−/−+ implicatives. Table 1 gives a few examples of both types of verbs and
constructions.
</bodyText>
<footnote confidence="0.992271666666667">
4 The term “stable” is in contrast with “short-term” for referents that have a limited life span. For example,
we can talk about a nonexistent car as in in I wish Mary had a cari. She could take me to work in iti. I could
drive the cari too. as long as we are elaborating a hypothetical situation.
</footnote>
<page confidence="0.990748">
448
</page>
<table confidence="0.476076">
Karttunen Word Play
</table>
<tableCaption confidence="0.995022">
Table 1
</tableCaption>
<bodyText confidence="0.481348">
Some two-way implicative verbs and constructions.
</bodyText>
<equation confidence="0.5330648">
+ + / − − implicatives + − / − + implicatives
manage (to) fail(to)
succeed (in) neglect (to)
remember (to) forget (to)
happen (to) avoid ... (ing)
</equation>
<bodyText confidence="0.226921333333333">
see fit (to) refrain (from)
take the time ... (to) shy away (from)
have the foresight (to) stop NP (from)
</bodyText>
<tableCaption confidence="0.8745188">
Table 2
Some one-way implicative verbs and constructions.
++ implicatives +− implicatives −− implicatives −+ implicatives
cause NP (to) prevent NP (from) can hesitate (to)
force NP (to) preclude NP (from) be able (to)
</tableCaption>
<bodyText confidence="0.722076">
Table 2 contains examples of verbs and constructions that certainly yield an entail-
ment in one direction but not necessarily the other way.
For example, it is tempting to conclude from (9a) that the president attended the
meeting—and if there is no reason to think otherwise, the reader is entitled to that
conclusion. Nevertheless, the author may take away that “invited inference” (Geis and
Zwicky 1971) without contradicting himself as in (9b).
</bodyText>
<listItem confidence="0.915424">
(9) a. The president was able to attend the meeting.
</listItem>
<bodyText confidence="0.62228725">
b. The president was able to attend the meeting but decided not to.
The entailments of constructions involving more than one implicative verb have to
be computed from “top–down.” The two examples in (10) establish a stable discourse
referent because they both entail that a picture was taken.
</bodyText>
<listItem confidence="0.990654">
(10) a. John managed not to forget to take a picture.
b. Bill failed to prevent John from taking a picture.
</listItem>
<bodyText confidence="0.9976603">
The early version of Kamp’s Discourse Representation Theory did not include any
mechanism for computing lexical entailments about existence. I found the DRS boxes
disappointingly static at the time.
The semantics of complementation that I proposed was picked up by some compu-
tational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph
Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can
be computed directly by the parser, in contrast to the then prevailing view of the AI
community that all inferences have to come from some giant inference engine. This
was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database
systems (Kaplan 1977).
</bodyText>
<subsectionHeader confidence="0.97886">
2.3 Presuppositions—Conventional Implicatures
</subsectionHeader>
<bodyText confidence="0.977317">
The semantics of two-way implicatives puzzled me greatly when I first discovered them
(Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow
</bodyText>
<page confidence="0.996666">
449
</page>
<note confidence="0.260164">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.6775215">
that the construction manage to is empty of meaning. In general, if p entails q and ¬p
entails ¬q, then it logically follows that p and q are equivalent: p ≡ q.
</bodyText>
<listItem confidence="0.9986845">
(11) a. John managed to speak. h John spoke.
b. John did not manage to speak. h John did not speak.
</listItem>
<bodyText confidence="0.6826645">
But this is of course wrong as far as (11) is concerned. Choosing the construction manage
to commits the speaker to the view that there was some difficulty involved. All the
verbs in Table 1 bring in some additional commitment over and beyond what is entailed
although it is difficult in some cases to pin down exactly what it is. Furthermore, the
commitment remains the same regardless of whether the sentence is affirmative or
negative. It is also present in questions and conditionals as shown in (12).
</bodyText>
<listItem confidence="0.628195">
(12) Did John manage to speak?
If John managed to speak, it is a good sign.
</listItem>
<bodyText confidence="0.9996162">
The extra bits of meaning attached to the two-way implicatives were yet another in-
stance of a phenomenon that had already been discussed for some time under the term
presupposition. The term came from philosophers who had been debating heatedly
and for a long time whether The present king of France is false, meaningless, or lacking
a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964).
When linguists got into the act in the late 1960s, being more systematic observers of
language, within a span of just a few years they collected a large zoo of other types
of constructions besides definite descriptions that seem to involve presuppositions
(Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not
sort them into different habitats. Here are some examples:
</bodyText>
<listItem confidence="0.995235307692308">
• Factive verbs: Mary forgot/didn’t forget that John had left.
• Factive adjectives: It is/isn’t odd that the room is closed.
• Change-of-state verbs: John stopped/hasn’t stopped smoking.
• Verbs of judging: John criticized/didn’t criticize Harry for writing the letter.
• Wh-questions: Who is coming for dinner?
• Headless relatives: Chicago is/isn’t where Fred met Sally.
• Cleft sentences: It was/wasn’t John who caught the thief.
• Pseudo-clefts: What she wants/doesn’t want to talk about is herself.
• Temporal subordinate clauses: John left/didn’t leave after Mary called.
• Iteratives: Fred called/didn’t call again. Fred ate/didn’t eat another turnip.
In addition to vastly enlarging the presupposition population, the linguistic community
also came up with a problem that had been ignored in the philosophical literature up to
that point:
</listItem>
<bodyText confidence="0.4933995">
Projection problem: How are the presuppositions of a complex sentence derived
from the presuppositions of the component clauses?
</bodyText>
<page confidence="0.976032">
450
</page>
<note confidence="0.214987">
Karttunen Word Play
</note>
<bodyText confidence="0.997838375">
This question was first posed by Langendoen and Savin (1971). Their answer was
(page 57):
The projection principle for presuppositions, therefore, is as follows: presuppositions of
a subordinate clause do not amalgamate either with presuppositions or assertions of
higher clauses; rather they stand as presuppositions of the complex sentence in which
they occur.
They were badly mistaken. Although the consequent clause of (13) by itself presupposes
the existence of a unique king of France, (13) as a whole obviously does not.
</bodyText>
<listItem confidence="0.802081">
(13) If France has a king, I bet the king of France speaks only French.
</listItem>
<bodyText confidence="0.999446038461538">
In a conditional sentence, a presupposition of the consequent clause can be “filtered” or
“cancelled” away if it is entailed by the antecedent and general background knowledge.
If a presupposition is not filtered locally and is not part of the context of the discourse,
the reader or hearer must in some way adjust his or her state of knowledge to incorpo-
rate the new information. The idea is in Karttunen (1974, page 191):
If the current conversational context does not suffice, the listener is entitled and
expected to extend it as required. He must determine for himself what context he is
supposed to be in on the basis of what is said and, if he is willing to go along with it,
make the same tacit extension that his interlocutor appears to have made.
Lewis (1979) called this process accommodation. There is a huge literature on the
projection problem and accommodation. Among the papers often cited are Karttunen
(1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979),
Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der
Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts
(1999, page 5) sums up the early developments as follows:
An especially stark illustration of the disparity of the field, at least in its early days, is
the work of a Karttunen, who within the span of six years published three theories that
were mutually inconsistent, technically as well as conceptually.
I don’t disagree with that assessment.5 It seems to me that by now the notions of pre-
supposition projection and accommodation have outlived their usefulness. It is evident
that no uniform theory can account for all the phenomena that historically have been
lumped together under the label presupposition.
But at least one good insight has emerged from this line of research. The accommo-
dation strategy for definite descriptions is closely linked to anaphora resolution (van der
Sandt 1992). One motivation for Kamp’s DRS theory was to be able to handle “donkey
anaphora” in sentences such as (14a).
</bodyText>
<listItem confidence="0.9287975">
(14) a. If John has a donkeyi, he beats iti.
b. If John has children, all of John’s children are bald.
</listItem>
<bodyText confidence="0.954735">
What van der Sandt observed was that the treatment of the anaphor in (14a) could be
used in (14b) to eliminate the presupposition that John has children.
</bodyText>
<footnote confidence="0.413121">
5 Except for the dismissive Gricean implications triggered by the indefinite article in a Karttunen.
</footnote>
<page confidence="0.987947">
451
</page>
<note confidence="0.284593">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.999848">
Assimilating presupposition projection into anaphora resolution is probably the
right approach for definite descriptions and for iterative presuppositions triggered by
prefixes such as re- in verbs like recalculate and particles such as too and again. However,
it does not seem applicable to the kinds of presuppositions triggered by implicative
verbs or factives.
But the whole idea of accommodation is inappropriate for implicative verbs. Exam-
ples such as (11) and (12) commit the speaker to the view that it was difficult for John
to speak. The audience may take note of that piece of information but it does not need
to be accepted or accommodated for the discourse to proceed. Another phenomenon
that does not call for any accommodation is it-clefts. As Ellen Prince (1978) showed, a
sentence such as (15) does not covertly slip into the discourse a piece of new information
disguised as being old. On the contrary, the rhetorical force of the it-cleft is to tell you
something that presumably you did not know before in a manner that makes the new
piece of information incontestable.
</bodyText>
<listItem confidence="0.766364333333333">
(15) It was/wasn’t Barbara Partee who in a private conversation around
1980 suggested to me that anaphora resolution and the satisfaction
of the presuppositions of definite descriptions was the same problem.
</listItem>
<bodyText confidence="0.9999866">
In my joint last paper on presuppositions (Karttunen and Peters 1979), Stanley
Peters and I proposed to do the sensible thing, namely to divide up the heterogeneous
collection of phenomena that had been lumped together under this misbegotten label.
We suggested that many cases that had been called presupposition are best seen as
instances of what Grice (1979) had called conventional implicature. Conventional im-
plicatures are propositions that the speaker or the author of the sentence is committed
to by virtue of choosing particular words or constructions to express himself or herself.
However, whether those implicatures are true or not does not have any bearing on
whether the sentence is true or false. For example, because of the word even, (16)
commits the author to the view that Bill is an unlikely person to agree with Mary.
</bodyText>
<listItem confidence="0.748637">
(16) Even Bill agrees with Mary.
</listItem>
<bodyText confidence="0.998184833333333">
But the meaning contributed by even plays no role in determining the truth conditions
of the sentence. (16) is true if Bill agrees with Mary and false otherwise.
Our good advice went unheeded for a long time but in recent work by Christopher
Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley
and I sketched out that separates conventional implicatures from truth-conditional
aspects of meaning.
</bodyText>
<subsectionHeader confidence="0.997189">
2.4 Syntax and Semantics of Questions
</subsectionHeader>
<bodyText confidence="0.999843">
My paper on questions (Karttunen 1977) was an ambitious effort to give a unified
account in the framework of Montague Grammar of the meaning of all types of inter-
rogative phrases including direct questions such as the examples in (17) and embedded
interrogatives illustrated in (18).
</bodyText>
<listItem confidence="0.98521675">
(17) a. Is it raining?
b. Do you want to go or do you want to stay?
c. Which book did Mary read?
d. Which girls date which boys?
</listItem>
<page confidence="0.98565">
452
</page>
<author confidence="0.210843">
Karttunen Word Play
</author>
<listItem confidence="0.950289">
(18) a. John knows whether Bill smokes.
b. Mary is thinking about whether to stay home or go to the movies.
c. Bill remembers to whom John gave the book?
d. She doesn’t care about who did what to whom.
</listItem>
<bodyText confidence="0.999668785714286">
Examples (17a) and (18a) are yes/no questions. (17b) and (18b) are alternative ques-
tions that pose two or more choices. As (17c,d) and (18c,d) illustrate, wh-questions may
contain any number of interrogative quantifiers.
By and large, embedded yes/no and alternative questions have the same syntactic
distribution as embedded wh-questions.6 For that reason a syntactician would prefer
to have just a single category of embedded questions. In the framework of Montague
Grammar this is possible only if all types of embedded interrogatives have the same
type of meaning. From a semantic point of view, it would be desirable to assign a
single type of meaning to both direct and embedded questions. For example, which
girls date which boys should have the same meaning as a direct question that it has
when embedded under a verb such as find out. Finally, whatever meaning we assign
to interrogatives, it should help us to elucidate the meaning of question-embedding
verbs including the examples in (18) and the one in (19) that sets up a relation between
two questions.
</bodyText>
<listItem confidence="0.881214">
(19) Whether Mary comes to the party depends on who invites her.
</listItem>
<bodyText confidence="0.999979904761905">
With these desiderata in mind, I came to the conclusion that the best solution would
be to adopt an approach proposed by Hamblin (1973) for direct questions and carry it
further. Hamblin’s idea was to let every direct question denote a set of propositions,
namely, the set of propositions expressed by all the possible answers to the question.
For example, under Hamblin’s analysis Is it raining? denotes the set containing two
propositions {It is raining, It is not raining}. My improvement of that idea was to make
the meaning of a question be a function that in each possible world picks up the set of
true answers to the question.
Under this new analysis it is possible to relate, for example, the meaning of know
with a that-complement to the meaning of know with an embedded question as in (18a).
If Bill in our actual world is a smoker, then in our world whether Bill smokes picks out
the set consisting of the proposition that Bill smokes. In that case, what John knows is
that Bill smokes. In examples such as (19) the meaning of depend on can be explicated
as a function that in each world maps the true answers to who invites Mary onto the
true answer(s) to whether Mary comes to the party. I worked out these ideas with all the
rigor of a Montague grammarian. After years of apprenticeship I had finally become a
competent formal semanticist.7
A less restless soul would have stopped right there. But I didn’t. As others saw
it, I fell from the pinnacle of semantics into the low life of finite-state automata. My
semanticist friends kept asking, “What happened to you Lauri? You were such a good
semanticist.” The politely unstated premise was that I had fallen onto skid row.
</bodyText>
<footnote confidence="0.868153">
6 There are two yet unexplained exceptions to this generalization. So-called “emotive factives” such as
be surprised take embedded wh-questions but not whether-questions: You’d be surprised where you find us,
*You’d be surprised whether you find us. “Dubitative” verbs such as doubt have the opposite characteristic.
7 Compared to the lively activity on the presupposition playground, the field of questions attracts few
visitors. For later developments, see Hausser and Zaefferer (1979), Hausser (1983), Groenendijk and
Stokhof (1984), and Ginsburg (1992, 1996).
</footnote>
<page confidence="0.995614">
453
</page>
<note confidence="0.396434">
Computational Linguistics Volume 33, Number 4
</note>
<sectionHeader confidence="0.910733" genericHeader="introduction">
3. Interlude
</sectionHeader>
<bodyText confidence="0.999982769230769">
Towards the end of the 1970s I began to think that I had stumbled on, and helped to
create and frame, more problems in semantics than I could ever solve. It was time to
move on and leave the mess for others to clean up. In a bold move I signed up to teach
a course on computational linguistics. As every professor knows, teaching a course
on something you know next to nothing about is a great learning opportunity. To get
some idea of how the field had developed in the previous ten years I went to the 1979
ACL Annual Meeting in La Jolla, California, and immediately ran into two of my old
colleagues from RAND, Martin Kay and Ron Kaplan, very surprised to see me. “What
are you doing here?” they asked. I said I had picked up a new hobby and was planning
to do some computational work on Finnish morphology, the topic of my term paper for
Martin’s course in 1966.
On my sabbatical year at the Center for Advanced Study in Behavioral Sciences
(CASBS) at Stanford, 1981–1982, I often went to visit Martin at the Palo Alto Research
Center (PARC) just a short drive from CASBS. I learned about unification and InterLisp.
I got to compute on the Alto personal computer and even had my own personal 1 MB
floppy for it, about the size of a large briefcase. On the floppy was the project Martin
and I were collaborating on, a unification based parser/generator for Finnish. Finnish
was a good test case for Martin’s functional unification grammar (FUG) formalism. In
FUG, constituents could be labeled by a syntactic category such as NP and could be
assigned functional roles such as CONTRAST and TOPIC. We showed how the constraints
on Finnish word order could be described and implemented in those terms (Karttunen
and Kay 1985a).
I joined the Artificial Intelligence Center at SRI in 1984. It was a good time to
make the move from academia to industrial research. SRI had an excellent mix of
computational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, Robert
Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there,
among others. SRI’s AI Center and Xerox PARC were cofounders of the new Center for
the Study of Language and Information (CSLI) at Stanford, funded by a generous grant
from the System Development Foundation, an offshoot of the RAND Corporation. At
SRI, Stuart Shieber had designed and implemented his influential PATR II formalism
for unification-based grammars (Shieber et al. 1983). I implemented it (Karttunen 1984;
Karttunen and Kay 1985b; Karttunen 1986) at CSLI in Interlisp on a Xerox Dandelion, a
wonderful machine with Interlisp as the language of the operating system.
In 1987 I joined my friends at Xerox PARC to concentrate on my other computational
interest: finite-state morphology. In making the crosstown transit from Menlo Park to
Palo Alto, I graduated from my lovely Dandelion to the top-of-the-line Xerox Dorado,
still the best computing experience in my life. After all the years spent on theorizing
and playing with formalisms, I wanted to do something practical that would have an
impact on the real world.
</bodyText>
<sectionHeader confidence="0.84729" genericHeader="method">
4. Act II: Providing Solutions
</sectionHeader>
<bodyText confidence="0.9999334">
In the early 1980s, morphological analysis of natural language was a challenge to
computational linguists. Simple cut-and-paste programs could be written to analyze
strings in particular languages, but there was no general language-independent method
available. Furthermore, cut-and-paste programs for analysis were not reversible, they
could not be used to generate words.
</bodyText>
<page confidence="0.995135">
454
</page>
<note confidence="0.401804">
Karttunen Word Play
</note>
<bodyText confidence="0.999334">
Generative phonologists of that time described morphological alternations by
means of ordered rewrite rules introduced by Chomsky and Halle (1968). These rules
are of the form α → β / γ δ, where α, β, γ, and δ can be arbitrarily complex strings or
feature matrices. It was not understood how such rules could be used for analysis.
In 1981 I had organized a conference on parsing in Austin. Present at the conference
was a visitor from Finland, Kimmo Koskenniemi, who was looking for a dissertation
topic. Martin Kay and Ronald Kaplan were also there and it turned out that all four of
us were interested in morphology. I demoed a small system I had built with my students
for Finnish (Karttunen, Uszkoreit, and Root 1981). Martin and Ron reported that they
had recently made a breakthrough discovery in computing with rewrite rules. Kimmo
went on to California to visit them at PARC to learn more. That was the beginning of our
long collaboration.
</bodyText>
<subsectionHeader confidence="0.910456">
4.1 Origins
</subsectionHeader>
<bodyText confidence="0.99994375">
The discovery Kaplan and Kay had made was actually a rediscovery of a result that had
been published a decade before in a book that none of us knew about at that time, a
UC Berkeley dissertation by C. Douglas Johnson (1972). Johnson observed that although
the same context-sensitive rule could be applied several times recursively to its own
output, phonologists have always assumed implicitly that the site of application moves
to the right or to the left in the string after each application. For example, if the rule
α → β / γ δ is used to rewrite the string γαδ as γβδ, any subsequent application of the
same rule must leave the β part unchanged, affecting only γ or δ. Johnson demonstrated
that the effect of this constraint is that the pairs of inputs and outputs produced by a
phonological rewrite rule can be modeled by a finite-state transducer.8
Johnson was already aware of an important mathematical property of finite-state
transducers established by Sch¨utzenberger (1961): for any pair of transducers applied
sequentially there exists an equivalent single transducer. Any cascade of rule trans-
ducers can in principle be composed into a single transducer that maps lexical forms
directly into the corresponding surface forms, and vice versa, without any intermediate
representations.
Koskenniemi was impressed by the theoretical result he learned from Kaplan and
Kay, but not convinced about the practicality of the approach for morphological analysis.
Traditional phonological rewrite rules describe the correspondence between lexical
forms and surface forms as a one-directional, sequential mapping from lexical forms to
surface forms. Even if it were possible to model the generation of surface forms efficiently
by means of finite-state transducers, it was not evident that it would lead to an efficient
analysis procedure going in the reverse direction, from surface forms to lexical forms.
Let us consider a simple illustration of the problem with two sequentially applied
rewrite rules, N -&gt; m / p and p -&gt; m / m . The corresponding transducers map
the lexical form kaNpat unambiguously to kammat, with kampat as the intermediate
representation. However if we apply the same transducers in the opposite direction
to the input kammat, we get the three results shown in Figure 1. This asymmetry is an
</bodyText>
<footnote confidence="0.919466833333333">
8 Johnson did a careful analysis of what at the time was one of the most comprehensive descriptions of
phonological alternations described in the Chomsky–Halle paradigm. This was the unpublished MIT
Qualifying Paper by James D. McCawley on Finnish. The data came from McCawley’s classmate,
Paul Kiparsky, a native speaker of the language. One can claim that every advance in computational
morphology in the last 30 years involves Finnish and people whose last name begins with K. See
Section 4.3.
</footnote>
<page confidence="0.993975">
455
</page>
<figure confidence="0.806222">
Computational Linguistics Volume 33, Number 4
</figure>
<figureCaption confidence="0.6849425">
Figure 1
Deterministic generation, nondeterministic analysis.
</figureCaption>
<bodyText confidence="0.9993154">
inherent property of the generative approach to phonological description. If all the rules
are deterministic and obligatory and if the order of the rules is fixed, each lexical form
generates only one surface form. But a surface form can typically be generated in more
than one way, and the number of possible analyses grows with the number of rules that
are involved.
</bodyText>
<subsectionHeader confidence="0.909189">
4.2 Two-Level Morphology
</subsectionHeader>
<bodyText confidence="0.9998223">
Back in Finland, Koskenniemi invented a new way to describe phonological alterna-
tions in finite-state terms. Instead of cascaded rules with intermediate stages and the
computational problems they seemed to lead to, rules could be thought of as statements
that directly constrain the surface realization of lexical strings. The rules would not be
applied sequentially but in parallel. Each rule would constrain a certain lexical/surface
correspondence and the environment in which the correspondence was allowed, re-
quired, or prohibited. For his 1983 dissertation, Koskenniemi (1983) constructed an
ingenious implementation of his constraint-based model that did not depend on a rule
compiler, composition, or any other finite-state algorithm, and he called it two-level
morphology. Two-level morphology is based on three ideas:
</bodyText>
<listItem confidence="0.999883">
• Rules are symbol-to-symbol constraints that are applied in parallel, not
sequentially like rewrite rules.
• The constraints can refer to the lexical context, to the surface context, or to
both contexts at the same time.
• Lexical lookup and morphological analysis are performed in tandem.
</listItem>
<bodyText confidence="0.999488">
Applying the rules in parallel does not in itself solve the overanalysis problem illus-
trated in Figure 1. The two constraints just sketched allow kammat to be analyzed as
kaNpat, kampat, or kammat. However, the problem becomes manageable when there
are no intermediate levels of analysis. In Koskenniemi’s 1983 system, the lexicon was
represented as a forest of tries (= letter trees), tied together by continuation-class links
from leaves of one tree to the root of another tree or trees.9 Lexical lookup and the
</bodyText>
<footnote confidence="0.619343">
9 The TEXFIN analyzer I had demoed to Koskenniemi on his 1981 visit to Austin had the same lexicon
architecture (Karttunen, Uszkoreit, and Root 1981) .
</footnote>
<page confidence="0.992231">
456
</page>
<figure confidence="0.685805">
Karttunen Word Play
</figure>
<figureCaption confidence="0.979849">
Figure 2
</figureCaption>
<bodyText confidence="0.969896088235294">
Following a path in the lexicon.
analysis of the surface form are performed in tandem. In order to arrive at the point
shown in Figure 2, the analyzer has traversed a branch in the lexicon that contains
the lexical string kaN. At this point, it only considers symbol pairs whose lexical side
matches one of the outgoing arcs of the current state. It does not pursue analyses that
have no matching lexical path. All the rule networks must accept every lexical:surface
pair. In the case at hand, the p:m pair is accepted by the N:m Rule that requires a pas the
right context on the lexical side and by the p:m Rule that requires an m as the left context
on the surface side. In two-level rules, zero (epsilon) is treated as an ordinary symbol.
Because of this, a two-level rule represents an equal-length relation. Conceptually, the
system in Figure 2 simulates the intersection of the rules and the composition of the
rules with the lexicon.
Koskenniemi’s two-level morphology was the first practical general model in the
history of computational linguistics for the analysis of morphologically complex lan-
guages. The language-specific components, the rules and the lexicon, were combined
with a universal runtime engine applicable to all languages.
4.2.1 The Texas KIMMO System. I met Koskenniemi again in Finland around Christmas
time in 1982. He had just finished the first implementation of a two-level system and
gave me a printout of the program to take along, a thick stack of Pascal code. Back
home I unfolded the long printout on the floor of a corridor and spent quite a bit of time
crawling up and down the code trying to understand what it did, and learning Pascal
along the way. I was going to teach computational linguistics again in the spring. Hav-
ing figured out Kimmo’s program, it occurred to me that doing a Lisp implementation
of the two-level model would be a good class project.
We completed the project and published a collection of papers on the topic, along
with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi
got the credit for the invention, we called it the KIMMO system. The name stuck and
inspired many other KIMMO implementations. The most popular of these is PC-KIMMO,
a free C implementation from the Summer Institute of Linguistics (Antworth 1990).
In Europe, two-level morphological analyzers became a standard component
in several large systems for natural language processing such as the British Alvey
project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine
(Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the
MULTEXT project (Armstrong 1996).
</bodyText>
<footnote confidence="0.605613">
4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced
a formalism for two-level rules. The semantics of two-level rules was well-defined
</footnote>
<page confidence="0.976905">
457
</page>
<note confidence="0.480024">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.988296689655172">
but there was no rule compiler available at the time. Koskenniemi and other early
practitioners of two-level morphology constructed their rule automata by hand. This is
tedious in the extreme and very difficult for all but very simple rules.
To address this problem Kaplan, Kay, and I pooled our CSLI funds and invited
Koskenniemi to Stanford in the Summer of 1985. Although two-level rules are concep-
tually quite different from the rewrite rules studied by Kaplan and Kay, the methods
that had been developed for compiling rewrite rules were applicable to two-level rules
as well. In both formalisms, the most difficult case is a rule where the symbol that
is replaced or constrained also appears in the context part of the rule. This problem
Kaplan and Kay had already solved by an ingenious technique for introducing and
then eliminating auxiliary symbols to mark context boundaries. Another fundamental
insight they had was the encoding of context restrictions in terms of double negation.
For example, a constraint such as “p must be followed by q” can be expressed as “it is
not the case that something ending in p is not followed by something starting with q.”
In Koskenniemi’s formalism, p =&gt; q.
In the course of the summer, Kaplan and Koskenniemi worked out the basic com-
pilation algorithm for two-level rules. The first two-level rule compiler was written
in InterLisp by Koskenniemi and me in 1985–1987 using Kaplan’s implementation
of the finite-state calculus (Koskenniemi 1986; Karttunen, Koskenniemi, and Kaplan
1987). The current C-version two-level compiler, called TWOLC, was created at PARC
(Karttunen and Beesley 1992). It has extensive systems for helping the linguist to avoid
and resolve rule conflicts, the bane of all large-scale two-level descriptions.
4.2.3 Two-Level Descriptions. Many languages have been described morphologically in
the two-level framework. But in many cases the work has been done for companies
such as Lingsoft and Inxight that are in the morphology business, and the descrip-
tions have not been made public for obvious reasons. Here are some of the languages
for which there is a large-scale two-level grammar and a publication describing it:
Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern
S´ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994).
</bodyText>
<subsectionHeader confidence="0.999182">
4.3 Lexical Transducers
</subsectionHeader>
<bodyText confidence="0.999846538461538">
Soon after arriving at PARC I made a serendipitous discovery. At the time PARC was
collaborating with Microlytics, a company that marketed spell-checkers, the first suc-
cess story of finite-state morphology.10 Microlytics had licensed from Koskenniemi’s
company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the
Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan’s
compression routine to make a Finnish spell-checker in the Microlytics format. For
that task I designed an algorithm that simultaneously carried out the intersection of
Koskenniemi’s 23-rule automata and the composition with the lexicon. I was surprised
to see that not only did it work but the result was not significantly larger than the
original source lexicon. Figure 3 is a sketch of that process. Just intersecting the rule
automata by themselves was barely possible for us then because of the exponential
worst-case complexity of the intersection algorithm. We assumed that the composition
with a large lexicon might make the computation even harder to carry out. In fact the
</bodyText>
<page confidence="0.836788">
10 Ron Kaplan will tell you more about that some day.
458
</page>
<figure confidence="0.889798333333333">
Karttunen Word Play
Figure 3
Intersecting and composing two-level rules with a lexicon.
</figure>
<figureCaption confidence="0.980306">
Figure 4
</figureCaption>
<bodyText confidence="0.983298">
A path in a lexical transducer.
opposite happened. The reason should have been obvious from the beginning. The
intersection of a set of two-level rules explodes because it has to compute a result for any
lexical string. But if the set of lexical inputs is restricted to the forms that actually exist
in the language, there is no blowup. The same applies to the composition of transducers
derived from rewrite rules. If the rule cascade is computed starting with the lexicon, the
“overanalysis” problem illustrated in Figure 1 never arises.
The surface forms that Microlytics wanted for the Finnish spell-checker were easily
extracted from the transducer, but we realized that keeping the lexical forms and their
surface realizations in a single network would be even more valuable. I created a
transducer for English with a small number of two-level rules. It consisted of mappings
such as in Figure 4. Annie Zaenen and Carol Neidle created, with a large number
of rules, a much more ambitious proof-of-concept, a lexical transducer for French,
mapping lemmas such as vouloir+Verb+IndP+Sg+P3 to the corresponding surface form
veut. Such a transducer is the ultimate “two-level model” for a language as it compactly
encodes
</bodyText>
<listItem confidence="0.999911666666667">
• all the lemmas (lexical forms with morphological tags),
• all the inflected surface forms,
• all the mappings between lexical forms and surface forms.
</listItem>
<bodyText confidence="0.998762875">
A comprehensive analyzer such as we built for English and French consists of tens of
thousands of states and hundreds of thousands of arcs; but physically they can be quite
small, a couple of megabytes in size. The same network can be applied in two ways: to
provide an analysis for a surface form or to generate a surface from a lexical form in a
tiny fraction of a second. Karttunen, Kaplan, and Zaenen (1992) and Karttunen (1994)
are the first published reports on lexical transducers.
In 1993 Xerox established a new European research center (XRCE) near Grenoble,
France. Annie Zaenen and I went there to launch the Center’s research on natural
</bodyText>
<page confidence="0.996624">
459
</page>
<note confidence="0.491276">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.99989364516129">
language. We started with a couple of employees in an unfinished building with three
empty floors, an elevator, and a pile of Sun workstations stacked at the entrance. Not
knowing a word of French made it a hardship assignment for me, but in every other
respect it was a lucky break. Because XRCE was a start-up as a research center in need
of visibility and recognition on the level of the Xerox Corporation, I got more resources
and help for my work than I could possibly have had at PARC. A Xerox business unit
made a contract with XRCE to produce morphological analyzers and disambiguators
(= “taggers”) for six European languages. Kenneth R. Beesley, who had worked for
Microlytics, came to Grenoble to manage the development effort. I headed a small
finite-state team of researchers and programmers charged with the mission of creating
better development and run-time tools such as XFST (Xerox Finite State Tool) and LEXC
(Lexicon Compiler).
It became evident that large systems of two-level rules were difficult to debug.
We concluded that lexical transducers are easier to construct with sequentially applied
rules than with the parallel two-level rules. Andr´e Kempe and I therefore developed
a compiler for replace rules (Karttunen 1995, 1996; Kempe and Karttunen 1996).11 The
XFST regular expression language now includes a large set of different types of replace
expressions: parallel replacement, replacement with multiple contexts, replacement
with left-to-right or right-to-left shortest or longest match constraints, in addition to
the usual finite-state operations union, intersection, composition, and negation.
Ken Beesley and I managed to get permission from the XRCE management to release
to researchers most of the tools that were developed in Grenoble for creating and
applying finite-state networks, not just for morphological analysis but also for other
useful NLP tasks such as tokenization and named-entity recognition. Ken and I wrote
a book, Finite State Morphology (Beesley and Karttunen 2003), a pedagogical text that
explains and documents the tools that come with the book. There have been many
improvements in the software since then. A new edition of the book is in the making.
By the time I left Grenoble to come back to PARC in 2001, Inxight, a Xerox spinoff
company in California, was marketing finite-state morphological analyzers and stem-
mers for about three dozen languages. From a computational point of view morphology
was a solved problem.
</bodyText>
<subsectionHeader confidence="0.997045">
4.4 Computational vs. Paper-and-Pencil Morphology
</subsectionHeader>
<bodyText confidence="0.9999064">
Historically, computational linguists and their “paper-and-pencil” counterparts in lin-
guistics departments have been curiously out of sync in their approach to phonology
and morphology. When computational linguists implemented parallel two-level models
in the 1980s, paper-and-pencil linguists were stuck in the sequential Chomsky–Halle
paradigm. Many arguments had been advanced in the phonological literature in the
1970s to show that phonological alternations could not be described or explained ad-
equately without sequential rewrite rules. The idea of rules as constraints between a
lexical symbol and its surface realization was seen as misguided. It went unnoticed
that two-level rules could have the same effect as ordered rewrite rules because the
realization of a lexical symbol could be constrained by the lexical side and/or by the
</bodyText>
<footnote confidence="0.72426475">
11 Our compilation algorithm was inspired by the landmark article of Kaplan and Kay (1994). We found a
way to express the constraints on replacement using fewer auxiliary symbols than Kaplan and Kay. The
compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary
alphabet.
</footnote>
<page confidence="0.997283">
460
</page>
<note confidence="0.407722">
Karttunen Word Play
</note>
<bodyText confidence="0.993482333333333">
surface side. The standard arguments for rule ordering were based on the a priori
assumption that a rule could refer only to the input context (Karttunen 1993).
But in the mid 1990s when most computational linguists working with the Xerox
tools embraced the sequential model as the more practical approach, a two-level theory
took over paper-and-pencil linguistics by storm in the guise of Optimality Theory (OT)
(Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually
all working phonologists switched into the OT paradigm. From my perspective OT is a
two-level model where the ranking of the constraints plays the role that rule-ordering
has in the sequential model.
If one believes, as I do, that the mapping from lexical forms to inflected surface
forms is basically a regular relation, then the choice between the two ways of decompos-
ing it, either as a composed cascade of replace operations or as an intersection of parallel
rules, has important practical consequences but it is not a deep theoretical divide. In fact,
the two-level analyzer for French discussed in Karttunen, Kaplan, and Zaenen (1992)
combined parallel rules with composition. It is unclear to me why my paper-and-pencil
colleagues seem to think that it has to be absolutely one or the other.
I have written several papers in the hope of getting my paper-and-pencil colleagues
interested in, or at least aware of, what is happening in computational morphology
(Karttunen 1993, 1998, 2003, 2006). I have not succeeded. Paper-and-pencil morphol-
ogists in general are not interested in creating complete descriptions for particular
languages. They design formalisms for expressing generalizations about morphological
phenomena commonly found in all natural languages. Practical issues that arise in the
context of real-life applications such as completeness of coverage, physical size, and
speed of applications are irrelevant from an academic morphologist’s point of view.
The main purpose of a morphologist writing for an audience of fellow linguists is to be
convincing that his theory of word formation provides a more insightful and elegant
account of this aspect of the human linguistic endowment than the competing theories
and formalisms.
My frustration is best summed up in a fable that I attached to my paper on a
finite-state implementation of Gregory Stump’s realizational morphology (Stump 2001;
Karttunen 2003).
Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computa-
tional knights have presented themselves at the Royal Court of Linguistics, rushed up
to the Princess of Phonology and Morphology in great excitement to deliver the same
message:
Dear Princess. I have wonderful news for you: You are not like some of your
NP-complete sisters. You are regular. You are rational. You are finite-state.
Please marry me. Together we can do great things.
And time after time, the put-down response from the Princess has been the same:
Not interested. You do not understand Theory. Go away, you geek.
Because the most suitable suitor has always been rejected, I suspect that the Princess has
a vested interest in making simple things appear more complicated than they really are.
The good news that the computational knights are trying to deliver is unwelcome. The
Princess prefers the pretense that phonology/morphology is a profoundly complicated
subject, believing herself to be shrouded by veils of theories.
</bodyText>
<page confidence="0.998041">
461
</page>
<note confidence="0.492133">
Computational Linguistics Volume 33, Number 4
</note>
<bodyText confidence="0.9959494">
If that is the correct analysis of the situation, computational linguists should adopt
a different strategy. Instead of being the eternal rejected suitor at the Royal Court, they
should adopt the role of the innocent boy in the street shouting
The Princess has no clothes! The Princess has no clothes!...
That was my conclusion in the 2003 paper.
</bodyText>
<sectionHeader confidence="0.980783" genericHeader="conclusions">
5. Epilogue
</sectionHeader>
<bodyText confidence="0.988738785714286">
I am very happy to see that the topics I worked on at the very beginning of my career
have finally become relevant in NLP. To quote again the opening paragraph of my 1970
ACL presentation (Karttunen 1971b):
It is evident that logical relations between main sentences and their complements are of
great significance in any system of automatic data processing that depends on natural
language. For this reason a systematic study of such relations, of which this paper
is an example, will certainly have a great practical value, in addition to what it may
contribute to the theory of the semantics of natural languages.
This 37-year-old prediction of semantics having practical value is becoming a reality
in the context of automated question answering and reasoning initiatives such as the
PASCAL Textual Entailment Challenge (Dagan, Glickman, and Magnini 2005) and the
ARDA-sponsored AQUAINT project (Karttunen and Zaenen 2005; Zaenen, Karttunen,
and Crouch 2005). The first computational implementation of textual inferences arising
from the six types of implicative constructions in Tables 1 and 2, and their interaction
with factive verbs, is presented in Nairn, Condoravdi, and Karttunen (2006). We may
soon see search engines that actually make use of semantic processing in addition to
simple string matching. The ability to draw textual inferences will significantly improve
the quality of question answering and Web searches.
From a linguistic perspective, this is an auspicious time to take a fresh look at issues
such as the classification of complement constructions. The availability of search engines
such as Google makes it possible to check the linguist’s semantic intuitions against
actual usage. One question I have always had about the classification of implicative con-
structions is whether the commitment to the truth or falsity of the complement clause is
always based on a semantic entailment or whether some of these cases should be looked
upon as a usage convention. For example, if you google the pattern didn’t hesitate to, it
is immediately evident that hesitate to really is one of the rare −+implicatives.
(20) a. Head Coach Jon Gruden didn’t hesitate to share interesting Buccaneer
information with the two Chamber of Commerce crowds on Friday.
</bodyText>
<listItem confidence="0.903924">
b. John didn’t hesitate to refer the file to CIB and from there it went to the
Victoria Police.
c. George Patton with all his swagger and confidence didn’t hesitate to throw
himself and his men into the teeth of the German offensive and won the day.
</listItem>
<bodyText confidence="0.99908525">
When you see examples such as in (20) in their full context, it is obvious that the author
presents the complement clause of hesitate to as a fact. But it is difficult to explain why
things should be this way starting from the concept of hesitation or the semantics of the
verb hesitate.
</bodyText>
<page confidence="0.998072">
462
</page>
<figure confidence="0.467598125">
Karttunen Word Play
As a sample of things I am planning to do next, I will leave you two little puzzles to
solve. The construction didn’t wait to is ambiguous. Here are a couple of examples from
Google to to illustrate the ambiguity.
(21) a. Deena did not wait to talk to anyone. Instead, she ran home.
b. It hurt like hell, but I’m glad she didn’t wait to tell me.
(21a) implies Deena did not talk to anyone. But (21b) implies She told me something right
away.
</figure>
<figureCaption confidence="0.950902333333334">
Question 1: How does it come about that X didn’t wait to do Y means either that X
did Y right away or that X didn’t do Y at all?
When you look at examples with didn’t wait to in their full context, it is nearly always
possible to tell which of the two meanings the author has in mind. In (21a), for instance,
the negative polarity item anyone and the word instead are telltale indicators. In (21b),
the cataphoric pronoun it indicates that a telling event took place. I am sure that it is
possible to learn to pick the intended meaning by statistical techniques. But statistics
alone will not give you an answer to Question 1, nor will it solve the related problem in
Question 2.
Question 2: Why is it not possible to translate expressions such as Neil didn’t wait
to take off his coat to other languages in a way that preserves the ambiguity the
sentence has in English?
</figureCaption>
<bodyText confidence="0.84931925">
In languages such as Dutch, Finnish, French, German, Hungarian, and Japanese, among
others, it is of course possible to express the two meanings of X did not wait to Y, but not
in one and the same sentence. My answer to these two questions will have to wait until
my next semantics paper.
</bodyText>
<sectionHeader confidence="0.990178" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.6787058">
Many thanks to Kenneth R. Beesley, Daniel
G. Bobrow, Robert Dale, Aravind K. Joshi,
John T. Maxwell, III, Bonnie Webber, and
Annie Zaenen for their help on the style and
content of this article.
</bodyText>
<sectionHeader confidence="0.956407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994933820512821">
Antworth, Evan L. 1990. PC-KIMMO:
A Two-Level Processor for Morphological
Analysis. Number 16 in Occasional
publications in academic computing.
Summer Institute of Linguistics, Dallas.
Armstrong, Susan. 1996. Multext:
Multilingual text tools and corpora. In
H. Feldweg and E. W. Hinrichs, editors,
Lexikon und Text. Max Niemeyer Verlag,
Tuebingen, Germany, pages 107–112.
Beaver, David I. 1995. Presupposition and
Assertion in Dynamic Semantics. Ph.D.
thesis, Center for Cognitive Science,
University of Edinburgh, Edinburgh,
Scotland.
Beesley, Kenneth R. and Lauri Karttunen.
2003. Finite State Morphology. CSLI
Publications, Stanford, CA.
Bennett, Winfield S. and Jonathan Slocum.
1985. The LRC machine translation system.
Computational Linguistics, 11(2–3):111–121.
Black, A., G. Ritchie, S. Pulman, and
G. Russell. 1987. Formalisms for
morphographemic description. In
Proceedings of the Third Conference of the
European Chapter of the Association for
Computational Linguistics, pages 11–18,
Copenhagen, Denmark.
Carter, D. 1995. Rapid development of
morphological descriptions for full
language processing systems. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 202–209,
Dublin, Ireland.
Chomsky, Noam. 1957. Syntactic Structures.
Mouton, Gravenhage, The Netherlands.
Chomsky, Noam. 1973. Conditions on
transformations. In Steven Anderson and
</reference>
<page confidence="0.99927">
463
</page>
<note confidence="0.62563">
Computational Linguistics Volume 33, Number 4
</note>
<reference confidence="0.996635254237288">
Paul Kiparsky, editors, A Festschrift for
Morris Halle. Holt, Reinhard, and Winston,
New York, NY, pages 232–286.
Chomsky, Noam and Morris Halle. 1968. The
Sound Pattern of English. Harper and Row,
New York, NY.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2005. The PASCAL recognising
textual entailment challenge. In Proceedings
of the PASCAL Challenges Workshop
on Recognising Textual Entailment,
pages 1–8, Southampton, UK.
Eisner, Jason. 2002. Phonological
comprehension and the compilation of
optimality theory. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 56–63,
Washington, DC.
Ellison, T. Mark. 1994. Phonological
derivation in Optimality Theory. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 1007–1013, Kyoto, Japan.
Fillmore, Charles. 1971. Verbs of judging:
An exercise in semantic description.
In Charles J. Fillmore and Terence
Langendoen, editors, Studies in
Linguistic Semantics. Holt, Rinehart
and Winston, Inc., New York, NY,
pages 273–289.
Frege, Gottlob. 1892. ¨Uber Sinn und
Bedeutung. Zeitschrift f¨ur Philosophie und
Philosophische Kritik, pages 25–50. English
translation: “On Sense and Meaning,”
in Brian McGuiness, editor, Frege:
Collected Works. Basil Blackwell, Oxford,
pages 157–177.
Gajek, Oliver, Hanno T. Beck, Diane Elder,
and Greg Whittemore. 1983. Lisp
implementation. In Mary Dalrymple, Edit
Doron, John Goggin, Beverly Goodman,
and John McCarthy, editors, Texas
Linguistic Forum, Vol. 22. Department of
Linguistics, The University of Texas at
Austin, Austin, TX, pages 187–202.
Gazdar, Gerald. 1979. Conventional
implicature. In Choon-Kyu Oh and David
A. Dinneen, editors, Syntax and Semantics,
Volume 11: Presupposition. Academic
Press, New York, NY, pages 57–89.
Geis, Michael and Arnold Zwicky. 1971.
On invited inferences. Linguistic Inquiry,
2:561–565.
Geurts, Bart. 1999. Presuppositions and
Pronouns. Elsevier, Cambridge, MA.
Ginzburg, Jonathan. 1992. Questions, Queries
and Facts: A Semantics and Pragmatics for
Interrogatives. Ph.D. thesis, Stanford
University, Stanford, CA.
Ginzburg, Jonathan. 1996. Interrogatives:
Questions, facts, and dialogue. In Shalom
Lappin, editor, Handbook of Contemporary
Semantic Theory. Blackwell Publishers,
Oxford, UK, pages 385–422.
Grice, H. Paul. 1979. Logic and conversation.
In P. Cole and J. L. Morgan, editors,
Speech Acts. Academic Press, New York,
NY, pages 41–58.
Groenendijk, Jeroen and Martin Stokhof.
1984. On the semantics of questions and
the pragmantics of answers. In Fred
Landman and Frank Veltman, editors,
Varieties of Formal Semantics. Foris
Publications, Dordrecht, The Netherlands,
pages 143–170.
Hamblin, Charles L. 1973. Questions in
Montague English. Foundations of
Language, 10:41–53.
Harris, Randy Allen. 1995. The Linguistics
Wars. Oxford University Press,
Oxford, UK.
Hausser, Roland. 1983. The Syntax and
Semantics of English Mood. In Ferenc
Kiefer, editor, Questions and Answers.
Reidel, Dordrecht, The Netherlands,
pages 97–158.
Hausser, Roland and Dietmar Zaefferer.
1979. Questions and answers in a context
dependent Montague grammar. In
Siegfried Josef Schmidt and Franz
Guenthner, editors, Formal Semantics
and Pragmatics for Natural Languages.
Reidel, Dordrecht, The Netherlands,
pages 339–358.
Hays, David G. 1967. Introduction to
Computational Linguistics. Elsevier, New
York, NY.
Heim, Irene. 1982. The Semantics of Definite
and Indefinite Noun Phrases. Ph.D. thesis,
University of Massachusetts, Amherst, MA.
Heim, Irene. 1983. On the projection problem
for presuppositions. In West-Coast
Conference on Formal Linguistics, volume 2,
pages 114–126, Stanford, CA.
Hein¨am¨aki, Orvokki. 1974. Semantics of
English Temporal Connectives. Ph.D. thesis,
University of Texas, Austin, TX.
Johnson, C. Douglas. 1972. Formal Aspects
of Phonological Description. Mouton,
The Hague, The Netherlands.
Joshi, Aravind K. and Ralph M. Weischedel.
1973. Some frills for the modal tic-tac-toe
of Davies and Isard: Semantics of predicate
complement constructions. In IJCAI,
pages 352–355, Stanford, CA.
Kager, Ren´e. 1999. Optimality Theory.
Cambridge University Press,
Cambridge, UK.
</reference>
<page confidence="0.99703">
464
</page>
<note confidence="0.393798">
Karttunen Word Play
</note>
<reference confidence="0.991264966101695">
Kamp, Hans. 1981. Ev´enements,
repr´esentation discursive et r´ef´erence
temporelle. Langages, 64:39–64.
Kamp, Hans. 2001. The importance of
presupposition. In Christian Rohrer, Antje
Roßdeutscher, and Hans Kamp, editors,
Linguistic Form and its Computation. CSLI,
Stanford, CA, pages 207–254.
Kamp, Hans and Uwe Reyle.1993. From
Discourse to Logic. Kluwer, Dordrecht,
The Netherlands.
Kaplan, Ronald M. and Martin Kay. 1994.
Regular models of phonological rule
systems. Computational Linguistics,
20(3):331–378.
Kaplan, S. Jerrold. 1977. Cooperative Responses
from a Natural Language Query System.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Karttunen, Lauri. 1969a. Problems of Reference
in Syntax. Ph.D. thesis, Indiana University,
Bloomington, Indiana.
Karttunen, Lauri. 1969b. Pronouns and
variables. In CLS 5: Proceedings of the
Fifth Regional Meeting, pages 108–116,
Chicago, IL.
Karttunen, Lauri. 1971a. Implicative verbs.
Language, 47:340–358.
Karttunen, Lauri. 1971b. The logic of English
predicate complement constructions. The
Indiana University Linguistics Club.
Bloomington, Indiana.
Karttunen, Lauri. 1973. Presuppositions of
compound sentences. Linguistic Inquiry,
4:167–193.
Karttunen, Lauri. 1974. Presupposition and
linguistic context. Theoretical Linguistics,
1(1):181–194.
Karttunen, Lauri. 1976. Discourse referents.
In James D. McCawley, editor, Syntax and
Semantics Volume 7, Notes from the Linguistic
Underground. Academic Press, New York,
NY, pages 363–385.
Karttunen, Lauri. 1977. Syntax and semantics
of questions. Linguistics and Philosophy,
1:1–44.
Karttunen, Lauri. 1983. KIMMO: A general
morphological processor. In Mary
Dalrymple, Edit Doron, John Goggin,
Beverley Goodman, and John McCarthy,
editors, Texas Linguistic Forum, volume 22.
Department of Linguistics, The University
of Texas at Austin, Austin, TX,
pages 165–186.
Karttunen, Lauri. 1984. Features and values.
In COLING’84, pages 28–33, July 2–6,
Stanford, CA.
Karttunen, Lauri.1986. D-PATR:
A development environment for
unification-based grammars. In
COLING’86, pages 74–80, Bonn, Germany.
Karttunen, Lauri. 1993. Finite-state
constraints. In John Goldsmith, editor,
The Last Phonological Rule. University of
Chicago Press, Chicago, IL.
Karttunen, Lauri. 1994. Constructing lexical
transducers. In COLING’94, pages 406–411,
Kyoto, Japan.
Karttunen, Lauri. 1995. The replace
operator. In ACL’95, cmp-lg/9504032.
Cambridge, MA.
Karttunen, Lauri. 1996. Directed
replacement. In ACL’96, cmp-lg/9606029.
Santa Cruz, CA.
Karttunen, Lauri. 1998. The proper treatment
of optimality in computational phonology.
In FSMNLP’98. International Workshop on
Finite-State Methods in Natural Language
Processing, cmp-lg/9804002. Bilkent
University, Ankara, Turkey.
Karttunen, Lauri. 2003. Computing with
realizational morphology. In Alexander
Gelbukh, editor, Computational Linguistics
and Intelligent Text Processing, volume 2588
of Lecture Notes in Computer Science.
Springer Verlag, Heidelberg, Germany,
pages 205–216.
Karttunen, Lauri. 2006. The insufficiency of
paper-and-pencil linguistics: The case of
Finnish prosody. In Miriam Butt, Mary
Dalrymple, and Tracy Holloway King,
editors, Intelligent Linguistic Architectures,
pages 287–300. CSLI Publications,
Stanford, CA.
Karttunen, Lauri and Kenneth R. Beesley.
1992. Two-level rule compiler. Technical
Report ISTL-92-2, Xerox Palo Alto
Research Center, Palo Alto, CA.
Karttunen, Lauri, Ronald M. Kaplan,
and Annie Zaenen. 1992. Two-level
morphology with composition.
In COLING’92, pages 141–148,
Nantes, France.
Karttunen, Lauri and Martin Kay. 1985a.
Parsing in a free word order language. In
David R. Dowty, Lauri Karttunen, and
Arnold Zwicky, editors, Natural Language
Parsing. Cambridge University Press,
Cambridge, UK, pages 279–306.
Karttunen, Lauri and Martin Kay. 1985b.
Structure sharing with binary trees. In
Proceedings of the 23rd Meeting of the
Association for Computational Linguistics,
pages 133–136, Chicago, IL.
Karttunen, Lauri, Kimmo Koskenniemi, and
Ronald M. Kaplan. 1987. A compiler for
two-level phonological rules. In Mary
Dalrymple, Ronald Kaplan, Lauri
</reference>
<page confidence="0.997953">
465
</page>
<note confidence="0.537805">
Computational Linguistics Volume 33, Number 4
</note>
<reference confidence="0.998146567796611">
Karttunen, Kimmo Koskenniemi, Sami
Shaio, and Michael Wescoat, editors, Tools
for Morphological Analysis. Center for the
Study of Language and Information,
Stanford University, Palo Alto, CA,
pages 1–61.
Karttunen, Lauri and Stanley Peters. 1979.
Conventional implicature. In Choon-Kyu
Oh and David A. Dinneen, editors, Syntax
and Semantics, Volume 11: Presupposition.
Academic Press, New York, NY,
pages 1–56.
Karttunen, Lauri, Hans Uszkoreit, and
Rebecca Root. 1981. Morphological
analysis of Finnish by computer. In
Proceedings of the 71st Annual Meeting of
SASS, Albuquerque, NM.
Karttunen, Lauri and Annie Zaenen. 2005.
Veridicity. In Graham Katz, James
Pustejovsky, and Frank Schilder, editors,
Annotating, Extracting and Reasoning about
Time and Events, number 05151 in Dagstuhl
Seminar Proceedings. Internationales
Begegnungs-und Forschungszentrum
(IBFI), Schloss Dagstuhl, Germany,
http://drops.dagstuhl.de/opus/
volltexte/2005/314.
Keenan, Edward L. 1971. Two kinds of
presupposition in natural language. In
Charles J. Fillmore and Terence
Langendoen, editors, Studies in Linguistic
Semantics. Holt, Rinehart and Winston,
Inc., New York, NY, pages 45–54.
Kempe, Andr´e and Lauri Karttunen. 1996.
Parallel replacement in finite-state
calculus. In COLING’96, cmp-lg/9607007.
Copenhagen, Denmark.
Kiparsky, Paul and Carol Kiparsky.1971.
Fact. In D. Steinberg and L. Jakobovits,
editors, Semantics. An Inderdisciplinary
Reader, pages 345–369. Cambridge
University Press, Cambridge, UK.
Koskenniemi, Kimmo. 1983. Two-level
morphology: A general computational
model for word-form recognition and
production. Publication 11, University
of Helsinki, Department of General
Linguistics, Helsinki.
Koskenniemi, Kimmo. 1986. Compilation of
automata from morphological two-level
rules. In Fred Karlsson, editor, Papers
from the Fifth Scandinavian Conference on
Computational Linguistics, pages 143–149,
Helsinki, Finland.
Langendoen, Terence and Harris B. Savin.
1971. The projection problem for
presuppositions. In Charles J. Fillmore
and Terence Langendoen, editors,
Studies in Linguistic Semantics. Holt,
Rinehart and Winston, Inc., New York,
pages 55–62.
Lewis, David. 1979. Scorekeeping in a
language game. Journal of Philosophical
Logic, 8:339–359.
McCarthy, John J. 2002. The Foundations of
Optimality Theory. Cambridge University
Press, Cambridge, UK.
McCawley, James D. 1970. Where do noun
phrases come from? In Roderick A. Jacobs
and Peter S. Rosenbaum, editors, Readings
in English Transformational Grammar.
Ginn and Company, Waltham, MA,
pages 166–183.
Montague, Richard. 1970a. English as a
formal language. In B. Visentini et al.,
editors, Linguaggi nella Societ`a e nella
Tecnica. Edizioni di Comunit`a, Milan,
Italy, pages 189–224.
Montague, Richard. 1970b. Universal
grammar. Theoria, 36:373–398.
Montague, Richard. 1973. The proper
treatment of quantification in ordinary
English. In P. Suppes K. J. J. Hintikka,
J. M. E. Moravcsik, editors, Approaches to
Natural Language. Reidel, Dordrecht,
The Netherlands, pages 221–242.
Moshagen, Sjur, Pekka Sammallahti, and
Trond Trosterud. 2006. Twol at work.
In Antti Arppe, Lauri Carlson, Krister
Lind´en, Jussi Piitulainen, Mickael
Suominen, Martti Vainio, Hanna
Westerlund, and Anssi Yli-Jyr¨a,
editors, Inquiries into Words, Constraints
and Contexts. CSLI, Stanford, CA,
pages 94–105.
Nairn, Rowan, Cleo Condoravdi, and Lauri
Karttunen. 2006. Computing relative
polarity for textual inference. In Inference
in Computational Semantics (ICoS-5),
Buxton, UK.
Oflazer, Kemal. 1994. Two-level description
of Turkish morphology. Literary and
Linguistic Computing, 9(2):137–148.
Partee, Barbara. 1995. Montague grammar
and transformational grammar. Linguistic
Inquiry, 6:203–300.
Potts, Christopher. 2004. The Logic of
Conventional Implicatures. Oxford
University Press, Oxford, UK.
Prince, Allan and Paul Smolensky.
1993. Optimality Theory: Constraint
interaction in generative grammar.
RuCCS Technical Report 2. Rutgers
Center for Cognitive Science. Rutgers
University, Piscataway, NJ.
Prince, Ellen. 1978. A comparison of
Wh-clefts and it-clefts in discourse.
Language, 54:883–906.
</reference>
<page confidence="0.984687">
466
</page>
<reference confidence="0.994920783505155">
Karttunen Word Play
Pulman, Stephen. 1991. Two level
morphology. In H. Alshawi, D. Arnold,
R. Backofen, D. Carter, J. Lindop, K. Netter,
S. Pulman, J. Tsujii, and H. Uskoreit,
editors, ET6/1 Rule Formalism and Virtual
Machine Design Study. CEC, Luxembourg,
chapter 5.
Ritchie, G., A. Black, S. Pulman, and
G. Russell. 1987. The Edinburgh/
Cambridge morphological analyser
and dictionary system (version 3.0)
user manual. Technical Report Software
Paper No. 10, Department of Artificial
Intelligence, University of Edinburgh,
Edinburgh, UK.
Ritchie, G., G. Russell, A. Black, and
S. Pulman. 1992. Computational Morphology:
Practical Mechanisms for the English Lexicon.
The MIT Press, Cambridge, MA.
Russell, Bertrand. 1905. On denoting. Mind,
14:479–493.
Russell, Bertrand. 1957. Mr. Strawson on
referring. Mind, 66:385–389.
Schiller, Anne. 1996. Deutsche Flexions-
und Kompositionsmorphologie mit
PC-KIMMO. In Roland Hausser,
editor, Linguistische Verifikation:
Dokumentation zur Ersten Morpholympics
1994, number 34 in Sprache und
Information, pages 37–52. Max Niemeyer
Verlag, T¨ubingen.
Sch¨utzenberger, Marcel-Paul. 1961. A remark
on finite transducers. Information and
Control, 4:185–196.
Shieber, Stuart, Hans Uszkoreit, Fernando
Pereira, Jane Robinson, and Mabry Tyson.
1983. The formalism and implementation
of PATR-II. In Barbara J. Grosz and Mark
Stickel, editors, Research on Interactive
Acquisition and Use of Knowledge. SRI
International, Menlo Park, CA,
techreport 4, pages 39–79.
Soames, S. 1982. How presuppositions are
inherited: A solution to the projection
problem. Linguistic Inquiry, 13:483–545.
Stalnaker, Robert. 1973. Presuppositions.
The Journal of Philosophical Logic,
2:447–457.
Strawson, Peter. 1950. On referring. Mind,
59:320–344.
Strawson, Peter. 1964. Identifying reference
and truth values. Theoria, 30:320–344.
Stump, Gregory T. 2001. Inflectional
Morphology. A Theory of Paradigm Structure.
Cambridge University Press, Cambridge,
UK.
Uibo, Heli. 2006. Eesti keele morfoloogia
modelleerimisest l˜oplike muundurite abil.
[on modelling the estonian morphology by
the means of finite-state transducers]. In
M. Koit, R. Pajusalu, and H. ˜Oim, editors,
Keel ja arvuti, number 6 in Tartu ¨Ulikooli
¨uldkeeleteaduse ˜oppetooli toimetised. Max
Niemeyer Verlag, T¨ubingen, Germany.
van der Sandt, Rob. 1992. Presupposition
projection as anaphora resolution. Journal
of Semantics, 9:333–377.
van der Sandt, Rob A. and Bart Geurts. 1991.
Presupposition, anaphora, and lexical
content. In Text Understanding in LiLOG,
Integrating Computational Linguistics and
Artificial Intelligence, Final Report on the IBM
Germany LILOG-Project. Springer Verlag,
London, UK, pages 259–296.
Vendler, Zeno. 1967. Linguistics and
Philosophy. Cornell University Press,
Ithaca, NY.
Wall, Robert E. 1972. Introduction to
Mathematical Linguistics. Prentice Hall,
Englewood Cliffs, NJ.
Webber, Bonnie L. 1978. A Formal Approach to
Discourse Anaphora. Ph.D. thesis, Harvard
University, Cambridge, MA.
Weischedel, Ralph. 1975. Computation of a
Unique Class of Inferences: Presupposition
and Entailments. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA.
Zaenen, Annie, Lauri Karttunen, and
Richard Crouch. 2005. Local textual
inference: Can it be defined or
circumscribed? In Workshop on the Empirical
Modeling of Semantic Equivalence and
Entailment, pages 31–36, Ann Arbor, MI.
Zeevat, Henk. 1992. Presupposition and
accommodation in update semantics.
Journal of Semantics, 9:379–412.
</reference>
<page confidence="0.999299">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.982026">ACL Lifetime Achievement Award</title>
<author confidence="0.55989">Word Play</author>
<affiliation confidence="0.8416665">Palo Alto Research Center Stanford University</affiliation>
<abstract confidence="0.980753740740741">This article is a perspective on some important developments in semantics and in computational linguistics over the past forty years. It reviews two lines of research that lie at opposite ends of the field: semantics and morphology. The semantic part deals with issues from the 1970s such and The second presents a brief history of the application of transducers linguistic analysis with the advent of morphology the early 1980s and culminating in successful commercial applications in the 1990s. It offers some commentary on the relationship, or the lack thereof, between computational and paper-and-pencil linguistics. The final section returns to the semantic issues and their application to currently popular tasks such as textual inference and question answering. 1. Prologue years ago, in the summer of 1969 at the second meeting of S˚anga-S¨aby in Sweden, I stood for the first time in front of a computational audience started my talk on Referents reading the following passage (Karttunen 1976): Consider a device designed to read a text in some natural language, interpret it, and store the content in some manner, say, for the purpose of being able to answer questions about it. To accomplish this task, the machine will have to fulfill at least the following basic requirement. It has to be able to build a file that consists of records of all individuals, that is, events, objects, etc., mentioned in the text and, for each individual, record whatever is said about it. Of course, for the time being at least, it seems that such a text interpreter is not a practical idea, but this should not discourage us from studying in the abstract what kind of capabilities the machine would have to possess, provided that our study provides us with some insight into natural language in general. The paper went on to discuss the circumstances that allow a pronoun or a definite description to refer to an object introduced by an indefinite noun phrase. For example, (1a), the pronoun refer to Bill’s car, but in (1b) it cannot.</abstract>
<note confidence="0.9843662">Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94062, USA. E-mail: karttunen@parc.com. This article is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 a. Bill has a black. Bill doesn’t have a black. year later in 1970, I gave my first at the 8th annual meeting in Ohio. The title of the invited talk was Logic of English Predicate Comple- It started off with the following declaration (Karttunen 1971b):</note>
<abstract confidence="0.982051966442954">It is evident that logical relations between main sentences and their complements are of great significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. The paper presented a classification of verbs and constructions that take sentential and infinitival complements, based on whether the sentence commits the author to the truth or falsity of the complement clause. For example, all the sentences in (2) imply, for different reasons, that the complement is true while all the in (3) imply that the complement is (2) a. John forgot that Mary was sick. Mary was sick. b. Bill managed to solve the problem. Bill solved the problem. c. Harry forced Ed to leave. Ed left. (3) a. John pretended that Mary was sick. Mary was not sick. b. Bill failed to solve the problem. Bill did not solve the problem. c. Harry prevented Ed from leaving. Ed did not leave. one of these two papers would have been accepted at this 2007 There was no implementation, no evaluation, and very little discussion of related work. In the happy childhood of computational linguistics even the most junior person in the like myself, was allowed—even invited—to give a talk at the main session about uncharted linguistic phenomena. It was a small field then. A future historian of the field might be puzzled by the 1969 and 1970 papers. They were written by a postdoctoral research associate at the University of Texas at Austin, who had arrived from Finland in 1964 by way of the University of Indiana at Bloomington where he had just received a Ph.D. in Linguistics. Where did the young man acquire, and why was he spouting, that kind of computational rhetoric, when the record shows that for the next ten years he never laid his hands on a computer? The fact is that I did have a brush with computational linguistics before settling to do pure semantics in the 1970s. I wanted to do linguistics because of 1957) and when the Uralic and Altaic Studies Department in Bloomington offered me a job in 1964 as a “native informant” in Finnish I accepted and managed to get into the Linguistics department as a graduate student. My job title turned out not to be accurate. During my first two years in Bloomington I was teaching Finnish on my own for nine hours per week. Luckily, I signed up for a course on computational linguistics taught by an excellent teacher and mentor, Robert E. Wall. Bob had participated in an early at Harvard and in a project on automatic The subscripts iand jare referential indices. Two noun phrases with the same referential index are supposed to be coreferential, that is, they should refer to the same object. I am using the word a generic cover term for and implicate. about this in Sections 2.2 and 2.3. 444 Karttunen Word Play at In his course, we learned formal language theory from notes eventually became a book (Wall 1972), a bit of Fortran and a language by Victor Yngve at I wrote a program on punched cards to randomly generate sentences from a small grammar of Finnish. Thanks to Bob, I was rescued from my indentured servitude in the Uralic and Altaic Studies. In my third and final year in Bloomington, I worked as a research assistant in the Computer Center with no specific duties other than to be a liaison to the Linguistics Department. My only accomplishment in that role was to save piles of anthropological data from obsolescence by writing a program to transform rolls of 5-channel paper tape to 6-channel magnetic tapes. By that, I became one of the few linguists who could explain the joke, are 10 of linguists: those who know binary and those who don’t. suspect that the data on my tapes for the Control Data 3600 computer have now been lost. We still have the data on manuscripts hundreds of years old but much of the content created in the first decades of the computer age is gone forever. Just as I was starting to work on my dissertation in 1967, I had the good fortune of a one-year fellowship at the in Santa Monica, California, in the group headed by David G. Hays, the author of the first textbook in our field (Hays 1967), and the founder of the Association for Machine Translation and Computational the predecessor of our The main focus of Hays’s team was Russian-to-English machine translation. Remarkably, Hays was also one of the authors the infamous 1966 that inexorably caused the shutdown of all govfunded including the one at Because the term acquired a bad odor, the 1968 meeting of its and became At the 1970 meeting, the first one that I attended, people were still bitterly arguing about the matter. Hays’s group at again met Martin Kay, who had been my teacher in a on parsing at the 1966 Linguistic Institute at My term paper for Martin’s course had been on the computational analysis of Finnish morphology, a topic to which I would eventually return some fifteen years later. Happy to become Martin’s student again, I learned Algol, an elegant new programming language, and got an understanding of the beauty of recursive algorithms. Martin was running an exciting weekly colloquium series. I teamed up with an intern by the name of Ronald Kaplan for a small study project and we gave a joint presentation about our findings. Ron and I agree that we did this together but neither one remembers what we said. It probably was about the similarities and differences between pronouns and logical variables, the topic of my first published paper (Karttunen 1969b). At the time the prevailing assumption was that symbolic logic provides an appropriate system for semantic representation within transformational grammar (McCawley But as I showed in the 1969 even cases as simple as (4a) and (4b) could not be treated adequately within the proposed framework. a. The loved I gave each student a Some of them ate away. The shot at the chased problem with (4a) is that the phrase to be treated in situ as it cannot replaced by another coreferential noun phrase, say without changing the meaning. (4a) implies that only one man in some group of men loved his wife, which is not the same as there being just one man who loved Mary even if the two noun phrases pick out the same individual. For this reason there was no way in a system such as to link In the case of (4b), the problem is that there is no 445 Computational Linguistics Volume 33, Number 4 cookie to serve as the referent of Being an anaphoric pronoun linked to an antecedent does not necessarily mean that the two are coreferential, at least not in any naive sense of coreference. Even worse problem cases were known. (4c) is a typical “Bach-and-Peters sentence,” named after its inventors, Emmon Bach and Stanley Peters. As I was going to devote a chapter of my dissertation to this topic, I was lucky to run into them at a conference in San Diego. I found the two very intimidating in their suits and crew cuts. They looked like Haldeman and Ehrlichman, a pair of Nixon aides. But at least I found out that the problem had not been solved. the year at on, I spent less and less time at the computer and a lot of time walking up and down the Santa Monica pier just down the cliff from my office about pronouns, variables, reference, and definiteness. I went up to few times to discuss these issues with Barbara Partee and gave a talk in her seminar. topic of my dissertation was of Reference in in principle due before left finished half-a-year later (Karttunen 1969a). the summer of 1968 I had two job offers. David Hays was leaving in Buffalo and he offered me a job there. But I chose to become a Faculty Associate in the Linguistics Department at the University of Texas at Austin. Climate was one consideration, but, more importantly, Austin was where Emmon Bach and Stanley Peters were. My Indiana mentor, Bob Wall, had just moved into the same department. For the next ten years I had very little contact with Martin Kay and Ronald Kaplan but they became very important people in Act II of my life. 2. Act I: Framing Problems I started my career in Austin in the fall of 1968 and became a regular faculty member in 1970. My work on discourse referents was largely done when I arrived in Austin. went on to study so-called verbs as a subtopic in the discourse referents paper, and branched to other types of verbs that take sentential complements. One important semantic class of verbs with sentential complements, had already been identified and discussed by Zeno Vendler (1967) and and Carol Kiparsky (Kiparsky and Kiparsky 1971) at Factive verbs were said the complement clause is true. it happened, I started to think about factives and presuppositions at the Fall of 1972. In the spring before I had a surprise phone call from Paul Kiparsky who that the was still looking for a one-year replacement for David Perlmutter who was going on a sabbatical. Would I be interested? Of course I was. I had come to the U.S. seven years earlier to study linguistics because of Noam Chomsky and now I had an office just across from hall from his. But during the year I was there, I lost interest in transformational syntax. I found Chomsky’s Thursday lectures that year, on themes later published as on Transformations 1973), uncompelling. My sense of what was interesting had changed. The “Linguistic Wars” (Harris 1995) between generative (George Lakoff, John Ross, James D. McCawley, Paul Postal, and others) and interpretive semantics (Ray Jackendoff and others) had been won by Chomsky for the interpretivists, although Chomsky himself was, and still is, skeptical of any kind of formal theory of meaning. My sympathies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at model theory and intensional logic was the right approach to semantics. But 446 Karttunen Word Play it was going to take a while before I could do anything original within that emerging At gave a “formal methods” course for a few linguistic students starting with Bob Wall’s textbook (Wall 1972) and finishing with Montague Grammar that I was just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on my own topics: discourse referents, implicative verbs, and presuppositions. I had one star student in the seminar by the name of Mark Liberman, who wrote a Master’s Thesis poking holes in my emerging ideas about presuppositions. In the 1970s, the Linguistics Department in Austin was an excellent place for a young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excellent semantics and syntax students. David Dowty, Per-Kristian Halvorsen, Roland Hausser, Orvokki Jim McCloskey, and Hans Uszkoreit got their degrees from while I was there. Orvokki was my first Ph.D. student. She wrote an insightful thesis on the of other temporal connectives (Hein¨am¨aki In the 1970s I worked on four general topics: discourse referents, implicative verbs, presuppositions, and questions. This is not an occasion to deep-end into any of these topics but I will discuss each of them briefly in the following sections to give a general idea of what I think my contributions were. 2.1 Discourse Referents The obvious difference between definite and indefinite noun phrases is that gardendefinite such as car simple main clauses imply the existence of an or an object but indefinite noun phrases such as car do not. In that definite are similar to definite pronouns such as contexts where the pronoun does not play the role of a bound variable. The reason for the incoherence of (1b) is that it tells us explicitly that there is no such car. Given an indefinite noun phrase, when is there supposed to be a corresponding individual that it describes? This was the question I tried to answer in the 1969 paper on discourse referents, excerpted from the first chapter of my Indiana dissertation. The conclusion I came to was that a pronoun or a definite description could refer back to be an anaphor for) an indefinite antecedent) just in case the existence of the individual was semantically implied by the text. Put in this simple way, the answer seems obvious but it gave rise to many problems some of which remain unsolved to day. The novelty of the approach was that it rephrased the problem of pronomsemantic terms. Up to that point in transformational grammar, all the discussion about anaphors and antecedents had been about the constraints on their syntactic configurations. affirmative sentences such as has a car (1a) obviously imply existence simple negative sentences such as doesn’t have a car (1b) imply the opposite. The type of the verb matters, as seen in (5). a. The director is at innocent from Bean Blossom. The director is for innocent from Bean Blossom. verbs as and an ambiguity. (5b), the phrase innocent blonde be understood in two ways. In the specific it describes a particular individual that we can refer to as But (5b) can also be 3 As I was preparing this talk, I heard the sad news from Finland that Orvokki Hein¨am¨aki had died. 447 Computational Linguistics Volume 33, Number 4 interpreted nonspecifically, describing the type of girl the director is looking for. In that sense, the continuation is incoherent because there is not yet any individual to refer to. (5a) has no such ambiguity; it entails the existence of an innocent blonde in the actual world and we can talk about her. But matters are more complicated. Although (5b) under a nonspecific reading of innocent blonde not establish a discourse referent in the actual world, we can nevertheless have one in a modal or hypothetical context, as in (6). The director is looking for an innocent be 17 years old. is another problem here. If we interpret innocent blonde in (6), a deontic reading. It is a requirement that she be 17 years old. However, the specific reading an epistemic interpretation. That is, we have made an inference about the age of the girl in question from her looks or other evidence. My work on discourse referents was a harbinger of the vast literature yet to come on topic including Bonnie Webber’s 1978 dissertation (Webber 1978), Irene Heim’s semantics 1982), and the theory of representation structures Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993). Looking back at my old paper, I am amused by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation An indefinite noun phrase creates a stable discourse referent just in case the clause it bound to is implied to be true by the context in which it That was the main idea in the 1969 paper. In the course of seeking evidence for this thesis, I came across an interesting class of verbs and constructions that give rise to such implications (Karttunen 1971a, 1971b). For example, the contrast between (7a) and (7b) is explained the semantic properties of the two verbs, a. John get a in September. John get a in September. John got a sabbatical, (7b) entails that he didn’t. The interesting fact about these verbs is that when we change the polarity from positive to negative we still get an entailment, but of the opposite polarity as seen in (8). a. John manage get a in September. John fail get a in September. exists quite a number of such implicatives yield an entailment in positive and negative contexts. Verbs like a positive entailment in contexts and a negative entailment in negative contexts Let us call Two-way implicatives like the polarity, so we call Table 1 gives a few examples of both types of verbs and constructions. 4 The term “stable” is in contrast with “short-term” for referents that have a limited life span. For example, can talk about a nonexistent car as in in wish Mary had a She could take me to work in I could the long as we are elaborating a hypothetical situation. 448 Karttunen Word Play Table 1 Some two-way implicative verbs and constructions. + − manage (to) fail(to) succeed (in) neglect (to) remember (to) forget (to) happen (to) avoid ... (ing) see fit (to) refrain (from) take the time ... (to) shy away (from) the foresight (to) stop Table 2 Some one-way implicative verbs and constructions. prevent can hesitate (to) preclude be able (to) Table 2 contains examples of verbs and constructions that certainly yield an entailment in one direction but not necessarily the other way. For example, it is tempting to conclude from (9a) that the president attended the meeting—and if there is no reason to think otherwise, the reader is entitled to that conclusion. Nevertheless, the author may take away that “invited inference” (Geis and Zwicky 1971) without contradicting himself as in (9b). (9) a. The president was able to attend the meeting. b. The president was able to attend the meeting but decided not to. The entailments of constructions involving more than one implicative verb have to be computed from “top–down.” The two examples in (10) establish a stable discourse referent because they both entail that a picture was taken. (10) a. John managed not to forget to take a b. Bill failed to prevent John from taking a picture. The early version of Kamp’s Discourse Representation Theory did not include any for computing lexical entailments about existence. I found the disappointingly static at the time. The semantics of complementation that I proposed was picked up by some computational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can computed directly by the parser, in contrast to the then prevailing view of the community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database systems (Kaplan 1977). 2.3 Presuppositions—Conventional Implicatures The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow 449 Computational Linguistics Volume 33, Number 4 the construction to empty of meaning. In general, if then it logically follows that equivalent: a. John managed to speak. John did not manage to speak. did not speak. this is of course wrong as far as (11) is concerned. Choosing the construction the speaker to the view that there was some difficulty involved. All the verbs in Table 1 bring in some additional commitment over and beyond what is entailed although it is difficult in some cases to pin down exactly what it is. Furthermore, the commitment remains the same regardless of whether the sentence is affirmative or negative. It is also present in questions and conditionals as shown in (12). (12) Did John manage to speak? If John managed to speak, it is a good sign. The extra bits of meaning attached to the two-way implicatives were yet another instance of a phenomenon that had already been discussed for some time under the term The term came from philosophers who had been debating heatedly for a long time whether present king of France false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: Factive verbs: forgot/didn’t forget that John had left. Factive adjectives: is/isn’t odd that the room is closed. Change-of-state verbs: stopped/hasn’t stopped smoking. Verbs of judging: criticized/didn’t criticize Harry for writing the letter. • is coming for dinner? Headless relatives: is/isn’t where Fred met Sally. Cleft sentences: was/wasn’t John who caught the thief. Pseudo-clefts: she wants/doesn’t want to talk about is herself. Temporal subordinate clauses: left/didn’t leave after Mary called. Iteratives: called/didn’t call again. Fred ate/didn’t eat another turnip. In addition to vastly enlarging the presupposition population, the linguistic community also came up with a problem that had been ignored in the philosophical literature up to that point: problem: are the presuppositions of a complex sentence derived from the presuppositions of the component clauses? 450 Karttunen Word Play This question was first posed by Langendoen and Savin (1971). Their answer was (page 57): The projection principle for presuppositions, therefore, is as follows: presuppositions of a subordinate clause do not amalgamate either with presuppositions or assertions of higher clauses; rather they stand as presuppositions of the complex sentence in which they occur. They were badly mistaken. Although the consequent clause of (13) by itself presupposes the existence of a unique king of France, (13) as a whole obviously does not. (13) If France has a king, I bet the king of France speaks only French. In a conditional sentence, a presupposition of the consequent clause can be “filtered” or “cancelled” away if it is entailed by the antecedent and general background knowledge. If a presupposition is not filtered locally and is not part of the context of the discourse, the reader or hearer must in some way adjust his or her state of knowledge to incorporate the new information. The idea is in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. (1979) called this process There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. don’t disagree with that It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulness. It is evident that no uniform theory can account for all the phenomena that historically have been together under the label But at least one good insight has emerged from this line of research. The accommodation strategy for definite descriptions is closely linked to anaphora resolution (van der 1992). One motivation for Kamp’s was to be able to handle “donkey anaphora” in sentences such as (14a). a. If John has a he beats b. If John has children, all of John’s children are bald. What van der Sandt observed was that the treatment of the anaphor in (14a) could be used in (14b) to eliminate the presupposition that John has children. Except for the dismissive Gricean implications triggered by the indefinite article in 451 Computational Linguistics Volume 33, Number 4 Assimilating presupposition projection into anaphora resolution is probably the right approach for definite descriptions and for iterative presuppositions triggered by such as verbs like particles such as However, it does not seem applicable to the kinds of presuppositions triggered by implicative verbs or factives. But the whole idea of accommodation is inappropriate for implicative verbs. Examples such as (11) and (12) commit the speaker to the view that it was difficult for John to speak. The audience may take note of that piece of information but it does not need to be accepted or accommodated for the discourse to proceed. Another phenomenon does not call for any accommodation is As Ellen Prince (1978) showed, a sentence such as (15) does not covertly slip into the discourse a piece of new information as being old. On the contrary, the rhetorical force of the is to tell you something that presumably you did not know before in a manner that makes the new piece of information incontestable. (15) It was/wasn’t Barbara Partee who in a private conversation around 1980 suggested to me that anaphora resolution and the satisfaction of the presuppositions of definite descriptions was the same problem. In my joint last paper on presuppositions (Karttunen and Peters 1979), Stanley Peters and I proposed to do the sensible thing, namely to divide up the heterogeneous collection of phenomena that had been lumped together under this misbegotten label. We suggested that many cases that had been called presupposition are best seen as of what Grice (1979) had called Conventional implicatures are propositions that the speaker or the author of the sentence is committed to by virtue of choosing particular words or constructions to express himself or herself. However, whether those implicatures are true or not does not have any bearing on the sentence is true or false. For example, because of the word (16) commits the author to the view that Bill is an unlikely person to agree with Mary. (16) Even Bill agrees with Mary. the meaning contributed by no role in determining the truth conditions of the sentence. (16) is true if Bill agrees with Mary and false otherwise. Our good advice went unheeded for a long time but in recent work by Christopher Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley and I sketched out that separates conventional implicatures from truth-conditional aspects of meaning. 2.4 Syntax and Semantics of Questions My paper on questions (Karttunen 1977) was an ambitious effort to give a unified account in the framework of Montague Grammar of the meaning of all types of interrogative phrases including direct questions such as the examples in (17) and embedded interrogatives illustrated in (18). (17) a. Is it raining? b. Do you want to go or do you want to stay? c. Which book did Mary read? d. Which girls date which boys? 452 Karttunen Word Play (18) a. John knows whether Bill smokes. b. Mary is thinking about whether to stay home or go to the movies. c. Bill remembers to whom John gave the book? d. She doesn’t care about who did what to whom. (17a) and (18a) are (17b) and (18b) are quespose two or more choices. As (17c,d) and (18c,d) illustrate, contain any number of interrogative quantifiers. and large, embedded alternative questions have the same syntactic as embedded For that reason a syntactician would prefer to have just a single category of embedded questions. In the framework of Montague Grammar this is possible only if all types of embedded interrogatives have the same type of meaning. From a semantic point of view, it would be desirable to assign a type of meaning to both direct and embedded questions. For example, date which boys have the same meaning as a direct question that it has embedded under a verb such as Finally, whatever meaning we assign to interrogatives, it should help us to elucidate the meaning of question-embedding verbs including the examples in (18) and the one in (19) that sets up a relation between two questions. (19) Whether Mary comes to the party depends on who invites her. With these desiderata in mind, I came to the conclusion that the best solution would be to adopt an approach proposed by Hamblin (1973) for direct questions and carry it further. Hamblin’s idea was to let every direct question denote a set of propositions, namely, the set of propositions expressed by all the possible answers to the question. example, under Hamblin’s analysis it raining? the set containing two is raining, It is not My improvement of that idea was to make the meaning of a question be a function that in each possible world picks up the set of true answers to the question. this new analysis it is possible to relate, for example, the meaning of a to the meaning of an embedded question as in (18a). Bill in our actual world is a smoker, then in our world Bill smokes out the set consisting of the proposition that Bill smokes. In that case, what John knows is Bill smokes. In examples such as (19) the meaning of on be explicated a function that in each world maps the true answers to invites Mary the answer(s) to Mary comes to the I worked out these ideas with all the rigor of a Montague grammarian. After years of apprenticeship I had finally become a formal A less restless soul would have stopped right there. But I didn’t. As others saw it, I fell from the pinnacle of semantics into the low life of finite-state automata. My semanticist friends kept asking, “What happened to you Lauri? You were such a good semanticist.” The politely unstated premise was that I had fallen onto skid row. 6 There are two yet unexplained exceptions to this generalization. So-called “emotive factives” such as surprised embedded but not be surprised where you find be surprised whether you find us. verbs such as the opposite characteristic. 7 Compared to the lively activity on the presupposition playground, the field of questions attracts few visitors. For later developments, see Hausser and Zaefferer (1979), Hausser (1983), Groenendijk and</abstract>
<note confidence="0.793923">Stokhof (1984), and Ginsburg (1992, 1996). 453 Computational Linguistics Volume 33, Number 4</note>
<abstract confidence="0.971429413580247">3. Interlude Towards the end of the 1970s I began to think that I had stumbled on, and helped to create and frame, more problems in semantics than I could ever solve. It was time to move on and leave the mess for others to clean up. In a bold move I signed up to teach a course on computational linguistics. As every professor knows, teaching a course on something you know next to nothing about is a great learning opportunity. To get some idea of how the field had developed in the previous ten years I went to the 1979 Meeting in La Jolla, California, and immediately ran into two of my old from Martin Kay and Ron Kaplan, very surprised to see me. “What are you doing here?” they asked. I said I had picked up a new hobby and was planning to do some computational work on Finnish morphology, the topic of my term paper for Martin’s course in 1966. On my sabbatical year at the Center for Advanced Study in Behavioral Sciences at Stanford, 1981–1982, I often went to visit Martin at the Palo Alto Research just a short drive from I learned about InterLisp. got to compute on the Alto personal computer and even had my own personal 1 floppy for it, about the size of a large briefcase. On the floppy was the project Martin and I were collaborating on, a unification based parser/generator for Finnish. Finnish a good test case for Martin’s unification grammar formalism. In constituents could be labeled by a syntactic category such as could be functional roles such as We showed how the constraints on Finnish word order could be described and implemented in those terms (Karttunen and Kay 1985a). joined the Artificial Intelligence Center at 1984. It was a good time to the move from academia to industrial research. an excellent mix of linguists and Barbara Grosz, Jerry Hobbs, David Israel, Robert Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there, others. and Xerox cofounders of the new Center for Study of Language and Information at Stanford, funded by a generous grant the System Development Foundation, an offshoot of the At Stuart Shieber had designed and implemented his influential II for unification-based grammars (Shieber et al. 1983). I implemented it (Karttunen 1984; and Kay 1985b; Karttunen 1986) at Interlisp on a Xerox Dandelion, a wonderful machine with Interlisp as the language of the operating system. 1987 I joined my friends at Xerox concentrate on my other computational In making the crosstown transit from Menlo Park to Palo Alto, I graduated from my lovely Dandelion to the top-of-the-line Xerox Dorado, still the best computing experience in my life. After all the years spent on theorizing and playing with formalisms, I wanted to do something practical that would have an impact on the real world. 4. Act II: Providing Solutions In the early 1980s, morphological analysis of natural language was a challenge to computational linguists. Simple cut-and-paste programs could be written to analyze strings in particular languages, but there was no general language-independent method available. Furthermore, cut-and-paste programs for analysis were not reversible, they could not be used to generate words. 454 Karttunen Word Play Generative phonologists of that time described morphological alternations by of ordered rules by Chomsky and Halle (1968). These rules of the form / γ where and be arbitrarily complex strings or feature matrices. It was not understood how such rules could be used for analysis. In 1981 I had organized a conference on parsing in Austin. Present at the conference was a visitor from Finland, Kimmo Koskenniemi, who was looking for a dissertation topic. Martin Kay and Ronald Kaplan were also there and it turned out that all four of us were interested in morphology. I demoed a small system I had built with my students for Finnish (Karttunen, Uszkoreit, and Root 1981). Martin and Ron reported that they had recently made a breakthrough discovery in computing with rewrite rules. Kimmo on to California to visit them at learn more. That was the beginning of our long collaboration. 4.1 Origins The discovery Kaplan and Kay had made was actually a rediscovery of a result that had been published a decade before in a book that none of us knew about at that time, a dissertation by C. Douglas Johnson (1972). Johnson observed that although the same context-sensitive rule could be applied several times recursively to its own output, phonologists have always assumed implicitly that the site of application moves to the right or to the left in the string after each application. For example, if the rule / γ δ used to rewrite the string any subsequent application of the rule must leave the unchanged, affecting only Johnson demonstrated that the effect of this constraint is that the pairs of inputs and outputs produced by a rewrite rule can be modeled by a Johnson was already aware of an important mathematical property of finite-state transducers established by Sch¨utzenberger (1961): for any pair of transducers applied sequentially there exists an equivalent single transducer. Any cascade of rule transducers can in principle be composed into a single transducer that maps lexical forms directly into the corresponding surface forms, and vice versa, without any intermediate representations. Koskenniemi was impressed by the theoretical result he learned from Kaplan and but not convinced about the practicality of the approach for morphological Traditional phonological rewrite rules describe the correspondence between lexical forms and surface forms as a one-directional, sequential mapping from lexical forms to forms. Even if it were possible to model the surface forms efficiently by means of finite-state transducers, it was not evident that it would lead to an efficient analysis procedure going in the reverse direction, from surface forms to lexical forms. Let us consider a simple illustration of the problem with two sequentially applied rules, -&gt; m / p -&gt; m / m The corresponding transducers map lexical form to with the intermediate representation. However if we apply the same transducers in the opposite direction the input we get the three results shown in Figure 1. This asymmetry is an 8 Johnson did a careful analysis of what at the time was one of the most comprehensive descriptions of alternations described in the Chomsky–Halle paradigm. This was the unpublished Qualifying Paper by James D. McCawley on Finnish. The data came from McCawley’s classmate, Paul Kiparsky, a native speaker of the language. One can claim that every advance in computational in the last 30 years involves Finnish and people whose last name begins with See Section 4.3. 455 Computational Linguistics Volume 33, Number 4 Figure 1 Deterministic generation, nondeterministic analysis. inherent property of the generative approach to phonological description. If all the rules are deterministic and obligatory and if the order of the rules is fixed, each lexical form generates only one surface form. But a surface form can typically be generated in more than one way, and the number of possible analyses grows with the number of rules that are involved. 4.2 Two-Level Morphology Back in Finland, Koskenniemi invented a new way to describe phonological alternations in finite-state terms. Instead of cascaded rules with intermediate stages and the computational problems they seemed to lead to, rules could be thought of as statements that directly constrain the surface realization of lexical strings. The rules would not be applied sequentially but in parallel. Each rule would constrain a certain lexical/surface correspondence and the environment in which the correspondence was allowed, required, or prohibited. For his 1983 dissertation, Koskenniemi (1983) constructed an ingenious implementation of his constraint-based model that did not depend on a rule composition, or any other finite-state algorithm, and he called it Two-level morphology is based on three ideas: • Rules are symbol-to-symbol constraints that are applied in parallel, not sequentially like rewrite rules. • The constraints can refer to the lexical context, to the surface context, or to both contexts at the same time. • Lexical lookup and morphological analysis are performed in tandem. Applying the rules in parallel does not in itself solve the overanalysis problem illusin Figure 1. The two constraints just sketched allow be analyzed as or However, the problem becomes manageable when there are no intermediate levels of analysis. In Koskenniemi’s 1983 system, the lexicon was as a forest of letter trees), tied together by continuation-class links leaves of one tree to the root of another tree or Lexical lookup and the The I had demoed to Koskenniemi on his 1981 visit to Austin had the same lexicon architecture (Karttunen, Uszkoreit, and Root 1981) . 456 Karttunen Word Play Figure 2 Following a path in the lexicon. analysis of the surface form are performed in tandem. In order to arrive at the point shown in Figure 2, the analyzer has traversed a branch in the lexicon that contains lexical string At this point, it only considers symbol pairs whose lexical side matches one of the outgoing arcs of the current state. It does not pursue analyses that have no matching lexical path. All the rule networks must accept every lexical:surface In the case at hand, the is accepted by the Rule requires a the context on the lexical side and by the Rule requires an the left context on the surface side. In two-level rules, zero (epsilon) is treated as an ordinary symbol. of this, a two-level rule represents an Conceptually, the system in Figure 2 simulates the intersection of the rules and the composition of the rules with the lexicon. Koskenniemi’s two-level morphology was the first practical general model in the history of computational linguistics for the analysis of morphologically complex languages. The language-specific components, the rules and the lexicon, were combined with a universal runtime engine applicable to all languages. The Texas I met Koskenniemi again in Finland around Christmas time in 1982. He had just finished the first implementation of a two-level system and gave me a printout of the program to take along, a thick stack of Pascal code. Back home I unfolded the long printout on the floor of a corridor and spent quite a bit of time crawling up and down the code trying to understand what it did, and learning Pascal along the way. I was going to teach computational linguistics again in the spring. Having figured out Kimmo’s program, it occurred to me that doing a Lisp implementation of the two-level model would be a good class project. We completed the project and published a collection of papers on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi the credit for the invention, we called it the The name stuck and many other The most popular of these is a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey</abstract>
<note confidence="0.745065857142857">(Black et al. 1987; Ritchie et al. 1987, 1992), Language Engine 1995), the Language Engineering Platform (Pulman 1991), and the (Armstrong 1996). A Compiler for Two-Level In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4</note>
<abstract confidence="0.990403861344538">but there was no rule compiler available at the time. Koskenniemi and other early of two-level morphology constructed their rule automata This is tedious in the extreme and very difficult for all but very simple rules. address this problem Kaplan, Kay, and I pooled our and invited Koskenniemi to Stanford in the Summer of 1985. Although two-level rules are conceptually quite different from the rewrite rules studied by Kaplan and Kay, the methods that had been developed for compiling rewrite rules were applicable to two-level rules as well. In both formalisms, the most difficult case is a rule where the symbol that is replaced or constrained also appears in the context part of the rule. This problem Kaplan and Kay had already solved by an ingenious technique for introducing and then eliminating auxiliary symbols to mark context boundaries. Another fundamental insight they had was the encoding of context restrictions in terms of double negation. example, a constraint such as be followed by can be expressed as “it is the case that something ending in not followed by something starting with Koskenniemi’s formalism, =&gt; In the course of the summer, Kaplan and Koskenniemi worked out the basic compilation algorithm for two-level rules. The first two-level rule compiler was written in InterLisp by Koskenniemi and me in 1985–1987 using Kaplan’s implementation of the finite-state calculus (Koskenniemi 1986; Karttunen, Koskenniemi, and Kaplan The current C-version two-level compiler, called was created at (Karttunen and Beesley 1992). It has extensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. Two-Level Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some of the languages for which there is a large-scale two-level grammar and a publication describing it: Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern S´ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994). 4.3 Lexical Transducers after arriving at made a serendipitous discovery. At the time collaborating with Microlytics, a company that marketed spell-checkers, the first sucstory of finite-state Microlytics had licensed from Koskenniemi’s company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan’s compression routine to make a Finnish spell-checker in the Microlytics format. For that task I designed an algorithm that simultaneously carried out the intersection of Koskenniemi’s 23-rule automata and the composition with the lexicon. I was surprised to see that not only did it work but the result was not significantly larger than the original source lexicon. Figure 3 is a sketch of that process. Just intersecting the rule automata by themselves was barely possible for us then because of the exponential worst-case complexity of the intersection algorithm. We assumed that the composition with a large lexicon might make the computation even harder to carry out. In fact the 10 Ron Kaplan will tell you more about that some day. 458 Karttunen Word Play Figure 3 Intersecting and composing two-level rules with a lexicon. Figure 4 A path in a lexical transducer. opposite happened. The reason should have been obvious from the beginning. The of a set of two-level rules explodes because it has to compute a result for lexical string. But if the set of lexical inputs is restricted to the forms that actually exist in the language, there is no blowup. The same applies to the composition of transducers derived from rewrite rules. If the rule cascade is computed starting with the lexicon, the “overanalysis” problem illustrated in Figure 1 never arises. The surface forms that Microlytics wanted for the Finnish spell-checker were easily extracted from the transducer, but we realized that keeping the lexical forms and their surface realizations in a single network would be even more valuable. I created a transducer for English with a small number of two-level rules. It consisted of mappings such as in Figure 4. Annie Zaenen and Carol Neidle created, with a large number of rules, a much more ambitious proof-of-concept, a lexical transducer for French, lemmas such as the corresponding surface form Such a transducer is the ultimate “two-level model” for a language as it compactly encodes all the forms with morphological tags), • all the inflected surface forms, • all the mappings between lexical forms and surface forms. A comprehensive analyzer such as we built for English and French consists of tens of thousands of states and hundreds of thousands of arcs; but physically they can be quite small, a couple of megabytes in size. The same network can be applied in two ways: to provide an analysis for a surface form or to generate a surface from a lexical form in a tiny fraction of a second. Karttunen, Kaplan, and Zaenen (1992) and Karttunen (1994) are the first published reports on lexical transducers. 1993 Xerox established a new European research center near Grenoble, France. Annie Zaenen and I went there to launch the Center’s research on natural 459 Computational Linguistics Volume 33, Number 4 language. We started with a couple of employees in an unfinished building with three empty floors, an elevator, and a pile of Sun workstations stacked at the entrance. Not knowing a word of French made it a hardship assignment for me, but in every other it was a lucky break. Because a start-up as a research center in need of visibility and recognition on the level of the Xerox Corporation, I got more resources help for my work than I could possibly have had at A Xerox business unit a contract with produce morphological analyzers and disambiguators (= “taggers”) for six European languages. Kenneth R. Beesley, who had worked for Microlytics, came to Grenoble to manage the development effort. I headed a small finite-state team of researchers and programmers charged with the mission of creating development and run-time tools such as Finite State Tool) and (Lexicon Compiler). It became evident that large systems of two-level rules were difficult to debug. We concluded that lexical transducers are easier to construct with sequentially applied rules than with the parallel two-level rules. Andr´e Kempe and I therefore developed compiler for rules 1995, 1996; Kempe and Karttunen The expression now includes a large set of different types of replace expressions: parallel replacement, replacement with multiple contexts, replacement with left-to-right or right-to-left shortest or longest match constraints, in addition to the usual finite-state operations union, intersection, composition, and negation. Beesley and I managed to get permission from the to release to researchers most of the tools that were developed in Grenoble for creating and applying finite-state networks, not just for morphological analysis but also for other such as tokenization and named-entity recognition. Ken and I wrote book, State Morphology and Karttunen 2003), a pedagogical text that explains and documents the tools that come with the book. There have been many improvements in the software since then. A new edition of the book is in the making. the time I left Grenoble to come back to 2001, Inxight, a Xerox spinoff company in California, was marketing finite-state morphological analyzers and stemmers for about three dozen languages. From a computational point of view morphology was a solved problem. 4.4 Computational vs. Paper-and-Pencil Morphology Historically, computational linguists and their “paper-and-pencil” counterparts in linguistics departments have been curiously out of sync in their approach to phonology and morphology. When computational linguists implemented parallel two-level models in the 1980s, paper-and-pencil linguists were stuck in the sequential Chomsky–Halle paradigm. Many arguments had been advanced in the phonological literature in the 1970s to show that phonological alternations could not be described or explained adequately without sequential rewrite rules. The idea of rules as constraints between a lexical symbol and its surface realization was seen as misguided. It went unnoticed that two-level rules could have the same effect as ordered rewrite rules because the realization of a lexical symbol could be constrained by the lexical side and/or by the 11 Our compilation algorithm was inspired by the landmark article of Kaplan and Kay (1994). We found a way to express the constraints on replacement using fewer auxiliary symbols than Kaplan and Kay. The compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary alphabet. 460 Karttunen Word Play surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). But in the mid 1990s when most computational linguists working with the Xerox tools embraced the sequential model as the more practical approach, a two-level theory over paper-and-pencil linguistics by storm in the guise of Theory (Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually working phonologists switched into the From my perspective a two-level model where the ranking of the constraints plays the role that rule-ordering has in the sequential model. If one believes, as I do, that the mapping from lexical forms to inflected surface forms is basically a regular relation, then the choice between the two ways of decomposing it, either as a composed cascade of replace operations or as an intersection of parallel rules, has important practical consequences but it is not a deep theoretical divide. In fact, the two-level analyzer for French discussed in Karttunen, Kaplan, and Zaenen (1992) combined parallel rules with composition. It is unclear to me why my paper-and-pencil colleagues seem to think that it has to be absolutely one or the other. I have written several papers in the hope of getting my paper-and-pencil colleagues interested in, or at least aware of, what is happening in computational morphology (Karttunen 1993, 1998, 2003, 2006). I have not succeeded. Paper-and-pencil morphologists in general are not interested in creating complete descriptions for particular languages. They design formalisms for expressing generalizations about morphological phenomena commonly found in all natural languages. Practical issues that arise in the context of real-life applications such as completeness of coverage, physical size, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms. My frustration is best summed up in a fable that I attached to my paper on a implementation of Gregory Stump’s morphology 2001; Karttunen 2003). Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computational knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: Dear Princess. I have wonderful news for you: You are not like some of your sisters. You are You are You are Please marry me. Together we can do great things. And time after time, the put-down response from the Princess has been the same: Not interested. You do not understand Theory. Go away, you geek. Because the most suitable suitor has always been rejected, I suspect that the Princess has a vested interest in making simple things appear more complicated than they really are. The good news that the computational knights are trying to deliver is unwelcome. The Princess prefers the pretense that phonology/morphology is a profoundly complicated subject, believing herself to be shrouded by veils of theories. 461 Computational Linguistics Volume 33, Number 4 If that is the correct analysis of the situation, computational linguists should adopt a different strategy. Instead of being the eternal rejected suitor at the Royal Court, they should adopt the role of the innocent boy in the street shouting The Princess has no clothes! The Princess has no clothes!... That was my conclusion in the 2003 paper. 5. Epilogue I am very happy to see that the topics I worked on at the very beginning of my career finally become relevant in To quote again the opening paragraph of my 1970 (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. 37-year-old prediction of semantics having value becoming a reality in the context of automated question answering and reasoning initiatives such as the Entailment Challenge (Dagan, Glickman, and Magnini 2005) and the (Karttunen and Zaenen 2005; Zaenen, Karttunen, and Crouch 2005). The first computational implementation of textual inferences arising from the six types of implicative constructions in Tables 1 and 2, and their interaction with factive verbs, is presented in Nairn, Condoravdi, and Karttunen (2006). We may soon see search engines that actually make use of semantic processing in addition to simple string matching. The ability to draw textual inferences will significantly improve the quality of question answering and Web searches. From a linguistic perspective, this is an auspicious time to take a fresh look at issues such as the classification of complement constructions. The availability of search engines such as Google makes it possible to check the linguist’s semantic intuitions against actual usage. One question I have always had about the classification of implicative constructions is whether the commitment to the truth or falsity of the complement clause is always based on a semantic entailment or whether some of these cases should be looked as a usage convention. For example, if you google the pattern hesitate it immediately evident that to is one of the rare (20) a. Head Coach Jon Gruden didn’t hesitate to share interesting Buccaneer information with the two Chamber of Commerce crowds on Friday. b. John didn’t hesitate to refer the file to CIB and from there it went to the Victoria Police. c. George Patton with all his swagger and confidence didn’t hesitate to throw himself and his men into the teeth of the German offensive and won the day. When you see examples such as in (20) in their full context, it is obvious that the author the complement clause of to a fact. But it is difficult to explain why should be this way starting from the concept of the semantics of the 462 Karttunen Word Play As a sample of things I am planning to do next, I will leave you two little puzzles to The construction wait to ambiguous. Here are a couple of examples from Google to to illustrate the ambiguity. (21) a. Deena did not wait to talk to anyone. Instead, she ran home. b. It hurt like hell, but I’m glad she didn’t wait to tell me. implies did not talk to But (21b) implies told me something right 1: does it come about that didn’t wait to do Y either that X did Y right away or that X didn’t do Y at all? you look at examples with wait to their full context, it is nearly always possible to tell which of the two meanings the author has in mind. In (21a), for instance, negative polarity item the word telltale indicators. In (21b), cataphoric pronoun that a telling event took place. I am sure that it is possible to learn to pick the intended meaning by statistical techniques. But statistics alone will not give you an answer to Question 1, nor will it solve the related problem in Question 2. 2: is it not possible to translate expressions such as didn’t wait take off his coat other languages in a way that preserves the ambiguity the sentence has in English? In languages such as Dutch, Finnish, French, German, Hungarian, and Japanese, among it is of course possible to express the two meanings of did not wait to but not in one and the same sentence. My answer to these two questions will have to wait until my next semantics paper.</abstract>
<title confidence="0.452935">Acknowledgments</title>
<author confidence="0.817747666666667">Many thanks to Kenneth R Beesley</author>
<author confidence="0.817747666666667">Daniel G Bobrow</author>
<author confidence="0.817747666666667">Robert Dale</author>
<author confidence="0.817747666666667">Aravind K Joshi</author>
<author confidence="0.817747666666667">John T Maxwell</author>
<author confidence="0.817747666666667">Bonnie Webber</author>
<abstract confidence="0.6755455">Annie Zaenen for their help on the style and content of this article.</abstract>
<note confidence="0.780726142857143">References Evan L. 1990. A Two-Level Processor for Morphological Number 16 in Occasional publications in academic computing. Summer Institute of Linguistics, Dallas. Armstrong, Susan. 1996. Multext:</note>
<title confidence="0.469711">Multilingual text tools and corpora. In</title>
<author confidence="0.746249">H Feldweg</author>
<author confidence="0.746249">E W Hinrichs</author>
<author confidence="0.746249">editors</author>
<affiliation confidence="0.914861">und Max Niemeyer Verlag,</affiliation>
<address confidence="0.977487">Tuebingen, Germany, pages 107–112.</address>
<author confidence="0.737654">I David</author>
<affiliation confidence="0.862668333333333">in Dynamic Ph.D. thesis, Center for Cognitive Science, University of Edinburgh, Edinburgh,</affiliation>
<address confidence="0.891844">Scotland.</address>
<note confidence="0.903377166666667">Beesley, Kenneth R. and Lauri Karttunen. State CSLI Publications, Stanford, CA. Bennett, Winfield S. and Jonathan Slocum. 1985. The LRC machine translation system. 11(2–3):111–121. Black, A., G. Ritchie, S. Pulman, and G. Russell. 1987. Formalisms for morphographemic description. In Proceedings of the Third Conference of the European Chapter of the Association for pages 11–18,</note>
<address confidence="0.783948">Copenhagen, Denmark. Carter, D. 1995. Rapid development of</address>
<abstract confidence="0.910281">morphological descriptions for full language processing systems. In</abstract>
<note confidence="0.687741583333333">Proceedings of the Seventh Conference of the European Chapter of the Association for pages 202–209, Dublin, Ireland. Noam. 1957. Mouton, Gravenhage, The Netherlands. Chomsky, Noam. 1973. Conditions on transformations. In Steven Anderson and 463 Computational Linguistics Volume 33, Number 4 Kiparsky, editors, Festschrift for Holt, Reinhard, and Winston, New York, NY, pages 232–286. Noam and Morris Halle. 1968. Pattern of Harper and Row, New York, NY. Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising entailment challenge. In of the PASCAL Challenges Workshop Recognising Textual pages 1–8, Southampton, UK. Eisner, Jason. 2002. Phonological comprehension and the compilation of theory. In of the 40th Annual Meeting of the Association for pages 56–63, Washington, DC. Ellison, T. Mark. 1994. Phonological derivation in Optimality Theory. In Proceedings of the 15th International on Computational pages 1007–1013, Kyoto, Japan. Fillmore, Charles. 1971. Verbs of judging: An exercise in semantic description. In Charles J. Fillmore and Terence</note>
<degree confidence="0.707616">editors, in</degree>
<author confidence="0.545111">Rinehart Holt</author>
<author confidence="0.545111">Inc Winston</author>
<author confidence="0.545111">New York</author>
<author confidence="0.545111">NY</author>
<note confidence="0.70010025">pages 273–289. Gottlob. 1892. Sinn und f¨ur Philosophie und pages 25–50. English</note>
<title confidence="0.656673">translation: “On Sense and Meaning,”</title>
<author confidence="0.655357">Brian McGuiness</author>
<author confidence="0.655357">Basil Blackwell editor</author>
<author confidence="0.655357">Oxford</author>
<note confidence="0.9475524">pages 157–177. Gajek, Oliver, Hanno T. Beck, Diane Elder, and Greg Whittemore. 1983. Lisp implementation. In Mary Dalrymple, Edit Doron, John Goggin, Beverly Goodman, John McCarthy, editors, Forum, Vol. Department of Linguistics, The University of Texas at Austin, Austin, TX, pages 187–202. Gazdar, Gerald. 1979. Conventional implicature. In Choon-Kyu Oh and David Dinneen, editors, and Semantics, 11: Academic Press, New York, NY, pages 57–89. Geis, Michael and Arnold Zwicky. 1971. invited inferences. 2:561–565. Bart. 1999. and Elsevier, Cambridge, MA. Jonathan. 1992. Queries and Facts: A Semantics and Pragmatics for Ph.D. thesis, Stanford University, Stanford, CA. Ginzburg, Jonathan. 1996. Interrogatives: Questions, facts, and dialogue. In Shalom</note>
<affiliation confidence="0.8229705">editor, of Contemporary Blackwell Publishers,</affiliation>
<address confidence="0.755711">Oxford, UK, pages 385–422.</address>
<note confidence="0.944170466666667">Grice, H. Paul. 1979. Logic and conversation. In P. Cole and J. L. Morgan, editors, Academic Press, New York, NY, pages 41–58. Groenendijk, Jeroen and Martin Stokhof. 1984. On the semantics of questions and the pragmantics of answers. In Fred Landman and Frank Veltman, editors, of Formal Foris Publications, Dordrecht, The Netherlands, pages 143–170. Hamblin, Charles L. 1973. Questions in English. of 10:41–53. Randy Allen. 1995. Linguistics</note>
<affiliation confidence="0.999269">Oxford University Press,</affiliation>
<address confidence="0.6754255">Oxford, UK. Hausser, Roland. 1983. The Syntax and</address>
<note confidence="0.6014145625">Semantics of English Mood. In Ferenc editor, and Reidel, Dordrecht, The Netherlands, pages 97–158. Hausser, Roland and Dietmar Zaefferer. 1979. Questions and answers in a context dependent Montague grammar. In Siegfried Josef Schmidt and Franz editors, Semantics Pragmatics for Natural Reidel, Dordrecht, The Netherlands, pages 339–358. David G. 1967. to Elsevier, New York, NY. Irene. 1982. Semantics of Definite</note>
<affiliation confidence="0.7527815">Indefinite Noun Ph.D. thesis, University of Massachusetts, Amherst, MA.</affiliation>
<address confidence="0.694481">Heim, Irene. 1983. On the projection problem</address>
<abstract confidence="0.4613075">presuppositions. In on Formal volume 2,</abstract>
<note confidence="0.835875615384615">pages 114–126, Stanford, CA. Orvokki. 1974. of Temporal Ph.D. thesis, University of Texas, Austin, TX. C. Douglas. 1972. Aspects Phonological Mouton, The Hague, The Netherlands. Joshi, Aravind K. and Ralph M. Weischedel. 1973. Some frills for the modal tic-tac-toe of Davies and Isard: Semantics of predicate constructions. In pages 352–355, Stanford, CA. Ren´e. 1999.</note>
<affiliation confidence="0.999033">Cambridge University Press,</affiliation>
<address confidence="0.989972">Cambridge, UK.</address>
<note confidence="0.7711901875">464 Karttunen Word Play Kamp, Hans. 1981. Ev´enements, repr´esentation discursive et r´ef´erence 64:39–64. Kamp, Hans. 2001. The importance of presupposition. In Christian Rohrer, Antje Roßdeutscher, and Hans Kamp, editors, Form and its CSLI, Stanford, CA, pages 207–254. Hans and Uwe Reyle.1993. to Kluwer, Dordrecht, The Netherlands. Kaplan, Ronald M. and Martin Kay. 1994. Regular models of phonological rule 20(3):331–378.</note>
<author confidence="0.549657">Responses</author>
<affiliation confidence="0.6680685">a Natural Language Query Ph.D. thesis, University of Pennsylvania,</affiliation>
<address confidence="0.913873">Philadelphia, PA.</address>
<note confidence="0.40281">Lauri. 1969a. of Reference</note>
<affiliation confidence="0.965674">Ph.D. thesis, Indiana University,</affiliation>
<address confidence="0.906214">Bloomington, Indiana. Karttunen, Lauri. 1969b. Pronouns and</address>
<note confidence="0.885129285714286">In 5: Proceedings of the Regional pages 108–116, Chicago, IL. Karttunen, Lauri. 1971a. Implicative verbs. 47:340–358. Karttunen, Lauri. 1971b. The logic of English predicate complement constructions. The</note>
<affiliation confidence="0.999734">Indiana University Linguistics Club.</affiliation>
<address confidence="0.984349">Bloomington, Indiana. Karttunen, Lauri. 1973. Presuppositions of</address>
<abstract confidence="0.850409">sentences.</abstract>
<note confidence="0.656986083333333">4:167–193. Karttunen, Lauri. 1974. Presupposition and context. 1(1):181–194. Karttunen, Lauri. 1976. Discourse referents. James D. McCawley, editor, and Semantics Volume 7, Notes from the Linguistic Academic Press, New York, NY, pages 363–385. Karttunen, Lauri. 1977. Syntax and semantics questions. and 1:1–44.</note>
<author confidence="0.734794">KIMMO A general morphological processor In Mary Dalrymple</author>
<author confidence="0.734794">Edit Doron</author>
<author confidence="0.734794">John Goggin</author>
<author confidence="0.734794">Beverley Goodman</author>
<author confidence="0.734794">John McCarthy</author>
<author confidence="0.734794">Linguistic Forum</author>
<author confidence="0.734794">volume</author>
<affiliation confidence="0.99555">Department of Linguistics, The University</affiliation>
<address confidence="0.48683">of Texas at Austin, Austin, TX,</address>
<note confidence="0.80980537254902">pages 165–186. Karttunen, Lauri. 1984. Features and values. pages 28–33, July 2–6, Stanford, CA. Karttunen, Lauri.1986. D-PATR: A development environment for unification-based grammars. In pages 74–80, Bonn, Germany. Karttunen, Lauri. 1993. Finite-state constraints. In John Goldsmith, editor, Last Phonological University of Chicago Press, Chicago, IL. Karttunen, Lauri. 1994. Constructing lexical In pages 406–411, Kyoto, Japan. Karttunen, Lauri. 1995. The replace In cmp-lg/9504032. Cambridge, MA. Karttunen, Lauri. 1996. Directed In cmp-lg/9606029. Santa Cruz, CA. Karttunen, Lauri. 1998. The proper treatment of optimality in computational phonology. International Workshop on Finite-State Methods in Natural Language cmp-lg/9804002. Bilkent University, Ankara, Turkey. Karttunen, Lauri. 2003. Computing with realizational morphology. In Alexander editor, Linguistics Intelligent Text volume 2588 Notes in Computer Springer Verlag, Heidelberg, Germany, pages 205–216. Karttunen, Lauri. 2006. The insufficiency of paper-and-pencil linguistics: The case of Finnish prosody. In Miriam Butt, Mary Dalrymple, and Tracy Holloway King, Linguistic pages 287–300. CSLI Publications, Stanford, CA. Karttunen, Lauri and Kenneth R. Beesley. 1992. Two-level rule compiler. Technical Report ISTL-92-2, Xerox Palo Alto Research Center, Palo Alto, CA. Karttunen, Lauri, Ronald M. Kaplan, and Annie Zaenen. 1992. Two-level morphology with composition. pages 141–148, Nantes, France. Karttunen, Lauri and Martin Kay. 1985a.</note>
<title confidence="0.926938">Parsing in a free word order language. In</title>
<author confidence="0.957214">David R Dowty</author>
<author confidence="0.957214">Lauri Karttunen</author>
<affiliation confidence="0.7283525">Zwicky, editors, Language Cambridge University Press,</affiliation>
<address confidence="0.983701">Cambridge, UK, pages 279–306.</address>
<note confidence="0.648297636363636">Karttunen, Lauri and Martin Kay. 1985b. Structure sharing with binary trees. In Proceedings of the 23rd Meeting of the for Computational pages 133–136, Chicago, IL. Karttunen, Lauri, Kimmo Koskenniemi, and Ronald M. Kaplan. 1987. A compiler for two-level phonological rules. In Mary Dalrymple, Ronald Kaplan, Lauri 465 Computational Linguistics Volume 33, Number 4</note>
<author confidence="0.4317445">Kimmo Koskenniemi Karttunen</author>
<author confidence="0.4317445">Sami</author>
<author confidence="0.4317445">Michael Wescoat</author>
<author confidence="0.4317445">editors</author>
<affiliation confidence="0.826227333333333">Morphological Center for the Study of Language and Information, Stanford University, Palo Alto, CA,</affiliation>
<note confidence="0.8661589">pages 1–61. Karttunen, Lauri and Stanley Peters. 1979. Conventional implicature. In Choon-Kyu and David A. Dinneen, editors, Semantics, Volume 11: Academic Press, New York, NY, pages 1–56. Karttunen, Lauri, Hans Uszkoreit, and Rebecca Root. 1981. Morphological analysis of Finnish by computer. In Proceedings of the 71st Annual Meeting of Albuquerque, NM. Karttunen, Lauri and Annie Zaenen. 2005. Veridicity. In Graham Katz, James Pustejovsky, and Frank Schilder, editors, Annotating, Extracting and Reasoning about and number 05151 in Dagstuhl Seminar Proceedings. Internationales Begegnungs-und Forschungszentrum (IBFI), Schloss Dagstuhl, Germany,</note>
<web confidence="0.990328">http://drops.dagstuhl.de/opus/</web>
<degree confidence="0.85870625">Keenan, Edward L. 1971. Two kinds of presupposition in natural language. In Charles J. Fillmore and Terence editors, in Linguistic</degree>
<affiliation confidence="0.582446">Holt, Rinehart and Winston,</affiliation>
<address confidence="0.72799">Inc., New York, NY, pages 45–54.</address>
<note confidence="0.893730857142857">Kempe, Andr´e and Lauri Karttunen. 1996. Parallel replacement in finite-state In cmp-lg/9607007. Copenhagen, Denmark. Kiparsky, Paul and Carol Kiparsky.1971. Fact. In D. Steinberg and L. Jakobovits, An Inderdisciplinary pages 345–369. Cambridge University Press, Cambridge, UK. Koskenniemi, Kimmo. 1983. Two-level morphology: A general computational model for word-form recognition and production. Publication 11, University of Helsinki, Department of General</note>
<address confidence="0.751198">Linguistics, Helsinki. Koskenniemi, Kimmo. 1986. Compilation of</address>
<abstract confidence="0.78917725">automata from morphological two-level In Fred Karlsson, editor, from the Fifth Scandinavian Conference on pages 143–149,</abstract>
<address confidence="0.572692">Helsinki, Finland. Langendoen, Terence and Harris B. Savin. 1971. The projection problem for</address>
<author confidence="0.988287">In Charles J Fillmore</author>
<affiliation confidence="0.683519">and Terence Langendoen, editors, in Linguistic Holt, Rinehart and Winston, Inc., New York,</affiliation>
<address confidence="0.799472">pages 55–62. Lewis, David. 1979. Scorekeeping in a</address>
<abstract confidence="0.5434065">game. of Philosophical 8:339–359.</abstract>
<author confidence="0.565119">J John</author>
<affiliation confidence="0.998871">Cambridge University</affiliation>
<address confidence="0.970191">Press, Cambridge, UK.</address>
<author confidence="0.781229">Where do noun phrases come from In Roderick A Jacobs Peter S Rosenbaum</author>
<author confidence="0.781229">English Transformational editors</author>
<affiliation confidence="0.49739">Ginn and Company, Waltham, MA,</affiliation>
<address confidence="0.583775">pages 166–183. Montague, Richard. 1970a. English as a</address>
<abstract confidence="0.6317475">formal language. In B. Visentini et al., nella Societ`a e nella</abstract>
<affiliation confidence="0.959766">Edizioni di Comunit`a, Milan,</affiliation>
<address confidence="0.773808">Italy, pages 189–224. Montague, Richard. 1970b. Universal 36:373–398. Montague, Richard. 1973. The proper</address>
<title confidence="0.405634">treatment of quantification in ordinary</title>
<author confidence="0.6993605">In P Suppes K J J Hintikka</author>
<author confidence="0.6993605">M E Moravcsik</author>
<author confidence="0.6993605">to editors</author>
<note confidence="0.780735771428571">Reidel, Dordrecht, The Netherlands, pages 221–242. Moshagen, Sjur, Pekka Sammallahti, and Trond Trosterud. 2006. Twol at work. In Antti Arppe, Lauri Carlson, Krister Lind´en, Jussi Piitulainen, Mickael Suominen, Martti Vainio, Hanna Westerlund, and Anssi Yli-Jyr¨a, into Words, Constraints CSLI, Stanford, CA, pages 94–105. Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative for textual inference. In Computational Semantics Buxton, UK. Oflazer, Kemal. 1994. Two-level description Turkish morphology. and 9(2):137–148. Partee, Barbara. 1995. Montague grammar transformational grammar. 6:203–300. Christopher. 2004. Logic of Oxford University Press, Oxford, UK. Prince, Allan and Paul Smolensky. 1993. Optimality Theory: Constraint interaction in generative grammar. RuCCS Technical Report 2. Rutgers Center for Cognitive Science. Rutgers University, Piscataway, NJ. Prince, Ellen. 1978. A comparison of and in discourse. 54:883–906. 466</note>
<title confidence="0.821504">Karttunen Word Play</title>
<author confidence="0.93383125">Two level morphology In H Alshawi</author>
<author confidence="0.93383125">D Arnold</author>
<author confidence="0.93383125">R Backofen</author>
<author confidence="0.93383125">D Carter</author>
<author confidence="0.93383125">J Lindop</author>
<author confidence="0.93383125">K Netter</author>
<author confidence="0.93383125">S Pulman</author>
<author confidence="0.93383125">J Tsujii</author>
<author confidence="0.93383125">H Uskoreit</author>
<affiliation confidence="0.595471">Rule Formalism and Virtual Design CEC, Luxembourg,</affiliation>
<address confidence="0.505506">chapter 5. Ritchie, G., A. Black, S. Pulman, and G. Russell. 1987. The Edinburgh/</address>
<abstract confidence="0.49977675">Cambridge morphological analyser and dictionary system (version 3.0) user manual. Technical Report Software Paper No. 10, Department of Artificial</abstract>
<affiliation confidence="0.998481">Intelligence, University of Edinburgh,</affiliation>
<address confidence="0.750931">Edinburgh, UK. Ritchie, G., G. Russell, A. Black, and</address>
<note confidence="0.761725631578947">Pulman. 1992. Morphology: Mechanisms for the English The MIT Press, Cambridge, MA. Bertrand. 1905. On denoting. 14:479–493. Russell, Bertrand. 1957. Mr. Strawson on 66:385–389. Schiller, Anne. 1996. Deutsche Flexionsund Kompositionsmorphologie mit PC-KIMMO. In Roland Hausser, Verifikation: Dokumentation zur Ersten Morpholympics number 34 in Sprache und Information, pages 37–52. Max Niemeyer Verlag, T¨ubingen. Sch¨utzenberger, Marcel-Paul. 1961. A remark finite transducers. and 4:185–196. Shieber, Stuart, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Barbara J. Grosz and Mark editors, on Interactive and Use of SRI International, Menlo Park, CA, techreport 4, pages 39–79. Soames, S. 1982. How presuppositions are inherited: A solution to the projection 13:483–545. Stalnaker, Robert. 1973. Presuppositions. Journal of Philosophical 2:447–457. Peter. 1950. On referring. 59:320–344. Strawson, Peter. 1964. Identifying reference truth values. 30:320–344. Gregory T. 2001. A Theory of Paradigm</note>
<affiliation confidence="0.652349">Cambridge University Press, Cambridge, UK.</affiliation>
<address confidence="0.545809">Uibo, Heli. 2006. Eesti keele morfoloogia</address>
<abstract confidence="0.787408615384615">modelleerimisest l˜oplike muundurite abil. [on modelling the estonian morphology by the means of finite-state transducers]. In Koit, R. Pajusalu, and H. editors, ja number 6 in Tartu ¨uldkeeleteaduse ˜oppetooli toimetised. Max Niemeyer Verlag, T¨ubingen, Germany. van der Sandt, Rob. 1992. Presupposition as anaphora resolution. 9:333–377. van der Sandt, Rob A. and Bart Geurts. 1991. Presupposition, anaphora, and lexical In Understanding in LiLOG,</abstract>
<affiliation confidence="0.861401333333333">Integrating Computational Linguistics and Artificial Intelligence, Final Report on the IBM Springer Verlag,</affiliation>
<address confidence="0.936954">London, UK, pages 259–296.</address>
<affiliation confidence="0.998809">Cornell University Press,</affiliation>
<address confidence="0.998997">Ithaca, NY.</address>
<note confidence="0.799543714285714">Robert E. 1972. to Prentice Hall, Englewood Cliffs, NJ. Bonnie L. 1978. Formal Approach to Ph.D. thesis, Harvard University, Cambridge, MA. Ralph. 1975. of a</note>
<title confidence="0.403728">Unique Class of Inferences: Presupposition</title>
<degree confidence="0.307678666666667">Ph.D. thesis, University of Pennsylvania, Philadelphia, PA. Zaenen, Annie, Lauri Karttunen, and</degree>
<author confidence="0.744712">Local textual</author>
<abstract confidence="0.976802571428571">inference: Can it be defined or In on the Empirical Modeling of Semantic Equivalence and pages 31–36, Ann Arbor, MI. Zeevat, Henk. 1992. Presupposition and accommodation in update semantics. of 9:379–412.</abstract>
<intro confidence="0.662893">467</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evan L Antworth</author>
</authors>
<title>PC-KIMMO: A Two-Level Processor for Morphological Analysis.</title>
<date>1990</date>
<journal>Number</journal>
<booktitle>in Occasional publications in academic computing. Summer Institute of Linguistics,</booktitle>
<volume>16</volume>
<location>Dallas.</location>
<contexts>
<context position="47743" citStr="Antworth 1990" startWordPosition="7989" endWordPosition="7990">ng to teach computational linguistics again in the spring. Having figured out Kimmo’s program, it occurred to me that doing a Lisp implementation of the two-level model would be a good class project. We completed the project and published a collection of papers on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was n</context>
</contexts>
<marker>Antworth, 1990</marker>
<rawString>Antworth, Evan L. 1990. PC-KIMMO: A Two-Level Processor for Morphological Analysis. Number 16 in Occasional publications in academic computing. Summer Institute of Linguistics, Dallas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Armstrong</author>
</authors>
<title>Multext: Multilingual text tools and corpora.</title>
<date>1996</date>
<pages>107--112</pages>
<editor>In H. Feldweg and E. W. Hinrichs, editors, Lexikon und Text. Max Niemeyer Verlag,</editor>
<location>Tuebingen, Germany,</location>
<contexts>
<context position="48105" citStr="Armstrong 1996" startWordPosition="8042" endWordPosition="8043">nniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand. This is tedious in the extreme and very difficult for all but very simple rules. To address this problem Kaplan, Kay, and I pooled our CSLI funds and invited Koskenniemi to Stanford in the Summer of 1985. Although tw</context>
</contexts>
<marker>Armstrong, 1996</marker>
<rawString>Armstrong, Susan. 1996. Multext: Multilingual text tools and corpora. In H. Feldweg and E. W. Hinrichs, editors, Lexikon und Text. Max Niemeyer Verlag, Tuebingen, Germany, pages 107–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David I Beaver</author>
</authors>
<title>Presupposition and Assertion in Dynamic Semantics.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Center for Cognitive Science, University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="27724" citStr="Beaver (1995)" startWordPosition="4678" endWordPosition="4679"> extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulness. It is evident that no uniform theory can account for all the phenomena that historically have b</context>
</contexts>
<marker>Beaver, 1995</marker>
<rawString>Beaver, David I. 1995. Presupposition and Assertion in Dynamic Semantics. Ph.D. thesis, Center for Cognitive Science, University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>Finite State Morphology.</title>
<date>2003</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="56066" citStr="Beesley and Karttunen 2003" startWordPosition="9302" endWordPosition="9305"> replacement, replacement with multiple contexts, replacement with left-to-right or right-to-left shortest or longest match constraints, in addition to the usual finite-state operations union, intersection, composition, and negation. Ken Beesley and I managed to get permission from the XRCE management to release to researchers most of the tools that were developed in Grenoble for creating and applying finite-state networks, not just for morphological analysis but also for other useful NLP tasks such as tokenization and named-entity recognition. Ken and I wrote a book, Finite State Morphology (Beesley and Karttunen 2003), a pedagogical text that explains and documents the tools that come with the book. There have been many improvements in the software since then. A new edition of the book is in the making. By the time I left Grenoble to come back to PARC in 2001, Inxight, a Xerox spinoff company in California, was marketing finite-state morphological analyzers and stemmers for about three dozen languages. From a computational point of view morphology was a solved problem. 4.4 Computational vs. Paper-and-Pencil Morphology Historically, computational linguists and their “paper-and-pencil” counterparts in lingui</context>
</contexts>
<marker>Beesley, Karttunen, 2003</marker>
<rawString>Beesley, Kenneth R. and Lauri Karttunen. 2003. Finite State Morphology. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winfield S Bennett</author>
<author>Jonathan Slocum</author>
</authors>
<date>1985</date>
<booktitle>The LRC machine translation system. Computational Linguistics,</booktitle>
<pages>11--2</pages>
<marker>Bennett, Slocum, 1985</marker>
<rawString>Bennett, Winfield S. and Jonathan Slocum. 1985. The LRC machine translation system. Computational Linguistics, 11(2–3):111–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Black</author>
<author>G Ritchie</author>
<author>S Pulman</author>
<author>G Russell</author>
</authors>
<title>Formalisms for morphographemic description.</title>
<date>1987</date>
<booktitle>In Proceedings of the Third Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>11--18</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="47927" citStr="Black et al. 1987" startWordPosition="8014" endWordPosition="8017">ood class project. We completed the project and published a collection of papers on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand. This is tedious in the extreme and ver</context>
</contexts>
<marker>Black, Ritchie, Pulman, Russell, 1987</marker>
<rawString>Black, A., G. Ritchie, S. Pulman, and G. Russell. 1987. Formalisms for morphographemic description. In Proceedings of the Third Conference of the European Chapter of the Association for Computational Linguistics, pages 11–18, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Carter</author>
</authors>
<title>Rapid development of morphological descriptions for full language processing systems.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>202--209</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="48001" citStr="Carter 1995" startWordPosition="8028" endWordPosition="8029">s on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand. This is tedious in the extreme and very difficult for all but very simple rules. To address this problem Kaplan,</context>
</contexts>
<marker>Carter, 1995</marker>
<rawString>Carter, D. 1995. Rapid development of morphological descriptions for full language processing systems. In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics, pages 202–209, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<location>Mouton, Gravenhage, The Netherlands.</location>
<contexts>
<context position="5220" citStr="Chomsky 1957" startWordPosition="863" endWordPosition="864"> They were written by a postdoctoral research associate at the University of Texas at Austin, who had arrived from Finland in 1964 by way of the University of Indiana at Bloomington where he had just received a Ph.D. in Linguistics. Where did the young man acquire, and why was he spouting, that kind of computational rhetoric, when the record shows that for the next ten years he never laid his hands on a computer? The fact is that I did have a brush with computational linguistics before settling down to do pure semantics in the 1970s. I wanted to do linguistics because of Syntactic Structures (Chomsky 1957) and when the Uralic and Altaic Studies Department in Bloomington offered me a job in 1964 as a “native informant” in Finnish I accepted and managed to get into the Linguistics department as a graduate student. My job title turned out not to be accurate. During my first two years in Bloomington I was teaching Finnish on my own for nine hours per week. Luckily, I signed up for a course on computational linguistics taught by an excellent teacher and mentor, Robert E. Wall. Bob Wall had participated in an early MT project at Harvard and in a project on automatic 1 The subscripts i and j are refer</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Chomsky, Noam. 1957. Syntactic Structures. Mouton, Gravenhage, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Conditions on transformations.</title>
<date>1973</date>
<pages>232--286</pages>
<editor>In Steven Anderson and Paul Kiparsky, editors, A Festschrift for Morris Halle. Holt, Reinhard, and Winston,</editor>
<location>New York, NY,</location>
<contexts>
<context position="13173" citStr="Chomsky 1973" startWordPosition="2265" endWordPosition="2266">ns at MIT in the Fall of 1972. In the spring before I had a surprise phone call from Paul Kiparsky who said that the MIT Department was still looking for a one-year replacement for David Perlmutter who was going on a sabbatical. Would I be interested? Of course I was. I had come to the U.S. seven years earlier to study linguistics because of Noam Chomsky and now I had an office just across from hall from his. But during the year I was there, I lost interest in transformational syntax. I found Chomsky’s Thursday lectures of that year, on themes later published as Conditions on Transformations (Chomsky 1973), uncompelling. My sense of what was interesting had changed. The “Linguistic Wars” (Harris 1995) between generative (George Lakoff, John Ross, James D. McCawley, Paul Postal, and others) and interpretive semantics (Ray Jackendoff and others) had been won by Chomsky for the interpretivists, although Chomsky himself was, and still is, skeptical of any kind of formal theory of meaning. My sympathies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at UCL</context>
</contexts>
<marker>Chomsky, 1973</marker>
<rawString>Chomsky, Noam. 1973. Conditions on transformations. In Steven Anderson and Paul Kiparsky, editors, A Festschrift for Morris Halle. Holt, Reinhard, and Winston, New York, NY, pages 232–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English. Harper and Row,</title>
<date>1968</date>
<location>New York, NY.</location>
<contexts>
<context position="39475" citStr="Chomsky and Halle (1968)" startWordPosition="6631" endWordPosition="6634">cal that would have an impact on the real world. 4. Act II: Providing Solutions In the early 1980s, morphological analysis of natural language was a challenge to computational linguists. Simple cut-and-paste programs could be written to analyze strings in particular languages, but there was no general language-independent method available. Furthermore, cut-and-paste programs for analysis were not reversible, they could not be used to generate words. 454 Karttunen Word Play Generative phonologists of that time described morphological alternations by means of ordered rewrite rules introduced by Chomsky and Halle (1968). These rules are of the form α → β / γ δ, where α, β, γ, and δ can be arbitrarily complex strings or feature matrices. It was not understood how such rules could be used for analysis. In 1981 I had organized a conference on parsing in Austin. Present at the conference was a visitor from Finland, Kimmo Koskenniemi, who was looking for a dissertation topic. Martin Kay and Ronald Kaplan were also there and it turned out that all four of us were interested in morphology. I demoed a small system I had built with my students for Finnish (Karttunen, Uszkoreit, and Root 1981). Martin and Ron reported</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Chomsky, Noam and Morris Halle. 1968. The Sound Pattern of English. Harper and Row, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>1--8</pages>
<location>Southampton, UK.</location>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 1–8, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Phonological comprehension and the compilation of optimality theory.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>56--63</pages>
<location>Washington, DC.</location>
<contexts>
<context position="60262" citStr="Eisner (2002)" startWordPosition="9957" endWordPosition="9958">, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms. My frustration is best summed up in a fable that I attached to my paper on a finite-state implementation of Gregory Stump’s realizational morphology (Stump 2001; Karttunen 2003). Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computational knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: Dear Princess. I have wonderful news for you: You are not like some of your NP-complete sisters. You are regular. You are rational. You are finite-state. Please marry me. Together we can do great things. And time after time, the put-down response from the Princess has been the same: Not interested. You do not understand Theory. Go away, you geek. Because the most suitable suitor has always been rejected, I su</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Eisner, Jason. 2002. Phonological comprehension and the compilation of optimality theory. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 56–63, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>Phonological derivation in Optimality Theory.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>1007--1013</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="60245" citStr="Ellison (1994)" startWordPosition="9954" endWordPosition="9955">age, physical size, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms. My frustration is best summed up in a fable that I attached to my paper on a finite-state implementation of Gregory Stump’s realizational morphology (Stump 2001; Karttunen 2003). Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computational knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: Dear Princess. I have wonderful news for you: You are not like some of your NP-complete sisters. You are regular. You are rational. You are finite-state. Please marry me. Together we can do great things. And time after time, the put-down response from the Princess has been the same: Not interested. You do not understand Theory. Go away, you geek. Because the most suitable suitor has always be</context>
</contexts>
<marker>Ellison, 1994</marker>
<rawString>Ellison, T. Mark. 1994. Phonological derivation in Optimality Theory. In Proceedings of the 15th International Conference on Computational Linguistics, pages 1007–1013, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>Verbs of judging: An exercise in semantic description.</title>
<date>1971</date>
<booktitle>Studies in Linguistic Semantics.</booktitle>
<pages>273--289</pages>
<editor>In Charles J. Fillmore and Terence Langendoen, editors,</editor>
<publisher>and Winston, Inc.,</publisher>
<location>Holt, Rinehart</location>
<contexts>
<context position="24859" citStr="Fillmore 1971" startWordPosition="4225" endWordPosition="4226">er instance of a phenomenon that had already been discussed for some time under the term presupposition. The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: • Factive verbs: Mary forgot/didn’t forget that John had left. • Factive adjectives: It is/isn’t odd that the room is closed. • Change-of-state verbs: John stopped/hasn’t stopped smoking. • Verbs of judging: John criticized/didn’t criticize Harry for writing the letter. • Wh-questions: Who is coming for dinner? • Headless relatives: Chicago is/isn’t where Fred met Sally. • Cleft sentences: It was/wasn’t John who caught the thief. • Pseudo-clefts: What she wants/doe</context>
</contexts>
<marker>Fillmore, 1971</marker>
<rawString>Fillmore, Charles. 1971. Verbs of judging: An exercise in semantic description. In Charles J. Fillmore and Terence Langendoen, editors, Studies in Linguistic Semantics. Holt, Rinehart and Winston, Inc., New York, NY, pages 273–289.</rawString>
</citation>
<citation valid="false">
<title>Uber Sinn und Bedeutung. Zeitschrift f¨ur Philosophie und Philosophische Kritik,</title>
<booktitle>English translation: “On Sense and Meaning,” in</booktitle>
<pages>25--50</pages>
<editor>Brian McGuiness, editor, Frege: Collected Works. Basil Blackwell,</editor>
<location>Oxford,</location>
<marker></marker>
<rawString>Frege, Gottlob. 1892. ¨Uber Sinn und Bedeutung. Zeitschrift f¨ur Philosophie und Philosophische Kritik, pages 25–50. English translation: “On Sense and Meaning,” in Brian McGuiness, editor, Frege: Collected Works. Basil Blackwell, Oxford, pages 157–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Gajek</author>
<author>Hanno T Beck</author>
<author>Diane Elder</author>
<author>Greg Whittemore</author>
</authors>
<title>Lisp implementation.</title>
<date>1983</date>
<booktitle>Texas Linguistic Forum, Vol. 22. Department of Linguistics, The University of Texas at</booktitle>
<pages>187--202</pages>
<editor>In Mary Dalrymple, Edit Doron, John Goggin, Beverly Goodman, and John McCarthy, editors,</editor>
<location>Austin, Austin, TX,</location>
<contexts>
<context position="47448" citStr="Gajek et al. 1983" startWordPosition="7940" endWordPosition="7943">vel system and gave me a printout of the program to take along, a thick stack of Pascal code. Back home I unfolded the long printout on the floor of a corridor and spent quite a bit of time crawling up and down the code trying to understand what it did, and learning Pascal along the way. I was going to teach computational linguistics again in the spring. Having figured out Kimmo’s program, it occurred to me that doing a Lisp implementation of the two-level model would be a good class project. We completed the project and published a collection of papers on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platfor</context>
</contexts>
<marker>Gajek, Beck, Elder, Whittemore, 1983</marker>
<rawString>Gajek, Oliver, Hanno T. Beck, Diane Elder, and Greg Whittemore. 1983. Lisp implementation. In Mary Dalrymple, Edit Doron, John Goggin, Beverly Goodman, and John McCarthy, editors, Texas Linguistic Forum, Vol. 22. Department of Linguistics, The University of Texas at Austin, Austin, TX, pages 187–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Conventional implicature.</title>
<date>1979</date>
<booktitle>In Choon-Kyu Oh and</booktitle>
<pages>57--89</pages>
<editor>David A. Dinneen, editors,</editor>
<publisher>Presupposition. Academic Press,</publisher>
<location>New York, NY,</location>
<contexts>
<context position="27597" citStr="Gazdar (1979)" startWordPosition="4658" endWordPosition="4659">in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation </context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>Gazdar, Gerald. 1979. Conventional implicature. In Choon-Kyu Oh and David A. Dinneen, editors, Syntax and Semantics, Volume 11: Presupposition. Academic Press, New York, NY, pages 57–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Geis</author>
<author>Arnold Zwicky</author>
</authors>
<title>On invited inferences. Linguistic Inquiry,</title>
<date>1971</date>
<pages>2--561</pages>
<contexts>
<context position="21834" citStr="Geis and Zwicky 1971" startWordPosition="3722" endWordPosition="3725">ble 2 Some one-way implicative verbs and constructions. ++ implicatives +− implicatives −− implicatives −+ implicatives cause NP (to) prevent NP (from) can hesitate (to) force NP (to) preclude NP (from) be able (to) Table 2 contains examples of verbs and constructions that certainly yield an entailment in one direction but not necessarily the other way. For example, it is tempting to conclude from (9a) that the president attended the meeting—and if there is no reason to think otherwise, the reader is entitled to that conclusion. Nevertheless, the author may take away that “invited inference” (Geis and Zwicky 1971) without contradicting himself as in (9b). (9) a. The president was able to attend the meeting. b. The president was able to attend the meeting but decided not to. The entailments of constructions involving more than one implicative verb have to be computed from “top–down.” The two examples in (10) establish a stable discourse referent because they both entail that a picture was taken. (10) a. John managed not to forget to take a picture. b. Bill failed to prevent John from taking a picture. The early version of Kamp’s Discourse Representation Theory did not include any mechanism for computing</context>
</contexts>
<marker>Geis, Zwicky, 1971</marker>
<rawString>Geis, Michael and Arnold Zwicky. 1971. On invited inferences. Linguistic Inquiry, 2:561–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Geurts</author>
</authors>
<title>Presuppositions and Pronouns.</title>
<date>1999</date>
<publisher>Elsevier,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="27739" citStr="Geurts (1999)" startWordPosition="4680" endWordPosition="4681">equired. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulness. It is evident that no uniform theory can account for all the phenomena that historically have been lumped toge</context>
</contexts>
<marker>Geurts, 1999</marker>
<rawString>Geurts, Bart. 1999. Presuppositions and Pronouns. Elsevier, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ginzburg</author>
</authors>
<title>Questions, Queries and Facts: A Semantics and Pragmatics for Interrogatives.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<marker>Ginzburg, 1992</marker>
<rawString>Ginzburg, Jonathan. 1992. Questions, Queries and Facts: A Semantics and Pragmatics for Interrogatives. Ph.D. thesis, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ginzburg</author>
</authors>
<title>Interrogatives: Questions, facts, and dialogue.</title>
<date>1996</date>
<booktitle>Handbook of Contemporary Semantic Theory.</booktitle>
<pages>385--422</pages>
<editor>In Shalom Lappin, editor,</editor>
<publisher>Blackwell Publishers,</publisher>
<location>Oxford, UK,</location>
<marker>Ginzburg, 1996</marker>
<rawString>Ginzburg, Jonathan. 1996. Interrogatives: Questions, facts, and dialogue. In Shalom Lappin, editor, Handbook of Contemporary Semantic Theory. Blackwell Publishers, Oxford, UK, pages 385–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1979</date>
<pages>41--58</pages>
<editor>In P. Cole and J. L. Morgan, editors, Speech Acts.</editor>
<publisher>Academic Press,</publisher>
<location>New York, NY,</location>
<contexts>
<context position="30737" citStr="Grice (1979)" startWordPosition="5169" endWordPosition="5170">he new piece of information incontestable. (15) It was/wasn’t Barbara Partee who in a private conversation around 1980 suggested to me that anaphora resolution and the satisfaction of the presuppositions of definite descriptions was the same problem. In my joint last paper on presuppositions (Karttunen and Peters 1979), Stanley Peters and I proposed to do the sensible thing, namely to divide up the heterogeneous collection of phenomena that had been lumped together under this misbegotten label. We suggested that many cases that had been called presupposition are best seen as instances of what Grice (1979) had called conventional implicature. Conventional implicatures are propositions that the speaker or the author of the sentence is committed to by virtue of choosing particular words or constructions to express himself or herself. However, whether those implicatures are true or not does not have any bearing on whether the sentence is true or false. For example, because of the word even, (16) commits the author to the view that Bill is an unlikely person to agree with Mary. (16) Even Bill agrees with Mary. But the meaning contributed by even plays no role in determining the truth conditions of </context>
</contexts>
<marker>Grice, 1979</marker>
<rawString>Grice, H. Paul. 1979. Logic and conversation. In P. Cole and J. L. Morgan, editors, Speech Acts. Academic Press, New York, NY, pages 41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeroen Groenendijk</author>
<author>Martin Stokhof</author>
</authors>
<title>On the semantics of questions and the pragmantics of answers.</title>
<date>1984</date>
<booktitle>Varieties of Formal Semantics. Foris Publications, Dordrecht, The Netherlands,</booktitle>
<pages>143--170</pages>
<editor>In Fred Landman and Frank Veltman, editors,</editor>
<contexts>
<context position="35714" citStr="Groenendijk and Stokhof (1984)" startWordPosition="6009" endWordPosition="6012">e such a good semanticist.” The politely unstated premise was that I had fallen onto skid row. 6 There are two yet unexplained exceptions to this generalization. So-called “emotive factives” such as be surprised take embedded wh-questions but not whether-questions: You’d be surprised where you find us, *You’d be surprised whether you find us. “Dubitative” verbs such as doubt have the opposite characteristic. 7 Compared to the lively activity on the presupposition playground, the field of questions attracts few visitors. For later developments, see Hausser and Zaefferer (1979), Hausser (1983), Groenendijk and Stokhof (1984), and Ginsburg (1992, 1996). 453 Computational Linguistics Volume 33, Number 4 3. Interlude Towards the end of the 1970s I began to think that I had stumbled on, and helped to create and frame, more problems in semantics than I could ever solve. It was time to move on and leave the mess for others to clean up. In a bold move I signed up to teach a course on computational linguistics. As every professor knows, teaching a course on something you know next to nothing about is a great learning opportunity. To get some idea of how the field had developed in the previous ten years I went to the 1979</context>
</contexts>
<marker>Groenendijk, Stokhof, 1984</marker>
<rawString>Groenendijk, Jeroen and Martin Stokhof. 1984. On the semantics of questions and the pragmantics of answers. In Fred Landman and Frank Veltman, editors, Varieties of Formal Semantics. Foris Publications, Dordrecht, The Netherlands, pages 143–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L Hamblin</author>
</authors>
<date>1973</date>
<booktitle>Questions in Montague English. Foundations of Language,</booktitle>
<pages>10--41</pages>
<contexts>
<context position="33624" citStr="Hamblin (1973)" startWordPosition="5659" endWordPosition="5660">ng to both direct and embedded questions. For example, which girls date which boys should have the same meaning as a direct question that it has when embedded under a verb such as find out. Finally, whatever meaning we assign to interrogatives, it should help us to elucidate the meaning of question-embedding verbs including the examples in (18) and the one in (19) that sets up a relation between two questions. (19) Whether Mary comes to the party depends on who invites her. With these desiderata in mind, I came to the conclusion that the best solution would be to adopt an approach proposed by Hamblin (1973) for direct questions and carry it further. Hamblin’s idea was to let every direct question denote a set of propositions, namely, the set of propositions expressed by all the possible answers to the question. For example, under Hamblin’s analysis Is it raining? denotes the set containing two propositions {It is raining, It is not raining}. My improvement of that idea was to make the meaning of a question be a function that in each possible world picks up the set of true answers to the question. Under this new analysis it is possible to relate, for example, the meaning of know with a that-compl</context>
</contexts>
<marker>Hamblin, 1973</marker>
<rawString>Hamblin, Charles L. 1973. Questions in Montague English. Foundations of Language, 10:41–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randy Allen Harris</author>
</authors>
<title>The Linguistics Wars.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="13270" citStr="Harris 1995" startWordPosition="2279" endWordPosition="2280"> who said that the MIT Department was still looking for a one-year replacement for David Perlmutter who was going on a sabbatical. Would I be interested? Of course I was. I had come to the U.S. seven years earlier to study linguistics because of Noam Chomsky and now I had an office just across from hall from his. But during the year I was there, I lost interest in transformational syntax. I found Chomsky’s Thursday lectures of that year, on themes later published as Conditions on Transformations (Chomsky 1973), uncompelling. My sense of what was interesting had changed. The “Linguistic Wars” (Harris 1995) between generative (George Lakoff, John Ross, James D. McCawley, Paul Postal, and others) and interpretive semantics (Ray Jackendoff and others) had been won by Chomsky for the interpretivists, although Chomsky himself was, and still is, skeptical of any kind of formal theory of meaning. My sympathies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at UCLA that model theory and intensional logic was the right approach to semantics. But 446 Karttunen </context>
</contexts>
<marker>Harris, 1995</marker>
<rawString>Harris, Randy Allen. 1995. The Linguistics Wars. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Hausser</author>
</authors>
<title>The Syntax and Semantics of English Mood.</title>
<date>1983</date>
<pages>97--158</pages>
<editor>In Ferenc Kiefer, editor, Questions and Answers. Reidel,</editor>
<location>Dordrecht, The Netherlands,</location>
<contexts>
<context position="35682" citStr="Hausser (1983)" startWordPosition="6007" endWordPosition="6008">u Lauri? You were such a good semanticist.” The politely unstated premise was that I had fallen onto skid row. 6 There are two yet unexplained exceptions to this generalization. So-called “emotive factives” such as be surprised take embedded wh-questions but not whether-questions: You’d be surprised where you find us, *You’d be surprised whether you find us. “Dubitative” verbs such as doubt have the opposite characteristic. 7 Compared to the lively activity on the presupposition playground, the field of questions attracts few visitors. For later developments, see Hausser and Zaefferer (1979), Hausser (1983), Groenendijk and Stokhof (1984), and Ginsburg (1992, 1996). 453 Computational Linguistics Volume 33, Number 4 3. Interlude Towards the end of the 1970s I began to think that I had stumbled on, and helped to create and frame, more problems in semantics than I could ever solve. It was time to move on and leave the mess for others to clean up. In a bold move I signed up to teach a course on computational linguistics. As every professor knows, teaching a course on something you know next to nothing about is a great learning opportunity. To get some idea of how the field had developed in the previ</context>
</contexts>
<marker>Hausser, 1983</marker>
<rawString>Hausser, Roland. 1983. The Syntax and Semantics of English Mood. In Ferenc Kiefer, editor, Questions and Answers. Reidel, Dordrecht, The Netherlands, pages 97–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Hausser</author>
<author>Dietmar Zaefferer</author>
</authors>
<title>Questions and answers in a context dependent Montague grammar.</title>
<date>1979</date>
<booktitle>Formal Semantics and Pragmatics for Natural Languages.</booktitle>
<pages>339--358</pages>
<editor>In Siegfried Josef Schmidt and Franz Guenthner, editors,</editor>
<location>Reidel, Dordrecht, The Netherlands,</location>
<contexts>
<context position="35666" citStr="Hausser and Zaefferer (1979)" startWordPosition="6003" endWordPosition="6006">t asking, “What happened to you Lauri? You were such a good semanticist.” The politely unstated premise was that I had fallen onto skid row. 6 There are two yet unexplained exceptions to this generalization. So-called “emotive factives” such as be surprised take embedded wh-questions but not whether-questions: You’d be surprised where you find us, *You’d be surprised whether you find us. “Dubitative” verbs such as doubt have the opposite characteristic. 7 Compared to the lively activity on the presupposition playground, the field of questions attracts few visitors. For later developments, see Hausser and Zaefferer (1979), Hausser (1983), Groenendijk and Stokhof (1984), and Ginsburg (1992, 1996). 453 Computational Linguistics Volume 33, Number 4 3. Interlude Towards the end of the 1970s I began to think that I had stumbled on, and helped to create and frame, more problems in semantics than I could ever solve. It was time to move on and leave the mess for others to clean up. In a bold move I signed up to teach a course on computational linguistics. As every professor knows, teaching a course on something you know next to nothing about is a great learning opportunity. To get some idea of how the field had develo</context>
</contexts>
<marker>Hausser, Zaefferer, 1979</marker>
<rawString>Hausser, Roland and Dietmar Zaefferer. 1979. Questions and answers in a context dependent Montague grammar. In Siegfried Josef Schmidt and Franz Guenthner, editors, Formal Semantics and Pragmatics for Natural Languages. Reidel, Dordrecht, The Netherlands, pages 339–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Introduction to Computational Linguistics.</title>
<date>1967</date>
<publisher>Elsevier,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="7553" citStr="Hays 1967" startWordPosition="1278" endWordPosition="1279">s who could explain the joke, There are 10 kinds of linguists: those who know binary and those who don’t. I suspect that the data on my tapes for the Control Data 3600 computer have now been lost. We still have the data on manuscripts hundreds of years old but much of the content created in the first decades of the computer age is gone forever. Just as I was starting to work on my dissertation in 1967, I had the good fortune of getting a one-year fellowship at the RAND corporation in Santa Monica, California, in the group headed by David G. Hays, the author of the first textbook in our field (Hays 1967), and the founder of the Association for Machine Translation and Computational Linguistics (AMTCL, the predecessor of our ACL). The main focus of Hays’s team was Russian-to-English machine translation. Remarkably, Hays was also one of the authors of the infamous 1966 ALPAC report that inexorably caused the shutdown of all governmentally funded MT projects, including the one at RAND. Because the term machine translation had acquired a bad odor, the 1968 meeting of AMTCL dropped MT from its name and became ACL. At the 1970 meeting, the first one that I attended, people were still bitterly arguin</context>
</contexts>
<marker>Hays, 1967</marker>
<rawString>Hays, David G. 1967. Introduction to Computational Linguistics. Elsevier, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Heim</author>
</authors>
<title>The Semantics of Definite and Indefinite Noun Phrases.</title>
<date>1982</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="18696" citStr="Heim 1982" startWordPosition="3188" endWordPosition="3189"> looking for an innocent blondei. Shei must be 17 years old. There is another problem here. If we interpret an innocent blonde nonspecifically in (6), then must has a deontic reading. It is a requirement that she be 17 years old. However, on the specific reading must gets an epistemic interpretation. That is, we have made an inference about the age of the girl in question from her looks or other evidence. My work on discourse referents was a harbinger of the vast literature yet to come on this topic including Bonnie Webber’s 1978 dissertation (Webber 1978), Irene Heim’s file change semantics (Heim 1982), and the theory of discourse representation structures (DR(S) Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993). Looking back at my old paper, I am amused by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation An indefinite noun phrase creates a stable discourse referent just in case the clause it is bound to is implied to be true by the context in which it appears.4 That was the mai</context>
</contexts>
<marker>Heim, 1982</marker>
<rawString>Heim, Irene. 1982. The Semantics of Definite and Indefinite Noun Phrases. Ph.D. thesis, University of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Heim</author>
</authors>
<title>On the projection problem for presuppositions.</title>
<date>1983</date>
<booktitle>In West-Coast Conference on Formal Linguistics,</booktitle>
<volume>2</volume>
<pages>114--126</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="27639" citStr="Heim (1983)" startWordPosition="4664" endWordPosition="4665">t conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulness. It is evid</context>
</contexts>
<marker>Heim, 1983</marker>
<rawString>Heim, Irene. 1983. On the projection problem for presuppositions. In West-Coast Conference on Formal Linguistics, volume 2, pages 114–126, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Orvokki Hein¨am¨aki</author>
</authors>
<title>Semantics of English Temporal Connectives.</title>
<date>1974</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Texas,</institution>
<location>Austin, TX.</location>
<marker>Hein¨am¨aki, 1974</marker>
<rawString>Hein¨am¨aki, Orvokki. 1974. Semantics of English Temporal Connectives. Ph.D. thesis, University of Texas, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Douglas Johnson</author>
</authors>
<title>Formal Aspects of Phonological Description. Mouton, The Hague, The Netherlands.</title>
<date>1972</date>
<contexts>
<context position="40512" citStr="Johnson (1972)" startWordPosition="6821" endWordPosition="6822">hat all four of us were interested in morphology. I demoed a small system I had built with my students for Finnish (Karttunen, Uszkoreit, and Root 1981). Martin and Ron reported that they had recently made a breakthrough discovery in computing with rewrite rules. Kimmo went on to California to visit them at PARC to learn more. That was the beginning of our long collaboration. 4.1 Origins The discovery Kaplan and Kay had made was actually a rediscovery of a result that had been published a decade before in a book that none of us knew about at that time, a UC Berkeley dissertation by C. Douglas Johnson (1972). Johnson observed that although the same context-sensitive rule could be applied several times recursively to its own output, phonologists have always assumed implicitly that the site of application moves to the right or to the left in the string after each application. For example, if the rule α → β / γ δ is used to rewrite the string γαδ as γβδ, any subsequent application of the same rule must leave the β part unchanged, affecting only γ or δ. Johnson demonstrated that the effect of this constraint is that the pairs of inputs and outputs produced by a phonological rewrite rule can be modele</context>
<context position="60227" citStr="Johnson (1972)" startWordPosition="9951" endWordPosition="9952">pleteness of coverage, physical size, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms. My frustration is best summed up in a fable that I attached to my paper on a finite-state implementation of Gregory Stump’s realizational morphology (Stump 2001; Karttunen 2003). Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computational knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: Dear Princess. I have wonderful news for you: You are not like some of your NP-complete sisters. You are regular. You are rational. You are finite-state. Please marry me. Together we can do great things. And time after time, the put-down response from the Princess has been the same: Not interested. You do not understand Theory. Go away, you geek. Because the most suitable su</context>
</contexts>
<marker>Johnson, 1972</marker>
<rawString>Johnson, C. Douglas. 1972. Formal Aspects of Phonological Description. Mouton, The Hague, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Ralph M Weischedel</author>
</authors>
<title>Some frills for the modal tic-tac-toe of Davies and Isard: Semantics of predicate complement constructions.</title>
<date>1973</date>
<booktitle>In IJCAI,</booktitle>
<pages>352--355</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="22683" citStr="Joshi and Weischedel (1973)" startWordPosition="3860" endWordPosition="3863">mplicative verb have to be computed from “top–down.” The two examples in (10) establish a stable discourse referent because they both entail that a picture was taken. (10) a. John managed not to forget to take a picture. b. Bill failed to prevent John from taking a picture. The early version of Kamp’s Discourse Representation Theory did not include any mechanism for computing lexical entailments about existence. I found the DRS boxes disappointingly static at the time. The semantics of complementation that I proposed was picked up by some computational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can be computed directly by the parser, in contrast to the then prevailing view of the AI community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database systems (Kaplan 1977). 2.3 Presuppositions—Conventional Implicatures The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow 449 Com</context>
</contexts>
<marker>Joshi, Weischedel, 1973</marker>
<rawString>Joshi, Aravind K. and Ralph M. Weischedel. 1973. Some frills for the modal tic-tac-toe of Davies and Isard: Semantics of predicate complement constructions. In IJCAI, pages 352–355, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ren´e Kager</author>
</authors>
<title>Optimality Theory.</title>
<date>1999</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="58215" citStr="Kager 1999" startWordPosition="9638" endWordPosition="9639">aplan and Kay. The compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary alphabet. 460 Karttunen Word Play surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). But in the mid 1990s when most computational linguists working with the Xerox tools embraced the sequential model as the more practical approach, a two-level theory took over paper-and-pencil linguistics by storm in the guise of Optimality Theory (OT) (Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually all working phonologists switched into the OT paradigm. From my perspective OT is a two-level model where the ranking of the constraints plays the role that rule-ordering has in the sequential model. If one believes, as I do, that the mapping from lexical forms to inflected surface forms is basically a regular relation, then the choice between the two ways of decomposing it, either as a composed cascade of replace operations or as an intersection of parallel rules, has important practical consequences but it is not a deep theoretical divide. In f</context>
</contexts>
<marker>Kager, 1999</marker>
<rawString>Kager, Ren´e. 1999. Optimality Theory. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>Ev´enements, repr´esentation discursive et r´ef´erence temporelle.</title>
<date>1981</date>
<journal>Langages,</journal>
<pages>64--39</pages>
<contexts>
<context position="18795" citStr="Kamp (1981)" startWordPosition="3202" endWordPosition="3203">interpret an innocent blonde nonspecifically in (6), then must has a deontic reading. It is a requirement that she be 17 years old. However, on the specific reading must gets an epistemic interpretation. That is, we have made an inference about the age of the girl in question from her looks or other evidence. My work on discourse referents was a harbinger of the vast literature yet to come on this topic including Bonnie Webber’s 1978 dissertation (Webber 1978), Irene Heim’s file change semantics (Heim 1982), and the theory of discourse representation structures (DR(S) Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993). Looking back at my old paper, I am amused by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation An indefinite noun phrase creates a stable discourse referent just in case the clause it is bound to is implied to be true by the context in which it appears.4 That was the main idea in the 1969 paper. In the course of seeking evidence for this thesis, I came across an inter</context>
</contexts>
<marker>Kamp, 1981</marker>
<rawString>Kamp, Hans. 1981. Ev´enements, repr´esentation discursive et r´ef´erence temporelle. Langages, 64:39–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>The importance of presupposition.</title>
<date>2001</date>
<booktitle>Linguistic Form and its Computation. CSLI,</booktitle>
<pages>207--254</pages>
<editor>In Christian Rohrer, Antje Roßdeutscher, and Hans Kamp, editors,</editor>
<location>Stanford, CA,</location>
<contexts>
<context position="27756" citStr="Kamp (2001)" startWordPosition="4683" endWordPosition="4684">termine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulness. It is evident that no uniform theory can account for all the phenomena that historically have been lumped together under the la</context>
</contexts>
<marker>Kamp, 2001</marker>
<rawString>Kamp, Hans. 2001. The importance of presupposition. In Christian Rohrer, Antje Roßdeutscher, and Hans Kamp, editors, Linguistic Form and its Computation. CSLI, Stanford, CA, pages 207–254.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hans Kamp</author>
</authors>
<title>and Uwe Reyle.1993. From Discourse to Logic.</title>
<publisher>Kluwer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<marker>Kamp, </marker>
<rawString>Kamp, Hans and Uwe Reyle.1993. From Discourse to Logic. Kluwer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="57510" citStr="Kaplan and Kay (1994)" startWordPosition="9522" endWordPosition="9525">sequential Chomsky–Halle paradigm. Many arguments had been advanced in the phonological literature in the 1970s to show that phonological alternations could not be described or explained adequately without sequential rewrite rules. The idea of rules as constraints between a lexical symbol and its surface realization was seen as misguided. It went unnoticed that two-level rules could have the same effect as ordered rewrite rules because the realization of a lexical symbol could be constrained by the lexical side and/or by the 11 Our compilation algorithm was inspired by the landmark article of Kaplan and Kay (1994). We found a way to express the constraints on replacement using fewer auxiliary symbols than Kaplan and Kay. The compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary alphabet. 460 Karttunen Word Play surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). But in the mid 1990s when most computational linguists working with the Xerox tools embraced the sequential model as the more practical approach, a two-level theory took over paper-and-</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, Ronald M. and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jerrold Kaplan</author>
</authors>
<title>Cooperative Responses from a Natural Language Query System.</title>
<date>1977</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="23051" citStr="Kaplan 1977" startWordPosition="3918" endWordPosition="3919">cal entailments about existence. I found the DRS boxes disappointingly static at the time. The semantics of complementation that I proposed was picked up by some computational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can be computed directly by the parser, in contrast to the then prevailing view of the AI community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database systems (Kaplan 1977). 2.3 Presuppositions—Conventional Implicatures The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow 449 Computational Linguistics Volume 33, Number 4 that the construction manage to is empty of meaning. In general, if p entails q and ¬p entails ¬q, then it logically follows that p and q are equivalent: p ≡ q. (11) a. John managed to speak. h John spoke. b. John did not manage to speak. h John did not speak. But this is of course wrong as far as (11) is concerned. Choosin</context>
</contexts>
<marker>Kaplan, 1977</marker>
<rawString>Kaplan, S. Jerrold. 1977. Cooperative Responses from a Natural Language Query System. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<date>1969</date>
<booktitle>Problems of Reference in Syntax. Ph.D. thesis,</booktitle>
<institution>Indiana University,</institution>
<location>Bloomington, Indiana.</location>
<contexts>
<context position="9055" citStr="Karttunen 1969" startWordPosition="1528" endWordPosition="1529"> return some fifteen years later. Happy to become Martin’s student again, I learned Algol, an elegant new programming language, and got an understanding of the beauty of recursive algorithms. Martin was running an exciting weekly colloquium series. I teamed up with an intern by the name of Ronald Kaplan for a small study project and we gave a joint presentation about our findings. Ron and I agree that we did this together but neither one remembers what we said. It probably was about the similarities and differences between pronouns and logical variables, the topic of my first published paper (Karttunen 1969b). At the time the prevailing assumption was that symbolic logic provides an appropriate system for semantic representation within transformational grammar (McCawley 1970). But as I showed in the 1969 CSL paper, even cases as simple as (4a) and (4b) could not be treated adequately within the proposed framework. (4) a. The mani who loved hisi wifej kissed herj. b. I gave each student a cookiei. Some of them ate iti right away. c. The piloti who shot at itj hit the Migj that chased himi. The problem with (4a) is that the phrase hisi wife has to be treated in situ as it cannot be replaced by ano</context>
<context position="11264" citStr="Karttunen 1969" startWordPosition="1932" endWordPosition="1933">They looked like Haldeman and Ehrlichman, a pair of Nixon aides. But at least I found out that the problem had not been solved. As the year at RAND when on, I spent less and less time at the computer and a lot of time walking up and down the Santa Monica pier just down the cliff from my office thinking about pronouns, variables, reference, and definiteness. I went up to UCLA a few times to discuss these issues with Barbara Partee and gave a talk in her seminar. The topic of my dissertation was Problems of Reference in Syntax, in principle due before I left RAND but finished half-a-year later (Karttunen 1969a). By the summer of 1968 I had two job offers. David Hays was leaving RAND for SUNY in Buffalo and he offered me a job there. But I chose to become a Faculty Associate in the Linguistics Department at the University of Texas at Austin. Climate was one consideration, but, more importantly, Austin was where Emmon Bach and Stanley Peters were. My Indiana mentor, Bob Wall, had just moved into the same department. For the next ten years I had very little contact with Martin Kay and Ronald Kaplan but they became very important people in Act II of my life. 2. Act I: Framing Problems I started my car</context>
</contexts>
<marker>Karttunen, 1969</marker>
<rawString>Karttunen, Lauri. 1969a. Problems of Reference in Syntax. Ph.D. thesis, Indiana University, Bloomington, Indiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Pronouns and variables.</title>
<date>1969</date>
<booktitle>In CLS 5: Proceedings of the Fifth Regional Meeting,</booktitle>
<pages>108--116</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="9055" citStr="Karttunen 1969" startWordPosition="1528" endWordPosition="1529"> return some fifteen years later. Happy to become Martin’s student again, I learned Algol, an elegant new programming language, and got an understanding of the beauty of recursive algorithms. Martin was running an exciting weekly colloquium series. I teamed up with an intern by the name of Ronald Kaplan for a small study project and we gave a joint presentation about our findings. Ron and I agree that we did this together but neither one remembers what we said. It probably was about the similarities and differences between pronouns and logical variables, the topic of my first published paper (Karttunen 1969b). At the time the prevailing assumption was that symbolic logic provides an appropriate system for semantic representation within transformational grammar (McCawley 1970). But as I showed in the 1969 CSL paper, even cases as simple as (4a) and (4b) could not be treated adequately within the proposed framework. (4) a. The mani who loved hisi wifej kissed herj. b. I gave each student a cookiei. Some of them ate iti right away. c. The piloti who shot at itj hit the Migj that chased himi. The problem with (4a) is that the phrase hisi wife has to be treated in situ as it cannot be replaced by ano</context>
<context position="11264" citStr="Karttunen 1969" startWordPosition="1932" endWordPosition="1933">They looked like Haldeman and Ehrlichman, a pair of Nixon aides. But at least I found out that the problem had not been solved. As the year at RAND when on, I spent less and less time at the computer and a lot of time walking up and down the Santa Monica pier just down the cliff from my office thinking about pronouns, variables, reference, and definiteness. I went up to UCLA a few times to discuss these issues with Barbara Partee and gave a talk in her seminar. The topic of my dissertation was Problems of Reference in Syntax, in principle due before I left RAND but finished half-a-year later (Karttunen 1969a). By the summer of 1968 I had two job offers. David Hays was leaving RAND for SUNY in Buffalo and he offered me a job there. But I chose to become a Faculty Associate in the Linguistics Department at the University of Texas at Austin. Climate was one consideration, but, more importantly, Austin was where Emmon Bach and Stanley Peters were. My Indiana mentor, Bob Wall, had just moved into the same department. For the next ten years I had very little contact with Martin Kay and Ronald Kaplan but they became very important people in Act II of my life. 2. Act I: Framing Problems I started my car</context>
</contexts>
<marker>Karttunen, 1969</marker>
<rawString>Karttunen, Lauri. 1969b. Pronouns and variables. In CLS 5: Proceedings of the Fifth Regional Meeting, pages 108–116, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Implicative verbs.</title>
<date>1971</date>
<journal>Language,</journal>
<pages>47--340</pages>
<contexts>
<context position="2957" citStr="Karttunen 1971" startWordPosition="481" endWordPosition="482"> Road, Palo Alto, CA 94062, USA. E-mail: karttunen@parc.com. This article is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 (1) a. Bill has a cari. Iti/The cari is black. b. Bill doesn’t have a cari. *Iti/*The cari is black. 1 A year later in 1970, I gave my first ACL presentation at the 8th annual meeting in Columbus, Ohio. The title of the invited talk was The Logic of English Predicate Complement Constructions. It started off with the following declaration (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. The paper presented a classification of verbs and constructions that take sentential complements, that-clauses and infinitival complements, based on whether the sentence commits the auth</context>
<context position="19486" citStr="Karttunen 1971" startWordPosition="3323" endWordPosition="3324">ed by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation An indefinite noun phrase creates a stable discourse referent just in case the clause it is bound to is implied to be true by the context in which it appears.4 That was the main idea in the 1969 paper. In the course of seeking evidence for this thesis, I came across an interesting class of verbs and constructions that give rise to such implications (Karttunen 1971a, 1971b). For example, the contrast between (7a) and (7b) is explained by the semantic properties of the two verbs, manage and fail. (7) a. John managed to get a sabbaticali. Iti starts in September. b. John failed to get a sabbaticali. *Iti starts in September. (7a) entails that John got a sabbatical, (7b) entails that he didn’t. The interesting fact about these verbs is that when we change the polarity from positive to negative we still get an entailment, but of the opposite polarity as seen in (8). (8) a. John didn’t manage to get a sabbaticali. *Iti starts in September. b. John didn’t fai</context>
<context position="23200" citStr="Karttunen 1971" startWordPosition="3936" endWordPosition="3937">ked up by some computational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can be computed directly by the parser, in contrast to the then prevailing view of the AI community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database systems (Kaplan 1977). 2.3 Presuppositions—Conventional Implicatures The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow 449 Computational Linguistics Volume 33, Number 4 that the construction manage to is empty of meaning. In general, if p entails q and ¬p entails ¬q, then it logically follows that p and q are equivalent: p ≡ q. (11) a. John managed to speak. h John spoke. b. John did not manage to speak. h John did not speak. But this is of course wrong as far as (11) is concerned. Choosing the construction manage to commits the speaker to the view that there was some difficulty involved. All the verbs in Table 1 bring in some addition</context>
<context position="61823" citStr="Karttunen 1971" startWordPosition="10214" endWordPosition="10215">. 461 Computational Linguistics Volume 33, Number 4 If that is the correct analysis of the situation, computational linguists should adopt a different strategy. Instead of being the eternal rejected suitor at the Royal Court, they should adopt the role of the innocent boy in the street shouting The Princess has no clothes! The Princess has no clothes!... That was my conclusion in the 2003 paper. 5. Epilogue I am very happy to see that the topics I worked on at the very beginning of my career have finally become relevant in NLP. To quote again the opening paragraph of my 1970 ACL presentation (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. This 37-year-old prediction of semantics having practical value is becoming a reality in the context of automated question answering and reasoning initiatives such as the PASCAL Textual </context>
</contexts>
<marker>Karttunen, 1971</marker>
<rawString>Karttunen, Lauri. 1971a. Implicative verbs. Language, 47:340–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The logic of English predicate complement constructions.</title>
<date>1971</date>
<institution>The Indiana University Linguistics Club.</institution>
<location>Bloomington, Indiana.</location>
<contexts>
<context position="2957" citStr="Karttunen 1971" startWordPosition="481" endWordPosition="482"> Road, Palo Alto, CA 94062, USA. E-mail: karttunen@parc.com. This article is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 (1) a. Bill has a cari. Iti/The cari is black. b. Bill doesn’t have a cari. *Iti/*The cari is black. 1 A year later in 1970, I gave my first ACL presentation at the 8th annual meeting in Columbus, Ohio. The title of the invited talk was The Logic of English Predicate Complement Constructions. It started off with the following declaration (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. The paper presented a classification of verbs and constructions that take sentential complements, that-clauses and infinitival complements, based on whether the sentence commits the auth</context>
<context position="19486" citStr="Karttunen 1971" startWordPosition="3323" endWordPosition="3324">ed by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation An indefinite noun phrase creates a stable discourse referent just in case the clause it is bound to is implied to be true by the context in which it appears.4 That was the main idea in the 1969 paper. In the course of seeking evidence for this thesis, I came across an interesting class of verbs and constructions that give rise to such implications (Karttunen 1971a, 1971b). For example, the contrast between (7a) and (7b) is explained by the semantic properties of the two verbs, manage and fail. (7) a. John managed to get a sabbaticali. Iti starts in September. b. John failed to get a sabbaticali. *Iti starts in September. (7a) entails that John got a sabbatical, (7b) entails that he didn’t. The interesting fact about these verbs is that when we change the polarity from positive to negative we still get an entailment, but of the opposite polarity as seen in (8). (8) a. John didn’t manage to get a sabbaticali. *Iti starts in September. b. John didn’t fai</context>
<context position="23200" citStr="Karttunen 1971" startWordPosition="3936" endWordPosition="3937">ked up by some computational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can be computed directly by the parser, in contrast to the then prevailing view of the AI community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database systems (Kaplan 1977). 2.3 Presuppositions—Conventional Implicatures The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow 449 Computational Linguistics Volume 33, Number 4 that the construction manage to is empty of meaning. In general, if p entails q and ¬p entails ¬q, then it logically follows that p and q are equivalent: p ≡ q. (11) a. John managed to speak. h John spoke. b. John did not manage to speak. h John did not speak. But this is of course wrong as far as (11) is concerned. Choosing the construction manage to commits the speaker to the view that there was some difficulty involved. All the verbs in Table 1 bring in some addition</context>
<context position="61823" citStr="Karttunen 1971" startWordPosition="10214" endWordPosition="10215">. 461 Computational Linguistics Volume 33, Number 4 If that is the correct analysis of the situation, computational linguists should adopt a different strategy. Instead of being the eternal rejected suitor at the Royal Court, they should adopt the role of the innocent boy in the street shouting The Princess has no clothes! The Princess has no clothes!... That was my conclusion in the 2003 paper. 5. Epilogue I am very happy to see that the topics I worked on at the very beginning of my career have finally become relevant in NLP. To quote again the opening paragraph of my 1970 ACL presentation (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. This 37-year-old prediction of semantics having practical value is becoming a reality in the context of automated question answering and reasoning initiatives such as the PASCAL Textual </context>
</contexts>
<marker>Karttunen, 1971</marker>
<rawString>Karttunen, Lauri. 1971b. The logic of English predicate complement constructions. The Indiana University Linguistics Club. Bloomington, Indiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Presuppositions of compound sentences. Linguistic Inquiry,</title>
<date>1973</date>
<pages>4--167</pages>
<contexts>
<context position="27517" citStr="Karttunen (1973)" startWordPosition="4648" endWordPosition="4649">just his or her state of knowledge to incorporate the new information. The idea is in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It see</context>
</contexts>
<marker>Karttunen, 1973</marker>
<rawString>Karttunen, Lauri. 1973. Presuppositions of compound sentences. Linguistic Inquiry, 4:167–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Presupposition and linguistic context.</title>
<date>1974</date>
<journal>Theoretical Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="27002" citStr="Karttunen (1974" startWordPosition="4560" endWordPosition="4561">h the consequent clause of (13) by itself presupposes the existence of a unique king of France, (13) as a whole obviously does not. (13) If France has a king, I bet the king of France speaks only French. In a conditional sentence, a presupposition of the consequent clause can be “filtered” or “cancelled” away if it is entailed by the antecedent and general background knowledge. If a presupposition is not filtered locally and is not part of the context of the discourse, the reader or hearer must in some way adjust his or her state of knowledge to incorporate the new information. The idea is in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lew</context>
</contexts>
<marker>Karttunen, 1974</marker>
<rawString>Karttunen, Lauri. 1974. Presupposition and linguistic context. Theoretical Linguistics, 1(1):181–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Discourse referents.</title>
<date>1976</date>
<booktitle>Syntax and Semantics Volume 7, Notes from the Linguistic Underground.</booktitle>
<pages>363--385</pages>
<editor>In James D. McCawley, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York, NY,</location>
<contexts>
<context position="1260" citStr="Karttunen 1976" startWordPosition="187" endWordPosition="188">s and culminating in successful commercial applications in the 1990s. It offers some commentary on the relationship, or the lack thereof, between computational and paper-and-pencil linguistics. The final section returns to the semantic issues and their application to currently popular tasks such as textual inference and question answering. 1. Prologue Thirty-eight years ago, in the summer of 1969 at the second meeting of COLING in S˚anga-S¨aby in Sweden, I stood for the first time in front of a computational audience and started my talk on Discourse Referents by reading the following passage (Karttunen 1976): Consider a device designed to read a text in some natural language, interpret it, and store the content in some manner, say, for the purpose of being able to answer questions about it. To accomplish this task, the machine will have to fulfill at least the following basic requirement. It has to be able to build a file that consists of records of all individuals, that is, events, objects, etc., mentioned in the text and, for each individual, record whatever is said about it. Of course, for the time being at least, it seems that such a text interpreter is not a practical idea, but this should n</context>
</contexts>
<marker>Karttunen, 1976</marker>
<rawString>Karttunen, Lauri. 1976. Discourse referents. In James D. McCawley, editor, Syntax and Semantics Volume 7, Notes from the Linguistic Underground. Academic Press, New York, NY, pages 363–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Syntax and semantics of questions.</title>
<date>1977</date>
<journal>Linguistics and Philosophy,</journal>
<pages>1--1</pages>
<contexts>
<context position="31755" citStr="Karttunen 1977" startWordPosition="5336" endWordPosition="5337">the author to the view that Bill is an unlikely person to agree with Mary. (16) Even Bill agrees with Mary. But the meaning contributed by even plays no role in determining the truth conditions of the sentence. (16) is true if Bill agrees with Mary and false otherwise. Our good advice went unheeded for a long time but in recent work by Christopher Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley and I sketched out that separates conventional implicatures from truth-conditional aspects of meaning. 2.4 Syntax and Semantics of Questions My paper on questions (Karttunen 1977) was an ambitious effort to give a unified account in the framework of Montague Grammar of the meaning of all types of interrogative phrases including direct questions such as the examples in (17) and embedded interrogatives illustrated in (18). (17) a. Is it raining? b. Do you want to go or do you want to stay? c. Which book did Mary read? d. Which girls date which boys? 452 Karttunen Word Play (18) a. John knows whether Bill smokes. b. Mary is thinking about whether to stay home or go to the movies. c. Bill remembers to whom John gave the book? d. She doesn’t care about who did what to whom.</context>
</contexts>
<marker>Karttunen, 1977</marker>
<rawString>Karttunen, Lauri. 1977. Syntax and semantics of questions. Linguistics and Philosophy, 1:1–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>KIMMO: A general morphological processor.</title>
<date>1983</date>
<booktitle>Texas Linguistic Forum,</booktitle>
<volume>22</volume>
<pages>165--186</pages>
<editor>In Mary Dalrymple, Edit Doron, John Goggin, Beverley Goodman, and John McCarthy, editors,</editor>
<institution>Department of Linguistics, The University of Texas at Austin,</institution>
<location>Austin, TX,</location>
<contexts>
<context position="47465" citStr="Karttunen 1983" startWordPosition="7944" endWordPosition="7945"> me a printout of the program to take along, a thick stack of Pascal code. Back home I unfolded the long printout on the floor of a corridor and spent quite a bit of time crawling up and down the code trying to understand what it did, and learning Pascal along the way. I was going to teach computational linguistics again in the spring. Having figured out Kimmo’s program, it occurred to me that doing a Lisp implementation of the two-level model would be a good class project. We completed the project and published a collection of papers on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), </context>
</contexts>
<marker>Karttunen, 1983</marker>
<rawString>Karttunen, Lauri. 1983. KIMMO: A general morphological processor. In Mary Dalrymple, Edit Doron, John Goggin, Beverley Goodman, and John McCarthy, editors, Texas Linguistic Forum, volume 22. Department of Linguistics, The University of Texas at Austin, Austin, TX, pages 165–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Features and values.</title>
<date>1984</date>
<booktitle>In COLING’84,</booktitle>
<pages>28--33</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="38287" citStr="Karttunen 1984" startWordPosition="6453" endWordPosition="6454">ad an excellent mix of computational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, Robert Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there, among others. SRI’s AI Center and Xerox PARC were cofounders of the new Center for the Study of Language and Information (CSLI) at Stanford, funded by a generous grant from the System Development Foundation, an offshoot of the RAND Corporation. At SRI, Stuart Shieber had designed and implemented his influential PATR II formalism for unification-based grammars (Shieber et al. 1983). I implemented it (Karttunen 1984; Karttunen and Kay 1985b; Karttunen 1986) at CSLI in Interlisp on a Xerox Dandelion, a wonderful machine with Interlisp as the language of the operating system. In 1987 I joined my friends at Xerox PARC to concentrate on my other computational interest: finite-state morphology. In making the crosstown transit from Menlo Park to Palo Alto, I graduated from my lovely Dandelion to the top-of-the-line Xerox Dorado, still the best computing experience in my life. After all the years spent on theorizing and playing with formalisms, I wanted to do something practical that would have an impact on the</context>
</contexts>
<marker>Karttunen, 1984</marker>
<rawString>Karttunen, Lauri. 1984. Features and values. In COLING’84, pages 28–33, July 2–6, Stanford, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lauri 1986 Karttunen</author>
</authors>
<title>D-PATR: A development environment for unification-based grammars.</title>
<booktitle>In COLING’86,</booktitle>
<pages>74--80</pages>
<location>Bonn, Germany.</location>
<marker>Karttunen, </marker>
<rawString>Karttunen, Lauri.1986. D-PATR: A development environment for unification-based grammars. In COLING’86, pages 74–80, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Finite-state constraints. In</title>
<date>1993</date>
<booktitle>The Last Phonological Rule.</booktitle>
<editor>John Goldsmith, editor,</editor>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="57923" citStr="Karttunen 1993" startWordPosition="9593" endWordPosition="9594">ewrite rules because the realization of a lexical symbol could be constrained by the lexical side and/or by the 11 Our compilation algorithm was inspired by the landmark article of Kaplan and Kay (1994). We found a way to express the constraints on replacement using fewer auxiliary symbols than Kaplan and Kay. The compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary alphabet. 460 Karttunen Word Play surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). But in the mid 1990s when most computational linguists working with the Xerox tools embraced the sequential model as the more practical approach, a two-level theory took over paper-and-pencil linguistics by storm in the guise of Optimality Theory (OT) (Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually all working phonologists switched into the OT paradigm. From my perspective OT is a two-level model where the ranking of the constraints plays the role that rule-ordering has in the sequential model. If one believes, as I do, that the mapping from lexical form</context>
<context position="59244" citStr="Karttunen 1993" startWordPosition="9806" endWordPosition="9807">posing it, either as a composed cascade of replace operations or as an intersection of parallel rules, has important practical consequences but it is not a deep theoretical divide. In fact, the two-level analyzer for French discussed in Karttunen, Kaplan, and Zaenen (1992) combined parallel rules with composition. It is unclear to me why my paper-and-pencil colleagues seem to think that it has to be absolutely one or the other. I have written several papers in the hope of getting my paper-and-pencil colleagues interested in, or at least aware of, what is happening in computational morphology (Karttunen 1993, 1998, 2003, 2006). I have not succeeded. Paper-and-pencil morphologists in general are not interested in creating complete descriptions for particular languages. They design formalisms for expressing generalizations about morphological phenomena commonly found in all natural languages. Practical issues that arise in the context of real-life applications such as completeness of coverage, physical size, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that h</context>
</contexts>
<marker>Karttunen, 1993</marker>
<rawString>Karttunen, Lauri. 1993. Finite-state constraints. In John Goldsmith, editor, The Last Phonological Rule. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Constructing lexical transducers.</title>
<date>1994</date>
<booktitle>In COLING’94,</booktitle>
<pages>406--411</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="53776" citStr="Karttunen (1994)" startWordPosition="8948" endWordPosition="8949">nguage as it compactly encodes • all the lemmas (lexical forms with morphological tags), • all the inflected surface forms, • all the mappings between lexical forms and surface forms. A comprehensive analyzer such as we built for English and French consists of tens of thousands of states and hundreds of thousands of arcs; but physically they can be quite small, a couple of megabytes in size. The same network can be applied in two ways: to provide an analysis for a surface form or to generate a surface from a lexical form in a tiny fraction of a second. Karttunen, Kaplan, and Zaenen (1992) and Karttunen (1994) are the first published reports on lexical transducers. In 1993 Xerox established a new European research center (XRCE) near Grenoble, France. Annie Zaenen and I went there to launch the Center’s research on natural 459 Computational Linguistics Volume 33, Number 4 language. We started with a couple of employees in an unfinished building with three empty floors, an elevator, and a pile of Sun workstations stacked at the entrance. Not knowing a word of French made it a hardship assignment for me, but in every other respect it was a lucky break. Because XRCE was a start-up as a research center </context>
</contexts>
<marker>Karttunen, 1994</marker>
<rawString>Karttunen, Lauri. 1994. Constructing lexical transducers. In COLING’94, pages 406–411, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The replace operator.</title>
<date>1995</date>
<booktitle>In ACL’95, cmp-lg/9504032.</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="55289" citStr="Karttunen 1995" startWordPosition="9193" endWordPosition="9194">es. Kenneth R. Beesley, who had worked for Microlytics, came to Grenoble to manage the development effort. I headed a small finite-state team of researchers and programmers charged with the mission of creating better development and run-time tools such as XFST (Xerox Finite State Tool) and LEXC (Lexicon Compiler). It became evident that large systems of two-level rules were difficult to debug. We concluded that lexical transducers are easier to construct with sequentially applied rules than with the parallel two-level rules. Andr´e Kempe and I therefore developed a compiler for replace rules (Karttunen 1995, 1996; Kempe and Karttunen 1996).11 The XFST regular expression language now includes a large set of different types of replace expressions: parallel replacement, replacement with multiple contexts, replacement with left-to-right or right-to-left shortest or longest match constraints, in addition to the usual finite-state operations union, intersection, composition, and negation. Ken Beesley and I managed to get permission from the XRCE management to release to researchers most of the tools that were developed in Grenoble for creating and applying finite-state networks, not just for morpholog</context>
</contexts>
<marker>Karttunen, 1995</marker>
<rawString>Karttunen, Lauri. 1995. The replace operator. In ACL’95, cmp-lg/9504032. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Directed replacement.</title>
<date>1996</date>
<booktitle>In ACL’96, cmp-lg/9606029.</booktitle>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="55322" citStr="Karttunen 1996" startWordPosition="9198" endWordPosition="9199">worked for Microlytics, came to Grenoble to manage the development effort. I headed a small finite-state team of researchers and programmers charged with the mission of creating better development and run-time tools such as XFST (Xerox Finite State Tool) and LEXC (Lexicon Compiler). It became evident that large systems of two-level rules were difficult to debug. We concluded that lexical transducers are easier to construct with sequentially applied rules than with the parallel two-level rules. Andr´e Kempe and I therefore developed a compiler for replace rules (Karttunen 1995, 1996; Kempe and Karttunen 1996).11 The XFST regular expression language now includes a large set of different types of replace expressions: parallel replacement, replacement with multiple contexts, replacement with left-to-right or right-to-left shortest or longest match constraints, in addition to the usual finite-state operations union, intersection, composition, and negation. Ken Beesley and I managed to get permission from the XRCE management to release to researchers most of the tools that were developed in Grenoble for creating and applying finite-state networks, not just for morphological analysis but also for other </context>
</contexts>
<marker>Karttunen, 1996</marker>
<rawString>Karttunen, Lauri. 1996. Directed replacement. In ACL’96, cmp-lg/9606029. Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The proper treatment of optimality in computational phonology.</title>
<date>1998</date>
<booktitle>In FSMNLP’98. International Workshop on Finite-State Methods in Natural Language Processing, cmp-lg/9804002.</booktitle>
<institution>Bilkent University,</institution>
<location>Ankara, Turkey.</location>
<marker>Karttunen, 1998</marker>
<rawString>Karttunen, Lauri. 1998. The proper treatment of optimality in computational phonology. In FSMNLP’98. International Workshop on Finite-State Methods in Natural Language Processing, cmp-lg/9804002. Bilkent University, Ankara, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Computing with realizational morphology.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>2588</volume>
<pages>205--216</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer Verlag,</publisher>
<location>Heidelberg, Germany,</location>
<contexts>
<context position="56066" citStr="Karttunen 2003" startWordPosition="9304" endWordPosition="9305">, replacement with multiple contexts, replacement with left-to-right or right-to-left shortest or longest match constraints, in addition to the usual finite-state operations union, intersection, composition, and negation. Ken Beesley and I managed to get permission from the XRCE management to release to researchers most of the tools that were developed in Grenoble for creating and applying finite-state networks, not just for morphological analysis but also for other useful NLP tasks such as tokenization and named-entity recognition. Ken and I wrote a book, Finite State Morphology (Beesley and Karttunen 2003), a pedagogical text that explains and documents the tools that come with the book. There have been many improvements in the software since then. A new edition of the book is in the making. By the time I left Grenoble to come back to PARC in 2001, Inxight, a Xerox spinoff company in California, was marketing finite-state morphological analyzers and stemmers for about three dozen languages. From a computational point of view morphology was a solved problem. 4.4 Computational vs. Paper-and-Pencil Morphology Historically, computational linguists and their “paper-and-pencil” counterparts in lingui</context>
<context position="60189" citStr="Karttunen 2003" startWordPosition="9945" endWordPosition="9946">t of real-life applications such as completeness of coverage, physical size, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms. My frustration is best summed up in a fable that I attached to my paper on a finite-state implementation of Gregory Stump’s realizational morphology (Stump 2001; Karttunen 2003). Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computational knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: Dear Princess. I have wonderful news for you: You are not like some of your NP-complete sisters. You are regular. You are rational. You are finite-state. Please marry me. Together we can do great things. And time after time, the put-down response from the Princess has been the same: Not interested. You do not understand Theory. Go away, </context>
</contexts>
<marker>Karttunen, 2003</marker>
<rawString>Karttunen, Lauri. 2003. Computing with realizational morphology. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 2588 of Lecture Notes in Computer Science. Springer Verlag, Heidelberg, Germany, pages 205–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The insufficiency of paper-and-pencil linguistics: The case of Finnish prosody.</title>
<date>2006</date>
<booktitle>Intelligent Linguistic Architectures,</booktitle>
<pages>287--300</pages>
<editor>In Miriam Butt, Mary Dalrymple, and Tracy Holloway King, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="62817" citStr="Karttunen (2006)" startWordPosition="10363" endWordPosition="10364">the semantics of natural languages. This 37-year-old prediction of semantics having practical value is becoming a reality in the context of automated question answering and reasoning initiatives such as the PASCAL Textual Entailment Challenge (Dagan, Glickman, and Magnini 2005) and the ARDA-sponsored AQUAINT project (Karttunen and Zaenen 2005; Zaenen, Karttunen, and Crouch 2005). The first computational implementation of textual inferences arising from the six types of implicative constructions in Tables 1 and 2, and their interaction with factive verbs, is presented in Nairn, Condoravdi, and Karttunen (2006). We may soon see search engines that actually make use of semantic processing in addition to simple string matching. The ability to draw textual inferences will significantly improve the quality of question answering and Web searches. From a linguistic perspective, this is an auspicious time to take a fresh look at issues such as the classification of complement constructions. The availability of search engines such as Google makes it possible to check the linguist’s semantic intuitions against actual usage. One question I have always had about the classification of implicative constructions </context>
</contexts>
<marker>Karttunen, 2006</marker>
<rawString>Karttunen, Lauri. 2006. The insufficiency of paper-and-pencil linguistics: The case of Finnish prosody. In Miriam Butt, Mary Dalrymple, and Tracy Holloway King, editors, Intelligent Linguistic Architectures, pages 287–300. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Kenneth R Beesley</author>
</authors>
<title>Two-level rule compiler.</title>
<date>1992</date>
<tech>Technical Report ISTL-92-2,</tech>
<institution>Xerox Palo Alto Research Center,</institution>
<location>Palo Alto, CA.</location>
<contexts>
<context position="49966" citStr="Karttunen and Beesley 1992" startWordPosition="8335" endWordPosition="8338">nstraint such as “p must be followed by q” can be expressed as “it is not the case that something ending in p is not followed by something starting with q.” In Koskenniemi’s formalism, p =&gt; q. In the course of the summer, Kaplan and Koskenniemi worked out the basic compilation algorithm for two-level rules. The first two-level rule compiler was written in InterLisp by Koskenniemi and me in 1985–1987 using Kaplan’s implementation of the finite-state calculus (Koskenniemi 1986; Karttunen, Koskenniemi, and Kaplan 1987). The current C-version two-level compiler, called TWOLC, was created at PARC (Karttunen and Beesley 1992). It has extensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions. Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some of the languages for which there is a large-scale two-level grammar and a publication describing it: Finnish (Koskenniemi 1983), Estonian (Uibo 200</context>
</contexts>
<marker>Karttunen, Beesley, 1992</marker>
<rawString>Karttunen, Lauri and Kenneth R. Beesley. 1992. Two-level rule compiler. Technical Report ISTL-92-2, Xerox Palo Alto Research Center, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Ronald M Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Two-level morphology with composition.</title>
<date>1992</date>
<marker>Karttunen, Kaplan, Zaenen, 1992</marker>
<rawString>Karttunen, Lauri, Ronald M. Kaplan, and Annie Zaenen. 1992. Two-level morphology with composition.</rawString>
</citation>
<citation valid="false">
<booktitle>In COLING’92,</booktitle>
<pages>141--148</pages>
<location>Nantes, France.</location>
<marker></marker>
<rawString>In COLING’92, pages 141–148, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Martin Kay</author>
</authors>
<title>Parsing in a free word order language. In</title>
<date>1985</date>
<pages>279--306</pages>
<editor>David R. Dowty, Lauri Karttunen, and Arnold Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<contexts>
<context position="37530" citStr="Karttunen and Kay 1985" startWordPosition="6332" endWordPosition="6335">on and InterLisp. I got to compute on the Alto personal computer and even had my own personal 1 MB floppy for it, about the size of a large briefcase. On the floppy was the project Martin and I were collaborating on, a unification based parser/generator for Finnish. Finnish was a good test case for Martin’s functional unification grammar (FUG) formalism. In FUG, constituents could be labeled by a syntactic category such as NP and could be assigned functional roles such as CONTRAST and TOPIC. We showed how the constraints on Finnish word order could be described and implemented in those terms (Karttunen and Kay 1985a). I joined the Artificial Intelligence Center at SRI in 1984. It was a good time to make the move from academia to industrial research. SRI had an excellent mix of computational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, Robert Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there, among others. SRI’s AI Center and Xerox PARC were cofounders of the new Center for the Study of Language and Information (CSLI) at Stanford, funded by a generous grant from the System Development Foundation, an offshoot of the RAND Corporation. At SRI, Stuart </context>
</contexts>
<marker>Karttunen, Kay, 1985</marker>
<rawString>Karttunen, Lauri and Martin Kay. 1985a. Parsing in a free word order language. In David R. Dowty, Lauri Karttunen, and Arnold Zwicky, editors, Natural Language Parsing. Cambridge University Press, Cambridge, UK, pages 279–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Martin Kay</author>
</authors>
<title>Structure sharing with binary trees.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>133--136</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="37530" citStr="Karttunen and Kay 1985" startWordPosition="6332" endWordPosition="6335">on and InterLisp. I got to compute on the Alto personal computer and even had my own personal 1 MB floppy for it, about the size of a large briefcase. On the floppy was the project Martin and I were collaborating on, a unification based parser/generator for Finnish. Finnish was a good test case for Martin’s functional unification grammar (FUG) formalism. In FUG, constituents could be labeled by a syntactic category such as NP and could be assigned functional roles such as CONTRAST and TOPIC. We showed how the constraints on Finnish word order could be described and implemented in those terms (Karttunen and Kay 1985a). I joined the Artificial Intelligence Center at SRI in 1984. It was a good time to make the move from academia to industrial research. SRI had an excellent mix of computational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, Robert Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there, among others. SRI’s AI Center and Xerox PARC were cofounders of the new Center for the Study of Language and Information (CSLI) at Stanford, funded by a generous grant from the System Development Foundation, an offshoot of the RAND Corporation. At SRI, Stuart </context>
</contexts>
<marker>Karttunen, Kay, 1985</marker>
<rawString>Karttunen, Lauri and Martin Kay. 1985b. Structure sharing with binary trees. In Proceedings of the 23rd Meeting of the Association for Computational Linguistics, pages 133–136, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Kimmo Koskenniemi</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A compiler for two-level phonological rules.</title>
<date>1987</date>
<booktitle>Tools for Morphological Analysis. Center for the Study of Language and Information,</booktitle>
<pages>1--61</pages>
<editor>In Mary Dalrymple, Ronald Kaplan, Lauri Karttunen, Kimmo Koskenniemi, Sami Shaio, and Michael Wescoat, editors,</editor>
<location>Stanford University, Palo Alto, CA,</location>
<marker>Karttunen, Koskenniemi, Kaplan, 1987</marker>
<rawString>Karttunen, Lauri, Kimmo Koskenniemi, and Ronald M. Kaplan. 1987. A compiler for two-level phonological rules. In Mary Dalrymple, Ronald Kaplan, Lauri Karttunen, Kimmo Koskenniemi, Sami Shaio, and Michael Wescoat, editors, Tools for Morphological Analysis. Center for the Study of Language and Information, Stanford University, Palo Alto, CA, pages 1–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Stanley Peters</author>
</authors>
<title>Conventional implicature.</title>
<date>1979</date>
<booktitle>In Choon-Kyu Oh and</booktitle>
<pages>1--56</pages>
<editor>David A. Dinneen, editors,</editor>
<publisher>Presupposition. Academic Press,</publisher>
<location>New York, NY,</location>
<contexts>
<context position="27582" citStr="Karttunen and Peters (1979)" startWordPosition="4654" endWordPosition="4657">new information. The idea is in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and</context>
<context position="30445" citStr="Karttunen and Peters 1979" startWordPosition="5120" endWordPosition="5123"> is it-clefts. As Ellen Prince (1978) showed, a sentence such as (15) does not covertly slip into the discourse a piece of new information disguised as being old. On the contrary, the rhetorical force of the it-cleft is to tell you something that presumably you did not know before in a manner that makes the new piece of information incontestable. (15) It was/wasn’t Barbara Partee who in a private conversation around 1980 suggested to me that anaphora resolution and the satisfaction of the presuppositions of definite descriptions was the same problem. In my joint last paper on presuppositions (Karttunen and Peters 1979), Stanley Peters and I proposed to do the sensible thing, namely to divide up the heterogeneous collection of phenomena that had been lumped together under this misbegotten label. We suggested that many cases that had been called presupposition are best seen as instances of what Grice (1979) had called conventional implicature. Conventional implicatures are propositions that the speaker or the author of the sentence is committed to by virtue of choosing particular words or constructions to express himself or herself. However, whether those implicatures are true or not does not have any bearing</context>
</contexts>
<marker>Karttunen, Peters, 1979</marker>
<rawString>Karttunen, Lauri and Stanley Peters. 1979. Conventional implicature. In Choon-Kyu Oh and David A. Dinneen, editors, Syntax and Semantics, Volume 11: Presupposition. Academic Press, New York, NY, pages 1–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Hans Uszkoreit</author>
<author>Rebecca Root</author>
</authors>
<title>Morphological analysis of Finnish by computer.</title>
<date>1981</date>
<booktitle>In Proceedings of the 71st Annual Meeting of SASS,</booktitle>
<location>Albuquerque, NM.</location>
<marker>Karttunen, Uszkoreit, Root, 1981</marker>
<rawString>Karttunen, Lauri, Hans Uszkoreit, and Rebecca Root. 1981. Morphological analysis of Finnish by computer. In Proceedings of the 71st Annual Meeting of SASS, Albuquerque, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Annie Zaenen</author>
</authors>
<date>2005</date>
<booktitle>Annotating, Extracting and Reasoning about Time and Events, number 05151 in Dagstuhl Seminar Proceedings. Internationales Begegnungs-und Forschungszentrum (IBFI), Schloss Dagstuhl,</booktitle>
<pages>2005--314</pages>
<editor>Veridicity. In Graham Katz, James Pustejovsky, and Frank Schilder, editors,</editor>
<location>Germany, http://drops.dagstuhl.de/opus/</location>
<contexts>
<context position="62545" citStr="Karttunen and Zaenen 2005" startWordPosition="10322" endWordPosition="10325">t significance in any system of automatic data processing that depends on natural language. For this reason a systematic study of such relations, of which this paper is an example, will certainly have a great practical value, in addition to what it may contribute to the theory of the semantics of natural languages. This 37-year-old prediction of semantics having practical value is becoming a reality in the context of automated question answering and reasoning initiatives such as the PASCAL Textual Entailment Challenge (Dagan, Glickman, and Magnini 2005) and the ARDA-sponsored AQUAINT project (Karttunen and Zaenen 2005; Zaenen, Karttunen, and Crouch 2005). The first computational implementation of textual inferences arising from the six types of implicative constructions in Tables 1 and 2, and their interaction with factive verbs, is presented in Nairn, Condoravdi, and Karttunen (2006). We may soon see search engines that actually make use of semantic processing in addition to simple string matching. The ability to draw textual inferences will significantly improve the quality of question answering and Web searches. From a linguistic perspective, this is an auspicious time to take a fresh look at issues suc</context>
</contexts>
<marker>Karttunen, Zaenen, 2005</marker>
<rawString>Karttunen, Lauri and Annie Zaenen. 2005. Veridicity. In Graham Katz, James Pustejovsky, and Frank Schilder, editors, Annotating, Extracting and Reasoning about Time and Events, number 05151 in Dagstuhl Seminar Proceedings. Internationales Begegnungs-und Forschungszentrum (IBFI), Schloss Dagstuhl, Germany, http://drops.dagstuhl.de/opus/ volltexte/2005/314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward L Keenan</author>
</authors>
<title>Two kinds of presupposition in natural language.</title>
<date>1971</date>
<booktitle>Studies in Linguistic Semantics.</booktitle>
<pages>45--54</pages>
<editor>In Charles J. Fillmore and Terence Langendoen, editors,</editor>
<publisher>and Winston, Inc.,</publisher>
<location>Holt, Rinehart</location>
<contexts>
<context position="24872" citStr="Keenan 1971" startWordPosition="4227" endWordPosition="4228">a phenomenon that had already been discussed for some time under the term presupposition. The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: • Factive verbs: Mary forgot/didn’t forget that John had left. • Factive adjectives: It is/isn’t odd that the room is closed. • Change-of-state verbs: John stopped/hasn’t stopped smoking. • Verbs of judging: John criticized/didn’t criticize Harry for writing the letter. • Wh-questions: Who is coming for dinner? • Headless relatives: Chicago is/isn’t where Fred met Sally. • Cleft sentences: It was/wasn’t John who caught the thief. • Pseudo-clefts: What she wants/doesn’t want to </context>
</contexts>
<marker>Keenan, 1971</marker>
<rawString>Keenan, Edward L. 1971. Two kinds of presupposition in natural language. In Charles J. Fillmore and Terence Langendoen, editors, Studies in Linguistic Semantics. Holt, Rinehart and Winston, Inc., New York, NY, pages 45–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Kempe</author>
<author>Lauri Karttunen</author>
</authors>
<title>Parallel replacement in finite-state calculus.</title>
<date>1996</date>
<booktitle>In COLING’96, cmp-lg/9607007.</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="55322" citStr="Kempe and Karttunen 1996" startWordPosition="9196" endWordPosition="9199">, who had worked for Microlytics, came to Grenoble to manage the development effort. I headed a small finite-state team of researchers and programmers charged with the mission of creating better development and run-time tools such as XFST (Xerox Finite State Tool) and LEXC (Lexicon Compiler). It became evident that large systems of two-level rules were difficult to debug. We concluded that lexical transducers are easier to construct with sequentially applied rules than with the parallel two-level rules. Andr´e Kempe and I therefore developed a compiler for replace rules (Karttunen 1995, 1996; Kempe and Karttunen 1996).11 The XFST regular expression language now includes a large set of different types of replace expressions: parallel replacement, replacement with multiple contexts, replacement with left-to-right or right-to-left shortest or longest match constraints, in addition to the usual finite-state operations union, intersection, composition, and negation. Ken Beesley and I managed to get permission from the XRCE management to release to researchers most of the tools that were developed in Grenoble for creating and applying finite-state networks, not just for morphological analysis but also for other </context>
</contexts>
<marker>Kempe, Karttunen, 1996</marker>
<rawString>Kempe, Andr´e and Lauri Karttunen. 1996. Parallel replacement in finite-state calculus. In COLING’96, cmp-lg/9607007. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="false">
<pages>345--369</pages>
<editor>Kiparsky, Paul and Carol Kiparsky.1971. Fact. In D. Steinberg and L. Jakobovits, editors, Semantics. An Inderdisciplinary Reader,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker></marker>
<rawString>Kiparsky, Paul and Carol Kiparsky.1971. Fact. In D. Steinberg and L. Jakobovits, editors, Semantics. An Inderdisciplinary Reader, pages 345–369. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level morphology: A general computational model for word-form recognition and production.</title>
<date>1983</date>
<journal>Publication</journal>
<volume>11</volume>
<institution>University of Helsinki, Department of General Linguistics,</institution>
<location>Helsinki.</location>
<contexts>
<context position="44193" citStr="Koskenniemi (1983)" startWordPosition="7397" endWordPosition="7398"> involved. 4.2 Two-Level Morphology Back in Finland, Koskenniemi invented a new way to describe phonological alternations in finite-state terms. Instead of cascaded rules with intermediate stages and the computational problems they seemed to lead to, rules could be thought of as statements that directly constrain the surface realization of lexical strings. The rules would not be applied sequentially but in parallel. Each rule would constrain a certain lexical/surface correspondence and the environment in which the correspondence was allowed, required, or prohibited. For his 1983 dissertation, Koskenniemi (1983) constructed an ingenious implementation of his constraint-based model that did not depend on a rule compiler, composition, or any other finite-state algorithm, and he called it two-level morphology. Two-level morphology is based on three ideas: • Rules are symbol-to-symbol constraints that are applied in parallel, not sequentially like rewrite rules. • The constraints can refer to the lexical context, to the surface context, or to both contexts at the same time. • Lexical lookup and morphological analysis are performed in tandem. Applying the rules in parallel does not in itself solve the ove</context>
<context position="48183" citStr="Koskenniemi (1983)" startWordPosition="8053" endWordPosition="8054"> name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand. This is tedious in the extreme and very difficult for all but very simple rules. To address this problem Kaplan, Kay, and I pooled our CSLI funds and invited Koskenniemi to Stanford in the Summer of 1985. Although two-level rules are conceptually quite different from the rewrite rules studied </context>
<context position="50546" citStr="Koskenniemi 1983" startWordPosition="8427" endWordPosition="8428"> at PARC (Karttunen and Beesley 1992). It has extensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions. Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some of the languages for which there is a large-scale two-level grammar and a publication describing it: Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern S´ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994). 4.3 Lexical Transducers Soon after arriving at PARC I made a serendipitous discovery. At the time PARC was collaborating with Microlytics, a company that marketed spell-checkers, the first success story of finite-state morphology.10 Microlytics had licensed from Koskenniemi’s company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan’s compression rout</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, Kimmo. 1983. Two-level morphology: A general computational model for word-form recognition and production. Publication 11, University of Helsinki, Department of General Linguistics, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Compilation of automata from morphological two-level rules.</title>
<date>1986</date>
<booktitle>Papers from the Fifth Scandinavian Conference on Computational Linguistics,</booktitle>
<pages>143--149</pages>
<editor>In Fred Karlsson, editor,</editor>
<location>Helsinki, Finland.</location>
<contexts>
<context position="49818" citStr="Koskenniemi 1986" startWordPosition="8317" endWordPosition="8318"> boundaries. Another fundamental insight they had was the encoding of context restrictions in terms of double negation. For example, a constraint such as “p must be followed by q” can be expressed as “it is not the case that something ending in p is not followed by something starting with q.” In Koskenniemi’s formalism, p =&gt; q. In the course of the summer, Kaplan and Koskenniemi worked out the basic compilation algorithm for two-level rules. The first two-level rule compiler was written in InterLisp by Koskenniemi and me in 1985–1987 using Kaplan’s implementation of the finite-state calculus (Koskenniemi 1986; Karttunen, Koskenniemi, and Kaplan 1987). The current C-version two-level compiler, called TWOLC, was created at PARC (Karttunen and Beesley 1992). It has extensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions. Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some</context>
</contexts>
<marker>Koskenniemi, 1986</marker>
<rawString>Koskenniemi, Kimmo. 1986. Compilation of automata from morphological two-level rules. In Fred Karlsson, editor, Papers from the Fifth Scandinavian Conference on Computational Linguistics, pages 143–149, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Langendoen</author>
<author>Harris B Savin</author>
</authors>
<title>The projection problem for presuppositions.</title>
<date>1971</date>
<booktitle>Studies in Linguistic Semantics.</booktitle>
<pages>55--62</pages>
<editor>In Charles J. Fillmore and Terence Langendoen, editors,</editor>
<publisher>and Winston, Inc.,</publisher>
<location>Holt, Rinehart</location>
<contexts>
<context position="26051" citStr="Langendoen and Savin (1971)" startWordPosition="4401" endWordPosition="4404">• Pseudo-clefts: What she wants/doesn’t want to talk about is herself. • Temporal subordinate clauses: John left/didn’t leave after Mary called. • Iteratives: Fred called/didn’t call again. Fred ate/didn’t eat another turnip. In addition to vastly enlarging the presupposition population, the linguistic community also came up with a problem that had been ignored in the philosophical literature up to that point: Projection problem: How are the presuppositions of a complex sentence derived from the presuppositions of the component clauses? 450 Karttunen Word Play This question was first posed by Langendoen and Savin (1971). Their answer was (page 57): The projection principle for presuppositions, therefore, is as follows: presuppositions of a subordinate clause do not amalgamate either with presuppositions or assertions of higher clauses; rather they stand as presuppositions of the complex sentence in which they occur. They were badly mistaken. Although the consequent clause of (13) by itself presupposes the existence of a unique king of France, (13) as a whole obviously does not. (13) If France has a king, I bet the king of France speaks only French. In a conditional sentence, a presupposition of the consequen</context>
</contexts>
<marker>Langendoen, Savin, 1971</marker>
<rawString>Langendoen, Terence and Harris B. Savin. 1971. The projection problem for presuppositions. In Charles J. Fillmore and Terence Langendoen, editors, Studies in Linguistic Semantics. Holt, Rinehart and Winston, Inc., New York, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Scorekeeping in a language game.</title>
<date>1979</date>
<journal>Journal of Philosophical Logic,</journal>
<pages>8--339</pages>
<contexts>
<context position="27360" citStr="Lewis (1979)" startWordPosition="4625" endWordPosition="4626">ckground knowledge. If a presupposition is not filtered locally and is not part of the context of the discourse, the reader or hearer must in some way adjust his or her state of knowledge to incorporate the new information. The idea is in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the sp</context>
</contexts>
<marker>Lewis, 1979</marker>
<rawString>Lewis, David. 1979. Scorekeeping in a language game. Journal of Philosophical Logic, 8:339–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J McCarthy</author>
</authors>
<title>The Foundations of Optimality Theory.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="58231" citStr="McCarthy 2002" startWordPosition="9640" endWordPosition="9641">y. The compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary alphabet. 460 Karttunen Word Play surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). But in the mid 1990s when most computational linguists working with the Xerox tools embraced the sequential model as the more practical approach, a two-level theory took over paper-and-pencil linguistics by storm in the guise of Optimality Theory (OT) (Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually all working phonologists switched into the OT paradigm. From my perspective OT is a two-level model where the ranking of the constraints plays the role that rule-ordering has in the sequential model. If one believes, as I do, that the mapping from lexical forms to inflected surface forms is basically a regular relation, then the choice between the two ways of decomposing it, either as a composed cascade of replace operations or as an intersection of parallel rules, has important practical consequences but it is not a deep theoretical divide. In fact, the two-lev</context>
</contexts>
<marker>McCarthy, 2002</marker>
<rawString>McCarthy, John J. 2002. The Foundations of Optimality Theory. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James D McCawley</author>
</authors>
<title>Where do noun phrases come from?</title>
<date>1970</date>
<booktitle>Readings in English Transformational Grammar. Ginn and Company,</booktitle>
<pages>166--183</pages>
<editor>In Roderick A. Jacobs and Peter S. Rosenbaum, editors,</editor>
<location>Waltham, MA,</location>
<contexts>
<context position="9227" citStr="McCawley 1970" startWordPosition="1551" endWordPosition="1552">cursive algorithms. Martin was running an exciting weekly colloquium series. I teamed up with an intern by the name of Ronald Kaplan for a small study project and we gave a joint presentation about our findings. Ron and I agree that we did this together but neither one remembers what we said. It probably was about the similarities and differences between pronouns and logical variables, the topic of my first published paper (Karttunen 1969b). At the time the prevailing assumption was that symbolic logic provides an appropriate system for semantic representation within transformational grammar (McCawley 1970). But as I showed in the 1969 CSL paper, even cases as simple as (4a) and (4b) could not be treated adequately within the proposed framework. (4) a. The mani who loved hisi wifej kissed herj. b. I gave each student a cookiei. Some of them ate iti right away. c. The piloti who shot at itj hit the Migj that chased himi. The problem with (4a) is that the phrase hisi wife has to be treated in situ as it cannot be replaced by another coreferential noun phrase, say Mary, without changing the meaning. (4a) implies that only one man in some group of men loved his wife, which is not the same as there b</context>
</contexts>
<marker>McCawley, 1970</marker>
<rawString>McCawley, James D. 1970. Where do noun phrases come from? In Roderick A. Jacobs and Peter S. Rosenbaum, editors, Readings in English Transformational Grammar. Ginn and Company, Waltham, MA, pages 166–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>English as a formal language. In</title>
<date>1970</date>
<booktitle>Linguaggi nella Societ`a e nella Tecnica. Edizioni di Comunit`a,</booktitle>
<pages>189--224</pages>
<editor>B. Visentini et al., editors,</editor>
<location>Milan, Italy,</location>
<contexts>
<context position="14180" citStr="Montague 1970" startWordPosition="2426" endWordPosition="2427">athies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at UCLA that model theory and intensional logic was the right approach to semantics. But 446 Karttunen Word Play it was going to take a while before I could do anything original within that emerging paradigm. At MIT I gave a “formal methods” course for a few linguistic students starting with Bob Wall’s textbook (Wall 1972) and finishing with Montague Grammar that I was just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on my own topics: discourse referents, implicative verbs, and presuppositions. I had one star student in the seminar by the name of Mark Liberman, who wrote a Master’s Thesis poking holes in my emerging ideas about presuppositions. In the 1970s, the Linguistics Department in Austin was an excellent place for a young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excellent semantics and syntax students. David Dowty, Per-Kristian Halvorsen, Roland Hausser, </context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Montague, Richard. 1970a. English as a formal language. In B. Visentini et al., editors, Linguaggi nella Societ`a e nella Tecnica. Edizioni di Comunit`a, Milan, Italy, pages 189–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<date>1970</date>
<booktitle>Universal grammar. Theoria,</booktitle>
<pages>36--373</pages>
<contexts>
<context position="14180" citStr="Montague 1970" startWordPosition="2426" endWordPosition="2427">athies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at UCLA that model theory and intensional logic was the right approach to semantics. But 446 Karttunen Word Play it was going to take a while before I could do anything original within that emerging paradigm. At MIT I gave a “formal methods” course for a few linguistic students starting with Bob Wall’s textbook (Wall 1972) and finishing with Montague Grammar that I was just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on my own topics: discourse referents, implicative verbs, and presuppositions. I had one star student in the seminar by the name of Mark Liberman, who wrote a Master’s Thesis poking holes in my emerging ideas about presuppositions. In the 1970s, the Linguistics Department in Austin was an excellent place for a young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excellent semantics and syntax students. David Dowty, Per-Kristian Halvorsen, Roland Hausser, </context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Montague, Richard. 1970b. Universal grammar. Theoria, 36:373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>Approaches to Natural Language. Reidel, Dordrecht, The Netherlands,</booktitle>
<pages>221--242</pages>
<editor>In P. Suppes K. J. J. Hintikka, J. M. E. Moravcsik, editors,</editor>
<marker>Montague, 1973</marker>
<rawString>Montague, Richard. 1973. The proper treatment of quantification in ordinary English. In P. Suppes K. J. J. Hintikka, J. M. E. Moravcsik, editors, Approaches to Natural Language. Reidel, Dordrecht, The Netherlands, pages 221–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sjur Moshagen</author>
</authors>
<title>Pekka Sammallahti, and Trond Trosterud.</title>
<date>2006</date>
<booktitle>Inquiries into Words, Constraints and Contexts. CSLI,</booktitle>
<pages>94--105</pages>
<editor>In Antti Arppe, Lauri Carlson, Krister Lind´en, Jussi Piitulainen, Mickael Suominen, Martti Vainio, Hanna Westerlund, and Anssi Yli-Jyr¨a, editors,</editor>
<location>Stanford, CA,</location>
<marker>Moshagen, 2006</marker>
<rawString>Moshagen, Sjur, Pekka Sammallahti, and Trond Trosterud. 2006. Twol at work. In Antti Arppe, Lauri Carlson, Krister Lind´en, Jussi Piitulainen, Mickael Suominen, Martti Vainio, Hanna Westerlund, and Anssi Yli-Jyr¨a, editors, Inquiries into Words, Constraints and Contexts. CSLI, Stanford, CA, pages 94–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference.</title>
<date>2006</date>
<booktitle>In Inference in Computational Semantics (ICoS-5),</booktitle>
<location>Buxton, UK.</location>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In Inference in Computational Semantics (ICoS-5), Buxton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1994</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="50679" citStr="Oflazer 1994" startWordPosition="8444" endWordPosition="8445"> all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions. Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some of the languages for which there is a large-scale two-level grammar and a publication describing it: Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern S´ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994). 4.3 Lexical Transducers Soon after arriving at PARC I made a serendipitous discovery. At the time PARC was collaborating with Microlytics, a company that marketed spell-checkers, the first success story of finite-state morphology.10 Microlytics had licensed from Koskenniemi’s company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan’s compression routine to make a Finnish spell-checker in the Microlytics format. For that task I designed an algorithm that simultaneously carried out </context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>Oflazer, Kemal. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 9(2):137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Partee</author>
</authors>
<title>Montague grammar and transformational grammar. Linguistic Inquiry,</title>
<date>1995</date>
<pages>6--203</pages>
<contexts>
<context position="14208" citStr="Partee 1995" startWordPosition="2430" endWordPosition="2431">ide. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at UCLA that model theory and intensional logic was the right approach to semantics. But 446 Karttunen Word Play it was going to take a while before I could do anything original within that emerging paradigm. At MIT I gave a “formal methods” course for a few linguistic students starting with Bob Wall’s textbook (Wall 1972) and finishing with Montague Grammar that I was just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on my own topics: discourse referents, implicative verbs, and presuppositions. I had one star student in the seminar by the name of Mark Liberman, who wrote a Master’s Thesis poking holes in my emerging ideas about presuppositions. In the 1970s, the Linguistics Department in Austin was an excellent place for a young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excellent semantics and syntax students. David Dowty, Per-Kristian Halvorsen, Roland Hausser, Orvokki Hein¨am¨aki, Jim McC</context>
</contexts>
<marker>Partee, 1995</marker>
<rawString>Partee, Barbara. 1995. Montague grammar and transformational grammar. Linguistic Inquiry, 6:203–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>The Logic of Conventional Implicatures.</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="31502" citStr="Potts (2004)" startWordPosition="5299" endWordPosition="5300"> of choosing particular words or constructions to express himself or herself. However, whether those implicatures are true or not does not have any bearing on whether the sentence is true or false. For example, because of the word even, (16) commits the author to the view that Bill is an unlikely person to agree with Mary. (16) Even Bill agrees with Mary. But the meaning contributed by even plays no role in determining the truth conditions of the sentence. (16) is true if Bill agrees with Mary and false otherwise. Our good advice went unheeded for a long time but in recent work by Christopher Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley and I sketched out that separates conventional implicatures from truth-conditional aspects of meaning. 2.4 Syntax and Semantics of Questions My paper on questions (Karttunen 1977) was an ambitious effort to give a unified account in the framework of Montague Grammar of the meaning of all types of interrogative phrases including direct questions such as the examples in (17) and embedded interrogatives illustrated in (18). (17) a. Is it raining? b. Do you want to go or do you want to stay? c. Which book did Mary read? d. W</context>
</contexts>
<marker>Potts, 2004</marker>
<rawString>Potts, Christopher. 2004. The Logic of Conventional Implicatures. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality Theory: Constraint interaction in generative grammar.</title>
<date>1993</date>
<tech>RuCCS Technical Report 2.</tech>
<institution>Rutgers Center for Cognitive Science. Rutgers University,</institution>
<location>Piscataway, NJ.</location>
<contexts>
<context position="58203" citStr="Prince and Smolensky 1993" startWordPosition="9634" endWordPosition="9637">er auxiliary symbols than Kaplan and Kay. The compilation time and the size of the intermediate networks is very sensitive to the size of the auxiliary alphabet. 460 Karttunen Word Play surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). But in the mid 1990s when most computational linguists working with the Xerox tools embraced the sequential model as the more practical approach, a two-level theory took over paper-and-pencil linguistics by storm in the guise of Optimality Theory (OT) (Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually all working phonologists switched into the OT paradigm. From my perspective OT is a two-level model where the ranking of the constraints plays the role that rule-ordering has in the sequential model. If one believes, as I do, that the mapping from lexical forms to inflected surface forms is basically a regular relation, then the choice between the two ways of decomposing it, either as a composed cascade of replace operations or as an intersection of parallel rules, has important practical consequences but it is not a deep theoretical </context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Prince, Allan and Paul Smolensky. 1993. Optimality Theory: Constraint interaction in generative grammar. RuCCS Technical Report 2. Rutgers Center for Cognitive Science. Rutgers University, Piscataway, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Prince</author>
</authors>
<title>A comparison of Wh-clefts and it-clefts in discourse.</title>
<date>1978</date>
<journal>Language,</journal>
<pages>54--883</pages>
<contexts>
<context position="29856" citStr="Prince (1978)" startWordPosition="5026" endWordPosition="5027">by prefixes such as re- in verbs like recalculate and particles such as too and again. However, it does not seem applicable to the kinds of presuppositions triggered by implicative verbs or factives. But the whole idea of accommodation is inappropriate for implicative verbs. Examples such as (11) and (12) commit the speaker to the view that it was difficult for John to speak. The audience may take note of that piece of information but it does not need to be accepted or accommodated for the discourse to proceed. Another phenomenon that does not call for any accommodation is it-clefts. As Ellen Prince (1978) showed, a sentence such as (15) does not covertly slip into the discourse a piece of new information disguised as being old. On the contrary, the rhetorical force of the it-cleft is to tell you something that presumably you did not know before in a manner that makes the new piece of information incontestable. (15) It was/wasn’t Barbara Partee who in a private conversation around 1980 suggested to me that anaphora resolution and the satisfaction of the presuppositions of definite descriptions was the same problem. In my joint last paper on presuppositions (Karttunen and Peters 1979), Stanley P</context>
</contexts>
<marker>Prince, 1978</marker>
<rawString>Prince, Ellen. 1978. A comparison of Wh-clefts and it-clefts in discourse. Language, 54:883–906.</rawString>
</citation>
<citation valid="false">
<institution>Karttunen Word Play</institution>
<marker></marker>
<rawString>Karttunen Word Play</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Pulman</author>
</authors>
<title>Two level morphology. In</title>
<date>1991</date>
<booktitle>ET6/1 Rule Formalism and Virtual Machine Design Study. CEC, Luxembourg, chapter 5.</booktitle>
<editor>H. Alshawi, D. Arnold, R. Backofen, D. Carter, J. Lindop, K. Netter, S. Pulman, J. Tsujii, and H. Uskoreit, editors,</editor>
<contexts>
<context position="48063" citStr="Pulman 1991" startWordPosition="8036" endWordPosition="8037">arttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand. This is tedious in the extreme and very difficult for all but very simple rules. To address this problem Kaplan, Kay, and I pooled our CSLI funds and invited Koskenniemi to S</context>
</contexts>
<marker>Pulman, 1991</marker>
<rawString>Pulman, Stephen. 1991. Two level morphology. In H. Alshawi, D. Arnold, R. Backofen, D. Carter, J. Lindop, K. Netter, S. Pulman, J. Tsujii, and H. Uskoreit, editors, ET6/1 Rule Formalism and Virtual Machine Design Study. CEC, Luxembourg, chapter 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ritchie</author>
<author>A Black</author>
<author>S Pulman</author>
<author>G Russell</author>
</authors>
<title>The Edinburgh/ Cambridge morphological analyser and dictionary system (version 3.0) user manual.</title>
<date>1987</date>
<tech>Technical Report Software Paper No. 10,</tech>
<institution>Department of Artificial Intelligence, University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="47948" citStr="Ritchie et al. 1987" startWordPosition="8018" endWordPosition="8021">We completed the project and published a collection of papers on the topic, along with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is PC-KIMMO, a free C implementation from the Summer Institute of Linguistics (Antworth 1990). In Europe, two-level morphological analyzers became a standard component in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), SRI’s CLE Core Language Engine (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules. In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined 457 Computational Linguistics Volume 33, Number 4 but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand. This is tedious in the extreme and very difficult for all b</context>
</contexts>
<marker>Ritchie, Black, Pulman, Russell, 1987</marker>
<rawString>Ritchie, G., A. Black, S. Pulman, and G. Russell. 1987. The Edinburgh/ Cambridge morphological analyser and dictionary system (version 3.0) user manual. Technical Report Software Paper No. 10, Department of Artificial Intelligence, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ritchie</author>
<author>G Russell</author>
<author>A Black</author>
<author>S Pulman</author>
</authors>
<title>Computational Morphology: Practical Mechanisms for the English Lexicon.</title>
<date>1992</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Ritchie, Russell, Black, Pulman, 1992</marker>
<rawString>Ritchie, G., G. Russell, A. Black, and S. Pulman. 1992. Computational Morphology: Practical Mechanisms for the English Lexicon. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertrand Russell</author>
</authors>
<date>1905</date>
<booktitle>On denoting. Mind,</booktitle>
<pages>14--479</pages>
<contexts>
<context position="24542" citStr="Russell 1905" startWordPosition="4175" endWordPosition="4176"> the commitment remains the same regardless of whether the sentence is affirmative or negative. It is also present in questions and conditionals as shown in (12). (12) Did John manage to speak? If John managed to speak, it is a good sign. The extra bits of meaning attached to the two-way implicatives were yet another instance of a phenomenon that had already been discussed for some time under the term presupposition. The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: • Factive verbs: Mary forgot/didn’t forget that John had left. • Factive adjectives: It is/isn’t odd that the room is closed. • Change-of-state verbs: J</context>
</contexts>
<marker>Russell, 1905</marker>
<rawString>Russell, Bertrand. 1905. On denoting. Mind, 14:479–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertrand Russell</author>
</authors>
<date>1957</date>
<booktitle>Mr. Strawson on referring. Mind,</booktitle>
<pages>66--385</pages>
<contexts>
<context position="24571" citStr="Russell 1957" startWordPosition="4179" endWordPosition="4180">ame regardless of whether the sentence is affirmative or negative. It is also present in questions and conditionals as shown in (12). (12) Did John manage to speak? If John managed to speak, it is a good sign. The extra bits of meaning attached to the two-way implicatives were yet another instance of a phenomenon that had already been discussed for some time under the term presupposition. The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: • Factive verbs: Mary forgot/didn’t forget that John had left. • Factive adjectives: It is/isn’t odd that the room is closed. • Change-of-state verbs: John stopped/hasn’t stopped sm</context>
</contexts>
<marker>Russell, 1957</marker>
<rawString>Russell, Bertrand. 1957. Mr. Strawson on referring. Mind, 66:385–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
</authors>
<title>Deutsche Flexionsund Kompositionsmorphologie mit PC-KIMMO.</title>
<date>1996</date>
<booktitle>Linguistische Verifikation: Dokumentation zur Ersten Morpholympics 1994, number 34 in Sprache und Information,</booktitle>
<pages>37--52</pages>
<editor>In Roland Hausser, editor,</editor>
<publisher>Max Niemeyer Verlag, T¨ubingen.</publisher>
<contexts>
<context position="50592" citStr="Schiller 1996" startWordPosition="8433" endWordPosition="8434">ensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions. Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some of the languages for which there is a large-scale two-level grammar and a publication describing it: Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern S´ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994). 4.3 Lexical Transducers Soon after arriving at PARC I made a serendipitous discovery. At the time PARC was collaborating with Microlytics, a company that marketed spell-checkers, the first success story of finite-state morphology.10 Microlytics had licensed from Koskenniemi’s company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan’s compression routine to make a Finnish spell-checker in the Mic</context>
</contexts>
<marker>Schiller, 1996</marker>
<rawString>Schiller, Anne. 1996. Deutsche Flexionsund Kompositionsmorphologie mit PC-KIMMO. In Roland Hausser, editor, Linguistische Verifikation: Dokumentation zur Ersten Morpholympics 1994, number 34 in Sprache und Information, pages 37–52. Max Niemeyer Verlag, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel-Paul Sch¨utzenberger</author>
</authors>
<title>A remark on finite transducers.</title>
<date>1961</date>
<journal>Information and Control,</journal>
<pages>4--185</pages>
<marker>Sch¨utzenberger, 1961</marker>
<rawString>Sch¨utzenberger, Marcel-Paul. 1961. A remark on finite transducers. Information and Control, 4:185–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Hans Uszkoreit</author>
<author>Fernando Pereira</author>
<author>Jane Robinson</author>
<author>Mabry Tyson</author>
</authors>
<title>The formalism and implementation of PATR-II.</title>
<date>1983</date>
<booktitle>Research on Interactive Acquisition and Use of Knowledge. SRI International, Menlo Park, CA, techreport 4,</booktitle>
<pages>39--79</pages>
<editor>In Barbara J. Grosz and Mark Stickel, editors,</editor>
<contexts>
<context position="38253" citStr="Shieber et al. 1983" startWordPosition="6446" endWordPosition="6449">m academia to industrial research. SRI had an excellent mix of computational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, Robert Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there, among others. SRI’s AI Center and Xerox PARC were cofounders of the new Center for the Study of Language and Information (CSLI) at Stanford, funded by a generous grant from the System Development Foundation, an offshoot of the RAND Corporation. At SRI, Stuart Shieber had designed and implemented his influential PATR II formalism for unification-based grammars (Shieber et al. 1983). I implemented it (Karttunen 1984; Karttunen and Kay 1985b; Karttunen 1986) at CSLI in Interlisp on a Xerox Dandelion, a wonderful machine with Interlisp as the language of the operating system. In 1987 I joined my friends at Xerox PARC to concentrate on my other computational interest: finite-state morphology. In making the crosstown transit from Menlo Park to Palo Alto, I graduated from my lovely Dandelion to the top-of-the-line Xerox Dorado, still the best computing experience in my life. After all the years spent on theorizing and playing with formalisms, I wanted to do something practica</context>
</contexts>
<marker>Shieber, Uszkoreit, Pereira, Robinson, Tyson, 1983</marker>
<rawString>Shieber, Stuart, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Barbara J. Grosz and Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge. SRI International, Menlo Park, CA, techreport 4, pages 39–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soames</author>
</authors>
<title>How presuppositions are inherited: A solution to the projection problem. Linguistic Inquiry,</title>
<date>1982</date>
<pages>13--483</pages>
<contexts>
<context position="27626" citStr="Soames (1982)" startWordPosition="4662" endWordPosition="4663">: If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulnes</context>
</contexts>
<marker>Soames, 1982</marker>
<rawString>Soames, S. 1982. How presuppositions are inherited: A solution to the projection problem. Linguistic Inquiry, 13:483–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Stalnaker</author>
</authors>
<date>1973</date>
<journal>Presuppositions. The Journal of Philosophical Logic,</journal>
<pages>2--447</pages>
<contexts>
<context position="27535" citStr="Stalnaker (1973)" startWordPosition="4650" endWordPosition="4651">ate of knowledge to incorporate the new information. The idea is in Karttunen (1974, page 191): If the current conversational context does not suffice, the listener is entitled and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by n</context>
</contexts>
<marker>Stalnaker, 1973</marker>
<rawString>Stalnaker, Robert. 1973. Presuppositions. The Journal of Philosophical Logic, 2:447–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Strawson</author>
</authors>
<title>On referring.</title>
<date>1950</date>
<journal>Mind,</journal>
<pages>59--320</pages>
<contexts>
<context position="24557" citStr="Strawson 1950" startWordPosition="4177" endWordPosition="4178">t remains the same regardless of whether the sentence is affirmative or negative. It is also present in questions and conditionals as shown in (12). (12) Did John manage to speak? If John managed to speak, it is a good sign. The extra bits of meaning attached to the two-way implicatives were yet another instance of a phenomenon that had already been discussed for some time under the term presupposition. The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: • Factive verbs: Mary forgot/didn’t forget that John had left. • Factive adjectives: It is/isn’t odd that the room is closed. • Change-of-state verbs: John stopped/has</context>
</contexts>
<marker>Strawson, 1950</marker>
<rawString>Strawson, Peter. 1950. On referring. Mind, 59:320–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Strawson</author>
</authors>
<title>Identifying reference and truth values.</title>
<date>1964</date>
<tech>Theoria,</tech>
<pages>30--320</pages>
<contexts>
<context position="24587" citStr="Strawson 1964" startWordPosition="4181" endWordPosition="4182"> of whether the sentence is affirmative or negative. It is also present in questions and conditionals as shown in (12). (12) Did John manage to speak? If John managed to speak, it is a good sign. The extra bits of meaning attached to the two-way implicatives were yet another instance of a phenomenon that had already been discussed for some time under the term presupposition. The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964). When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples: • Factive verbs: Mary forgot/didn’t forget that John had left. • Factive adjectives: It is/isn’t odd that the room is closed. • Change-of-state verbs: John stopped/hasn’t stopped smoking. • Verbs o</context>
</contexts>
<marker>Strawson, 1964</marker>
<rawString>Strawson, Peter. 1964. Identifying reference and truth values. Theoria, 30:320–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory T Stump</author>
</authors>
<title>Inflectional Morphology. A Theory of Paradigm Structure.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="60172" citStr="Stump 2001" startWordPosition="9943" endWordPosition="9944">n the context of real-life applications such as completeness of coverage, physical size, and speed of applications are irrelevant from an academic morphologist’s point of view. The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms. My frustration is best summed up in a fable that I attached to my paper on a finite-state implementation of Gregory Stump’s realizational morphology (Stump 2001; Karttunen 2003). Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computational knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: Dear Princess. I have wonderful news for you: You are not like some of your NP-complete sisters. You are regular. You are rational. You are finite-state. Please marry me. Together we can do great things. And time after time, the put-down response from the Princess has been the same: Not interested. You do not understand </context>
</contexts>
<marker>Stump, 2001</marker>
<rawString>Stump, Gregory T. 2001. Inflectional Morphology. A Theory of Paradigm Structure. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heli Uibo</author>
</authors>
<title>Eesti keele morfoloogia modelleerimisest l˜oplike muundurite abil. [on modelling the estonian morphology by the means of finite-state transducers].</title>
<date>2006</date>
<booktitle>Keel ja arvuti, number 6 in Tartu ¨Ulikooli ¨uldkeeleteaduse ˜oppetooli toimetised.</booktitle>
<editor>In M. Koit, R. Pajusalu, and H. ˜Oim, editors,</editor>
<publisher>Max Niemeyer Verlag,</publisher>
<location>T¨ubingen, Germany.</location>
<contexts>
<context position="50568" citStr="Uibo 2006" startWordPosition="8430" endWordPosition="8431">ey 1992). It has extensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions. Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies such as Lingsoft and Inxight that are in the morphology business, and the descriptions have not been made public for obvious reasons. Here are some of the languages for which there is a large-scale two-level grammar and a publication describing it: Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern S´ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994). 4.3 Lexical Transducers Soon after arriving at PARC I made a serendipitous discovery. At the time PARC was collaborating with Microlytics, a company that marketed spell-checkers, the first success story of finite-state morphology.10 Microlytics had licensed from Koskenniemi’s company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan’s compression routine to make a Finnish </context>
</contexts>
<marker>Uibo, 2006</marker>
<rawString>Uibo, Heli. 2006. Eesti keele morfoloogia modelleerimisest l˜oplike muundurite abil. [on modelling the estonian morphology by the means of finite-state transducers]. In M. Koit, R. Pajusalu, and H. ˜Oim, editors, Keel ja arvuti, number 6 in Tartu ¨Ulikooli ¨uldkeeleteaduse ˜oppetooli toimetised. Max Niemeyer Verlag, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob van der Sandt</author>
</authors>
<title>Presupposition projection as anaphora resolution.</title>
<date>1992</date>
<journal>Journal of Semantics,</journal>
<pages>9--333</pages>
<marker>van der Sandt, 1992</marker>
<rawString>van der Sandt, Rob. 1992. Presupposition projection as anaphora resolution. Journal of Semantics, 9:333–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob A van der Sandt</author>
<author>Bart Geurts</author>
</authors>
<title>Presupposition, anaphora, and lexical content.</title>
<date>1991</date>
<booktitle>In Text Understanding in LiLOG, Integrating Computational Linguistics and Artificial Intelligence, Final Report on the IBM Germany LILOG-Project.</booktitle>
<pages>259--296</pages>
<publisher>Springer Verlag,</publisher>
<location>London, UK,</location>
<marker>van der Sandt, Geurts, 1991</marker>
<rawString>van der Sandt, Rob A. and Bart Geurts. 1991. Presupposition, anaphora, and lexical content. In Text Understanding in LiLOG, Integrating Computational Linguistics and Artificial Intelligence, Final Report on the IBM Germany LILOG-Project. Springer Verlag, London, UK, pages 259–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Linguistics and Philosophy.</title>
<date>1967</date>
<publisher>Cornell University Press,</publisher>
<location>Ithaca, NY.</location>
<contexts>
<context position="12353" citStr="Vendler (1967)" startWordPosition="2122" endWordPosition="2123">Kay and Ronald Kaplan but they became very important people in Act II of my life. 2. Act I: Framing Problems I started my career in Austin in the fall of 1968 and became a regular faculty member in 1970. My work on discourse referents was largely done when I arrived in Austin. I went on to study so-called implicative verbs such as manage and fail, a subtopic in the discourse referents paper, and branched to other types of verbs that take sentential complements. One important semantic class of verbs with sentential complements, called factives, had already been identified and discussed by Zeno Vendler (1967) and Paul and Carol Kiparsky (Kiparsky and Kiparsky 1971) at MIT. Factive verbs were said to presuppose that the complement clause is true. As it happened, I started to think about factives and presuppositions at MIT in the Fall of 1972. In the spring before I had a surprise phone call from Paul Kiparsky who said that the MIT Department was still looking for a one-year replacement for David Perlmutter who was going on a sabbatical. Would I be interested? Of course I was. I had come to the U.S. seven years earlier to study linguistics because of Noam Chomsky and now I had an office just across </context>
</contexts>
<marker>Vendler, 1967</marker>
<rawString>Vendler, Zeno. 1967. Linguistics and Philosophy. Cornell University Press, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Wall</author>
</authors>
<title>Introduction to Mathematical Linguistics.</title>
<date>1972</date>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="6260" citStr="Wall 1972" startWordPosition="1045" endWordPosition="1046">excellent teacher and mentor, Robert E. Wall. Bob Wall had participated in an early MT project at Harvard and in a project on automatic 1 The subscripts i and j are referential indices. Two noun phrases with the same referential index are supposed to be coreferential, that is, they should refer to the same object. 2 I am using the word imply as a generic cover term for entail, presuppose, and conventionally implicate. More about this in Sections 2.2 and 2.3. 444 Karttunen Word Play summarization at IBM. In his course, we learned formal language theory from notes that eventually became a book (Wall 1972), a bit of Fortran and COMIT, a language developed by Victor Yngve at MIT. I wrote a program on punched cards to randomly generate sentences from a small grammar of Finnish. Thanks to Bob, I was rescued from my indentured servitude in the Uralic and Altaic Studies. In my third and final year in Bloomington, I worked as a research assistant in the Computer Center with no specific duties other than to be a liaison to the Linguistics Department. My only accomplishment in that role was to save piles of anthropological data from obsolescence by writing a program to transform rolls of 5-channel pape</context>
<context position="14091" citStr="Wall 1972" startWordPosition="2412" endWordPosition="2413"> himself was, and still is, skeptical of any kind of formal theory of meaning. My sympathies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at UCLA that model theory and intensional logic was the right approach to semantics. But 446 Karttunen Word Play it was going to take a while before I could do anything original within that emerging paradigm. At MIT I gave a “formal methods” course for a few linguistic students starting with Bob Wall’s textbook (Wall 1972) and finishing with Montague Grammar that I was just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on my own topics: discourse referents, implicative verbs, and presuppositions. I had one star student in the seminar by the name of Mark Liberman, who wrote a Master’s Thesis poking holes in my emerging ideas about presuppositions. In the 1970s, the Linguistics Department in Austin was an excellent place for a young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excel</context>
</contexts>
<marker>Wall, 1972</marker>
<rawString>Wall, Robert E. 1972. Introduction to Mathematical Linguistics. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
</authors>
<title>A Formal Approach to Discourse Anaphora.</title>
<date>1978</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="18648" citStr="Webber 1978" startWordPosition="3181" endWordPosition="3182">pothetical context, as in (6). (6) The director is looking for an innocent blondei. Shei must be 17 years old. There is another problem here. If we interpret an innocent blonde nonspecifically in (6), then must has a deontic reading. It is a requirement that she be 17 years old. However, on the specific reading must gets an epistemic interpretation. That is, we have made an inference about the age of the girl in question from her looks or other evidence. My work on discourse referents was a harbinger of the vast literature yet to come on this topic including Bonnie Webber’s 1978 dissertation (Webber 1978), Irene Heim’s file change semantics (Heim 1982), and the theory of discourse representation structures (DR(S) Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993). Looking back at my old paper, I am amused by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation An indefinite noun phrase creates a stable discourse referent just in case the clause it is bound to is implied to be true by th</context>
</contexts>
<marker>Webber, 1978</marker>
<rawString>Webber, Bonnie L. 1978. A Formal Approach to Discourse Anaphora. Ph.D. thesis, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
</authors>
<title>Computation of a Unique Class of Inferences: Presupposition and Entailments.</title>
<date>1975</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="22740" citStr="Weischedel 1975" startWordPosition="3868" endWordPosition="3869">s in (10) establish a stable discourse referent because they both entail that a picture was taken. (10) a. John managed not to forget to take a picture. b. Bill failed to prevent John from taking a picture. The early version of Kamp’s Discourse Representation Theory did not include any mechanism for computing lexical entailments about existence. I found the DRS boxes disappointingly static at the time. The semantics of complementation that I proposed was picked up by some computational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph Weischedel’s Ph.D. dissertation (Weischedel 1975) showed that useful inferences can be computed directly by the parser, in contrast to the then prevailing view of the AI community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan’s work on “cooperative responses” in database systems (Kaplan 1977). 2.3 Presuppositions—Conventional Implicatures The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow 449 Computational Linguistics Volume 33, Number 4 that the const</context>
</contexts>
<marker>Weischedel, 1975</marker>
<rawString>Weischedel, Ralph. 1975. Computation of a Unique Class of Inferences: Presupposition and Entailments. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Zaenen</author>
<author>Lauri Karttunen</author>
<author>Richard Crouch</author>
</authors>
<title>Local textual inference: Can it be defined or circumscribed?</title>
<date>2005</date>
<booktitle>In Workshop on the Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>31--36</pages>
<location>Ann Arbor, MI.</location>
<marker>Zaenen, Karttunen, Crouch, 2005</marker>
<rawString>Zaenen, Annie, Lauri Karttunen, and Richard Crouch. 2005. Local textual inference: Can it be defined or circumscribed? In Workshop on the Empirical Modeling of Semantic Equivalence and Entailment, pages 31–36, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Zeevat</author>
</authors>
<title>Presupposition and accommodation in update semantics.</title>
<date>1992</date>
<journal>Journal of Semantics,</journal>
<pages>9--379</pages>
<contexts>
<context position="27709" citStr="Zeevat (1992)" startWordPosition="4676" endWordPosition="4677">and expected to extend it as required. He must determine for himself what context he is supposed to be in on the basis of what is said and, if he is willing to go along with it, make the same tacit extension that his interlocutor appears to have made. Lewis (1979) called this process accommodation. There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows: An especially stark illustration of the disparity of the field, at least in its early days, is the work of a Karttunen, who within the span of six years published three theories that were mutually inconsistent, technically as well as conceptually. I don’t disagree with that assessment.5 It seems to me that by now the notions of presupposition projection and accommodation have outlived their usefulness. It is evident that no uniform theory can account for all the phenomena that hist</context>
</contexts>
<marker>Zeevat, 1992</marker>
<rawString>Zeevat, Henk. 1992. Presupposition and accommodation in update semantics. Journal of Semantics, 9:379–412.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>