<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.96688">
Towards History-based Grammars:
Using Richer Models for Probabilistic Parsing*
</title>
<author confidence="0.706813666666667">
Ezra Black Fred Jelinek John Lafferty David M. Magerman
Robert Mercer Salim Roukos
IBM T. J. Watson Research Center
</author>
<sectionHeader confidence="0.980434" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998247368421">
We describe a generative probabilistic model of
natural language, which we call HBG, that takes
advantage of detailed linguistic information to re-
solve ambiguity. HBG incorporates lexical, syn-
tactic, semantic, and structural information from
the parse tree into the disambiguation process in a
novel way. We use a corpus of bracketed sentences,
called a Treebank, in combination with decision
tree building to tease out the relevant aspects of a
parse tree that will determine the correct parse of
a sentence. This stands in contrast to the usual ap-
proach of further grammar tailoring via the usual
linguistic introspection in the hope of generating
the correct parse. In head-to-head tests against
one of the best existing robust probabilistic pars-
ing models, which we call P-CFG, the HBG model
significantly outperforms P-CFG, increasing the
parsing accuracy rate from 60% to 75%, a 37%
reduction in error.
</bodyText>
<subsectionHeader confidence="0.508907">
Intro duction
</subsectionHeader>
<bodyText confidence="0.993411277777778">
Almost any natural language sentence is ambigu-
ous in structure, reference, or nuance of mean-
ing. Humans overcome these apparent ambigu-
ities by examining the context of the sentence.
But what exactly is context? Frequently, the cor-
rect interpretation is apparent from the words or
constituents immediately surrounding the phrase
in question. This observation begs the following
question: How much information about the con-
text of a sentence or phrase is necessary and suffi-
cient to determine its meaning? This question is at
the crux of the debate among computational lin-
guists about the application and implementation
of statistical methods in natural language under-
standing.
Previous work on disambiguation and proba-
bilistic parsing has offered partial answers to this
question. Hidden Markov models of words and
</bodyText>
<footnote confidence="0.3873505">
*Thanks to Philip Resnik and Stanley Chen for
their valued input.
</footnote>
<bodyText confidence="0.999105">
their tags, introduced in (5) and (5) and pop-
ularized in the natural language community by
Church (5), demonstrate the power of short-term
n-gram statistics to deal with lexical ambiguity.
Hindle and Rooth (5) use a statistical measure
of lexical associations to resolve structural am-
biguities. Brent (5) acquires likely verb subcat-
egorization patterns using the frequencies of verb-
object-preposition triples. Magerman and Mar-
cus (5) propose a model of context that combines
the n-gram model with information from dominat-
ing constituents. All of these aspects of context
are necessary for disambiguation, yet none is suf-
ficient.
We propose a probabilistic model of context
for disambiguation in parsing, HBG, which incor-
porates the intuitions of these previous works into
one unified framework. Let p(T, w) be the joint
probability of generating the word string w7 and
the parse tree T. Given /42, our parser chooses as
its parse tree that tree T* for which
</bodyText>
<equation confidence="0.979386">
T* = arg maxp(T,w7) (1)
TEP(tor.)
</equation>
<bodyText confidence="0.999890384615384">
where P(7.4) is the set of all parses produced by
the grammar for the sentence w7. Many aspects of
the input sentence that might be relevant to the
decision-making process participate in the prob-
abilistic model, providing a very rich if not the
richest model of context ever attempted in a prob-
abilistic parsing model.
In this paper, we will motivate and define the
HBG model, describe the task domain, give an
overview of the grammar, describe the proposed
HBG model, and present the results of experi-
ments comparing HBG with an existing state-of-
the-art model.
</bodyText>
<subsectionHeader confidence="0.802903">
Motivation for History-based
Grammars
</subsectionHeader>
<bodyText confidence="0.9965225">
One goal of a parser is to produce a grammatical
interpretation of a sentence which represents the
</bodyText>
<page confidence="0.999784">
31
</page>
<bodyText confidence="0.999919111111111">
syntactic and semantic intent of the sentence. To
achieve this goal, the parser must have a mecha-
nism for estimating the coherence of an interpreta-
tion, both in isolation and in context. Probabilis-
tic language models provide such a mechanism.
A probabilistic language model attempts
to estimate the probability of a sequence
of sentences and their respective interpreta-
tions (parse trees) occurring in the language,
</bodyText>
<equation confidence="0.889935">
P(Si S2 T2 • • • Sn Tn ) •
</equation>
<bodyText confidence="0.999983060606061">
The difficulty in applying probabilistic mod-
els to natural language is deciding what aspects
of the sentence and the discourse are relevant to
the model. Most previous probabilistic models of
parsing assume the probabilities of sentences in a
discourse are independent of other sentences. In
fact, previous works have made much stronger in-
dependence assumptions. The P-CFG model con-
siders the probability of each constituent rule in-
dependent of all other constituents in the sen-
tence. The Pearl (5) model includes a slightly
richer model of context, allowing the probability
of a constituent rule to depend upon the immedi-
ate parent of the rule and a part-of-speech trigram
from the input sentence. But none of these mod-
els come close to incorporating enough context to
disambiguate many cases of ambiguity.
A significant reason researchers have limited
the contextual information used by their mod-
els is because of the difficulty in estimating very
rich probabilistic models of context. In this work,
we present a model, the history-based grammar
model, which incorporates a very rich model of
context, and we describe a technique for estimat-
ing the parameters for this model using decision
trees. The history-based grammar model provides
a mechanism for taking advantage of contextual
information from anywhere in the discourse his-
tory. Using decision tree technology, any question
which can be asked of the history (i.e. Is the sub-
ject of the previous sentence animate? Was the
previous sentence a question? etc.) can be incor-
porated into the language model.
</bodyText>
<subsectionHeader confidence="0.985011">
The History-based Grammar Model
</subsectionHeader>
<bodyText confidence="0.999783285714286">
The history-based grammar model defines context
of a parse tree in terms of the leftmost derivation
of the tree.
Following (5), we show in Figure 1 a context-
free grammar (CFG) for ab n and the parse tree
for the sentence aabb. The leftmost derivation of
the tree T in Figure 1 is:
</bodyText>
<equation confidence="0.7126245">
S ASB aSB aAB B 14 aaBB aabB 7) aabb
(2)
</equation>
<bodyText confidence="0.9347635">
where the rule used to expand the i-th node of
the tree is denoted by 7%. Note that we have in-
</bodyText>
<figure confidence="0.99936775">
S ASBIAB
A a
B —+ b
a a b b
</figure>
<figureCaption confidence="0.999991">
Figure 1: Grammar and parse tree for aabb.
</figureCaption>
<bodyText confidence="0.980902466666667">
dexed the non-terminal (NT) nodes of the tree
with this leftmost order. We denote by 2 the sen-
tential form obtained just before we expand node
i. Hence, t.; corresponds to the sentential form
aSB or equivalently to the string 7.17.2. In a left-
most derivation we produce the words in left-to-
right order.
Using the one-to-one correspondence between
leftmost derivations and parse trees, we can
rewrite the joint probability in (1) as:
p(T, w7) = 11p(ri
In a probabilistic context-free grammar (P-CFG),
the probability of an expansion at node i depends
only on the identity of the non-terminal Ni, i.e.,
p(ri ) = p(ri). Thus
</bodyText>
<equation confidence="0.918093666666667">
rri
p(T, 7.4) = 11 p(ri)
t=1
</equation>
<bodyText confidence="0.998642692307692">
So in P-CFG the derivation order does not affect
the probabilistic model&apos;.
A less crude approximation than the usual P-
CFG is to use a decision tree to determine which
aspects of the leftmost derivation have a bear-
ing on the probability of how node i will be ex-
panded. In other words, the probability distribu-
tion p(r2Itfl will be modeled by p(rilE[t.n) where
E[t] is the equivalence class of the history t as
determined by the decision tree. This allows our
&apos;Note the abuse of notation since we denote by
p(7%) the conditional probability of rewriting the non-
terminal Ni.
</bodyText>
<page confidence="0.996063">
32
</page>
<bodyText confidence="0.999961647058824">
probabilistic model to use any information any-
where in the partial derivation tree to determine
the probability of different expansions of the i-th
non-terminal. The use of decision trees and a large
bracketed corpus may shift some of the burden of
identifying the intended parse from the grammar-
ian to the statistical estimation methods. We refer
to probabilistic methods based on the derivation
as History-based Grammars (HBG).
In this paper, we explored a restricted imple-
mentation of this model in which only the path
from the current node to the root of the deriva-
tion along with the index of a branch (index of
the child of a parent ) are examined in the decision
tree model to build equivalence classes of histories.
Other parts of the subtree are not examined in the
implementation of HBG.
</bodyText>
<subsectionHeader confidence="0.904614">
Task Domain
</subsectionHeader>
<bodyText confidence="0.998527147058824">
We have chosen computer manuals as a task do-
main. We picked the most frequent 3000 words
in a corpus of 600,000 words from 10 manuals as
our vocabulary. We then extracted a few mil-
lion words of sentences that are completely cov-
ered by this vocabulary from 40,000,000 words of
computer manuals. A randomly chosen sentence
from a sample of 5000 sentences from this corpus
is:
396. It indicates whether a call completed suc-
cessfully or if some error was detected that
caused the call to fail.
To define what we mean by a correct parse,
we use a corpus of manually bracketed sentences
at the University of Lancaster called the Tree-
bank. The Treebank uses 17 non-terminal labels
and 240 tags. The bracketing of the above sen-
tence is shown in Figure 2.
A parse produced by the grammar is judged
to be correct if it agrees with the Treebank parse
structurally and the NT labels agree. The gram-
mar has a significantly richer NT label set (more
than 10000) than the Treebank but we have de-
fined an equivalence mapping between the gram-
mar NT labels and the Treebank NT labels. In
this paper, we do not include the tags in the mea-
sure of a correct parse.
We have used about 25,000 sentences to help
the grammarian develop the grammar with the
goal that the correct (as defined above) parse is
among the proposed (by the grammar) parses for
a sentence. Our most common test set consists of
1600 sentences that are never seen by the gram-
marian.
</bodyText>
<equation confidence="0.987809142857143">
[N It_PPH1 N]
[V indicates_VVZ
[Fn [Fn&amp;whether_CSW
[N a_AT1 call_NN1 N]
[V completed_VVD successfully_RR V]Fn8c]
or_CC
[Fn+ if_CSW
[N some_DD error_NN1 N]©
[V was_VBDZ detected_VVN V]
©[Fr that_CST
[V caused_VVD
[N the_AT call_NN1 N]
[Ti to_TO fail_VVI Ti]V]Fr]Fn-F]
Fn]V]._.
</equation>
<figureCaption confidence="0.9888595">
Figure 2: Sample bracketed sentence from Lan-
caster Treebank.
</figureCaption>
<subsectionHeader confidence="0.816465">
The Grammar
</subsectionHeader>
<bodyText confidence="0.9999364">
The grammar used in this experiment is a broad-
coverage, feature-based unification grammar. The
grammar is context-free but uses unification to ex-
press rule templates for the the context-free pro-
ductions. For example, the rule template:
</bodyText>
<equation confidence="0.891833">
i N P 1
I. :n I-4 [unspeciL:ni
Det irNi
</equation>
<bodyText confidence="0.999757">
corresponds to three CFG productions where the
second feature : n is either s, p, or : n. This rule
template may elicit up to 7 non-terminals. The
grammar has 21 features whose range of values
maybe from 2 to about 100 with a median of 8.
There are 672 rule templates of which 400 are ac-
tually exercised when we parse a corpus of 15,000
sentences. The number of productions that are
realized in this training corpus is several hundred
thousand.
</bodyText>
<subsectionHeader confidence="0.371719">
P-CFG
</subsectionHeader>
<bodyText confidence="0.9992525">
While a NT in the above grammar is a feature
vector, we group several NTs into one class we call
a mnemonic represented by the one NT that is
the least specified in that class. For example, the
mnemonic VBOPASTSG* corresponds to all NTs
that unify with:
</bodyText>
<equation confidence="0.996867">
[pos = v
v — type = be (4)
tense — aspect = past
</equation>
<bodyText confidence="0.999032285714286">
We use these mnemonics to label a parse tree
and we also use them to estimate a P-CFG, where
the probability of rewriting a NT is given by the
probability of rewriting the mnemonic. So from
a training set we induce a CFG from the actual
mnemonic productions that are elicited in pars-
ing the training corpus. Using the Inside-Outside
</bodyText>
<figure confidence="0.428959">
(3)
</figure>
<page confidence="0.986126">
33
</page>
<bodyText confidence="0.9999662">
algorithm, we can estimate P-CFG from a large
corpus of text. But since we also have a large
corpus of bracketed sentences, we can adapt the
Inside-Outside algorithm to reestimate the prob-
ability parameters subject to the constraint that
only parses consistent with the Treebank (where
consistency is as defined earlier) contribute to the
reestimation. From a training run of 15,000 sen-
tences we observed 87,704 mnemonic productions,
with 23,341 NT mnemonics of which 10,302 were
lexical. Running on a test set of 760 sentences 32%
of the rule templates were used, 7% of the lexi-
cal mnemonics, 10% of the constituent mnemon-
ics, and 5% of the mnemonic productions actually
contributed to parses of test sentences.
</bodyText>
<sectionHeader confidence="0.981823" genericHeader="keywords">
Grammar and Model Performance
Metrics
</sectionHeader>
<bodyText confidence="0.961070909090909">
To evaluate the performance of a grammar and an
accompanying model, we use two types of mea-
surements:
• the any-consistent rate, defined as the percent-
age of sentences for which the correct parse is
proposed among the many parses that the gram-
mar provides for a sentence. We also measure
the parse base, which is defined as the geomet-
ric mean of the number of proposed parses on a
per word basis, to quantify the ambiguity of the
grammar.
• the Viterbi rate defined as the percentage of sen-
tences for which the most likely parse is consis-
tent.
The any-consistent rate is a measure of the gram-
mar&apos;s coverage of linguistic phenomena. The
Viterbi rate evaluates the grammar&apos;s coverage
with the statistical model imposed on the gram-
mar. The goal of probabilistic modelling is to pro-
duce a Viterbi rate close to the any-consistent rate.
The any-consistent rate is 90% when we re-
quire the structure and the labels to agree and
96% when unlabeled bracketing is required. These
results are obtained on 760 sentences from 7 to 17
words long from test material that has never been
seen by the grammarian. The parse base is 1.35
parses/word. This translates to about 23 parses
for a 12-word sentence. The unlabeled Viterbi rate
stands at 64% and the labeled Viterbi rate is 60%.
While we believe that the above Viterbi rate
is close if not the state-of-the-art performance,
there is room for improvement by using a more re-
fined statistical model to achieve the labeled any-
consistent rate of 90% with this grammar. There
is a significant gap between the labeled Viterbi and
any-consistent rates: 30 percentage points.
Instead of the usual approach where a gram-
marian tries to fine tune the grammar in the hope
of improving the Viterbi rate we use the combina-
tion of a large Treebank and the resulting deriva-
tion histories with a decision tree building algo-
rithm to extract statistical parameters that would
improve the Viterbi rate. The grammarian&apos;s task
remains that of improving the any-consistent rate.
The history-based grammar model is distin-
guished from the context-free grammar model in
that each constituent structure depends not only
on the input string, but also the entire history up
to that point in the sentence. In HBGs, history
is interpreted as any element of the output struc-
ture, or the parse tree, which has already been de-
termined, including previous words, non-terminal
categories, constituent structure, and any other
linguistic information which is generated as part
of the parse structure.
</bodyText>
<sectionHeader confidence="0.554352" genericHeader="method">
The HBG Model
</sectionHeader>
<bodyText confidence="0.999805631578947">
Unlike P-CFG which assigns a probability to a
mnemonic production, the HBG model assigns a
probability to a rule template. Because of this the
HBG formulation allows one to handle any gram-
mar formalism that has a derivation process.
For the HBG model, we have defined about
50 syntactic categories, referred to as Syn, and
about 50 semantic categories, referred to as Sem.
Each NT (and therefore mnemonic) of the gram-
mar has been assigned a syntactic (Syn) and a
semantic (Sem) category. We also associate with
a non-terminal a primary lexical head, denoted by
Hi, and a secondary lexical head, denoted by H2 .2
When a rule is applied to a non-terminal, it indi-
cates which child will generate the lexical primary
head and which child will generate the secondary
lexical head.
The proposed generative model associates for
each constituent in the parse tree the probability:
</bodyText>
<equation confidence="0.756817">
P(SYn, Sem, R, H1, H2
Syn,, Sem,, Ii, Ipc,Hip, H21,)
</equation>
<bodyText confidence="0.9999103">
In HBG, we predict the syntactic and seman-
tic labels of a constituent, its rewrite rule, and its
two lexical heads using the labels of the parent
constituent, the parent&apos;s lexical heads, the par-
ent&apos;s rule Rp that lead to the constituent and
the constituent&apos;s index Ipc as a child of Rp. As
we discuss in a later section, we have also used
with success more information about the deriva-
tion tree than the immediate parent in condition-
ing the probability of expanding a constituent.
</bodyText>
<footnote confidence="0.998483833333333">
2The primary lexical head II1 corresponds
(roughly) to the linguistic notion of a lexical head.
The secondary lexical head 112 has no linguistic par-
allel. It merely represents a word in the constituent
besides the head which contains predictive information
about the constituent.
</footnote>
<page confidence="0.998272">
34
</page>
<bodyText confidence="0.999577">
We have approximated the above probability
by the following five factors:
</bodyText>
<listItem confidence="0.9933712">
1. p(Syfl IRp, /pc, Hip, Synp, SemP)
2. p(Sem ISyn, Rp, /pc, H1, H2p, Synp &amp;nip)
3. P(H ISyn, Sem, Rp /pc, H1p, H2p, Syn,, &amp;nip)
4. p(1/1 IR, Syn, Sem, FIT, Ipc, 1lip, H2p)
5. P(H2 IH1, R, Syn, Sem, Rp, /pc, Syn)
</listItem>
<bodyText confidence="0.999013">
While a different order for these predictions is pos-
sible, we only experimented with this one.
</bodyText>
<subsectionHeader confidence="0.991389">
Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999797025641026">
We only have built a decision tree to the rule prob-
ability component (3) of the model. For the mo-
ment, we are using n-gram models with the usual
deleted interpolation for smoothing for the other
four components of the model.
We have assigned bit strings to the syntactic
and semantic categories and to the rules manually.
Our intention is that bit strings differing in the
least significant bit positions correspond to cate-
gories of non-terminals or rules that are similar.
We also have assigned bitstrings for the words in
the vocabulary (the lexical heads) using automatic
clustering algorithms using the bigram mutual in-
formation clustering algorithm (see (5)). Given
the bitsting of a history, we then designed a deci-
sion tree for modeling the probability that a rule
will be used for rewriting a node in the parse tree.
Since the grammar produces parses which may
be more detailed than the Treebank, the decision
tree was built using a training set constructed in
the following manner. Using the grammar with
the P-CFG model we determined the most likely
parse that is consistent with the Treebank and
considered the resulting sentence-tree pair as an
event. Note that the grammar parse will also pro-
vide the lexical head structure of the parse. Then,
we extracted using leftmost derivation order tu-
ples of a history (truncated to the definition of a
history in the HBG model) and the corresponding
rule used in expanding a node. Using the resulting
data set we built a decision tree by classifying his-
tories to locally minimize the entropy of the rule
template.
With a training set of about 9000 sentence-
tree pairs, we had about 240,000 tuples and we
grew a tree with about 40,000 nodes. This re-
quired 18 hours on a 25 MIPS RISC-based ma-
chine and the resulting decision tree was nearly
100 megabytes.
</bodyText>
<subsectionHeader confidence="0.890337">
Immediate vs. Functional Parents
</subsectionHeader>
<bodyText confidence="0.998794">
The HBG model employs two types of parents, the
immediate parent and the functional parent. The
</bodyText>
<figure confidence="0.9642615">
a
list
</figure>
<figureCaption confidence="0.9932285">
Figure 3: Sample representation of &amp;quot;with a list&amp;quot;
in HBG model.
</figureCaption>
<figure confidence="0.9986486">
R: PP1
Syn: PP
Sem: With—Data
H1: list
with
R: NBAR4
Syn: NP
Sem: Data
H1: list
H2: a
R: N1
Syn: N
Sem: Data
H1: list
H2: *
</figure>
<page confidence="0.995709">
35
</page>
<bodyText confidence="0.999989321428571">
immediate parent is the constituent that immedi-
ately dominates the constituent being predicted.
If the immediate parent of a constituent has a dif-
ferent syntactic type from that of the constituent,
then the immediate parent is also the functional
parent; otherwise, the functional parent is the
functional parent of the immediate parent. The
distinction between functional parents and imme-
diate parents arises primarily to cope with unit
productions. When unit productions of the form
XP2 ---&gt; XP1 occur, the immediate parent of XP1
is XP2. But, in general, the constituent XP2 does
not contain enough useful information for ambi-
guity resolution. In particular, when considering
only immediate parents, unit rules such as NP2 —■
NP1 prevent the probabilistic model from allow-
ing the NP1 constituent to interact with the VP
rule which is the functional parent of NP1.
When the two parents are identical as it of-
ten happens, the duplicate information will be ig-
nored. However, when they differ, the decision
tree will select that parental context which best
resolves ambiguities.
Figure 3 shows an example of the represen-
tation of a history in HBG for the prepositional
phrase &amp;quot;with a list.&amp;quot; In this example, the imme-
diate parent of the Ni node is the NBAR4 node
and the functional parent of Ni is the PP1 node.
</bodyText>
<sectionHeader confidence="0.946208" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.999640666666667">
We compared the performance of HBG to the
&amp;quot;broad-coverage&amp;quot; probabilistic context-free gram-
mar, P-CFG. The any-consistent rate of the gram-
mar is 90% on test sentences of 7 to 17 words. The
Viterbi rate of P-CFG is 60% on the same test cor-
pus of 760 sentences used in our experiments. On
the same test sentences, the HBG model has a
Viterbi rate of 75%. This is a reduction of 37% in
error rate.
</bodyText>
<table confidence="0.936118">
Accuracy
P-CFG 59.8%
HBG 74.6%
Error Reduction 36.8%
</table>
<figureCaption confidence="0.998063">
Figure 4: Parsing accuracy: P-CFG vs. HBG
</figureCaption>
<bodyText confidence="0.999986214285714">
In developing HBG, we experimented with
similar models of varying complexity. One discov-
ery made during this experimentation is that mod-
els which incorporated more context than HBG
performed slightly worse than HBG. This suggests
that the current training corpus may not contain
enough sentences to estimate richer models. Based
on the results of these experiments, it appears
likely that significantly increasing the size of the
training corpus should result in a corresponding
improvement in the accuracy of HBG and richer
HBG-like models.
To check the value of the above detailed his-
tory, we tried the simpler model:
</bodyText>
<listItem confidence="0.8033578">
1. p(1/1 II-14, H2p, Rp, /pc)
2. p(H2
3. p(Syn
4. p(Sem ISyn, Hi, Rp, Ipc)
5. p(R ISyn, Sem, Hi, H2)
</listItem>
<bodyText confidence="0.999882166666667">
This model corresponds to a P-CFG with NTs
that are the crude syntax and semantic categories
annotated with the lexical heads. The Viterbi rate
in this case was 66%, a small improvement over the
P-CFG model indicating the value of using more
context from the derivation tree.
</bodyText>
<sectionHeader confidence="0.897928" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.999988076923077">
The success of the HBG model encourages fu-
ture development of general history-based gram-
mars as a more promising approach than the usual
P-CFG. More experimentation is needed with a
larger Treebank than was used in this study and
with different aspects of the derivation history. In
addition, this paper illustrates a new approach to
grammar development where the parsing problem
is divided (and hopefully conquered) into two sub-
problems: one of grammar coverage for the gram-
marian to address and the other of statistical mod-
eling to increase the probability of picking the cor-
rect parse of a sentence.
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99986215">
Baker, J. K., 1975. Stochastic Modeling for Au-
tomatic Speech Understanding. In Speech
Recognition, edited by Raj Reddy, Academic
Press, pp. 521-542.
Brent, M. R. 1991. Automatic Acquisition of Sub-
categorization Frames from Untagged Free-
text Corpora. In Proceedings of the 29th An-
nual Meeting of the Association for Computa-
tional Linguistics. Berkeley, California.
Brill, E., Magerman, D., Marcus, M., and San-
torini, B. 1990. Deducing Linguistic Structure
from the Statistics of Large Corpora. In Pro-
ceedings of the June 1990 DARPA Speech and
Natural Language Workshop. Hidden Valley,
Pennsylvania.
Brown, P. F., Della Pietra, V. J., deSouza, P. V.,
Lai, J. C., and Mercer, R. L. Class-based n-
gram Models of Natural Language. In Pro-
ceedings of the IBM Natural Language ITL,
March, 1990. Paris, France.
</reference>
<page confidence="0.974946">
36
</page>
<reference confidence="0.999623382352941">
Church, K. 1988. A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text. In
Proceedings of the Second Conference on Ap-
plied Natural Language Processing. Austin,
Texas.
Gale, W. A. and Church, K. 1990. Poor Estimates
of Context are Worse than None. In Proceed-
ings of the June 1990 DARPA Speech and
Natural Language Workshop. Hidden Valley,
Pennsylvania.
Harrison, M. A. 1978. Introduction to Formal
Language Theory. Addison-Wesley Publishing
Company.
Hindle, D. and Rooth, M. 1990. Structural Am-
biguity and Lexical Relations. In Proceedings
of the June 1990 DARPA Speech and Natural
Language Workshop. Hidden Valley, Pennsyl-
vania.
Jelinek, F. 1985. Self-organizing Language Model-
ing for Speech Recognition. IBM Report.
Magerman, D. M. and Marcus, M. P. 1991. Pearl:
A Probabilistic Chart Parser. In Proceedings
of the February 1991 DARPA Speech and Nat-
ural Language Workshop. Asilomar, Califor-
nia.
Derouault, A., and Merialdo, B., 1985. Probabilis-
tic Grammar for Phonetic to French Tran-
scription. ICASSP 85 Proceedings. Tampa,
Florida, pp. 1577-1580.
Sharman, R. A., Jelinek, F., and Mercer, R. 1990.
Generating a Grammar for Statistical Train-
ing. In Proceedings of the June 1990 DARPA
Speech and Natural Language Workshop. Hid-
den Valley, Pennsylvania.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.998645">Towards History-based Grammars: Using Richer Models for Probabilistic Parsing*</title>
<author confidence="0.953435333333333">Ezra Black Fred Jelinek John Lafferty David M Magerman Robert Mercer Salim Roukos T J Research Center</author>
<abstract confidence="0.998034739910314">We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error. Intro duction Almost any natural language sentence is ambiguous in structure, reference, or nuance of meaning. Humans overcome these apparent ambiguby examining the the sentence. what exactly Frequently, the correct interpretation is apparent from the words or constituents immediately surrounding the phrase in question. This observation begs the following question: How much information about the context of a sentence or phrase is necessary and sufficient to determine its meaning? This question is at the crux of the debate among computational linguists about the application and implementation of statistical methods in natural language understanding. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and *Thanks to Philip Resnik and Stanley Chen for their valued input. their tags, introduced in (5) and (5) and popularized in the natural language community by Church (5), demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth (5) use a statistical measure of lexical associations to resolve structural ambiguities. Brent (5) acquires likely verb subcategorization patterns using the frequencies of verbobject-preposition triples. Magerman and Marcus (5) propose a model of context that combines the n-gram model with information from dominating constituents. All of these aspects of context are necessary for disambiguation, yet none is sufficient. We propose a probabilistic model of context for disambiguation in parsing, HBG, which incorporates the intuitions of these previous works into unified framework. Let w) the joint probability of generating the word string w7 and parse tree our parser chooses as parse tree that tree which = maxp(T,w7) where P(7.4) is the set of all parses produced by the grammar for the sentence w7. Many aspects of the input sentence that might be relevant to the decision-making process participate in the probabilistic model, providing a very rich if not the richest model of context ever attempted in a probabilistic parsing model. In this paper, we will motivate and define the HBG model, describe the task domain, give an overview of the grammar, describe the proposed HBG model, and present the results of experiments comparing HBG with an existing state-ofthe-art model. Motivation for History-based Grammars One goal of a parser is to produce a grammatical interpretation of a sentence which represents the 31 syntactic and semantic intent of the sentence. To achieve this goal, the parser must have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. A probabilistic language model attempts to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, T2 • • Sn Tn ) • The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are independent of other sentences. In fact, previous works have made much stronger independence assumptions. The P-CFG model considers the probability of each constituent rule independent of all other constituents in the sentence. The Pearl (5) model includes a slightly richer model of context, allowing the probability of a constituent rule to depend upon the immediate parent of the rule and a part-of-speech trigram from the input sentence. But none of these models come close to incorporating enough context to disambiguate many cases of ambiguity. A significant reason researchers have limited the contextual information used by their models is because of the difficulty in estimating very rich probabilistic models of context. In this work, we present a model, the history-based grammar model, which incorporates a very rich model of context, and we describe a technique for estimating the parameters for this model using decision trees. The history-based grammar model provides a mechanism for taking advantage of contextual information from anywhere in the discourse history. Using decision tree technology, any question which can be asked of the history (i.e. Is the subject of the previous sentence animate? Was the previous sentence a question? etc.) can be incorporated into the language model. Grammar Model The history-based grammar model defines context of a parse tree in terms of the leftmost derivation of the tree. Following (5), we show in Figure 1 a contextgrammar (CFG) for n the parse tree the sentence leftmost derivation of tree Figure 1 ASB aSB aAB B aaBB aabB aabb (2) where the rule used to expand the i-th node of tree is denoted by 7%. Note that we have in- B —+ b a b 1: Grammar and parse tree for dexed the non-terminal (NT) nodes of the tree with this leftmost order. We denote by 2 the sentential form obtained just before we expand node Hence, corresponds to the sentential form equivalently to the string In a leftmost derivation we produce the words in left-toright order. Using the one-to-one correspondence between leftmost derivations and parse trees, we can rewrite the joint probability in (1) as: w7) In a probabilistic context-free grammar (P-CFG), the probability of an expansion at node i depends only on the identity of the non-terminal Ni, i.e., p(ri ) = p(ri). Thus rri 7.4) = t=1 So in P-CFG the derivation order does not affect the probabilistic model&apos;. A less crude approximation than the usual P- CFG is to use a decision tree to determine which aspects of the leftmost derivation have a bearing on the probability of how node i will be expanded. In other words, the probability distribup(r2Itflwill be modeled by p(rilE[t.n) where the equivalence class of the history t as determined by the decision tree. This allows our abuse of notation since we denote by p(7%) the conditional probability of rewriting the nonterminal Ni. 32 probabilistic model to use any information anywhere in the partial derivation tree to determine the probability of different expansions of the i-th non-terminal. The use of decision trees and a large bracketed corpus may shift some of the burden of identifying the intended parse from the grammarian to the statistical estimation methods. We refer to probabilistic methods based on the derivation as History-based Grammars (HBG). In this paper, we explored a restricted implementation of this model in which only the path from the current node to the root of the derivation along with the index of a branch (index of the child of a parent ) are examined in the decision tree model to build equivalence classes of histories. Other parts of the subtree are not examined in the implementation of HBG. Task Domain We have chosen computer manuals as a task domain. We picked the most frequent 3000 words in a corpus of 600,000 words from 10 manuals as our vocabulary. We then extracted a few million words of sentences that are completely covered by this vocabulary from 40,000,000 words of computer manuals. A randomly chosen sentence from a sample of 5000 sentences from this corpus is: indicates whether a call completed successfully or if some error was detected that caused the call to fail. To define what we mean by a correct parse, we use a corpus of manually bracketed sentences at the University of Lancaster called the Treebank. The Treebank uses 17 non-terminal labels and 240 tags. The bracketing of the above sentence is shown in Figure 2. A parse produced by the grammar is judged to be correct if it agrees with the Treebank parse structurally and the NT labels agree. The grammar has a significantly richer NT label set (more than 10000) than the Treebank but we have defined an equivalence mapping between the grammar NT labels and the Treebank NT labels. In this paper, we do not include the tags in the measure of a correct parse. We have used about 25,000 sentences to help the grammarian develop the grammar with the goal that the correct (as defined above) parse is the proposed (by the grammar) parses a sentence. Our most common test set consists of 1600 sentences that are never seen by the grammarian.</abstract>
<note confidence="0.951579933333333">[N It_PPH1 N] [V indicates_VVZ [Fn [Fn&amp;whether_CSW [N a_AT1 call_NN1 N] [V completed_VVD successfully_RR V]Fn8c] or_CC [Fn+ if_CSW [N some_DD error_NN1 N]© [V was_VBDZ detected_VVN V] ©[Fr that_CST [V caused_VVD [N the_AT call_NN1 N] [Ti to_TO fail_VVI Ti]V]Fr]Fn-F] Fn]V]._. Figure 2: Sample bracketed sentence from Lan-</note>
<abstract confidence="0.97791708">caster Treebank. The Grammar The grammar used in this experiment is a broadcoverage, feature-based unification grammar. The grammar is context-free but uses unification to express rule templates for the the context-free productions. For example, the rule template: iN P 1 :n [unspeciL:ni corresponds to three CFG productions where the feature : n is either : n. This rule template may elicit up to 7 non-terminals. The grammar has 21 features whose range of values maybe from 2 to about 100 with a median of 8. There are 672 rule templates of which 400 are actually exercised when we parse a corpus of 15,000 sentences. The number of productions that are realized in this training corpus is several hundred thousand. P-CFG While a NT in the above grammar is a feature vector, we group several NTs into one class we call by the one NT that is the least specified in that class. For example, the mnemonic VBOPASTSG* corresponds to all NTs that unify with: v — type = = past We use these mnemonics to label a parse tree and we also use them to estimate a P-CFG, where the probability of rewriting a NT is given by the probability of rewriting the mnemonic. So from a training set we induce a CFG from the actual mnemonic productions that are elicited in parsing the training corpus. Using the Inside-Outside 33 algorithm, we can estimate P-CFG from a large corpus of text. But since we also have a large corpus of bracketed sentences, we can adapt the Inside-Outside algorithm to reestimate the probability parameters subject to the constraint that only parses consistent with the Treebank (where consistency is as defined earlier) contribute to the reestimation. From a training run of 15,000 sentences we observed 87,704 mnemonic productions, with 23,341 NT mnemonics of which 10,302 were lexical. Running on a test set of 760 sentences 32% of the rule templates were used, 7% of the lexical mnemonics, 10% of the constituent mnemonics, and 5% of the mnemonic productions actually contributed to parses of test sentences. Grammar and Model Performance Metrics To evaluate the performance of a grammar and an accompanying model, we use two types of measurements: the defined as the percentage of sentences for which the correct parse is proposed among the many parses that the grammar provides for a sentence. We also measure base, is defined as the geometric mean of the number of proposed parses on a per word basis, to quantify the ambiguity of the grammar. the defined as the percentage of sentences for which the most likely parse is consistent. rate is measure of the grammar&apos;s coverage of linguistic phenomena. The evaluates the grammar&apos;s coverage with the statistical model imposed on the grammar. The goal of probabilistic modelling is to proa close to the rate is when we require the structure and the labels to agree and 96% when unlabeled bracketing is required. These results are obtained on 760 sentences from 7 to 17 words long from test material that has never been by the grammarian. The base 1.35 parses/word. This translates to about 23 parses a 12-word sentence. The unlabeled at 64% and the labeled is 60%. we believe that the above is close if not the state-of-the-art performance, there is room for improvement by using a more restatistical model to achieve the labeled anyof 90% with this grammar. There a significant gap between the labeled 30 percentage points. Instead of the usual approach where a grammarian tries to fine tune the grammar in the hope improving the rate use the combination of a large Treebank and the resulting derivation histories with a decision tree building algorithm to extract statistical parameters that would the The grammarian&apos;s task that of improving the The history-based grammar model is distinguished from the context-free grammar model in that each constituent structure depends not only on the input string, but also the entire history up to that point in the sentence. In HBGs, history is interpreted as any element of the output structure, or the parse tree, which has already been determined, including previous words, non-terminal categories, constituent structure, and any other linguistic information which is generated as part of the parse structure. The HBG Model Unlike P-CFG which assigns a probability to a mnemonic production, the HBG model assigns a probability to a rule template. Because of this the HBG formulation allows one to handle any grammar formalism that has a derivation process. For the HBG model, we have defined about syntactic categories, referred to as 50 semantic categories, referred to as Each NT (and therefore mnemonic) of the grammar has been assigned a syntactic (Syn) and a We also associate with a non-terminal a primary lexical head, denoted by and a secondary lexical head, denoted When a rule is applied to a non-terminal, it indicates which child will generate the lexical primary head and which child will generate the secondary lexical head. The proposed generative model associates for each constituent in the parse tree the probability: Sem, R, H1, Sem,, Ipc,Hip, In HBG, we predict the syntactic and semantic labels of a constituent, its rewrite rule, and its two lexical heads using the labels of the parent constituent, the parent&apos;s lexical heads, the parrule that lead to the constituent and constituent&apos;s index as child of As we discuss in a later section, we have also used with success more information about the derivation tree than the immediate parent in conditioning the probability of expanding a constituent. primary lexical head II1 corresponds (roughly) to the linguistic notion of a lexical head. The secondary lexical head 112 has no linguistic parallel. It merely represents a word in the constituent besides the head which contains predictive information about the constituent. 34 We have approximated the above probability by the following five factors:</abstract>
<note confidence="0.8667754">p(Syfl Synp, SemP) p(Sem ISyn, Synp &amp;nip) P(H ISyn, Rp H2p, &amp;nip) p(1/1 IR, Syn, 1lip, P(H2 IH1, R, Syn, Sem,</note>
<abstract confidence="0.984322979452054">While a different order for these predictions is possible, we only experimented with this one. Parameter Estimation We only have built a decision tree to the rule probability component (3) of the model. For the mowe are using with the usual interpolation smoothing for the other four components of the model. We have assigned bit strings to the syntactic and semantic categories and to the rules manually. Our intention is that bit strings differing in the least significant bit positions correspond to categories of non-terminals or rules that are similar. We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic clustering algorithms using the bigram mutual information clustering algorithm (see (5)). Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree. Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following manner. Using the grammar with the P-CFG model we determined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event. Note that the grammar parse will also provide the lexical head structure of the parse. Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node. Using the resulting data set we built a decision tree by classifying histories to locally minimize the entropy of the rule template. With a training set of about 9000 sentencetree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes. This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes. Immediate vs. Functional Parents model employs two types of parents, the and the The a list Figure 3: Sample representation of &amp;quot;with a list&amp;quot; in HBG model. R: PP1 Syn: PP H1: list with R: NBAR4 Syn: NP Sem: Data H1: list H2: a R: N1 Syn: N Sem: Data H1: list H2: * 35 immediate parent is the constituent that immediately dominates the constituent being predicted. If the immediate parent of a constituent has a different syntactic type from that of the constituent, then the immediate parent is also the functional parent; otherwise, the functional parent is the functional parent of the immediate parent. The distinction between functional parents and immediate parents arises primarily to cope with unit productions. When unit productions of the form XP2 ---&gt; XP1 occur, the immediate parent of XP1 is XP2. But, in general, the constituent XP2 does not contain enough useful information for ambiguity resolution. In particular, when considering only immediate parents, unit rules such as NP2 —■ NP1 prevent the probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1. When the two parents are identical as it often happens, the duplicate information will be ignored. However, when they differ, the decision tree will select that parental context which best resolves ambiguities. Figure 3 shows an example of the representation of a history in HBG for the prepositional phrase &amp;quot;with a list.&amp;quot; In this example, the immediate parent of the Ni node is the NBAR4 node and the functional parent of Ni is the PP1 node. Results We compared the performance of HBG to the &amp;quot;broad-coverage&amp;quot; probabilistic context-free gram- P-CFG. The of the grammar is 90% on test sentences of 7 to 17 words. The of P-CFG is 60% on the same test corpus of 760 sentences used in our experiments. On the same test sentences, the HBG model has a of 75%. This is a reduction of 37% in error rate. Accuracy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar models of varying complexity. One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG. This suggests that the current training corpus may not contain enough sentences to estimate richer models. Based on the results of these experiments, it appears likely that significantly increasing the size of the training corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models. To check the value of the above detailed history, we tried the simpler model: 1. 2. 3. p(Syn p(Sem ISyn, p(R ISyn, Sem, This model corresponds to a P-CFG with NTs that are the crude syntax and semantic categories with the lexical heads. The in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree. Conclusions The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG. More experimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history. In addition, this paper illustrates a new approach to grammar development where the parsing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the grammarian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence.</abstract>
<note confidence="0.656311538461538">REFERENCES Baker, J. K., 1975. Stochastic Modeling for Automatic Speech Understanding. In Speech Recognition, edited by Raj Reddy, Academic Press, pp. 521-542. Brent, M. R. 1991. Automatic Acquisition of Subcategorization Frames from Untagged Freetext Corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. Berkeley, California. Brill, E., Magerman, D., Marcus, M., and Santorini, B. 1990. Deducing Linguistic Structure from the Statistics of Large Corpora. In Pro-</note>
<affiliation confidence="0.8212235">ceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley,</affiliation>
<address confidence="0.970723">Pennsylvania.</address>
<author confidence="0.7376965">P F Brown</author>
<author confidence="0.7376965">Della Pietra</author>
<author confidence="0.7376965">V J</author>
<author confidence="0.7376965">P V deSouza</author>
<author confidence="0.7376965">J C Lai</author>
<author confidence="0.7376965">R L Class-based n- Mercer</author>
<affiliation confidence="0.933769">Models of Natural Language. In Proof the Language ITL,</affiliation>
<address confidence="0.994612">March, 1990. Paris, France.</address>
<email confidence="0.293358">36</email>
<author confidence="0.388382">In</author>
<affiliation confidence="0.7377945">Proceedings of the Second Conference on Applied Natural Language Processing. Austin,</affiliation>
<address confidence="0.846418">Texas.</address>
<author confidence="0.509197">W A Gale</author>
<author confidence="0.509197">K Church</author>
<affiliation confidence="0.7819">of Context are Worse than None. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley,</affiliation>
<address confidence="0.989958">Pennsylvania.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Stochastic Modeling for Automatic Speech Understanding. In Speech Recognition, edited by Raj Reddy,</title>
<date>1975</date>
<pages>521--542</pages>
<publisher>Academic Press,</publisher>
<marker>Baker, 1975</marker>
<rawString>Baker, J. K., 1975. Stochastic Modeling for Automatic Speech Understanding. In Speech Recognition, edited by Raj Reddy, Academic Press, pp. 521-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>Automatic Acquisition of Subcategorization Frames from Untagged Freetext Corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Berkeley, California.</location>
<marker>Brent, 1991</marker>
<rawString>Brent, M. R. 1991. Automatic Acquisition of Subcategorization Frames from Untagged Freetext Corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>D Magerman</author>
<author>M Marcus</author>
<author>B Santorini</author>
</authors>
<title>Deducing Linguistic Structure from the Statistics of Large Corpora.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<marker>Brill, Magerman, Marcus, Santorini, 1990</marker>
<rawString>Brill, E., Magerman, D., Marcus, M., and Santorini, B. 1990. Deducing Linguistic Structure from the Statistics of Large Corpora. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>V J</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based ngram Models of Natural Language.</title>
<date>1990</date>
<booktitle>In Proceedings of the IBM Natural Language ITL,</booktitle>
<location>Paris, France.</location>
<marker>Brown, Pietra, J, deSouza, Lai, Mercer, 1990</marker>
<rawString>Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. Class-based ngram Models of Natural Language. In Proceedings of the IBM Natural Language ITL, March, 1990. Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing.</booktitle>
<location>Austin, Texas.</location>
<marker>Church, 1988</marker>
<rawString>Church, K. 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In Proceedings of the Second Conference on Applied Natural Language Processing. Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K Church</author>
</authors>
<title>Poor Estimates of Context are Worse than None.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<marker>Gale, Church, 1990</marker>
<rawString>Gale, W. A. and Church, K. 1990. Poor Estimates of Context are Worse than None. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley Publishing Company.</publisher>
<marker>Harrison, 1978</marker>
<rawString>Harrison, M. A. 1978. Introduction to Formal Language Theory. Addison-Wesley Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural Ambiguity and Lexical Relations.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<marker>Hindle, Rooth, 1990</marker>
<rawString>Hindle, D. and Rooth, M. 1990. Structural Ambiguity and Lexical Relations. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organizing Language Modeling for Speech Recognition.</title>
<date>1985</date>
<tech>IBM Report.</tech>
<marker>Jelinek, 1985</marker>
<rawString>Jelinek, F. 1985. Self-organizing Language Modeling for Speech Recognition. IBM Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
<author>M P Marcus</author>
</authors>
<title>Pearl: A Probabilistic Chart Parser.</title>
<date>1991</date>
<booktitle>In Proceedings of the February</booktitle>
<location>Asilomar, California.</location>
<marker>Magerman, Marcus, 1991</marker>
<rawString>Magerman, D. M. and Marcus, M. P. 1991. Pearl: A Probabilistic Chart Parser. In Proceedings of the February 1991 DARPA Speech and Natural Language Workshop. Asilomar, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Derouault</author>
<author>B Merialdo</author>
</authors>
<title>Probabilistic Grammar for Phonetic to French Transcription.</title>
<date>1985</date>
<booktitle>ICASSP 85 Proceedings.</booktitle>
<pages>1577--1580</pages>
<location>Tampa, Florida,</location>
<marker>Derouault, Merialdo, 1985</marker>
<rawString>Derouault, A., and Merialdo, B., 1985. Probabilistic Grammar for Phonetic to French Transcription. ICASSP 85 Proceedings. Tampa, Florida, pp. 1577-1580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Sharman</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Generating a Grammar for Statistical Training.</title>
<date>1990</date>
<booktitle>In Proceedings of the</booktitle>
<location>Valley, Pennsylvania.</location>
<marker>Sharman, Jelinek, Mercer, 1990</marker>
<rawString>Sharman, R. A., Jelinek, F., and Mercer, R. 1990. Generating a Grammar for Statistical Training. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. Hidden Valley, Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>