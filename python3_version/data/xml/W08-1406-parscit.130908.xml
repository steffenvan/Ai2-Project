<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000179">
<title confidence="0.970756">
Mixed-Source Multi-Document Speech-to-Text Summarization
</title>
<author confidence="0.940476">
Ricardo Ribeiro
</author>
<affiliation confidence="0.717162">
INESC ID Lisboa/ISCTE/IST
Spoken Language Systems Lab
</affiliation>
<address confidence="0.9087355">
Rua Alves Redol, 9
1000-029 Lisboa, Portugal
</address>
<email confidence="0.995789">
rdmr@l2f.inesc-id.pt
</email>
<author confidence="0.751686">
David Martins de Matos
</author>
<affiliation confidence="0.585183">
INESC ID Lisboa/IST
Spoken Language Systems Lab
</affiliation>
<address confidence="0.868076">
Rua Alves Redol, 9
1000-029 Lisboa, Portugal
</address>
<email confidence="0.996878">
david@l2f.inesc-id.pt
</email>
<sectionHeader confidence="0.993805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987496">
Speech-to-text summarization systems
usually take as input the output of an
automatic speech recognition (ASR)
system that is affected by issues like
speech recognition errors, disfluencies, or
difficulties in the accurate identification
of sentence boundaries. We propose the
inclusion of related, solid background
information to cope with the difficulties
of summarizing spoken language and the
use of multi-document summarization
techniques in single document speech-
to-text summarization. In this work, we
explore the possibilities offered by pho-
netic information to select the background
information and conduct a perceptual
evaluation to better assess the relevance of
the inclusion of that information. Results
show that summaries generated using
this approach are considerably better than
those produced by an up-to-date latent
semantic analysis (LSA) summarization
method and suggest that humans prefer
summaries restricted to the information
conveyed in the input source.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999539">
News have been the subject of summarization
for a long time, demonstrating the importance
of both the subject and the process. Systems
like NewsInEssence (Radev et al., 2005), News-
blaster (McKeown et al., 2002), or even Google
</bodyText>
<footnote confidence="0.90724225">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.994390894736842">
News substantiate this relevance that is also sup-
ported by the spoken language scenario, where
most speech summarization systems concentrate
on broadcast news (McKeown et al., 2005). Nev-
ertheless, although the pioneering efforts on sum-
marization go back to the work of Luhn (1958)
and Edmundson (1969), it is only after the re-
naissance of summarization as a research area of
great activity—following up on the Dagstuhl Sem-
inar (Endres-Niggemeyer et al., 1995)—that the
first multi-document news summarization system,
SUMMONS (McKeown and Radev, 1995), makes
its breakthrough (Radev et al., 2005; Sp¨arck Jones,
2007). In what concerns speech summarization,
the state of affairs is more problematic: news sum-
marization systems appeared later and still focus
only on single document summarization (McKe-
own et al., 2005). In fact, while text summarization
has attained some degree of success (Hovy, 2003;
McKeown et al., 2005; Sp¨arck Jones, 2007) due to
the considerable body of work, speech summariza-
tion still requires further research, both in speech
and text analysis, in order to overcome the specific
challenges of the task (McKeown et al., 2005; Fu-
rui, 2007). Issues like speech recognition errors,
disfluencies, and difficulties in accurately identi-
fying sentence boundaries must be taken into ac-
count when summarizing spoken language. How-
ever, if on the one hand, recognition errors seem
not to have a considerable impact on the summa-
rization task (Murray et al., 2006; Murray et al.,
2005), on the other hand, spoken language summa-
rization systems often explore ways of minimizing
that impact (Zechner and Waibel, 2000; Hori et al.,
2003; Kikuchi et al., 2003).
We argue that by including related solid back-
ground information from a different source less
prone to this kind of errors (e.g., a textual source)
</bodyText>
<page confidence="0.990339">
33
</page>
<note confidence="0.694988">
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33–40
Manchester, August 2008
</note>
<bodyText confidence="0.999982590909091">
in the summarization process, we are able to re-
duce the influence of recognition errors on the re-
sulting summary. To support this argument, we de-
veloped a new approach to speech-to-text summa-
rization that combines information from multiple
information sources to produce a summary driven
by the spoken language document to be summa-
rized. The idea mimics the natural human behav-
ior, in which information acquired from different
sources is used to build a better understanding of
a given topic (Wan et al., 2007). Furthermore, we
build on the conjecture that this background infor-
mation is often used by humans to overcome per-
ception difficulties. In that sense, one of our goals
is also to understand what is expected in a sum-
mary: a comprehensive, shorter, text that addresses
the same subject of the input source to be summa-
rized (possibly introducing new information); or a
text restricted to the information conveyed in the
input source.
This work explores the use of phonetic domain
information to overcome speech recognition errors
and disfluencies. Instead of using the traditional
output of the ASR module, we use the phonetic
transliteration of the output and compare it to the
phonetic transliteration of solid background infor-
mation. This enables the use of text, related to the
input source, free from the common speech recog-
nition issues, in further processing.
We use broadcast news as a case study and
news stories from online newspapers provide the
background information. Media monitoring sys-
tems, used to transcribe and disseminate news,
provide an adequate framework to test the pro-
posed method.
This document is organized as follows: sec-
tion 2 briefly introduces the related work; section
3 presents a characterization of the speech-to-text
summarization problem and how we propose to
address it; section 4 explicits our use of phonetic
domain information, given the previously defined
context; the next section describes the case study,
including the experimental set up and results; con-
clusions close the document.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999946477272727">
McKeown et al. (2005) depict spoken language
summarization as a much harder task than text
summarization. In fact, the previously enumerated
problems that make speech summarization such
a difficult task constrain the applicability of text
summarization techniques to speech summariza-
tion (although in the presence of planned speech,
as it partly happens in the broadcast news domain,
that portability is more feasible (Christensen et al.,
2003)). On the other hand, speech offers possibili-
ties like the use of prosody and speaker identifica-
tion to ascertain relevant content.
Furui (2007) identifies three main approaches
to speech summarization: sentence extraction-
based methods, sentence compaction-based meth-
ods, and combinations of both.
Sentence extractive methods comprehend, es-
sentially, methods like LSA (Gong and Liu,
2001), Maximal Marginal Relevance (Carbonell
and Goldstein, 1998), and feature-based meth-
ods (Edmundson, 1969). Feature-based methods
combine several types of features: current work
uses lexical, acoustic/prosodic, structural, and dis-
course features to summarize documents from do-
mains like broadcast news or meetings (Maskey
and Hirschberg, 2005; Murray et al., 2006; Ribeiro
and de Matos, 2007). Even so, spoken language
summarization is still quite distant from text sum-
marization in what concerns the use of discourse
features, and shallow approaches is what can be
found in state-of-the-art work such as the one pre-
sented by Maskey and Hirschberg (2005) or Mur-
ray et al. (2006). Sentence compaction methods
are based on word removal from the transcription,
with recognition confidence scores playing a ma-
jor role (Hori et al., 2003). A combination of these
two types of methods was developed by Kikuchi
et al. (2003), where summarization is performed
in two steps: first, sentence extraction is done
through feature combination; second, compaction
is done by scoring the words in each sentence and
then a dynamic programming technique is applied
to select the words that will remain in the sentence
to be included in the summary.
</bodyText>
<sectionHeader confidence="0.996214" genericHeader="method">
3 Problem Characterization
</sectionHeader>
<subsectionHeader confidence="0.895842">
Summarization can be seen as a reductive transfor-
</subsectionHeader>
<bodyText confidence="0.3566575">
mation 0 that, given an input source I, produces a
summary S:
</bodyText>
<equation confidence="0.972462">
S = 0(I),
</equation>
<bodyText confidence="0.999473">
where len(S) &lt; len(I) and inf (S) is as close
as possible of inf (I); len() is the length of the
given input and inf () is the information conveyed
by its argument.
The problem is that in order to compute S, we
are not using I, but ˜I, a noisy representation of I.
</bodyText>
<page confidence="0.998377">
34
</page>
<bodyText confidence="0.9999075">
Thus, we are computing ˜5, which is a summary
affected by the noise present in ˜I:
</bodyText>
<equation confidence="0.98494575">
5˜= φ(
This means that
inf (˜5) C inf (5) C inf (I), whereas
len(˜5) Pz� len(5) &lt; len(I).
</equation>
<bodyText confidence="0.9790536875">
Our argument is that using a similar reductive
transformation 0, where solid background infor-
mation B is also given as input, it is possible to
compute a summary ˆ5:
Feature Values
Type vowel, consonant
Vowel length short, long, diphthong,
schwa
Vowel height high, mid, low
Vowel frontness front mid back
Lip rounding yes, no
Consonant type stop, fricative, affricative,
nasal, liquid
Place of articulation labial, alveolar, palatal,
labio-dental, dental, velar
Consonant voicing yes, no
</bodyText>
<tableCaption confidence="0.980499">
Table 1: Phone features.
</tableCaption>
<equation confidence="0.8453135">
˜I).
5ˆ= 0( ˜I, B), such that
inf (˜5) C (inf (ˆ5) n inf (5)) C inf (I), with
len(ˆ5) Pz� len(˜5) Pz� len(5) &lt; len(I).
</equation>
<bodyText confidence="0.999321333333334">
As seen in section 2, the most common method
to perform these transformations is by selecting
sentences (or extracts) from the corresponding in-
put sources.
Thus, let the input source representation I˜ be
composed by a sequence of extracts ei,
</bodyText>
<equation confidence="0.885276">
I˜= e1, e2,...,en
</equation>
<bodyText confidence="0.999604">
and the background information be defined as a
sequence of sentences
</bodyText>
<equation confidence="0.972313">
B = s1, s2, ... , sm.
</equation>
<bodyText confidence="0.998174">
The proposed method consists of selecting sen-
tences si form the background information B such
that
</bodyText>
<equation confidence="0.896761">
sim(si,ej) &lt; ε ∧ 0 &lt; i &lt; m ∧ 0 &lt; j &lt; n,
</equation>
<bodyText confidence="0.99942">
with sim() being a similarity function and ε an
adequate threshold. The difficulty lies in defining
the function and the threshold.
</bodyText>
<sectionHeader confidence="0.802227" genericHeader="method">
4 Working in the phonetic domain
</sectionHeader>
<bodyText confidence="0.999968444444444">
The approach we introduce minimizes the effects
of recognition errors through the selection, from
previously determined background knowledge, of
sentence-like units close to the ones of the news
story transcription. In order to select sentence-like
units, while diminishing recognition problems, we
compute the similarity between them at the pho-
netic level. The estimation of the threshold is
based on the distance, measured in the phonetic
domain, between the output of the ASR and its
hand-corrected version.
The selection of sentences from the background
information is based on the alignment cost of the
phonetic transcriptions of sentences from the input
source and sentence from the background informa-
tion. Sentences from the background information
with alignment costs below the estimated threshold
are selected to be used in summary generation.
</bodyText>
<subsectionHeader confidence="0.999149">
4.1 Similarity Between Segments
</subsectionHeader>
<bodyText confidence="0.999980090909091">
There are several ways to compute phonetic simi-
larity. Kessler (2005) states that phonetic distance
can be seen as, among other things, differences
between acoustic properties of the speech stream,
differences in the articulatory positions during pro-
duction, or as the perceptual distance between iso-
lated sounds. Choosing a way to calculate phonetic
distance is a complex process.
The phone similarity function used in this pro-
cess is based on a model of phone production,
where the phone features correspond to the articu-
latory positions during production: the greater the
matching between phone features, the smaller the
distance between phones. The phone features used
are described in table 1.
The computation of the similarity between
sentence-like units is based on the alignment of
the phonetic transcriptions of the given segments.
The generation of the possible alignments and the
selection of the best alignment is done through
the use of Weighted Finite-State Transducers (WF-
STs) (Mohri, 1997; Paulo and Oliveira, 2002).
</bodyText>
<page confidence="0.995354">
35
</page>
<subsectionHeader confidence="0.981543">
4.2 Threshold Estimation Process
</subsectionHeader>
<bodyText confidence="0.9996485">
To estimate the threshold to be used in the sentence
selection process, we use the algorithm presented
in figure 1. The procedure consists of comparing
automatic transcriptions and their hand-corrected
versions: the output is the average difference be-
tween the submitted inputs.
</bodyText>
<figureCaption confidence="0.999629">
Figure 1: Threshold estimation process.
</figureCaption>
<bodyText confidence="0.999928375">
The idea is that the phonetic distance between
the automatic transcription and its hand-corrected
version would be similar to the phonetic distance
between the automatic transcription and the back-
ground information. Even though this heuristic
may appear naif, we believe it is adequate as a
rough approach, considering the target material
(broadcast news).
</bodyText>
<sectionHeader confidence="0.980549" genericHeader="method">
5 A Case Study Using Broadcast News
</sectionHeader>
<subsectionHeader confidence="0.996424">
5.1 Media Monitoring System
</subsectionHeader>
<bodyText confidence="0.99998375925926">
SSNT (Amaral et al., 2007) is a system for selec-
tive dissemination of multimedia contents, work-
ing primarily with Portuguese broadcast news ser-
vices. The system is based on an ASR mod-
ule, that generates the transcriptions used by
the topic segmentation, topic indexing, and ti-
tle&amp;summarization modules. User profiles enable
the system to deliver e-mails containing relevant
news stories. These messages contain the name
of the news service, a generated title, a summary,
a link to the corresponding video segment, and a
classification according to a thesaurus used by the
broadcasting company.
Preceding the speech recognition module, an au-
dio preprocessing module, based on Multi-layer
Perceptrons, classifies the audio in accordance to
several criteria: speech/non-speech, speaker seg-
mentation and clustering, gender, and background
conditions.
The ASR module, based on a hybrid speech
recognition system that combines Hidden Markov
Models with Multi-layer Perceptrons, with an av-
erage word error rate of 24% (Amaral et al., 2007),
greatly influences the performance of the subse-
quent modules.
The topic segmentation and topic indexing
modules were developed by Amaral and Tran-
coso (2004). Topic segmentation is based on clus-
tering and groups transcribed segments into sto-
ries. The algorithm relies on a heuristic derived
from the structure of the news services: each story
starts with a segment spoken by the anchor. This
module achieved an F-measure of 68% (Amaral
et al., 2007). The main problem identified by the
authors was boundary deletion: a problem which
impacts the summarization task. Topic indexing is
based on a hierarchically organized thematic the-
saurus provided by the broadcasting company. The
hierarchy has 22 thematic areas on the first level,
for which the module achieved a correctness of
91.4% (Amaral et al., 2006; Amaral et al., 2007).
Batista et al. (2007) inserted a module for re-
covering punctuation marks, based on maximum
entropy models, after the ASR module. The punc-
tuation marks addressed were the “full stop” and
“comma”, which provide the sentence units nec-
essary for use in the title&amp;summarization mod-
ule. This module achieved an F-measure of 56%
and SER (Slot Error Rate, the measure commonly
used to evaluate this kind of task) of 0.74.
Currently, the title&amp;summarization module pro-
duces a summary composed by the first n sen-
tences, as detected by the previous module, of each
news story and a title (the first sentence).
</bodyText>
<subsectionHeader confidence="0.984196">
5.2 Corpora
</subsectionHeader>
<bodyText confidence="0.999649">
Two corpora were used in this experiment: a
broadcast news corpus, the subject of our summa-
rization efforts; and a written newspaper corpus,
used to select the background information.
</bodyText>
<figure confidence="0.998088666666667">
Sentence segmented
ASR output
Phonetic
transliteration
Sentence segmented
Manual transcription
Projection of the
sentences of
the ASR ouput
over the manual
transcription
Sentence-by-
sentence
distance
calculation
Manual transcription
Phonetic
transliteration
</figure>
<page confidence="0.967055">
36
</page>
<table confidence="0.9959">
Corpus Stories SUs Tokens Duration
train 184 2661 57063 5h
test 26 627 7360 1h
</table>
<tableCaption confidence="0.997203">
Table 2: Broadcast news corpus composition.
</tableCaption>
<bodyText confidence="0.999953647058824">
The broadcast news corpus is composed by 6
Portuguese news programs, and exists in two ver-
sions: an automatically processed one, and a hand-
corrected one. Its composition (number of stories,
number of sentence-like units (SUs), number of to-
kens, and duration) is detailed in table 2. To es-
timate the threshold used for the selection of the
background information, 5 news programs were
used. The last one was used for evaluation.
The written newspaper corpus consists of the
online version a Portuguese newspaper, down-
loaded daily from the Internet. In this experiment,
three editions of the newspaper were used, corre-
sponding to the day and the two previous days of
the news program to be summarized. The corpus
is composed by 135 articles, 1418 sentence-like
units, and 43102 tokens.
</bodyText>
<subsectionHeader confidence="0.993023">
5.3 The Summarization Process
</subsectionHeader>
<bodyText confidence="0.999161">
The summarization process we implemented is
characterized by the use of LSA to compute the
relevance of the extracts (sentence-like units) of
the given input source.
LSA is based on the singular vector decomposi-
tion (SVD) of the term-sentence frequency m x n
matrix, M. U is an m x n matrix of left singular
vectors; Σ is the n x n diagonal matrix of singular
values; and, V is the n x n matrix of right singular
vectors (only possible if m ≥ n):
</bodyText>
<equation confidence="0.961556">
M = UΣVT
</equation>
<bodyText confidence="0.999922153846154">
The idea behind the method is that the decom-
position captures the underlying topics of the doc-
ument by means of co-occurrence of terms (the la-
tent semantic analysis), and identifies the best rep-
resentative sentence-like units of each topic. Sum-
mary creation can be done by picking the best rep-
resentatives of the most relevant topics according
to a defined strategy.
For this summarization process, we imple-
mented a module following the original ideas of
Gong and Liu (2001) and the ones of Murray, Re-
nals, and Carletta (2005) for solving dimensional-
ity problems, and using, for matrix operations, the
</bodyText>
<subsectionHeader confidence="0.6632855">
GNU Scientific Library1.
5.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999764340425532">
Our main objective was to understand if it is pos-
sible to select relevant information from back-
ground information that could improve the quality
of speech-to-text summaries. To assess the valid-
ity of this hypothesis, five different processes of
generating a summary were considered. To bet-
ter analyze the influence of the background in-
formation, all automatic summarization methods
are based on the up-to-date LSA method previ-
ously described: one taking as input only the news
story to be summarized (Simple) and used as base-
line; other taking as input only the selected back-
ground information (Background only); and, the
last one, using both the news story and the back-
ground information (Background + News). The
other two processes were human: extractive (using
only the news story) and abstractive (understand-
ing the news story and condensing it by means
of paraphrase). Since the abstractive summaries
had already been created, summary size was de-
termined by their size (which means creating sum-
maries using a compression rate of around 10% of
the original size).
As mentioned before, the whole summariza-
tion process begins with the selection of the back-
ground information. Using the threshold estimated
as described in section 4.2 and the method de-
scribed in section 4.1 to compute similarity be-
tween sentence-like units, no background informa-
tion was selected for 11 of the 26 news stories of
the test corpus. For the remaining 15 news sto-
ries, summaries were generated using the three au-
tomatic summarization strategies described before.
In what concerns the evaluation process, al-
though ROUGE (Lin, 2004) is the most common
evaluation metric for the automatic evaluation of
summarization, since our approach might intro-
duce in the summary information that it is not
present in the original input source, we found that a
human evaluation was more adequate to assess the
relevance of that additional information. A percep-
tual evaluation is also adequate to assess the per-
ceive quality of the summaries and a better indica-
tor of the what is expected to be in a summary.
We asked an heterogeneous group of sixteen
people to evaluate the summaries created for the
15 news stories for which background information
</bodyText>
<footnote confidence="0.988688">
1http://www.gnu.org/software/gsl/
</footnote>
<page confidence="0.99929">
37
</page>
<figureCaption confidence="0.947468">
Figure 2: Overall results for each summary cre-
ation method (nsnn identifies a news story).
</figureCaption>
<bodyText confidence="0.983214451612903">
was selected. Each evaluator was given, for each
story, the news story itself (without background in-
formation) and five summaries, corresponding to
the five different methods presented before. The
evaluation procedure consisted in identifying the
best summary and in the classification of each
summary (1–5, 5 is better) according to its content
and readability (which covers issues like grammat-
icality, existence of redundant information, or en-
tity references (Nenkova, 2006)).
Figure 3: Relative results for each news story
(nsnn identifies a news story; stack order is inverse
of legend order).
Surprisingly enough (see figures 2 and 3), in
general, the extractive human summaries were pre-
ferred over the abstractive ones. Moreover, the
summaries generated automatically using back-
ground information (exclusively or not) were also
selected as best summary (over the human created
ones) a non-negligible number of times. The poor-
est performance was attained, as expected, by the
simple LSA summarizer, only preferred on two
news stories for which all summaries were very
similar. The results of the two approaches using
background information were very close, a result
that can be explained by the fact the summaries
generated by these two approaches were equal for
11 of the 15 news stories (in the remaining 4, the
average distribution was 31.25% from the news
story versus 68.75% from the background infor-
mation).
</bodyText>
<figureCaption confidence="0.97609475">
Figure 4 further discriminates the results in
terms of content and readability.
Figure 4: Average of the content and readability
scores for each summary creation method.
</figureCaption>
<bodyText confidence="0.999954157894737">
Regarding content, the results suggest that the
choice of the best summary is highly correlated
with its content, as the average content scores
mimic the overall ones of figure 2. In what con-
cerns readability, the summaries generated using
background information achieved the best results.
The reasons underlying these results are that the
newspaper writing is naturally better planned than
speech and that speech transcriptions are affected
by the several problems described before (and the
original motivation for the work), hence the idea
of using them as background information. How-
ever, what is odd is that the result obtained by
the human abstractive summary creation method
is worse than the ones obtained by automatic
generation using background information, which
could suffer from coherence and cohesion prob-
lems. One possible explanation is that the human
abstractive summaries tend to mix both informa-
</bodyText>
<figure confidence="0.992037">
Human Abstractive ns00
Human Extractive ns01
Background + News ns02
Background only ns03
Simple (News only) ns04
ns05
ns06
ns07
ns08
ns09
ns10
ns11
ns12
ns13
ns14
0 20 40 60 80 100 120
100%
90%
80%
60%
50%
40%
30%
20%
70%
10%
0%
ns00 ns01 ns02 ns03 ns04 ns05 ns06 ns07 ns08 ns09 ns10 ns11 ns12 ns13 ns14
Simple (News only)
Background only
Background + News
Human Extractive
Human Abstractive
5.00
4.50
4.00
3.50
3.00
2.50
2.00
1.50
1.00
0.50
0.00
Simple (News Background Background + Human Human
only) only News Extractive Abstractive
content readability
</figure>
<page confidence="0.925261">
38
</page>
<bodyText confidence="0.565499">
tive and indicative styles of summary.
</bodyText>
<figureCaption confidence="0.994437">
Figure 5: Standard deviation of the content and
readability scores.
</figureCaption>
<bodyText confidence="0.9666065">
Figure 5 presents the standard deviation for con-
tent and readability scores: concerning content,
automatically generated summaries using back-
ground information achieved the highest standard
deviation scores (see also figure 6 for a sample
story). That is in part supported by some commen-
taries made by the human evaluators on whether
a summary should contain information that is not
present in the input source. This aspect and the ob-
tained results, suggest that this issue should be fur-
ther analyzed, possibly using an extrinsic evalua-
tion setup. On the other hand, readability standard
deviation scores show that there is a considerable
agreement in what concerns this criterion.
</bodyText>
<figureCaption confidence="0.601734">
Figure 6: Average and standard deviation of the
content and readability scores for one news story.
</figureCaption>
<sectionHeader confidence="0.996963" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999984214285714">
We present a new approach to speech summariza-
tion that goes in the direction of the integration of
text and speech analysis, as suggested by McKe-
own et al. (2005). The main idea is the inclusion
of related, solid background information to cope
with the difficulties of summarizing spoken lan-
guage and the use of multi-document summariza-
tion techniques in single document speech-to-text
summarization. In this work, we explore the pos-
sibilities offered by phonetic information to select
the background information and conducted a per-
ceptual evaluation to assess the relevance of the in-
clusion of that information.
The results obtained show that the human eval-
uators preferred human extractive summaries over
human abstractive summaries. Moreover, simple
LSA summaries attained the poorest results both in
terms of content and readability, while human ex-
tractive summaries achieved the best performance
in what concerns content, and a considerably bet-
ter performance than simple LSA in what concerns
readability. This suggests that it is sill relevant to
pursue new methods for relevance estimation. On
the other hand, automatically generated summaries
using background information were significantly
better than simple LSA. This indicates that back-
ground information is a viable way to increase the
quality of automatic summarization systems.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996665307692308">
Amaral, R. and I. Trancoso. 2004. Improving the Topic
Indexation and Segmentation Modules of a Media
Watch System. In Proceedings of INTERSPEECH
2004 - ICSLP, pages 1609–1612. ISCA.
Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, and
J. P. Neto. 2006. Automatic vs. Manual Topic Seg-
mentation and Indexation in Broadcast News. In
Proc. of the IV Jornadas en Tecnologia del Habla.
Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, and
J. P. Neto. 2007. A Prototype System for Selective
Dissemination of Broadcast News in European Por-
tuguese. EURASIP Journal on Advances in Signal
Processing, 2007.
Batista, F., D. Caseiro, N. J. Mamede, and I. Tran-
coso. 2007. Recovering Punctuation Marks for Au-
tomatic Speech Recognition. In Proceedings of IN-
TERSPEECH 2007, pages 2153–2156. ISCA.
Carbonell, J. and J. Goldstein. 1998. The Use of MMR,
Diversity-Based Reranking for Reordering Docu-
ments and Producing Summaries. In SIGIR 1998:
Proceedings of the 21&amp;quot; Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 335–336. ACM.
Christensen, H., Y. Gotoh, B. Kolluru, and S. Renals.
2003. Are Extractive Text Summarisation Tech-
niques Portable To Broadcast News? In Proceedings
</reference>
<figure confidence="0.999415268292683">
0.80
0.60
0.40
0.20
0.00
1.20
1.00
Simple (News
only)
Background
only
content readability
Background +
News
Human
Extractive
Human
Abstractive
5.00
4.50
4.00
3.50
3.00
2.50
2.00
0.50
0.00
1.50
1.00
Simple (News
only)
Content (avg) Readability (avg)
Content (stdev) Readability (stdev)
Background
only
Background +
News
Human
Extractive
Human
Abstractive
</figure>
<page confidence="0.989042">
39
</page>
<reference confidence="0.998439990740741">
of the IEEE Workshop on Automatic Speech Recog-
nition and Understanding, pages 489–494. IEEE.
Edmundson, H. P. 1969. New methods in automatic
abstracting. Journal of the Association for Comput-
ing Machinery, 16(2):264–285.
Endres-Niggemeyer, B., J. R. Hobbs, and K. Sp¨arck
Jones, editors. 1995. Summarizing Text
for Intelligent Communication—Dagstuhl-Seminar-
Report 79. IBFI.
Furui, S. 2007. Recent Advances in Automatic Speech
Summarization. In Proceedings of the 8th Confer-
ence on Recherche d’Information Assist´ee par Or-
dinateur (RIAO). Centre des Hautes ´Etudes Interna-
tionales d’Informatique Documentaire.
Gong, Y. and X. Liu. 2001. Generic Text Summariza-
tion Using Relevance Measure and Latent Semantic
Analysis. In SIGIR 2001: Proceedings of the 24it
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 19–25. ACM.
Hori, T., C. Hori, and Y. Minami. 2003. Speech Sum-
marization using Weighted Finite-State Transducers.
In Proceedings of the 8th EUROSPEECH - INTER-
SPEECH 2003, pages 2817–2820. ISCA.
Hovy, E., 2003. The Oxford Handbook of Compu-
tational Linguistics, chapter Text Summarization,
pages 583–598. Oxford University Press.
Kessler, B. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243–260.
Kikuchi, T., S. Furui, and C. Hori. 2003. Two-
stage Automatic Speech Summarization by Sen-
tence Extraction and Compaction. In Proceedings
of the ISCA &amp; IEEE Workshop on Spontaneous
Speech Processing and Recognition (SSPR-2003),
pages 207–210. ISCA.
Lin, C. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81. ACL.
Luhn, H. P. 1958. The Automatic Creation of Litera-
ture Abstracts. IBM Journal of Research and Devel-
opment, 2(2):159–165.
Maskey, S. and J. Hirschberg. 2005. Comparing Lexi-
cal, Acoustic/Prosodic, Strucural and Discourse Fea-
tures for Speech Summarization. In Proceedings
of the 9th EUROSPEECH - INTERSPEECH 2005,
pages 621–624. ISCA.
McKeown, K. R. and D. Radev. 1995. Generating
Summaries of Multiple News Articles. In SIGIR
1995: Proceedings of the 18th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 74–82. ACM.
McKeown, K. R., R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Track-
ing and Summarizing News on a Daily Basis with
Columbia’s Newsblaster. In Proc. of the 2nd Inter-
national Conference on Human Language Technol-
ogy Research, pages 280–285. Morgan Kaufmann.
McKeown, K. R., J. Hirschberg, M. Galley, and
S. Maskey. 2005. From Text to Speech Summa-
rization. In 2005 IEEE International Conference on
Acoustics, Speech, and Signal Processing. Proceed-
ings, volume V, pages 997–1000. IEEE.
Mohri, M. 1997. Finite-State Transducers in Language
and Speech Processing. Computational Linguistics,
23(2):269–311.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Records. In Proceedings
of the 9th EUROSPEECH - INTERSPEECH 2005,
pages 593–596. ISCA.
Murray, G., S. Renals, J. Carletta, and J. Moore.
2006. Incorporating Speaker and Discourse Features
into Speech Summarization. In Proceedings of the
HLT/NAACL, pages 367–374. ACL.
Nenkova, A. 2006. Summarization Evaluation for Text
and Speech: Issues and Approaches. In Proceedings
of INTERSPEECH 2006 - ICSLP, pages 1527–1530.
ISCA.
Paulo, S. and L. C. Oliveira. 2002. Multilevel Annota-
tion Of Speech Signals Using Weighted Finite State
Transducers. In Proc. of the 2002 IEEE Workshop
on Speech Synthesis, pages 111–114. IEEE.
Radev, D., J. Otterbacher, A. Winkel, and S. Blair-
Goldensohn. 2005. NewsInEssence: Summarizing
Online News Topics. Communications of the ACM,
48(10):95–98.
Ribeiro, R. and D. M. de Matos. 2007. Extractive Sum-
marization of Broadcast News: Comparing Strate-
gies for European Portuguese. In Text, Speech and
Dialogue – 10th International Conference. Proceed-
ings, volume 4629 of Lecture Notes in Computer Sci-
ence (Subseries LNAI), pages 115–122. Springer.
Sp¨arck Jones, K. 2007. Automatic summarising: The
state of the art. Information Processing and Man-
agement, 43:1449–1481.
Wan, X., J. Yang, and J. Xiao. 2007. CollabSum: Ex-
ploiting Multiple Document Clustering for Collabo-
rative Single Document Summarizations. In SIGIR
2007: Proc. of the 30th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 143–150. ACM.
Zechner, K. and A. Waibel. 2000. Minimizing Word
Error Rate in Textual Summaries of Spoken Lan-
guage. In Proceedings of the 1it conference of the
North American chapter of the ACL, pages 186–193.
Morgan Kaufmann.
</reference>
<page confidence="0.998634">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480332">
<title confidence="0.999938">Mixed-Source Multi-Document Speech-to-Text Summarization</title>
<author confidence="0.989613">Ricardo</author>
<affiliation confidence="0.970403">INESC ID</affiliation>
<title confidence="0.869357">Spoken Language Systems</title>
<author confidence="0.844876">Rua Alves Redol</author>
<address confidence="0.969343">1000-029 Lisboa,</address>
<email confidence="0.983126">rdmr@l2f.inesc-id.pt</email>
<author confidence="0.990739">David Martins de</author>
<affiliation confidence="0.804627">INESC ID</affiliation>
<title confidence="0.889375">Spoken Language Systems</title>
<author confidence="0.862774">Rua Alves Redol</author>
<address confidence="0.986394">1000-029 Lisboa,</address>
<email confidence="0.989201">david@l2f.inesc-id.pt</email>
<abstract confidence="0.998392461538462">Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries. We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speechto-text summarization. In this work, we explore the possibilities offered by phonetic information to select the background information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information. Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Amaral</author>
<author>I Trancoso</author>
</authors>
<title>Improving the Topic Indexation and Segmentation Modules of a Media Watch System. In</title>
<date>2004</date>
<booktitle>Proceedings of INTERSPEECH 2004 - ICSLP,</booktitle>
<pages>1609--1612</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="13583" citStr="Amaral and Trancoso (2004)" startWordPosition="2105" endWordPosition="2109"> by the broadcasting company. Preceding the speech recognition module, an audio preprocessing module, based on Multi-layer Perceptrons, classifies the audio in accordance to several criteria: speech/non-speech, speaker segmentation and clustering, gender, and background conditions. The ASR module, based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an average word error rate of 24% (Amaral et al., 2007), greatly influences the performance of the subsequent modules. The topic segmentation and topic indexing modules were developed by Amaral and Trancoso (2004). Topic segmentation is based on clustering and groups transcribed segments into stories. The algorithm relies on a heuristic derived from the structure of the news services: each story starts with a segment spoken by the anchor. This module achieved an F-measure of 68% (Amaral et al., 2007). The main problem identified by the authors was boundary deletion: a problem which impacts the summarization task. Topic indexing is based on a hierarchically organized thematic thesaurus provided by the broadcasting company. The hierarchy has 22 thematic areas on the first level, for which the module achi</context>
</contexts>
<marker>Amaral, Trancoso, 2004</marker>
<rawString>Amaral, R. and I. Trancoso. 2004. Improving the Topic Indexation and Segmentation Modules of a Media Watch System. In Proceedings of INTERSPEECH 2004 - ICSLP, pages 1609–1612. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Amaral</author>
<author>H Meinedo</author>
<author>D Caseiro</author>
<author>I Trancoso</author>
<author>J P Neto</author>
</authors>
<title>Automatic vs. Manual Topic Segmentation and Indexation in Broadcast News.</title>
<date>2006</date>
<booktitle>In Proc. of the IV Jornadas en Tecnologia del Habla.</booktitle>
<contexts>
<context position="14231" citStr="Amaral et al., 2006" startWordPosition="2211" endWordPosition="2214"> on clustering and groups transcribed segments into stories. The algorithm relies on a heuristic derived from the structure of the news services: each story starts with a segment spoken by the anchor. This module achieved an F-measure of 68% (Amaral et al., 2007). The main problem identified by the authors was boundary deletion: a problem which impacts the summarization task. Topic indexing is based on a hierarchically organized thematic thesaurus provided by the broadcasting company. The hierarchy has 22 thematic areas on the first level, for which the module achieved a correctness of 91.4% (Amaral et al., 2006; Amaral et al., 2007). Batista et al. (2007) inserted a module for recovering punctuation marks, based on maximum entropy models, after the ASR module. The punctuation marks addressed were the “full stop” and “comma”, which provide the sentence units necessary for use in the title&amp;summarization module. This module achieved an F-measure of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74. Currently, the title&amp;summarization module produces a summary composed by the first n sentences, as detected by the previous module, of each news story and a title</context>
</contexts>
<marker>Amaral, Meinedo, Caseiro, Trancoso, Neto, 2006</marker>
<rawString>Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, and J. P. Neto. 2006. Automatic vs. Manual Topic Segmentation and Indexation in Broadcast News. In Proc. of the IV Jornadas en Tecnologia del Habla.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Amaral</author>
<author>H Meinedo</author>
<author>D Caseiro</author>
<author>I Trancoso</author>
<author>J P Neto</author>
</authors>
<title>A Prototype System for Selective Dissemination of Broadcast News</title>
<date>2007</date>
<booktitle>in European Portuguese. EURASIP Journal on Advances in Signal Processing,</booktitle>
<contexts>
<context position="12419" citStr="Amaral et al., 2007" startWordPosition="1931" endWordPosition="1934">aring automatic transcriptions and their hand-corrected versions: the output is the average difference between the submitted inputs. Figure 1: Threshold estimation process. The idea is that the phonetic distance between the automatic transcription and its hand-corrected version would be similar to the phonetic distance between the automatic transcription and the background information. Even though this heuristic may appear naif, we believe it is adequate as a rough approach, considering the target material (broadcast news). 5 A Case Study Using Broadcast News 5.1 Media Monitoring System SSNT (Amaral et al., 2007) is a system for selective dissemination of multimedia contents, working primarily with Portuguese broadcast news services. The system is based on an ASR module, that generates the transcriptions used by the topic segmentation, topic indexing, and title&amp;summarization modules. User profiles enable the system to deliver e-mails containing relevant news stories. These messages contain the name of the news service, a generated title, a summary, a link to the corresponding video segment, and a classification according to a thesaurus used by the broadcasting company. Preceding the speech recognition</context>
<context position="13875" citStr="Amaral et al., 2007" startWordPosition="2155" endWordPosition="2158"> based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an average word error rate of 24% (Amaral et al., 2007), greatly influences the performance of the subsequent modules. The topic segmentation and topic indexing modules were developed by Amaral and Trancoso (2004). Topic segmentation is based on clustering and groups transcribed segments into stories. The algorithm relies on a heuristic derived from the structure of the news services: each story starts with a segment spoken by the anchor. This module achieved an F-measure of 68% (Amaral et al., 2007). The main problem identified by the authors was boundary deletion: a problem which impacts the summarization task. Topic indexing is based on a hierarchically organized thematic thesaurus provided by the broadcasting company. The hierarchy has 22 thematic areas on the first level, for which the module achieved a correctness of 91.4% (Amaral et al., 2006; Amaral et al., 2007). Batista et al. (2007) inserted a module for recovering punctuation marks, based on maximum entropy models, after the ASR module. The punctuation marks addressed were the “full stop” and “comma”, which provide the sentenc</context>
</contexts>
<marker>Amaral, Meinedo, Caseiro, Trancoso, Neto, 2007</marker>
<rawString>Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, and J. P. Neto. 2007. A Prototype System for Selective Dissemination of Broadcast News in European Portuguese. EURASIP Journal on Advances in Signal Processing, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Batista</author>
<author>D Caseiro</author>
<author>N J Mamede</author>
<author>I Trancoso</author>
</authors>
<title>Recovering Punctuation Marks for Automatic Speech Recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of INTERSPEECH</booktitle>
<pages>2153--2156</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="14276" citStr="Batista et al. (2007)" startWordPosition="2219" endWordPosition="2222">nts into stories. The algorithm relies on a heuristic derived from the structure of the news services: each story starts with a segment spoken by the anchor. This module achieved an F-measure of 68% (Amaral et al., 2007). The main problem identified by the authors was boundary deletion: a problem which impacts the summarization task. Topic indexing is based on a hierarchically organized thematic thesaurus provided by the broadcasting company. The hierarchy has 22 thematic areas on the first level, for which the module achieved a correctness of 91.4% (Amaral et al., 2006; Amaral et al., 2007). Batista et al. (2007) inserted a module for recovering punctuation marks, based on maximum entropy models, after the ASR module. The punctuation marks addressed were the “full stop” and “comma”, which provide the sentence units necessary for use in the title&amp;summarization module. This module achieved an F-measure of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74. Currently, the title&amp;summarization module produces a summary composed by the first n sentences, as detected by the previous module, of each news story and a title (the first sentence). 5.2 Corpora Two corpor</context>
</contexts>
<marker>Batista, Caseiro, Mamede, Trancoso, 2007</marker>
<rawString>Batista, F., D. Caseiro, N. J. Mamede, and I. Trancoso. 2007. Recovering Punctuation Marks for Automatic Speech Recognition. In Proceedings of INTERSPEECH 2007, pages 2153–2156. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries.</title>
<date>1998</date>
<booktitle>In SIGIR 1998: Proceedings of the 21&amp;quot; Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6652" citStr="Carbonell and Goldstein, 1998" startWordPosition="995" endWordPosition="998">s to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003)). On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content. Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Mur</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Carbonell, J. and J. Goldstein. 1998. The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In SIGIR 1998: Proceedings of the 21&amp;quot; Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 335–336. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>Y Gotoh</author>
<author>B Kolluru</author>
<author>S Renals</author>
</authors>
<title>Are Extractive Text Summarisation Techniques Portable To Broadcast News?</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>489--494</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6204" citStr="Christensen et al., 2003" startWordPosition="933" endWordPosition="936">tion, given the previously defined context; the next section describes the case study, including the experimental set up and results; conclusions close the document. 2 Related Work McKeown et al. (2005) depict spoken language summarization as a much harder task than text summarization. In fact, the previously enumerated problems that make speech summarization such a difficult task constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003)). On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content. Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, st</context>
</contexts>
<marker>Christensen, Gotoh, Kolluru, Renals, 2003</marker>
<rawString>Christensen, H., Y. Gotoh, B. Kolluru, and S. Renals. 2003. Are Extractive Text Summarisation Techniques Portable To Broadcast News? In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 489–494. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic abstracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="2042" citStr="Edmundson (1969)" startWordPosition="279" endWordPosition="280">of both the subject and the process. Systems like NewsInEssence (Radev et al., 2005), Newsblaster (McKeown et al., 2002), or even Google © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005). Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research area of great activity—following up on the Dagstuhl Seminar (Endres-Niggemeyer et al., 1995)—that the first multi-document news summarization system, SUMMONS (McKeown and Radev, 1995), makes its breakthrough (Radev et al., 2005; Sp¨arck Jones, 2007). In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document summarization (McKeown et al., 2005). In fact, while text summarization has attained some degree of success (Hovy, 2003;</context>
<context position="6697" citStr="Edmundson, 1969" startWordPosition="1003" endWordPosition="1004">ned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003)). On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content. Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Murray et al. (2006). Sentence compaction method</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H. P. 1969. New methods in automatic abstracting. Journal of the Association for Computing Machinery, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Summarizing Text for Intelligent Communication—Dagstuhl-SeminarReport 79.</booktitle>
<editor>Endres-Niggemeyer, B., J. R. Hobbs, and K. Sp¨arck Jones, editors.</editor>
<publisher>IBFI.</publisher>
<marker>1995</marker>
<rawString>Endres-Niggemeyer, B., J. R. Hobbs, and K. Sp¨arck Jones, editors. 1995. Summarizing Text for Intelligent Communication—Dagstuhl-SeminarReport 79. IBFI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
</authors>
<title>Recent Advances in Automatic Speech Summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th Conference on Recherche d’Information Assist´ee par Ordinateur (RIAO). Centre des Hautes ´Etudes Internationales d’Informatique Documentaire.</booktitle>
<contexts>
<context position="2904" citStr="Furui, 2007" startWordPosition="413" endWordPosition="415"> makes its breakthrough (Radev et al., 2005; Sp¨arck Jones, 2007). In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document summarization (McKeown et al., 2005). In fact, while text summarization has attained some degree of success (Hovy, 2003; McKeown et al., 2005; Sp¨arck Jones, 2007) due to the considerable body of work, speech summarization still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003). We argue that by including related solid background information from a different source less </context>
<context position="6348" citStr="Furui (2007)" startWordPosition="958" endWordPosition="959">ocument. 2 Related Work McKeown et al. (2005) depict spoken language summarization as a much harder task than text summarization. In fact, the previously enumerated problems that make speech summarization such a difficult task constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003)). On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content. Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al.</context>
</contexts>
<marker>Furui, 2007</marker>
<rawString>Furui, S. 2007. Recent Advances in Automatic Speech Summarization. In Proceedings of the 8th Conference on Recherche d’Information Assist´ee par Ordinateur (RIAO). Centre des Hautes ´Etudes Internationales d’Informatique Documentaire.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gong</author>
<author>X Liu</author>
</authors>
<title>Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis.</title>
<date>2001</date>
<booktitle>In SIGIR 2001: Proceedings of the 24it Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>pages</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6592" citStr="Gong and Liu, 2001" startWordPosition="988" endWordPosition="991">the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003)). On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content. Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such</context>
<context position="17177" citStr="Gong and Liu (2001)" startWordPosition="2701" endWordPosition="2704">tors; Σ is the n x n diagonal matrix of singular values; and, V is the n x n matrix of right singular vectors (only possible if m ≥ n): M = UΣVT The idea behind the method is that the decomposition captures the underlying topics of the document by means of co-occurrence of terms (the latent semantic analysis), and identifies the best representative sentence-like units of each topic. Summary creation can be done by picking the best representatives of the most relevant topics according to a defined strategy. For this summarization process, we implemented a module following the original ideas of Gong and Liu (2001) and the ones of Murray, Renals, and Carletta (2005) for solving dimensionality problems, and using, for matrix operations, the GNU Scientific Library1. 5.4 Experimental Results Our main objective was to understand if it is possible to select relevant information from background information that could improve the quality of speech-to-text summaries. To assess the validity of this hypothesis, five different processes of generating a summary were considered. To better analyze the influence of the background information, all automatic summarization methods are based on the up-to-date LSA method p</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Gong, Y. and X. Liu. 2001. Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis. In SIGIR 2001: Proceedings of the 24it Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 19–25. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hori</author>
<author>C Hori</author>
<author>Y Minami</author>
</authors>
<title>Speech Summarization using Weighted Finite-State Transducers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th EUROSPEECH - INTERSPEECH</booktitle>
<pages>2817--2820</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="3386" citStr="Hori et al., 2003" startWordPosition="489" endWordPosition="492">esearch, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003). We argue that by including related solid background information from a different source less prone to this kind of errors (e.g., a textual source) 33 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33–40 Manchester, August 2008 in the summarization process, we are able to reduce the influence of recognition errors on the resulting summary. To support this argument, we developed a new approach to speech-to-text summarization that combines information from multiple information sources to produce a summ</context>
<context position="7424" citStr="Hori et al., 2003" startWordPosition="1115" endWordPosition="1118">ctural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Murray et al. (2006). Sentence compaction methods are based on word removal from the transcription, with recognition confidence scores playing a major role (Hori et al., 2003). A combination of these two types of methods was developed by Kikuchi et al. (2003), where summarization is performed in two steps: first, sentence extraction is done through feature combination; second, compaction is done by scoring the words in each sentence and then a dynamic programming technique is applied to select the words that will remain in the sentence to be included in the summary. 3 Problem Characterization Summarization can be seen as a reductive transformation 0 that, given an input source I, produces a summary S: S = 0(I), where len(S) &lt; len(I) and inf (S) is as close as possi</context>
</contexts>
<marker>Hori, Hori, Minami, 2003</marker>
<rawString>Hori, T., C. Hori, and Y. Minami. 2003. Speech Summarization using Weighted Finite-State Transducers. In Proceedings of the 8th EUROSPEECH - INTERSPEECH 2003, pages 2817–2820. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics, chapter Text Summarization,</booktitle>
<pages>583--598</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2641" citStr="Hovy, 2003" startWordPosition="370" endWordPosition="371">dson (1969), it is only after the renaissance of summarization as a research area of great activity—following up on the Dagstuhl Seminar (Endres-Niggemeyer et al., 1995)—that the first multi-document news summarization system, SUMMONS (McKeown and Radev, 1995), makes its breakthrough (Radev et al., 2005; Sp¨arck Jones, 2007). In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document summarization (McKeown et al., 2005). In fact, while text summarization has attained some degree of success (Hovy, 2003; McKeown et al., 2005; Sp¨arck Jones, 2007) due to the considerable body of work, speech summarization still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), o</context>
</contexts>
<marker>Hovy, 2003</marker>
<rawString>Hovy, E., 2003. The Oxford Handbook of Computational Linguistics, chapter Text Summarization, pages 583–598. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kessler</author>
</authors>
<title>Phonetic comparison algorithms.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="10653" citStr="Kessler (2005)" startWordPosition="1663" endWordPosition="1664">e phonetic level. The estimation of the threshold is based on the distance, measured in the phonetic domain, between the output of the ASR and its hand-corrected version. The selection of sentences from the background information is based on the alignment cost of the phonetic transcriptions of sentences from the input source and sentence from the background information. Sentences from the background information with alignment costs below the estimated threshold are selected to be used in summary generation. 4.1 Similarity Between Segments There are several ways to compute phonetic similarity. Kessler (2005) states that phonetic distance can be seen as, among other things, differences between acoustic properties of the speech stream, differences in the articulatory positions during production, or as the perceptual distance between isolated sounds. Choosing a way to calculate phonetic distance is a complex process. The phone similarity function used in this process is based on a model of phone production, where the phone features correspond to the articulatory positions during production: the greater the matching between phone features, the smaller the distance between phones. The phone features u</context>
</contexts>
<marker>Kessler, 2005</marker>
<rawString>Kessler, B. 2005. Phonetic comparison algorithms. Transactions of the Philological Society, 103(2):243–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kikuchi</author>
<author>S Furui</author>
<author>C Hori</author>
</authors>
<title>Twostage Automatic Speech Summarization by Sentence Extraction and Compaction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition (SSPR-2003),</booktitle>
<pages>207--210</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="3409" citStr="Kikuchi et al., 2003" startWordPosition="493" endWordPosition="496">eech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003). We argue that by including related solid background information from a different source less prone to this kind of errors (e.g., a textual source) 33 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33–40 Manchester, August 2008 in the summarization process, we are able to reduce the influence of recognition errors on the resulting summary. To support this argument, we developed a new approach to speech-to-text summarization that combines information from multiple information sources to produce a summary driven by the spoke</context>
<context position="7508" citStr="Kikuchi et al. (2003)" startWordPosition="1130" endWordPosition="1133"> news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Murray et al. (2006). Sentence compaction methods are based on word removal from the transcription, with recognition confidence scores playing a major role (Hori et al., 2003). A combination of these two types of methods was developed by Kikuchi et al. (2003), where summarization is performed in two steps: first, sentence extraction is done through feature combination; second, compaction is done by scoring the words in each sentence and then a dynamic programming technique is applied to select the words that will remain in the sentence to be included in the summary. 3 Problem Characterization Summarization can be seen as a reductive transformation 0 that, given an input source I, produces a summary S: S = 0(I), where len(S) &lt; len(I) and inf (S) is as close as possible of inf (I); len() is the length of the given input and inf () is the information</context>
</contexts>
<marker>Kikuchi, Furui, Hori, 2003</marker>
<rawString>Kikuchi, T., S. Furui, and C. Hori. 2003. Twostage Automatic Speech Summarization by Sentence Extraction and Compaction. In Proceedings of the ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition (SSPR-2003), pages 207–210. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="18967" citStr="Lin, 2004" startWordPosition="2991" endWordPosition="2992">ies using a compression rate of around 10% of the original size). As mentioned before, the whole summarization process begins with the selection of the background information. Using the threshold estimated as described in section 4.2 and the method described in section 4.1 to compute similarity between sentence-like units, no background information was selected for 11 of the 26 news stories of the test corpus. For the remaining 15 news stories, summaries were generated using the three automatic summarization strategies described before. In what concerns the evaluation process, although ROUGE (Lin, 2004) is the most common evaluation metric for the automatic evaluation of summarization, since our approach might introduce in the summary information that it is not present in the original input source, we found that a human evaluation was more adequate to assess the relevance of that additional information. A perceptual evaluation is also adequate to assess the perceive quality of the summaries and a better indicator of the what is expected to be in a summary. We asked an heterogeneous group of sixteen people to evaluate the summaries created for the 15 news stories for which background informat</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, C. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of Literature Abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2021" citStr="Luhn (1958)" startWordPosition="276" endWordPosition="277"> the importance of both the subject and the process. Systems like NewsInEssence (Radev et al., 2005), Newsblaster (McKeown et al., 2002), or even Google © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005). Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research area of great activity—following up on the Dagstuhl Seminar (Endres-Niggemeyer et al., 1995)—that the first multi-document news summarization system, SUMMONS (McKeown and Radev, 1995), makes its breakthrough (Radev et al., 2005; Sp¨arck Jones, 2007). In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document summarization (McKeown et al., 2005). In fact, while text summarization has attained some degree of</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn, H. P. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Comparing Lexical, Acoustic/Prosodic, Strucural and Discourse Features for Speech Summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th EUROSPEECH - INTERSPEECH</booktitle>
<pages>621--624</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="6933" citStr="Maskey and Hirschberg, 2005" startWordPosition="1033" endWordPosition="1036">o ascertain relevant content. Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Murray et al. (2006). Sentence compaction methods are based on word removal from the transcription, with recognition confidence scores playing a major role (Hori et al., 2003). A combination of these two types of methods was developed by Kikuchi et al. (2003), where summarization is </context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>Maskey, S. and J. Hirschberg. 2005. Comparing Lexical, Acoustic/Prosodic, Strucural and Discourse Features for Speech Summarization. In Proceedings of the 9th EUROSPEECH - INTERSPEECH 2005, pages 621–624. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>D Radev</author>
</authors>
<title>Generating Summaries of Multiple News Articles.</title>
<date>1995</date>
<booktitle>In SIGIR 1995: Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2291" citStr="McKeown and Radev, 1995" startWordPosition="314" endWordPosition="317">http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005). Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research area of great activity—following up on the Dagstuhl Seminar (Endres-Niggemeyer et al., 1995)—that the first multi-document news summarization system, SUMMONS (McKeown and Radev, 1995), makes its breakthrough (Radev et al., 2005; Sp¨arck Jones, 2007). In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document summarization (McKeown et al., 2005). In fact, while text summarization has attained some degree of success (Hovy, 2003; McKeown et al., 2005; Sp¨arck Jones, 2007) due to the considerable body of work, speech summarization still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005;</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>McKeown, K. R. and D. Radev. 1995. Generating Summaries of Multiple News Articles. In SIGIR 1995: Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74–82. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>A Nenkova</author>
<author>C Sable</author>
<author>B Schiffman</author>
<author>S Sigelman</author>
</authors>
<title>Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proc. of the 2nd International Conference on Human Language Technology Research,</booktitle>
<pages>280--285</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="1546" citStr="McKeown et al., 2002" startWordPosition="210" endWordPosition="213">ground information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information. Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source. 1 Introduction News have been the subject of summarization for a long time, demonstrating the importance of both the subject and the process. Systems like NewsInEssence (Radev et al., 2005), Newsblaster (McKeown et al., 2002), or even Google © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005). Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research area of great activity—following up on</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>McKeown, K. R., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman, and S. Sigelman. 2002. Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster. In Proc. of the 2nd International Conference on Human Language Technology Research, pages 280–285. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>J Hirschberg</author>
<author>M Galley</author>
<author>S Maskey</author>
</authors>
<title>From Text to Speech Summarization. In</title>
<date>2005</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings,</booktitle>
<volume>volume V,</volume>
<pages>997--1000</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1922" citStr="McKeown et al., 2005" startWordPosition="257" endWordPosition="260">n the input source. 1 Introduction News have been the subject of summarization for a long time, demonstrating the importance of both the subject and the process. Systems like NewsInEssence (Radev et al., 2005), Newsblaster (McKeown et al., 2002), or even Google © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005). Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research area of great activity—following up on the Dagstuhl Seminar (Endres-Niggemeyer et al., 1995)—that the first multi-document news summarization system, SUMMONS (McKeown and Radev, 1995), makes its breakthrough (Radev et al., 2005; Sp¨arck Jones, 2007). In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document </context>
<context position="5781" citStr="McKeown et al. (2005)" startWordPosition="871" endWordPosition="874"> provide the background information. Media monitoring systems, used to transcribe and disseminate news, provide an adequate framework to test the proposed method. This document is organized as follows: section 2 briefly introduces the related work; section 3 presents a characterization of the speech-to-text summarization problem and how we propose to address it; section 4 explicits our use of phonetic domain information, given the previously defined context; the next section describes the case study, including the experimental set up and results; conclusions close the document. 2 Related Work McKeown et al. (2005) depict spoken language summarization as a much harder task than text summarization. In fact, the previously enumerated problems that make speech summarization such a difficult task constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003)). On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content. Furui (2007) identifies three main approaches</context>
<context position="23823" citStr="McKeown et al. (2005)" startWordPosition="3764" endWordPosition="3768">summary should contain information that is not present in the input source. This aspect and the obtained results, suggest that this issue should be further analyzed, possibly using an extrinsic evaluation setup. On the other hand, readability standard deviation scores show that there is a considerable agreement in what concerns this criterion. Figure 6: Average and standard deviation of the content and readability scores for one news story. 6 Conclusions We present a new approach to speech summarization that goes in the direction of the integration of text and speech analysis, as suggested by McKeown et al. (2005). The main idea is the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech-to-text summarization. In this work, we explore the possibilities offered by phonetic information to select the background information and conducted a perceptual evaluation to assess the relevance of the inclusion of that information. The results obtained show that the human evaluators preferred human extractive summaries over human abstractive summaries. Moreover, simple LSA summa</context>
</contexts>
<marker>McKeown, Hirschberg, Galley, Maskey, 2005</marker>
<rawString>McKeown, K. R., J. Hirschberg, M. Galley, and S. Maskey. 2005. From Text to Speech Summarization. In 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings, volume V, pages 997–1000. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<date>1997</date>
<booktitle>Finite-State Transducers in Language and Speech Processing. Computational Linguistics,</booktitle>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="11588" citStr="Mohri, 1997" startWordPosition="1808" endWordPosition="1809">imilarity function used in this process is based on a model of phone production, where the phone features correspond to the articulatory positions during production: the greater the matching between phone features, the smaller the distance between phones. The phone features used are described in table 1. The computation of the similarity between sentence-like units is based on the alignment of the phonetic transcriptions of the given segments. The generation of the possible alignments and the selection of the best alignment is done through the use of Weighted Finite-State Transducers (WFSTs) (Mohri, 1997; Paulo and Oliveira, 2002). 35 4.2 Threshold Estimation Process To estimate the threshold to be used in the sentence selection process, we use the algorithm presented in figure 1. The procedure consists of comparing automatic transcriptions and their hand-corrected versions: the output is the average difference between the submitted inputs. Figure 1: Threshold estimation process. The idea is that the phonetic distance between the automatic transcription and its hand-corrected version would be similar to the phonetic distance between the automatic transcription and the background information. </context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mohri, M. 1997. Finite-State Transducers in Language and Speech Processing. Computational Linguistics, 23(2):269–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
</authors>
<title>Extractive Summarization of Meeting Records.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th EUROSPEECH - INTERSPEECH</booktitle>
<pages>593--596</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="3238" citStr="Murray et al., 2005" startWordPosition="465" endWordPosition="468"> of success (Hovy, 2003; McKeown et al., 2005; Sp¨arck Jones, 2007) due to the considerable body of work, speech summarization still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003). We argue that by including related solid background information from a different source less prone to this kind of errors (e.g., a textual source) 33 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33–40 Manchester, August 2008 in the summarization process, we are able to reduce the influence of recognition errors on the resulting summary. To support this</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>Murray, G., S. Renals, and J. Carletta. 2005. Extractive Summarization of Meeting Records. In Proceedings of the 9th EUROSPEECH - INTERSPEECH 2005, pages 593–596. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
<author>J Moore</author>
</authors>
<title>Incorporating Speaker and Discourse Features into Speech Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT/NAACL,</booktitle>
<pages>367--374</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3216" citStr="Murray et al., 2006" startWordPosition="461" endWordPosition="464"> attained some degree of success (Hovy, 2003; McKeown et al., 2005; Sp¨arck Jones, 2007) due to the considerable body of work, speech summarization still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003). We argue that by including related solid background information from a different source less prone to this kind of errors (e.g., a textual source) 33 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33–40 Manchester, August 2008 in the summarization process, we are able to reduce the influence of recognition errors on the resulting su</context>
<context position="6954" citStr="Murray et al., 2006" startWordPosition="1037" endWordPosition="1040"> Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both. Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969). Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007). Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Murray et al. (2006). Sentence compaction methods are based on word removal from the transcription, with recognition confidence scores playing a major role (Hori et al., 2003). A combination of these two types of methods was developed by Kikuchi et al. (2003), where summarization is performed in two step</context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2006</marker>
<rawString>Murray, G., S. Renals, J. Carletta, and J. Moore. 2006. Incorporating Speaker and Discourse Features into Speech Summarization. In Proceedings of the HLT/NAACL, pages 367–374. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
</authors>
<title>Summarization Evaluation for Text and Speech: Issues and Approaches.</title>
<date>2006</date>
<booktitle>In Proceedings of INTERSPEECH 2006 - ICSLP,</booktitle>
<pages>1527--1530</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="20175" citStr="Nenkova, 2006" startWordPosition="3180" endWordPosition="3181">formation 1http://www.gnu.org/software/gsl/ 37 Figure 2: Overall results for each summary creation method (nsnn identifies a news story). was selected. Each evaluator was given, for each story, the news story itself (without background information) and five summaries, corresponding to the five different methods presented before. The evaluation procedure consisted in identifying the best summary and in the classification of each summary (1–5, 5 is better) according to its content and readability (which covers issues like grammaticality, existence of redundant information, or entity references (Nenkova, 2006)). Figure 3: Relative results for each news story (nsnn identifies a news story; stack order is inverse of legend order). Surprisingly enough (see figures 2 and 3), in general, the extractive human summaries were preferred over the abstractive ones. Moreover, the summaries generated automatically using background information (exclusively or not) were also selected as best summary (over the human created ones) a non-negligible number of times. The poorest performance was attained, as expected, by the simple LSA summarizer, only preferred on two news stories for which all summaries were very sim</context>
</contexts>
<marker>Nenkova, 2006</marker>
<rawString>Nenkova, A. 2006. Summarization Evaluation for Text and Speech: Issues and Approaches. In Proceedings of INTERSPEECH 2006 - ICSLP, pages 1527–1530. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Paulo</author>
<author>L C Oliveira</author>
</authors>
<title>Multilevel Annotation Of Speech Signals Using Weighted Finite State Transducers.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 IEEE Workshop on Speech Synthesis,</booktitle>
<pages>111--114</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11615" citStr="Paulo and Oliveira, 2002" startWordPosition="1810" endWordPosition="1813">ction used in this process is based on a model of phone production, where the phone features correspond to the articulatory positions during production: the greater the matching between phone features, the smaller the distance between phones. The phone features used are described in table 1. The computation of the similarity between sentence-like units is based on the alignment of the phonetic transcriptions of the given segments. The generation of the possible alignments and the selection of the best alignment is done through the use of Weighted Finite-State Transducers (WFSTs) (Mohri, 1997; Paulo and Oliveira, 2002). 35 4.2 Threshold Estimation Process To estimate the threshold to be used in the sentence selection process, we use the algorithm presented in figure 1. The procedure consists of comparing automatic transcriptions and their hand-corrected versions: the output is the average difference between the submitted inputs. Figure 1: Threshold estimation process. The idea is that the phonetic distance between the automatic transcription and its hand-corrected version would be similar to the phonetic distance between the automatic transcription and the background information. Even though this heuristic </context>
</contexts>
<marker>Paulo, Oliveira, 2002</marker>
<rawString>Paulo, S. and L. C. Oliveira. 2002. Multilevel Annotation Of Speech Signals Using Weighted Finite State Transducers. In Proc. of the 2002 IEEE Workshop on Speech Synthesis, pages 111–114. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>J Otterbacher</author>
<author>A Winkel</author>
<author>S BlairGoldensohn</author>
</authors>
<title>NewsInEssence: Summarizing Online News Topics.</title>
<date>2005</date>
<journal>Communications of the ACM,</journal>
<volume>48</volume>
<issue>10</issue>
<contexts>
<context position="1510" citStr="Radev et al., 2005" startWordPosition="204" endWordPosition="207">tic information to select the background information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information. Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source. 1 Introduction News have been the subject of summarization for a long time, demonstrating the importance of both the subject and the process. Systems like NewsInEssence (Radev et al., 2005), Newsblaster (McKeown et al., 2002), or even Google © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005). Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research ar</context>
</contexts>
<marker>Radev, Otterbacher, Winkel, BlairGoldensohn, 2005</marker>
<rawString>Radev, D., J. Otterbacher, A. Winkel, and S. BlairGoldensohn. 2005. NewsInEssence: Summarizing Online News Topics. Communications of the ACM, 48(10):95–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ribeiro</author>
<author>D M de Matos</author>
</authors>
<title>Extractive Summarization of Broadcast News: Comparing Strategies for European Portuguese.</title>
<date>2007</date>
<booktitle>In Text, Speech and Dialogue – 10th International Conference. Proceedings,</booktitle>
<volume>4629</volume>
<pages>115--122</pages>
<publisher>Springer.</publisher>
<marker>Ribeiro, de Matos, 2007</marker>
<rawString>Ribeiro, R. and D. M. de Matos. 2007. Extractive Summarization of Broadcast News: Comparing Strategies for European Portuguese. In Text, Speech and Dialogue – 10th International Conference. Proceedings, volume 4629 of Lecture Notes in Computer Science (Subseries LNAI), pages 115–122. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sp¨arck Jones</author>
<author>K</author>
</authors>
<title>Automatic summarising: The state of the art.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--1449</pages>
<marker>Jones, K, 2007</marker>
<rawString>Sp¨arck Jones, K. 2007. Automatic summarising: The state of the art. Information Processing and Management, 43:1449–1481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>CollabSum: Exploiting Multiple Document Clustering for Collaborative Single Document Summarizations.</title>
<date>2007</date>
<booktitle>In SIGIR 2007: Proc. of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>143--150</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4219" citStr="Wan et al., 2007" startWordPosition="622" endWordPosition="625">on Multi-source Multilingual Information Extraction and Summarization, pages 33–40 Manchester, August 2008 in the summarization process, we are able to reduce the influence of recognition errors on the resulting summary. To support this argument, we developed a new approach to speech-to-text summarization that combines information from multiple information sources to produce a summary driven by the spoken language document to be summarized. The idea mimics the natural human behavior, in which information acquired from different sources is used to build a better understanding of a given topic (Wan et al., 2007). Furthermore, we build on the conjecture that this background information is often used by humans to overcome perception difficulties. In that sense, one of our goals is also to understand what is expected in a summary: a comprehensive, shorter, text that addresses the same subject of the input source to be summarized (possibly introducing new information); or a text restricted to the information conveyed in the input source. This work explores the use of phonetic domain information to overcome speech recognition errors and disfluencies. Instead of using the traditional output of the ASR modu</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Wan, X., J. Yang, and J. Xiao. 2007. CollabSum: Exploiting Multiple Document Clustering for Collaborative Single Document Summarizations. In SIGIR 2007: Proc. of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 143–150. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>A Waibel</author>
</authors>
<title>Minimizing Word Error Rate in Textual Summaries of Spoken Language.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1it conference of the North American chapter of the ACL,</booktitle>
<pages>186--193</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3367" citStr="Zechner and Waibel, 2000" startWordPosition="485" endWordPosition="488">n still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007). Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language. However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003). We argue that by including related solid background information from a different source less prone to this kind of errors (e.g., a textual source) 33 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33–40 Manchester, August 2008 in the summarization process, we are able to reduce the influence of recognition errors on the resulting summary. To support this argument, we developed a new approach to speech-to-text summarization that combines information from multiple information source</context>
</contexts>
<marker>Zechner, Waibel, 2000</marker>
<rawString>Zechner, K. and A. Waibel. 2000. Minimizing Word Error Rate in Textual Summaries of Spoken Language. In Proceedings of the 1it conference of the North American chapter of the ACL, pages 186–193. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>