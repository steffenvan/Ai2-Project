<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062352">
<title confidence="0.99808">
Comparing the roles of textual, acoustic and spoken-language features
on spontaneous-conversation summarization
</title>
<author confidence="0.999288">
Xiaodan Zhu Gerald Penn
</author>
<affiliation confidence="0.9998">
Department of Computer Science, University of Toronto
</affiliation>
<address confidence="0.985579">
10 Kings College Rd., Toronto, Canada
</address>
<email confidence="0.99864">
{xzhu, gpenn} @cs.toronto.edu
</email>
<sectionHeader confidence="0.993402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995072555555556">
This paper is concerned with the
summarization of spontaneous
conversations. Compared with broadcast
news, which has received intensive study,
spontaneous conversations have been less
addressed in the literature. Previous
work has focused on textual features
extracted from transcripts. This paper
explores and compares the effectiveness
of both textual features and speech-
related features. The experiments show
that these features incrementally improve
summarization performance. We also find
that speech disfluencies, which have been
removed as noise in previous work, help
identify important utterances, while the
structural feature is less effective than it
is in broadcast news.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995972881356">
Spontaneous conversations are a very important
type of speech data. Distilling important
information from them has commercial and other
importance. Compared with broadcast news, which
has received the most intensive studies (Hori and
Furui, 2003; Christensen et al. 2004; Maskey and
Hirschberg, 2005), spontaneous conversations have
been less addressed in the literature.
Spontaneous conversations are different from
broadcast news in several aspects: (1) spontaneous
conversations are often less well formed
linguistically, e.g., containing more speech
disfluencies and false starts; (2) the distribution of
important utterances in spontaneous conversations
could be different from that in broadcast news, e.g.,
the beginning part of news often contains
important information, but in conversations,
information may be more evenly distributed; (3)
conversations often contain discourse clues, e.g.,
question-answer pairs and speakers’ information,
which can be utilized to keep the summary
coherent; (4) word error rates (WERs) from speech
recognition are usually much higher in
spontaneous conversations.
Previous work on spontaneous-conversation
summarization has mainly focused on textual
features (Zechner, 2001; Gurevych and Strube,
2004), while speech-related features have not been
explored for this type of speech source. This paper
explores and compares the effectiveness of both
textual features and speech-related features. The
experiments show that these features incrementally
improve summarization performance. We also
discuss problems (1) and (2) mentioned above. For
(1), Zechner (2001) proposes to detect and remove
false starts and speech disfluencies from transcripts,
in order to make the text-format summary concise
and more readable. Nevertheless, it is not always
necessary to remove them. One reason is that
original utterances are often more desired to ensure
comprehensibility and naturalness if the summaries
are to be delivered as excerpts of audio (see section
2), in order to avoid the impact of WER. Second,
disfluencies are not necessarily noise; instead, they
show regularities in a number of dimensions
(Shriberg, 1994), and correlate with many factors
including topic difficulty (Bortfeld et al, 2001).
Rather than removing them, we explore the effects
of disfluencies on summarization, which, to our
knowledge, has not yet been addressed in the
literature. Our experiments show that they improve
summarization performance.
To discuss problem (2), we explore and compare
both textual features and speech-related features,
as they are explored in broadcast news (Maskey
and Hirschberg, 2005). The experiments show that
the structural feature (e.g. utterance position) is
less effective for summarizing spontaneous
conversations than it is in broadcast news. MMR
</bodyText>
<page confidence="0.96474">
197
</page>
<note confidence="0.86975">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197–200,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.98576615">
and lexical features are the best. Speech-related
features follow. The structural feature is least
effective. We do not discuss problem (3) and (4) in
this paper. For problem (3), a similar idea has been
proposed to summarize online blogs and
discussions. Problem (4) has been partially
addressed by (Zechner &amp; Waibel, 2000); but it has
not been studied together with acoustic features.
2 Utterance-extraction-based
summarization
Still at its early stage, current research on speech
summarization targets a less ambitious goal:
conducting extractive, single-document, generic,
and surface-level-feature-based summarization.
The pieces to be extracted could correspond to
words (Koumpis, 2002; Hori and Furui, 2003). The
extracts could be utterances, too. Utterance
selection is useful. First, it could be a preliminary
stage applied before word extraction, as proposed
by Kikuchi et al. (2003) in their two-stage
summarizer. Second, with utterance-level extracts,
one can play the corresponding audio to users, as
with the speech-to-speech summarizer discussed in
Furui et al. (2003). The advantage of outputting
audio segments rather than transcripts is that it
avoids the impact of WERs caused by automatic
speech recognition (ASR). We will focus on
utterance-level extraction, which at present appears
to be the only way to ensure comprehensibility and
naturalness if the summaries are to be delivered as
excerpts of audio themselves.
Previous work on spontaneous conversations
mainly focuses on using textual features. Gurevych
&amp; Strube (2004) develop a shallow knowledge-
based approach. The noun portion of WordNet is
used as a knowledge source. The noun senses were
manually disambiguated rather than automatically.
Zechner (2001) applies maximum marginal
relevance (MMR) to select utterances for
spontaneous conversation transcripts.
</bodyText>
<sectionHeader confidence="0.9947255" genericHeader="method">
3 Classification based utterance
extraction
</sectionHeader>
<bodyText confidence="0.9997394375">
Spontaneous conversations contain more
information than textual features. To utilize these
features, we reformulate the utterance selection
task as a binary classification problem, an
utterance is either labeled as “1” (in-summary) or
“0” (not-in-summary). Two state-of-the-art
classifiers, support vector machine (SVM) and
logistic regression (LR), are used. SVM seeks an
optimal separating hyperplane, where the margin is
maximal. In our experiments, we use the OSU-
SVM package. Logistic regression (LR) is indeed a
softmax linear regression, which models the
posterior probabilities of the class label with the
softmax of linear functions of feature vectors. For
the binary classification that we require in our
experiments, the model format is simple.
</bodyText>
<subsectionHeader confidence="0.92697">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.997272">
The features explored in this paper include:
</bodyText>
<listItem confidence="0.98168">
(1) MMR score: the score calculated with MMR
(Zechner, 2001) for each utterance.
(2) Lexicon features: number of named entities,
and utterance length (number of words). The
number of named entities includes: person-
name number, location-name number,
organization-name number, and the total
number. Named entities are annotated
automatically with a dictionary.
(3) Structural features: a value is assigned to
indicate whether a given utterance is in the first,
</listItem>
<bodyText confidence="0.86728125">
middle, or last one-third of the conversation.
Another Boolean value is assigned to indicate
whether this utterance is adjacent to a speaker
turn or not.
</bodyText>
<listItem confidence="0.961146538461539">
(4) Prosodic features: we use basic prosody: the
maximum, minimum, average and range of
energy, as well as those of fundamental
frequency, normalized by speakers. All these
features are automatically extracted.
(5) Spoken-language features: the spoken-language
features include number of repetitions, filled
pauses, and the total number of them.
Disfluencies adjacent to a speaker turn are not
counted, because they are normally used to
coordinate interaction among speakers.
Repetitions and pauses are detected in the same
way as described in Zechner (2001).
</listItem>
<sectionHeader confidence="0.996572" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.977393">
4.1 Experiment settings
</subsectionHeader>
<bodyText confidence="0.999932">
The data used for our experiments come from
SWITCHBOARD. We randomly select 27
conversations, containing around 3660 utterances.
The important utterances of each conversation are
</bodyText>
<page confidence="0.99428">
198
</page>
<bodyText confidence="0.996987222222222">
manually annotated. We use f-score and the
ROUGE score as evaluation metrics. Ten-fold
cross validation is applied to obtain the results
presented in this section.
ROUGE-L show the same tendency, but are not
presented here due to the space limit.
Both the f-score and ROUGE indicate that, in
general, rich features incrementally improve
summarization performance.
</bodyText>
<subsectionHeader confidence="0.844794">
4.2 Summarization performance
</subsectionHeader>
<subsubsectionHeader confidence="0.93916">
4.2.1 F-score
</subsubsectionHeader>
<bodyText confidence="0.97595025">
Table-1 shows the f-score of logistic regression
(LR) based summarizers, under different
compression ratios, and with incremental features
used.
</bodyText>
<table confidence="0.940979833333333">
10% 15% 20% 25% 30%
(1) MMR .246 .309 .346 .355 .368
(2) (1) +lexicon .293 .338 .373 .380 .394
(3) (2)+structure .334 .366 .400 .409 .404
(4) (3)+acoustic .336 .364 .388 .410 .415
(5) (4)+spoken language .333 .376 .410 .431 .422
</table>
<tableCaption confidence="0.998602">
Table 1. f-score of LR summarizers using incremental features
</tableCaption>
<bodyText confidence="0.806082">
Below is the f-score of SVM-based summarizer:
</bodyText>
<table confidence="0.911653333333333">
10% 15% 20% 25% 30%
(1) MMR .246 .309 .346 .355 .368
(2) (1) +lexicon .281 .338 .354 .358 .377
(3) (2)+structural .326 .371 .401 .409 .408
(4) (3)+acoustic .337 .380 .400 .422 .418
(5) (4)+spoken language .353 .380 .416 .424 .423
</table>
<tableCaption confidence="0.993742">
Table 2. f-score of SVM summarizers using incremental features
</tableCaption>
<bodyText confidence="0.973777833333333">
Both tables show that the performance of
summarizers improved, in general, with more
features used. The use of lexicon and structural
features outperforms MMR, and the speech-related
features, acoustic features and spoken language
features produce additional improvements.
</bodyText>
<subsubsectionHeader confidence="0.990787">
4.2.2 ROUGE
</subsubsectionHeader>
<bodyText confidence="0.964043">
The following tables provide the ROUGE-1 scores:
</bodyText>
<table confidence="0.997432333333333">
10% 15% 20% 25% 30%
(1) MMR .585 .563 .523 .492 .467
(2) (1) +lexicon .602 .579 .543 .506 .476
(3) (2)+structure .621 .591 .553 .516 .482
(4) (3)+acoustic .619 .594 .554 .519 .485
(5) (4)+spoken language .619 .600 .566 .530 .492
</table>
<tableCaption confidence="0.989473">
Table 3. ROUGE-1 of LR summarizers using incremental features
</tableCaption>
<table confidence="0.998099">
10% 15% 20% 25% 30%
(1) MMR .585 .563 .523 .492 .467
(2) (1) +lexicon .604 .581 .542 .504 .577
(3) (2)+structure .617 .600 .563 .523 .490
(4) (3)+acoustic .629 .610 .573 .533 .496
(5)(4)+spoken language .628 .611 .576 .535 .502
</table>
<tableCaption confidence="0.999773">
Table 4. ROUGE-1 of SVM summarizers using incremental features
</tableCaption>
<bodyText confidence="0.99932225">
The ROUGE-1 scores show similar tendencies to
the f-scores: the rich features improve
summarization performance over the baseline
MMR summarizers. Other ROUGE scores like
</bodyText>
<subsectionHeader confidence="0.999573">
4.3 Comparison of features
</subsectionHeader>
<bodyText confidence="0.992381896551724">
To study the effectiveness of individual features,
the receiver operating characteristic (ROC) curves
of these features are presented in Figure-1 below.
The larger the area under a curve is, the better the
performance of this feature is. To be more exact,
the definition for the y-coordinate (sensitivity) and
the x-coordinate (1-specificity) is:
= = true positive rate
where TP, FN, TN and FP are true positive, false
negative, true negative, and false positive,
respectively.
Figure-1. ROC curves for individual features
Lexicon and MMR features are the best two
individual features, followed by spoken-language
and acoustic features. The structural feature is least
effective.
Let us first revisit the problem (2) discussed
above in the introduction. The effectiveness of the
structural feature is less significant than it is in
broadcast news. According to the ROC curves
presented in Christensen et al. (2004), the
structural feature (utterance position) is one of the
best features for summarizing read news stories,
and is less effective when news stories contain
spontaneous speech. Both their ROC curves cover
larger area than the structural feature here in figure
1, that is, the structure feature is less effective for
summarizing spontaneous conversation than it is in
broadcast news. This reflects, to some extent, that
</bodyText>
<figure confidence="0.807920555555556">
sensitivity
specificity
= = truenegtiverate
TN
TN FP
+
TP FN
+
TP
</figure>
<page confidence="0.995684">
199
</page>
<bodyText confidence="0.993765475">
information is more evenly distributed in
spontaneous conversations.
Now let us turn to the role of speech disfluencies,
which are very common in spontaneous
conversations. Previous work detects and removes
disfluencies as noise. Indeed, disfluencies show
regularities in a number of dimensions (Shriberg,
1994). They correlate with many factors including
the topic difficulty (Bortfeld et al, 2001). Tables 1-
4 above show that they improve summarization
performance when added upon other features.
Figure-1 shows that when used individually, they
are better than the structural feature, and also better
than acoustic features at the left 1/3 part of the
figure, where the summary contains relatively
fewer utterances. Disfluencies, e.g., pauses, are
often inserted when speakers have word-searching
problem, e.g., a problem finding topic-specific
keywords:
Speaker A: with all the uh sulfur and all that other
stuff they&apos;re dumping out into the atmosphere.
The above example is taken from a conversation
that discusses pollution. The speaker inserts a filled
pause uh in front of the word sulfur. Pauses are not
randomly inserted. To show this, we remove them
from transcripts. Section-2 of SWITCHBOARD
(about 870 dialogues and 189,000 utterances) is
used for this experiment. Then we insert these
pauses back randomly, or insert them back at their
original places, and compare the difference. For
both cases, we consider a window with 4 words
after each filled pause. We average the tf.idf scores
of the words in each of these windows. Then, for
all speaker-inserted pauses, we obtain a set of
averaged tf.idf scores. And for all randomly-
inserted pauses, we have another set. The mean of
the former set (5.79 in table 5) is statistically
higher than that of the latter set (5.70 in table 5).
We can adjust the window size to 3, 2 and 1, and
then get the following table.
</bodyText>
<table confidence="0.9345092">
Window size 1 2 3 4
Mean of Insert Randomly 5.69 5.69 5.70 5.70
tf.idf score
Insert by speaker 5.72 5.82 5.81 5.79
Difference is significant? (t-test, p&lt;0.05) Yes Yes Yes Yes
</table>
<tableCaption confidence="0.996437">
Table 5. Average tf.idf scores of words following filled pauses.
</tableCaption>
<bodyText confidence="0.999965">
The above table shows that instead of randomly
inserting pauses, real speakers insert them in front
of words with higher tf.idf scores. This helps
explain why disfluencies work.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999082">
Previous work on summarizing spontaneous
conversations has mainly focused on textual
features. This paper explores and compares both
textual and speech-related features. The
experiments show that these features incrementally
improve summarization performance. We also find
that speech disfluencies, which are removed as
noise in previous work, help identify important
utterances, while the structural feature is less
effective than it is in broadcast news.
</bodyText>
<sectionHeader confidence="0.999729" genericHeader="references">
6 References
</sectionHeader>
<reference confidence="0.999101777777778">
Bortfeld, H., Leon, S.D., Bloom, J.E., Schober, M.F., &amp;
Brennan, S.E. 2001. Disfluency Rates in Conversation:
Effects of Age, Relationship, Topic Role, and Gender.
Language and Speech, 44(2): 123-147
Christensen, H., Kolluru, B., Gotoh, Y., Renals, S., 2004.
From text summarisation to style-specific
summarisation for broadcast news. Proc. ECIR-2004.
Furui, S., Kikuichi T. Shinnaka Y., and Hori C. 2003.
Speech-to-speech and speech to text summarization,.
First International workshop on Language
Understanding and Agents for Real World Interaction,
2003.
Gurevych I. and Strube M. 2004. Semantic Similarity
Applied to Spoken Dialogue Summarization. COLING-
2004.
Hori C. and Furui S., 2003. A New Approach to Automatic
Speech Summarization IEEE Transactions on
Multimedia, Vol. 5, NO. 3, September 2003,
Kikuchi T., Furui S. and Hori C., 2003. Automatic Speech
Summarization Based on Sentence Extraction and
Compaction, Proc. ICASSP-2003.
Koumpis K., 2002. Automatic Voicemail Summarisation
for Mobile Messaging Ph.D. Thesis, University of
Sheffield, UK, 2002.
Maskey, S.R., Hirschberg, J. &amp;quot;Comparing Lexial,
Acoustic/Prosodic, Discourse and Structural Features
for Speech Summarization&amp;quot;, Eurospeech 2005.
Shriberg, E.E. (1994). Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, University of California at
Berkeley.
Zechner K. and Waibel A., 2000. Minimizing word error
rate in textual summaries of spoken language. NAACL-
2000.
Zechner K., 2001. Automatic Summarization of Spoken
Dialogues in Unrestricted Domains. Ph.D. thesis,
Carnegie Mellon University, November 2001.
</reference>
<page confidence="0.99668">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970343">
<title confidence="0.998492">Comparing the roles of textual, acoustic and spoken-language on spontaneous-conversation summarization</title>
<author confidence="0.999894">Xiaodan Zhu Gerald Penn</author>
<affiliation confidence="0.999976">Department of Computer Science, University of Toronto</affiliation>
<address confidence="0.999968">10 Kings College Rd., Toronto, Canada</address>
<email confidence="0.99958">xzhu@cs.toronto.edu</email>
<email confidence="0.99958">gpenn@cs.toronto.edu</email>
<abstract confidence="0.998596421052632">This paper is concerned with the summarization of conversations. Compared with broadcast news, which has received intensive study, spontaneous conversations have been less addressed in the literature. Previous work has focused on textual features extracted from transcripts. This paper explores and compares the effectiveness of both textual features and speechrelated features. The experiments show that these features incrementally improve summarization performance. We also find that speech disfluencies, which have been removed as noise in previous work, help identify important utterances, while the structural feature is less effective than it is in broadcast news.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Bortfeld</author>
<author>S D Leon</author>
<author>J E Bloom</author>
<author>M F Schober</author>
<author>S E Brennan</author>
</authors>
<title>Disfluency Rates in Conversation: Effects of Age, Relationship, Topic Role, and Gender. Language and Speech,</title>
<date>2001</date>
<volume>44</volume>
<issue>2</issue>
<pages>123--147</pages>
<contexts>
<context position="3202" citStr="Bortfeld et al, 2001" startWordPosition="442" endWordPosition="445">detect and remove false starts and speech disfluencies from transcripts, in order to make the text-format summary concise and more readable. Nevertheless, it is not always necessary to remove them. One reason is that original utterances are often more desired to ensure comprehensibility and naturalness if the summaries are to be delivered as excerpts of audio (see section 2), in order to avoid the impact of WER. Second, disfluencies are not necessarily noise; instead, they show regularities in a number of dimensions (Shriberg, 1994), and correlate with many factors including topic difficulty (Bortfeld et al, 2001). Rather than removing them, we explore the effects of disfluencies on summarization, which, to our knowledge, has not yet been addressed in the literature. Our experiments show that they improve summarization performance. To discuss problem (2), we explore and compare both textual features and speech-related features, as they are explored in broadcast news (Maskey and Hirschberg, 2005). The experiments show that the structural feature (e.g. utterance position) is less effective for summarizing spontaneous conversations than it is in broadcast news. MMR 197 Proceedings of the Human Language Te</context>
<context position="12148" citStr="Bortfeld et al, 2001" startWordPosition="1786" endWordPosition="1789"> is, the structure feature is less effective for summarizing spontaneous conversation than it is in broadcast news. This reflects, to some extent, that sensitivity specificity = = truenegtiverate TN TN FP + TP FN + TP 199 information is more evenly distributed in spontaneous conversations. Now let us turn to the role of speech disfluencies, which are very common in spontaneous conversations. Previous work detects and removes disfluencies as noise. Indeed, disfluencies show regularities in a number of dimensions (Shriberg, 1994). They correlate with many factors including the topic difficulty (Bortfeld et al, 2001). Tables 1- 4 above show that they improve summarization performance when added upon other features. Figure-1 shows that when used individually, they are better than the structural feature, and also better than acoustic features at the left 1/3 part of the figure, where the summary contains relatively fewer utterances. Disfluencies, e.g., pauses, are often inserted when speakers have word-searching problem, e.g., a problem finding topic-specific keywords: Speaker A: with all the uh sulfur and all that other stuff they&apos;re dumping out into the atmosphere. The above example is taken from a conver</context>
</contexts>
<marker>Bortfeld, Leon, Bloom, Schober, Brennan, 2001</marker>
<rawString>Bortfeld, H., Leon, S.D., Bloom, J.E., Schober, M.F., &amp; Brennan, S.E. 2001. Disfluency Rates in Conversation: Effects of Age, Relationship, Topic Role, and Gender. Language and Speech, 44(2): 123-147</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>B Kolluru</author>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>From text summarisation to style-specific summarisation for broadcast news.</title>
<date>2004</date>
<booktitle>Proc. ECIR-2004.</booktitle>
<contexts>
<context position="1236" citStr="Christensen et al. 2004" startWordPosition="165" endWordPosition="168">textual features and speechrelated features. The experiments show that these features incrementally improve summarization performance. We also find that speech disfluencies, which have been removed as noise in previous work, help identify important utterances, while the structural feature is less effective than it is in broadcast news. 1 Introduction Spontaneous conversations are a very important type of speech data. Distilling important information from them has commercial and other importance. Compared with broadcast news, which has received the most intensive studies (Hori and Furui, 2003; Christensen et al. 2004; Maskey and Hirschberg, 2005), spontaneous conversations have been less addressed in the literature. Spontaneous conversations are different from broadcast news in several aspects: (1) spontaneous conversations are often less well formed linguistically, e.g., containing more speech disfluencies and false starts; (2) the distribution of important utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (3) conversations often co</context>
<context position="11260" citStr="Christensen et al. (2004)" startWordPosition="1650" endWordPosition="1653">te (sensitivity) and the x-coordinate (1-specificity) is: = = true positive rate where TP, FN, TN and FP are true positive, false negative, true negative, and false positive, respectively. Figure-1. ROC curves for individual features Lexicon and MMR features are the best two individual features, followed by spoken-language and acoustic features. The structural feature is least effective. Let us first revisit the problem (2) discussed above in the introduction. The effectiveness of the structural feature is less significant than it is in broadcast news. According to the ROC curves presented in Christensen et al. (2004), the structural feature (utterance position) is one of the best features for summarizing read news stories, and is less effective when news stories contain spontaneous speech. Both their ROC curves cover larger area than the structural feature here in figure 1, that is, the structure feature is less effective for summarizing spontaneous conversation than it is in broadcast news. This reflects, to some extent, that sensitivity specificity = = truenegtiverate TN TN FP + TP FN + TP 199 information is more evenly distributed in spontaneous conversations. Now let us turn to the role of speech disf</context>
</contexts>
<marker>Christensen, Kolluru, Gotoh, Renals, 2004</marker>
<rawString>Christensen, H., Kolluru, B., Gotoh, Y., Renals, S., 2004. From text summarisation to style-specific summarisation for broadcast news. Proc. ECIR-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
<author>Kikuichi T Shinnaka Y</author>
<author>C Hori</author>
</authors>
<title>Speech-to-speech and speech to text summarization,. First International workshop on Language Understanding and Agents for Real World Interaction,</title>
<date>2003</date>
<contexts>
<context position="5032" citStr="Furui et al. (2003)" startWordPosition="707" endWordPosition="710">research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer. Second, with utterance-level extracts, one can play the corresponding audio to users, as with the speech-to-speech summarizer discussed in Furui et al. (2003). The advantage of outputting audio segments rather than transcripts is that it avoids the impact of WERs caused by automatic speech recognition (ASR). We will focus on utterance-level extraction, which at present appears to be the only way to ensure comprehensibility and naturalness if the summaries are to be delivered as excerpts of audio themselves. Previous work on spontaneous conversations mainly focuses on using textual features. Gurevych &amp; Strube (2004) develop a shallow knowledgebased approach. The noun portion of WordNet is used as a knowledge source. The noun senses were manually dis</context>
</contexts>
<marker>Furui, Y, Hori, 2003</marker>
<rawString>Furui, S., Kikuichi T. Shinnaka Y., and Hori C. 2003. Speech-to-speech and speech to text summarization,. First International workshop on Language Understanding and Agents for Real World Interaction, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Gurevych</author>
<author>M Strube</author>
</authors>
<title>Semantic Similarity Applied to Spoken Dialogue Summarization.</title>
<date>2004</date>
<tech>COLING2004.</tech>
<contexts>
<context position="2209" citStr="Gurevych and Strube, 2004" startWordPosition="294" endWordPosition="297">t utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (3) conversations often contain discourse clues, e.g., question-answer pairs and speakers’ information, which can be utilized to keep the summary coherent; (4) word error rates (WERs) from speech recognition are usually much higher in spontaneous conversations. Previous work on spontaneous-conversation summarization has mainly focused on textual features (Zechner, 2001; Gurevych and Strube, 2004), while speech-related features have not been explored for this type of speech source. This paper explores and compares the effectiveness of both textual features and speech-related features. The experiments show that these features incrementally improve summarization performance. We also discuss problems (1) and (2) mentioned above. For (1), Zechner (2001) proposes to detect and remove false starts and speech disfluencies from transcripts, in order to make the text-format summary concise and more readable. Nevertheless, it is not always necessary to remove them. One reason is that original ut</context>
<context position="5496" citStr="Gurevych &amp; Strube (2004)" startWordPosition="777" endWordPosition="780">. Second, with utterance-level extracts, one can play the corresponding audio to users, as with the speech-to-speech summarizer discussed in Furui et al. (2003). The advantage of outputting audio segments rather than transcripts is that it avoids the impact of WERs caused by automatic speech recognition (ASR). We will focus on utterance-level extraction, which at present appears to be the only way to ensure comprehensibility and naturalness if the summaries are to be delivered as excerpts of audio themselves. Previous work on spontaneous conversations mainly focuses on using textual features. Gurevych &amp; Strube (2004) develop a shallow knowledgebased approach. The noun portion of WordNet is used as a knowledge source. The noun senses were manually disambiguated rather than automatically. Zechner (2001) applies maximum marginal relevance (MMR) to select utterances for spontaneous conversation transcripts. 3 Classification based utterance extraction Spontaneous conversations contain more information than textual features. To utilize these features, we reformulate the utterance selection task as a binary classification problem, an utterance is either labeled as “1” (in-summary) or “0” (not-in-summary). Two st</context>
</contexts>
<marker>Gurevych, Strube, 2004</marker>
<rawString>Gurevych I. and Strube M. 2004. Semantic Similarity Applied to Spoken Dialogue Summarization. COLING2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
<author>S Furui</author>
</authors>
<title>A New Approach to Automatic Speech Summarization</title>
<date>2003</date>
<journal>IEEE Transactions on Multimedia,</journal>
<volume>5</volume>
<contexts>
<context position="1211" citStr="Hori and Furui, 2003" startWordPosition="161" endWordPosition="164">effectiveness of both textual features and speechrelated features. The experiments show that these features incrementally improve summarization performance. We also find that speech disfluencies, which have been removed as noise in previous work, help identify important utterances, while the structural feature is less effective than it is in broadcast news. 1 Introduction Spontaneous conversations are a very important type of speech data. Distilling important information from them has commercial and other importance. Compared with broadcast news, which has received the most intensive studies (Hori and Furui, 2003; Christensen et al. 2004; Maskey and Hirschberg, 2005), spontaneous conversations have been less addressed in the literature. Spontaneous conversations are different from broadcast news in several aspects: (1) spontaneous conversations are often less well formed linguistically, e.g., containing more speech disfluencies and false starts; (2) the distribution of important utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (</context>
<context position="4663" citStr="Hori and Furui, 2003" startWordPosition="652" endWordPosition="655">least effective. We do not discuss problem (3) and (4) in this paper. For problem (3), a similar idea has been proposed to summarize online blogs and discussions. Problem (4) has been partially addressed by (Zechner &amp; Waibel, 2000); but it has not been studied together with acoustic features. 2 Utterance-extraction-based summarization Still at its early stage, current research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer. Second, with utterance-level extracts, one can play the corresponding audio to users, as with the speech-to-speech summarizer discussed in Furui et al. (2003). The advantage of outputting audio segments rather than transcripts is that it avoids the impact of WERs caused by automatic speech recognition (ASR). We will focus on utterance-level extraction, which at present appears to be the</context>
</contexts>
<marker>Hori, Furui, 2003</marker>
<rawString>Hori C. and Furui S., 2003. A New Approach to Automatic Speech Summarization IEEE Transactions on Multimedia, Vol. 5, NO. 3, September 2003,</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kikuchi</author>
<author>S Furui</author>
<author>C Hori</author>
</authors>
<title>Automatic Speech Summarization Based on Sentence Extraction and Compaction,</title>
<date>2003</date>
<booktitle>Proc. ICASSP-2003.</booktitle>
<contexts>
<context position="4842" citStr="Kikuchi et al. (2003)" startWordPosition="680" endWordPosition="683">been partially addressed by (Zechner &amp; Waibel, 2000); but it has not been studied together with acoustic features. 2 Utterance-extraction-based summarization Still at its early stage, current research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer. Second, with utterance-level extracts, one can play the corresponding audio to users, as with the speech-to-speech summarizer discussed in Furui et al. (2003). The advantage of outputting audio segments rather than transcripts is that it avoids the impact of WERs caused by automatic speech recognition (ASR). We will focus on utterance-level extraction, which at present appears to be the only way to ensure comprehensibility and naturalness if the summaries are to be delivered as excerpts of audio themselves. Previous work on spontaneous conversations mainly focus</context>
</contexts>
<marker>Kikuchi, Furui, Hori, 2003</marker>
<rawString>Kikuchi T., Furui S. and Hori C., 2003. Automatic Speech Summarization Based on Sentence Extraction and Compaction, Proc. ICASSP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koumpis</author>
</authors>
<title>Automatic Voicemail Summarisation for Mobile Messaging Ph.D. Thesis,</title>
<date>2002</date>
<location>University of Sheffield, UK,</location>
<contexts>
<context position="4640" citStr="Koumpis, 2002" startWordPosition="650" endWordPosition="651">ral feature is least effective. We do not discuss problem (3) and (4) in this paper. For problem (3), a similar idea has been proposed to summarize online blogs and discussions. Problem (4) has been partially addressed by (Zechner &amp; Waibel, 2000); but it has not been studied together with acoustic features. 2 Utterance-extraction-based summarization Still at its early stage, current research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer. Second, with utterance-level extracts, one can play the corresponding audio to users, as with the speech-to-speech summarizer discussed in Furui et al. (2003). The advantage of outputting audio segments rather than transcripts is that it avoids the impact of WERs caused by automatic speech recognition (ASR). We will focus on utterance-level extraction, which at pr</context>
</contexts>
<marker>Koumpis, 2002</marker>
<rawString>Koumpis K., 2002. Automatic Voicemail Summarisation for Mobile Messaging Ph.D. Thesis, University of Sheffield, UK, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Comparing Lexial, Acoustic/Prosodic, Discourse and Structural Features for Speech Summarization&amp;quot;,</title>
<date>2005</date>
<location>Eurospeech</location>
<contexts>
<context position="1266" citStr="Maskey and Hirschberg, 2005" startWordPosition="169" endWordPosition="172">chrelated features. The experiments show that these features incrementally improve summarization performance. We also find that speech disfluencies, which have been removed as noise in previous work, help identify important utterances, while the structural feature is less effective than it is in broadcast news. 1 Introduction Spontaneous conversations are a very important type of speech data. Distilling important information from them has commercial and other importance. Compared with broadcast news, which has received the most intensive studies (Hori and Furui, 2003; Christensen et al. 2004; Maskey and Hirschberg, 2005), spontaneous conversations have been less addressed in the literature. Spontaneous conversations are different from broadcast news in several aspects: (1) spontaneous conversations are often less well formed linguistically, e.g., containing more speech disfluencies and false starts; (2) the distribution of important utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (3) conversations often contain discourse clues, e.g., q</context>
<context position="3591" citStr="Maskey and Hirschberg, 2005" startWordPosition="499" endWordPosition="502">order to avoid the impact of WER. Second, disfluencies are not necessarily noise; instead, they show regularities in a number of dimensions (Shriberg, 1994), and correlate with many factors including topic difficulty (Bortfeld et al, 2001). Rather than removing them, we explore the effects of disfluencies on summarization, which, to our knowledge, has not yet been addressed in the literature. Our experiments show that they improve summarization performance. To discuss problem (2), we explore and compare both textual features and speech-related features, as they are explored in broadcast news (Maskey and Hirschberg, 2005). The experiments show that the structural feature (e.g. utterance position) is less effective for summarizing spontaneous conversations than it is in broadcast news. MMR 197 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197–200, New York, June 2006. c�2006 Association for Computational Linguistics and lexical features are the best. Speech-related features follow. The structural feature is least effective. We do not discuss problem (3) and (4) in this paper. For problem (3), a similar idea has been proposed to summarize online blogs and</context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>Maskey, S.R., Hirschberg, J. &amp;quot;Comparing Lexial, Acoustic/Prosodic, Discourse and Structural Features for Speech Summarization&amp;quot;, Eurospeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="3119" citStr="Shriberg, 1994" startWordPosition="432" endWordPosition="433">ss problems (1) and (2) mentioned above. For (1), Zechner (2001) proposes to detect and remove false starts and speech disfluencies from transcripts, in order to make the text-format summary concise and more readable. Nevertheless, it is not always necessary to remove them. One reason is that original utterances are often more desired to ensure comprehensibility and naturalness if the summaries are to be delivered as excerpts of audio (see section 2), in order to avoid the impact of WER. Second, disfluencies are not necessarily noise; instead, they show regularities in a number of dimensions (Shriberg, 1994), and correlate with many factors including topic difficulty (Bortfeld et al, 2001). Rather than removing them, we explore the effects of disfluencies on summarization, which, to our knowledge, has not yet been addressed in the literature. Our experiments show that they improve summarization performance. To discuss problem (2), we explore and compare both textual features and speech-related features, as they are explored in broadcast news (Maskey and Hirschberg, 2005). The experiments show that the structural feature (e.g. utterance position) is less effective for summarizing spontaneous conve</context>
<context position="12060" citStr="Shriberg, 1994" startWordPosition="1775" endWordPosition="1776">ir ROC curves cover larger area than the structural feature here in figure 1, that is, the structure feature is less effective for summarizing spontaneous conversation than it is in broadcast news. This reflects, to some extent, that sensitivity specificity = = truenegtiverate TN TN FP + TP FN + TP 199 information is more evenly distributed in spontaneous conversations. Now let us turn to the role of speech disfluencies, which are very common in spontaneous conversations. Previous work detects and removes disfluencies as noise. Indeed, disfluencies show regularities in a number of dimensions (Shriberg, 1994). They correlate with many factors including the topic difficulty (Bortfeld et al, 2001). Tables 1- 4 above show that they improve summarization performance when added upon other features. Figure-1 shows that when used individually, they are better than the structural feature, and also better than acoustic features at the left 1/3 part of the figure, where the summary contains relatively fewer utterances. Disfluencies, e.g., pauses, are often inserted when speakers have word-searching problem, e.g., a problem finding topic-specific keywords: Speaker A: with all the uh sulfur and all that other</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Shriberg, E.E. (1994). Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>A Waibel</author>
</authors>
<title>Minimizing word error rate in textual summaries of spoken language.</title>
<date>2000</date>
<pages>2000</pages>
<contexts>
<context position="4273" citStr="Zechner &amp; Waibel, 2000" startWordPosition="602" endWordPosition="605">terance position) is less effective for summarizing spontaneous conversations than it is in broadcast news. MMR 197 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197–200, New York, June 2006. c�2006 Association for Computational Linguistics and lexical features are the best. Speech-related features follow. The structural feature is least effective. We do not discuss problem (3) and (4) in this paper. For problem (3), a similar idea has been proposed to summarize online blogs and discussions. Problem (4) has been partially addressed by (Zechner &amp; Waibel, 2000); but it has not been studied together with acoustic features. 2 Utterance-extraction-based summarization Still at its early stage, current research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer.</context>
</contexts>
<marker>Zechner, Waibel, 2000</marker>
<rawString>Zechner K. and Waibel A., 2000. Minimizing word error rate in textual summaries of spoken language. NAACL2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Automatic Summarization of Spoken Dialogues in Unrestricted Domains.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="2181" citStr="Zechner, 2001" startWordPosition="292" endWordPosition="293">ion of important utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (3) conversations often contain discourse clues, e.g., question-answer pairs and speakers’ information, which can be utilized to keep the summary coherent; (4) word error rates (WERs) from speech recognition are usually much higher in spontaneous conversations. Previous work on spontaneous-conversation summarization has mainly focused on textual features (Zechner, 2001; Gurevych and Strube, 2004), while speech-related features have not been explored for this type of speech source. This paper explores and compares the effectiveness of both textual features and speech-related features. The experiments show that these features incrementally improve summarization performance. We also discuss problems (1) and (2) mentioned above. For (1), Zechner (2001) proposes to detect and remove false starts and speech disfluencies from transcripts, in order to make the text-format summary concise and more readable. Nevertheless, it is not always necessary to remove them. On</context>
<context position="5684" citStr="Zechner (2001)" startWordPosition="807" endWordPosition="808">gments rather than transcripts is that it avoids the impact of WERs caused by automatic speech recognition (ASR). We will focus on utterance-level extraction, which at present appears to be the only way to ensure comprehensibility and naturalness if the summaries are to be delivered as excerpts of audio themselves. Previous work on spontaneous conversations mainly focuses on using textual features. Gurevych &amp; Strube (2004) develop a shallow knowledgebased approach. The noun portion of WordNet is used as a knowledge source. The noun senses were manually disambiguated rather than automatically. Zechner (2001) applies maximum marginal relevance (MMR) to select utterances for spontaneous conversation transcripts. 3 Classification based utterance extraction Spontaneous conversations contain more information than textual features. To utilize these features, we reformulate the utterance selection task as a binary classification problem, an utterance is either labeled as “1” (in-summary) or “0” (not-in-summary). Two state-of-the-art classifiers, support vector machine (SVM) and logistic regression (LR), are used. SVM seeks an optimal separating hyperplane, where the margin is maximal. In our experiments</context>
<context position="7817" citStr="Zechner (2001)" startWordPosition="1117" endWordPosition="1118">er this utterance is adjacent to a speaker turn or not. (4) Prosodic features: we use basic prosody: the maximum, minimum, average and range of energy, as well as those of fundamental frequency, normalized by speakers. All these features are automatically extracted. (5) Spoken-language features: the spoken-language features include number of repetitions, filled pauses, and the total number of them. Disfluencies adjacent to a speaker turn are not counted, because they are normally used to coordinate interaction among speakers. Repetitions and pauses are detected in the same way as described in Zechner (2001). 4 Experimental results 4.1 Experiment settings The data used for our experiments come from SWITCHBOARD. We randomly select 27 conversations, containing around 3660 utterances. The important utterances of each conversation are 198 manually annotated. We use f-score and the ROUGE score as evaluation metrics. Ten-fold cross validation is applied to obtain the results presented in this section. ROUGE-L show the same tendency, but are not presented here due to the space limit. Both the f-score and ROUGE indicate that, in general, rich features incrementally improve summarization performance. 4.2 </context>
</contexts>
<marker>Zechner, 2001</marker>
<rawString>Zechner K., 2001. Automatic Summarization of Spoken Dialogues in Unrestricted Domains. Ph.D. thesis, Carnegie Mellon University, November 2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>