<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006603">
<title confidence="0.792109">
Situated Models of Meaning for Sports Video Retrieval
</title>
<note confidence="0.7196715">
Michael Fleischman
MIT Media Lab
</note>
<email confidence="0.626562">
mbf@mit.edu
</email>
<sectionHeader confidence="0.719143" genericHeader="abstract">
Abstract
</sectionHeader>
<tableCaption confidence="0.972926">
Situated models of meaning ground words in the
non-linguistic context, or situation, to which they
refer. Applying such models to sports video re-
trieval requires learning appropriate representa-
tions for complex events. We propose a method
that uses data mining to discover temporal pat-
terns in video, and pair these patterns with associ-
ated closed captioning text. This paired corpus is
used to train a situated model of meaning that sig-
nificantly improves video retrieval performance.
</tableCaption>
<sectionHeader confidence="0.996145" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999733724137931">
Recent advances in digital broadcasting and re-
cording allow fans access to an unprecedented
amount of sports video. The growing need to
manage and search large video collections presents
a challenge to traditional information retrieval (IR)
technologies. Such methods cannot be directly
applied to video data, even when closed caption
transcripts are available; for, unlike text docu-
ments, the occurrence of a query term in a video is
often not enough to assume the video’s relevance
to that query. For example, when searching
through video of baseball games, returning all clips
in which the phrase “home run” occurs, results
primarily in video of events where a home run
does not actually occur. This follows from the fact
that in sports, as in life, people often talk not about
what is currently happening, but rather, they talk
about what did, might, or will happen in the future.
Traditional IR techniques cannot address such
problems because they model the meaning of a
query term strictly by that term’s relationship to
other terms. To build systems that successfully
search video, IR techniques should be extended to
exploit not just linguistic information but also ele-
ments of the non-linguistic context, or situation,
that surrounds language use. This paper presents a
method for video event retrieval from broadcast
sports that achieves this by learning a situated
model of meaning from an unlabeled video corpus.
</bodyText>
<page confidence="0.992245">
37
</page>
<note confidence="0.473631">
Deb Roy
MIT Media Lab
</note>
<email confidence="0.58053">
dkroy@media.mit.edu
</email>
<bodyText confidence="0.99997047826087">
The framework for the current model is derived
from previous work on computational models of
verb learning (Fleischman &amp; Roy, 2005). In this
earlier work, meaning is defined by a probabilistic
mapping between words and representations of the
non-linguistic events to which those words refer.
In applying this framework to events in video, we
follow recent work on video surveillance in which
complex events are represented as temporal rela-
tions between lower level sub-events (Hongen et
al., 2004). While in the surveillance domain, hand
crafted event representations have been used suc-
cessfully, the greater variability of content in
broadcast sports demands an automatic method for
designing event representations.
The primary focus of this paper is to present a
method for mining such representations from large
video corpora, and to describe how these represen-
tations can be mapped to natural language. We
focus on a pilot dataset of broadcast baseball
games. Pilot video retrieval tests show that using a
situated model significantly improves perform-
ances over traditional language modeling methods.
</bodyText>
<sectionHeader confidence="0.989559" genericHeader="method">
2 Situated Models of Meaning
</sectionHeader>
<bodyText confidence="0.999734375">
Building situated models of meaning operates in
three phases (see Figure 1): first, raw video data is
abstracted into multiple streams of discrete fea-
tures. Temporal data mining techniques are then
applied to these feature streams to discover hierar-
chical temporal patterns. These temporal patterns
form the event representations that are then
mapped to words from the closed caption stream.
</bodyText>
<subsectionHeader confidence="0.986654">
2.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999807666666667">
The first step in representing events in video is to
abstract the very high dimensional raw video data
into more semantically meaningful streams of in-
formation. Ideally, these streams would corre-
spond to basic events that occur in sports video
(e.g., hitting, throwing, catching, kicking, etc.).
Due to the limitations of computer vision tech-
niques, extracting such ideal features is often in-
feasible. However, by exploiting the “language of
</bodyText>
<note confidence="0.620035">
Proceedings of NAACL HLT 2007, Companion Volume, pages 37–40,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999045">
Figure 1. Video processing pipeline for learning situated models of meaning.
</figureCaption>
<bodyText confidence="0.999963125">
film” that is used to produce sports video, informa-
tive features can be extracted that are also easy to
compute. Thus, although we cannot easily identify
a player hitting the ball, we can easily detect fea-
tures that correlate with hitting: e.g., when a scene
focusing on the pitching mound immediately
jumps to one zooming in on the field (Figure 1).
While such correlations are not perfect, pilot tests
show that baseball events can be classified using
such features (Fleischman et. al., in prep).
Importantly, this is the only phase of our frame-
work that is domain specific; i.e., it is the only as-
pect of the framework designed specifically for use
with baseball data. Although many feature types
can be extracted, we focus on only two feature
types: visual context, and camera motion.
</bodyText>
<subsectionHeader confidence="0.924008">
Visual Context
</subsectionHeader>
<bodyText confidence="0.999926789473684">
Visual context features encode general properties
of the visual scene in a video segment. The first
step in extracting such features is to split the raw
video into “shots” based on changes in the visual
scene due to editing (e.g., jumping from a close up
of the pitcher to a wide angle of the field). Shot
detection is a well studied problem in multimedia
research; in this work, we use the method of
Tardini et al. (2005) because of its speed and
proven performance on sports video.
After a game is segmented into shots, each shot
is categorized into one of three categories: pitch-
ing-scene, field-scene, or other. Categorization is
based on image features (e.g., color histograms,
edge detection, motion analysis) extracted from an
individual key frame chosen from that shot. A de-
cision tree is trained (with bagging and boosting)
using the WEKA machine learning toolkit that
achieves over 97% accuracy on a held out dataset.
</bodyText>
<subsectionHeader confidence="0.800365">
Camera Motion
</subsectionHeader>
<bodyText confidence="0.99986045">
Whereas visual context features provide informa-
tion about the global situation that is being ob-
served, camera motion features afford more precise
information about the actions occurring in the
video. The intuition here is that the camera is a
stand in for a viewer’s focus of attention. As ac-
tion in the video takes place, the camera moves to
follow it, mirroring the action itself, and providing
an informative feature for event representation.
Detecting camera motion (i.e., pan/tilt/zoom) is a
well-studied problem in video analysis. We use
the system of (Bouthemy et al., 1999) which com-
putes the pan, tilt, and zoom motions using the pa-
rameters of a two-dimensional affine model fit to
every pair of sequential frames in a video segment.
The output of this system is then clustered into
characteristic camera motions (e.g. zooming in fast
while panning slightly left) using a 1st order Hid-
den Markov Model with 15 states, implemented
using the Graphical Modeling Toolkit (GMTK).
</bodyText>
<subsectionHeader confidence="0.965302">
2.2 Temporal Pattern Mining
</subsectionHeader>
<bodyText confidence="0.999955692307692">
In this step, temporal patterns are mined from the
features abstracted from the raw video data. As
described above, ideal semantic features (such as
hitting and catching) cannot be extracted easily
from video. We hypothesize that finding temporal
patterns between scene and camera motion features
can produce representations that are highly corre-
lated with sports events. Importantly, such tempo-
ral patterns are not strictly sequential, but rather,
are composed of features that can occur in complex
and varied temporal relations to each other. For
example, Figure 1 shows the representation for a
fly ball event that is composed of: a camera pan-
</bodyText>
<page confidence="0.99606">
38
</page>
<bodyText confidence="0.999047071428572">
ning up followed by a camera pan down, occurring
during a field scene, and before a pitching scene.
Following previous work in video content classi-
fication (Fleischman et al., 2006), we use tech-
niques from temporal data mining to discover
event patterns from feature streams. The algorithm
we use is fully unsupervised. It processes feature
streams by examining the relations that occur be-
tween individual features within a moving time
window. Following Allen (1984), any two features
that occur within this window must be in one of
seven temporal relations with each other (e.g. be-
fore, during, etc.). The algorithm keeps track of
how often each of these relations is observed, and
after the entire video corpus is analyzed, uses chi-
square analyses to determine which relations are
significant. The algorithm iterates through the
data, and relations between individual features that
are found significant in one iteration (e.g.
[BEFORE, camera panning up, camera panning
down]), are themselves treated as individual fea-
tures in the next. This allows the system to build
up higher-order nested relations in each iteration
(e.g. [DURING, [BEFORE, camera panning up,
camera panning down], field scene]]). The tempo-
ral patterns found significant in this way are then
used as the event representations that are then
mapped to words.
</bodyText>
<subsectionHeader confidence="0.973217">
2.3 Linguistic Mapping
</subsectionHeader>
<bodyText confidence="0.999863692307692">
The last step in building a situated model of mean-
ing is to map words onto the representations of
events mined from the raw video. We equate the
learning of this mapping to the problem of estimat-
ing the conditional probability distribution of a
word given a video event representation. Similar
to work in image retrieval (Barnard et al., 2003),
we cast the problem in terms of Machine Transla-
tion: given a paired corpus of words and a set of
video event representations to which they refer, we
make the IBM Model 1 assumption and use the
expectation-maximization method to estimate the
parameters (Brown et al., 1993):
</bodyText>
<equation confidence="0.976972428571429">
m C(1)
p word video
(  |) = ( |
+ ∏= p word video )
m j aj
( 1)
l j 1
</equation>
<bodyText confidence="0.999965470588235">
This paired corpus is created from a corpus of
raw video by first abstracting each video into the
feature streams described above. For every shot
classified as a pitching scene, a new instance is
created in the paired corpus corresponding to an
event that starts at the beginning of that shot and
ends exactly four shots after. This definition of an
event follows from the fact that most events in
baseball must start with a pitch and usually do not
last longer than four shots (Gong et al., 2004).
For each of these events in the paired corpus, a
representation of the video is generated by match-
ing all patterns (and the nested sub-patterns) found
from temporal mining to the feature streams of the
event. These video representations are then paired
with all the words from the closed captioning that
occur during that event (plus/minus 10 seconds).
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9998731">
Work on video IR in the news domain often fo-
cuses on indexing video data using a set of image
classifiers that categorize shots into pre-determined
concepts (e.g. flag, outdoors, George Bush, etc.).
Text queries must then be translated (sometimes
manually) in terms of these concepts (Worring &amp;
Snoek, 2006). Our work focuses on a more auto-
mated approach that is closer to traditional IR tech-
niques. Our framework extends the language
modeling approach of Ponte and Croft (1998) by
incorporating a situated model of meaning.
In Ponte and Croft (1998), documents relevant to
a query are ranked based on the probability that
each document generated each query term. We
follow this approach for video events, making the
assumption that the relevance of an event to a
query depends both on the words associated with
the event (i.e. what was said while the event oc-
curred), as well as the situational context modeled
by the video event representations:
</bodyText>
<equation confidence="0.987801428571429">
query α (2)
α −
p query event
(  |) = ∏ ) (  |)(1 )
p word caption p word video
(  |∗
word
</equation>
<bodyText confidence="0.99938325">
The p(word|caption) is estimated using the lan-
guage modeling technique described in Ponte and
Croft (1998). The p(word|video) is estimated as in
equation 1 above. a is used to weight the models.
</bodyText>
<subsectionHeader confidence="0.780888">
Data
</subsectionHeader>
<bodyText confidence="0.999254545454545">
The system has been evaluated on a pilot set of 6
broadcast baseball games totaling about 15 hours
and 1200 distinct events. The data represents
video of 9 different teams, at 4 different stadiums,
broadcast on 4 different stations. Highlights (i.e.,
events which terminate with the player either out
or safe) were hand annotated, and categorized ac-
cording to the type of the event (e.g., strikeout vs.
homerun), the location of the event (e.g., right field
vs. infield), and the nature of the event (e.g., fly
ball vs. line drive). Each of these categories was
</bodyText>
<page confidence="0.998584">
39
</page>
<bodyText confidence="0.997138">
used to automatically select query terms to be used
in testing. Similar to Berger &amp; Lafferty (1999), the
probability distribution of terms given a category is
estimated using a normalized log-likelihood ratio
(Moore, 2004), and query terms are sampled ran-
domly from this distribution. This gives us a set of
queries for each annotated category (e.g., strikeout:
“miss, chasing”; flyball: “fly, streak”). Although
much noisier than human produced queries, this
procedure generates a large amount of test queries
for which relevant results can easily be determined
(e.g., if a returned event for the query “fly, streak”
is of the flyball category, it is marked relevant).
Experiments are reported using 6-fold cross
validation during which five games are used to
train the situated model while the sixth is held out
for testing. Because data is sparse, the situation
model is trained only on the hand annotated high-
light events. However, retrieval is always tested
using both highlight and non-highlight events.
</bodyText>
<figureCaption confidence="0.998084">
Figure 2. Effect of situated model on video IR.
</figureCaption>
<sectionHeader confidence="0.900431" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.9998169">
Figure 2 shows results for 520 automatically gen-
erated queries of one to four words in length.
Mean average precision (MAP), a common metric
that combines elements of precision, recall, and
ranking, is used to measure the relevance of the top
five results returned for each query. We show re-
sults for the system using only linguistic informa-
tion (i.e. a=1), only non-linguistic information (i.e.
a=0), and both information together (i.e. a=0.5).
The poor performance of the system using only
non-linguistic information is expected given the
limited training data and the simple features used
to represent events. Interestingly, using only lin-
guistic information produces similarly poor per-
formance. This is a direct result of announcers’
tendency to discuss topics not currently occurring
in the video. By combining text and video analy-
ses, though, the system performs significantly bet-
ter (p&lt;0.01) by determining when the observed
language actually refers to the situation at hand.
</bodyText>
<sectionHeader confidence="0.998421" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999944818181818">
We have presented a framework for video retrieval
that significantly out-performs traditional IR meth-
ods applied to closed caption text. Our new ap-
proach incorporates the visual content of baseball
video using automatically learned event represen-
tations to model the situated meaning of words.
Results indicate that integration of situational con-
text dramatically improves performance over tradi-
tional methods alone. In future work we will
examine the effects of applying situated models of
meaning to other tasks (e.g., machine translation).
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999956825">
Allen, J.F. (1984). A General Model of Action and Time. Arti-
ficial Intelligence. 23(2).
Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, D,
and Jordan, M. (2003), &amp;quot;Matching Words and Pictures,&amp;quot;
Journal of Machine Learning Research, Vol 3.
Berger, A. and Lafferty, J. (1999). Information Retrieval as
Statistical Translation. In Proceedings of SIGIR-99.
Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A unified
approach to shot change detection and camera motion char-
acterization. IEEE Trans. on Circuits and Systems for Video
Technology, 9(7):1030-1044.
Brown, P., Della Pietra, S., Della Pietra, V. Mercer, R. (1993).
The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(10).
Fleischman, M. and Roy, D. (2005). Intentional Context in
Situated Language Learning. In Proc. of 9th Conference on
Comp. Natural Language Learning.
Fleischman, M., DeCamp, P. Roy, D. (2006). Mining Tempo-
ral Patterns of Movement for Video Content Classification.
The 8th ACM SIGMM International Workshop on Multi-
media Information Retrieval.
Fleischman, M., Roy, B., Roy, D. (in prep.). Automated Fea-
ture Engineering inBaseball Highlight Classification.
Gong, Y., Han, M., Hua, W., Xu, W. (2004). Maximum en-
tropy model-based baseball highlight detection and classifi-
cation. Computer Vision and Image Understanding. 96(2).
Hongen, S., Nevatia, R. Bremond, F. (2004). Video-based
event recognition: activity representation and probabilistic
recognition methods. Computer Vision and Image Under-
standing. 96(2). pp: 129 - 162
Moore, Robert C. (2004). Improving IBM Word Alignment
Model 1. in Proc. of 42nd ACL.
Ponte, J.M., and Croft, W.B. (1998). A Language Modeling
Approach to Information Retrieval. In Proc. of SIGIR’98.
Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). Shot
Detection and Motion Analysis for Automatic MPEG-7
Annotation of Sports Videos. In 13th International Confer-
ence on Image Analysis and Processing.
Worring, M., Snoek, C.. (2006). Semantic Indexing and Re-
trieval of Video. Tutorial at ACM Multimedia
</reference>
<page confidence="0.99864">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930207">
<title confidence="0.9985">Situated Models of Meaning for Sports Video Retrieval</title>
<author confidence="0.999997">Michael Fleischman</author>
<affiliation confidence="0.989823">MIT Media Lab</affiliation>
<email confidence="0.999385">mbf@mit.edu</email>
<abstract confidence="0.993867545454545">Situated models of meaning ground words in the non-linguistic context, or situation, to which they refer. Applying such models to sports video retrieval requires learning appropriate representations for complex events. We propose a method that uses data mining to discover temporal patterns in video, and pair these patterns with associated closed captioning text. This paired corpus is used to train a situated model of meaning that significantly improves video retrieval performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>A General Model of Action and Time.</title>
<date>1984</date>
<journal>Artificial Intelligence.</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="8158" citStr="Allen (1984)" startWordPosition="1299" endWordPosition="1300"> varied temporal relations to each other. For example, Figure 1 shows the representation for a fly ball event that is composed of: a camera pan38 ning up followed by a camera pan down, occurring during a field scene, and before a pitching scene. Following previous work in video content classification (Fleischman et al., 2006), we use techniques from temporal data mining to discover event patterns from feature streams. The algorithm we use is fully unsupervised. It processes feature streams by examining the relations that occur between individual features within a moving time window. Following Allen (1984), any two features that occur within this window must be in one of seven temporal relations with each other (e.g. before, during, etc.). The algorithm keeps track of how often each of these relations is observed, and after the entire video corpus is analyzed, uses chisquare analyses to determine which relations are significant. The algorithm iterates through the data, and relations between individual features that are found significant in one iteration (e.g. [BEFORE, camera panning up, camera panning down]), are themselves treated as individual features in the next. This allows the system to b</context>
</contexts>
<marker>Allen, 1984</marker>
<rawString>Allen, J.F. (1984). A General Model of Action and Time. Artificial Intelligence. 23(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>N de Freitas</author>
<author>D Forsyth</author>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Matching Words and Pictures,&amp;quot;</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<marker>Barnard, Duygulu, de Freitas, Forsyth, Blei, Jordan, 2003</marker>
<rawString>Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, D, and Jordan, M. (2003), &amp;quot;Matching Words and Pictures,&amp;quot; Journal of Machine Learning Research, Vol 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR-99.</booktitle>
<contexts>
<context position="12498" citStr="Berger &amp; Lafferty (1999)" startWordPosition="2046" endWordPosition="2049"> set of 6 broadcast baseball games totaling about 15 hours and 1200 distinct events. The data represents video of 9 different teams, at 4 different stadiums, broadcast on 4 different stations. Highlights (i.e., events which terminate with the player either out or safe) were hand annotated, and categorized according to the type of the event (e.g., strikeout vs. homerun), the location of the event (e.g., right field vs. infield), and the nature of the event (e.g., fly ball vs. line drive). Each of these categories was 39 used to automatically select query terms to be used in testing. Similar to Berger &amp; Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. This gives us a set of queries for each annotated category (e.g., strikeout: “miss, chasing”; flyball: “fly, streak”). Although much noisier than human produced queries, this procedure generates a large amount of test queries for which relevant results can easily be determined (e.g., if a returned event for the query “fly, streak” is of the flyball category, it is marked relevant). Experiments are reported usin</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Berger, A. and Lafferty, J. (1999). Information Retrieval as Statistical Translation. In Proceedings of SIGIR-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bouthemy</author>
<author>M Gelgon</author>
<author>F Ganansia</author>
</authors>
<title>A unified approach to shot change detection and camera motion characterization.</title>
<date>1999</date>
<booktitle>IEEE Trans. on Circuits and Systems for Video Technology,</booktitle>
<pages>9--7</pages>
<contexts>
<context position="6612" citStr="Bouthemy et al., 1999" startWordPosition="1049" endWordPosition="1052">acy on a held out dataset. Camera Motion Whereas visual context features provide information about the global situation that is being observed, camera motion features afford more precise information about the actions occurring in the video. The intuition here is that the camera is a stand in for a viewer’s focus of attention. As action in the video takes place, the camera moves to follow it, mirroring the action itself, and providing an informative feature for event representation. Detecting camera motion (i.e., pan/tilt/zoom) is a well-studied problem in video analysis. We use the system of (Bouthemy et al., 1999) which computes the pan, tilt, and zoom motions using the parameters of a two-dimensional affine model fit to every pair of sequential frames in a video segment. The output of this system is then clustered into characteristic camera motions (e.g. zooming in fast while panning slightly left) using a 1st order Hidden Markov Model with 15 states, implemented using the Graphical Modeling Toolkit (GMTK). 2.2 Temporal Pattern Mining In this step, temporal patterns are mined from the features abstracted from the raw video data. As described above, ideal semantic features (such as hitting and catching</context>
</contexts>
<marker>Bouthemy, Gelgon, Ganansia, 1999</marker>
<rawString>Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A unified approach to shot change detection and camera motion characterization. IEEE Trans. on Circuits and Systems for Video Technology, 9(7):1030-1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Mercer</author>
<author>R</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="9661" citStr="Brown et al., 1993" startWordPosition="1544" endWordPosition="1547">g The last step in building a situated model of meaning is to map words onto the representations of events mined from the raw video. We equate the learning of this mapping to the problem of estimating the conditional probability distribution of a word given a video event representation. Similar to work in image retrieval (Barnard et al., 2003), we cast the problem in terms of Machine Translation: given a paired corpus of words and a set of video event representations to which they refer, we make the IBM Model 1 assumption and use the expectation-maximization method to estimate the parameters (Brown et al., 1993): m C(1) p word video ( |) = ( | + ∏= p word video ) m j aj ( 1) l j 1 This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above. For every shot classified as a pitching scene, a new instance is created in the paired corpus corresponding to an event that starts at the beginning of that shot and ends exactly four shots after. This definition of an event follows from the fact that most events in baseball must start with a pitch and usually do not last longer than four shots (Gong et al., 2004). For each of these events in th</context>
</contexts>
<marker>Brown, Pietra, S, Mercer, R, 1993</marker>
<rawString>Brown, P., Della Pietra, S., Della Pietra, V. Mercer, R. (1993). The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>D Roy</author>
</authors>
<title>Intentional Context in Situated Language Learning.</title>
<date>2005</date>
<booktitle>In Proc. of 9th Conference on Comp. Natural Language Learning.</booktitle>
<contexts>
<context position="2210" citStr="Fleischman &amp; Roy, 2005" startWordPosition="346" endWordPosition="349">ery term strictly by that term’s relationship to other terms. To build systems that successfully search video, IR techniques should be extended to exploit not just linguistic information but also elements of the non-linguistic context, or situation, that surrounds language use. This paper presents a method for video event retrieval from broadcast sports that achieves this by learning a situated model of meaning from an unlabeled video corpus. 37 Deb Roy MIT Media Lab dkroy@media.mit.edu The framework for the current model is derived from previous work on computational models of verb learning (Fleischman &amp; Roy, 2005). In this earlier work, meaning is defined by a probabilistic mapping between words and representations of the non-linguistic events to which those words refer. In applying this framework to events in video, we follow recent work on video surveillance in which complex events are represented as temporal relations between lower level sub-events (Hongen et al., 2004). While in the surveillance domain, hand crafted event representations have been used successfully, the greater variability of content in broadcast sports demands an automatic method for designing event representations. The primary fo</context>
</contexts>
<marker>Fleischman, Roy, 2005</marker>
<rawString>Fleischman, M. and Roy, D. (2005). Intentional Context in Situated Language Learning. In Proc. of 9th Conference on Comp. Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>P Roy DeCamp</author>
<author>D</author>
</authors>
<title>Mining Temporal Patterns of Movement for Video Content Classification.</title>
<date>2006</date>
<booktitle>The 8th ACM SIGMM International Workshop on Multimedia Information Retrieval.</booktitle>
<contexts>
<context position="7873" citStr="Fleischman et al., 2006" startWordPosition="1253" endWordPosition="1256">eo. We hypothesize that finding temporal patterns between scene and camera motion features can produce representations that are highly correlated with sports events. Importantly, such temporal patterns are not strictly sequential, but rather, are composed of features that can occur in complex and varied temporal relations to each other. For example, Figure 1 shows the representation for a fly ball event that is composed of: a camera pan38 ning up followed by a camera pan down, occurring during a field scene, and before a pitching scene. Following previous work in video content classification (Fleischman et al., 2006), we use techniques from temporal data mining to discover event patterns from feature streams. The algorithm we use is fully unsupervised. It processes feature streams by examining the relations that occur between individual features within a moving time window. Following Allen (1984), any two features that occur within this window must be in one of seven temporal relations with each other (e.g. before, during, etc.). The algorithm keeps track of how often each of these relations is observed, and after the entire video corpus is analyzed, uses chisquare analyses to determine which relations ar</context>
</contexts>
<marker>Fleischman, DeCamp, D, 2006</marker>
<rawString>Fleischman, M., DeCamp, P. Roy, D. (2006). Mining Temporal Patterns of Movement for Video Content Classification. The 8th ACM SIGMM International Workshop on Multimedia Information Retrieval.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Fleischman</author>
<author>B Roy</author>
<author>D Roy</author>
</authors>
<title>(in prep.). Automated Feature Engineering inBaseball Highlight Classification.</title>
<marker>Fleischman, Roy, Roy, </marker>
<rawString>Fleischman, M., Roy, B., Roy, D. (in prep.). Automated Feature Engineering inBaseball Highlight Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gong</author>
<author>M Han</author>
<author>W Hua</author>
<author>W Xu</author>
</authors>
<title>Maximum entropy model-based baseball highlight detection and classification.</title>
<date>2004</date>
<journal>Computer Vision and Image Understanding.</journal>
<volume>96</volume>
<issue>2</issue>
<contexts>
<context position="10229" citStr="Gong et al., 2004" startWordPosition="1657" endWordPosition="1660">od to estimate the parameters (Brown et al., 1993): m C(1) p word video ( |) = ( | + ∏= p word video ) m j aj ( 1) l j 1 This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above. For every shot classified as a pitching scene, a new instance is created in the paired corpus corresponding to an event that starts at the beginning of that shot and ends exactly four shots after. This definition of an event follows from the fact that most events in baseball must start with a pitch and usually do not last longer than four shots (Gong et al., 2004). For each of these events in the paired corpus, a representation of the video is generated by matching all patterns (and the nested sub-patterns) found from temporal mining to the feature streams of the event. These video representations are then paired with all the words from the closed captioning that occur during that event (plus/minus 10 seconds). 3 Experiments Work on video IR in the news domain often focuses on indexing video data using a set of image classifiers that categorize shots into pre-determined concepts (e.g. flag, outdoors, George Bush, etc.). Text queries must then be transl</context>
</contexts>
<marker>Gong, Han, Hua, Xu, 2004</marker>
<rawString>Gong, Y., Han, M., Hua, W., Xu, W. (2004). Maximum entropy model-based baseball highlight detection and classification. Computer Vision and Image Understanding. 96(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hongen</author>
<author>R Bremond Nevatia</author>
<author>F</author>
</authors>
<title>Video-based event recognition: activity representation and probabilistic recognition methods.</title>
<date>2004</date>
<journal>Computer Vision and Image Understanding.</journal>
<volume>96</volume>
<issue>2</issue>
<pages>129--162</pages>
<contexts>
<context position="2576" citStr="Hongen et al., 2004" startWordPosition="403" endWordPosition="406"> this by learning a situated model of meaning from an unlabeled video corpus. 37 Deb Roy MIT Media Lab dkroy@media.mit.edu The framework for the current model is derived from previous work on computational models of verb learning (Fleischman &amp; Roy, 2005). In this earlier work, meaning is defined by a probabilistic mapping between words and representations of the non-linguistic events to which those words refer. In applying this framework to events in video, we follow recent work on video surveillance in which complex events are represented as temporal relations between lower level sub-events (Hongen et al., 2004). While in the surveillance domain, hand crafted event representations have been used successfully, the greater variability of content in broadcast sports demands an automatic method for designing event representations. The primary focus of this paper is to present a method for mining such representations from large video corpora, and to describe how these representations can be mapped to natural language. We focus on a pilot dataset of broadcast baseball games. Pilot video retrieval tests show that using a situated model significantly improves performances over traditional language modeling m</context>
</contexts>
<marker>Hongen, Nevatia, F, 2004</marker>
<rawString>Hongen, S., Nevatia, R. Bremond, F. (2004). Video-based event recognition: activity representation and probabilistic recognition methods. Computer Vision and Image Understanding. 96(2). pp: 129 - 162</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM Word Alignment Model 1. in</title>
<date>2004</date>
<booktitle>Proc. of 42nd ACL.</booktitle>
<contexts>
<context position="12621" citStr="Moore, 2004" startWordPosition="2065" endWordPosition="2066"> 4 different stadiums, broadcast on 4 different stations. Highlights (i.e., events which terminate with the player either out or safe) were hand annotated, and categorized according to the type of the event (e.g., strikeout vs. homerun), the location of the event (e.g., right field vs. infield), and the nature of the event (e.g., fly ball vs. line drive). Each of these categories was 39 used to automatically select query terms to be used in testing. Similar to Berger &amp; Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. This gives us a set of queries for each annotated category (e.g., strikeout: “miss, chasing”; flyball: “fly, streak”). Although much noisier than human produced queries, this procedure generates a large amount of test queries for which relevant results can easily be determined (e.g., if a returned event for the query “fly, streak” is of the flyball category, it is marked relevant). Experiments are reported using 6-fold cross validation during which five games are used to train the situated model while the sixth is held out for test</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Moore, Robert C. (2004). Improving IBM Word Alignment Model 1. in Proc. of 42nd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR’98.</booktitle>
<contexts>
<context position="11076" citStr="Ponte and Croft (1998)" startWordPosition="1796" endWordPosition="1799">presentations are then paired with all the words from the closed captioning that occur during that event (plus/minus 10 seconds). 3 Experiments Work on video IR in the news domain often focuses on indexing video data using a set of image classifiers that categorize shots into pre-determined concepts (e.g. flag, outdoors, George Bush, etc.). Text queries must then be translated (sometimes manually) in terms of these concepts (Worring &amp; Snoek, 2006). Our work focuses on a more automated approach that is closer to traditional IR techniques. Our framework extends the language modeling approach of Ponte and Croft (1998) by incorporating a situated model of meaning. In Ponte and Croft (1998), documents relevant to a query are ranked based on the probability that each document generated each query term. We follow this approach for video events, making the assumption that the relevance of an event to a query depends both on the words associated with the event (i.e. what was said while the event occurred), as well as the situational context modeled by the video event representations: query α (2) α − p query event ( |) = ∏ ) ( |)(1 ) p word caption p word video ( |∗ word The p(word|caption) is estimated using the</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Ponte, J.M., and Croft, W.B. (1998). A Language Modeling Approach to Information Retrieval. In Proc. of SIGIR’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grana C Tardini</author>
<author>R Marchi</author>
<author>R Cucchiara</author>
</authors>
<title>Shot Detection and Motion Analysis for Automatic MPEG-7 Annotation of Sports Videos.</title>
<date>2005</date>
<booktitle>In 13th International Conference on Image Analysis and Processing.</booktitle>
<contexts>
<context position="5511" citStr="Tardini et al. (2005)" startWordPosition="873" endWordPosition="876">of the framework designed specifically for use with baseball data. Although many feature types can be extracted, we focus on only two feature types: visual context, and camera motion. Visual Context Visual context features encode general properties of the visual scene in a video segment. The first step in extracting such features is to split the raw video into “shots” based on changes in the visual scene due to editing (e.g., jumping from a close up of the pitcher to a wide angle of the field). Shot detection is a well studied problem in multimedia research; in this work, we use the method of Tardini et al. (2005) because of its speed and proven performance on sports video. After a game is segmented into shots, each shot is categorized into one of three categories: pitching-scene, field-scene, or other. Categorization is based on image features (e.g., color histograms, edge detection, motion analysis) extracted from an individual key frame chosen from that shot. A decision tree is trained (with bagging and boosting) using the WEKA machine learning toolkit that achieves over 97% accuracy on a held out dataset. Camera Motion Whereas visual context features provide information about the global situation t</context>
</contexts>
<marker>Tardini, Marchi, Cucchiara, 2005</marker>
<rawString>Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). Shot Detection and Motion Analysis for Automatic MPEG-7 Annotation of Sports Videos. In 13th International Conference on Image Analysis and Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Worring</author>
<author>C Snoek</author>
</authors>
<date>2006</date>
<booktitle>Semantic Indexing and Retrieval of Video. Tutorial at ACM Multimedia</booktitle>
<contexts>
<context position="10905" citStr="Worring &amp; Snoek, 2006" startWordPosition="1767" endWordPosition="1770">resentation of the video is generated by matching all patterns (and the nested sub-patterns) found from temporal mining to the feature streams of the event. These video representations are then paired with all the words from the closed captioning that occur during that event (plus/minus 10 seconds). 3 Experiments Work on video IR in the news domain often focuses on indexing video data using a set of image classifiers that categorize shots into pre-determined concepts (e.g. flag, outdoors, George Bush, etc.). Text queries must then be translated (sometimes manually) in terms of these concepts (Worring &amp; Snoek, 2006). Our work focuses on a more automated approach that is closer to traditional IR techniques. Our framework extends the language modeling approach of Ponte and Croft (1998) by incorporating a situated model of meaning. In Ponte and Croft (1998), documents relevant to a query are ranked based on the probability that each document generated each query term. We follow this approach for video events, making the assumption that the relevance of an event to a query depends both on the words associated with the event (i.e. what was said while the event occurred), as well as the situational context mod</context>
</contexts>
<marker>Worring, Snoek, 2006</marker>
<rawString>Worring, M., Snoek, C.. (2006). Semantic Indexing and Retrieval of Video. Tutorial at ACM Multimedia</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>