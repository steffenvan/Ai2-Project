<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997620333333333">
Lexical Cohesion Computed by Thesaural
Relations as an Indicator of the Structure
of Text
</title>
<author confidence="0.998682">
Jane Morris. Graeme Hirstt
</author>
<affiliation confidence="0.99961">
York University University of Toronto
</affiliation>
<bodyText confidence="0.9992045">
In text, lexical cohesion is the result of chains of related words that contribute to the continuity
of lexical meaning. These lexical chains are a direct result of units of text being &amp;quot;about the
same thing,&amp;quot; and finding text structure involves finding units of text that are about the same
thing. Hence, computing the chains is useful, since they will have a correspondence to the
structure of the text. Determining the structure of text is an essential step in determining the
deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for
computing lexical chains. Correspondences between lexical chains and structural elements are
shown to exist. Since the lexical chains are computable, and exist in non–domain-specific text,
they provide a valuable indicator of text structure. The lexical chains also provide a semantic
context for interpreting words, concepts, and sentences.
</bodyText>
<sectionHeader confidence="0.682379" genericHeader="method">
1. Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.992616">
A text or discourse is not just a set of sentences, each on some random topic. Rather,
the sentences and phrases of any sensible text will each tend to be about the same
things — that is, the text will have a quality of unity. This is the property of cohesion
— the sentences &amp;quot;stick together&amp;quot; to function as a whole. Cohesion is achieved through
back-reference, conjunction, and semantic word relations. Cohesion is not a guarantee
of unity in text but rather a device for creating it. As aptly stated by Halliday and
Hasan (1976), it is a way of getting text to &amp;quot;hang together as a whole.&amp;quot; Their work on
cohesion has underscored its importance as an indicator of text unity.
Lexical cohesion is the cohesion that arises from semantic relationships between
words. All that is required is that there be some recognizable relation between the
words.
Halliday and Hasan have provided a classification of lexical cohesion based on
the type of dependency relationship that exists between words. There are five basic
classes:
</bodyText>
<listItem confidence="0.91181875">
1. Reiteration with identity of reference:
Example 1
1. Mary bit into a peach.
2. Unfortunately the peach wasn&apos;t ripe.
</listItem>
<affiliation confidence="0.757349666666667">
* Department of Computer Science, York University, North York, Ontario, Canada M3J 1P3
t Department of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S 1A4
C) 1991 Association for Computational Linguistics
</affiliation>
<figure confidence="0.7290094">
Computational Linguistics Volume 17, Number 1
2. Reiteration without identity of reference:
Example 2
1. Mary ate some peaches.
2. She likes peaches very much.
3. Reiteration by means of superordinate:
Example 3
1. Mary ate a peach.
2. She likes fruit.
4. Systematic semantic relation (systematically classifiable):
Example 4
1. Mary likes green apples.
2. She does not like red ones.
5. Nonsystematic semantic relation (not systematically classifiable):
Example 5
</figure>
<listItem confidence="0.772348">
1. Mary spent three hours in the garden yesterday.
2. She was digging potatoes.
</listItem>
<bodyText confidence="0.991066833333333">
Examples 1, 2, and 3 fall into the class of reiteration. Note that reiteration includes
not only identity of reference or repetition of the same word, but also the use of
superordinates, subordinates, and synonyms.
Examples 4 and 5 fall into the class of collocation, that is, semantic relationships
between words that often co-occur. They can be further divided into two categories
of relationship: systematic semantic, and nonsystematic semantic.
Systematic semantic relationships can be classified in a fairly straightforward way.
This type of relation includes antonyms, members of an ordered set such as {one, two,
three}, members of an unordered set such as {white, black, red}, and part-to-whole re-
lationships like {eyes, mouth, face}. Example 5 is an illustration of collocation where
the word relationship, {garden, digging}, is nonsystematic. This type of relationship
is the most problematic, especially from a knowledge representation point of view.
Such collocation relationships exist between words that tend to occur in similar lexical
environments. Words tend to occur in similar lexical environments because they de-
scribe things that tend to occur in similar situations or contexts in the world. Hence,
context-specific examples such as {post office, service, stamps, pay, leave} are included
in the class. (This example is from Ventola (1987), who analyzed the patterns of lex-
ical cohesion specific to the context of service encounters.) Another example of this
type is {car, lights, turning}, taken from example 14 in Section 4.2. These words are
related in the situation of driving a car, but taken out of that situation, they are not
related in a systematic way. Also contained in the class of collocation are word associa-
tions. Examples from Postman and Keppel (1970) are {priest, church}, {citizen, U.S.A.},
and {whistle, stop}. Again, the exact relationship between these words can be hard to
classify, but there does exist a recognizable relationship.
</bodyText>
<subsectionHeader confidence="0.99886">
1.1 Lexical Chains
</subsectionHeader>
<bodyText confidence="0.997766">
Often, lexical cohesion occurs not simply between pairs of words but over a succes-
sion of a number of nearby related words spanning a topical unit of the text. These
</bodyText>
<page confidence="0.995275">
22
</page>
<note confidence="0.859934">
Morris and Hirst Lexical Cohesion
</note>
<bodyText confidence="0.969811133333333">
sequences of related words will be called lexical chains. There is a distance relation be-
tween each word in the chain, and the words co-occur within a given span. Lexical
chains do not stop at sentence boundaries. They can connect a pair of adjacent words
or range over an entire text.
Lexical chains tend to delineate portions of text that have a strong unity of mean-
ing. Consider this example (sentences 31-33 from the long example given in Sec-
tion 4.2):
Example 6
In front of me lay a virgin crescent cut out of pine bush. A dozen houses were going
up, in various stages of construction, surrounded by hummocks of dry earth and
stands of precariously tall trees nude halfway up their trunks. They were the kind of
trees you might see in the mountains.
A lexical chain spanning these three sentences is {virgin, pine, bush, trees, trunks, trees}.
Section 3 will explain how such chains are formed. Section 4 is an analysis of the
correspondence between lexical chains and the structure of the text.
</bodyText>
<subsectionHeader confidence="0.932325">
1.2 Why Lexical Cohesion Is Important
</subsectionHeader>
<bodyText confidence="0.9917735">
There are two major reasons why lexical cohesion is important for computational text
understanding systems:
</bodyText>
<listItem confidence="0.997949">
1. Lexical chains provide an easy-to-determine context to aid in the
resolution of ambiguity and in the narrowing to a specific meaning of a
word.
2. Lexical chains provide a clue for the determination of coherence and
discourse structure, and hence the larger meaning of the text.
</listItem>
<bodyText confidence="0.9077015">
1.2.1 Word Interpretation in Context. Word meanings do not exist in isolation. Each
word must be interpreted in its context. For example, in the context {gin, alcohol, sober,
drinks}, the meaning of the noun drinks is narrowed down to alcoholic drinks. In the
context {hair, curl, comb, wave} (Halliday and Hasan 1976), wave means a hair wave,
not a water wave, a physics wave, or a friendly hand wave. In these examples, lexical
chains can be used as a contextual aid to interpreting word meanings.
In earlier work, Hirst (1987) used a system called &amp;quot;Polaroid Words&amp;quot; to provide
for intrasentential lexical disambiguation. Polaroid Words relied on a variety of cues,
including syntax, selectional restrictions, case frames, and — most relevant here —
a notion of semantic distance or relatedness to other words in the sentences; a sense
that had such a relationship was preferred over one that didn&apos;t. Relationships were
determined by marker passing along the arcs in a knowledge base. The intuition was
that semantically related concepts will be physically close in the knowledge base, and
can thus be found by traversing the arcs for a limited distance. But Polaroid Words
looked only for possible relatedness between words in the same sentence; trying to find
connections with all the words in preceding sentences was too complicated and too
likely to be led astray. The idea of lexical chains, however, can address this weakness in
Polaroid Words; lexical chains provide a constrained easy-to-determine representation
of context for consideration of semantic distance.
1.2.2 Cohesion and Discourse Structure. The second major importance of lexical
chains is that they provide a clue for the determination of coherence and discourse
structure.
</bodyText>
<page confidence="0.995082">
23
</page>
<note confidence="0.800528">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.999713386363636">
When a chunk of text forms a unit within a discourse, there is a tendency for
related words to be used. It follows that if lexical chains can be determined, they will
tend to indicate the structure of the text.
We will describe the application of lexical cohesion to the determination of the
discourse structure that was proposed by Grosz and Sidner (1986). Grosz and Sidner
propose a structure common to all discourse, which could be used along with a struc-
turally dependent focus of attention to delineate and constrain referring expressions.
In this theory there are three interacting components: linguistic structure, intentional
structure, and attentional state.
Linguistic structure is the segmentation of discourse into groups of sentences,
each fulfilling a distinct role in the discourse. Boundaries of segments can be fuzzy,
but some factors aiding in their determination are clue words, changes in intonation
(not helpful in written text), and changes in aspect and tense. When found, these
segments indicate changes in the topics or ideas being discussed, and hence will have
an effect on potential referents.
The second major component of the theory is the intentional structure. It is based
on the idea that people have definite purposes for engaging in discourse. There is
an overall discourse purpose, and also a discourse segment purpose for each of the
segments in the linguistic structure described above. Each segment purpose specifies
how the segment contributes to the overall discourse purpose. There are two structural
relationships between these segments. The first is called a dominance relation, which
occurs when the satisfaction (i.e., successful completion) of one segment&apos;s intention
contributes to the satisfaction of another segment&apos;s intention. The second relation is
called satisfaction precedence, which occurs when the satisfaction of one discourse seg-
ment purpose must occur before the satisfaction of another discourse segment purpose
can occur.
The third component of this theory is the attentional state. This is a stack-based
model of the set of things that attention is focused on at any given point in the dis-
course. It is &amp;quot;parasitic&amp;quot; on the intentional and linguistic structures, since for each
discourse segment there exists a separate focus space. The dominance relations and
satisfaction precedence relations determine the pushes and pops of this stack space.
When a discourse segment purpose contributes to a discourse segment purpose of the
immediately preceding discourse segment, the new focus space is pushed onto the
stack. If the new discourse segment purpose contributes to a discourse segment pur-
pose earlier in the discourse, focus spaces are popped off the stack until the discourse
segment that the new one contributes to is on the top of the stack.
It is crucial to this theory that the linguistic segments be identified, and as stated
by Grosz and Sidner, this is a problem area. This paper will show that lexical chains
are a good indication of the linguistic segmentation. When a lexical chain ends, there
is a tendency for a linguistic segment to end, as the lexical chains tend to indicate the
topicality of segments. If a new lexical chain begins, this is an indication or clue that
a new segment has begun. If an old chain is referred to again (a chain return), it is a
strong indication that a previous segment is being returned to. We will demonstrate
this in Section 4.
</bodyText>
<subsectionHeader confidence="0.999477">
1.3 Cohesion and Coherence
</subsectionHeader>
<bodyText confidence="0.99923">
The theory of coherence relations (H obbs 1978; Hirst 1981; McKeown 1985) will now be
considered in relation to cohesion. There has been some confusion as to the differences
between the phenomena of cohesion and coherence, e.g., Reichman (1985). There is a
danger of lumping the two together and losing the distinct contributions of each to
the understanding of the unity of text.
</bodyText>
<page confidence="0.993699">
24
</page>
<note confidence="0.424428">
Morris and Hirst Lexical Cohesion
</note>
<bodyText confidence="0.999519882352941">
Ultimately, the difference between cohesion and coherence is this: cohesion is a term
for sticking together; it means that the text all hangs together. Coherence is a term for
making sense; it means that there is sense in the text. Hence the term coherence relations
refers to the relations between sentences that contribute to their making sense.
Cohesion and coherence relations may be distinguished in the following way. A
coherence relation is a relation among clauses or sentences, such as elaboration, sup-
port, cause, or exemplification. There have been various attempts to classify all possible
coherence relations, but there is as yet no widespread agreement. There does not exist
a general computationally feasible mechanism for identifying coherence relations. In
contrast, cohesion relations are relations among elements in a text: reference, ellipsis,
substitution, conjunction, and lexical cohesion.
Since cohesion is well defined, one might expect that it would be computationally
easier to identify, because the identification of ellipsis, reference, substitution, conjunc-
tion, and lexical cohesion is a straightforward task for people. We will show below that
lexical cohesion is computationally feasible to identify. In contrast, the identification of
a specific coherence relation from a given set is not a straightforward task, even for
people. Consider this example from Hobbs (1978):
</bodyText>
<subsectionHeader confidence="0.59108">
Example 7
</subsectionHeader>
<listItem confidence="0.997959">
1. John can open Bill&apos;s safe.
2. He knows the combination.
</listItem>
<bodyText confidence="0.9962112">
Hobbs identifies the coherence relation as elaboration. But it could just as easily be
explanation. This distinction depends on context, knowledge, and beliefs. For example,
if you questioned John&apos;s ability to open Bill&apos;s safe, you would probably identify the
relation as explanation. Otherwise you could identify it as elaboration. Here is another
example:
</bodyText>
<subsectionHeader confidence="0.396417">
Example 8
</subsectionHeader>
<listItem confidence="0.9912935">
1. John bought a raincoat.
2. He went shopping yesterday on Queen Street and it rained.
</listItem>
<bodyText confidence="0.998411272727273">
The coherence relation here could be elaboration (on the buying), or explanation (of
when, how, or why), or cause (he bought the raincoat because it was raining out).
The point is that the identity of coherence relations is &amp;quot;interpretative,&amp;quot; whereas the
identity of cohesion relations is not. At a general level, even if the precise coherence
relation is not known, the relation &amp;quot;is about the same thing&amp;quot; exists if coherence exists.
In the example from Hobbs above, safe and combination are lexically related, which in
a general sense means they &amp;quot;are about the same thing in some way.&amp;quot; In example 8,
bought and shopping are lexically related, as are raincoat and rained. This shows how
cohesion can be useful in identifying sentences that are coherently related.
Cohesion and coherence are independent, in that cohesion can exist in sentences
that are not related coherently:
</bodyText>
<subsectionHeader confidence="0.865334">
Example 9
</subsectionHeader>
<bodyText confidence="0.984414666666667">
Wash and core six apples. Use them to cut out the material for your new suit. They
tend to add a lot to the color and texture of clothing. Actually, maybe you should use
five of them instead of six, since they are quite large.
</bodyText>
<page confidence="0.990865">
25
</page>
<figure confidence="0.397325666666667">
Computational Linguistics Volume 17, Number 1
Similarly, coherence can exist without textual cohesion:
Example 10
</figure>
<bodyText confidence="0.783883">
I came home from work at 6:00 p.m. Dinner consisted of two chicken breasts and a
bowl of rice.
Of course, most sentences that relate coherently do exhibit cohesion as well)
</bodyText>
<subsectionHeader confidence="0.989714">
1.4 The Importance of Both Cohesion and Coherence
</subsectionHeader>
<bodyText confidence="0.715271333333333">
Halliday and Hasan (1976) give two examples of lexical cohesion involving identity
of reference:
Example 11
</bodyText>
<listItem confidence="0.9813634">
1. Wash and core six cooking apples.
2. Put them into a fireproof dish.
Example 12
1. Wash and core six cooking apples.
2. Put the apples into a fireproof dish.
</listItem>
<bodyText confidence="0.999738923076923">
Reichman (1985, p. 180) writes &amp;quot;It is not the use of a pronoun that gives cohesion
to the wash-and-core-apples text. These utterances form a coherent piece of text not
because the pronoun them is used but because they jointly describe a set of cooking
instructions&amp;quot; (emphasis added). This is an example of lumping cohesion and coherence
together as one phenomenon. Pronominal reference is defined as a type of cohesion
(Halliday and Hasan 1976). Therefore the them in example 11 is an instance of it. The
important point is that both cohesion and coherence are distinct phenomena creating
unity in text.
Reichman also writes (1985, p. 179) &amp;quot;that similar words (apples, them, apples) appear
in a given stretch of discourse is an artifact of the content of discussion.&amp;quot; It follows that
if content is related in a stretch of discourse, there will be coherence. Lexical cohesion
is a computationally feasible clue to identifying a coherent stretch of text. In example
12, it is computationally trivial to get the word relationship between apples and apples,
and this relation fits the definition of lexical cohesion. Surely this simple indicator
of coherence is useful, since as stated above, there does not exist a computationally
feasible method of identifying coherence in non—domain-specific text. Cohesion is a
useful indicator of coherence regardless of whether it is used intentionally by writers
to create coherence, or is a result of the coherence of text.
Hobbs (1978) sees the resolution of coreference (which is a form of cohesion)
as being subsumed by the identification of coherence. He uses a formal definition
of coherence relations, an extensive knowledge base of assertions and properties of
objects and actions, and a mechanism that searches this knowledge source and makes
simple inferences. Also, certain elements must be assumed to be coreferential.
He shows how, in example (7), an assumption of coherence allows the combination
to be identified as the combination of Bill&apos;s safe and John and he to be found to be
coreferential.
</bodyText>
<footnote confidence="0.995937333333333">
1 There is an interesting analogy between cohesion and syntax, and coherence and semantics. Jabberwocky
(Carroll 1872) is an example of syntax sticking text together without semantics. Example 10 illustrates
coherence sticking text together without cohesion.
</footnote>
<page confidence="0.996244">
26
</page>
<note confidence="0.426679">
Morris and Hirst Lexical Cohesion
</note>
<bodyText confidence="0.998883666666667">
But lexical cohesion would also indicate that safe and combination can be assumed
to be coreferential. And more importantly, one should not be misled by chicken-and-
egg questions when dealing with cohesion and coherence. Rather, one should use
each where applicable. Since the lexical cohesion between combination and safe is easy
to compute, we argue that it makes sense to use this information as an indicator of
coherence.
</bodyText>
<sectionHeader confidence="0.52762" genericHeader="method">
2. The Thesaurus and Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.999826">
The thesaurus was conceived by Peter Mark Roget, who described it as being the
&amp;quot;converse&amp;quot; of a dictionary. A dictionary explains the meaning of words, whereas a
thesaurus aids in finding the words that best express an idea or meaning. In Section 3,
we will show how a thesaurus can be used to find lexical chains in text.
</bodyText>
<subsectionHeader confidence="0.937664">
2.1 The Structure of the Thesaurus
</subsectionHeader>
<bodyText confidence="0.985444071428571">
Roget&apos;s International Thesaurus, 4th Edition (1977) is composed of 1042 sequentially num-
bered basic categories. There is a hierarchical structure both above and below this
level (see Figure 1). Three structure levels are above the category level. The topmost
level consists of eight major classes developed by Roget in 1852: abstract relations,
space, physics, matter, sensation, intellect, volition, and affections. Each class is di-
vided into (roman-numbered) subclasses, and under each subclass there is a (capital-
letter-sequenced) sub-subclass. These in turn are divided into the basic categories.
Where applicable, categories are organized into antonym pairs. For example, cate-
gory 407 is Life, and category 408 is Death.
Each category contains a series of numbered paragraphs to group closely related
words. Within each paragraph, still finer groups are marked by semicolons. In addition,
a semicolon group may have cross-references or pointers to other related categories
or paragraphs. A paragraph contains words of only one syntactic category. The noun
paragraphs are grouped at the start of a category, followed by the paragraphs for
</bodyText>
<figure confidence="0.4543546">
Class 1 • • •
Class 4: Matter
Ill Organic Matter
B Vitality
407 Life
</figure>
<footnote confidence="0.5310026">
1. NOUNS life, living, vitality, being alive, having life, animation, ani-
mate existence; liveliness, animal spirits, vivacity, spriteliness; long life,
longevity; viability; lifetime 110.5; immortality 112.3; birth 167; eels.
trace 1; bio-, organ-; -biosis.
408 Death • • •
</footnote>
<figureCaption confidence="0.849829">
Figure 1
</figureCaption>
<bodyText confidence="0.362318">
The structure of Roget&apos;s Thesaurus
</bodyText>
<page confidence="0.981192">
27
</page>
<figure confidence="0.951189166666667">
Computational Linguistics Volume 17, Number 1
Lid
clothing 231.35
cover 228.5
eyelid 439.9
stopper 266.4
</figure>
<figureCaption confidence="0.990226">
Figure 2
</figureCaption>
<bodyText confidence="0.9700525">
Index entry for the word lid
verbs, adjectives, and so on.
The thesaurus has an index, which allows for retrieval of words related to a given
one. For each entry, a list of words suggesting its various distinct subsenses is given,
and a category or paragraph number for each of these. Figure 2 shows the index entry
for lid. To find words related to lid in its sense of cover, one would turn to paragraph
5 of category 228. An index entry may be a pointer to a category or paragraph if there
are no subsenses to be distinguished.
</bodyText>
<subsectionHeader confidence="0.538107">
2.2 Differences from Traditional Knowledge Bases
</subsectionHeader>
<bodyText confidence="0.999942233333333">
In the structure of traditional artificial intelligence knowledge bases, such as frames or
semantic networks, words or ideas that are related are actually &amp;quot;physically close&amp;quot; in
the representation. In a thesaurus this need not be true. Physical closeness has some
importance, as can be seen clearly from the hierarchy, but words in the index of the
thesaurus often have widely scattered categories, and each category often points to a
widely scattered selection of categories.
The thesaurus simply groups words by idea. It does not have to name or classify
the idea or relationship. In traditional knowledge bases, the relationships must be
named. For example, in a semantic net, a relationship might be isa or color-of, and in
a frame database, there might be a slot for color or location.
In Section 1, different types of word relationships were discussed: systematic se-
mantic, nonsystematic semantic, word association, and words related by a common
situation. A factor common to all but situational relationships is that there is a strong
tendency for the word relationships to be captured in the thesaurus. This holds even
for the nonsystematic semantic relations, which are the most problematic by defini-
tion. A thesaurus simply groups related words without attempting to explicitly name
each relationship. In a traditional computer database, a systematic semantic relation-
ship can be represented by a slot value for a frame, or by a named link in a semantic
network. If it is hard to classify a relationship in a systematic semantic way, it will be
hard to represent the relationship in a traditional frame or semantic network formal-
ism. Of the 16 nonsystematic semantic lexical chains given as examples in Halliday
and Hasan (1976), 14 were found in Roget&apos;s Thesaurus (1977) using the relations given
in Section 3.2.2. This represents an 87% hit rate (but not a big sample space). Word
associations show a strong tendency to be findable in a thesaurus. Of the 16 word
association pairs given in Hirst (1987), 14 were found in Roget&apos;s Thesaurus (1977). Since
two of the word senses were not contained in the thesaurus at all, this represents a
100% hit rate among those that were. Situational word relationships are not as likely
to be found in a general thesaurus. An example of a situational relationship is between
car and lights, where the two words are clearly related in the situation involving a car&apos;s
lights, but the relationship will not be found between them in a general thesaurus.
</bodyText>
<page confidence="0.987399">
28
</page>
<note confidence="0.604848">
Morris and Hirst Lexical Cohesion
</note>
<sectionHeader confidence="0.838092" genericHeader="method">
3. Finding Lexical Chains
</sectionHeader>
<subsectionHeader confidence="0.984257">
3.1 General Methodology
</subsectionHeader>
<bodyText confidence="0.99999">
We now describe a method of building lexical chains for use as an aid in determining
the structure of text. This section details how these lexical chains are formed, using a
thesaurus as the main knowledge base. The method is intended to be useful for text in
any general domain. Unlike methods that depend on a full understanding of text, our
method is the basis of a computationally feasible approach to determining discourse
structure.
We developed our method in the following way. First, we took five texts, total-
ing 183 sentences, from general-interest magazines (Reader&apos;s Digest, Equinox, The New
Yorker, Toronto, and The Toronto Star). Using our intuition (i.e., common sense and a
knowledge of English), we identified the lexical chains in each text. We then formal-
ized our intuitions into an algorithm, using our experience with the texts to set values
for the following parameters (to be discussed below).
</bodyText>
<listItem confidence="0.999959666666667">
• thesaural relations
• transitivity of word relations
• distance (in sentences) allowable between words in a chain
</listItem>
<bodyText confidence="0.999865">
The aim was to find efficient, plausible methods that will cover enough cases to ensure
the production of meaningful results.
</bodyText>
<subsectionHeader confidence="0.999553">
3.2 Forming Lexical Chains
</subsectionHeader>
<subsubsectionHeader confidence="0.669402">
3.2.1 Candidate Words. The first decision in lexical chain formation is which words in
</subsubsectionHeader>
<bodyText confidence="0.999757666666667">
the text are candidates for inclusion in chains. As pointed out by Halliday and Hasan
(1976), repetitive occurrences of closed-class words such as pronouns, prepositions,
and verbal auxiliaries are obviously not considered. Also, high-frequency words like
good, do, and taking do not normally enter into lexical chains (with some exceptions
such as takings used in the sense of earnings). For example, in (13) only the italicized
words should be considered as lexical chain candidates:
</bodyText>
<subsectionHeader confidence="0.493335">
Example 13
</subsectionHeader>
<bodyText confidence="0.974570785714286">
My maternal grandfather lived to be 111. Zayde was lucid to the end, but a few years
before he died the family assigned me the task of talking to him about his problem with
alcohol.
It should be noted that morphological analysis on candidate words was done intu-
itively, and would actually have to be formally implemented in an automated system.
3.2.2 Building Chains. Once the candidate words are chosen, the lexical chains can be
formed. For this work an abridged version of Roget&apos;s Thesaurus (1977) was used. The
chains were built by hand. Automation was not possible, for lack of a machine-readable
copy of the thesaurus. Given a copy, implementation would clearly be straightforward.
It is expected that research with an automated system and a large sample space of text
would give valuable information on the fine-tuning of the parameter settings used in
the general algorithm.
Five types of thesaural relations between words were found to be necessary in
forming chains, but two (the first two below) are by far the most prevalent, constituting
</bodyText>
<page confidence="0.976541">
29
</page>
<note confidence="0.247133">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.703831">
over 90% of the lexical relationships. The relationships are the following:
</bodyText>
<listItem confidence="0.9591858">
1. Two words have a category common in their index entries. For example,
residentialness and apartment both have category 189 in their index entries
(see Figure 3.1).
2. One word has a category in its index entry that contains a pointer to a
category of the other word. For example car has category 273 in its index
entry, and that contains a pointer to category 276, which is a category of
the word driving (see Figure 3.2).
3. A word is either a label in the other word&apos;s index entry (see Figure 3.3b),
or is in a category of the other word. For example, blind has category 442
in its index entry, which contains the word see (see Figure 3.3a).
4. Two words are in the same group, and hence are semantically related.
For example, blind has category 442, blindness, in its index entry and see
has category 441, vision, in its index entry (see Figure 3.4).
5. The two words have categories in their index entries that both point to a
common category. For example, brutal has category 851, which in turn
</listItem>
<figureCaption confidence="0.955777551724138">
word 1 index
label 1: 521 thesaurus category 521
label 2: 589
label 3: 626
word 2 index
label 1: 860
label 2: 521
thesaurus category 521
Rs860
thesaurus category 860
word 1
label 1: 1: 521
label 2: 589
label 3: 626
word 2 index
label 1: 300
label 2: 660
word 1 index
label 1: 521
label 2: 589
label 3: 626
thesaurus category 521
word 2
word 1 index
label 1: 521
word 2: 589
label 3: 626
Figure 3
Thesaural Relations, parts (1)-(3)
</figureCaption>
<figure confidence="0.971715416666666">
(b)
30
(a)
(3)
Morris and Hirst Lexical Cohesion
thesaurus category 23
thesaurus category 521
8,457
word 1 index
label lx
word 2: 589
label 3: 621
</figure>
<figureCaption confidence="0.750156857142857">
word 2 index
label 1: x+1
label 2: 860
word 1 index
label 1: 300
label 2: 521
label 3: 621
</figureCaption>
<figure confidence="0.873374142857143">
&amp;457
word 2 index
label 1: 23
label 2: 600
thesaurus category 457
Figure 3
Continued. Thesaural Relations, parts (4)-(5)
</figure>
<bodyText confidence="0.999574740740741">
has a pointer to category 830. Terrified has category 860 that likewise has
a pointer to category 830 (see Figure 3.5).
One must consider how much transitivity to use when computing lexical chains.
Specifically, if word a is related to word b, word b is related to word c, and word c is
related to word d then is word a related to words c and d?
Consider this chain: {cow, sheep, wool, scarf, boots, hat, snow} . If unlimited transitivity
were allowed, then cow and snow would be considered related, which is definitely
counter intuitive. Our intuition was to allow one transitive link: word a is related
to word c but not to word d. It seemed that two or more transitive links would so
severely weaken the word relationship as to cause it to be nonintuitive. Our analysis
of our sample texts supported this. To summarize, a transitivity of one link is sufficient
to successfully compute the intuitive chains. An automated system could be used to
test this out extensively, varying the number of transitive links and calculating the
consequences. It is likely that it varies slightly with respect to style, author, or type of
text.
There are two ways in which a transitive relation involving one link can cause
two words to be related. In the first way, if word a is related to word b, and word b
is related to word c, then word a is related to word c. In the second way, if word a is
related to word b, and word a is related to word c, then word b is related to word c.
But lexical chains are calculated only with respect to the text read so far. For example,
if word c is related to word a and to word b, then word a and word b are not related,
since at the time of processing, they were not relatable. Symmetry was not found to
be necessary for computing the lexical chains.
We now consider how many sentences can separate two words in a lexical chain
before the words should be considered unrelated. Now, sometimes, several sentences
after a chain has clearly stopped, it is returned to. Such chain returns link together
larger expanses of text than are contained in single chains or chain segments. Returns
</bodyText>
<page confidence="0.998974">
31
</page>
<note confidence="0.484272">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.999737130434783">
to existing chains often correspond to intentional boundaries, as they occur after di-
gressions or subintentions, thereby signalling a resumption of some structural text
entity.
Intuitively, the distance between words in a chain is a factor in chain formation.
The distance will not be &amp;quot;large,&amp;quot; because words in a chain co-relate due to recognizable
relations, and large distances would interfere with the recognition of relations.
The five texts were analyzed with respect to distance between clearly related
words. The analysis showed that there can be up to two or three intermediary sen-
tences between a word and the preceding element of a chain segment with which it
can be linked. At distances of four or more intermediary sentences, the word is only
able to signal a return to an existing chain. Returns happened after between 4 and
19 intermediary sentences in the sample texts. One significant fact emerged from this
analysis: returns consisting of one word only were always made with a repetition of
one of the words in the returned-to chain. Returns consisting of more than one word
did not necessarily use repetition — in fact in most cases, the first word in the return
was not a repetition.
The question of chain returns and when they can occur requires further research.
When distances between relatable words are not tightly bound (as in the case of
returns), the chances of incorrect chain linkages increase. It is anticipated that chain
return analysis would become integrated with other text processing tools in order to
prevent this. Also, we believe that chain strength analysis will be required for this
purpose. Intuitively, some lexical chains are &amp;quot;stronger&amp;quot; than others, and possibly only
strong chains can be returned to. There are three factors contributing to chain strength.
</bodyText>
<listItem confidence="0.999938">
1. Reiteration — the more repetitions, the stronger the chain.
2. Density — the denser the chain, the stronger it is.
3. Length — the longer the chain, the stronger it is.
</listItem>
<bodyText confidence="0.9887955">
Ideally, some combination of values reflecting these three factors should result in a
chain strength value that can be useful in determining whether a chain is strong
enough to be returned to. Also, a strong chain should be more likely to have a struc-
tural correspondence than a weak one. It seems likely that chains could contain par-
ticularly strong portions with special implications for structure. These issues will not
be addressed here.
3.2.3 Notation and Data Structures. In the computation of lexical chains, the following
information is kept for each word in a chain:
</bodyText>
<listItem confidence="0.997439">
• A word number, which is a sequential, chain-based number for each
word so that it can be uniquely identified.
• The sentence number in which the word occurs.
• The chain created so far.
Each lexical relationship in a chain is represented as (u,v) where:
• u is the current word number,
• v is the word number of the related word,
• x is the transitive distance:
</listItem>
<page confidence="0.979587">
32
</page>
<figure confidence="0.8731605">
Morris and Hirst Lexical Cohesion
Chain 1
Word Sentence Lexical Chain
1. evade 15
2. feigning 15 (2,1)
3. escaped 16 (3,1)? (3,2)T1
</figure>
<figureCaption confidence="0.8655125">
Figure 4
Lexical chain notation
</figureCaption>
<listItem confidence="0.838963">
— 0 means no transitive link was used to form the word
relationship
— 1 means one transitive link was used to form the word
relationship
• y is either
— the number of the thesaural relationship between the two words
(as given in Section 3.2.2)
— Tq where
</listItem>
<bodyText confidence="0.918082166666667">
* T stands for transitively related
* q is the word number through which the transitive
relation is formed.
A full example of this notation is shown in Figure 4.
Figure 5 shows the generalized algorithm for computing lexical chains. The pa-
rameter values that we used are shown for the following:
</bodyText>
<listItem confidence="0.9998785">
• candidate words
• thesaural relations
• transitivity of word relations
• distance between words in a chain.
</listItem>
<bodyText confidence="0.999631">
The only parameter not addressed in this work is which (if any) chains should be
eliminated from the chain-finding process.
</bodyText>
<subsectionHeader confidence="0.999739">
3.3 Problems and Concerns
</subsectionHeader>
<bodyText confidence="0.999950666666667">
This section is a discussion of problems encountered during the computation of the
lexical chains contained in our corpus of texts. The text example used in this paper is
in Section 4.2, and the chains found in the example are in Appendix A.
</bodyText>
<subsectionHeader confidence="0.675792">
3.3.1 Where the Thesaurus Failed to Find Lexical Relations. The algorithm found
</subsectionHeader>
<bodyText confidence="0.999209625">
well over 90% of the intuitive lexical relations in the five examples we studied. The
following is an analysis of when the thesaurus failed to find a relationship and why.
One problem was when the relationship between words was due more to their
&amp;quot;feel&amp;quot; than their meaning. For example, in chain 6, the intuitive chain {hand-in-hand,
matching, whispering, laughing, warm} was not entirely computable. Only the italicized
words were relatable. The words in chain 6 are cohesive by virtue of being general, but
strong, &amp;quot;good&amp;quot; words related by their goodness, rather than by their specific meanings.
Chain 10, {environment, setting, surrounding}, was not thesaurally relatable. Setting was
</bodyText>
<page confidence="0.98381">
33
</page>
<figure confidence="0.938578625">
Computational Linguistics Volume 17, Number 1
REPEAT
READ next word
IF word is suitable for lexical analysis (see section 3.2.1) THEN
CHECK for chains within a suitable span
(up to 3 intermediary sentences, and no limitation on
returns):
CHECK thesaurus for relationships (section 3.2.2).
CHECK other knowledge sources
(situational, general words, proper names).
IF chain relationship is found THEN
INCLUDE word in chain.
CALCULATE chain so far
(allow one transitive link).
END IF
IF there are words that have not formed a chain for a suitable
number of sentences (up to 3) THEN
ELIMINATE words from the span.
END IF
CHECK new word for relevance to existing chains that
are suitable for checking.
ELIMINATE chains that are not suitable for checking.
END IF
END REPEAT
</figure>
<figureCaption confidence="0.958407">
Figure 5
</figureCaption>
<subsectionHeader confidence="0.529769">
Algorithm for Finding Lexical Chains
</subsectionHeader>
<bodyText confidence="0.999651571428571">
not in the thesaurus, and while it seems as though environment and surrounding should
be thesaurally connected, they were not.
Place names, street names, and people&apos;s names are generally not to be found
in Roget&apos;s Thesaurus (1977). However, they are certainly contained in one&apos;s &amp;quot;mental
thesaurus.&amp;quot; Chain 1, which contains several major Toronto street names, is a good
example of this. These names were certainly related to the rest of chain 1 in the
authors&apos; mental thesaurus, since we are residents of Toronto (and indeed the article
assumed a knowledge of the geography of the city). In chain 5, the thesaurus did not
connect the words pine and trunk with the rest of the chain {virgin, bush, trees, trees}.
In a general thesaurus, specific information on, and classification of, plants, animals,
minerals, etc., is not available.
To summarize, there were few cases in which the thesaurus failed to confirm an
intuitive lexical chain. For those cases in which the thesaurus did fail, three missing
knowledge sources became apparent.
</bodyText>
<listItem confidence="0.999438">
1. General semantic relations between words of similar &amp;quot;feeling.&amp;quot;
2. Situational knowledge.
3. Specific proper names.
</listItem>
<subsubsectionHeader confidence="0.509562">
3.3.2 Problems with Distances and Chain Returns. Occasionally the algorithm would
</subsubsectionHeader>
<bodyText confidence="0.541385">
cause two chains to merge together, whereas intuition would lead one to keep them
</bodyText>
<page confidence="0.989658">
34
</page>
<note confidence="0.672804">
Morris and Hirst Lexical Cohesion
</note>
<bodyText confidence="0.996646727272727">
separate. We found the following intuitively separate chain beginning in sentence
38: {people, Metropolitan Toronto, people, urban, population, people, population, popula-
tion, people} . However, the algorithm linked this chain with chain 1, which runs
through the entire example and consists of these words and others: {city, suburbs,
traffic, community}. Fortunately, this was a rare occurrence. But note that there will be
cases in which lexical chains should be merged as a result of the intentional merging
of ideas or concepts in the text.
Conversely, there were a few cases of unfortunate chain returns occurring where
they were definitely counter intuitive. In chain 3, word 4, wife, was taken as a one-
word return to the chain {married, wife, wife}. However, there is no intuitive reason for
this.
</bodyText>
<sectionHeader confidence="0.491578" genericHeader="method">
4. Using Lexical Chains to Determine Text Structure
</sectionHeader>
<bodyText confidence="0.906259">
This section describes how lexical chains formed by the algorithm given in Section
3.2.3 can be used as a tool.
</bodyText>
<subsectionHeader confidence="0.999832">
4.1 Lexical Chains and Text Structure
</subsectionHeader>
<bodyText confidence="0.999667142857143">
Any structural theory of text must be concerned with identifying units of text that are
about the same thing. When a unit of text is about the same thing there is a strong
tendency for semantically related words to be used within that unit. By definition,
lexical chains are chains of semantically related words. Therefore it makes sense to
use them as clues to the structure of the text.
This section will concentrate on analyzing correspondences between lexical chains
and structural units of text, including:
</bodyText>
<listItem confidence="0.995183571428572">
• the correspondence of chain boundaries to structural unit boundaries;
• returns to existing chains and what they indicate about structural units;
• lexical chain strength and reliability of predicting correspondences
between chains and structural units;
• an analysis of problems encountered, and when extra textual information
is required to validate the correspondences between lexical chains and
structural components.
</listItem>
<bodyText confidence="0.9996482">
The text structure theory chosen for this analysis was that of Grosz and Sidner (1986).
It was chosen because it is an attempt at a general domain-independent theory of
text structure that has gained a significant acceptance in the field as a good standard
approach.
The methodology we used in our analyses was as follows:
</bodyText>
<listItem confidence="0.680441375">
1. We determined the lexical chain structure of the text using the algorithm
given in Section 3.2.3. (In certain rare cases where the algorithm did not
form intuitive lexical chains properly, it is noted, both in Section 3.4 and
in the analysis in this section. The intuitive chain was used for the
analysis; however the lexical chain data given in Appendix A show the
rare mismatches between intuition and the algorithm.)
2. We determined the intentional structure of the text using the theory
outlined by Grosz and Sidner.
</listItem>
<page confidence="0.990601">
35
</page>
<note confidence="0.382146">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.547434">
3. We compared the lexical structure formed in step 1 with the intentional
structure formed in step 2, and looked for correspondences between
them.
</bodyText>
<subsectionHeader confidence="0.999736">
4.2 An Example
</subsectionHeader>
<bodyText confidence="0.99354925">
Example 14 shows one of the five texts that we analyzed. It is the first section of an
article in Toronto magazine, December 1987, by Jay Teitel, entitled &amp;quot;Outland.&amp;quot;2 The
tables in Appendix A show the lexical chains for the text. (The other four texts and
their analyses are given in Morris 1988.)
</bodyText>
<subsectionHeader confidence="0.250038">
Example 14
</subsectionHeader>
<listItem confidence="0.996057151515151">
1. III spent the first 19 years of my life in the suburbs, the initial 14 or so relatively
contented, the last four or five wanting mainly to be elsewhere.
2. The final two I remember vividly: I passed them driving to and from the University
of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness.
3. The car&apos;s lights never worked -- every dusk turned into a kind of medieval race
against darkness, a panicky, mournful rush north, away from everything I knew was
exciting, toward everything I knew was deadly.
4. I remember looking through the windows at the commuters mired in traffic beside
me and actively hating them for their passivity.
5. I actually punched holes in the white vinyl ceiling of the Volks and then, by way of
penance, wrote beside them the names and phone numbers of the girls I would call
when I had my own apartment in the city.
6. One thing I swore to myself: I would never live in the suburbs again.
7. ¶My aversion was as much a matter of environment as it was traffic — one particular
piece of the suburban setting: the &amp;quot;cruel sun.&amp;quot;
8. Growing up in the suburbs you can get used to a surprising number of things —
the relentless &amp;quot;residentialness&amp;quot; of your surroundings, the weird certainty you have
that everything will stay vaguely new-looking and immune to historic soul no matter
how many years pass.
9. You don&apos;t notice the eerie silence that descends each weekday when every sound is
drained out of your neighbourhood along with all the people who&apos;ve gone to work.
10. I got used to pizza, and cars, and the fact that the cultural hub of my community
was the collective TV set.
11. But once a week I would step outside as dusk was about to fall and be absolutely
bowled over by the setting sun, slanting huge and cold across the untreed front lawns,
reminding me not just how barren and sterile, but how undefended life could be.
12. As much as I hated the suburban drive to school, I wanted to get away from the
cruel suburban sun.
13. ¶When I was married a few years later, my attitude hadn&apos;t changed.
14. My wife was a city girl herself, and although her reaction to the suburbs was less
intense than mine, we lived in a series of apartments safely straddling Bloor Street.
15. But four years ago, we had a second child, and simultaneously the school my wife
taught at moved to Bathurst Street north of Finch Avenue.
</listItem>
<page confidence="0.77606">
2 C) Jay Teitel. Reprinted with kind permission of the author.
36
</page>
<note confidence="0.571213">
Morris and Hirst Lexical Cohesion
</note>
<listItem confidence="0.952773536585366">
16. She was now driving 45 minutes north to work every morning, along a route that
was perversely identical to the one I&apos;d driven in college.
17. IfWe started looking for a house.
18. Our first limit was St. Clair — we would go no farther north.
19. When we took a closer look at the price tags in the area though, we conceded that
maybe we&apos;d have to go to Eglinton — but that was definitely it.
20. But the streets whose names had once been magical barriers, latitudes of tolerance,
quickly changed to something else as the Sundays passed.
21. Eglinton became Lawrence, which became Wilson, which became Sheppard.
22. One wind-swept day in May I found myself sitting in a town-house development
north of Steeles Avenue called Shakespeare Estates.
23. It wasn&apos;t until we stepped outside, and the sun, blazing unopposed over a country
club, smacked me in the eyes, that I came to.
24. It was the cruel sun.
25. We got into the car and drove back to the Danforth and porches as fast as we
could, grateful to have been reprieved.
26. IfAnd then one Sunday in June I drove north alone.
27. This time I drove up Bathurst past my wife&apos;s new school, hit Steeles, and kept
going, beyond Centre Street and past Highway 7 as well.
28. I passed farms, a man selling lobsters out of his trunk on the shoulder of the road,
a chronic care hospital, a country club and what looked like a mosque.
29. I reached a light and turned right.
30. I saw a sign that said Houses and turned right again.
31. ¶In front of me lay a virgin crescent cut out of pine bush.
32. A dozen houses were going up, in various stages of construction, surrounded by
hummocks of dry earth and stands of precariously tall trees nude halfway up their
trunks.
33. They were the kind of trees you might see in the mountains.
34. A couple was walking hand-in-hand up the dusty dirt roadway, wearing matching
blue track suits.
35. On a &amp;quot;front lawn&amp;quot; beyond them, several little girls with hair exactly the same
colour of blond as my daughter&apos;s were whispering and laughing together.
36. The air smelled of sawdust and sun.
37. IfIt was a suburb, but somehow different from any suburb I knew.
38. It felt warm.
39. IfIt was Casa Drive.
40. In 1976 there were 2,124,291 people in Metropolitan Toronto, an area bordered by
Steeles Avenue to the north, Etobicoke Creek on the west, and the Rouge River to the
east.
41. In 1986, the same area contained 2,192,721 people, an increase of 3 percent, all but
negligible on an urban scale.
</listItem>
<bodyText confidence="0.9250562">
42. In the same span of time the three outlying regions stretching across the top of
Metro — Peel, Durham, and York — increased in population by 55 percent, from
814,000 to some 1,262,000.
43. Half a million people had poured into the crescent north of Toronto in the space of
a decade, during which time the population of the City of Toronto actually declined
as did the populations of the &amp;quot;old&amp;quot; suburbs with the exception of Etobicoke and
Scarborough.
44. If the sprawling agglomeration of people known as Toronto has boomed in the
past 10 years it has boomed outside the traditional city confines in a totally new city,
a new suburbia containing one and a quarter million people.
</bodyText>
<page confidence="0.996847">
37
</page>
<note confidence="0.45944">
Computational Linguistics Volume 17, Number 1
</note>
<subsectionHeader confidence="0.975284">
4.3 The Correspondences between Lexical and Intentional Structures
</subsectionHeader>
<bodyText confidence="0.996689">
In Figure 6 we show the intentional structure of the text of Section 4.2, and in Figure 7
we show the correspondences between the lexical chains and intentions of the example.
There is a clear correspondence between chain 1, {. , driving, car&apos;s, . . . }, and
intention 1 (changing attitudes to suburban life). The continuity of the subject matter is
reflected by the continuous lexical chain. From sentence 40 to sentence 44, two words,
population and people are used repetitively in the chain. Population is repeated three
times, and people is repeated five times. If chain strength (indicated by the reiteration)
were used to delineate &amp;quot;strong&amp;quot; portions of a chain, this strength information could
also be used to indicate structural attributes of the text. Specifically, sentences 40 to
44 form intention 1.3 (why new suburbs exist), and hence a strong portion of the
</bodyText>
<figure confidence="0.997980107142857">
1 (1-44)
Changing attitudes to suburban life.
1.1 (1-25)
Earlier aversion to suburban life.
1.1.1 (1-7)
Hatred of commuting.
1.1.2 (8-12)
The hated suburb environment.
1.1.3 (13-25)
How this old aversion to suburbs held, when a recent attempt was made to
buy a new house in the suburbs.
1.1.3.1 (13-16)
How life changed, giving author reason to look for a new house.
1.1.3.2 (17-22)
Houses are too expensive in Metro Toronto, hence one must look in the
suburbs to buy a house.
1.1.3.3 (23-25)
The old familiar aversion to suburbs came back.
1.2 (26-39)
A new suburb that seems livable in and nice.
1.2.1 (26-30)
The drive to the new suburb.
1.2.2 (31-33)
The forested area.
1.2.3 (34-39)
The pleasant environment.
1.3 (40-44)
Why the new suburbs exist.
</figure>
<figureCaption confidence="0.975573">
Figure 6
</figureCaption>
<table confidence="0.924438153846154">
The Intentional Structure of Example 14 (showing topics the writer intends to discuss)
Chain Chain Intention Intention
Range Range
1 1-44 1 1-44
2.1 2-12 1.1.1, 1.1.2 1-12
2.2 16 end of 1.1.3.1 16
2.3 24 end of 1.1.3.3 25
3 13-15 1.1.3.1 13-16
4 19-20 1.1.3.2 17-22
5 31-33 1.2.2 31-33
6 34-38 1.2.3 34-39
7,8 1-3 1.1.1 1-7
9 7-8 1.1.2 8-12
</table>
<figureCaption confidence="0.7794415">
Figure 7
Correspondences between lexical and intentional structures
</figureCaption>
<page confidence="0.994183">
38
</page>
<note confidence="0.739716">
Morris and Hirst Lexical Cohesion
</note>
<bodyText confidence="0.9979314">
chain would correspond exactly to a structural unit. In addition, drive was repeated
eight times between sentence 2 and sentence 26, corresponding to intention 1.1 (earlier
aversion to suburban life). Suburb was repeated eleven times throughout the entire
example, indicating the continuity in structure between sentences 1-44.
Chain 2.1, {afflicted, darkness, . . . }, from sentence 2 to sentence 12, corresponds
to intentions 1.1.1 (hatred of commuting) and 1.1.2 (hatred of suburbs). More textual
information is needed to separate intentions 1.1.1 and 1.1.2. There is a one-word return
to chain 2 at sentences 16 and 24, strongly indicating that chain 2 corresponds to
intention 1.1, which runs from sentence 1 to sentence 25. Also, segment 2.2 coincides
with the end of intention 1.1.3.1 (how life changed), and segment 2.3 coincides with
the end of intention 1.1.3.3 (old familiar aversion to suburbs). This situation illustrates
how chain returns help indicate the structure of the text. If chain returns were not
considered, chain 2 would end at sentence 12, and the structural implications of the
two single-word returns would be lost. It is intuitive that the two words perverse and
cruel indicate links back to the rest of intention 1.1. The link provided by the last return,
cruel, is especially strong, since it occurs after the diversion describing the attempt to
find a nice house in the suburbs. Cruel is the third reiteration of the word in chain 2.
Chain 3, {married, wife, . . .}, corresponds to intention 1.1.3.1 (if the unfortunate
chain return mentioned in section 3.4.2 is ignored) and chain 4 {conceded, tolerance},
corresponds to intention 1.1.3.2 (expensive houses in Metro Toronto). The boundaries
of chain 4 are two sentences inside the boundaries of the intention. The existence of a
lexical chain is a clue to the existence of a separate intention, and boundaries within
one or two sentences of the intention boundaries are considered to be close matches.
Chain 5, {virgin, pine, . . . }, corresponds closely to intention 1.2.2 (forested area).
Chain 6, {hand-in-hand, matching, . . . }, corresponds closely to intention 1.2.3 (pleasant
environment). Chains 7, {first, initial, final}, and 8, {night, dusk, darkness}, are a couple of
short chains (three words long) that overlap. They collectively correspond to intention
1.1.1 (hatred of commuting). The fact that they are short and overlapping suggests
that they could be taken together as a whole.
Chain 9, {environment, setting, surrounding}, corresponds to intention 1.1.2 (hated
suburbs). Even though the chain is a lot shorter in length than the intention, its pres-
ence is a clue to the existence of a separate intention in its textual vicinity. Since the
lexical chain boundary is more than two sentences away from the intention boundary,
other textual information would be required to confirm the structure.
Overall, the lexical chains found in this example provide a good clue for the
determination of the intentional structure. In some cases, the chains correspond exactly
to an intention. It should also be stressed, however, that the lexical structures cannot
be used on their own to predict an exact structural partitioning of the text. This of
course was never expected. As a good example of the limitations of the tool, intention
1.2 (nice new suburb) starts in sentence 26, but there are no new lexical chains starting
there. The only clue to the start of the new intention would be the ending of chain 2
{afflicted, darkness, . . . }.
This example also provides a good illustration (chain 2) of the importance of chain
returns being used to indicate a high-level intention spanning the length of the entire
chain (including all segments). Also, the returns coincided with intentional boundaries.
</bodyText>
<sectionHeader confidence="0.940327" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.899553666666667">
The motivation behind this work was that lexical cohesion in text should correspond
in some way to the structure of the text. Since lexical cohesion is a result of a unit of
text being, in some recognizable semantic way, about a single topic, and text structure
</bodyText>
<page confidence="0.9974">
39
</page>
<note confidence="0.594604">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.999866842105263">
analysis involves finding the units of text that are about the same topic, one should
have something to say about the other. This was found to be true. The lexical chains
computed by the algorithm given in Section 3.2.3 correspond closely to the intentional
structure produced from the structural analysis method of Grosz and Sidner (1986).
This is important, since Grosz and Sidner give no method for computing the intentions
or linguistic segments that make up the structure that they propose.
Hence the concept of lexical cohesion, defined originally by Halliday and Hasan
(1976) and expanded in this work, has a definite use in an automated text under-
standing system. Lexical chains are shown to be almost entirely computable with
the relations defined in Section 3.2.2. The computer implementation of this type of
thesaurus access would be a straightforward task involving traditional database tech-
niques. The program to implement the algorithm given in Section 3.2.3 would also
be straightforward. However, automated testing could help fine-tune the parameters,
and would help to indicate any unfortunate chain linkages. Although straightforward
from an engineering point of view, the automation would require a significant effort. A
machine-readable thesaurus with automated index searching and lookup is required.
The texts we have analyzed, here and elsewhere (Morris 1988) are general-interest
articles taken from magazines. They were chosen specifically to illustrate that lexical
cohesion, and hence this tool, is not domain-specific.
</bodyText>
<subsectionHeader confidence="0.661711">
5.1 Improvements on Earlier Research
</subsectionHeader>
<bodyText confidence="0.9998863">
The methods used in this work improve on those from Halliday and Hasan (1976).
Halliday and Hasan related words back to the first word to which they are tied, rather
than forming explicit lexical chains that include the relationships to intermediate words
in the chain. They had no notions of transitivity, distance between words in a chain,
or chain returns. Their intent was not a computational means of finding lexical chains,
and they did not suggest a thesaurus for this purpose.
Ventola (1987) analyzed lexical cohesion and text structure within the framework of
systemic linguistics and the specific domain of service encounters such as the exchange
of words between a client at a post office and a postal worker. Ventola&apos;s chain-building
rule was that each lexical item is &amp;quot;taken back once to the nearest preceding lexically
cohesive item regardless of distance&amp;quot; (p. 131). In our work the related words in a chain
are seen as indicating structural units of text, and hence distance between words
is relevant. Ventola did not have the concept of chain returns, and transitivity was
allowed up to any level. Her research was specific to the domain used. She does not
discuss a computational method of determining the lexical chains.
Hahn (1985) developed a text parsing system that considers lexical cohesion.
Nouns in the text are mapped directly to the underlying model of the domain, which
was implemented as a frame-structured knowledge base. Hahn viewed lexical cohe-
sion as a local phenomenon between words in a sentence and the preceding one. There
was also an extended recognizer that worked for cohesion contained within paragraph
boundaries. Recognizing lexical cohesion was a matter of searching for ways of relat-
ing frames and slots in the database that are activated by words in the text. Heavy
reliance is put on the &amp;quot;formally clear cut model of the underlying domain&amp;quot; (Hahn 1985,
p. 3). However, general-interest articles such as we analyzed do not have domains that
can be a priori formally represented as frames with slot values in such a manner that
lexical cohesion will correspond directly to them. Our work uses lexical cohesion as
it naturally occurs in domain-independent text as an indicator of unity, rather than
fitting a domain model to the lexical cohesion. Hahn does not use the concept of chain
returns or transitivity.
Sedelow and Sedelow (1986, 1987) have done a significant amount of research
</bodyText>
<page confidence="0.993334">
40
</page>
<note confidence="0.73838">
Morris and Hirst Lexical Cohesion
</note>
<bodyText confidence="0.999975947368421">
on the thesaurus as a knowledge source for use in a natural language understanding
system. They have been interested in the application of clustering patterns in the the-
saurus. Their student Bryan (1973) proposed a graph-theoretic model of the thesaurus.
A boolean matrix is created with words on one axis and categories on the other. A
cell is marked as true if a word associated with a cell intersects with the category
associated with a cell. Paths or chains in this model are formed by traveling along
rows or columns to other true cells. Semantic &amp;quot;neighborhoods&amp;quot; are grown, consisting
of the set of chains emanating from an entry. It was found that without some concept
of chain strength, the semantic relatedness of these neighborhoods decays, partly due
to homographs. Strong links are defined in terms of the degree of overlap between
categories and words. A strong link exists where at least two categories contain more
than one word in common, or at least two words contain more than one category in
common. The use of strong links was found to enable the growth of strong semantic
chains with homograph disambiguation.
This concept is different from that used in our work. Here, by virtue of words co-
occurring in a text and then also containing at least one category in common or being
in the same category, they are considered lexically related and no further strength is
needed. We use the thesaurus as a validator of lexical relations that are possible due
to the semantic relations among words in a text.
</bodyText>
<subsectionHeader confidence="0.952776">
5.2 Further Research
</subsectionHeader>
<bodyText confidence="0.999950166666667">
It has already been mentioned that the concept of chain strength needs much fur-
ther work. The intuition is that the stronger a chain, the more likely it is to have a
corresponding structural component.
The integration of this tool with other text understanding tools is an area that will
require a lot of work. Lexical chains do not always correspond exactly to intentional
structure, and when they do not, other textual information is needed to obtain the
correct correspondences. In the example given, there were cases where a lexical chain
did correspond to an intention, but the sentences spanned by the lexical chain and
the intention differed by more than two. In these cases, verification of the possible
correspondence must be accomplished through the use of other textual information
such as semantics or pragmatics. Cue words would be interesting to address, since such
information seems to be more computationally accessible than underlying intentions.
It would be useful to automate this tool and run a large corpus of text through
it. We suspect that the chain-forming parameter settings (regarding transitivity and
distances between words) will be shown to vary slightly according to author&apos;s style
and the type of text. As it is impossible to do a complete and error-free lexical analysis
of large text examples in a limited time-frame, automation is desirable. It could help
shed some light on possible unfortunate chain linkages. Do they become problematic,
and if so, when does this tend to happen? Research into limiting unfortunate linkages
and detecting when the method is likely to produce incorrect results should be done
(cf. Charniak 1986).
Analysis using different theories of text structure was not done, but could prove
insightful. The independence of different people&apos;s intuitive chains and structure as-
signments was also not addressed by this paper.
A practical limitation of this work is that it depends on a thesaurus as its knowl-
edge base. A thesaurus is as good as the work that went into creating it, and also
depends on the perceptions, experience, and knowledge of its creators. Since language
is not static, a thesaurus would have to be continually updated to remain current. Fur-
thermore, no one thesaurus exists that meets all needs. Roget&apos;s Thesaurus, for example,
is a general thesaurus that does not contain lexical relations specific to the geography
</bodyText>
<page confidence="0.997745">
41
</page>
<note confidence="0.590469">
Computational Linguistics Volume 17, Number 1
</note>
<bodyText confidence="0.998989928571429">
of Africa or quantum mechanics. Therefore, further work needs to be done on identi-
fying other sources of word knowledge, such as domain-specific thesauri, dictionaries,
and statistical word usage information, that should be integrated with this work. As
an anonymous referee pointed out to us, Volks and Volkswagen were not included in
the chain containing driving and car. These words were not in a general thesaurus,
and were also missed by the authors!
Section 1 mentioned that lexical chains would be also useful in providing a con-
text for word sense disambiguation and in narrowing to specific word meanings. As
an example of a chain providing useful information for word sense disambiguation,
consider words 1 to 15 of chain 2.1 of the example: {afflicted, darkness, panicky, mournful,
exciting, deadly, hating, aversion, cruel, relentless, weird, eerie, cold, barren, sterile, ... }. In
the context of all of these words, it is clear that barren and sterile do not refer to an
inability to reproduce, but to a cruel coldness. The use of lexical chains for ambiguity
resolution is a promising area for further research.
</bodyText>
<sectionHeader confidence="0.992404" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988366923076923">
Thanks to Robin Cohen, Jerry Hobbs,
Eduard Hovy, Ian Lancashire, and
anonymous referees for valuable discussions
of the ideas in this paper. Thanks to
Chrysanne DiMarco, Mark Ryan, and John
Morris for commenting on earlier drafts.
This work was financially assisted by the
Government of Ontario, the Department of
Computer Science of the University of
Toronto, and the Natural Sciences and
Engineering Research Council of Canada.
We are grateful to Jay Teitel for allowing us
to reprint text from his article &amp;quot;Outland.&amp;quot;
</bodyText>
<sectionHeader confidence="0.983666" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994120426470588">
Bryan, Robert M. (1973). &amp;quot;Abstract thesauri
and graph theory applications to
thesaurus research,&amp;quot; in Automated
language analysis, edited by Sally Yeates
Sedelow, University of Kansas.
Carroll, Lewis (1872). Through the Looking
Glass.
Charniak, Eugene (1986). &amp;quot;A neat theory of
marker parsing.&amp;quot; In Proceedings, 5th
National Conference on Artificial Intelligence,
Philadelphia, August 1986, 584-588.
Grosz, Barbara and Sidner, Candance (1986).
&amp;quot;Attention, intentions and the structure of
discourse.&amp;quot; Computational Linguistics,
12(3), 175-204.
Hahn, Udo (1985). &amp;quot;On lexically distributed
text parsing. A computational model for
the analysis of textuality on the level of
text cohesion and text coherence.&amp;quot; In
Linking in text, edited by Ferenc Kiefer,
Universitat Konstanz.
Halliday, Michael and Hasan, Rugaiya
(1976). Cohesion in English. Longman
Group.
Hirst, Graeme (1987). Semantic Interpretation
and the Resolution of Ambiguity. Studies in
Natural Language Processing. Cambridge
University Press.
Hirst, G. (1981). Anaphora in Natural
Language Understanding: A Survey. Lecture
Notes in Computer Science. Springer
Verlag.
Hobbs, Jerry (1978). &amp;quot;Coherence and
coreference.&amp;quot; Technical note 168, SRI
International.
McKeown, K. (1985). Text Generation: Using
Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Studies in
Natural Language Processing. Cambridge
University Press.
Morris, Jane (1988). &amp;quot;Lexical cohesion, the
thesaurus, and the structure of text.&amp;quot;
Technical report CSRI-219, Department of
Computer Science, University of Toronto.
Postman, Leo and Keppel, Geoffrey, editors
(1970). Norms of Word Association.
Academic Press.
Reichman, Rachel (1985). Getting Computers
to Talk Like You and Me: Discourse Context,
Focus, and Semantics (An ATN Model). The
MIT Press.
Roget, P. (1977). Roget&apos;s International
Thesaurus, Fourth Edition. Harper and
Row Publishers Inc.
Sedelow, Sally and Sedelow, Walter (1987).
&amp;quot;Semantic space.&amp;quot; Computers and
translation, 2, 235-245.
Sedelow, Sally and Sedelow, Walter (1986).
&amp;quot;Thesaural knowledge representation.&amp;quot; In
Proceedings, 2nd Annual Conference of the
University of Waterloo Centre for the New
Oxford English Dictionary: Advances in
Lexicology. University of Waterloo.
Ventola, E. (1987). The Structure of Social
Interaction: A Systemic Approach to the
Semiotics of Service Encounters. Open
Linguistic Series. Frances Pinter
Publishers.
</reference>
<page confidence="0.998437">
42
</page>
<table confidence="0.981203318181818">
Morris and Hirst Lexical Cohesion
Appendix A
Chain 1
Word Sentence Lexical Chain
1. suburbs 1
2. driving 2
3. Volkswagen 2
4. car&apos;s 3 (4, 2)6
5. lights 3
6. commuters 4
7. traffic 4 (7, 2)6 (7, 4).1)
8. Volks 5
9. apartment 5 (9, 1)10
10. city 5 (10, 1)(1) (10, 2) (10, 4)(7 (10, 7)6 (10, 9)1)
11. suburbs 6 (11, 1)8 (11, 9-10)1) (11, 2-7)r°
12. traffic 7 (12, 2)6 (12, 4-10)(1) (12, 7)8 (12, 11)110
13. suburban 7 (13, 1-11)8 (13, 9-10)(1) (13, 2-12)r°
14. suburbs 8 (14, 1-11-13)8 (14, 9-10-13)6 (14, 2-12)r°
15. residentialness 8 (15, 1-9-10-13-14)(1) (15, 2-7-12)r°
16. neighbourhood 9 (16, 1-11-13-14)6 (16, 9-10-13)114
17. community 10
18. suburban 12 (18, 1-11-13-14)8 (18, 9-10-16)1) (18, 2-12)r°
19. drive 12 (19, 2)g (19, 7-10-141) (19, 4)(i (19, 1-9-11-13-14-
15-16-18)T1° (20, 9-10-16) (20, 2-12-19)P°
20. suburban 12 (20, 1-11-13-14-18)8 (20, 9-10-16)1) (20, 2-12-19)T1
21. city 14 (21, 10)8 (21, 1-2-7-9-13-14-15-16-19)r° (21,
4-12)T19
22. suburbs 14 (22, 1-11-13-14-18-20)8 (22, 9-10-16-21)(1) (22; 2-
12-19)r°
23. apartments 14 (23, 9)8 (23, 1-10-11-13-14-15-1648-20-21-22)1)
(23, 2-4-7-12-19)T21
24. Bloor St. 14
25. Bathurst St. 15
26. Finch St. 15
27. driving 16 (27, 2-19)8 (27, 7-10-12-21)1) (27, 4)6 (27, 1-9-11-
13-14-15-1
28. route 16 (28, 1-2-9-10-11-13-14-15-16-18-19-20-21-22-23-27)6
(28, 4-7-12)T27
29. driven 16 (29, 2-19-27-29)8 (29, 7-10-12-21)(1) (29, 4-28)6 (29,
1-9-11-13-14-15-16-18-20-22-23)r°
30. house 17 (30, 1-9-10-11-13-14-15-16-18-20-21-22-23)(1) (30, 2-
4-7-12-19-27-28-29)T1°
31. St. Clair 18
32. Eglinton 19
</table>
<page confidence="0.828673">
43
</page>
<table confidence="0.719903170212766">
Computational Linguistics Volume 17, Number 1
Chain 1 (continued)
Word Sentence Lexical Chain
33. streets 20 (33, 1-10-13-14-15-16-18-20-21-22-23-30) (33, 2-
4-7-12-19-27-28-29)rio
34. Eglinton 21
35. Lawrence 21
36. Wilson 21
37. Sheppard 21
38. town-house 22 (38, 30)8 (38, 1-10-13-14-15-16-18-20-21-22-23)6
(38, 2-4-7-12-19-27-28-29-33)P°
39. Steeles 22
40. car 25 (40, 2-19-27-29)6 (40, 4-7-10-12-21-28)T29
41. drove 25 (41, 2-19-27-29)8 (41, 7-10-12-21)6 (41, 4-28)6 (41,
1-9-11-13-14-15-16-18-20-22-30-38)P°
42. Danforth 25
43. porches 25 (43, 33)6 (43, 1-4-10-13-14-15-18-20-21-22-23-30-
38-40)6 (43, 16)T38 (43, 2-19-23-29)P°
44. drove 26 (44, 2-19-27-29-41)8 (44, 7-10-12-21)6 (44, 4-28)6
(44, 1-9-11-13-14-15-16-18-20-22-23-30-38)P°
45. drove 27 (45, 2-19-27-29-41-44)8 (45, 7-10-12-21)6 (45,
4-28)6 (45, 1-9-11-13-14-15-16-18-20-22-23-30-
38)Tio
46. Bathurst 27
47. Steeles 27
48. Centre St. 27
49. Highway 7 27
50. trunk 28
51. road 28 (51, 1-9-10-11-13-14-15-16-18-20-21-22-23-28-30-
38)6 (51, 43)(2) (51, 7)T10 (51, 16)T38
52. light 29 (52, 5)8
53. turned 29
54. houses 30 (54, 30-38)8 (54, 1-9-10-11-13-14-15-18-20-21-22-
23-33-43-52)6 (54, 16-28)6 (54, 2-7-12-19-29-41-
44)T10
55. turned 30 (55, 53)g
56. houses 32 (56, 30-38-54)8 (56, 1-9-10-11-13-14-15-18-20-21-
22-23-33-43-51)6 (56, 16-28)6 (56, 2-7-12-19-29-41
-44)P°
57. roadway 34 (57, 51)8 (57, 1-9-10-11-13-14-15-16-18-20-21-22-
23-28-30-38)6 (57, 43)6 (57, 7)T10 (57, 16)T38
58. lawn 35 (58, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-
43-51-54-56-57)6 (58, 28)8 (58, 2-12-19-27-29-41-
44)T1° (58, 16)T56
59. suburb 37 (59, 1-11-13-14-18-20-22)8 (59, 30-38-56)6 (59, 9-
10-15-21-23-33-43-51)6 (59, 16-28)6 (59, 2-7-12-19
-29-41-44)P°
</table>
<page confidence="0.690666">
44
</page>
<figure confidence="0.984987868131868">
Morris and Hirst Lexical Cohesion
Chain 1 (continued)
Word Sentence Lexical Chain
60. suburb 37 (60, 1-11-13-14-18-20-22-59)8 (60, 30-38-56) (60,
9-10-15-21-23-33-43-51-54-56-57-59) (60, 16-28)6
(60, 2-7-12-19-29-41-4446-47)T1°
61. people 40 (61, 15) (61, 1-9-10-11-13-14-18-20-21-22-23-30-
33-38-51-54-56-57-59-60)6 (61, 2-7-12-19-27-29-41-
)T1° (61, 16-43-58)T56
62. Metropolitan 40 (62, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51
Toronto -54-56-57-59-60) (62, 2-7-12-19-27-29-41-44) &apos;°
(62, 16-43-58)6
63. Steeles 40
64. people 41 (64, 61)8 (64, 15) (64, 1-9-10-11-13-14-18-20-21-
22-23-30-33-38-51-54-56-57-59-60-62)6 (65, 2-7-12-
19-27-29-41-44)° (61, 16-43-58)T56
65. urban 41 (65, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51-
54-56-57-59-60-62) (65, 2-7-12-19-27-29-41-44)T1°
(65, 16-43-58)6
66. Metro 42 (66, 62)8 (66, 1-9-10-11-13-14-15-18-20-21-22-23-
30-33-38-51-54-56-57-59-60) (66, 2-7-12-19-27-29-
41-44)T1° (66, 16-43-58-64)6
67. Peel 42
68. Durham 42
69. York 42
70. population 42 (70, 30-38-54-56-61-64) (70, 1-9-10-11-13-14-15-
18-20-21-22-23-33-51-57-59-60-62-65-66)6 (70, 43-
58)8 (70, 2-7-12-19-27-29-41-44)1&apos;1° (70, 16)1&apos;64
71. people 43 (71, 61-64)8 (71, 15-70) (71, 1-9-10-11-13-14-18-
20-21-22-23-30-33-38-51-54-56-57-59-60-62-65-66)6
(71, 2-7-12-19-27-29-41-44)T1° (71, 16-43-58-64)T56
72. Toronto 43
73. population 43 (73, 70)8 (73, 30-38-51-54-56-61-65-71) (73, 1-9-
10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62-65
-66)6 (73, 43-58)8 (73, 2-7-12-19-27-29-41-44)T1°
(73, 16)1&apos;64
74. city 43 (74, 10-21)(0) (74, 1-2-7-9-11-12-13-14-15-18-19-20-
22-23-27-29-30-33-38-41-44-51-54-56-57-59-60-62-
65) (74, 16-28-43-58-65-70-71-73)6 (74, 4-40)1&apos;47
75. Toronto 43
76. population 43 (76, 70-73)8 (76, 30-38-54-56-61-64-71) (76, 1-9-
10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62-
65-66-74)6 (76, 43-58)8 (76, 2-7-12-19-27-29-41-
4)T1° (76, 16)1&apos;64
77. suburbs 43 (77, 1-11-13-14-18-20-22-59-60)8 (77, 30-38-56-62-
65-66-74)1, (77, 9-10-15-21-23-33-43-51)(13 (77, 16-28
-64-70-71-72-73-76)6 (77, 2-7-12-19-29-41-44-)TI°
78. Etobicoke 43
45
Computational Linguistics Volume 17, Number 1
Chain 1 (continued)
Word Sentence Lexical Chain
79. Scarborough 43
80. people 44 (80, 61-64-71)S (80, 15-70)(1) (80, 1-9-10-11-13-14-
18-20-21-22-23-30-33-38-51-54-56-57-59-60-62-65-
66-73-76-77) (80, 2-7-12-19-27-29-41-44)T1° (80,
16-43-58)1&apos;56
81. Toronto 44
82. city 44 (82, 10-21-74)S (82, 1-2-7-9-11-12-13-1445-18-19-
20-22-23-27-29-30-33-38-41-44-46-47-51-54-56-57-
59-60-62-65-77)(1) (82, 16-28-43-58-64-70-71-73-76-
80)(2) (82, 4-40)1&apos;47
83. suburbia 44 (83, 1-11-13-14-18-20-22-59-60-77)S (83, 30-38-56-
82)(1) (83, 9-10-15-21-23-33-43-51-82) (83, 16-28-
80) (83, 2-7-12-19-29-41-44)T1°
84. people 44 (84, 61-64-71-80)8 (84, 15-70-82)(1) (84, 1-9-10-11-
13-14-18-20-21-22-23-30-33-38-51-54-56-57-59-60
-62-65-66-73-76-77-82) (84, 2-7-12-19-27-29-41-
44)T1° (84, 16-43-58)1&apos;56
Chain 2, Segment 1
Word Sentence Lexical Chain
1 afflicted 2
2. darkness 3 (2, 1),
3. panicky 3 (3, 1) (3, 2)3
4. mournful 3 (4, 1) (4, 2)1) (4, 3)
5. exciting 3 (5, 1-4) (5, 2-3)3
6. deadly 3 (6, 1-4) (6, 2-3-5)8
7. hating 4 (7, 1-4),1) (7, 2-3-5-6)
8. aversion 7 (8, 7) (8, 1-4) (8, 2-3-5-6)g
9. cruel 7 (9, 1-4-7)(1) (9, 2-3-5-6-8)
10. relentless 8 (10, 9)10- (10, 1-4-7) (10, 2-3-5-6-8)8
11. weird 8 (11, 3),13 (11, 1-4-7-10)F) (11, 2-3-5-6-8)8
12. eerie 9 (12, 3-11) (12, 1-4-7-10) (12, 2-3-5-6-8)8
13. cold 11 (13, 3-6-7-8-11-12),13 (13, 1-4-9)(i (13, 2-3-5-6-10)8
14. barren 11 (14, 6-7)(1 (14, 1-2-3-4-5-8-9-10-11-12-13)r
15. sterile 11 (15, 14) (15, 6-7) (15, 1-2-3-4-5-8-9-10-11-12-
13)T7
16. hated 12 (16, 7)8 (16, 1-4-6-8-9-13) (16, 14-15) (16, 2-3-
5-10-11-12)
17. cruel 12 (17, 9)8 (17, 1-4-7-10) (17, 2-3-5-6-8-11-12-13)8
(17, 14-15)r
</figure>
<page confidence="0.82931">
46
</page>
<figure confidence="0.83725536">
Morris and Hirst Lexical Cohesion
Chain 2, Segment 2
Word Sentence Lexical Chain
18. perversely 16 (18, 10)-F (18, 1-2-3-4-5-6-7-8-9-11-12-13-16-17)P0
Chain 2, Segment 3
Word Sentence Lexical Chain
19. cruel 24 (19, 9-17)8 (19, 1-4-7-10)8 (19, 2-3-5-6-8-11-12-
13)8 (19, 14-15)T7
Chain 3
Word Sentence Lexical Chain
1. married 13
2. wife 14 (2, 1)8
3. wife 15 (3, 1)8 (3, 2)8
4. wife 27 (4, 2-3)8 (4, 1)8
Chain 4
Word Sentence Lexical Chain
1. conceded 19 (2, 1)8
2. tolerance 20
Chain 5
Word Sentence Lexical Chain
1. virgin 31
2. pine 31
3. bush 31 (3, 1)8
4. trees 32 (4, 1)8 (4, 3)8
5. trunks 32
6. trees 33 (6, 4)8 (6, 1-3)8
Chain 6
Word Sentence Lexical Chain
1. hand-in-hand 34
2. matching 34
3. whispering 35
4. laughing 35
5. warm 38 (5, 1)8 (5, 4)8
Chain 7
Word Sentence Lexical Chain
1. first 1
2. initial 1 (2, 1)8
3. final 2 (3, 2-1)8
47
Computational Linguistics Volume 17, Number 1
Chain 8
Word Sentence Lexical Chain
1. night 2
2. dusk 3 (2, 1))
3. darkness 3 (3, 1-2)1)
Chain 9
Word Sentence Lexical Chain
1. environment 7
2. setting 7
3. surrounding 8
</figure>
<page confidence="0.988051">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985710666666667">Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text</title>
<author confidence="0.983028">Graeme Hirstt</author>
<affiliation confidence="0.982079">York University University of Toronto</affiliation>
<abstract confidence="0.993990180887372">In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being &amp;quot;about the same thing,&amp;quot; and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non–domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences. 1. Lexical Cohesion A text or discourse is not just a set of sentences, each on some random topic. Rather, the sentences and phrases of any sensible text will each tend to be about the same — that is, the text will have a quality of unity. This is the property of — the sentences &amp;quot;stick together&amp;quot; to function as a whole. Cohesion is achieved through back-reference, conjunction, and semantic word relations. Cohesion is not a guarantee of unity in text but rather a device for creating it. As aptly stated by Halliday and Hasan (1976), it is a way of getting text to &amp;quot;hang together as a whole.&amp;quot; Their work on cohesion has underscored its importance as an indicator of text unity. Lexical cohesion is the cohesion that arises from semantic relationships between words. All that is required is that there be some recognizable relation between the words. Halliday and Hasan have provided a classification of lexical cohesion based on the type of dependency relationship that exists between words. There are five basic classes: 1. Reiteration with identity of reference: Example 1 Mary bit into a Unfortunately the ripe. of Computer Science, York University, North York, Ontario, Canada M3J 1P3 t Department of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S 1A4 C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 1 2. Reiteration without identity of reference: Example 2 Mary ate some She likes much. 3. Reiteration by means of superordinate: Example 3 Mary ate a She likes 4. Systematic semantic relation (systematically classifiable): Example 4 Mary likes She does not like 5. Nonsystematic semantic relation (not systematically classifiable): Example 5 Mary spent three hours in the She was 1, 2, and 3 fall into the class of that reiteration includes not only identity of reference or repetition of the same word, but also the use of superordinates, subordinates, and synonyms. 4 and 5 fall into the class of is, semantic relationships between words that often co-occur. They can be further divided into two categories relationship: semantic, semantic. semantic relationships can be a fairly straightforward way. type of relation includes antonyms, members of an ordered set such as two, of an unordered set such as black, red}, part-to-whole relike mouth, face}. 5 is an illustration of collocation where word relationship, digging}, nonsystematic. This type of relationship is the most problematic, especially from a knowledge representation point of view. Such collocation relationships exist between words that tend to occur in similar lexical environments. Words tend to occur in similar lexical environments because they describe things that tend to occur in similar situations or contexts in the world. Hence, examples such as office, service, stamps, pay, leave} included in the class. (This example is from Ventola (1987), who analyzed the patterns of lexical cohesion specific to the context of service encounters.) Another example of this is lights, turning}, from example 14 in Section 4.2. These words are related in the situation of driving a car, but taken out of that situation, they are not in a systematic way. Also contained in the class of collocation are associafrom Postman and Keppel (1970) are church}, {citizen, U.S.A.}, stop}. the exact relationship between these words can be hard to classify, but there does exist a recognizable relationship. 1.1 Lexical Chains Often, lexical cohesion occurs not simply between pairs of words but over a succession of a number of nearby related words spanning a topical unit of the text. These 22 Morris and Hirst Lexical Cohesion of related words will be called chains. is a beeach word in the chain, and the words co-occur within a given chains do not stop at sentence boundaries. They can connect a pair of adjacent words or range over an entire text. Lexical chains tend to delineate portions of text that have a strong unity of meaning. Consider this example (sentences 31-33 from the long example given in Section 4.2): Example 6 front me lay a virgin crescent cut out of pine bush. A dozen houses were going up, in various stages of construction, surrounded by hummocks of dry earth and stands of precariously tall trees nude halfway up their trunks. They were the kind of trees you might see in the mountains. lexical chain spanning these three sentences is pine, bush, trees, trunks, trees}. Section 3 will explain how such chains are formed. Section 4 is an analysis of the correspondence between lexical chains and the structure of the text. 1.2 Why Lexical Cohesion Is Important There are two major reasons why lexical cohesion is important for computational text understanding systems: 1. Lexical chains provide an easy-to-determine context to aid in the resolution of ambiguity and in the narrowing to a specific meaning of a word. 2. Lexical chains provide a clue for the determination of coherence and discourse structure, and hence the larger meaning of the text. Word Interpretation in Context. meanings do not exist in isolation. Each must be interpreted in its context. For example, in the context alcohol, sober, meaning of the noun narrowed down to alcoholic the curl, comb, wave} and Hasan 1976), a hair wave, not a water wave, a physics wave, or a friendly hand wave. In these examples, lexical chains can be used as a contextual aid to interpreting word meanings. In earlier work, Hirst (1987) used a system called &amp;quot;Polaroid Words&amp;quot; to provide for intrasentential lexical disambiguation. Polaroid Words relied on a variety of cues, including syntax, selectional restrictions, case frames, and — most relevant here — a notion of semantic distance or relatedness to other words in the sentences; a sense that had such a relationship was preferred over one that didn&apos;t. Relationships were determined by marker passing along the arcs in a knowledge base. The intuition was that semantically related concepts will be physically close in the knowledge base, and can thus be found by traversing the arcs for a limited distance. But Polaroid Words looked only for possible relatedness between words in the same sentence; trying to find connections with all the words in preceding sentences was too complicated and too likely to be led astray. The idea of lexical chains, however, can address this weakness in Polaroid Words; lexical chains provide a constrained easy-to-determine representation of context for consideration of semantic distance. Cohesion and Discourse Structure. second major importance of lexical chains is that they provide a clue for the determination of coherence and discourse structure. 23 Computational Linguistics Volume 17, Number 1 When a chunk of text forms a unit within a discourse, there is a tendency for related words to be used. It follows that if lexical chains can be determined, they will tend to indicate the structure of the text. We will describe the application of lexical cohesion to the determination of the discourse structure that was proposed by Grosz and Sidner (1986). Grosz and Sidner propose a structure common to all discourse, which could be used along with a structurally dependent focus of attention to delineate and constrain referring expressions. this theory there are three interacting components: structure, intentional state. Linguistic structure is the segmentation of discourse into groups of sentences, each fulfilling a distinct role in the discourse. Boundaries of segments can be fuzzy, some factors aiding in their determination are words, in intonation (not helpful in written text), and changes in aspect and tense. When found, these segments indicate changes in the topics or ideas being discussed, and hence will have an effect on potential referents. The second major component of the theory is the intentional structure. It is based on the idea that people have definite purposes for engaging in discourse. There is an overall discourse purpose, and also a discourse segment purpose for each of the segments in the linguistic structure described above. Each segment purpose specifies how the segment contributes to the overall discourse purpose. There are two structural between these segments. The first is called a which occurs when the satisfaction (i.e., successful completion) of one segment&apos;s intention contributes to the satisfaction of another segment&apos;s intention. The second relation is precedence, occurs when the satisfaction of one discourse segment purpose must occur before the satisfaction of another discourse segment purpose can occur. The third component of this theory is the attentional state. This is a stack-based model of the set of things that attention is focused on at any given point in the discourse. It is &amp;quot;parasitic&amp;quot; on the intentional and linguistic structures, since for each discourse segment there exists a separate focus space. The dominance relations and satisfaction precedence relations determine the pushes and pops of this stack space. When a discourse segment purpose contributes to a discourse segment purpose of the immediately preceding discourse segment, the new focus space is pushed onto the stack. If the new discourse segment purpose contributes to a discourse segment purpose earlier in the discourse, focus spaces are popped off the stack until the discourse segment that the new one contributes to is on the top of the stack. It is crucial to this theory that the linguistic segments be identified, and as stated by Grosz and Sidner, this is a problem area. This paper will show that lexical chains are a good indication of the linguistic segmentation. When a lexical chain ends, there is a tendency for a linguistic segment to end, as the lexical chains tend to indicate the topicality of segments. If a new lexical chain begins, this is an indication or clue that new segment has begun. If an old chain is referred to again (a return), is a strong indication that a previous segment is being returned to. We will demonstrate this in Section 4. 1.3 Cohesion and Coherence theory of relations obbs 1978; Hirst 1981; McKeown 1985) will now be considered in relation to cohesion. There has been some confusion as to the differences the phenomena of Reichman (1985). There is a danger of lumping the two together and losing the distinct contributions of each to the understanding of the unity of text. 24 Morris and Hirst Lexical Cohesion the difference between cohesion and coherence is this: a term sticking together; it means that the text all hangs together. a term for sense; it means that there is sense in the text. Hence the term relations refers to the relations between sentences that contribute to their making sense. Cohesion and coherence relations may be distinguished in the following way. A relation is a relation among clauses or sentences, such as supcause, have been various attempts to classify all possible coherence relations, but there is as yet no widespread agreement. There does not exist a general computationally feasible mechanism for identifying coherence relations. In cohesion relations are relations among elements in a text: ellipsis, conjunction, cohesion. Since cohesion is well defined, one might expect that it would be computationally easier to identify, because the identification of ellipsis, reference, substitution, conjunction, and lexical cohesion is a straightforward task for people. We will show below that is computationally feasible to identify. In contrast, the identification of a specific coherence relation from a given set is not a straightforward task, even for people. Consider this example from Hobbs (1978): Example 7 1. John can open Bill&apos;s safe. 2. He knows the combination. identifies the coherence relation as it could just as easily be distinction depends on context, knowledge, and beliefs. For example, if you questioned John&apos;s ability to open Bill&apos;s safe, you would probably identify the relation as explanation. Otherwise you could identify it as elaboration. Here is another example: Example 8 1. John bought a raincoat. 2. He went shopping yesterday on Queen Street and it rained. The coherence relation here could be elaboration (on the buying), or explanation (of when, how, or why), or cause (he bought the raincoat because it was raining out). The point is that the identity of coherence relations is &amp;quot;interpretative,&amp;quot; whereas the identity of cohesion relations is not. At a general level, even if the precise coherence relation is not known, the relation &amp;quot;is about the same thing&amp;quot; exists if coherence exists. the example from Hobbs above, lexically related, which in a general sense means they &amp;quot;are about the same thing in some way.&amp;quot; In example 8, lexically related, as are shows how cohesion can be useful in identifying sentences that are coherently related. Cohesion and coherence are independent, in that cohesion can exist in sentences that are not related coherently: Example 9 Wash and core six apples. Use them to cut out the material for your new suit. They tend to add a lot to the color and texture of clothing. Actually, maybe you should use five of them instead of six, since they are quite large. 25 Computational Linguistics Volume 17, Number 1 Similarly, coherence can exist without textual cohesion: Example 10 I came home from work at 6:00 p.m. Dinner consisted of two chicken breasts and a bowl of rice. Of course, most sentences that relate coherently do exhibit cohesion as well) 1.4 The Importance of Both Cohesion and Coherence Halliday and Hasan (1976) give two examples of lexical cohesion involving identity of reference: Example 11 Wash and core six cooking Put a fireproof dish. Example 12 Wash and core six cooking Put the a fireproof dish. (1985, p. 180) writes &amp;quot;It is not the use of a pronoun that gives the wash-and-core-apples text. These utterances form a of text not the pronoun used but because they jointly describe a set of cooking instructions&amp;quot; (emphasis added). This is an example of lumping cohesion and coherence as one phenomenon. Pronominal reference is defined as a type of and Hasan 1976). Therefore the example 11 is an instance of it. The point is that and coherence are distinct phenomena creating unity in text. also writes (1985, p. 179) &amp;quot;that similar words them, apples) in a given stretch of discourse is an artifact of the content of discussion.&amp;quot; It follows that if content is related in a stretch of discourse, there will be coherence. Lexical cohesion is a computationally feasible clue to identifying a coherent stretch of text. In example it is computationally trivial to get the word relationship between and this relation fits the definition of lexical cohesion. Surely this simple indicator of coherence is useful, since as stated above, there does not exist a computationally feasible method of identifying coherence in non—domain-specific text. Cohesion is a useful indicator of coherence regardless of whether it is used intentionally by writers to create coherence, or is a result of the coherence of text. Hobbs (1978) sees the resolution of coreference (which is a form of cohesion) as being subsumed by the identification of coherence. He uses a formal definition of coherence relations, an extensive knowledge base of assertions and properties of objects and actions, and a mechanism that searches this knowledge source and makes simple inferences. Also, certain elements must be assumed to be coreferential. shows how, in example (7), an assumption of coherence allows the be identified as the combination of safe be found to be coreferential. There is an interesting analogy between cohesion and syntax, and coherence and semantics. (Carroll 1872) is an example of syntax sticking text together without semantics. Example 10 illustrates coherence sticking text together without cohesion. 26 Morris and Hirst Lexical Cohesion lexical cohesion would also indicate that be assumed to be coreferential. And more importantly, one should not be misled by chicken-andegg questions when dealing with cohesion and coherence. Rather, one should use where applicable. Since the lexical cohesion between easy to compute, we argue that it makes sense to use this information as an indicator of coherence. 2. The Thesaurus and Lexical Cohesion The thesaurus was conceived by Peter Mark Roget, who described it as being the &amp;quot;converse&amp;quot; of a dictionary. A dictionary explains the meaning of words, whereas a thesaurus aids in finding the words that best express an idea or meaning. In Section 3, we will show how a thesaurus can be used to find lexical chains in text. 2.1 The Structure of the Thesaurus International Thesaurus, 4th Edition is composed of 1042 sequentially numbered basic categories. There is a hierarchical structure both above and below this level (see Figure 1). Three structure levels are above the category level. The topmost consists of eight major by Roget in 1852: abstract relations, space, physics, matter, sensation, intellect, volition, and affections. Each class is diinto (roman-numbered) under each subclass there is a (capitalin turn are divided into the basic categories. applicable, categories are organized into pairs. example, cate- 407 is category 408 is Each category contains a series of numbered paragraphs to group closely related words. Within each paragraph, still finer groups are marked by semicolons. In addition, a semicolon group may have cross-references or pointers to other related categories or paragraphs. A paragraph contains words of only one syntactic category. The noun paragraphs are grouped at the start of a category, followed by the paragraphs for</abstract>
<note confidence="0.908061">Class 1 • • • Class 4: Matter</note>
<author confidence="0.719443">Ill Organic Matter</author>
<affiliation confidence="0.767455">B Vitality</affiliation>
<address confidence="0.751121">407 Life</address>
<abstract confidence="0.986763468438538">1. NOUNS life, living, vitality, being alive, having life, animation, animate existence; liveliness, animal spirits, vivacity, spriteliness; long life, longevity; viability; lifetime 110.5; immortality 112.3; birth 167; eels. trace 1; bio-, organ-; -biosis. 408 Death • • • Figure 1 structure of Thesaurus 27 Computational Linguistics Volume 17, Number 1 Lid clothing 231.35 cover 228.5 eyelid 439.9 stopper 266.4 Figure 2 entry for the word verbs, adjectives, and so on. The thesaurus has an index, which allows for retrieval of words related to a given one. For each entry, a list of words suggesting its various distinct subsenses is given, and a category or paragraph number for each of these. Figure 2 shows the index entry find words related to its sense of would turn to paragraph 5 of category 228. An index entry may be a pointer to a category or paragraph if there are no subsenses to be distinguished. 2.2 Differences from Traditional Knowledge Bases In the structure of traditional artificial intelligence knowledge bases, such as frames or semantic networks, words or ideas that are related are actually &amp;quot;physically close&amp;quot; in the representation. In a thesaurus this need not be true. Physical closeness has some importance, as can be seen clearly from the hierarchy, but words in the index of the thesaurus often have widely scattered categories, and each category often points to a scattered selection of The thesaurus simply groups words by idea. It does not have to name or classify the idea or relationship. In traditional knowledge bases, the relationships must be For example, in a semantic net, a relationship might be in frame database, there might be a slot for In Section 1, different types of word relationships were discussed: systematic semantic, nonsystematic semantic, word association, and words related by a common situation. A factor common to all but situational relationships is that there is a strong tendency for the word relationships to be captured in the thesaurus. This holds even for the nonsystematic semantic relations, which are the most problematic by definition. A thesaurus simply groups related words without attempting to explicitly name each relationship. In a traditional computer database, a systematic semantic relationship can be represented by a slot value for a frame, or by a named link in a semantic network. If it is hard to classify a relationship in a systematic semantic way, it will be hard to represent the relationship in a traditional frame or semantic network formalism. Of the 16 nonsystematic semantic lexical chains given as examples in Halliday Hasan (1976), 14 were found in Thesaurus using relations given in Section 3.2.2. This represents an 87% hit rate (but not a big sample space). Word associations show a strong tendency to be findable in a thesaurus. Of the 16 word pairs given in Hirst (1987), 14 were found in Thesaurus Since two of the word senses were not contained in the thesaurus at all, this represents a 100% hit rate among those that were. Situational word relationships are not as likely to be found in a general thesaurus. An example of a situational relationship is between the two words are clearly related in the situation involving a car&apos;s lights, but the relationship will not be found between them in a general thesaurus. 28 Morris and Hirst Lexical Cohesion 3. Finding Lexical Chains 3.1 General Methodology We now describe a method of building lexical chains for use as an aid in determining the structure of text. This section details how these lexical chains are formed, using a thesaurus as the main knowledge base. The method is intended to be useful for text in any general domain. Unlike methods that depend on a full understanding of text, our method is the basis of a computationally feasible approach to determining discourse structure. We developed our method in the following way. First, we took five texts, total- 183 sentences, from general-interest magazines Digest, Equinox, The New Toronto, Toronto Star). our intuition (i.e., common sense and a knowledge of English), we identified the lexical chains in each text. We then formalized our intuitions into an algorithm, using our experience with the texts to set values for the following parameters (to be discussed below). • thesaural relations • transitivity of word relations • distance (in sentences) allowable between words in a chain The aim was to find efficient, plausible methods that will cover enough cases to ensure the production of meaningful results. 3.2 Forming Lexical Chains Candidate Words. first decision in lexical chain formation is which words in the text are candidates for inclusion in chains. As pointed out by Halliday and Hasan (1976), repetitive occurrences of closed-class words such as pronouns, prepositions, and verbal auxiliaries are obviously not considered. Also, high-frequency words like do, not normally enter into lexical chains (with some exceptions as in the sense of example, in (13) only the italicized words should be considered as lexical chain candidates: Example 13 grandfather lived be Zayde the a few assigned the him about his alcohol. It should be noted that morphological analysis on candidate words was done intuitively, and would actually have to be formally implemented in an automated system. Building Chains. the candidate words are chosen, the lexical chains can be For this work an abridged version of Thesaurus was used. The chains were built by hand. Automation was not possible, for lack of a machine-readable copy of the thesaurus. Given a copy, implementation would clearly be straightforward. It is expected that research with an automated system and a large sample space of text would give valuable information on the fine-tuning of the parameter settings used in the general algorithm. Five types of thesaural relations between words were found to be necessary in forming chains, but two (the first two below) are by far the most prevalent, constituting 29 Computational Linguistics Volume 17, Number 1 over 90% of the lexical relationships. The relationships are the following: 1. Two words have a category common in their index entries. For example, have category 189 in their index entries (see Figure 3.1). 2. One word has a category in its index entry that contains a pointer to a of the other word. For example category 273 in its index entry, and that contains a pointer to category 276, which is a category of word Figure 3.2). 3. A word is either a label in the other word&apos;s index entry (see Figure 3.3b), is in a category of the other word. For example, category 442 its index entry, which contains the word Figure 3.3a). 4. Two words are in the same group, and hence are semantically related. example, category 442, blindness, in its index entry and has category 441, vision, in its index entry (see Figure 3.4). 5. The two words have categories in their index entries that both point to a category. For example, category 851, which in turn word 1 index label 1: thesaurus category 521 label 2: label 3: 626 word 2 index 1: label 2: 521 thesaurus category 521 Rs860 thesaurus category 860 word 1 label 1: 1: 521 label 2: 589 label 3: 626 word 2 index label 1: 300 label 2: 660 word 1 index label 1: 521 label 2: 589 label 3: 626 thesaurus category 521 word 2 word 1 index label 1: 521 word 2: 589 label 3: 626 Figure 3 Thesaural Relations, parts (1)-(3) (b) 30 (a) Morris and Hirst Lexical Cohesion thesaurus category 23 thesaurus category 521 8,457 word 1 index label word 2: label 3: 621 word 2 index label 1: x+1 label 2: 860 word 1 index label 1: label 2: label 3: 621 &amp;457 word 2 index label 1: 23 label 2: 600 thesaurus category 457 Figure 3 Relations, parts (4)-(5) a pointer to category 830. category 860 likewise has a pointer to category 830 (see Figure 3.5). One must consider how much transitivity to use when computing lexical chains. if word related to word related to word word to word is word to words this chain: sheep, wool, scarf, boots, hat, snow} . unlimited transitivity allowed, then be considered related, which is definitely intuitive. Our intuition was to allow one transitive link: word related word not to word seemed that two or more transitive links would so severely weaken the word relationship as to cause it to be nonintuitive. Our analysis our sample texts supported this. To summarize, a of one link is sufficient to successfully compute the intuitive chains. An automated system could be used to test this out extensively, varying the number of transitive links and calculating the likely that it varies slightly with respect to style, author, or type of text. There are two ways in which a transitive relation involving one link can cause words to be related. In the first way, if word related to word word related to word word related to word the second way, if word to word word related to word word related to word But lexical chains are calculated only with respect to the text read so far. For example, word related to word to word word word not related, since at the time of processing, they were not relatable. Symmetry was not found to be necessary for computing the lexical chains. We now consider how many sentences can separate two words in a lexical chain before the words should be considered unrelated. Now, sometimes, several sentences a chain has clearly stopped, it is returned to. Such returns together expanses of text than are contained in single chains or segments. 31 Computational Linguistics Volume 17, Number 1 to existing chains often correspond to intentional boundaries, as they occur after digressions or subintentions, thereby signalling a resumption of some structural text entity. Intuitively, the distance between words in a chain is a factor in chain formation. The distance will not be &amp;quot;large,&amp;quot; because words in a chain co-relate due to recognizable relations, and large distances would interfere with the recognition of relations. The five texts were analyzed with respect to distance between clearly related words. The analysis showed that there can be up to two or three intermediary sentences between a word and the preceding element of a chain segment with which it can be linked. At distances of four or more intermediary sentences, the word is only able to signal a return to an existing chain. Returns happened after between 4 and 19 intermediary sentences in the sample texts. One significant fact emerged from this analysis: returns consisting of one word only were always made with a repetition of one of the words in the returned-to chain. Returns consisting of more than one word did not necessarily use repetition — in fact in most cases, the first word in the return was not a repetition. The question of chain returns and when they can occur requires further research. When distances between relatable words are not tightly bound (as in the case of returns), the chances of incorrect chain linkages increase. It is anticipated that chain return analysis would become integrated with other text processing tools in order to this. Also, we believe that chain will be required for this purpose. Intuitively, some lexical chains are &amp;quot;stronger&amp;quot; than others, and possibly only strong chains can be returned to. There are three factors contributing to chain strength. 1. Reiteration — the more repetitions, the stronger the chain. 2. Density — the denser the chain, the stronger it is. 3. Length — the longer the chain, the stronger it is. Ideally, some combination of values reflecting these three factors should result in a chain strength value that can be useful in determining whether a chain is strong enough to be returned to. Also, a strong chain should be more likely to have a structural correspondence than a weak one. It seems likely that chains could contain particularly strong portions with special implications for structure. These issues will not be addressed here. Notation and Data Structures. the computation of lexical chains, the following information is kept for each word in a chain: • A word number, which is a sequential, chain-based number for each word so that it can be uniquely identified. • The sentence number in which the word occurs. • The chain created so far. Each lexical relationship in a chain is represented as (u,v) where: • u is the current word number, • v is the word number of the related word, x the transitive distance: 32 Morris and Hirst Lexical Cohesion Chain 1 Lexical Chain 1. evade 15 2. feigning 15 (2,1) escaped 16 (3,1)? Figure 4 Lexical chain notation 0 no transitive link was used to form the word relationship 1 one transitive link was used to form the word relationship • y is either — the number of the thesaural relationship between the two words (as given in Section 3.2.2) Tq for transitively related q the word number through which the transitive relation is formed. A full example of this notation is shown in Figure 4. Figure 5 shows the generalized algorithm for computing lexical chains. The parameter values that we used are shown for the following: • candidate words • thesaural relations • transitivity of word relations • distance between words in a chain. The only parameter not addressed in this work is which (if any) chains should be eliminated from the chain-finding process. 3.3 Problems and Concerns This section is a discussion of problems encountered during the computation of the lexical chains contained in our corpus of texts. The text example used in this paper is in Section 4.2, and the chains found in the example are in Appendix A. Where the Thesaurus Failed to Find Lexical Relations. algorithm found well over 90% of the intuitive lexical relations in the five examples we studied. The following is an analysis of when the thesaurus failed to find a relationship and why. One problem was when the relationship between words was due more to their than their meaning. For example, in chain 6, the intuitive chain whispering, warm} not entirely computable. Only words were relatable. The words in chain 6 are cohesive by virtue of being general, but strong, &amp;quot;good&amp;quot; words related by their goodness, rather than by their specific meanings. 10, setting, surrounding}, not thesaurally relatable. 33 Computational Linguistics Volume 17, Number 1 REPEAT word IF word is suitable for lexical analysis (see section 3.2.1) THEN CHECK for chains within a suitable span (up to 3 intermediary sentences, and no limitation on returns): CHECK thesaurus for relationships (section 3.2.2). CHECK other knowledge sources (situational, general words, proper names). IF chain relationship is found THEN INCLUDE word in chain. CALCULATE chain so far (allow one transitive link). END IF IF there are words that have not formed a chain for a suitable number of sentences (up to 3) THEN ELIMINATE words from the span. END IF CHECK new word for relevance to existing chains that are suitable for checking. ELIMINATE chains that are not suitable for checking.</abstract>
<title confidence="0.537822">END IF END REPEAT Figure 5 Algorithm for Finding Lexical Chains</title>
<abstract confidence="0.968439094059406">in the thesaurus, and while it seems as though be thesaurally connected, they were not. Place names, street names, and people&apos;s names are generally not to be found Thesaurus However, they are certainly contained in one&apos;s &amp;quot;mental thesaurus.&amp;quot; Chain 1, which contains several major Toronto street names, is a good example of this. These names were certainly related to the rest of chain 1 in the authors&apos; mental thesaurus, since we are residents of Toronto (and indeed the article assumed a knowledge of the geography of the city). In chain 5, the thesaurus did not the words the rest of the chain bush, trees, trees}. In a general thesaurus, specific information on, and classification of, plants, animals, minerals, etc., is not available. To summarize, there were few cases in which the thesaurus failed to confirm an intuitive lexical chain. For those cases in which the thesaurus did fail, three missing knowledge sources became apparent. 1. General semantic relations between words of similar &amp;quot;feeling.&amp;quot; 2. Situational knowledge. 3. Specific proper names. Problems with Distances and Chain Returns. the algorithm would cause two chains to merge together, whereas intuition would lead one to keep them 34 Morris and Hirst Lexical Cohesion separate. We found the following intuitively separate chain beginning in sentence Metropolitan Toronto, people, urban, population, people, population, populapeople} . the algorithm linked this chain with chain 1, which runs the entire example and consists of these words and others: suburbs, community}. this was a rare occurrence. But note that there will be cases in which lexical chains should be merged as a result of the intentional merging of ideas or concepts in the text. Conversely, there were a few cases of unfortunate chain returns occurring where were definitely counter intuitive. In chain 3, word 4, taken as a onereturn to the chain wife, wife}. there is no intuitive reason for this. 4. Using Lexical Chains to Determine Text Structure This section describes how lexical chains formed by the algorithm given in Section 3.2.3 can be used as a tool. 4.1 Lexical Chains and Text Structure Any structural theory of text must be concerned with identifying units of text that are about the same thing. When a unit of text is about the same thing there is a strong tendency for semantically related words to be used within that unit. By definition, lexical chains are chains of semantically related words. Therefore it makes sense to use them as clues to the structure of the text. This section will concentrate on analyzing correspondences between lexical chains and structural units of text, including: • the correspondence of chain boundaries to structural unit boundaries; • returns to existing chains and what they indicate about structural units; • lexical chain strength and reliability of predicting correspondences between chains and structural units; • an analysis of problems encountered, and when extra textual information is required to validate the correspondences between lexical chains and structural components. The text structure theory chosen for this analysis was that of Grosz and Sidner (1986). It was chosen because it is an attempt at a general domain-independent theory of text structure that has gained a significant acceptance in the field as a good standard approach. The methodology we used in our analyses was as follows: 1. We determined the lexical chain structure of the text using the algorithm given in Section 3.2.3. (In certain rare cases where the algorithm did not form intuitive lexical chains properly, it is noted, both in Section 3.4 and in the analysis in this section. The intuitive chain was used for the analysis; however the lexical chain data given in Appendix A show the rare mismatches between intuition and the algorithm.) We determined the structure the text using the theory outlined by Grosz and Sidner. 35 Computational Linguistics Volume 17, Number 1 3. We compared the lexical structure formed in step 1 with the intentional structure formed in step 2, and looked for correspondences between them. 4.2 An Example Example 14 shows one of the five texts that we analyzed. It is the first section of an in December 1987, by Jay Teitel, entitled The tables in Appendix A show the lexical chains for the text. (The other four texts and their analyses are given in Morris 1988.) Example 14 1. III spent the first 19 years of my life in the suburbs, the initial 14 or so relatively contented, the last four or five wanting mainly to be elsewhere. The final two vividly: I passed them driving to and from the University of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness. 3. The car&apos;s lights never worked -every dusk turned into a kind of medieval race against darkness, a panicky, mournful rush north, away from everything I knew was exciting, toward everything I knew was deadly. I looking through the windows at the commuters mired in traffic beside me and actively hating them for their passivity. 5. I actually punched holes in the white vinyl ceiling of the Volks and then, by way of penance, wrote beside them the names and phone numbers of the girls I would call when I had my own apartment in the city. 6. One thing I swore to myself: I would never live in the suburbs again. 7. ¶My aversion was as much a matter of environment as it was traffic — one particular piece of the suburban setting: the &amp;quot;cruel sun.&amp;quot; 8. Growing up in the suburbs you can get used to a surprising number of things — the relentless &amp;quot;residentialness&amp;quot; of your surroundings, the weird certainty you have that everything will stay vaguely new-looking and immune to historic soul no matter how many years pass. 9. You don&apos;t notice the eerie silence that descends each weekday when every sound is drained out of your neighbourhood along with all the people who&apos;ve gone to work. 10. I got used to pizza, and cars, and the fact that the cultural hub of my community was the collective TV set. 11. But once a week I would step outside as dusk was about to fall and be absolutely bowled over by the setting sun, slanting huge and cold across the untreed front lawns, reminding me not just how barren and sterile, but how undefended life could be. As much as I hated the suburban drive to school, to get away from the cruel suburban sun. ¶When married a few years later, my attitude hadn&apos;t changed. 14. My wife was a city girl herself, and although her reaction to the suburbs was less intense than mine, we lived in a series of apartments safely straddling Bloor Street. 15. But four years ago, we had a second child, and simultaneously the school my wife taught at moved to Bathurst Street north of Finch Avenue. 2 C) Jay Teitel. Reprinted with kind permission of the author. 36 Morris and Hirst Lexical Cohesion 16. She was now driving 45 minutes north to work every morning, along a route that was perversely identical to the one I&apos;d driven in college. 17. IfWe started looking for a house. 18. Our first limit was St. Clair — we would go no farther north. 19. When we took a closer look at the price tags in the area though, we conceded that maybe we&apos;d have to go to Eglinton — but that was definitely it. 20. But the streets whose names had once been magical barriers, latitudes of tolerance, quickly changed to something else as the Sundays passed. 21. Eglinton became Lawrence, which became Wilson, which became Sheppard. 22. One wind-swept day in May I found myself sitting in a town-house development north of Steeles Avenue called Shakespeare Estates. 23. It wasn&apos;t until we stepped outside, and the sun, blazing unopposed over a country club, smacked me in the eyes, that I came to. 24. It was the cruel sun. 25. We got into the car and drove back to the Danforth and porches as fast as we could, grateful to have been reprieved. 26. IfAnd then one Sunday in June I drove north alone. 27. This time I drove up Bathurst past my wife&apos;s new school, hit Steeles, and kept going, beyond Centre Street and past Highway 7 as well. 28. I passed farms, a man selling lobsters out of his trunk on the shoulder of the road, a chronic care hospital, a country club and what looked like a mosque. 29. I reached a light and turned right. 30. I saw a sign that said Houses and turned right again. 31. ¶In front of me lay a virgin crescent cut out of pine bush. 32. A dozen houses were going up, in various stages of construction, surrounded by hummocks of dry earth and stands of precariously tall trees nude halfway up their trunks. 33. They were the kind of trees you might see in the mountains. 34. A couple was walking hand-in-hand up the dusty dirt roadway, wearing matching blue track suits. 35. On a &amp;quot;front lawn&amp;quot; beyond them, several little girls with hair exactly the same colour of blond as my daughter&apos;s were whispering and laughing together. 36. The air smelled of sawdust and sun. 37. IfIt was a suburb, but somehow different from any suburb I knew. 38. It felt warm. 39. IfIt was Casa Drive. 40. In 1976 there were 2,124,291 people in Metropolitan Toronto, an area bordered by Steeles Avenue to the north, Etobicoke Creek on the west, and the Rouge River to the east. 41. In 1986, the same area contained 2,192,721 people, an increase of 3 percent, all but negligible on an urban scale. 42. In the same span of time the three outlying regions stretching across the top of Metro — Peel, Durham, and York — increased in population by 55 percent, from 814,000 to some 1,262,000. 43. Half a million people had poured into the crescent north of Toronto in the space of a decade, during which time the population of the City of Toronto actually declined as did the populations of the &amp;quot;old&amp;quot; suburbs with the exception of Etobicoke and Scarborough. 44. If the sprawling agglomeration of people known as Toronto has boomed in the past 10 years it has boomed outside the traditional city confines in a totally new city, a new suburbia containing one and a quarter million people. 37 Computational Linguistics Volume 17, Number 1 4.3 The Correspondences between Lexical and Intentional Structures In Figure 6 we show the intentional structure of the text of Section 4.2, and in Figure 7 we show the correspondences between the lexical chains and intentions of the example. is a clear correspondence between chain 1, {. , car&apos;s, . . . and intention 1 (changing attitudes to suburban life). The continuity of the subject matter is reflected by the continuous lexical chain. From sentence 40 to sentence 44, two words, used repetitively in the chain. repeated three and repeated five times. If chain strength (indicated by the reiteration) were used to delineate &amp;quot;strong&amp;quot; portions of a chain, this strength information could also be used to indicate structural attributes of the text. Specifically, sentences 40 to 44 form intention 1.3 (why new suburbs exist), and hence a strong portion of the 1 (1-44) Changing attitudes to suburban life. 1.1 (1-25) Earlier aversion to suburban life. 1.1.1 (1-7) Hatred of commuting. 1.1.2 (8-12) The hated suburb environment. 1.1.3 (13-25) How this old aversion to suburbs held, when a recent attempt was made to buy a new house in the suburbs. 1.1.3.1 (13-16) How life changed, giving author reason to look for a new house. 1.1.3.2 (17-22) Houses are too expensive in Metro Toronto, hence one must look in the suburbs to buy a house. 1.1.3.3 (23-25) The old familiar aversion to suburbs came back. 1.2 (26-39) A new suburb that seems livable in and nice. 1.2.1 (26-30) The drive to the new suburb. 1.2.2 (31-33) The forested area. 1.2.3 (34-39) The pleasant environment. 1.3 (40-44) Why the new suburbs exist.</abstract>
<note confidence="0.63212775">Figure 6 The Intentional Structure of Example 14 (showing topics the writer intends to discuss) Chain Range Intention Range 1 1-44 1 1-44</note>
<phone confidence="0.851660222222222">2.1 2-12 1.1.1, 1.1.2 1-12 2.2 16 end of 1.1.3.1 16 2.3 24 end of 1.1.3.3 25 3 13-15 1.1.3.1 13-16 4 19-20 1.1.3.2 17-22 5 31-33 1.2.2 31-33 6 34-38 1.2.3 34-39 7,8 1-3 1.1.1 1-7 9 7-8 1.1.2 8-12</phone>
<abstract confidence="0.976424866666666">Figure 7 Correspondences between lexical and intentional structures 38 Morris and Hirst Lexical Cohesion would correspond exactly to a structural unit. In addition, repeated eight times between sentence 2 and sentence 26, corresponding to intention 1.1 (earlier to suburban life). repeated eleven times throughout the entire example, indicating the continuity in structure between sentences 1-44. 2.1, darkness, . . . from sentence 2 to sentence 12, corresponds to intentions 1.1.1 (hatred of commuting) and 1.1.2 (hatred of suburbs). More textual information is needed to separate intentions 1.1.1 and 1.1.2. There is a one-word return to chain 2 at sentences 16 and 24, strongly indicating that chain 2 corresponds to intention 1.1, which runs from sentence 1 to sentence 25. Also, segment 2.2 coincides with the end of intention 1.1.3.1 (how life changed), and segment 2.3 coincides with the end of intention 1.1.3.3 (old familiar aversion to suburbs). This situation illustrates how chain returns help indicate the structure of the text. If chain returns were not considered, chain 2 would end at sentence 12, and the structural implications of the single-word returns would be lost. It is intuitive that the two words links back to the rest of intention 1.1. The link provided by the last return, especially strong, since it occurs after the diversion describing the attempt to a nice house in the suburbs. the third reiteration of the word in chain 2. 3, wife, . . .}, to intention 1.1.3.1 (if the unfortunate return mentioned in section 3.4.2 is ignored) and chain 4 tolerance}, corresponds to intention 1.1.3.2 (expensive houses in Metro Toronto). The boundaries of chain 4 are two sentences inside the boundaries of the intention. The existence of a lexical chain is a clue to the existence of a separate intention, and boundaries within one or two sentences of the intention boundaries are considered to be close matches. 5, pine, . . . corresponds closely to intention 1.2.2 (forested area). 6, matching, . . . corresponds closely to intention 1.2.3 (pleasant Chains 7, initial, final}, 8, dusk, darkness}, a couple of short chains (three words long) that overlap. They collectively correspond to intention 1.1.1 (hatred of commuting). The fact that they are short and overlapping suggests that they could be taken together as a whole. 9, setting, surrounding}, to intention 1.1.2 (hated Even though the chain is a lot shorter in length than the intention, its presence is a clue to the existence of a separate intention in its textual vicinity. Since the lexical chain boundary is more than two sentences away from the intention boundary, other textual information would be required to confirm the structure. Overall, the lexical chains found in this example provide a good clue for the determination of the intentional structure. In some cases, the chains correspond exactly to an intention. It should also be stressed, however, that the lexical structures cannot be used on their own to predict an exact structural partitioning of the text. This of course was never expected. As a good example of the limitations of the tool, intention 1.2 (nice new suburb) starts in sentence 26, but there are no new lexical chains starting there. The only clue to the start of the new intention would be the ending of chain 2 darkness, . . . This example also provides a good illustration (chain 2) of the importance of chain returns being used to indicate a high-level intention spanning the length of the entire chain (including all segments). Also, the returns coincided with intentional boundaries. 5. Conclusions The motivation behind this work was that lexical cohesion in text should correspond in some way to the structure of the text. Since lexical cohesion is a result of a unit of text being, in some recognizable semantic way, about a single topic, and text structure 39 Computational Linguistics Volume 17, Number 1 analysis involves finding the units of text that are about the same topic, one should have something to say about the other. This was found to be true. The lexical chains computed by the algorithm given in Section 3.2.3 correspond closely to the intentional structure produced from the structural analysis method of Grosz and Sidner (1986). This is important, since Grosz and Sidner give no method for computing the intentions or linguistic segments that make up the structure that they propose. Hence the concept of lexical cohesion, defined originally by Halliday and Hasan (1976) and expanded in this work, has a definite use in an automated text understanding system. Lexical chains are shown to be almost entirely computable with the relations defined in Section 3.2.2. The computer implementation of this type of thesaurus access would be a straightforward task involving traditional database techniques. The program to implement the algorithm given in Section 3.2.3 would also be straightforward. However, automated testing could help fine-tune the parameters, and would help to indicate any unfortunate chain linkages. Although straightforward from an engineering point of view, the automation would require a significant effort. A machine-readable thesaurus with automated index searching and lookup is required. The texts we have analyzed, here and elsewhere (Morris 1988) are general-interest articles taken from magazines. They were chosen specifically to illustrate that lexical cohesion, and hence this tool, is not domain-specific. 5.1 Improvements on Earlier Research The methods used in this work improve on those from Halliday and Hasan (1976). Halliday and Hasan related words back to the first word to which they are tied, rather than forming explicit lexical chains that include the relationships to intermediate words in the chain. They had no notions of transitivity, distance between words in a chain, or chain returns. Their intent was not a computational means of finding lexical chains, and they did not suggest a thesaurus for this purpose. Ventola (1987) analyzed lexical cohesion and text structure within the framework of systemic linguistics and the specific domain of service encounters such as the exchange of words between a client at a post office and a postal worker. Ventola&apos;s chain-building rule was that each lexical item is &amp;quot;taken back once to the nearest preceding lexically cohesive item regardless of distance&amp;quot; (p. 131). In our work the related words in a chain are seen as indicating structural units of text, and hence distance between words is relevant. Ventola did not have the concept of chain returns, and transitivity was allowed up to any level. Her research was specific to the domain used. She does not discuss a computational method of determining the lexical chains. Hahn (1985) developed a text parsing system that considers lexical cohesion. Nouns in the text are mapped directly to the underlying model of the domain, which was implemented as a frame-structured knowledge base. Hahn viewed lexical cohesion as a local phenomenon between words in a sentence and the preceding one. There was also an extended recognizer that worked for cohesion contained within paragraph boundaries. Recognizing lexical cohesion was a matter of searching for ways of relating frames and slots in the database that are activated by words in the text. Heavy reliance is put on the &amp;quot;formally clear cut model of the underlying domain&amp;quot; (Hahn 1985, p. 3). However, general-interest articles such as we analyzed do not have domains that be priori represented as frames with slot values in such a manner that lexical cohesion will correspond directly to them. Our work uses lexical cohesion as it naturally occurs in domain-independent text as an indicator of unity, rather than fitting a domain model to the lexical cohesion. Hahn does not use the concept of chain returns or transitivity. Sedelow and Sedelow (1986, 1987) have done a significant amount of research 40 Morris and Hirst Lexical Cohesion on the thesaurus as a knowledge source for use in a natural language understanding system. They have been interested in the application of clustering patterns in the thesaurus. Their student Bryan (1973) proposed a graph-theoretic model of the thesaurus. A boolean matrix is created with words on one axis and categories on the other. A cell is marked as true if a word associated with a cell intersects with the category associated with a cell. Paths or chains in this model are formed by traveling along rows or columns to other true cells. Semantic &amp;quot;neighborhoods&amp;quot; are grown, consisting of the set of chains emanating from an entry. It was found that without some concept of chain strength, the semantic relatedness of these neighborhoods decays, partly due to homographs. Strong links are defined in terms of the degree of overlap between categories and words. A strong link exists where at least two categories contain more than one word in common, or at least two words contain more than one category in common. The use of strong links was found to enable the growth of strong semantic chains with homograph disambiguation. This concept is different from that used in our work. Here, by virtue of words cooccurring in a text and then also containing at least one category in common or being in the same category, they are considered lexically related and no further strength is needed. We use the thesaurus as a validator of lexical relations that are possible due to the semantic relations among words in a text. 5.2 Further Research It has already been mentioned that the concept of chain strength needs much further work. The intuition is that the stronger a chain, the more likely it is to have a corresponding structural component. The integration of this tool with other text understanding tools is an area that will require a lot of work. Lexical chains do not always correspond exactly to intentional structure, and when they do not, other textual information is needed to obtain the correct correspondences. In the example given, there were cases where a lexical chain did correspond to an intention, but the sentences spanned by the lexical chain and the intention differed by more than two. In these cases, verification of the possible correspondence must be accomplished through the use of other textual information such as semantics or pragmatics. Cue words would be interesting to address, since such information seems to be more computationally accessible than underlying intentions. It would be useful to automate this tool and run a large corpus of text through it. We suspect that the chain-forming parameter settings (regarding transitivity and distances between words) will be shown to vary slightly according to author&apos;s style and the type of text. As it is impossible to do a complete and error-free lexical analysis of large text examples in a limited time-frame, automation is desirable. It could help shed some light on possible unfortunate chain linkages. Do they become problematic, and if so, when does this tend to happen? Research into limiting unfortunate linkages and detecting when the method is likely to produce incorrect results should be done (cf. Charniak 1986). Analysis using different theories of text structure was not done, but could prove insightful. The independence of different people&apos;s intuitive chains and structure assignments was also not addressed by this paper. A practical limitation of this work is that it depends on a thesaurus as its knowledge base. A thesaurus is as good as the work that went into creating it, and also depends on the perceptions, experience, and knowledge of its creators. Since language is not static, a thesaurus would have to be continually updated to remain current. Furno one thesaurus exists that meets all needs. Thesaurus, example, is a general thesaurus that does not contain lexical relations specific to the geography 41 Computational Linguistics Volume 17, Number 1 of Africa or quantum mechanics. Therefore, further work needs to be done on identifying other sources of word knowledge, such as domain-specific thesauri, dictionaries, and statistical word usage information, that should be integrated with this work. As anonymous referee pointed out to us, not included in chain containing words were not in a general thesaurus, and were also missed by the authors! Section 1 mentioned that lexical chains would be also useful in providing a context for word sense disambiguation and in narrowing to specific word meanings. As an example of a chain providing useful information for word sense disambiguation, words 1 to 15 of chain 2.1 of the example: darkness, panicky, mournful, deadly, hating, aversion, cruel, relentless, weird, eerie, cold, barren, sterile, ... In context of all of these words, it is clear that not refer to an to reproduce, but to a coldness. use of lexical chains for ambiguity resolution is a promising area for further research. Acknowledgments Thanks to Robin Cohen, Jerry Hobbs, Eduard Hovy, Ian Lancashire, and anonymous referees for valuable discussions of the ideas in this paper. Thanks to Chrysanne DiMarco, Mark Ryan, and John Morris for commenting on earlier drafts.</abstract>
<note confidence="0.9223435">This work was financially assisted by the Government of Ontario, the Department of</note>
<affiliation confidence="0.52473">Computer Science of the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.</affiliation>
<title confidence="0.660071333333333">We are grateful to Jay Teitel for allowing us to reprint text from his article &amp;quot;Outland.&amp;quot; References</title>
<author confidence="0.67521">Robert M Bryan</author>
<abstract confidence="0.5912745">and graph theory applications to research,&amp;quot; in analysis, by Sally Yeates Sedelow, University of Kansas.</abstract>
<note confidence="0.754215">Lewis (1872). the Looking Glass. Charniak, Eugene (1986). &amp;quot;A neat theory of parsing.&amp;quot; In 5th National Conference on Artificial Intelligence, Philadelphia, August 1986, 584-588. Grosz, Barbara and Sidner, Candance (1986). &amp;quot;Attention, intentions and the structure of Linguistics, 12(3), 175-204. Hahn, Udo (1985). &amp;quot;On lexically distributed</note>
<abstract confidence="0.63181925">text parsing. A computational model for the analysis of textuality on the level of text cohesion and text coherence.&amp;quot; In in text, by Ferenc Kiefer,</abstract>
<affiliation confidence="0.997713">Universitat Konstanz.</affiliation>
<address confidence="0.963977">Halliday, Michael and Hasan, Rugaiya</address>
<note confidence="0.881294571428571">in English. Group. Graeme (1987). Interpretation the Resolution of Ambiguity. in Natural Language Processing. Cambridge University Press. G. (1981). in Natural Understanding: A Survey. Notes in Computer Science. Springer Verlag. Hobbs, Jerry (1978). &amp;quot;Coherence and coreference.&amp;quot; Technical note 168, SRI International. K. (1985). Generation: Using</note>
<title confidence="0.330256">Discourse Strategies and Focus Constraints to</title>
<author confidence="0.369019">in</author>
<affiliation confidence="0.9170555">Natural Language Processing. Cambridge University Press.</affiliation>
<address confidence="0.839167">Morris, Jane (1988). &amp;quot;Lexical cohesion, the</address>
<note confidence="0.883227741935484">thesaurus, and the structure of text.&amp;quot; Technical report CSRI-219, Department of Computer Science, University of Toronto. Postman, Leo and Keppel, Geoffrey, editors of Word Association. Academic Press. Rachel (1985). Computers to Talk Like You and Me: Discourse Context, and Semantics (An ATN Model). MIT Press. P. (1977). International Fourth Edition. and Row Publishers Inc. Sedelow, Sally and Sedelow, Walter (1987). space.&amp;quot; and 235-245. Sedelow, Sally and Sedelow, Walter (1986). &amp;quot;Thesaural knowledge representation.&amp;quot; In Proceedings, 2nd Annual Conference of the University of Waterloo Centre for the New Oxford English Dictionary: Advances in of Waterloo. E. (1987). Structure of Social Interaction: A Systemic Approach to the of Service Encounters. Linguistic Series. Frances Pinter Publishers. 42 Morris and Hirst Lexical Cohesion Appendix A Chain 1</note>
<title confidence="0.402137">Word Sentence Lexical Chain</title>
<abstract confidence="0.8823375">1. suburbs 1 2. driving 2 3. Volkswagen 2 4. car&apos;s 3 (4, 2)6 5. lights 3 6. commuters 4 7. traffic 4 2)6 (7, 8. Volks 5 9. apartment 5 10. city 5 (10, 2) (10, (10, 7)6 (10,</abstract>
<note confidence="0.839923629213483">11. suburbs 6 1)8 (11, (11, 12. traffic 7 2)6 (12, (12, 7)8 (12, 13. suburban 7 1-11)8 (13, (13, 14. suburbs 8 (14, 1-11-13)8 (14, 9-10-13)6 (14, 2-12)r° 15. residentialness 8 (15, 2-7-12)r° 16. neighbourhood 9 1-11-13-14)6 (16, 17. community 10 18. suburban 12 1-11-13-14)8 (18, (18, 19. drive 12 (19, (19, 1-9-11-13-14- (20, 9-10-16) (20, 20. suburban 12 1-11-13-14-18)8 (20, (20, 21. city 14 10)8 (21, (21, 22. suburbs 14 1-11-13-14-18-20)8 (22, (22; 2- 23. apartments 14 9)8 (23, 24. Bloor St. 14 25. Bathurst St. 15 26. Finch St. 15 27. driving 16 2-19)8 (27, (27, 4)6 (27, 1-9-11- 13-14-15-1 28. route 16 (28, 1-2-9-10-11-13-14-15-16-18-19-20-21-22-23-27)6 29. driven 16 2-19-27-29)8 (29, (29, 4-28)6 (29, 30. house 17 (30, 2- 31. St. Clair 18 32. Eglinton 19 43 Computational Linguistics Volume 17, Number 1 Chain 1 (continued) Word Sentence Lexical Chain 33. streets 20 (33, 1-10-13-14-15-16-18-20-21-22-23-30) (33, 2- 34. Eglinton 21 35. Lawrence 21 36. Wilson 21 37. Sheppard 21 38. town-house 22 (38, 30)8 (38, 1-10-13-14-15-16-18-20-21-22-23)6 (38, 2-4-7-12-19-27-28-29-33)P° 39. Steeles 22 40. car 25 2-19-27-29)6 (40, 41. drove 25 (41, 2-19-27-29)8 (41, 7-10-12-21)6 (41, 4-28)6 (41, 1-9-11-13-14-15-16-18-20-22-30-38)P° 42. Danforth 25 43. porches 25 33)6 (43, 1-4-10-13-14-15-18-20-21-22-23-30- (43, (43, 44. drove 26 (44, 2-19-27-29-41)8 (44, 7-10-12-21)6 (44, 4-28)6 (44, 1-9-11-13-14-15-16-18-20-22-23-30-38)P° 45. drove 27 (45, 2-19-27-29-41-44)8 (45, 7-10-12-21)6 (45, 4-28)6 (45, 1-9-11-13-14-15-16-18-20-22-23-30- 38)Tio 46. Bathurst 27 47. Steeles 27 48. Centre St. 27 49. Highway 7 27 50. trunk 28 51. road 28 (51, (51, (51, 52. light 29 (52, 5)8 53. turned 29 54. houses 30 30-38)8 (54, 1-9-10-11-13-14-15-18-20-21-22- (54, 16-28)6 (54, 2-7-12-19-29-41- 55. turned 30 (55, 53)g 56. houses 32 (56, 30-38-54)8 (56, 1-9-10-11-13-14-15-18-20-21- 22-23-33-43-51)6 (56, 16-28)6 (56, 2-7-12-19-29-41 -44)P° 57. roadway 34 51)8 (57, 1-9-10-11-13-14-15-16-18-20-21-22- (57, 43)6 7)T10 16)T38 58. lawn 35 (58, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38- (58, 28)8 (58, 2-12-19-27-29-41- (58, 59. suburb 37 (59, 1-11-13-14-18-20-22)8 (59, 30-38-56)6 (59, 9- 10-15-21-23-33-43-51)6 (59, 16-28)6 (59, 2-7-12-19 -29-41-44)P° 44 Morris and Hirst Lexical Cohesion Chain 1 (continued) Word Sentence Lexical Chain 60. suburb 37 (60, 1-11-13-14-18-20-22-59)8 (60, 30-38-56) (60, 9-10-15-21-23-33-43-51-54-56-57-59) (60, 16-28)6 61. people 40 15) (61, 1-9-10-11-13-14-18-20-21-22-23-30- (61, 2-7-12-19-27-29-41- (61, 62. Metropolitan 40 (62, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51 Toronto -54-56-57-59-60) (62, 2-7-12-19-27-29-41-44) &apos;° (62, 16-43-58)6 63. Steeles 40 64. people 41 61)8 (64, 15) (64, 1-9-10-11-13-14-18-20-21- (65, 2-7-12- (61, 65. urban 41 (65, 1-9-10-11-13-14-15-18-20-21-22-23-30-33-38-51- (65, (65, 16-43-58)6 66. Metro 42 62)8 (66, 1-9-10-11-13-14-15-18-20-21-22-23- (66, 2-7-12-19-27-29- (66, 16-43-58-64)6 67. Peel 42 68. Durham 42 69. York 42 70. population 42 30-38-54-56-61-64) (70, 1-9-10-11-13-14-15- (70, 43- (70, (70, 71. people 43 (71, 61-64)8 (71, 15-70) (71, 1-9-10-11-13-14-18- 20-21-22-23-30-33-38-51-54-56-57-59-60-62-65-66)6 (71, 72. Toronto 43 73. population 43 70)8 (73, 30-38-51-54-56-61-65-71) (73, 1-9- 10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62-65 (73, 43-58)8 (73, 74. city 43 (74, 1-2-7-9-11-12-13-14-15-18-19-20- 22-23-27-29-30-33-38-41-44-51-54-56-57-59-60-62- (74, 16-28-43-58-65-70-71-73)6 (74, 75. Toronto 43 76. population 43 (76, 70-73)8 (76, 30-38-54-56-61-64-71) (76, 1-9- 10-11-13-14-15-18-20-21-22-23-33-51-57-59-60-62- (76, 43-58)8 (76, 2-7-12-19-27-29-41- (76, 77. suburbs 43 1-11-13-14-18-20-22-59-60)8 (77, 30-38-56-62- (77, (77, 16-28 (77, 78. Etobicoke 43 45 Computational Linguistics Volume 17, Number 1 Chain 1 (continued) Word Sentence Lexical Chain 79. Scarborough 43 80. people 44 61-64-71)S (80, (80, 1-9-10-11-13-14- 18-20-21-22-23-30-33-38-51-54-56-57-59-60-62-65- (80, (80, 81. Toronto 44 82. city 44 (82, 10-21-74)S (82, 1-2-7-9-11-12-13-1445-18-19- 20-22-23-27-29-30-33-38-41-44-46-47-51-54-56-57- (82, 16-28-43-58-64-70-71-73-76- (82, 83. suburbia 44 (83, 1-11-13-14-18-20-22-59-60-77)S (83, 30-38-56- (83, 9-10-15-21-23-33-43-51-82) (83, 16-28- (83, 84. people 44 61-64-71-80)8 (84, (84, 1-9-10-11- 13-14-18-20-21-22-23-30-33-38-51-54-56-57-59-60 (84, 2-7-12-19-27-29-41- (84, Chain 2, Segment 1 Word Sentence Lexical Chain 1 afflicted 2 2. darkness 3 (2, 1), 3. panicky 3 (3, 1) (3, 2)3 4. mournful 3 1) (4, (4, 3) 5. exciting 3 (5, 1-4) (5, 2-3)3 6. deadly 3 (6, 1-4) (6, 2-3-5)8 7. hating 4 (7, 2-3-5-6) 8. aversion 7 (8, 7) (8, 1-4) (8, 2-3-5-6)g 9. cruel 7 (9, 2-3-5-6-8) 10. relentless 8 (10, 1-4-7) (10, 2-3-5-6-8)8 11. weird 8 (11, (11, 2-3-5-6-8)8 12. eerie 9 (12, 3-11) (12, 1-4-7-10) (12, 2-3-5-6-8)8 13. cold 11 (13, (13, 2-3-5-6-10)8 14. barren 11 (14, 1-2-3-4-5-8-9-10-11-12-13)r 15. sterile 11 (15, 14) (15, 6-7) (15, 1-2-3-4-5-8-9-10-11-12- 16. hated 12 7)8 (16, 1-4-6-8-9-13) (16, 14-15) (16, 2-3- 5-10-11-12) 17. cruel 12 (17, 9)8 (17, 1-4-7-10) (17, 2-3-5-6-8-11-12-13)8 (17, 14-15)r 46 Morris and Hirst Lexical Cohesion Chain 2, Segment 2 Word Sentence Lexical Chain 18. perversely 16 (18, Chain 2, Segment 3 Word Sentence Lexical Chain 19. cruel 24 9-17)8 (19, 1-4-7-10)8 (19, 2-3-5-6-8-11-12- (19, Chain 3 Word Sentence Lexical Chain 1. married 13 2. wife 14 (2, 1)8 3. wife 15 (3, 1)8 (3, 2)8 4. wife 27 (4, 2-3)8 (4, 1)8 Chain 4 Word Sentence Lexical Chain 1. conceded 19 (2, 1)8 2. tolerance 20 Chain 5</note>
<title confidence="0.236229">Word Sentence Lexical Chain</title>
<abstract confidence="0.7900823">1. virgin 31 2. pine 31 3. bush 31 (3, 1)8 4. trees 32 (4, 1)8 (4, 3)8 5. trunks 32 6. trees 33 (6, 4)8 (6, 1-3)8 Chain 6 Word Sentence Lexical Chain 1. hand-in-hand 34 2. matching 34 3. whispering 35 4. laughing 35 5. warm 38 (5, 1)8 (5, 4)8 Chain 7 Word Sentence Lexical Chain 1. first 1 2. initial 1 (2, 1)8 3. final 2 (3, 2-1)8 47 Computational Linguistics Volume 17, Number 1 Chain 8 Word Sentence Lexical Chain 1. night 2 2. dusk 3 3. darkness 3 Chain 9 Word Sentence Lexical Chain 1. environment 7 2. setting 7 3. surrounding 8</abstract>
<intro confidence="0.692672">48</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert M Bryan</author>
</authors>
<title>Abstract thesauri and graph theory applications to thesaurus research,&amp;quot; in Automated language analysis, edited by Sally Yeates Sedelow,</title>
<date>1973</date>
<institution>University of Kansas.</institution>
<contexts>
<context position="57422" citStr="Bryan (1973)" startWordPosition="9512" endWordPosition="9513">a manner that lexical cohesion will correspond directly to them. Our work uses lexical cohesion as it naturally occurs in domain-independent text as an indicator of unity, rather than fitting a domain model to the lexical cohesion. Hahn does not use the concept of chain returns or transitivity. Sedelow and Sedelow (1986, 1987) have done a significant amount of research 40 Morris and Hirst Lexical Cohesion on the thesaurus as a knowledge source for use in a natural language understanding system. They have been interested in the application of clustering patterns in the thesaurus. Their student Bryan (1973) proposed a graph-theoretic model of the thesaurus. A boolean matrix is created with words on one axis and categories on the other. A cell is marked as true if a word associated with a cell intersects with the category associated with a cell. Paths or chains in this model are formed by traveling along rows or columns to other true cells. Semantic &amp;quot;neighborhoods&amp;quot; are grown, consisting of the set of chains emanating from an entry. It was found that without some concept of chain strength, the semantic relatedness of these neighborhoods decays, partly due to homographs. Strong links are defined in</context>
</contexts>
<marker>Bryan, 1973</marker>
<rawString>Bryan, Robert M. (1973). &amp;quot;Abstract thesauri and graph theory applications to thesaurus research,&amp;quot; in Automated language analysis, edited by Sally Yeates Sedelow, University of Kansas.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carroll</author>
</authors>
<title>Lewis (1872). Through the Looking Glass.</title>
<marker>Carroll, </marker>
<rawString>Carroll, Lewis (1872). Through the Looking Glass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A neat theory of marker parsing.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 5th National Conference on Artificial Intelligence,</booktitle>
<pages>584--588</pages>
<location>Philadelphia,</location>
<contexts>
<context position="60424" citStr="Charniak 1986" startWordPosition="10010" endWordPosition="10011"> suspect that the chain-forming parameter settings (regarding transitivity and distances between words) will be shown to vary slightly according to author&apos;s style and the type of text. As it is impossible to do a complete and error-free lexical analysis of large text examples in a limited time-frame, automation is desirable. It could help shed some light on possible unfortunate chain linkages. Do they become problematic, and if so, when does this tend to happen? Research into limiting unfortunate linkages and detecting when the method is likely to produce incorrect results should be done (cf. Charniak 1986). Analysis using different theories of text structure was not done, but could prove insightful. The independence of different people&apos;s intuitive chains and structure assignments was also not addressed by this paper. A practical limitation of this work is that it depends on a thesaurus as its knowledge base. A thesaurus is as good as the work that went into creating it, and also depends on the perceptions, experience, and knowledge of its creators. Since language is not static, a thesaurus would have to be continually updated to remain current. Furthermore, no one thesaurus exists that meets al</context>
</contexts>
<marker>Charniak, 1986</marker>
<rawString>Charniak, Eugene (1986). &amp;quot;A neat theory of marker parsing.&amp;quot; In Proceedings, 5th National Conference on Artificial Intelligence, Philadelphia, August 1986, 584-588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candance Sidner</author>
</authors>
<title>Attention, intentions and the structure of discourse.&amp;quot;</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="8780" citStr="Grosz and Sidner (1986)" startWordPosition="1415" endWordPosition="1418">ext for consideration of semantic distance. 1.2.2 Cohesion and Discourse Structure. The second major importance of lexical chains is that they provide a clue for the determination of coherence and discourse structure. 23 Computational Linguistics Volume 17, Number 1 When a chunk of text forms a unit within a discourse, there is a tendency for related words to be used. It follows that if lexical chains can be determined, they will tend to indicate the structure of the text. We will describe the application of lexical cohesion to the determination of the discourse structure that was proposed by Grosz and Sidner (1986). Grosz and Sidner propose a structure common to all discourse, which could be used along with a structurally dependent focus of attention to delineate and constrain referring expressions. In this theory there are three interacting components: linguistic structure, intentional structure, and attentional state. Linguistic structure is the segmentation of discourse into groups of sentences, each fulfilling a distinct role in the discourse. Boundaries of segments can be fuzzy, but some factors aiding in their determination are clue words, changes in intonation (not helpful in written text), and c</context>
<context position="39937" citStr="Grosz and Sidner (1986)" startWordPosition="6553" endWordPosition="6556">on will concentrate on analyzing correspondences between lexical chains and structural units of text, including: • the correspondence of chain boundaries to structural unit boundaries; • returns to existing chains and what they indicate about structural units; • lexical chain strength and reliability of predicting correspondences between chains and structural units; • an analysis of problems encountered, and when extra textual information is required to validate the correspondences between lexical chains and structural components. The text structure theory chosen for this analysis was that of Grosz and Sidner (1986). It was chosen because it is an attempt at a general domain-independent theory of text structure that has gained a significant acceptance in the field as a good standard approach. The methodology we used in our analyses was as follows: 1. We determined the lexical chain structure of the text using the algorithm given in Section 3.2.3. (In certain rare cases where the algorithm did not form intuitive lexical chains properly, it is noted, both in Section 3.4 and in the analysis in this section. The intuitive chain was used for the analysis; however the lexical chain data given in Appendix A sho</context>
<context position="53506" citStr="Grosz and Sidner (1986)" startWordPosition="8889" endWordPosition="8892">work was that lexical cohesion in text should correspond in some way to the structure of the text. Since lexical cohesion is a result of a unit of text being, in some recognizable semantic way, about a single topic, and text structure 39 Computational Linguistics Volume 17, Number 1 analysis involves finding the units of text that are about the same topic, one should have something to say about the other. This was found to be true. The lexical chains computed by the algorithm given in Section 3.2.3 correspond closely to the intentional structure produced from the structural analysis method of Grosz and Sidner (1986). This is important, since Grosz and Sidner give no method for computing the intentions or linguistic segments that make up the structure that they propose. Hence the concept of lexical cohesion, defined originally by Halliday and Hasan (1976) and expanded in this work, has a definite use in an automated text understanding system. Lexical chains are shown to be almost entirely computable with the relations defined in Section 3.2.2. The computer implementation of this type of thesaurus access would be a straightforward task involving traditional database techniques. The program to implement the</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, Barbara and Sidner, Candance (1986). &amp;quot;Attention, intentions and the structure of discourse.&amp;quot; Computational Linguistics, 12(3), 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
</authors>
<title>On lexically distributed text parsing. A computational model for the analysis of textuality on the level of text cohesion and text coherence.&amp;quot; In Linking in text, edited by Ferenc Kiefer,</title>
<date>1985</date>
<location>Universitat Konstanz.</location>
<contexts>
<context position="56001" citStr="Hahn (1985)" startWordPosition="9280" endWordPosition="9281">h as the exchange of words between a client at a post office and a postal worker. Ventola&apos;s chain-building rule was that each lexical item is &amp;quot;taken back once to the nearest preceding lexically cohesive item regardless of distance&amp;quot; (p. 131). In our work the related words in a chain are seen as indicating structural units of text, and hence distance between words is relevant. Ventola did not have the concept of chain returns, and transitivity was allowed up to any level. Her research was specific to the domain used. She does not discuss a computational method of determining the lexical chains. Hahn (1985) developed a text parsing system that considers lexical cohesion. Nouns in the text are mapped directly to the underlying model of the domain, which was implemented as a frame-structured knowledge base. Hahn viewed lexical cohesion as a local phenomenon between words in a sentence and the preceding one. There was also an extended recognizer that worked for cohesion contained within paragraph boundaries. Recognizing lexical cohesion was a matter of searching for ways of relating frames and slots in the database that are activated by words in the text. Heavy reliance is put on the &amp;quot;formally clea</context>
</contexts>
<marker>Hahn, 1985</marker>
<rawString>Hahn, Udo (1985). &amp;quot;On lexically distributed text parsing. A computational model for the analysis of textuality on the level of text cohesion and text coherence.&amp;quot; In Linking in text, edited by Ferenc Kiefer, Universitat Konstanz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Rugaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group.</publisher>
<contexts>
<context position="1619" citStr="Halliday and Hasan (1976)" startWordPosition="263" endWordPosition="266">rovide a semantic context for interpreting words, concepts, and sentences. 1. Lexical Cohesion A text or discourse is not just a set of sentences, each on some random topic. Rather, the sentences and phrases of any sensible text will each tend to be about the same things — that is, the text will have a quality of unity. This is the property of cohesion — the sentences &amp;quot;stick together&amp;quot; to function as a whole. Cohesion is achieved through back-reference, conjunction, and semantic word relations. Cohesion is not a guarantee of unity in text but rather a device for creating it. As aptly stated by Halliday and Hasan (1976), it is a way of getting text to &amp;quot;hang together as a whole.&amp;quot; Their work on cohesion has underscored its importance as an indicator of text unity. Lexical cohesion is the cohesion that arises from semantic relationships between words. All that is required is that there be some recognizable relation between the words. Halliday and Hasan have provided a classification of lexical cohesion based on the type of dependency relationship that exists between words. There are five basic classes: 1. Reiteration with identity of reference: Example 1 1. Mary bit into a peach. 2. Unfortunately the peach wasn</context>
<context position="6946" citStr="Halliday and Hasan 1976" startWordPosition="1120" endWordPosition="1123"> understanding systems: 1. Lexical chains provide an easy-to-determine context to aid in the resolution of ambiguity and in the narrowing to a specific meaning of a word. 2. Lexical chains provide a clue for the determination of coherence and discourse structure, and hence the larger meaning of the text. 1.2.1 Word Interpretation in Context. Word meanings do not exist in isolation. Each word must be interpreted in its context. For example, in the context {gin, alcohol, sober, drinks}, the meaning of the noun drinks is narrowed down to alcoholic drinks. In the context {hair, curl, comb, wave} (Halliday and Hasan 1976), wave means a hair wave, not a water wave, a physics wave, or a friendly hand wave. In these examples, lexical chains can be used as a contextual aid to interpreting word meanings. In earlier work, Hirst (1987) used a system called &amp;quot;Polaroid Words&amp;quot; to provide for intrasentential lexical disambiguation. Polaroid Words relied on a variety of cues, including syntax, selectional restrictions, case frames, and — most relevant here — a notion of semantic distance or relatedness to other words in the sentences; a sense that had such a relationship was preferred over one that didn&apos;t. Relationships we</context>
<context position="15718" citStr="Halliday and Hasan (1976)" startWordPosition="2522" endWordPosition="2525">ot related coherently: Example 9 Wash and core six apples. Use them to cut out the material for your new suit. They tend to add a lot to the color and texture of clothing. Actually, maybe you should use five of them instead of six, since they are quite large. 25 Computational Linguistics Volume 17, Number 1 Similarly, coherence can exist without textual cohesion: Example 10 I came home from work at 6:00 p.m. Dinner consisted of two chicken breasts and a bowl of rice. Of course, most sentences that relate coherently do exhibit cohesion as well) 1.4 The Importance of Both Cohesion and Coherence Halliday and Hasan (1976) give two examples of lexical cohesion involving identity of reference: Example 11 1. Wash and core six cooking apples. 2. Put them into a fireproof dish. Example 12 1. Wash and core six cooking apples. 2. Put the apples into a fireproof dish. Reichman (1985, p. 180) writes &amp;quot;It is not the use of a pronoun that gives cohesion to the wash-and-core-apples text. These utterances form a coherent piece of text not because the pronoun them is used but because they jointly describe a set of cooking instructions&amp;quot; (emphasis added). This is an example of lumping cohesion and coherence together as one phe</context>
<context position="23089" citStr="Halliday and Hasan (1976)" startWordPosition="3711" endWordPosition="3714">is holds even for the nonsystematic semantic relations, which are the most problematic by definition. A thesaurus simply groups related words without attempting to explicitly name each relationship. In a traditional computer database, a systematic semantic relationship can be represented by a slot value for a frame, or by a named link in a semantic network. If it is hard to classify a relationship in a systematic semantic way, it will be hard to represent the relationship in a traditional frame or semantic network formalism. Of the 16 nonsystematic semantic lexical chains given as examples in Halliday and Hasan (1976), 14 were found in Roget&apos;s Thesaurus (1977) using the relations given in Section 3.2.2. This represents an 87% hit rate (but not a big sample space). Word associations show a strong tendency to be findable in a thesaurus. Of the 16 word association pairs given in Hirst (1987), 14 were found in Roget&apos;s Thesaurus (1977). Since two of the word senses were not contained in the thesaurus at all, this represents a 100% hit rate among those that were. Situational word relationships are not as likely to be found in a general thesaurus. An example of a situational relationship is between car and lights</context>
<context position="25301" citStr="Halliday and Hasan (1976)" startWordPosition="4077" endWordPosition="4080">hains in each text. We then formalized our intuitions into an algorithm, using our experience with the texts to set values for the following parameters (to be discussed below). • thesaural relations • transitivity of word relations • distance (in sentences) allowable between words in a chain The aim was to find efficient, plausible methods that will cover enough cases to ensure the production of meaningful results. 3.2 Forming Lexical Chains 3.2.1 Candidate Words. The first decision in lexical chain formation is which words in the text are candidates for inclusion in chains. As pointed out by Halliday and Hasan (1976), repetitive occurrences of closed-class words such as pronouns, prepositions, and verbal auxiliaries are obviously not considered. Also, high-frequency words like good, do, and taking do not normally enter into lexical chains (with some exceptions such as takings used in the sense of earnings). For example, in (13) only the italicized words should be considered as lexical chain candidates: Example 13 My maternal grandfather lived to be 111. Zayde was lucid to the end, but a few years before he died the family assigned me the task of talking to him about his problem with alcohol. It should be </context>
<context position="53749" citStr="Halliday and Hasan (1976)" startWordPosition="8927" endWordPosition="8930">putational Linguistics Volume 17, Number 1 analysis involves finding the units of text that are about the same topic, one should have something to say about the other. This was found to be true. The lexical chains computed by the algorithm given in Section 3.2.3 correspond closely to the intentional structure produced from the structural analysis method of Grosz and Sidner (1986). This is important, since Grosz and Sidner give no method for computing the intentions or linguistic segments that make up the structure that they propose. Hence the concept of lexical cohesion, defined originally by Halliday and Hasan (1976) and expanded in this work, has a definite use in an automated text understanding system. Lexical chains are shown to be almost entirely computable with the relations defined in Section 3.2.2. The computer implementation of this type of thesaurus access would be a straightforward task involving traditional database techniques. The program to implement the algorithm given in Section 3.2.3 would also be straightforward. However, automated testing could help fine-tune the parameters, and would help to indicate any unfortunate chain linkages. Although straightforward from an engineering point of v</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, Michael and Hasan, Rugaiya (1976). Cohesion in English. Longman Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity.</title>
<date>1987</date>
<booktitle>Studies in Natural Language Processing.</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7157" citStr="Hirst (1987)" startWordPosition="1160" endWordPosition="1161">on of coherence and discourse structure, and hence the larger meaning of the text. 1.2.1 Word Interpretation in Context. Word meanings do not exist in isolation. Each word must be interpreted in its context. For example, in the context {gin, alcohol, sober, drinks}, the meaning of the noun drinks is narrowed down to alcoholic drinks. In the context {hair, curl, comb, wave} (Halliday and Hasan 1976), wave means a hair wave, not a water wave, a physics wave, or a friendly hand wave. In these examples, lexical chains can be used as a contextual aid to interpreting word meanings. In earlier work, Hirst (1987) used a system called &amp;quot;Polaroid Words&amp;quot; to provide for intrasentential lexical disambiguation. Polaroid Words relied on a variety of cues, including syntax, selectional restrictions, case frames, and — most relevant here — a notion of semantic distance or relatedness to other words in the sentences; a sense that had such a relationship was preferred over one that didn&apos;t. Relationships were determined by marker passing along the arcs in a knowledge base. The intuition was that semantically related concepts will be physically close in the knowledge base, and can thus be found by traversing the ar</context>
<context position="23365" citStr="Hirst (1987)" startWordPosition="3761" endWordPosition="3762">by a slot value for a frame, or by a named link in a semantic network. If it is hard to classify a relationship in a systematic semantic way, it will be hard to represent the relationship in a traditional frame or semantic network formalism. Of the 16 nonsystematic semantic lexical chains given as examples in Halliday and Hasan (1976), 14 were found in Roget&apos;s Thesaurus (1977) using the relations given in Section 3.2.2. This represents an 87% hit rate (but not a big sample space). Word associations show a strong tendency to be findable in a thesaurus. Of the 16 word association pairs given in Hirst (1987), 14 were found in Roget&apos;s Thesaurus (1977). Since two of the word senses were not contained in the thesaurus at all, this represents a 100% hit rate among those that were. Situational word relationships are not as likely to be found in a general thesaurus. An example of a situational relationship is between car and lights, where the two words are clearly related in the situation involving a car&apos;s lights, but the relationship will not be found between them in a general thesaurus. 28 Morris and Hirst Lexical Cohesion 3. Finding Lexical Chains 3.1 General Methodology We now describe a method of </context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Hirst, Graeme (1987). Semantic Interpretation and the Resolution of Ambiguity. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Anaphora in Natural Language Understanding: A Survey.</title>
<date>1981</date>
<booktitle>Lecture Notes in Computer Science.</booktitle>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="11950" citStr="Hirst 1981" startWordPosition="1923" endWordPosition="1924">, this is a problem area. This paper will show that lexical chains are a good indication of the linguistic segmentation. When a lexical chain ends, there is a tendency for a linguistic segment to end, as the lexical chains tend to indicate the topicality of segments. If a new lexical chain begins, this is an indication or clue that a new segment has begun. If an old chain is referred to again (a chain return), it is a strong indication that a previous segment is being returned to. We will demonstrate this in Section 4. 1.3 Cohesion and Coherence The theory of coherence relations (H obbs 1978; Hirst 1981; McKeown 1985) will now be considered in relation to cohesion. There has been some confusion as to the differences between the phenomena of cohesion and coherence, e.g., Reichman (1985). There is a danger of lumping the two together and losing the distinct contributions of each to the understanding of the unity of text. 24 Morris and Hirst Lexical Cohesion Ultimately, the difference between cohesion and coherence is this: cohesion is a term for sticking together; it means that the text all hangs together. Coherence is a term for making sense; it means that there is sense in the text. Hence th</context>
</contexts>
<marker>Hirst, 1981</marker>
<rawString>Hirst, G. (1981). Anaphora in Natural Language Understanding: A Survey. Lecture Notes in Computer Science. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Coherence and coreference.&amp;quot; Technical note 168,</title>
<date>1978</date>
<publisher>SRI International.</publisher>
<contexts>
<context position="13713" citStr="Hobbs (1978)" startWordPosition="2191" endWordPosition="2192">n relations are relations among elements in a text: reference, ellipsis, substitution, conjunction, and lexical cohesion. Since cohesion is well defined, one might expect that it would be computationally easier to identify, because the identification of ellipsis, reference, substitution, conjunction, and lexical cohesion is a straightforward task for people. We will show below that lexical cohesion is computationally feasible to identify. In contrast, the identification of a specific coherence relation from a given set is not a straightforward task, even for people. Consider this example from Hobbs (1978): Example 7 1. John can open Bill&apos;s safe. 2. He knows the combination. Hobbs identifies the coherence relation as elaboration. But it could just as easily be explanation. This distinction depends on context, knowledge, and beliefs. For example, if you questioned John&apos;s ability to open Bill&apos;s safe, you would probably identify the relation as explanation. Otherwise you could identify it as elaboration. Here is another example: Example 8 1. John bought a raincoat. 2. He went shopping yesterday on Queen Street and it rained. The coherence relation here could be elaboration (on the buying), or expl</context>
<context position="17436" citStr="Hobbs (1978)" startWordPosition="2805" endWordPosition="2806">cal cohesion is a computationally feasible clue to identifying a coherent stretch of text. In example 12, it is computationally trivial to get the word relationship between apples and apples, and this relation fits the definition of lexical cohesion. Surely this simple indicator of coherence is useful, since as stated above, there does not exist a computationally feasible method of identifying coherence in non—domain-specific text. Cohesion is a useful indicator of coherence regardless of whether it is used intentionally by writers to create coherence, or is a result of the coherence of text. Hobbs (1978) sees the resolution of coreference (which is a form of cohesion) as being subsumed by the identification of coherence. He uses a formal definition of coherence relations, an extensive knowledge base of assertions and properties of objects and actions, and a mechanism that searches this knowledge source and makes simple inferences. Also, certain elements must be assumed to be coreferential. He shows how, in example (7), an assumption of coherence allows the combination to be identified as the combination of Bill&apos;s safe and John and he to be found to be coreferential. 1 There is an interesting </context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Hobbs, Jerry (1978). &amp;quot;Coherence and coreference.&amp;quot; Technical note 168, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Studies in Natural Language Processing.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11965" citStr="McKeown 1985" startWordPosition="1925" endWordPosition="1926">problem area. This paper will show that lexical chains are a good indication of the linguistic segmentation. When a lexical chain ends, there is a tendency for a linguistic segment to end, as the lexical chains tend to indicate the topicality of segments. If a new lexical chain begins, this is an indication or clue that a new segment has begun. If an old chain is referred to again (a chain return), it is a strong indication that a previous segment is being returned to. We will demonstrate this in Section 4. 1.3 Cohesion and Coherence The theory of coherence relations (H obbs 1978; Hirst 1981; McKeown 1985) will now be considered in relation to cohesion. There has been some confusion as to the differences between the phenomena of cohesion and coherence, e.g., Reichman (1985). There is a danger of lumping the two together and losing the distinct contributions of each to the understanding of the unity of text. 24 Morris and Hirst Lexical Cohesion Ultimately, the difference between cohesion and coherence is this: cohesion is a term for sticking together; it means that the text all hangs together. Coherence is a term for making sense; it means that there is sense in the text. Hence the term coherenc</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, K. (1985). Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
</authors>
<title>Lexical cohesion, the thesaurus, and the structure of text.&amp;quot;</title>
<date>1988</date>
<tech>Technical report CSRI-219,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="41206" citStr="Morris 1988" startWordPosition="6770" endWordPosition="6771">) 2. We determined the intentional structure of the text using the theory outlined by Grosz and Sidner. 35 Computational Linguistics Volume 17, Number 1 3. We compared the lexical structure formed in step 1 with the intentional structure formed in step 2, and looked for correspondences between them. 4.2 An Example Example 14 shows one of the five texts that we analyzed. It is the first section of an article in Toronto magazine, December 1987, by Jay Teitel, entitled &amp;quot;Outland.&amp;quot;2 The tables in Appendix A show the lexical chains for the text. (The other four texts and their analyses are given in Morris 1988.) Example 14 1. III spent the first 19 years of my life in the suburbs, the initial 14 or so relatively contented, the last four or five wanting mainly to be elsewhere. 2. The final two I remember vividly: I passed them driving to and from the University of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness. 3. The car&apos;s lights never worked -- every dusk turned into a kind of medieval race against darkness, a panicky, mournful rush north, away from everything I knew was exciting, toward everything I knew was deadly. 4. I remember looking through the windows at the commuters </context>
<context position="54549" citStr="Morris 1988" startWordPosition="9047" endWordPosition="9048">2.2. The computer implementation of this type of thesaurus access would be a straightforward task involving traditional database techniques. The program to implement the algorithm given in Section 3.2.3 would also be straightforward. However, automated testing could help fine-tune the parameters, and would help to indicate any unfortunate chain linkages. Although straightforward from an engineering point of view, the automation would require a significant effort. A machine-readable thesaurus with automated index searching and lookup is required. The texts we have analyzed, here and elsewhere (Morris 1988) are general-interest articles taken from magazines. They were chosen specifically to illustrate that lexical cohesion, and hence this tool, is not domain-specific. 5.1 Improvements on Earlier Research The methods used in this work improve on those from Halliday and Hasan (1976). Halliday and Hasan related words back to the first word to which they are tied, rather than forming explicit lexical chains that include the relationships to intermediate words in the chain. They had no notions of transitivity, distance between words in a chain, or chain returns. Their intent was not a computational m</context>
</contexts>
<marker>Morris, 1988</marker>
<rawString>Morris, Jane (1988). &amp;quot;Lexical cohesion, the thesaurus, and the structure of text.&amp;quot; Technical report CSRI-219, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Postman</author>
<author>Geoffrey Keppel</author>
</authors>
<date>1970</date>
<publisher>Norms of Word Association. Academic Press.</publisher>
<location>editors</location>
<contexts>
<context position="4789" citStr="Postman and Keppel (1970)" startWordPosition="758" endWordPosition="761"> in similar situations or contexts in the world. Hence, context-specific examples such as {post office, service, stamps, pay, leave} are included in the class. (This example is from Ventola (1987), who analyzed the patterns of lexical cohesion specific to the context of service encounters.) Another example of this type is {car, lights, turning}, taken from example 14 in Section 4.2. These words are related in the situation of driving a car, but taken out of that situation, they are not related in a systematic way. Also contained in the class of collocation are word associations. Examples from Postman and Keppel (1970) are {priest, church}, {citizen, U.S.A.}, and {whistle, stop}. Again, the exact relationship between these words can be hard to classify, but there does exist a recognizable relationship. 1.1 Lexical Chains Often, lexical cohesion occurs not simply between pairs of words but over a succession of a number of nearby related words spanning a topical unit of the text. These 22 Morris and Hirst Lexical Cohesion sequences of related words will be called lexical chains. There is a distance relation between each word in the chain, and the words co-occur within a given span. Lexical chains do not stop </context>
</contexts>
<marker>Postman, Keppel, 1970</marker>
<rawString>Postman, Leo and Keppel, Geoffrey, editors (1970). Norms of Word Association. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Getting Computers to Talk Like You and Me: Discourse Context, Focus, and Semantics (An ATN Model).</title>
<date>1985</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12136" citStr="Reichman (1985)" startWordPosition="1952" endWordPosition="1953">tic segment to end, as the lexical chains tend to indicate the topicality of segments. If a new lexical chain begins, this is an indication or clue that a new segment has begun. If an old chain is referred to again (a chain return), it is a strong indication that a previous segment is being returned to. We will demonstrate this in Section 4. 1.3 Cohesion and Coherence The theory of coherence relations (H obbs 1978; Hirst 1981; McKeown 1985) will now be considered in relation to cohesion. There has been some confusion as to the differences between the phenomena of cohesion and coherence, e.g., Reichman (1985). There is a danger of lumping the two together and losing the distinct contributions of each to the understanding of the unity of text. 24 Morris and Hirst Lexical Cohesion Ultimately, the difference between cohesion and coherence is this: cohesion is a term for sticking together; it means that the text all hangs together. Coherence is a term for making sense; it means that there is sense in the text. Hence the term coherence relations refers to the relations between sentences that contribute to their making sense. Cohesion and coherence relations may be distinguished in the following way. A </context>
<context position="15976" citStr="Reichman (1985" startWordPosition="2569" endWordPosition="2570">ational Linguistics Volume 17, Number 1 Similarly, coherence can exist without textual cohesion: Example 10 I came home from work at 6:00 p.m. Dinner consisted of two chicken breasts and a bowl of rice. Of course, most sentences that relate coherently do exhibit cohesion as well) 1.4 The Importance of Both Cohesion and Coherence Halliday and Hasan (1976) give two examples of lexical cohesion involving identity of reference: Example 11 1. Wash and core six cooking apples. 2. Put them into a fireproof dish. Example 12 1. Wash and core six cooking apples. 2. Put the apples into a fireproof dish. Reichman (1985, p. 180) writes &amp;quot;It is not the use of a pronoun that gives cohesion to the wash-and-core-apples text. These utterances form a coherent piece of text not because the pronoun them is used but because they jointly describe a set of cooking instructions&amp;quot; (emphasis added). This is an example of lumping cohesion and coherence together as one phenomenon. Pronominal reference is defined as a type of cohesion (Halliday and Hasan 1976). Therefore the them in example 11 is an instance of it. The important point is that both cohesion and coherence are distinct phenomena creating unity in text. Reichman a</context>
</contexts>
<marker>Reichman, 1985</marker>
<rawString>Reichman, Rachel (1985). Getting Computers to Talk Like You and Me: Discourse Context, Focus, and Semantics (An ATN Model). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Roget</author>
</authors>
<title>Roget&apos;s International Thesaurus, Fourth Edition. Harper and Row</title>
<date>1977</date>
<publisher>Publishers Inc.</publisher>
<marker>Roget, 1977</marker>
<rawString>Roget, P. (1977). Roget&apos;s International Thesaurus, Fourth Edition. Harper and Row Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Sedelow</author>
<author>Walter Sedelow</author>
</authors>
<title>Semantic space.&amp;quot;</title>
<date>1987</date>
<journal>Computers and translation,</journal>
<volume>2</volume>
<pages>235--245</pages>
<marker>Sedelow, Sedelow, 1987</marker>
<rawString>Sedelow, Sally and Sedelow, Walter (1987). &amp;quot;Semantic space.&amp;quot; Computers and translation, 2, 235-245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Sedelow</author>
<author>Walter Sedelow</author>
</authors>
<title>Thesaural knowledge representation.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 2nd Annual Conference of the</booktitle>
<institution>University of Waterloo Centre for</institution>
<contexts>
<context position="57131" citStr="Sedelow and Sedelow (1986" startWordPosition="9463" endWordPosition="9466">e database that are activated by words in the text. Heavy reliance is put on the &amp;quot;formally clear cut model of the underlying domain&amp;quot; (Hahn 1985, p. 3). However, general-interest articles such as we analyzed do not have domains that can be a priori formally represented as frames with slot values in such a manner that lexical cohesion will correspond directly to them. Our work uses lexical cohesion as it naturally occurs in domain-independent text as an indicator of unity, rather than fitting a domain model to the lexical cohesion. Hahn does not use the concept of chain returns or transitivity. Sedelow and Sedelow (1986, 1987) have done a significant amount of research 40 Morris and Hirst Lexical Cohesion on the thesaurus as a knowledge source for use in a natural language understanding system. They have been interested in the application of clustering patterns in the thesaurus. Their student Bryan (1973) proposed a graph-theoretic model of the thesaurus. A boolean matrix is created with words on one axis and categories on the other. A cell is marked as true if a word associated with a cell intersects with the category associated with a cell. Paths or chains in this model are formed by traveling along rows o</context>
</contexts>
<marker>Sedelow, Sedelow, 1986</marker>
<rawString>Sedelow, Sally and Sedelow, Walter (1986). &amp;quot;Thesaural knowledge representation.&amp;quot; In Proceedings, 2nd Annual Conference of the University of Waterloo Centre for the New Oxford English Dictionary: Advances in Lexicology. University of Waterloo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ventola</author>
</authors>
<title>The Structure of Social Interaction: A Systemic Approach to the Semiotics of Service Encounters. Open Linguistic Series.</title>
<date>1987</date>
<publisher>Frances Pinter Publishers.</publisher>
<contexts>
<context position="4360" citStr="Ventola (1987)" startWordPosition="687" endWordPosition="688">mple 5 is an illustration of collocation where the word relationship, {garden, digging}, is nonsystematic. This type of relationship is the most problematic, especially from a knowledge representation point of view. Such collocation relationships exist between words that tend to occur in similar lexical environments. Words tend to occur in similar lexical environments because they describe things that tend to occur in similar situations or contexts in the world. Hence, context-specific examples such as {post office, service, stamps, pay, leave} are included in the class. (This example is from Ventola (1987), who analyzed the patterns of lexical cohesion specific to the context of service encounters.) Another example of this type is {car, lights, turning}, taken from example 14 in Section 4.2. These words are related in the situation of driving a car, but taken out of that situation, they are not related in a systematic way. Also contained in the class of collocation are word associations. Examples from Postman and Keppel (1970) are {priest, church}, {citizen, U.S.A.}, and {whistle, stop}. Again, the exact relationship between these words can be hard to classify, but there does exist a recognizab</context>
<context position="55250" citStr="Ventola (1987)" startWordPosition="9157" endWordPosition="9158">llustrate that lexical cohesion, and hence this tool, is not domain-specific. 5.1 Improvements on Earlier Research The methods used in this work improve on those from Halliday and Hasan (1976). Halliday and Hasan related words back to the first word to which they are tied, rather than forming explicit lexical chains that include the relationships to intermediate words in the chain. They had no notions of transitivity, distance between words in a chain, or chain returns. Their intent was not a computational means of finding lexical chains, and they did not suggest a thesaurus for this purpose. Ventola (1987) analyzed lexical cohesion and text structure within the framework of systemic linguistics and the specific domain of service encounters such as the exchange of words between a client at a post office and a postal worker. Ventola&apos;s chain-building rule was that each lexical item is &amp;quot;taken back once to the nearest preceding lexically cohesive item regardless of distance&amp;quot; (p. 131). In our work the related words in a chain are seen as indicating structural units of text, and hence distance between words is relevant. Ventola did not have the concept of chain returns, and transitivity was allowed up</context>
</contexts>
<marker>Ventola, 1987</marker>
<rawString>Ventola, E. (1987). The Structure of Social Interaction: A Systemic Approach to the Semiotics of Service Encounters. Open Linguistic Series. Frances Pinter Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>